{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF DEV Summit - Modu Labs\n",
    "\n",
    "\n",
    "## TF.DATA - Deep NLP 신성진"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF의 시작, 결국 데이터를 빠르고, 쉽게 다루는 것이 핵심\n",
    "\n",
    "## **The tf.data mission**\n",
    "\n",
    "Input piplines for Tensorflow should be:\n",
    "\n",
    "**Fast** : to keep up with GPUs and TPUs\n",
    "\n",
    "**Flexible** : to handle diverse data sources and use cases\n",
    "\n",
    "**Easy to use** : to democratize machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Transform Load for Tensorflow\n",
    "\n",
    "```python\n",
    "#Extract\n",
    "files = tf.data.Dataset.list_files(file_pattern)\n",
    "dataset = tf.data.TFRecordDataset(files)\n",
    "\n",
    "#Transform\n",
    "dataset = dataset.shuffle(10000)\n",
    "dataset = dataset.repeat(NUM_EPOCHS)\n",
    "dataset = dataset.map(lambda x: tf.parse_single_example(x, features)) #single example?\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "#Load\n",
    "iterator = dataset.make_one_shot_iterator() #Sequencial Access\n",
    "features = iterator.get_next()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n",
    "\n",
    "- CNN benchmarks reach > **13,000 images/second** with tf.data -> 8달 전과 비교하여 성능 2배 성장 (DGX - Imagenet)\n",
    " <br />\n",
    "- 텐서플로우 벤치마크 프로젝트를 사용하여 활용: www.tensorflow.org/performance/datasets_performance\n",
    "\n",
    "- New tf.contrib.data.prefetch_to_device() for GPUs tf 1.8 (tf-nightly에 보유)\n",
    "\n",
    "```python\n",
    "\n",
    "## 속도를 올리기 위해서는?? Parallel!!\n",
    "\n",
    "#Extract\n",
    "files = tf.data.Dataset.list_files(file_pattern)\n",
    "# dataset = tf.data.TFRecordDataset(files) ->\n",
    "dataset = tf.data.TFRecordDataset(files, num_parallel_reads=32)\n",
    "\n",
    "#Transform\n",
    "\n",
    "#shuffle_and_repeat: epochs와 buffers사이에서 정지하는 현상 방지\n",
    "dataset = dataset.apply(\n",
    "    tf.contrib.data.shuffle_and_repeat(10000, NUM_EPOCHS))\n",
    "\n",
    "#map_and_batch: map과 data transfer를 동시에 함\n",
    "dataset = dataset.apply(\n",
    "    tf.contrib.data.map_and_batch(lambda x: ..., BATCH_SIZE))\n",
    "\n",
    "#Load\n",
    "\n",
    "#prefetch_to_device = 그 다음 batch가 미리 GPU 메모리 대기\n",
    "dataset = dataset.apply(tf.contrib.data.prefetch_to_device(\"/gpu:0\"))\n",
    "iterator = dataset.make_one_shot_iterator() #Sequencial Access\n",
    "features = iterator.get_next()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flexibility\n",
    "\n",
    "- tf.SparseTensor를 지원 (1.5ver) -> 복잡한 Categorical data나 embedding 모델을 다룰때\n",
    "\n",
    "- Custom Python code via Dataset.from_generator() ->?\n",
    "\n",
    "- Custom C++ code via DatasetOpKernel plugis\n",
    "\n",
    "-> 실무 새로운 데이터셋을 만들거나 개선할때 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easy of Use\n",
    "\n",
    "**더 이상 데이터를 읽고 하는데 고생할 필요가 없음**\n",
    "\n",
    "### Use Python for loops in eager exeuction mode\n",
    "\n",
    "```python\n",
    "#Extract\n",
    "files = tf.data.Dataset.list_files(file_pattern)\n",
    "dataset = tf.data.TFRecordDataset(files)\n",
    "\n",
    "#Transform\n",
    "dataset = dataset.shuffle(10000)\n",
    "dataset = dataset.repeat(NUM_EPOCHS)\n",
    "dataset = dataset.map(lambda x: tf.parse_single_example(x, features))\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "#Eager execution make datset a normal Python iterable.\n",
    "\n",
    "for batch in dataset:\n",
    "    train_model(batch)\n",
    "```\n",
    "\n",
    "</br>\n",
    "\n",
    "### Standard Method CSV file with protocal buffer (tf 1.8)\n",
    "\n",
    "```python\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "#make_batched_features_dataset\n",
    "dataset = tf.contrib.data.make_batched_features_dataset(\n",
    "    file_pattern, BATCH_SIZE, features, num_epochs=NUM_EPOCHS)\n",
    "\n",
    "#일반적으로는 속도를 위해서는 tf.example, tfrecord와 같은 binary를 추천\n",
    "#하지만 큰 데이터를 항상 가지고 있는 것은 아님.\n",
    "\n",
    "#kaggle의 예\n",
    "#$ pip install kaggle\n",
    "#$ kaggle datasets downaload -d theronk/million-headlins -p .\n",
    "\n",
    "for batch in dataset:\n",
    "    train_model(batch[\"publish_data\"], batch[\"headline_text\"])\n",
    "\n",
    "```\n",
    "\n",
    "### Integration with Esitmators(and Keras comming soon!!)\n",
    "\n",
    "```python\n",
    "def input_fn():\n",
    "    dataset = tf.contrib.data.make_csv_dataset(\n",
    "        \"*.csv\", BATCH_SIZE, num_epochs=NUM_EPOCHS)\n",
    "    return dataset\n",
    "\n",
    "# train an estimator on the dataset\n",
    "tf.esitmator.Estimator(model_fn=train_model).train(input_fn=input_fn)\n",
    "\n",
    "```\n",
    "\n",
    "추가 정보들\n",
    "\n",
    "- www.tensorflow.org/programmers_guide/datasets\n",
    "- www.tensorflow.org/performance/datasets_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습 및 활용\n",
    "\n",
    "https://towardsdatascience.com/how-to-use-dataset-in-tensorflow-c758ef9e4428\n",
    "\n",
    "1. Importing Data: 데이터셋 생성\n",
    " - From numpy\n",
    " - From tensor\n",
    " - From a placeholder\n",
    " - From generator\n",
    " \n",
    " </br>\n",
    "\n",
    "2. Create an Iterator: 생성된 데이터셋을 바탕으로 Iterator 인스턴스를 만들기\n",
    " - One shot Iterator\n",
    " - Initializable Iterator\n",
    " - Reinitializable Iterator\n",
    " - Feedable Iterator\n",
    " \n",
    " </br>\n",
    "\n",
    "3. Consuming Data. By using the created iterator we can get the elements from the dataset to feed the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 2)\n",
      "[0.82791794 0.90319535]\n"
     ]
    }
   ],
   "source": [
    "#numpy 데이터 처리\n",
    "\n",
    "x = np.random.sample((300,2))\n",
    "print(x.shape)\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(el))\n",
    "\n",
    "features, labels = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((2,), (1,)), types: (tf.float64, tf.float64)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
