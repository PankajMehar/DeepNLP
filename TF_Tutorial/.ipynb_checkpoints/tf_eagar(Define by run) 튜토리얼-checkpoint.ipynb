{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Eager Tutorial (Define by Run)\n",
    "\n",
    "정리 및 요약 by Ryah Shin\n",
    "\n",
    "[참고1: 구글 블로그](https://research.googleblog.com/2017/10/eager-execution-imperative-define-by.html)\n",
    "\n",
    "[참고2: 구글 깃허브](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/g3doc/guide.md)\n",
    "\n",
    "[Eager 모드 설치방법(TF Nightly)](https://github.com/tensorflow/tensorflow#installation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "tf.enable_eager_execution must be called at program startup.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-839d49fb4552>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_eager_execution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/tf17_py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36menable_eager_execution\u001b[0;34m(config, device_policy)\u001b[0m\n\u001b[1;32m   5265\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5266\u001b[0m     raise ValueError(\n\u001b[0;32m-> 5267\u001b[0;31m         \"tf.enable_eager_execution must be called at program startup.\")\n\u001b[0m\u001b[1;32m   5268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: tf.enable_eager_execution must be called at program startup."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I could do this all day! 1\n",
      "I could do this all day! 2\n",
      "I could do this all day! 3\n",
      "I could do this all day! 4\n",
      "I could do this all day! 5\n",
      "I could do this all day! 6\n",
      "I could do this all day! 7\n",
      "I could do this all day! 8\n",
      "I could do this all day! 9\n",
      "I could do this all day! 10\n",
      "I could do this all day! 11\n",
      "I could do this all day! 12\n",
      "I could do this all day! 13\n",
      "I could do this all day! 14\n",
      "I could do this all day! 15\n",
      "I could do this all day! 16\n",
      "I could do this all day! 17\n",
      "I could do this all day! 18\n",
      "I could do this all day! 19\n",
      "I could do this all day! 20\n",
      "I could do this all day! 21\n",
      "I could do this all day! 22\n",
      "I could do this all day! 23\n",
      "I could do this all day! 24\n",
      "I could do this all day! 25\n",
      "I could do this all day! 26\n",
      "I could do this all day! 27\n",
      "I could do this all day! 28\n",
      "I could do this all day! 29\n",
      "I could do this all day! 30\n",
      "I could do this all day! 31\n",
      "I could do this all day! 32\n",
      "I could do this all day! 33\n",
      "I could do this all day! 34\n",
      "I could do this all day! 35\n",
      "I could do this all day! 36\n",
      "I could do this all day! 37\n",
      "I could do this all day! 38\n",
      "I could do this all day! 39\n",
      "I could do this all day! 40\n",
      "I could do this all day! 41\n",
      "I could do this all day! 42\n",
      "I could do this all day! 43\n",
      "I could do this all day! 44\n",
      "I could do this all day! 45\n",
      "I could do this all day! 46\n",
      "I could do this all day! 47\n",
      "I could do this all day! 48\n",
      "I could do this all day! 49\n",
      "I could do this all day! 50\n",
      "I could do this all day! 51\n",
      "I could do this all day! 52\n",
      "I could do this all day! 53\n",
      "I could do this all day! 54\n",
      "I could do this all day! 55\n",
      "I could do this all day! 56\n",
      "I could do this all day! 57\n",
      "I could do this all day! 58\n",
      "I could do this all day! 59\n",
      "I could do this all day! 60\n",
      "I could do this all day! 61\n",
      "I could do this all day! 62\n",
      "I could do this all day! 63\n",
      "I could do this all day! 64\n",
      "I could do this all day! 65\n",
      "I could do this all day! 66\n",
      "I could do this all day! 67\n",
      "I could do this all day! 68\n",
      "I could do this all day! 69\n",
      "I could do this all day! 70\n",
      "I could do this all day! 71\n",
      "I could do this all day! 72\n",
      "I could do this all day! 73\n",
      "I could do this all day! 74\n",
      "I could do this all day! 75\n",
      "I could do this all day! 76\n",
      "I could do this all day! 77\n",
      "I could do this all day! 78\n",
      "I could do this all day! 79\n",
      "I could do this all day! 80\n",
      "I could do this all day! 81\n",
      "I could do this all day! 82\n",
      "I could do this all day! 83\n",
      "I could do this all day! 84\n",
      "I could do this all day! 85\n",
      "I could do this all day! 86\n",
      "I could do this all day! 87\n",
      "I could do this all day! 88\n",
      "I could do this all day! 89\n",
      "I could do this all day! 90\n",
      "I could do this all day! 91\n",
      "I could do this all day! 92\n",
      "I could do this all day! 93\n",
      "I could do this all day! 94\n",
      "I could do this all day! 95\n",
      "I could do this all day! 96\n",
      "I could do this all day! 97\n",
      "I could do this all day! 98\n",
      "I could do this all day! 99\n",
      "I could do this all day! 100\n",
      "I could do this all day! 101\n",
      "I could do this all day! 102\n",
      "I could do this all day! 103\n",
      "I could do this all day! 104\n",
      "I could do this all day! 105\n",
      "I could do this all day! 106\n",
      "I could do this all day! 107\n",
      "I could do this all day! 108\n",
      "I could do this all day! 109\n",
      "I could do this all day! 110\n",
      "I could do this all day! 111\n",
      "I could do this all day! 112\n",
      "I could do this all day! 113\n",
      "I could do this all day! 114\n",
      "I could do this all day! 115\n",
      "I could do this all day! 116\n",
      "I could do this all day! 117\n",
      "I could do this all day! 118\n",
      "I could do this all day! 119\n",
      "I could do this all day! 120\n",
      "I could do this all day! 121\n",
      "I could do this all day! 122\n",
      "I could do this all day! 123\n",
      "I could do this all day! 124\n",
      "I could do this all day! 125\n",
      "I could do this all day! 126\n",
      "I could do this all day! 127\n",
      "I could do this all day! 128\n",
      "I could do this all day! 129\n",
      "I could do this all day! 130\n",
      "I could do this all day! 131\n",
      "I could do this all day! 132\n",
      "I could do this all day! 133\n",
      "I could do this all day! 134\n",
      "I could do this all day! 135\n",
      "I could do this all day! 136\n",
      "I could do this all day! 137\n",
      "I could do this all day! 138\n",
      "I could do this all day! 139\n",
      "I could do this all day! 140\n",
      "I could do this all day! 141\n",
      "I could do this all day! 142\n",
      "I could do this all day! 143\n",
      "I could do this all day! 144\n",
      "I could do this all day! 145\n",
      "I could do this all day! 146\n",
      "I could do this all day! 147\n",
      "I could do this all day! 148\n",
      "I could do this all day! 149\n",
      "I could do this all day! 150\n",
      "I could do this all day! 151\n",
      "I could do this all day! 152\n",
      "I could do this all day! 153\n",
      "I could do this all day! 154\n",
      "I could do this all day! 155\n",
      "I could do this all day! 156\n",
      "I could do this all day! 157\n",
      "I could do this all day! 158\n",
      "I could do this all day! 159\n",
      "I could do this all day! 160\n",
      "I could do this all day! 161\n",
      "I could do this all day! 162\n",
      "I could do this all day! 163\n",
      "I could do this all day! 164\n",
      "I could do this all day! 165\n",
      "I could do this all day! 166\n",
      "I could do this all day! 167\n",
      "I could do this all day! 168\n",
      "I could do this all day! 169\n",
      "I could do this all day! 170\n",
      "I could do this all day! 171\n",
      "I could do this all day! 172\n",
      "I could do this all day! 173\n",
      "I could do this all day! 174\n",
      "I could do this all day! 175\n",
      "I could do this all day! 176\n",
      "I could do this all day! 177\n",
      "I could do this all day! 178\n",
      "I could do this all day! 179\n",
      "I could do this all day! 180\n",
      "I could do this all day! 181\n",
      "I could do this all day! 182\n",
      "I could do this all day! 183\n",
      "I could do this all day! 184\n",
      "I could do this all day! 185\n",
      "I could do this all day! 186\n",
      "I could do this all day! 187\n",
      "I could do this all day! 188\n",
      "I could do this all day! 189\n",
      "I could do this all day! 190\n",
      "I could do this all day! 191\n",
      "I could do this all day! 192\n",
      "I could do this all day! 193\n",
      "I could do this all day! 194\n",
      "I could do this all day! 195\n",
      "I could do this all day! 196\n",
      "I could do this all day! 197\n",
      "I could do this all day! 198\n",
      "I could do this all day! 199\n",
      "I could do this all day! 200\n",
      "I could do this all day! 201\n",
      "I could do this all day! 202\n",
      "I could do this all day! 203\n",
      "I could do this all day! 204\n",
      "I could do this all day! 205\n",
      "I could do this all day! 206\n",
      "I could do this all day! 207\n",
      "I could do this all day! 208\n",
      "I could do this all day! 209\n",
      "I could do this all day! 210\n",
      "I could do this all day! 211\n",
      "I could do this all day! 212\n",
      "I could do this all day! 213\n",
      "I could do this all day! 214\n",
      "I could do this all day! 215\n",
      "I could do this all day! 216\n",
      "I could do this all day! 217\n",
      "I could do this all day! 218\n",
      "I could do this all day! 219\n",
      "I could do this all day! 220\n",
      "I could do this all day! 221\n",
      "I could do this all day! 222\n",
      "I could do this all day! 223\n",
      "I could do this all day! 224\n",
      "I could do this all day! 225\n",
      "I could do this all day! 226\n",
      "I could do this all day! 227\n",
      "I could do this all day! 228\n",
      "I could do this all day! 229\n",
      "I could do this all day! 230\n",
      "I could do this all day! 231\n",
      "I could do this all day! 232\n",
      "I could do this all day! 233\n",
      "I could do this all day! 234\n",
      "I could do this all day! 235\n",
      "I could do this all day! 236\n",
      "I could do this all day! 237\n",
      "I could do this all day! 238\n",
      "I could do this all day! 239\n",
      "I could do this all day! 240\n",
      "I could do this all day! 241\n",
      "I could do this all day! 242\n",
      "I could do this all day! 243\n",
      "I could do this all day! 244\n",
      "I could do this all day! 245\n",
      "I could do this all day! 246\n",
      "I could do this all day! 247\n",
      "I could do this all day! 248\n",
      "I could do this all day! 249\n",
      "I could do this all day! 250\n",
      "I could do this all day! 251\n",
      "I could do this all day! 252\n",
      "I could do this all day! 253\n",
      "I could do this all day! 254\n",
      "I could do this all day! 255\n",
      "I could do this all day! 256\n",
      "I could do this all day! 257\n",
      "I could do this all day! 258\n",
      "I could do this all day! 259\n",
      "I could do this all day! 260\n",
      "I could do this all day! 261\n",
      "I could do this all day! 262\n",
      "I could do this all day! 263\n",
      "I could do this all day! 264\n",
      "I could do this all day! 265\n",
      "I could do this all day! 266\n",
      "I could do this all day! 267\n",
      "I could do this all day! 268\n",
      "I could do this all day! 269\n",
      "I could do this all day! 270\n",
      "I could do this all day! 271\n",
      "I could do this all day! 272\n",
      "I could do this all day! 273\n",
      "I could do this all day! 274\n",
      "I could do this all day! 275\n",
      "I could do this all day! 276\n",
      "I could do this all day! 277\n",
      "I could do this all day! 278\n",
      "I could do this all day! 279\n",
      "I could do this all day! 280\n",
      "I could do this all day! 281\n",
      "I could do this all day! 282\n",
      "I could do this all day! 283\n",
      "I could do this all day! 284\n",
      "I could do this all day! 285\n",
      "I could do this all day! 286\n",
      "I could do this all day! 287\n",
      "I could do this all day! 288\n",
      "I could do this all day! 289\n",
      "I could do this all day! 290\n",
      "I could do this all day! 291\n",
      "I could do this all day! 292\n",
      "I could do this all day! 293\n",
      "I could do this all day! 294\n",
      "I could do this all day! 295\n",
      "I could do this all day! 296\n",
      "I could do this all day! 297\n",
      "I could do this all day! 298\n",
      "I could do this all day! 299\n",
      "I could do this all day! 300\n",
      "I could do this all day! 301\n",
      "I could do this all day! 302\n",
      "I could do this all day! 303\n",
      "I could do this all day! 304\n",
      "I could do this all day! 305\n",
      "I could do this all day! 306\n",
      "I could do this all day! 307\n",
      "I could do this all day! 308\n",
      "I could do this all day! 309\n",
      "I could do this all day! 310\n",
      "I could do this all day! 311\n",
      "I could do this all day! 312\n",
      "I could do this all day! 313\n",
      "I could do this all day! 314\n",
      "I could do this all day! 315\n",
      "I could do this all day! 316\n",
      "I could do this all day! 317\n",
      "I could do this all day! 318\n",
      "I could do this all day! 319\n",
      "I could do this all day! 320\n",
      "I could do this all day! 321\n",
      "I could do this all day! 322\n",
      "I could do this all day! 323\n",
      "I could do this all day! 324\n",
      "I could do this all day! 325\n",
      "I could do this all day! 326\n",
      "I could do this all day! 327\n",
      "I could do this all day! 328\n",
      "I could do this all day! 329\n",
      "I could do this all day! 330\n",
      "I could do this all day! 331\n",
      "I could do this all day! 332\n",
      "I could do this all day! 333\n",
      "I could do this all day! 334\n",
      "I could do this all day! 335\n",
      "I could do this all day! 336\n",
      "I could do this all day! 337\n",
      "I could do this all day! 338\n",
      "I could do this all day! 339\n",
      "I could do this all day! 340\n",
      "I could do this all day! 341\n",
      "I could do this all day! 342\n",
      "I could do this all day! 343\n",
      "I could do this all day! 344\n",
      "I could do this all day! 345\n",
      "I could do this all day! 346\n",
      "I could do this all day! 347\n",
      "I could do this all day! 348\n",
      "I could do this all day! 349\n",
      "I could do this all day! 350\n",
      "I could do this all day! 351\n",
      "I could do this all day! 352\n",
      "I could do this all day! 353\n",
      "I could do this all day! 354\n",
      "I could do this all day! 355\n",
      "I could do this all day! 356\n",
      "I could do this all day! 357\n",
      "I could do this all day! 358\n",
      "I could do this all day! 359\n",
      "I could do this all day! 360\n",
      "I could do this all day! 361\n",
      "I could do this all day! 362\n",
      "I could do this all day! 363\n",
      "I could do this all day! 364\n",
      "I could do this all day! 365\n",
      "I could do this all day! 366\n",
      "I could do this all day! 367\n",
      "I could do this all day! 368\n",
      "I could do this all day! 369\n",
      "I could do this all day! 370\n",
      "I could do this all day! 371\n",
      "I could do this all day! 372\n",
      "I could do this all day! 373\n",
      "I could do this all day! 374\n",
      "I could do this all day! 375\n",
      "I could do this all day! 376\n",
      "I could do this all day! 377\n",
      "I could do this all day! 378\n",
      "I could do this all day! 379\n",
      "I could do this all day! 380\n",
      "I could do this all day! 381\n",
      "I could do this all day! 382\n",
      "I could do this all day! 383\n",
      "I could do this all day! 384\n",
      "I could do this all day! 385\n",
      "I could do this all day! 386\n",
      "I could do this all day! 387\n",
      "I could do this all day! 388\n",
      "I could do this all day! 389\n",
      "I could do this all day! 390\n",
      "I could do this all day! 391\n",
      "I could do this all day! 392\n",
      "I could do this all day! 393\n",
      "I could do this all day! 394\n",
      "I could do this all day! 395\n",
      "I could do this all day! 396\n",
      "I could do this all day! 397\n",
      "I could do this all day! 398\n",
      "I could do this all day! 399\n",
      "I could do this all day! 400\n",
      "I could do this all day! 401\n",
      "I could do this all day! 402\n",
      "I could do this all day! 403\n",
      "I could do this all day! 404\n",
      "I could do this all day! 405\n",
      "I could do this all day! 406\n",
      "I could do this all day! 407\n",
      "I could do this all day! 408\n",
      "I could do this all day! 409\n",
      "I could do this all day! 410\n",
      "I could do this all day! 411\n",
      "I could do this all day! 412\n",
      "I could do this all day! 413\n",
      "I could do this all day! 414\n",
      "I could do this all day! 415\n",
      "I could do this all day! 416\n",
      "I could do this all day! 417\n",
      "I could do this all day! 418\n",
      "I could do this all day! 419\n",
      "I could do this all day! 420\n",
      "I could do this all day! 421\n",
      "I could do this all day! 422\n",
      "I could do this all day! 423\n",
      "I could do this all day! 424\n",
      "I could do this all day! 425\n",
      "I could do this all day! 426\n",
      "I could do this all day! 427\n",
      "I could do this all day! 428\n",
      "I could do this all day! 429\n",
      "I could do this all day! 430\n",
      "I could do this all day! 431\n",
      "I could do this all day! 432\n",
      "I could do this all day! 433\n",
      "I could do this all day! 434\n",
      "I could do this all day! 435\n",
      "I could do this all day! 436\n",
      "I could do this all day! 437\n",
      "I could do this all day! 438\n",
      "I could do this all day! 439\n",
      "I could do this all day! 440\n",
      "I could do this all day! 441\n",
      "I could do this all day! 442\n",
      "I could do this all day! 443\n",
      "I could do this all day! 444\n",
      "I could do this all day! 445\n",
      "I could do this all day! 446\n",
      "I could do this all day! 447\n",
      "I could do this all day! 448\n",
      "I could do this all day! 449\n",
      "I could do this all day! 450\n",
      "I could do this all day! 451\n",
      "I could do this all day! 452\n",
      "I could do this all day! 453\n",
      "I could do this all day! 454\n",
      "I could do this all day! 455\n",
      "I could do this all day! 456\n",
      "I could do this all day! 457\n",
      "I could do this all day! 458\n",
      "I could do this all day! 459\n",
      "I could do this all day! 460\n",
      "I could do this all day! 461\n",
      "I could do this all day! 462\n",
      "I could do this all day! 463\n",
      "I could do this all day! 464\n",
      "I could do this all day! 465\n",
      "I could do this all day! 466\n",
      "I could do this all day! 467\n",
      "I could do this all day! 468\n",
      "I could do this all day! 469\n",
      "I could do this all day! 470\n",
      "I could do this all day! 471\n",
      "I could do this all day! 472\n",
      "I could do this all day! 473\n",
      "I could do this all day! 474\n",
      "I could do this all day! 475\n",
      "I could do this all day! 476\n",
      "I could do this all day! 477\n",
      "I could do this all day! 478\n",
      "I could do this all day! 479\n",
      "I could do this all day! 480\n",
      "I could do this all day! 481\n",
      "I could do this all day! 482\n",
      "I could do this all day! 483\n",
      "I could do this all day! 484\n",
      "I could do this all day! 485\n",
      "I could do this all day! 486\n",
      "I could do this all day! 487\n",
      "I could do this all day! 488\n",
      "I could do this all day! 489\n",
      "I could do this all day! 490\n",
      "I could do this all day! 491\n",
      "I could do this all day! 492\n",
      "I could do this all day! 493\n",
      "I could do this all day! 494\n",
      "I could do this all day! 495\n",
      "I could do this all day! 496\n",
      "I could do this all day! 497\n",
      "I could do this all day! 498\n",
      "I could do this all day! 499\n",
      "I could do this all day! 500\n",
      "I could do this all day! 501\n",
      "I could do this all day! 502\n",
      "I could do this all day! 503\n",
      "I could do this all day! 504\n",
      "I could do this all day! 505\n",
      "I could do this all day! 506\n",
      "I could do this all day! 507\n",
      "I could do this all day! 508\n",
      "I could do this all day! 509\n",
      "I could do this all day! 510\n",
      "I could do this all day! 511\n",
      "I could do this all day! 512\n",
      "I could do this all day! 513\n",
      "I could do this all day! 514\n",
      "I could do this all day! 515\n",
      "I could do this all day! 516\n",
      "I could do this all day! 517\n",
      "I could do this all day! 518\n",
      "I could do this all day! 519\n",
      "I could do this all day! 520\n",
      "I could do this all day! 521\n",
      "I could do this all day! 522\n",
      "I could do this all day! 523\n",
      "I could do this all day! 524\n",
      "I could do this all day! 525\n",
      "I could do this all day! 526\n",
      "I could do this all day! 527\n",
      "I could do this all day! 528\n",
      "I could do this all day! 529\n",
      "I could do this all day! 530\n",
      "I could do this all day! 531\n",
      "I could do this all day! 532\n",
      "I could do this all day! 533\n",
      "I could do this all day! 534\n",
      "I could do this all day! 535\n",
      "I could do this all day! 536\n",
      "I could do this all day! 537\n",
      "I could do this all day! 538\n",
      "I could do this all day! 539\n",
      "I could do this all day! 540\n",
      "I could do this all day! 541\n",
      "I could do this all day! 542\n",
      "I could do this all day! 543\n",
      "I could do this all day! 544\n",
      "I could do this all day! 545\n",
      "I could do this all day! 546\n",
      "I could do this all day! 547\n",
      "I could do this all day! 548\n",
      "I could do this all day! 549\n",
      "I could do this all day! 550\n",
      "I could do this all day! 551\n",
      "I could do this all day! 552\n",
      "I could do this all day! 553\n",
      "I could do this all day! 554\n",
      "I could do this all day! 555\n",
      "I could do this all day! 556\n",
      "I could do this all day! 557\n",
      "I could do this all day! 558\n",
      "I could do this all day! 559\n",
      "I could do this all day! 560\n",
      "I could do this all day! 561\n",
      "I could do this all day! 562\n",
      "I could do this all day! 563\n",
      "I could do this all day! 564\n",
      "I could do this all day! 565\n",
      "I could do this all day! 566\n",
      "I could do this all day! 567\n",
      "I could do this all day! 568\n",
      "I could do this all day! 569\n",
      "I could do this all day! 570\n",
      "I could do this all day! 571\n",
      "I could do this all day! 572\n",
      "I could do this all day! 573\n",
      "I could do this all day! 574\n",
      "I could do this all day! 575\n",
      "I could do this all day! 576\n",
      "I could do this all day! 577\n",
      "I could do this all day! 578\n",
      "I could do this all day! 579\n",
      "I could do this all day! 580\n",
      "I could do this all day! 581\n",
      "I could do this all day! 582\n",
      "I could do this all day! 583\n",
      "I could do this all day! 584\n",
      "I could do this all day! 585\n",
      "I could do this all day! 586\n",
      "I could do this all day! 587\n",
      "I could do this all day! 588\n",
      "I could do this all day! 589\n",
      "I could do this all day! 590\n",
      "I could do this all day! 591\n",
      "I could do this all day! 592\n",
      "I could do this all day! 593\n",
      "I could do this all day! 594\n",
      "I could do this all day! 595\n",
      "I could do this all day! 596\n",
      "I could do this all day! 597\n",
      "I could do this all day! 598\n",
      "I could do this all day! 599\n",
      "I could do this all day! 600\n",
      "I could do this all day! 601\n",
      "I could do this all day! 602\n",
      "I could do this all day! 603\n",
      "I could do this all day! 604\n",
      "I could do this all day! 605\n",
      "I could do this all day! 606\n",
      "I could do this all day! 607\n",
      "I could do this all day! 608\n",
      "I could do this all day! 609\n",
      "I could do this all day! 610\n",
      "I could do this all day! 611\n",
      "I could do this all day! 612\n",
      "I could do this all day! 613\n",
      "I could do this all day! 614\n",
      "I could do this all day! 615\n",
      "I could do this all day! 616\n",
      "I could do this all day! 617\n",
      "I could do this all day! 618\n",
      "I could do this all day! 619\n",
      "I could do this all day! 620\n",
      "I could do this all day! 621\n",
      "I could do this all day! 622\n",
      "I could do this all day! 623\n",
      "I could do this all day! 624\n",
      "I could do this all day! 625\n",
      "I could do this all day! 626\n",
      "I could do this all day! 627\n",
      "I could do this all day! 628\n",
      "I could do this all day! 629\n",
      "I could do this all day! 630\n",
      "I could do this all day! 631\n",
      "I could do this all day! 632\n",
      "I could do this all day! 633\n",
      "I could do this all day! 634\n",
      "I could do this all day! 635\n",
      "I could do this all day! 636\n",
      "I could do this all day! 637\n",
      "I could do this all day! 638\n",
      "I could do this all day! 639\n",
      "I could do this all day! 640\n",
      "I could do this all day! 641\n",
      "I could do this all day! 642\n",
      "I could do this all day! 643\n",
      "I could do this all day! 644\n",
      "I could do this all day! 645\n",
      "I could do this all day! 646\n",
      "I could do this all day! 647\n",
      "I could do this all day! 648\n",
      "I could do this all day! 649\n",
      "I could do this all day! 650\n",
      "I could do this all day! 651\n",
      "I could do this all day! 652\n",
      "I could do this all day! 653\n",
      "I could do this all day! 654\n",
      "I could do this all day! 655\n",
      "I could do this all day! 656\n",
      "I could do this all day! 657\n",
      "I could do this all day! 658\n",
      "I could do this all day! 659\n",
      "I could do this all day! 660\n",
      "I could do this all day! 661\n",
      "I could do this all day! 662\n",
      "I could do this all day! 663\n",
      "I could do this all day! 664\n",
      "I could do this all day! 665\n",
      "I could do this all day! 666\n",
      "I could do this all day! 667\n",
      "I could do this all day! 668\n",
      "I could do this all day! 669\n",
      "I could do this all day! 670\n",
      "I could do this all day! 671\n",
      "I could do this all day! 672\n",
      "I could do this all day! 673\n",
      "I could do this all day! 674\n",
      "I could do this all day! 675\n",
      "I could do this all day! 676\n",
      "I could do this all day! 677\n",
      "I could do this all day! 678\n",
      "I could do this all day! 679\n",
      "I could do this all day! 680\n",
      "I could do this all day! 681\n",
      "I could do this all day! 682\n",
      "I could do this all day! 683\n",
      "I could do this all day! 684\n",
      "I could do this all day! 685\n",
      "I could do this all day! 686\n",
      "I could do this all day! 687\n",
      "I could do this all day! 688\n",
      "I could do this all day! 689\n",
      "I could do this all day! 690\n",
      "I could do this all day! 691\n",
      "I could do this all day! 692\n",
      "I could do this all day! 693\n",
      "I could do this all day! 694\n",
      "I could do this all day! 695\n",
      "I could do this all day! 696\n",
      "I could do this all day! 697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I could do this all day! 698\n",
      "I could do this all day! 699\n",
      "I could do this all day! 700\n",
      "I could do this all day! 701\n",
      "I could do this all day! 702\n",
      "I could do this all day! 703\n",
      "I could do this all day! 704\n",
      "I could do this all day! 705\n",
      "I could do this all day! 706\n",
      "I could do this all day! 707\n",
      "I could do this all day! 708\n",
      "I could do this all day! 709\n",
      "I could do this all day! 710\n",
      "I could do this all day! 711\n",
      "I could do this all day! 712\n",
      "I could do this all day! 713\n",
      "I could do this all day! 714\n",
      "I could do this all day! 715\n",
      "I could do this all day! 716\n",
      "I could do this all day! 717\n",
      "I could do this all day! 718\n",
      "I could do this all day! 719\n",
      "I could do this all day! 720\n",
      "I could do this all day! 721\n",
      "I could do this all day! 722\n",
      "I could do this all day! 723\n",
      "I could do this all day! 724\n",
      "I could do this all day! 725\n",
      "I could do this all day! 726\n",
      "I could do this all day! 727\n",
      "I could do this all day! 728\n",
      "I could do this all day! 729\n",
      "I could do this all day! 730\n",
      "I could do this all day! 731\n",
      "I could do this all day! 732\n",
      "I could do this all day! 733\n",
      "I could do this all day! 734\n",
      "I could do this all day! 735\n",
      "I could do this all day! 736\n",
      "I could do this all day! 737\n",
      "I could do this all day! 738\n",
      "I could do this all day! 739\n",
      "I could do this all day! 740\n",
      "I could do this all day! 741\n",
      "I could do this all day! 742\n",
      "I could do this all day! 743\n",
      "I could do this all day! 744\n",
      "I could do this all day! 745\n",
      "I could do this all day! 746\n",
      "I could do this all day! 747\n",
      "I could do this all day! 748\n",
      "I could do this all day! 749\n",
      "I could do this all day! 750\n",
      "I could do this all day! 751\n",
      "I could do this all day! 752\n",
      "I could do this all day! 753\n",
      "I could do this all day! 754\n",
      "I could do this all day! 755\n",
      "I could do this all day! 756\n",
      "I could do this all day! 757\n",
      "I could do this all day! 758\n",
      "I could do this all day! 759\n",
      "I could do this all day! 760\n",
      "I could do this all day! 761\n",
      "I could do this all day! 762\n",
      "I could do this all day! 763\n",
      "I could do this all day! 764\n",
      "I could do this all day! 765\n",
      "I could do this all day! 766\n",
      "I could do this all day! 767\n",
      "I could do this all day! 768\n",
      "I could do this all day! 769\n",
      "I could do this all day! 770\n",
      "I could do this all day! 771\n",
      "I could do this all day! 772\n",
      "I could do this all day! 773\n",
      "I could do this all day! 774\n",
      "I could do this all day! 775\n",
      "I could do this all day! 776\n",
      "I could do this all day! 777\n",
      "I could do this all day! 778\n",
      "I could do this all day! 779\n",
      "I could do this all day! 780\n",
      "I could do this all day! 781\n",
      "I could do this all day! 782\n",
      "I could do this all day! 783\n",
      "I could do this all day! 784\n",
      "I could do this all day! 785\n",
      "I could do this all day! 786\n",
      "I could do this all day! 787\n",
      "I could do this all day! 788\n",
      "I could do this all day! 789\n",
      "I could do this all day! 790\n",
      "I could do this all day! 791\n",
      "I could do this all day! 792\n",
      "I could do this all day! 793\n",
      "I could do this all day! 794\n",
      "I could do this all day! 795\n",
      "I could do this all day! 796\n",
      "I could do this all day! 797\n",
      "I could do this all day! 798\n",
      "I could do this all day! 799\n",
      "I could do this all day! 800\n",
      "I could do this all day! 801\n",
      "I could do this all day! 802\n",
      "I could do this all day! 803\n",
      "I could do this all day! 804\n",
      "I could do this all day! 805\n",
      "I could do this all day! 806\n",
      "I could do this all day! 807\n",
      "I could do this all day! 808\n",
      "I could do this all day! 809\n",
      "I could do this all day! 810\n",
      "I could do this all day! 811\n",
      "I could do this all day! 812\n",
      "I could do this all day! 813\n",
      "I could do this all day! 814\n",
      "I could do this all day! 815\n",
      "I could do this all day! 816\n",
      "I could do this all day! 817\n",
      "I could do this all day! 818\n",
      "I could do this all day! 819\n",
      "I could do this all day! 820\n",
      "I could do this all day! 821\n",
      "I could do this all day! 822\n",
      "I could do this all day! 823\n",
      "I could do this all day! 824\n",
      "I could do this all day! 825\n",
      "I could do this all day! 826\n",
      "I could do this all day! 827\n",
      "I could do this all day! 828\n",
      "I could do this all day! 829\n",
      "I could do this all day! 830\n",
      "I could do this all day! 831\n",
      "I could do this all day! 832\n",
      "I could do this all day! 833\n",
      "I could do this all day! 834\n",
      "I could do this all day! 835\n",
      "I could do this all day! 836\n",
      "I could do this all day! 837\n",
      "I could do this all day! 838\n",
      "I could do this all day! 839\n",
      "I could do this all day! 840\n",
      "I could do this all day! 841\n",
      "I could do this all day! 842\n",
      "I could do this all day! 843\n",
      "I could do this all day! 844\n",
      "I could do this all day! 845\n",
      "I could do this all day! 846\n",
      "I could do this all day! 847\n",
      "I could do this all day! 848\n",
      "I could do this all day! 849\n",
      "I could do this all day! 850\n",
      "I could do this all day! 851\n",
      "I could do this all day! 852\n",
      "I could do this all day! 853\n",
      "I could do this all day! 854\n",
      "I could do this all day! 855\n",
      "I could do this all day! 856\n",
      "I could do this all day! 857\n",
      "I could do this all day! 858\n",
      "I could do this all day! 859\n",
      "I could do this all day! 860\n",
      "I could do this all day! 861\n",
      "I could do this all day! 862\n",
      "I could do this all day! 863\n",
      "I could do this all day! 864\n",
      "I could do this all day! 865\n",
      "I could do this all day! 866\n",
      "I could do this all day! 867\n",
      "I could do this all day! 868\n",
      "I could do this all day! 869\n",
      "I could do this all day! 870\n",
      "I could do this all day! 871\n",
      "I could do this all day! 872\n",
      "I could do this all day! 873\n",
      "I could do this all day! 874\n",
      "I could do this all day! 875\n",
      "I could do this all day! 876\n",
      "I could do this all day! 877\n",
      "I could do this all day! 878\n",
      "I could do this all day! 879\n",
      "I could do this all day! 880\n",
      "I could do this all day! 881\n",
      "I could do this all day! 882\n",
      "I could do this all day! 883\n",
      "I could do this all day! 884\n",
      "I could do this all day! 885\n",
      "I could do this all day! 886\n",
      "I could do this all day! 887\n",
      "I could do this all day! 888\n",
      "I could do this all day! 889\n",
      "I could do this all day! 890\n",
      "I could do this all day! 891\n",
      "I could do this all day! 892\n",
      "I could do this all day! 893\n",
      "I could do this all day! 894\n",
      "I could do this all day! 895\n",
      "I could do this all day! 896\n",
      "I could do this all day! 897\n",
      "I could do this all day! 898\n",
      "I could do this all day! 899\n",
      "I could do this all day! 900\n",
      "I could do this all day! 901\n",
      "I could do this all day! 902\n",
      "I could do this all day! 903\n",
      "I could do this all day! 904\n",
      "I could do this all day! 905\n",
      "I could do this all day! 906\n",
      "I could do this all day! 907\n",
      "I could do this all day! 908\n",
      "I could do this all day! 909\n",
      "I could do this all day! 910\n",
      "I could do this all day! 911\n",
      "I could do this all day! 912\n",
      "I could do this all day! 913\n",
      "I could do this all day! 914\n",
      "I could do this all day! 915\n",
      "I could do this all day! 916\n",
      "I could do this all day! 917\n",
      "I could do this all day! 918\n",
      "I could do this all day! 919\n",
      "I could do this all day! 920\n",
      "I could do this all day! 921\n",
      "I could do this all day! 922\n",
      "I could do this all day! 923\n",
      "I could do this all day! 924\n",
      "I could do this all day! 925\n",
      "I could do this all day! 926\n",
      "I could do this all day! 927\n",
      "I could do this all day! 928\n",
      "I could do this all day! 929\n",
      "I could do this all day! 930\n",
      "I could do this all day! 931\n",
      "I could do this all day! 932\n",
      "I could do this all day! 933\n",
      "I could do this all day! 934\n",
      "I could do this all day! 935\n",
      "I could do this all day! 936\n",
      "I could do this all day! 937\n",
      "I could do this all day! 938\n",
      "I could do this all day! 939\n",
      "I could do this all day! 940\n",
      "I could do this all day! 941\n",
      "I could do this all day! 942\n",
      "I could do this all day! 943\n",
      "I could do this all day! 944\n",
      "I could do this all day! 945\n",
      "I could do this all day! 946\n",
      "I could do this all day! 947\n",
      "I could do this all day! 948\n",
      "I could do this all day! 949\n",
      "I could do this all day! 950\n",
      "I could do this all day! 951\n",
      "I could do this all day! 952\n",
      "I could do this all day! 953\n",
      "I could do this all day! 954\n",
      "I could do this all day! 955\n",
      "I could do this all day! 956\n",
      "I could do this all day! 957\n",
      "I could do this all day! 958\n",
      "I could do this all day! 959\n",
      "I could do this all day! 960\n",
      "I could do this all day! 961\n",
      "I could do this all day! 962\n",
      "I could do this all day! 963\n",
      "I could do this all day! 964\n",
      "I could do this all day! 965\n",
      "I could do this all day! 966\n",
      "I could do this all day! 967\n",
      "I could do this all day! 968\n",
      "I could do this all day! 969\n",
      "I could do this all day! 970\n",
      "I could do this all day! 971\n",
      "I could do this all day! 972\n",
      "I could do this all day! 973\n",
      "I could do this all day! 974\n",
      "I could do this all day! 975\n",
      "I could do this all day! 976\n",
      "I could do this all day! 977\n",
      "I could do this all day! 978\n",
      "I could do this all day! 979\n",
      "I could do this all day! 980\n",
      "I could do this all day! 981\n",
      "I could do this all day! 982\n",
      "I could do this all day! 983\n",
      "I could do this all day! 984\n",
      "I could do this all day! 985\n",
      "I could do this all day! 986\n",
      "I could do this all day! 987\n",
      "I could do this all day! 988\n",
      "I could do this all day! 989\n",
      "I could do this all day! 990\n",
      "I could do this all day! 991\n",
      "I could do this all day! 992\n",
      "I could do this all day! 993\n",
      "I could do this all day! 994\n",
      "I could do this all day! 995\n",
      "I could do this all day! 996\n",
      "I could do this all day! 997\n",
      "I could do this all day! 998\n",
      "I could do this all day! 999\n",
      "I could do this all day! 1000\n"
     ]
    }
   ],
   "source": [
    "# 디버깅이 용이함 - pdb.set_trace()\n",
    "# 즉각적으로 에러를 잡을 수 있음\n",
    "# python data구조를 활용가능 (numpy등)\n",
    "# Python의 철학을 따라 갈 수 있음\n",
    "\n",
    "i = tf.constant(0)\n",
    "while i < 1000:\n",
    "    i = tf.add(i,1)\n",
    "    print(\"I could do this all day! %d\" % i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "tf.placeholder() is not compatible with eager execution.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-07da7f53da9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Boilerplate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf17_py3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   1772\u001b[0m   \"\"\"\n\u001b[1;32m   1773\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1774\u001b[0;31m     raise RuntimeError(\"tf.placeholder() is not compatible with \"\n\u001b[0m\u001b[1;32m   1775\u001b[0m                        \"eager execution.\")\n\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: tf.placeholder() is not compatible with eager execution."
     ]
    }
   ],
   "source": [
    "# 생각할 필요가 없음..\n",
    "# placeholders\n",
    "# sessions\n",
    "# control dependencies\n",
    "# \"lazy loading\"\n",
    "# {name, variable, op} scopes\n",
    "\n",
    "# Boilerplate\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[1, 1])\n",
    "m = tf.matmul(x, x)\n",
    "\n",
    "print(m)\n",
    "# Tensor(\"MatMul:0\", shape=(1, 1), dtype=float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    m_out = sess.run(m, feed_dict={x: [[2.]]})\n",
    "\n",
    "print(m_out)\n",
    "\n",
    "# [[4.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[4.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = [[2.]]  # No need for placeholders!\n",
    "m = tf.matmul(x, x)\n",
    "\n",
    "print(m)  # No sessions!\n",
    "# tf.Tensor([[4.]], shape=(1, 1), dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.73952615, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7076243, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7819936, shape=(), dtype=float32)\n",
      "tf.Tensor(0.3552755, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Lazy Loading\n",
    "\n",
    "# x = tf.random_uniform([2,2])\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     for i in range(x.shape[0]):\n",
    "#         for j in range(x.shape[1]):\n",
    "#             print(sess.run(x[i,j]))            \n",
    "            \n",
    "x = tf.random_uniform([2, 2])\n",
    "\n",
    "for i in range(x.shape[0]):\n",
    "    for j in range(x.shape[1]):\n",
    "        print(x[i, j])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Tensor를 Numpy Array로 활용\n",
    "\n",
    "x = tf.constant([1,2,3])\n",
    "\n",
    "assert type(x.numpy()) == np.ndarray\n",
    "squared = np.square(x)\n",
    "\n",
    "for i in x:\n",
    "    print(i)\n",
    "    \n",
    "#텐서 비교시 tf.equal을 활용하는 것을 추천함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32)\n",
      "[(<tf.Tensor: id=5168, shape=(), dtype=float32, numpy=-24.0>, <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0>)]\n"
     ]
    }
   ],
   "source": [
    "# Automatic differentiation is built into eager execution\n",
    "\n",
    "# Under the hood ...\n",
    "# Operations are recorded on a tape\n",
    "# The tape is played back to compute gradients\n",
    "# This is reverse-mode differentiation (backpropagation).\n",
    "# 변수 생성 등 아직 정식적으로 나온 부분이 있어, 몇몇 기능은 tf.contrib아래에 들어가 있음\n",
    "\n",
    "def square(x):\n",
    "    return x ** 2\n",
    "\n",
    "x = tf.contrib.eager.Variable(2.0)\n",
    "\n",
    "def loss(y):\n",
    "    return (y - x ** 2) ** 2\n",
    "\n",
    "grad = tf.contrib.eager.implicit_gradients(loss)\n",
    "\n",
    "print(loss(7))\n",
    "print(grad(7))\n",
    "\n",
    "# tf.Tensor(9.0, shape=(), dtype=float32)\n",
    "# [(<tf.Tensor: id=5168, shape=(), dtype=float32, numpy=-24.0>, <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0>)]\n",
    "\n",
    "# Grad의 종류\n",
    "# APIs for computing gradients work even when eager execution is not enabled\n",
    "# tfe.gradients_function()\n",
    "# tfe.value_and_gradients_function()\n",
    "# tfe.implicit_gradients()\n",
    "# tfe.implicit_value_and_gradients()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'data/birth_life_2010.txt'\n",
    "\n",
    "# Read the data into a dataset.\n",
    "data, n_samples = utils.read_birth_life_data(DATA_FILE)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data[:,0], data[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weight and bias variables, initialized to 0.0.\n",
    "#############################\n",
    "########## TO DO ############\n",
    "#############################\n",
    "w = tf.contrib.eager.Variable(0,0)\n",
    "b = tf.contrib.eager.Variable(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training; loss function: huber_loss\n",
      "trying to switch the dtype to  <dtype: 'float32'>  from  <dtype: 'int32'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incompatible type conversion requested to type 'float32' for variable of type 'int32'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-c67e8336410e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m         'expect performance to increase substantially in the near future!')\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhuber_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-50-c67e8336410e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(loss_fn)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m           \u001b[0;31m# Compute the loss and gradient, and take an optimization step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf17_py3/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgrad_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0mthis_tape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_new_tape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m       \u001b[0mend_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mend_node\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         raise ValueError(\"Cannot differentiate a function that returns None; \"\n",
      "\u001b[0;32m<ipython-input-50-c67e8336410e>\u001b[0m in \u001b[0;36mloss_for_example\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Define the function through which to differentiate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss_for_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Obtain a gradients function using `tfe.implicit_value_and_gradients`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-c67e8336410e>\u001b[0m in \u001b[0;36mprediction\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Define loss functions of the form: L(y, y_predicted)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf17_py3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    960\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m           \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    963\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m           \u001b[0;31m# If the RHS is not a tensor, it might be a tensor aware object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf17_py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[1;32m    948\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf17_py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf17_py3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_TensorConversionFunction\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    760\u001b[0m       raise ValueError(\n\u001b[1;32m    761\u001b[0m           \u001b[0;34m\"Incompatible type conversion requested to type '%s' for variable \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m           \"of type '%s'\" % (dtype.name, v.dtype.name))\n\u001b[0m\u001b[1;32m    763\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible type conversion requested to type 'float32' for variable of type 'int32'"
     ]
    }
   ],
   "source": [
    "# Define the linear predictor.\n",
    "def prediction(x):\n",
    "\n",
    "    return x * w + b\n",
    "\n",
    "# Define loss functions of the form: L(y, y_predicted)\n",
    "def squared_loss(y, y_predicted):\n",
    "    \n",
    "    \n",
    "    return (y - y_predicted) ** 2\n",
    "    \n",
    "\n",
    "def huber_loss(y, y_predicted, m=1.0):\n",
    "    \"\"\"Huber loss with `m` set to `1.0`.\"\"\"\n",
    "    t = y - y_predicted\n",
    "    \n",
    "    return t ** 2 if tf.abs(t) <= m else m * (2 * tf.abs(t) - m)\n",
    "    \n",
    "def train(loss_fn):\n",
    "    \"\"\"Train a regression model evaluated using `loss_fn`.\"\"\"\n",
    "    print('Training; loss function: ' + loss_fn.__name__)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "    # Define the function through which to differentiate.\n",
    "    def loss_for_example(x, y):\n",
    "        return loss_fn(y, prediction(x))\n",
    "\n",
    "    # Obtain a gradients function using `tfe.implicit_value_and_gradients`.\n",
    "    grad_fn = tf.contrib.eager.implicit_value_and_gradients(loss_for_example)\n",
    "\n",
    "    start = time.time()\n",
    "    for epoch in range(100):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for x_i, y_i in tf.contrib.eager.Iterator(dataset):\n",
    "          # Compute the loss and gradient, and take an optimization step.\n",
    "            loss, gradients = grad_fn(x_i, y_i)\n",
    "            optimizer.apply_gradients(gradients)\n",
    "            total_loss += loss\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "              print('Epoch {0}: {1}'.format(epoch, total_loss / n_samples))\n",
    "        print('Took: %f seconds' % (time.time() - start))\n",
    "        print('Eager execution exhibits significant overhead per operation. '\n",
    "        'As you increase your batch size, the impact of the overhead will '\n",
    "        'become less noticeable. Eager execution is under active development: '\n",
    "        'expect performance to increase substantially in the near future!')\n",
    "\n",
    "train(huber_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(data[:,0], data[:,1], 'bo')\n",
    "# The `.numpy()` method of a tensor retrieves the NumPy array backing it.\n",
    "# In future versions of eager, you won't need to call `.numpy()` and will\n",
    "# instead be able to, in most cases, pass Tensors wherever NumPy arrays are\n",
    "# expected.\n",
    "plt.plot(data[:,0], data[:,0] * w.numpy() + b.numpy(), 'r',\n",
    "         label=\"huber regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eagar 모드가 켜져 있을때는....\n",
    "#tf.contrib.eager.Variable 활용 (graph structure에도 사용 가능)\n",
    "#use tf.contrib.summary\n",
    "#use tf.contrib.eager.Iterator\n",
    "#use simple layers such as tf.layers.Dense\n",
    "#tf.contrib.eager.py_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Imperative 하게 진행하고 돌아오기\n",
    "#일반적으로 작성을 시작\n",
    "#Checkpoints는 함께 활용이 가능하다\n",
    "#eager execution이 켜져 있을 때, graph를 만들자: tf.defun -> compile computation into graph\n",
    "\n",
    "- eager를 사용해야 하는 경우는 어떤게 있을가?\n",
    "- 연구자들이 조금 더 유동적인 프레임워크를 활용\n",
    "- 새롭게 Tensorflow를 활용하시는 분들\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[16 19]\n",
      " [36 43]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[17 20]\n",
      " [37 44]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 0.09674954  0.16250503  0.5043776 ]\n",
      " [ 0.79610562  0.58864689  0.78830826]\n",
      " [ 0.35892475  0.01019669  0.68315387]\n",
      " [ 0.41202796  0.45991957  0.51999116]\n",
      " [ 0.10582864  0.7424233   0.0782392 ]], shape=(5, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Multiply 2 * 2 matrix\n",
    "# Multiply two 2x2 matrices\n",
    "x = tf.matmul([[1, 2],\n",
    "               [3, 4]],\n",
    "              [[4, 5],\n",
    "               [6, 7]])\n",
    "\n",
    "# Add one to each element\n",
    "# (tf.add supports broadcasting)\n",
    "#broadcasting: 퍼트리다의 의미, 크기가 작은 행렬을 크기가 큰 행렬로 맞추어 주는 기능, 축소는 데이터 손실 때문에 불가\n",
    "#아래와 같은 경우는 1이 [[1 1], [1 1]], shape(2,2)로 변환\n",
    "#차원이 다를 경우, expand_dims() 함수에 맞게 넣어야 함. \n",
    "#설명 참조: http://excelsior-cjh.tistory.com/entry/Matrix-Broadcasting-%ED%96%89%EB%A0%AC%EC%9D%98-%EB%B8%8C%EB%A1%9C%EB%93%9C%EC%BA%90%EC%8A%A4%ED%8C%85\n",
    "\n",
    "y = tf.add(x, 1)\n",
    "\n",
    "# Create a random random 5x3 matrix\n",
    "z = tf.random_uniform([5, 3])\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16, 19],\n",
       "       [36, 43]], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 3.], shape=(1,), dtype=float32)\n",
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "# Tensor object를 활용하여, tf.add, tf.subtract, tf.multiply로 활용도 가능하다.\n",
    "x = (tf.ones([1], dtype=tf.float32) + 1) * 2 - 1\n",
    "print(x)\n",
    "\n",
    "# Numpy를 활용하여 값을 변환하기\n",
    "import numpy as np\n",
    "\n",
    "x = tf.add(1, 1)                     # tf.Tensor with a value of 2\n",
    "y = tf.add(np.array(1), np.array(1)) # tf.Tensor with a value of 2\n",
    "z = np.multiply(x, y)                # numpy.int64 with a value of \n",
    "\n",
    "#반대로 tf.constant 활용하여 numpy -> tf로 변화\n",
    "np_x = np.array(2., dtype=np.float32)\n",
    "x = tf.constant(np_x)\n",
    "\n",
    "py_y = 3.\n",
    "y = tf.constant(py_y)\n",
    "\n",
    "z = x + y + 1\n",
    "print(z)\n",
    "print(z.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data\n",
    "\n",
    "- 데이터셋 만들기 (tf.data.Dataset 활용)-> Datasets API를 활용하면 성능이나 복잡한 파이프라인을 손쉽게 활용이 가능함.\n",
    "\n",
    "- eager모드가 켜진 상태에서 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step1: 데이터셋 만들기\n",
    "#Dataset.from_tensors와 Dataset.from_tensor_slices를 사용하면 데이터 생성\n",
    "\n",
    "ds_tensors = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# Create a CSV file\n",
    "import tempfile\n",
    "_, filename = tempfile.mkstemp()\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(\"\"\"Line 1\n",
    "Line 2\n",
    "Line 3\n",
    "  \"\"\")\n",
    "ds_file = tf.data.TextLineDataset(filename)\n",
    "\n",
    "#Step2: Apply transformations\n",
    "#map, batch, shuffle등을 활용하여 dataset records를 활용. tf.data.Dataset을 좀 더 자세히..\n",
    "\n",
    "ds_tensors = ds_tensors.map(tf.square).shuffle(2).batch(2)\n",
    "ds_file = ds_file.batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element of ds_tensors\n",
      "tf.Tensor([1 4], shape=(2,), dtype=int32)\n",
      "tf.Tensor([16  9], shape=(2,), dtype=int32)\n",
      "tf.Tensor([25 36], shape=(2,), dtype=int32)\n",
      "\n",
      "Elements in ds_file:\n",
      "tf.Tensor([b'Line 1' b'Line 2'], shape=(2,), dtype=string)\n",
      "tf.Tensor([b'Line 3' b'  '], shape=(2,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "#Step3: Iterate\n",
    "#Use tfe.Iterator on the Dataset obj. \n",
    "print('Element of ds_tensors')\n",
    "for x in tfe.Iterator(ds_tensors):\n",
    "    print(x)\n",
    "    \n",
    "print('\\nElements in ds_file:')\n",
    "for x in tfe.Iterator(ds_file):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Print Tensorflow Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'x:0' shape=() dtype=float32, numpy=45.0>\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'x:0' shape=() dtype=float32, numpy=0.0>\n",
      "0.0\n",
      "<bound method ResourceVariable.assign of <tf.Variable 'x:0' shape=() dtype=float32, numpy=42.0>>\n",
      "tf.Tensor(45.0, shape=(), dtype=float32)\n",
      "tf.Tensor(48.0, shape=(), dtype=float32)\n",
      "tf.Tensor([  45.   90.  180.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.get_variable(name=\"x\", shape=[], dtype=tf.float32, initializer=tf.zeros_initializer)\n",
    "print(x)\n",
    "\n",
    "#Tensorflow의 변수는 tensor로 나타냄으로, read_value()를 통해 현재 값으로 접근이 가능함.\n",
    "#Tensorflow의 함수는 자동으로 초기화\n",
    "\n",
    "#numpy를 통한 변환\n",
    "print(x.read_value().numpy())\n",
    "\n",
    "#Tensorflow변수의 값을 변경하기\n",
    "x.assign(42)\n",
    "print(x.assign)\n",
    "\n",
    "x.assign_add(3)\n",
    "print(x.read_value())\n",
    "\n",
    "#텐서 변수를 자유자제로 활용해보기\n",
    "print(x + 3)\n",
    "\n",
    "print(x * [1, 2, 4]) #자동으로 broadcasting해줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Difference (Gradients)\n",
    "\n",
    " - tfe.gradients_function(f): 입력 f에 대해 arg 미분값을 돌려준다.\n",
    " - tfe.value_and_gradients_function(f): tfe.gradients_function(f)과 비슷하지만, 함수가 들어오면 이전 f값과 미분값에 대해 돌려줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32)\n",
      "[<tf.Tensor: id=19321, shape=(), dtype=float32, numpy=6.0>]\n",
      "2nd grad: (<tf.Tensor: id=19330, shape=(), dtype=float32, numpy=6.0>, [<tf.Tensor: id=19337, shape=(), dtype=float32, numpy=2.0>])\n",
      "[<tf.Tensor: id=19362, shape=(), dtype=float32, numpy=2.0>]\n",
      "[<tf.Tensor: id=19369, shape=(), dtype=float32, numpy=1.0>]\n",
      "[<tf.Tensor: id=19378, shape=(), dtype=float32, numpy=-1.0>]\n"
     ]
    }
   ],
   "source": [
    "def square(x):\n",
    "    return tf.multiply(x, x)\n",
    "assert 9 == square(3.).numpy()\n",
    "\n",
    "grad = tfe.gradients_function(square)\n",
    "assert 6 == grad(3.)[0].numpy()\n",
    "\n",
    "print(square(3.))\n",
    "print(grad(3.)) #x^2 -> 2x -> 6\n",
    "\n",
    "#2차 gradients_function\n",
    "grad2 = tfe.value_and_gradients_function(lambda x: grad(x)[0])\n",
    "#assert 2 == grad2(3.)[0].numpy()\n",
    "print(\"2nd grad: {}\".format(grad2(3.)))\n",
    "\n",
    "#3차 grad.\n",
    "grad3 = tfe.gradients_function(lambda x: grad2(x)[0])\n",
    "#assert 0 == grad3(3.)[0].numpy()\n",
    "print(grad3(3.))\n",
    "\n",
    "#absolute value\n",
    "def abs(x):\n",
    "    return x if x > 0. else -x\n",
    "\n",
    "grad = tfe.gradients_function(abs)\n",
    "\n",
    "print(grad(2.0))  # [1.]\n",
    "print(grad(-2.0)) # [-1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 69.072067\n",
      "Loss at step 0: 66.364388\n",
      "Loss at step 20: 30.010012\n",
      "Loss at step 40: 13.850556\n",
      "Loss at step 60: 6.667610\n",
      "Loss at step 80: 3.474723\n",
      "Loss at step 100: 2.055431\n",
      "Loss at step 120: 1.424524\n",
      "Loss at step 140: 1.144068\n",
      "Loss at step 160: 1.019394\n",
      "Loss at step 180: 0.963970\n",
      "Final loss: 0.940147\n",
      "W, B = 3.046539, 2.140244\n"
     ]
    }
   ],
   "source": [
    "#실제 linear regression을 통하여 활용해보자\n",
    "\n",
    "def prediction(input, weight, bias):\n",
    "    return input * weight + bias\n",
    "\n",
    "# A toy dataset of points around 3 * x + 2\n",
    "NUM_EXAMPLES = 1000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "# A loss function: Mean-squared error\n",
    "def loss(weight, bias):\n",
    "    error = prediction(training_inputs, weight, bias) - training_outputs\n",
    "    return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "# Function that returns the the derivative of loss with respect to\n",
    "# weight and bias\n",
    "grad = tfe.gradients_function(loss)\n",
    "\n",
    "# Train for 200 steps (starting from some random choice for W and B, on the same\n",
    "# batch of data).\n",
    "W = 5.\n",
    "B = 10.\n",
    "learning_rate = 0.01\n",
    "print(\"Initial loss: %f\" % loss(W, B).numpy())\n",
    "for i in range(200):\n",
    "    (dW, dB) = grad(W, B)\n",
    "    W -= dW * learning_rate\n",
    "    B -= dB * learning_rate\n",
    "    if i % 20 == 0:\n",
    "        print(\"Loss at step %d: %f\" % (i, loss(W, B).numpy()))\n",
    "print(\"Final loss: %f\" % loss(W, B).numpy())\n",
    "print(\"W, B = %f, %f\" % (W.numpy(), B.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Grad\n",
    "\n",
    "Custom Gradient 제작하기. \n",
    "\n",
    "주로 cross entropy나 log likelyhood에 쓰이는 예제로 log(1 + e^x) 제작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=9890, shape=(), dtype=float32, numpy=0.5>]\n",
      "[<tf.Tensor: id=9901, shape=(), dtype=float32, numpy=nan>]\n"
     ]
    }
   ],
   "source": [
    "def log1pexp(x):\n",
    "    return tf.log(1 + tf.exp(x))\n",
    "\n",
    "grad_log1pexp = tfe.gradients_function(log1pexp)\n",
    "\n",
    "print(grad_log1pexp(0.)) # [0.5]\n",
    "\n",
    "print(grad_log1pexp(100.)) # x = 100, nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=19202, shape=(), dtype=float32, numpy=0.5>]\n",
      "[<tf.Tensor: id=19214, shape=(), dtype=float32, numpy=1.0>]\n"
     ]
    }
   ],
   "source": [
    "@tfe.custom_gradient\n",
    "def log1pexp(x):\n",
    "    e = tf.exp(x)\n",
    "    def grad(dy):\n",
    "        return dy * (1 - 1 / (1 + e))\n",
    "    return tf.log(1 + e), grad\n",
    "\n",
    "grad_log1pexp = tfe.gradients_function(log1pexp)\n",
    "\n",
    "# Gradient at x = 0 works as before.\n",
    "print(grad_log1pexp(0.))\n",
    "# [0.5]\n",
    "# And now gradient computation at x=100 works as well.\n",
    "print(grad_log1pexp(100.))\n",
    "# [1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and training models\n",
    "\n",
    " - eager에서는 특별히 수정해야 하지 않는 한, tf.layers와 같은 모듈을 사용을 권장함\n",
    " - Optimizer와 layer를 간단하게 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable & Optimization\n",
    "\n",
    " - tfe.Variable: 변형가능한 Tensor값을 저장하는 객체로써, 학습이나 미분을 할때 값에 대한 access가 가능함. 모델의 파라메터들이 python변수에 저장 될 수 있다는 이야기임\n",
    " - tfe.gradients_function(f): 쉬운 미분을 지원하지만, 모든 파라메터들이 f와 연동이 되어있어야 하여, 학습시 큰 파라메터에 대한 대응이 힘듬\n",
    " - tfe.implicit_gradients: 비슷한 기능이지만 몇가지 특수 기능이 있음?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 69.978195\n",
      "Loss at step 0: 67.215584\n",
      "Loss at step 20: 30.237247\n",
      "Loss at step 40: 13.908978\n",
      "Loss at step 60: 6.698107\n",
      "Loss at step 80: 3.513218\n",
      "Loss at step 100: 2.106319\n",
      "Loss at step 120: 1.484737\n",
      "Loss at step 140: 1.210071\n",
      "Loss at step 160: 1.088680\n",
      "Loss at step 180: 1.035020\n",
      "Loss at step 200: 1.011296\n",
      "Final loss: 1.011296\n",
      "W, B = 3.03347, 2.11813\n"
     ]
    }
   ],
   "source": [
    "#실제 linear regression을 통하여 활용해보자\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self):\n",
    "        self.W = tfe.Variable(5., name='weight')\n",
    "        self.B = tfe.Variable(10., name='bias')\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return inputs * self.W + self.B\n",
    "\n",
    "\n",
    "# The loss function to be optimized\n",
    "def loss(model, inputs, targets):\n",
    "    error = model.predict(inputs) - targets\n",
    "    return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "# A toy dataset of points around 3 * x + 2\n",
    "NUM_EXAMPLES = 1000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "# Define:\n",
    "# 1. A model\n",
    "# 2. Derivatives of a loss function with respect to model parameters\n",
    "# 3. A strategy for updating the variables based on the derivatives\n",
    "model = Model()\n",
    "grad = tfe.implicit_gradients(loss)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "# The training loop\n",
    "print(\"Initial loss: %f\" %\n",
    "      loss(model, training_inputs, training_outputs).numpy())\n",
    "for i in range(201):\n",
    "    optimizer.apply_gradients(grad(model, training_inputs, training_outputs))\n",
    "    if i % 20 == 0:\n",
    "        print(\"Loss at step %d: %f\" %\n",
    "              (i, loss(model, training_inputs, training_outputs).numpy()))\n",
    "print(\"Final loss: %f\" % loss(model, training_inputs, training_outputs).numpy())\n",
    "print(\"W, B = %s, %s\" % (model.W.numpy(), model.B.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Models (개선필요)\n",
    "\n",
    "MNIST 2 Layer모델을 간단하게 Class로 만드는 예제\n",
    "\n",
    "tfe.Network: 기본적으로 layer의 Container역할을 하여, 다른 NW객체에 임비디드 되어 NW객체가 된다.\n",
    "\n",
    "추가로, inspection, saving, & restoring에 도움을 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(tfe.Network):\n",
    "    def __init__(self):\n",
    "        super(MNISTModel, self).__init__()\n",
    "        self.layer1 = self.track_layer(tf.layers.Dense(units=10))\n",
    "        self.layer2 = self.track_layer(tf.layers.Dense(units=10))\n",
    "    def call(self, input):\n",
    "        \"\"\"모델 실행\"\"\"\n",
    "        result = self.layer1(input)\n",
    "        result = self.layer2(result)\n",
    "        return result\n",
    "    \n",
    "#placeholder나 session에 대한 기능이 없고, input을 pass되면 자동으로 세팅 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 784)\n",
      "tf.Tensor([[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]], shape=(1, 1, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터셋 생성하기\n",
    "model = MNISTModel()\n",
    "batch = tf.zeros([1, 1, 784])\n",
    "print(batch.shape)\n",
    "result = model(batch)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-13bd9ea72257>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimplicit_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "#학습을 위한 loss func, grad, 그리고 업데이트\n",
    "\n",
    "#1. loss func\n",
    "def loss_function(model, x, y):\n",
    "    y_ = model(x)\n",
    "    return tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_)\n",
    "\n",
    "#2. training loop\n",
    "#implicit_gradients(): 모든 TF 값에 대한 미분을 계산한다.\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate =0.001)\n",
    "for (x, y) in tfe.Iterator(dataset):\n",
    "    grads = tfe.implicit_gradients(loss_function)(model, x, y)\n",
    "    optimizer.apply_gradients(grads)\n",
    "\n",
    "#GPU사용하기\n",
    "#optimizer.min을 통해서, 짧게 작성하였지만, apply_gradients()기능을 써도 가능\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    for (x, y) in tfe.Iterator(dataset):\n",
    "        optimizer.minimize(lambda: loss_function(model, x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug Errors with Instant Feedback\n",
    "\n",
    "- runtime 이슈들을 디버깅하고, 코드에 관련된 사항을 interactive하게 볼 수 있음\n",
    "- 간단하게 4 vector와 2개의 tf.slice()를 활용하여, 정상 케이스와 에러 케이스를 분류 하겠음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = tf.constant([10.0, 20.0, 30.0, 40.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 20.  30.  40.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#정상 케이스: 인덱스 값이 내부에 적용이 되기 때문\n",
    "print(tf.slice(vector, [1], [3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught error: Expected size[0] in [0, 3], but got 4 [Op:Slice]\n"
     ]
    }
   ],
   "source": [
    "#비정상 케이스: 인덱스 값이 데이터에서 넘가기 때문에\n",
    "try:\n",
    "    print(tf.slice(vector, [1], [4]))\n",
    "except tf.OpError as e:\n",
    "    print(\"Caught error: %s\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU 사용하기\n",
    "\n",
    " - eager에서는 GPU가 자동으로 실행되지 않기 때문에, 지정해주고 사용해야 한다.\n",
    " - .gpu()를 활용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available.\n"
     ]
    }
   ],
   "source": [
    "# The example code from here on will work only if your notebook\n",
    "# is running on a machine with a functional CUDA GPU. The following\n",
    "# line checks that.\n",
    "is_gpu_available = tfe.num_gpus() > 0\n",
    "\n",
    "# Create some Tensors\n",
    "SIZE = 1000\n",
    "cpu_tensor = tf.random_normal([SIZE, SIZE])\n",
    "\n",
    "if is_gpu_available:\n",
    "    gpu_tensor = cpu_tensor.gpu()\n",
    "else:\n",
    "    print(\"GPU not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to conduct matmul on CPU:\n",
      "CPU times: user 162 ms, sys: 6.41 ms, total: 169 ms\n",
      "Wall time: 28.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=13, shape=(1000, 1000), dtype=float32, numpy=\n",
       "array([[-13.64949799, -31.02673149,  15.75299549, ..., -19.90308762,\n",
       "        -80.68657684,  19.10900116],\n",
       "       [-31.3809948 ,  17.13792038, -23.38959122, ...,   8.40496445,\n",
       "         -7.32779026, -19.07193184],\n",
       "       [-19.6288166 , -14.6174736 ,  -7.89929771, ...,  15.44713306,\n",
       "         15.5273037 ,  21.50070572],\n",
       "       ..., \n",
       "       [ 12.01198769,  -7.41007423,  26.26524734, ...,   4.17672539,\n",
       "          0.81272852, -18.06852722],\n",
       "       [  1.57177544,   5.99784756,  25.80046272, ...,  18.59385681,\n",
       "         22.02895355,  11.35874557],\n",
       "       [-13.62286949, -78.86634064,  -0.60588956, ...,  24.94280243,\n",
       "        -44.15736008, -60.31943512]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time a CPU-based matrix multiplication\n",
    "\n",
    "print(\"Time to conduct matmul on CPU:\")\n",
    "%time tf.matmul(cpu_tensor, cpu_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time GPU-based matrix multiplications.\n",
    "\n",
    "if is_gpu_available:\n",
    "    # First use of the GPU will be slow:\n",
    "    print(\"Time to conduct first matmul on GPU:\")\n",
    "    %time tf.matmul(gpu_tensor, gpu_tensor)\n",
    "    print()\n",
    "\n",
    "    # Subsequent uses are much faster:\n",
    "    print(\"Time to conduct second matmul on GPU:\")\n",
    "    %time tf.matmul(gpu_tensor, gpu_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to conduct CPU matmul:\n",
      "CPU times: user 161 ms, sys: 927 µs, total: 162 ms\n",
      "Wall time: 26 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Second timing demo for GPUs, after it has been used once:\n",
    "\n",
    "cpu_tensor = tf.random_normal([SIZE, SIZE])\n",
    "print(\"Time to conduct CPU matmul:\")\n",
    "%time tf.matmul(cpu_tensor, cpu_tensor)\n",
    "print()\n",
    "\n",
    "if is_gpu_available:\n",
    "    gpu_tensor = cpu_tensor.gpu()\n",
    "    print(\"Time to conduct GPU matmul:\")\n",
    "    %time tf.matmul(gpu_tensor, gpu_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: Took 0.26638007164001465 seconds to multiply a (1000, 1000) matrix by itself 10 times\n"
     ]
    }
   ],
   "source": [
    "#1000*1000 매트릭스 계산하기\n",
    "\n",
    "import time\n",
    "\n",
    "def measure(x):\n",
    "    # The very first time a GPU is used by TensorFlow, it is initialized.\n",
    "    # So exclude the first run from timing.\n",
    "    tf.matmul(x, x)\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(10):\n",
    "        tf.matmul(x, x)\n",
    "    end = time.time()\n",
    "\n",
    "    return \"Took %s seconds to multiply a %s matrix by itself 10 times\" % (end - start, x.shape)\n",
    "\n",
    "# Run on CPU:\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    print(\"CPU: %s\" % measure(tf.random_normal([1000, 1000])))\n",
    "\n",
    "# If a GPU is available, run on GPU:\n",
    "if tfe.num_gpus() > 0:\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        print(\"GPU: %s\" % measure(tf.random_normal([1000, 1000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error copying tensor to device: GPU:0. GPU:0 unknown device.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-76e20da2e730>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mx_gpu0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mx_cpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_nightly/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mgpu\u001b[0;34m(self, gpu_index)\u001b[0m\n\u001b[1;32m    756\u001b[0m       \u001b[0;32mas\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m     \"\"\"\n\u001b[0;32m--> 758\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"GPU:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_nightly/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_copy\u001b[0;34m(self, ctx, device_name)\u001b[0m\n\u001b[1;32m    713\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m       \u001b[0mnew_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copy_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error copying tensor to device: GPU:0. GPU:0 unknown device."
     ]
    }
   ],
   "source": [
    "#Tensor 객체를 활용하여, 각 디바이스 별로 활용하는 것이 가능하다\n",
    "\n",
    "x = tf.random_normal([10, 10])\n",
    "\n",
    "x_gpu0 = x.gpu()\n",
    "x_cpu = x.cpu()\n",
    "\n",
    "_ = tf.matmul(x_cpu, x_cpu)  # Runs on CPU\n",
    "_ = tf.matmul(x_gpu0, x_gpu0)  # Runs on GPU:0\n",
    "\n",
    "if tfe.num_gpus() > 1:\n",
    "    x_gpu1 = x.gpu(1)\n",
    "    _ = tf.matmul(x_gpu1, x_gpu1)  # Runs on GPU:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Eagar with Graphs\n",
    "\n",
    "- Eagar 자체는 개발하고 디버깅 할 때 좋은 기능을 가지지만, Tensorflow graph형식이 분산 학습, 성능 최적화, 상용개발에 더 적합\n",
    "- 현재 모델을 graph형태로 변경하기 위해서는, eager를 disable하고 실행하면 됨\n",
    "- 관련 예제 코드: [MNIST with Eager](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples/mnist)\n",
    "- 위의 예제 코드는 checkpoints를 저장하고 불러올 수 있기 때문에, 상용에도 적합하다.\n",
    "\n",
    "## 현재 활용하고 있는 코드의 변화\n",
    " \n",
    "- 현재 사용하고 있는 데이터에서, 형태를 tf.data로 변경하는것을 추천 드립니다.\n",
    "  - [참고링크 1](https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html)\n",
    "  - [참고링크 2](https://www.tensorflow.org/programmers_guide/datasets)\n",
    "- tf.layer.Conv2D()와 같은 Object-oriented 기능 활용을 추천 (Explict storage for variables\n",
    "- 대부분의 모델이 eagar로 활용이 가능하지만, dynamic 모델에 대한 control flow같은 경우는 추가로 검토가 필요\n",
    "- tfe.enable_eagar_execution()을 활용하면, 끌수가 없기 때문에 Python 세션을 재시작 추천"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 향후 To-Do\n",
    "\n",
    "- 11/29 블로그를 지속적으로 백업하면서, 추가 개선 방향을 고민 할 것\n",
    " 1. 데이터 전처리\n",
    " 2. 제 3의 모델 (Conv Seq2Seq) 테스트\n",
    " 3. 기존의 심심이 데이터에서 Generation 한 것이 의미가 있어 보임 -> 블로그도 Generation 후, 실제 의마기 있는 문장에 대한 데이터 추출로 보강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
