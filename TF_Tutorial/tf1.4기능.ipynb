{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './dataset/chat.in'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "\n",
    "delimiter = \"\\t\"\n",
    "\n",
    "def read_in_data(data_path):\n",
    "    user = []\n",
    "    bot = []\n",
    "    with open(data_path, mode=\"rt\", encoding=\"utf-8\") as fh:\n",
    "        utt = fh.readlines()\n",
    "        \n",
    "        for i, line in enumerate(utt):\n",
    "            split_line = line.split(delimiter)\n",
    "            query = split_line[0].replace(\"\\\"\", \"\").replace(\"\\n\", \"\")\n",
    "            answer = split_line[1].replace(\"\\\"\", \"\").replace(\"\\n\", \"\")\n",
    "            \n",
    "            user.append(query)\n",
    "            bot.append(answer)\n",
    "            \n",
    "        return user, bot\n",
    "    \n",
    "user_query, answer_query = read_in_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 dataset으로 변환 및 문장 쪼개기\n",
    "dict_data = {'user': user_query, 'bot': answer_query}\n",
    "df_data = pd.DataFrame(dict_data, columns=['user', 'bot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "twit_tokenizer = lambda x: twitter.morphs(x)\n",
    "df_data['user_token'] = df_data['user'].apply(twit_tokenizer) #Tokenizer로 저장\n",
    "df_data['bot_token'] = df_data['bot'].apply(twit_tokenizer) #Tokenizer로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>bot</th>\n",
       "      <th>user_token</th>\n",
       "      <th>bot_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>메롱</td>\n",
       "      <td>저랑 놀고 싶으신거군요</td>\n",
       "      <td>[메롱]</td>\n",
       "      <td>[저, 랑, 놀고, 싶으, 신거, 군요]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>아 그러면 안되</td>\n",
       "      <td>네? 어떤 말씀이신지 궁금하네요</td>\n",
       "      <td>[아, 그러면, 안되]</td>\n",
       "      <td>[네, ?, 어떤, 말씀, 이, 신지, 궁금하네, 요]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>오늘 하루 어때?</td>\n",
       "      <td>당신과 함께해서 좋은거 같아요</td>\n",
       "      <td>[오늘, 하루, 어때, ?]</td>\n",
       "      <td>[당신, 과, 함께해서, 좋, 은, 거, 같아, 요]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>너 지금 얼마 있어?</td>\n",
       "      <td>저는 마음만큼은 부자에요.</td>\n",
       "      <td>[너, 지금, 얼마, 있어, ?]</td>\n",
       "      <td>[저, 는, 마음, 만큼은, 부자, 에요, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          user                bot          user_token  \\\n",
       "0           메롱       저랑 놀고 싶으신거군요                [메롱]   \n",
       "1     아 그러면 안되  네? 어떤 말씀이신지 궁금하네요        [아, 그러면, 안되]   \n",
       "2    오늘 하루 어때?   당신과 함께해서 좋은거 같아요     [오늘, 하루, 어때, ?]   \n",
       "3  너 지금 얼마 있어?     저는 마음만큼은 부자에요.  [너, 지금, 얼마, 있어, ?]   \n",
       "\n",
       "                        bot_token  \n",
       "0          [저, 랑, 놀고, 싶으, 신거, 군요]  \n",
       "1  [네, ?, 어떤, 말씀, 이, 신지, 궁금하네, 요]  \n",
       "2   [당신, 과, 함께해서, 좋, 은, 거, 같아, 요]  \n",
       "3      [저, 는, 마음, 만큼은, 부자, 에요, .]  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize 된 단어를 합쳐서 사전을 만들어보자\n",
    "token_dict = itertools.chain.from_iterable(list(df_data['user_token'] + list(df_data['bot_token'])))\n",
    "token_dict = list(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real vocabulary size    38\n",
      "Limited vocabulary size 38\n"
     ]
    }
   ],
   "source": [
    "#http://tensorlayer.readthedocs.io/en/latest/\n",
    "#딕셔너리 만들기\n",
    "\n",
    "vocab_size = 38\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = \\\n",
    "        tl.nlp.build_words_dataset(token_dict, vocab_size, True)\n",
    "    \n",
    "ids = lambda x: tl.nlp.words_to_word_ids(x, dictionary) #to ids\n",
    "context = lambda x: tl.nlp.word_ids_to_words(x, reverse_dictionary) #to ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            [저, 랑, 놀고, 싶으, 신거, 군요]\n",
       "1    [네, ?, 어떤, 말씀, 이, 신지, 궁금하네, 요]\n",
       "2     [당신, 과, 함께해서, 좋, 은, 거, 같아, 요]\n",
       "3      [저, 는, 마음, 만큼은, 부자, 에요, UNK]\n",
       "Name: bot_ids, dtype: object"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#token - ids 생성\n",
    "df_data['user_ids'] = df_data['user_token'].apply(ids)\n",
    "df_data['user_ids'].apply(context)\n",
    "\n",
    "df_data['bot_ids'] = df_data['bot_token'].apply(ids)\n",
    "df_data['bot_ids'].apply(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocabulary ./dataset/test5.in from data ./dataset/chat.in\n"
     ]
    }
   ],
   "source": [
    "#Vocab Dictionary 생성하기, most frequent word\n",
    "tl.nlp.create_vocabulary('./dataset/test5.in', './dataset/chat.in', max_vocabulary_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    [4]\n",
       "1           [10, 11, 12]\n",
       "2        [19, 20, 21, 1]\n",
       "3    [29, 30, 31, 32, 1]\n",
       "Name: user_ids, dtype: object"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data[\"user_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 'EOS'],\n",
       " [10, 11, 12, 'EOS'],\n",
       " [19, 20, 21, 1, 'EOS'],\n",
       " [29, 30, 31, 32, 1, 'EOS']]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.prepro.sequences_add_end_id(list(df_data['user_ids']), end_id='EOS') # 단순히 add end_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1], dtype=object)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.prepro.sequences_add_end_id(list(df_data['user_ids']), end_id=999) # 단순히 add end_ID\n",
    "tl.prepro.sequences_add_start_id(list(df_data['user_ids']), start_id=123) # 단순히 add end_ID\n",
    "tl.prepro.sequences_add_end_id_after_pad(list(df_data['user_ids']), end_id=99, pad_id=0) #add end_ID + padding, 근데 잘 안됨\n",
    "tl.prepro.sequences_get_mask(list(df_data['user_ids']), pad_val=0) #이것도 왜..?ㄴ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dataset():\n",
    "    with open(\"./dataset/test.in\", 'w') as f:\n",
    "        for i in range(len(df_data['user_ids'])):\n",
    "            f.write(str(df_data['user_ids'][i]) + '\\n')\n",
    "            f.write(str(df_data['bot_ids'][i]) + '\\n')        \n",
    "\n",
    "gen_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset(\"./dataset/test.in\")\n",
    "tr_data = tf.data.Dataset.from_tensor_slices((user_query, answer_query))\n",
    "\n",
    "iterator = tf.data.Iterator.from_structure(tr_data.output_types,\n",
    "                                   tr_data.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "training_init_op = iterator.make_initializer(tr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/vahidk/EffectiveTensorflow\n",
    "dataset = tf.data.TextLineDataset(\"./dataset/test.in\")\n",
    "\n",
    "tr_data = tf.data.Dataset.from_tensor_slices((user_query, answer_query))\n",
    "\n",
    "iterator = tf.data.Iterator.from_structure(tr_data.output_types,\n",
    "                                   tr_data.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "training_init_op = iterator.make_initializer(tr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'\\xeb\\xa9\\x94\\xeb\\xa1\\xb1', b'\\xec\\xa0\\x80\\xeb\\x9e\\x91 \\xeb\\x86\\x80\\xea\\xb3\\xa0 \\xec\\x8b\\xb6\\xec\\x9c\\xbc\\xec\\x8b\\xa0\\xea\\xb1\\xb0\\xea\\xb5\\xb0\\xec\\x9a\\x94')\n",
      "(b'\\xec\\x95\\x84 \\xea\\xb7\\xb8\\xeb\\x9f\\xac\\xeb\\xa9\\xb4 \\xec\\x95\\x88\\xeb\\x90\\x98', b'\\xeb\\x84\\xa4? \\xec\\x96\\xb4\\xeb\\x96\\xa4 \\xeb\\xa7\\x90\\xec\\x94\\x80\\xec\\x9d\\xb4\\xec\\x8b\\xa0\\xec\\xa7\\x80 \\xea\\xb6\\x81\\xea\\xb8\\x88\\xed\\x95\\x98\\xeb\\x84\\xa4\\xec\\x9a\\x94')\n",
      "(b'\\xec\\x98\\xa4\\xeb\\x8a\\x98 \\xed\\x95\\x98\\xeb\\xa3\\xa8 \\xec\\x96\\xb4\\xeb\\x95\\x8c?', b'\\xeb\\x8b\\xb9\\xec\\x8b\\xa0\\xea\\xb3\\xbc \\xed\\x95\\xa8\\xea\\xbb\\x98\\xed\\x95\\xb4\\xec\\x84\\x9c \\xec\\xa2\\x8b\\xec\\x9d\\x80\\xea\\xb1\\xb0 \\xea\\xb0\\x99\\xec\\x95\\x84\\xec\\x9a\\x94')\n",
      "(b'\\xeb\\x84\\x88 \\xec\\xa7\\x80\\xea\\xb8\\x88 \\xec\\x96\\xbc\\xeb\\xa7\\x88 \\xec\\x9e\\x88\\xec\\x96\\xb4?', b'\\xec\\xa0\\x80\\xeb\\x8a\\x94 \\xeb\\xa7\\x88\\xec\\x9d\\x8c\\xeb\\xa7\\x8c\\xed\\x81\\xbc\\xec\\x9d\\x80 \\xeb\\xb6\\x80\\xec\\x9e\\x90\\xec\\x97\\x90\\xec\\x9a\\x94.')\n",
      "End of training dataset.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    # initialize the iterator on the training data\n",
    "    sess.run(training_init_op)\n",
    "\n",
    "    # get each element of the training dataset until the end is reached\n",
    "    while True:\n",
    "        try:\n",
    "            elem = sess.run(next_element)\n",
    "            print(elem)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"End of training dataset.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real vocabulary size    8\n",
      "Limited vocabulary size 8\n"
     ]
    }
   ],
   "source": [
    "#Data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_data['user_ids'], df_data['bot_ids'], test_size=0.3,random_state=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
