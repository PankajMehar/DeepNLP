ChronoNet:ADeepRecurrentNeuralNetworkforAbnormalEEGIdentiﬁcation∗SubhrajitRoy,IsabellKiral-Kornek,andStefanHarrerIBMResearch–Australia,60CityRd,Southbank,VIC,3006,Australia{subhrajit.roy,isabeki,sharrer}@au1.ibm.comAbstractBrain-relateddisorderssuchasepilepsycanbediagnosedbyanalyzingelectroencephalograms(EEG).However,manualanalysisofEEGdatare-quireshighlytrainedclinicians,andisaproce-durethatisknowntohaverelativelylowinter-rateragreement(IRA).Moreover,thevolumeofthedataandtherateatwhichnewdatabecomesavail-ablemakemanualinterpretationatime-consuming,resource-hungry,andexpensiveprocess.Incon-trast,automatedanalysisofEEGdataoffersthepotentialtoimprovethequalityofpatientcarebyshorteningthetimetodiagnosisandreduc-ingmanualerror.Inthispaper,wefocusononeoftheﬁrststepsininterpretinganEEGsession-identifyingwhetherthebrainactivityisabnor-malornormal.Tosolvethistask,weproposeanovelrecurrentneuralnetwork(RNN)architec-turetermedChronoNetwhichisinspiredbyre-centdevelopmentsfromtheﬁeldofimageclassiﬁ-cationanddesignedtoworkefﬁcientlywithEEGdata.ChronoNetisformedbystackingmultiple1Dconvolutionlayersfollowedbydeepgatedre-currentunit(GRU)layerswhereeach1Dconvo-lutionlayerusesmultipleﬁltersofexponentiallyvaryinglengthsandthestackedGRUlayersaredenselyconnectedinafeed-forwardmanner.WeusedtherecentlyreleasedTUHAbnormalEEGCorpusdatasetforevaluatingtheperformanceofChronoNet.Unlikepreviousstudiesusingthisdataset,ChronoNetdirectlytakestime-seriesEEGasinputandlearnsmeaningfulrepresentationsofbrainactivitypatterns.ChronoNetoutperformsthepreviouslyreportedbestresultsby7.79%therebysettinganewbenchmarkforthisdataset.Further-more,wedemonstratethedomain-independentna-tureofChronoNetbysuccessfullyapplyingittoclassifyspeechcommands.∗ThismanuscripthasbeensubmittedtoIJCAI2018.1IntroductionElectroencephalography(EEG)isanoninvasivemethodtomeasurebrainactivitythroughtherecordingofelectricalactivityacrossapatient’sskullandscalpandisfrequentlyusedforthediagnosisandmanagementofvariousneuro-logicalconditionssuchasepilepsy,somnipathy,coma,en-cephalopathies,andothers.Despitehavinglowerspatialres-olutionthanbrainimagingtechniquessuchasmagneticreso-nanceimaging(MRI)andcomputedtomography(CT),EEGisapopulardiagnosticstoolamongphysiciansduetoitsex-cellenttemporalresolution,lowcost,andnoninvasivenature[Smith,2005].AssymptomsarenotguaranteedtobepresentintheEEGsignalatalltimes,thediagnosisofaneurologicalconditionviaEEGinterpretationtypicallyinvolveslong-termmoni-toringortherecordingofmultipleshortsessions.Inthisprocess,largeamountsofdataaregeneratedthatsubse-quentlyneedtobemanuallyinterpretedbyexpertinvesti-gators.Therelativelylowavailabilityofcertiﬁedexpertin-vestigatorsandhighvolumeofdatamakeEEGinterpreta-tionatime-consumingprocessthatcanintroduceadelayofhourstoweeksinthepatient’scourseoftreatment.More-over,EEGinterpretationisknowntohavelowinter-rateragreement,whichcanleadtomisdiagnosisormissedevents[Azumaetal.,2003].Introducingacertainlevelofautoma-tiontotheEEGinterpretationtaskcouldserveasanaidtoneurologistsbyacceleratingthereadingprocessandtherebyreducingworkload.Itisthesereasonswhyautomaticinter-pretationofEEGbymachinelearningtechniqueshasgainedpopularityinrecenttimes[Golmohammadietal.,2017;Schirrmeisteretal.,2017].WheninterpretinganEEGrecording,ﬁrst,anassessmentismadeastowhethertherecordedsignalappearstoshowab-normalornormalbrainactivitypatternsasper[L´opezetal.,2015].Thisdecisioncaninﬂuencewhichmedicationisbeingprescribedorwhetherfurtherinvestigationisnecessary.Typ-ically,both,patternsintherecordingandthepatient’sstateofconsciousnessarebeingconsideredwhendecidingwhetherarecordingshowsaabnormalornormalEEG.Themotivationbehindourworkistoautomatethisﬁrststepofinterpretation.WedosousingarecentlyreleaseddatasetknownastheTUHAbnormalEEGCorpus,whichisthelargestofitstypetodate[ObeidandPicone,2016].In-spiredbysuccessessintime-domainsignalclassiﬁcation,weexplorerecurrentneuralnetwork(RNN)architecturesusingtherawEEGtime-seriessignalasinput.Thissetsusapartfrompreviouspublications[L´opezetal.,2015]and[L´opez,2017],inwhichtheauthorsﬁrstextractedhand-engineeredfrequencyfeaturesfromtherecordingsthattheythenusedtoclassifythesignalusingbothtraditionalmachinelearningalgorithmssuchask-nearestneighbour,randomforests,andhiddenmarkovmodelsandmoderndeeplearningtechniquessuchasconvolutionalneuralnetworks(CNN).WeshowthatthecombinationofrawtimeseriesandRNNseliminatestheneedtoextracthand-craftedfeaturesandallowstheclassiﬁertoautomaticallylearnrelevantpat-terns,surpassingpreviousresultsby3.47%.Takinginspira-tionfrom1Dconvolutionlayers[Goodfellowetal.,2016],gatedrecurrentunits[Choetal.,2014],inceptionmod-ules[Szegedyetal.,2015],anddenselyconnectednetworks[Huangetal.,2016],webuildanoveldeepgatedRNNnamedChronoNetwhichfurtherincreasesaccuracybyanadditional4.32%,resultinginanoverall7.79%improvementoverpre-viousresultsand,thus,settinganewbenchmarkfortheTUHAbnormalEEGCorpus.ByapplyingChronoNettotheSpeechCommandsDataset[Warden,2016],weshowitsutilityforgeneraltimeseriesanalysisbeyondEEGinterpretation.2BackgroundandTheoryRawEEGsignalsaretemporalrecordingsthatmayexhibitpatternsandperiodicitiesatvarioustimescales.Amethodthathassuccessfullybeenusedtoclassifytimesignals,forexamplespeech,istheuseofrecurrentneuralnetworks(RNNs).Inthissection,weﬁrstdescribetheworkingprin-cipleofRNNs.Thiswillbefollowedbythedescriptionofamodernandsophisticatedrecurrentunitnamedgatedrecur-rentunit(GRU)thatiswellsuitedtolearninglonger-termde-pendenciesandcorrelations.Wewillthendiscusstheconceptofinceptionmodulesanddenselyconnectedneuralnetworks(conceptsusedinconvolutionalneuralnetworks)whichwewilluseforEEGdataanalysistoaccountforpatternsemerg-ingatdifferentscalesandformitigationofvanishinggradi-ents,respectively.ThiscollectionofprinciplesandmodulesprovidestheessentialbasisforunderstandingtheChronoNetarchitectureproposedinSection3.2.1RecurrentNeuralNetworksRNNsareafamilyofneuralnetworksforprocessingvariable-lengthsequentialdata.AnRNNmaintainsarecur-renthiddenstatewhoseactivationateachtimeisdependentonthatoftheprevioustimestep.Moreformally,givenase-quencex=(x1,x2,...,xT),ateachtimestept,anRNNup-datesitsrecurrenthiddenstatehtbasedonthecurrentinputvectorxtandtheprevioushiddenstateht−1asfollows:ht=(cid:26)0ift=0g(ht−1,xt)otherwise,(1)wheregisanonlinearfunction.InaclassicalRNN,therecurrenthiddenunitofEquation(1)isupdatedinthefollowingway:ht=f(Wxt+Uht−1+b),(2)wherefisapointwisenonlinearactivationfunctionsuchasalogisticsigmoidfunctionorahyperbolictangentfunction.WhileEquation(2)allowsanRNNtoprocesssequencesofarbitrarylength,ithasbeenobservedthatgradientsoffcangrowordecayexponentiallyoverlongsequencesdur-ingtraining[Bengioetal.,1994].ThisphenomenonmakesitdifﬁcultforanRNNtolearnlong-termdependenciesandcorrelations.Onewayoftacklingthisissueistodesignmoresophis-ticatedrecurrentunitswhichcomputeanafﬁnetransforma-tionfollowedbyasimpleelement-wisenonlinearitybyusinggatingunits.Twopopularmodelsinusearethelongshort-termmemory(LSTM)[HochreiterandSchmidhuber,1997;Graves,2013]andthegatedrecurrentunit(GRU)[Choetal.,2014].WhileithasbeenshownthatLSTMsandGRUssig-niﬁcantlyoutperformclassicalRNNs,whichoneamongthetwoperformsbetterisyettobeconclusivelyshown[Chungetal.,2014].Inthispaper,weuseGRUssincetheyusefewerparametersthanLSTMsandhenceofferafastertrainingtimewhilerequiringdatatogeneralize.2.2GatedRecurrentUnitAgatedrecurrentunit(GRU)producesthecurrentvalueofhiddenstatehtbyperformingalinearinterpolationbetweenanintermediatecandidatehiddenstate˜htderivedfromEqua-tion(2)andthepreviousvalueofhiddenstateht−1.AGRUemploystwogates:anupdategateztcontrollingtheextenttowhichthepreviousstatewillbeoverwrittenandaresetgatertdecidinghowmuchofthepreviousstateshouldbeforgottenwhilecomputingthecandidatehiddenstate.Moreformally,theGRUmodelcanbepresentedinthefollowingmathematicalform:ht=(1−zt)(cid:12)ht−1+zt(cid:12)˜ht(3)˜ht=g(Whxt+Uh(rt(cid:12)ht−1)+bh)(4)zt=σ(Wzxt+Uzht−1+bz)(5)rt=σ(Wrxt+Urht−1+br),(6)wheregandσarenonlinearactivationfunctionsand(cid:12)de-noteselement-wisemultiplication.2.3InceptionModuleTheinceptionmodulewasproposedbySzegedyetal.[Szegedyetal.,2015]asabuildingblockfortheGoogLeNetarchitecture.Unliketraditionalconvolutionalneuralnet-works,theinceptionmoduleusesﬁltersofvariedsizeinaconvolutionlayertocapturefeaturesofdifferentlevelsofab-straction.Processingvisualinformationatdifferentscalesandaggregatingthemallowsthenetworktoefﬁcientlyextractrelevantfeatures.Typically,themoduleusesthreeﬁltersofsizes1×1,3×3,and5×5.Moreover,analternativeparallelpathisalsoincludedwhichimplementsa3×3max-poolingoperation.However,naivelyintroducingmoreﬁltersinconvolutionallayersincreasesthenumberofparameters.Thismakestrain-ingofanetworkcomputationallymoreintensivecomparedtotraditionalCNNs.Hence,1×1ﬁltersareincludedinthein-ceptionmoduletoimplement”bottlenecklayers”fordimen-sionalityreduction.2.4DenseNetDenseNetisadeepconvolutionalneuralnetworkarchitecturerecentlyproposedby[Huangetal.,2016].ThemainideaofDenseNetisthatitconnectseachlayerwitheveryotherlayerinafeed-forwardfashion.Eachlayerusesthefeaturemapsofallitsprecedinglayersasinputandpassesitsownfeaturemapsasinputtoallsubsequentlayers.Hence,whileatra-ditionalCNNwithLlayershasLconnections,inDenseNetthereareL(L+1)/2directconnections.DenseNetmitigatestheproblemofvanishing/explodinggradientsthatisobservedinverydeepnetworks[Heetal.,2015].Itachievesthatbyprovidingshort-cutpathsforthegradientstopassduringbackpropagation.Thisallowsthelearningalgorithmtochoosetheappropriatemodelcomplex-ityduringtraining.3MethodsInceptionmodules(seeSection2.3)wereoriginallyproposedtoenableaconvolutionalneuralnetworktoaccountfordif-ferentabstractionlayersinthecontextofimageprocessing.Similarly,denselyconnectednetworks(seeSection2.4)weredevelopedtoaddressvanishinggradientsduetobackpropa-gationindeepconvolutionalneuralnetworks.Asdescribedpreviously,EEGdatacontainsinformationacrossdifferentscalesinthetime-domain.Furthermore,us-ingdeepRNNarchitecturesmightleadtotheproblemofvanishingorexplodinggradients.Ifdesignedproperly,ad-vantagesofinceptionmodulesanddenselyconnectedlayersmay,thus,equallyapplytoproblemsinthetime-domain.Inthefollowingsection,wewillusetheconceptsofinceptionmodulesanddenselyconnectednetworkstobuildnovelre-currentneuralnetworksarchitecturesfortime-seriesclassiﬁ-cation.3.1ConvolutionalGatedRecurrentNeuralNetwork(C-RNN)Consideringtheinputisatimeseries,anobviousﬁrstap-proachistostackmultipleGRUlayersasshowninFigure1a.Thispopulararchitectureforhandlingsequentialinputdatahasledtostate-of-the-artaccuracyinvariouspatternrecogni-tiontasks,especiallyinnaturallanguageprocessing[Tangetal.,2016;Yinetal.,2017].However,whenappliedtorelativelylonginputtime-seriesdata(asopposedtoembeddingvectors[Penningtonetal.,2014]inthecaseofnaturallanguageprocessing),thisap-proachturnsouttobecomputationallyveryintensiveandtimeconsumingtotrain.Tosolvethisproblem,datacanbedownsampledtoanacceptablelengthbeforeitisgivenasinputtoRNNs.However,usingﬁxedvaluesmeansthatnetworkswillnotbeabletoadapttothedataathand.Inordertomitigatetheseproblems,weusedmultiple1Dcon-volution(Conv1D)layerswithstrideslargerthan1,enablingthenetworktolearntoappropriatelyreducetheinputsignalautomatically.Theresultingarchitecture(C-RNN)isacombinationofConv1DlayersfollowedbystackedGRUlayers.Conv1Dlayershavetwoadvantages.First,theylearntosub-samplethesignaland,thus,reducetheinputvector’slengthasweGRU, 32 GRU, 32 GRU, 32 GRU, 32 SoftMaxInput(15000, 22)(32)(2)(1875, 32)(1875, 32)(1875, 32)(a)Conv1D, 4, 32, /2 Conv1D, 4, 32, /2 Conv1D, 4, 32, /2 GRU, 32 GRU, 32 GRU, 32 GRU, 32 SoftMaxInput(15000, 22)(7500, 32)(1875, 32)(1875, 32)(1875, 32)(1875, 32)(32)(2)(3750, 32)(b)Figure1:(a)AdeepgatedrecurrentneuralnetworkwhichfeedstheinputdirectlyintostackedGRUlayers.(b)Theconvolutionalgatedrecurrentneuralnetwork(C-RNN)whichstacksmultiple1DconvolutionlayersfollowedbyasmallnumberofGRUlayers.movetowardshigherlayers.Thisbecomesparticularlyrele-vantwhenreachingGRUlayers,whichduringtrainingcon-stitutethemostcomputationallyexpensivepartofthenet-work.Second,Conv1Dlayersextractlocalinformationfromneighbouringtimepoints,aﬁrststeptowardslearningtempo-raldependencies.FollowingConv1Dlayers,theGRUlayersareresponsibleforcapturingbothshort-andlong-termde-pendencies.ThespeciﬁcnetworkusedinthispaperispresentedinFig-ure1b.TheformatsusedthroughoutthepapertodescribeConv1DandGRUlayersare(layername,ﬁlterlength,num-berofﬁlters,stridesize)and(layername,numberofﬁlters)respectively.3.2InceptionConvolutionalGatedRecurrentNeuralNetwork(IC-RNN)InthepreviousC-RNNarchitecture,eachConv1Dlayerhadthecapabilitytoextractlocalinformationatonlyonetimescaledeterminedbyasingleﬁxedﬁltersize,limitingtheﬂex-ibilityofthemodel.Sincetherateofchangeofinformationinatimeseriesdependsonthetaskathand,theﬁltersizeforeachConv1Dlayerwouldhavetobehand-pickedtoﬁttheparticulardata.Toaddressthisproblem,takinginspirationfrom[Szegedyetal.,2015],wedesignedanarchitecturewhichexpandsuponC-RNNbyincludingmultipleﬁltersofvaryingsizesineachConv1Dlayer.Thisallowsforthenetworktoex-tractinformationovermultipletime-scales.However,unlike[Szegedyetal.,2015],inIC-RNN,ﬁlterlengthsusedintheConv1Dlayersweredrawnfromalogarithmicinsteadofalinearscale,leadingtoexponentiallyvaryingﬁlterlenghts.Ourexperimentsdemonstratedthatforthedatasetconsideredinthispaper,exponentiallyvaryingﬁlterlengthsleadtobetterConv1D, 2, 32, /2 Conv1D, 4, 32, /2 Conv1D, 8, 32, /2 Filter ConcatConv1D, 2, 32, /2 Conv1D, 4, 32, /2 Conv1D, 8, 32, /2 Filter ConcatConv1D, 2, 32, /2 Conv1D, 4, 32, /2 Conv1D, 8, 32, /2 Filter ConcatGRU, 32 GRU, 32 GRU, 32 GRU, 32 SoftMaxInput(15000, 22)(7500, 32)(3750, 32)(1875, 32)(7500, 96)(3750, 96)(1875, 96)(1875, 32)(1875, 32)(1875, 32)(32)(2)Figure2:TheInceptionConvolutionalGatedRecurrentNeuralNet-work(IC-RNN)modiﬁestheC-RNN(seeFigure1b)architecturebyincludingmultipleﬁltersofexponentiallyvaryinglengthsinthe1Dconvolutionlayers.performance.Wespeculatethatthisisbecausecomparedtoimageswhererelevantfeaturesvaryinthesameorderofmag-nitude,intimeseriestherangeoftimescalesinwhichfeaturesexistismuchwider.Notethat,tothebestofourknowledge,inceptionmoduleswithexponentiallyvaryingﬁltersizesarereportedfortheﬁrsttimeinthispaper.Thespeciﬁccon-ﬁgurationusedinourexperimentsisshowninFigure2.AFilterConcatlayerconcatenatestheincomingfeaturesalongthedepthaxis.3.3ConvolutionalDenselyConnectedGatedRecurrentNeuralNetwork(C-DRNN)TheC-RNNarchitectureisnotimmunetotheproblemofdegradationwhichsometimesimpedesthetrainingofverydeepneuralnetworks[Heetal.,2015].ForsimplerproblemsthatdonotneedthefullpotentialofthemodelcomplexityofferedbyaC-RNN,theoptimizationproceduremayleadtohighertrainingerrors.Totacklethisissue,inspiredbytheDenseNetarchitectureproposedby[Huangetal.,2016]forCNNs,weincorpo-rateskipconnectionsinthestackedGRUlayersofC-RNNtoformtheC-DRNNarchitecture.EachGRUlayeriscon-nectedtoeveryotherGRUlayerinafeed-forwardfashion.Intuitively,skipconnectionswillleadtoGRUlayersbeingignoredwhenthedatademandsalowermodelcomplexitythanofferedbytheentirenetwork.ThedetailsofthenetworkareshowninFigure3.GRU, 32 GRU, 32 GRU, 32 GRU, 32 Filter ConcatFilter ConcatSoftMax(1875, 32)(32)(2)(1875, 32)(1875, 64)(1875, 32)(1875, 96)Conv1D, 4, 32, /2 Conv1D, 4, 32, /2 Conv1D, 4, 32, /2 Input(15000, 22)(7500, 32)(1875, 32)(3750, 32)Figure3:TheConvolutionalDenselyConnectedGatedRecurrentNeuralNetwork(C-DRNN)architectureincludesdenseconnectionsintherecurrentstagei.etheoutputofeachGRUlayerisgivenasinputtoeveryotherGRUlayersinafeed-forwardmanner.3.4ChronoNet:InceptionConvolutionalDenselyConnectedGatedRecurrentNeuralNetworkFinally,wecombinebothmodiﬁcationsintroducedfortheprevioustwonetworks(IC-RNNandC-DRNN)withC-RNNtoformtheChronoNetarchitecture.Tothebestofourknowl-edge,thisistheﬁrsttimethisarchitecturehasbeenreported.Tosummarize,ChronoNetiscreatedbystackingmultipleConv1DlayersfollowedbymultipleGRUlayerswhereeachConv1DlayerhasmultipleﬁltersofvaryingsizesandthestackedGRUlayersaredenselyconnectedinafeed-forwardmanner.ThepresenceofmultipleﬁltersintheConv1Dlayersal-lowsChronoNettoextractandcombinefeaturesfromdiffer-enttimescales.TheoptimumﬁltersizeforaConv1Dlayerusuallydependsonboththetaskathandanditsrelativeposi-tioninthenetwork.ChronoNethastheﬂexibilitytoexploremultipleﬁlterlengthsforeachConv1Dlayer.Ontheotherhand,denselyconnectedGRUlayersallowChronoNettomit-igatetheproblemofdegradationoftrainingaccuracycausedbyvanishingorexplodinggradients.ThispotentiallyenablesthecreationofverydeepvariantsofChronoNetformorecomplextasks.Moreover,denseconnectionsalsostrengthenfeaturepropagationandencouragefeaturereuseintheGRUlayers.ThenetworkwedesignedfortheabnormalEEGclas-siﬁcationtaskconsideredinthispaperisdepictedinFigure4.4ExperimentsInthissection,webrieﬂydescribethedataset,thedataaug-mentationtechniqueweused,andpresentanddiscusstheob-tainedresults.Conv1D, 2, 32, /2 Conv1D, 4, 32, /2 Conv1D, 8, 32, /2 Filter ConcatConv1D, 2, 32, /2 Conv1D, 4, 32, /2 Conv1D, 8, 32, /2 Filter ConcatConv1D, 2, 32, /2 Conv1D, 4, 32, /2 Conv1D, 8, 32, /2 Filter ConcatGRU, 32 GRU, 32 GRU, 32 GRU, 32 Filter ConcatFilter ConcatSoftMax(15000, 22)(7500, 32)(3750, 32)(1875, 32)(7500, 96)(3750, 96)(1875, 96)(1875, 32)(32)(2)Input(1875, 32)(1875, 64)(1875, 32)(1875, 96)Figure4:ProposedChronoNetarchitecturewhichincludesbothmultipleﬁltersofexponentiallyvaryinglengthsinthe1Dconvo-lutionlayersanddenseconnectionswithintheGRUlayers.4.1DataSelectionInthispaper,weprimarilyfocusedontheTUHAbnormalEEGCorpus[L´opez,2017],whichcontainsEEGrecordsthatareannotatedaseitherclinicallyabnormalornormal.TheTUHAbnormalEEGCorpusisasubsetoftheTUHEEGCorpus[ObeidandPicone,2016]whichistheworld’slargestpubliclyavailabledatabaseofclinicalEEGdata.TheTUHEEGCorpuscomprises23257EEGsessionsrecordedover13551patients.Intheentiredataset,almost75%ofthedatarepresentabnormalEEGsessions.TheTUHEEGAbnormalCorpuswasformedbyselectingademographicallybalancedsubsetoftheTUHEEGCorpusthroughmanualreviewthatconsistedof1488abnormaland1529normalEEGsessions,respectively.Thesesetswerefurtherpartitionedintoatrain-ingset(1361abnormal/1379normal),andatestset(127ab-normal/150normal).4.2DataPreparationTUHAbnormalEEGCorpusconsistsofEEGsessionsrecordedaccordingtothe10/20electrodeconﬁguration[Homan,1988].WeconvertedtherecordedEEGsignalintoasetofmontagesordifferentialsbasedonguidelinesproposedbytheAmericanClinicalNeurophysiologySociety[Acharyaetal.,2016].Inthispaper,weusedthetransversecentralparietal(TCP)montagesystemforaccentuatingspikeactiv-–AbnormalNormalTraining1497115169Test127150Table1:SizeoftrainingandtestsetusedinexperimentsofSection4.3.ity[L´opez,2017].Notethatwedidnotextractanyhand-engineeredfeaturesfromthedatasetbecauseweenvisionedthatthedeepRNNsusedinthispaperwillbeabletoauto-maticallyextractrelevantfeaturesandlearnmeaningfulrep-resentations.Intheoriginalstudy[L´opez,2017],theauthorsnotedthatneurologistscanaccuratelyclassifyanEEGsessionintoei-therabnormalornormalbyonlyexaminingtheinitialportionofthesignal.ThismotivatedtheauthorstobuildmachinelearningalgorithmsthatcanclassifyanEEGsignalbytak-ingonlytheﬁrstminuteofdataasinput.Hence,trainingandtestsetweregeneratedbyextractingtheﬁrstminutefromtheavailableEEGsessions.Notethatduringtestingonlytheﬁrstminutewasusedtoenableafaircomparisonoftheclassiﬁertohuman-levelperformance.Usingonlytheﬁrstminutetocreatethetrainingset,ontheotherhand,wasadesignchoicemotivatedbythefactthattheﬁrstminutemightbemostrep-resentativeofthetestset.Onceelectrodesareplacedonthescalpanddatarecordingstarts,impedancesandtherewiththesignalwillgraduallychangeduetoexternalfactorssuchasslowlydryingconductivepaste.However,usingtheabovemethodsigniﬁcantlylimitstheamountofdatathatcanbeusedfortraining.Thisresultsintwoproblems.First,deeplearningisadatahungrytech-niqueandperformancesubstantiallyincreasesasmoredataisincludedinthetrainingset.Second,whenappliedtosmalldatasets,RNNshaveatendencytoquicklyoverﬁt,aneffectthatintensiﬁesasnetworksbecomedeeper,asisthecaseforthoseconsideredinthispaper.Tonotlimitourselvesunnecessarily,weanalyzedtheeffectofincludingmorethanjusttheﬁrstminutefromthetrainingsessions.Thiswasdonebychoosingarandomsubsetofses-sionsthatwasfurtherdividedintosmallertrainingandvalida-tionsets.Separatemodelsweretrainedforeachminuteofthetrainingsets,allowingustoanalyzetheperformanceduringinferenceontheﬁrstminuteofthevalidationsessions.Theoutcomeofthisexperimentdemonstratedthatwecanuseupto11minutesofdatafromthetrainingEEGsessionswithoutperformancedegradation.Thisledtoa11-foldin-creaseinourtrainingdataascomparedtothemethodusedin[L´opez,2017].ThesizesoftheﬁnaltrainingandtestsetusedinourexperimentsareshowninTable1.Inthisdataset,mostrecordingsweredonewithasamplingfrequencyof250Hz.Wherethiswasnotthecase,sessionswereresampledto250Hz.Aninputvectortothenetworkwas1minutelong,thus,consistingof15000timepoints.4.3ResultsWeusedthedatasetdescribedabovetotrainthefourdeepre-currentneuralnetworkarchitecturespresentedinSection3.Networksweretrainedusingtheadaptivemomentestimationoptimization[KingmaandBa,2014]algorithmwithalearn-–TrainingaccuracyTestingaccuracyC-RNN83.58%82.27%IC-RNN86.93%84.09%C-DRNN87.20%83.89%ChronoNet90.60%86.59%CNN-MLPN/A78.8%Table2:PerformancecomparisonofthefourdeeprecurrentneuralnetworksdescribedinSection3andstate-of-the-artresultsreportedin[L´opez,2017](seeCNN-MLP).ingrateof0.001.Moreover,weusedabatchsizeof64andtrainedthenetworksfor500epochs.Table2listsmeanaccu-raciesof5repetitionsoftheseexperiments.Bestreportedre-sultstodateby[L´opez,2017]onthisdatasetareincludedforcomparison.In[L´opez,2017],theauthorexploredvariousmachine-anddeeplearningalgorithmsandobservedthatbestperformanceisobtainedwhenfrequencyfeaturesextractedfromtheinputtime-seriessignalarefedintoaconvolutionalneuralnetwork[L´opez,2017](CNN-MLPinTable2).Table2clearlydepictsthatthedeeprecurrentneuralarchi-tecturesexploredinthispaperoutperformtheresultsshownbyCNN-MLP[L´opez,2017].ItisimportanttonotethatincontrasttoCNN-MLP,theproposedarchitecturesdonotrelyonhand-craftedfeatures.Moreover,weseethatC-RNN,IC-RNN,C-DRNN,andChronoNetaresurpassingstate-of-the-artaccuracyby3.47%,5.29%,5.09%,and7.79%,respec-tively.Outofthefourrecurrentarchitectures,ChronoNetachievesboththebesttrainingandtestingaccuracy.ThisshowsthatthecombinedpositiveeffectofincludingmultipleﬁltersinConv1DlayersandincorporatingdenseconnectionsintheGRUlayersismorepronouncedthanusingeitheroneornoneofthem.Next,wecomparetheconfusionmatricesobtainedwhenusingChronoNet(Table3)andCNN-MLP(Table4)[L´opez,2017].ItisevidentthatChronoNetreducesbothtypesofer-ror.TheerrorofincorrectlyclassifyinganabnormalEEGsessionasanormalEEGsessionisreducedby8.06%,theerrorofincorrectlyclassifyinganormalEEGsessionasanabnormalEEGsessionby7.5%.Furthermore,ChronoNetincreasestheaccuracyofcorrectlyclassifyinganabnormalEEGsessionby8.06%andanormalEEGsessionby7.43%,demonstratinganoverallsuperiorityofChronoNetoverto[L´opez,2017].Inamedicalcontext,incorrectlyclassify-inganEEGsessionasabnormalEEGmayleadtosituationswherepatientsdonotreceiveadequatetreatment,whileiden-tifyingnormalEEGasabnormalmightleadtounnecessaryfollowupvisits,therebyincreasingclinicalworkload.TodemonstratethatexponentiallyvaryingﬁltersizesintheConv1DlayersofChronoNetareanecessarycomponent,twoexperimentswereperformed.First,shorter(comparedtothelongest1DconvolutionﬁlterusedinChronoNet)linearlyvaryingﬁltersoflengths3,5,and7wereimplemented.Asaresult,trainingandtestingaccuraciesfallto89.15%and85.10%,respectively.Second,longerbutlinearlyvaryingﬁl-tersoflengthsof14,16,and18wereimplemented.Whiletrainingaccuracyincreasedto91.25%,thetestingaccuracywasreducedto85.93%.Wespeculatethatinbothcasesfea-turesextractedbythenetworkarenotsufﬁcientlydiverse,andPPPPPPPRefHypAbnormalNormalAbnormal83.46%16.54%Normal10.67%89.33%Table3:ConfusionmatrixforChronoNet[Thiswork].PPPPPPPRefHypAbnormalNormalAbnormal75.4%24.6%Normal18.17%81.9%Table4:ConfusionmatrixforCNN-MLP[L´opez,2017].furthermore,inthelatercase,theincreasedmodelcomplexityleadstooverﬁtting.4.4BeyondEEGIdentiﬁcationChronoNetisdesignedtoefﬁcientlyﬁndpatternsondifferenttimescalesintemporaldatawithitsmainadvantageslyinginitsﬂexibilityandadaptability.Whileinthispaperwepri-marilyconcentratedonapplyingChronoNetforclassifyingabnormal/normalEEG,itcanalsobeappliedtothebroaderdomainoftime-seriesclassiﬁcation.Inapreliminarystudy,weareusingChronoNettosolveaspeechclassiﬁcationtask,usingarecentlyreleaseddatasetknownastheSpeechCom-mandsDataset[Warden,2016].Thedatasetconsistsofone-secondlongutterancesof30shortwords,spokenbythou-sandsofdifferentpeople.Thesizesofthetraining,valida-tion,andtestsetare64721,6798,and6835samplesrespec-tively.UsingtheexactsamearchitectureasshowninFigure4,weachievedatestingaccuracyof92.84%(averagedover5runs)forthis30classproblem.Whilethisscoredoesgiveusaverygoodﬁrstindicationthatitisindeedpossibletoap-plyChronoNettoothertime-seriesclassiﬁcationproblems,afaircomparisontothestate-of-the-artwillonlybepossibleaftertheconclusionofaKagglecompetitionthatiscurrentlyunderway[Warden,2017].Weintendtoundertakealargerstudyondifferenttime-domaindatasetsinthenearfuture.5ConclusionDeterminingwhetheranEEGrecordingshowsabnormalornormalbrainactivityisoftentheﬁrststepinthediagnosisofaneurologicalcondition.SincemanualinterpretationofEEGisanexpensiveandtime-consumingprocess,anyclas-siﬁerthatautomatesthisﬁrstdistinctionwillreducedelaysintreatmentandrelieveclinicalcaregivers.WeintroduceChronoNet,anovelnetworkarchitechturethatisdesignedtobeﬂexibleandadaptableand,thus,uniquelysuitedfortheanalysisofEEGtime-seriesdata.ThisnovelRNNarchitec-tureoutperformsthebestpreviouslyreportedaccuracyonthedatasetusedby7.79%,settinganewbenchmark.Todemon-strateitsgeneralapplicabilitytotime-seriesdata,wefurtherdemonstratethatChronoNetcansuccessfullyclassifyspeech.References[Acharyaetal.,2016]JayantN.Acharya,AbeerJ.Hani,ParthaD.Thirumala,andTammyN.Tsuchida.Ameri-canClinicalNeurophysiologySocietyGuideline3:APro-posalforStandardMontagestoBeUsedinClinicalEEG.JClinNeurophysiol,33(4):312–316,August2016.[Azumaetal.,2003]HidekiAzuma,ShiroHori,MasaoNakanishi,ShinjiFujimoto,NorimasaIchikawa,andToshiakiA.Furukawa.Aninterventiontoimprovethein-terraterreliabilityofclinicalEEGinterpretations.Psychi-atryandclinicalneurosciences,57(5):485–489,2003.[Bengioetal.,1994]Y.Bengio,P.Simard,andP.Frasconi.LearningLong-termDependencieswithGradientDescentisDifﬁcult.Trans.Neur.Netw.,5(2):157–166,March1994.[Choetal.,2014]KyunghyunCho,BartvanMerrienboer,DzmitryBahdanau,andYoshuaBengio.OntheProper-tiesofNeuralMachineTranslation:Encoder-DecoderAp-proaches.arXiv:1409.1259v2,September2014.[Chungetal.,2014]JunyoungChung,CaglarGulcehre,KyungHyunCho,andYoshuaBengio.EmpiricalEvalu-ationofGatedRecurrentNeuralNetworksonSequenceModeling.arXiv:1412.3555[cs],December2014.arXiv:1412.3555.[Golmohammadietal.,2017]MeysamGolmohammadi,SaeedehZiyabari,VinitShah,SilviaLopezdeDiego,IyadObeid,andJosephPicone.DeepArchitecturesforAutomatedSeizureDetectioninScalpEEGs.arXiv:1712.09776[cs,eess,q-bio,stat],December2017.arXiv:1712.09776.[Goodfellowetal.,2016]IanGoodfellow,YoshuaBengio,andAaronCourville.DeepLearning|TheMITPress,2016.[Graves,2013]AlexGraves.GeneratingSequencesWithRecurrentNeuralNetworks.arXiv:1308.0850[cs],Au-gust2013.arXiv:1308.0850.[Heetal.,2015]KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.DeepResidualLearningforImageRecognition.arXiv:1512.03385[cs],December2015.arXiv:1512.03385.[HochreiterandSchmidhuber,1997]SeppHochreiterandJ¨urgenSchmidhuber.LongShort-TermMemory.NeuralComputation,9(8):1735–1780,November1997.[Homan,1988]RichardW.Homan.The10-20electrodesystemandcerebrallocation.AmericanJournalofEEGTechnology,28(4):269–279,1988.[Huangetal.,2016]GaoHuang,ZhuangLiu,KilianQ.Weinberger,andLaurensvanderMaaten.DenselyCon-nectedConvolutionalNetworks.arXiv:1608.06993,Au-gust2016.[KingmaandBa,2014]DiederikP.KingmaandJimmyBa.Adam:AMethodforStochasticOptimization.arXiv:1412.6980[cs],December2014.arXiv:1412.6980.[L´opezetal.,2015]S.L´opez,G.Suarez,D.Jungreis,I.Obeid,andJ.Picone.AutomatedIdentiﬁcationofAb-normalAdultEEGs.IEEESignalProcessMedBiolSymp,2015,December2015.[L´opez,2017]SilviaL´opez.AutomatedInter-pretationofAbnormalAdultElectroencephalo-grams.MSThesis,TempleUniversity.Link:https://www.isip.piconepress.com/publications/ms_theses/2017/abnormal/,2017.[ObeidandPicone,2016]IyadObeidandJosephPicone.TheTempleUniversityHospitalEEGDataCorpus.FrontNeurosci,10,May2016.[Penningtonetal.,2014]JeffreyPennington,RichardSocher,andChristopherD.Manning.GloVe:GlobalVectorsforWordRepresentation.InEmpiricalMeth-odsinNaturalLanguageProcessing(EMNLP),pages1532–1543,2014.[Schirrmeisteretal.,2017]RobinTiborSchirrmeister,JostTobiasSpringenberg,LukasDominiqueJosefFiederer,MartinGlasstetter,KatharinaEggensperger,MichaelTangermann,FrankHutter,WolframBurgard,andTonioBall.DeeplearningwithconvolutionalneuralnetworksforEEGdecodingandvisualization.HumanBrainMapping,38(11):5391–5420,November2017.arXiv:1703.05051.[Smith,2005]S.J.M.Smith.EEGinthediagnosis,classiﬁ-cation,andmanagementofpatientswithepilepsy.JournalofNeurology,Neurosurgery&Psychiatry,76(suppl2):ii2–ii7,2005.[Szegedyetal.,2015]C.Szegedy,WeiLiu,YangqingJia,P.Sermanet,S.Reed,D.Anguelov,D.Erhan,V.Van-houcke,andA.Rabinovich.Goingdeeperwithconvolu-tions.In2015IEEEConferenceonComputerVisionandPatternRecognition(CVPR),pages1–9,June2015.[Tangetal.,2016]Y.Tang,Y.Huang,Z.Wu,H.Meng,M.Xu,andL.Cai.Questiondetectionfromacousticfea-turesusingrecurrentneuralnetworkwithgatedrecurrentunit.In2016IEEEInternationalConferenceonAcous-tics,SpeechandSignalProcessing(ICASSP),pages6125–6129,March2016.[Warden,2016]PeteWarden.ResearchBlog:Launch-ingtheSpeechCommandsDataset.Link:https://research.googleblog.com/2017/08/launching-speech-commands-dataset.html,August2016.[Warden,2017]PeteWarden.TensorFlowSpeechRecognitionChallenge|Kaggle.Link:https://www.kaggle.com/c/tensorflow-speech-recognition-challenge,2017.[Yinetal.,2017]WenpengYin,KatharinaKann,MoYu,andHinrichSch¨utze.ComparativeStudyofCNNandRNNforNaturalLanguageProcessing.arXiv:1702.01923[cs],February2017.arXiv:1702.01923.
