Algorithmic Linearly Constrained Gaussian Processes
Markus Lange-Hegermann 1
Abstract
We algorithmically construct multi-output Gaus-
sian process priors which satisfy linear differential
equations. Our approach attempts to parametrize
all solutions of the equations using Gr¨obner bases.
If successful, a push forward Gaussian process
along the paramerization is the desired prior. We
consider several examples, among them the full
inhomogeneous system of Maxwell’s equations.
By bringing together stochastic learning and com-
puteralgebra in a novel way, we combine noisy
observations with precise algebraic computations.
1. Introduction
In recent years, Gaussian process regression has become a
prime regression technique (Rasmussen & Williams, 2006).
Roughly, a Gaussian process can be viewed as a suitable2
probability distribution on a set of functions, which we can
condition on observations using Bayes’ rule. The resulting
mean function is used for regression. Additionally, one can
also easily sample random functions, so-called realizations,
from this distribution. The strength of Gaussian process
regression lies in avoiding overﬁtting while still ﬁnding
functions complex enough to describe any behavior present
in given observations, even in noisy or unstructured data.
Gaussian processes are usually applied when observations
are rare or expensive to produce. Applications range, among
many others, from robotics (Deisenroth et al., 2015), biology
(Honkela et al., 2015), global optimization (Osborne et al.,
2009), astrophysics (Garnett et al., 2015) to engineering
(Thewes et al., 2015).
Incorporating justiﬁed assumptions into the prior helps these
applications: the full information content of the scarce ob-
servations can be utilized to create a more precise regression
model. Examples of such assumptions are smooth or rough
behavior, trends, homogeneous or heterogeneous noise, lo-
cal or global behavior, and periodicity (cf. §4 in (Rasmussen
Lange-Hegermann
1Correspondence
<markus.lange.hegermann@rwth-aachen.de>.
to:
Markus
2Gaussian processes are the maximum entropy prior for ﬁnite
mean and variance in the unknown behavior (Jaynes, 1968; Jaynes
& Bretthorst, 2003).
& Williams, 2006),(Duvenaud, 2014)). Such assumptions
are usually incorporated in the covariance structure of the
Gaussian process.
Even certain physical laws, given by certain linear differ-
ential equations, could be incorporated into the covariance
structures of Gaussian process priors. Thereby, despite their
random nature, all realizations and the mean function of
the posterior strictly adhere to these physical laws3. For
example, (Macˆedo & Castro, 2008) constructed covariance
structures for divergence-free and curl-free vector ﬁelds,
which (Wahlstr¨om et al., 2013; Solin et al., 2015) used to
model electromagnetic phenomena.
A ﬁrst step towards systematizing this construction was
achieved in (Jidling et al., 2017).
In certain cases, a
parametrization of all solutions for physical laws could be
found by a computation that does not necessarily terminate.
Having found a parametrization, one could assume a Gaus-
sian process prior for the parametrizing functions and push
it forward. This results in a Gaussian process prior for the
solutions of the physical laws.
In Section 3, this paper combines these ideas from (Jidling
et al., 2017) with an algorithm which computes this
parametrization if it exists, or reports failure if it does not
exist. This algorithm is a homological result in algebraic
system theory (cf. §7.(25) in (Oberst, 1990)).
This paper adds information to Gaussian processes in two
ways:
(i) restricting to solutions of linear operator matrices by
constructing a suitable prior and
(ii) conditioning on observations using Bayes’ rule.
Since these two constructions are compatible, we can com-
bine strict, global information from equations with noisy,
local information from observations. The author views
this combination of techniques from homological algebra
and machine learning as the main result of this paper, and
3For notational simplicity, we refrain from using the phrases
“almost surely” and “up to equivalence” in this paper. We say that
all realizations of a Gaussian process adhere to such laws if there
is an element in its equivalence class w.r.t. almost sure equality
that has this property, and similarly for continuity and smoothness.
Algorithmic Linearly Constrained Gaussian Processes
the construction of covariance functions satisfying physical
laws as a proof of concept.
Example 4.2 shows a typical application. It constructs a
Gaussian process such that all of its realizations satisfy the
inhomogeneous Maxwell equations of electromagnetism.
Conditioning this Gaussian process on a single observa-
tion of electric current yields, as expected, a magnetic ﬁeld
circling around this electric current.
Even though Gaussian processes are a highly precise interpo-
lation tool, they lack in two regards: missing extrapolation
capabilities and high computation time, cubically in the
amount of observations. These problems have, to a cer-
tain degree, been addressed: several fast approximations to
Gaussian process regression (Titsias, 2009; Hensman et al.,
2013; Wilson et al., 2015a; Izmailov et al., 2017; Dong
et al., 2017) and more powerfull covariance structures (Lee
et al., 2017; Wilson & Adams, 2013; Wilson et al., 2015b;
Calandra et al., 2016) have been proposed. This paper ad-
dresses these two problems from a complementary angle.
The linear differential equations allow to extrapolate and
reduce the needed amount of observations, which improves
computation time.
2. Differential Equations and Gaussian
Processes
This section is mostly expository and summarizes Gaus-
sian processes and how differential operators act on them.
Subsection 2.1 summarizes Gaussian process regression.
We then introduce differential (Subsection 2.2) and other
operators (Subsection 2.4), and sketch their connection to
constructing priors (Subsection 2.3).
2.1. Gaussian processes
A Gaussian process g = GP(µ, k) is a distribution on the
set of functions Rd → R(cid:96) such that the function values
g(x1), . . . , g(xn) at x1, . . . , xn ∈ Rd have a joint Gaussian
distribution. It is speciﬁed by a mean function
µ : Rd → R(cid:96) : x (cid:55)→ E(g(x))
and a positive semideﬁnite covariance function
k : Rd ⊕ Rd → R(cid:96)×(cid:96)(cid:23)0 :
(x, x(cid:48)) (cid:55)→ E((g(x) − µ(x))(g(x(cid:48)) − µ(x(cid:48)))T ) .
Assume the regression model yi = g(xi) and condition on
n observations
{(xi, yi) ∈ R1×d ⊕ R1×(cid:96) | i = 1, . . . , n} .
Denote by k(x, X) ∈ R(cid:96)×(cid:96)n resp. k(X, X) ∈ R(cid:96)n×(cid:96)n(cid:23)0
the
(covariance) matrices obtained by concatenating the matri-
ces k(x, xj) resp. the positive semideﬁnite block partitioned
matrix with blocks k(xi, xj). Write µ(X) ∈ R(cid:96)×n for the
matrix obtained by concatenating the vectors µ(xi) and
y ∈ R1×(cid:96)n for the row vector obtained by concatenating the
rows yi. The posterior
GP(cid:0) x (cid:55)→ µ(X) + (y − µ(X))k(X, X)−1k(x, X)T ,
(x, x(cid:48)) (cid:55)→ k(x, x(cid:48)) − k(x, X)k(X, X)−1k(x(cid:48), X)T(cid:1) ,
is a Gaussian process and its mean function x (cid:55)→ µ(X) +
(y − µ(X))k(X, X)−1k(x, X)T is used as regression
model.
2.2. Differential equations
From now on, let R = R[∂x1, . . . , ∂xd ] be the polynomial
ring in the partial differential operators. This ring models lin-
ear (partial) differential equations with constant coefﬁcients,
as it acts on the vector space F = C∞(Rd, R) of smooth
functions, where ∂xi acts by partial derivative w.r.t. xi. The
set of realizations of a Gaussian process with squared ex-
ponential covariance function is dense in F (cf. Thm. 12,
Prop. 42 in (Simon-Gabriel & Sch¨olkopf, 2016)).
Roughly speaking, Gaussian processes are the linear objects
among stochastic processes. Hence, it is not surprising
to ﬁnd a rich interplay of Gaussian processes and linear
differential equations.
The class of Gaussian processes is closed under matrices
B ∈ R(cid:96)×(cid:96)(cid:48)(cid:48)
of linear differential operators with constant
coefﬁcients. Let g = GP(µ, k) be a Gaussian process with
realizations in the space F (cid:96)(cid:48)(cid:48)
of vectors with functions in F
as entries. Deﬁne the Gaussian process B∗g as the Gaussian
process induces by the pushforward measure under B of the
Gaussian measure induced by g. It holds that
B∗g = GP(Bµ(x), Bk(x, x(cid:48))(B(cid:48))T ) ,
(1)
where B(cid:48) denotes the operation of the operators in B on
functions with argument x(cid:48) ∈ Rd (cf. Thm. 9 in (Bertinet &
Agnan, 2004)).
The covariance matrices for such Gaussian processes as in
(1) are often singular. This is to be expected, as B∗g is rarely
dense in F (cid:96). For numerical stability, we tacitely assume the
model yi = g(xi) + ε for small Gaussian white noise term ε
and adopt k by adding var(ε) to k(xi, xi) for observations
xi.
Example 2.1. Let g = GP(0, k(x, x(cid:48))) be a scalar univari-
ate Gaussian process with differentiable realizations. Then,
the Gaussian process of derivatives of functions is given by
(cid:2) ∂
(cid:19)
∂x∂x(cid:48) k(x, x(cid:48))
One can interpret this Gaussian process(cid:2) ∂
(cid:3)
(cid:3)
∗ g = GP
∗ g as taking
derivatives as measurement data and producing a regression
model of derivatives.
∂x
∂x
(cid:18)
∂2
0,
2.3. Parametrizations
2.4. Further operator rings
Algorithmic Linearly Constrained Gaussian Processes
We say that a Gaussian process is in a function space, if
its realizations are contained in said space. For A ∈ R(cid:96)(cid:48)×(cid:96)
deﬁne the solution set
solF (A) := {f ∈ F (cid:96) | Af = 0} .
Such solutions sets and Gaussian processes are connected.
Lemma 2.2. Let g = GP(µ, k) be a Gaussian process in
F (cid:96)×1. Then g is also a Gaussian process in solF (A) for
A ∈ R(cid:96)(cid:48)×(cid:96) if and only if µ ∈ solF (A) and A∗(g − µ) is the
constant zero process.
Proof. Assume that g is a Gaussian process in solF (A).
Then, as the mean function is a realization, µ ∈ solF (A).
Furthermore, for ˜g := (g − µ) = GP(0, k) we have that
all realizations are annihilated by A, and hence A∗˜g is the
constant zero process.
Conversely, assume that µ ∈ solF (A) and A∗(g − µ) is
the constant zero process. This implies 0 = A∗(g − µ) =
A∗g − A∗µ = A∗g, i.e. all realizations of g become zero
after a pushforward by A. In particular, all realizations of g
are contained in solF (A).
Our goal is to construct Gaussian processes with realizations
dense in the solution set solF (A) of an operator matrix
A ∈ R(cid:96)(cid:48)×(cid:96). The following remark, implicit in (Jidling et al.,
2017), is a ﬁrst step towards an answer.
Remark 2.3. Let A ∈ R(cid:96)(cid:48)×(cid:96) and B ∈ R(cid:96)×(cid:96)(cid:48)(cid:48)
Let g = GP(0, k) be a Gaussian process in F (cid:96)(cid:48)(cid:48)
set of realizations of B∗g is contained in solF (A).
with AB = 0.
. Then, the
This follows easily from Lemma 2.2, as A∗(B∗g) =
(AB)∗g = 0∗g = 0.
We call B ∈ R(cid:96)×(cid:96)(cid:48)(cid:48)
a parametrization of solF (A) if
solF (A) = BF (cid:96)(cid:48)(cid:48)
. Parametrizations yield the denseness
of the realizations of a Gaussian process B∗g in solF (A).
Proposition 2.4. Let B ∈ R(cid:96)×(cid:96)(cid:48)(cid:48)
be a parametrization of
solF (A) for A ∈ R(cid:96)(cid:48)×(cid:96). Let g = GP(0, k) be a Gaussian
process dense in F (cid:96)(cid:48)(cid:48)
. Then, the set of realizations of B∗g
is dense in solF (A).
This proposition is a consequence of partial derivatives be-
ing bounded, and hence continuous, when F is equipped
with the Fr´echet topology generated by the family of semi-
norms
(cid:107)f(cid:107)a,b := sup
i∈Zd≥0
|i|≤a
sup
z∈[−b,b]d
| ∂
∂zi f (z)|
for a, b ∈ Z≥0 (cf. §10 in (Treves, 1967)). Now, the con-
tinuous surjective map induced by B maps a dense set to a
dense set.
The theory presented for differential equations with constant
coefﬁcients also holds for other rings R of linear operators
and solution spaces F. The following three operator rings
are prominent examples.
The polynomial ring R = R[x1, . . . , xd] models polynomial
equations when it acts on the set F of smooth functions
deﬁned on a (Zariski-)open set in Rd.
To model ordinary linear differential equations with ratio-
nal4 coefﬁcients consider the Weyl algebra R = R(t)(cid:104)∂t(cid:105),
with the non-commutative relation ∂tt = t∂t + 1 represent-
ing the product rule of differentiation. Here, we consider
solutions in the set F of smooth functions deﬁned on a
co-ﬁnite set.
The polynomial ring R = R[σx1, . . . , σxd ] models linear
shift equations with constant coefﬁcients when it acts on the
set F = RZd≥0 of d-dimensional sequences by translation of
the arguments.
3. Computing parametrizations
By the last section, constructing a parametrization B of
solF (A) yields a Gaussian process dense in the solution set
solF (A) of an operator matrix A ∈ R(cid:96)(cid:48)×(cid:96). Subsection 3.1
gives necessary and sufﬁcient conditions for a parametriza-
tion to exist and Subsection 3.2 describes their computation.
It turns out that these computations are purely algebraic over
R.
3.1. Existence of parametrizations
It turns out that we can decide whether a parametrization
exists purely algebraically, with operations over R that do
not involve F.
By r-ker(A) we denote the right kernel of A ∈ R(cid:96)(cid:48)×(cid:96), i.e.
r-ker(A) = {m ∈ R(cid:96)×1 | Am = 0}. By l-ker(A) we
denote the left kernel of A, i.e. l-ker(A) = {m ∈ R1×(cid:96)(cid:48) |
mA = 0}. Abusing notation, denote any matrix as left resp.
right kernel if its rows resp. columns generate the kernel as
an R-module.
Theorem 3.1. Let A ∈ R(cid:96)(cid:48)×(cid:96). Deﬁne matrices B =
r-ker(A) and A(cid:48) = l-ker(B). Then solF (A(cid:48)) is the
largest subset of solF (A) that is parametrizable and B
parametrizes solF (A(cid:48)).
In constrast to vector spaces, the left kernel of the right
kernel of A is not necessarily A (up to an equivalence)
in case of operator rings. For example, the solution set
solF (A(cid:48)) is the subset of controllable behaviors in solF (A).
4No major changes for polynomial, holonomic, or meromor-
phic coefﬁcients.
Algorithmic Linearly Constrained Gaussian Processes
Corollary 3.2. In Theorem 3.1, solF (A) is parametrizable
if and only if the rows of A and A(cid:48) generate the same row-
module. Since AB = 0, this is the case if all rows of A(cid:48) are
contained in the row module generated by the rows of A.
In this case, solF (A) is parametrized by B. Furthermore, a
Gaussian process g with realizations dense in F (cid:96)(cid:48)(cid:48)
leads to
a Gaussian process B∗g with realizations dense in solF (A).
For a proof of this theorem and its corollary see Thm. 2 in
(Zerz et al., 2010), (cf. also Thm. 3, Alg. 1, and Lemma 1.2.3
in (Zerz, 2000) or §7.(24) in (Oberst, 1990)) and for addi-
tional characterizations, generalizations, and proofs using
more homological machinery see (Quadrat, 2013; 2010b;
Barakat, 2010; Seiler & Zerz, 2010; Chyzak et al., 2005;
Robertz, 2015) and references therein.
The approach assigns a prior to the parametrising functions
and pushes this prior forward to a prior of the solution set
solF (A). The paramerization is not canonical, and hence
different parametrizations might lead to different priors.
This did not lead to practical problems, yet.
3.2. Algorithms
Summarizing Theorem 3.1 and Corollary 3.2 algorithmi-
cally, we need to compute right kernels (of A), compute
left kernels (of B), and decide whether rows (of A(cid:48)) are
contained in a row module (generated by the rows of A).
All these computations are an application of Gr¨obner basis
algorithms.
In the recent decades, Gr¨obner bases algorithms have be-
come one of the core algorithms of computeralgebra, with
manifold applications among others in in geometry, system
theory, natural sciences, and automatic theorem proving.
Generalizing the Gaussian algorithm, reduced Gr¨obner
bases generalize the reduced echolon form to systems of
linear operator equations. In particular, using them, one can
compute all solutions in R (not in F) of the homogeneous
system and compute, if it exists, a particular solution in R
(not in F) for an inhomogeneous system. Solving homo-
geneous systems is the same as computing its right resp.
left kernel or ﬁnding all relations (the generalization of lin-
ear dependencies) between columns resp. rows of a matrix.
Solving inhomogeneous equations decides whether an ele-
ment is contained in a module. Alternatively, the uniqueness
of reduced Gr¨obner bases also decides submodule equality.
A formal description of Gr¨obner bases would exceed the
scope of this note. Instead, we refer to the excellent litera-
ture (Sturmfels, 2005; Eisenbud, 1995; Adams & Loustau-
nau, 1994; Greuel & Pﬁster, 2002; Gerdt, 2005; Buchberger,
2006) and explicitely show in the next section how to use
Gr¨obner bases in computer algebra systems. They are imple-
mented in various computer algebra systems, e.g., Singular
(Decker et al., 2016) and Macaulay2 (Grayson & Stillman)
are two well-known open source implementations.
3.3. Hyperparameters
Many covariance functions5 incorporate hyperparameters
and advanced methods speciﬁcally add more hyperparam-
eters to Gaussian processes, see e.g. (Snelson et al., 2003;
Calandra et al., 2016; Wilson & Adams, 2013), for addi-
tional ﬂexibility.
The approach in this paper is the opposite. We restrict the
Gaussian process prior, speciﬁcally to solutions of an oper-
ator matrix, guarding against overﬁtting. The prior of the
parametrizing functions can, and usually does, still contain
hyperparameters. These can be determined by maximizing
the likelihood, as usual.
Many important applications contain unknown parameters
in the equations. Such parameters can also be estimated by
the likelihood, when conditioning on data.
For ordinary differential equations, the solution set of an
operator matrix is a direct sum of parametrizable functions
and a ﬁnite dimensional set of functions, both with con-
stant resp. variable coefﬁcients, due to the Smith form resp.
Jacobson form. In many cases, in particular the case of con-
stant coefﬁcients, the solution set of the ﬁnite dimensional
summand can easily be computed. This paper also allows to
compute with the parametrizable summand of the solution
set and estimate parameters and hyperparameters of both
summands together.
4. Examples
Example 4.1. We reproduce the well-known fact that
divergence-free ﬁelds can be parametrized by the curl oper-
ator. This has been used in connection with Gaussian pro-
cesses to model electric and magnetic phenomena (Macˆedo
& Castro, 2008; Wahlstr¨om et al., 2013; Solin et al., 2015).
The same algebraic computation also constructs a prior for
tangent ﬁelds of a sphere.
The computer algebra system Macaulay2 (Grayson & Still-
man) performs the Gr¨obner basis computations.
Let R be the polynomial ring in three indeterminates, which
we can both interpret as the polynomial ring Q[∂1, ∂2, ∂3]
in the differential operators resp. the polynomial ring
Q[x1, x2, x3] in the indeterminates.
i1 :
o1 = R
o1 :
PolynomialRing
R=QQ[d1,d2,d3]
5Sometimes even the mean function contains hyperparameters.
These additional hyperparameters are usually not very expressive,
compared to the non-parametric Gaussian process model.
Algorithmic Linearly Constrained Gaussian Processes
and consider the matrix A = (cid:2)∂1 ∂2 ∂3
(cid:3) representing
the divergence resp. the normals of circles centered around
the origin.
i2 :
o2 = | d1 d2 d3 |
A=matrix{{d1,d2,d3}}
o2 :
Matrix R <--- R
The right kernel of A is given by the operator B representing
the curl resp. tangent spaces of circles centered around the
origin.
i3 :
o3 = 1 | -d2 0
B = generators kernel A
1 | d1 -d3 0
1 | 0
-d3 |
d2 d1 |
Matrix R <--- R
o3 :
Since the right kernel A(cid:48) of B is again the A, the matrix B
is really parametrization matrix of the solutions of A.
i4 :
transpose B
o4 = | d1 d2 d3 |
A1 = transpose generators kernel
Matrix R <--- R
o4 :
(As kernel in Macaulay2 yields right kernels, we
compute the left kernel by transposition.)
We construct a prior for tangent ﬁelds on the sphere by
assuming equal covariance functions k for 3 uncorrelated
parametrizing functions. A mean ﬁeld is demonstrated in
Figure 1 and the covariance function for the tangent ﬁeld is
k(x1, y1, z1, x2, y2, z2)·
−y1y2 − z1z2
x1y2
x1z2
y1x2
−x1x2 − z1z2
y1z2
z1x2
z1y2
−x1x2 − y1y2
 .
Example 4.2. Maxwell’s equations of electromagnetism
uses curl and divergence operators as building blocks. It is a
well-known result that the solutions of the inhomogeneous
Maxwell equations are parametrized by the electric and mag-
netic potentials. We verify this and use the parametrization
to construct a Gaussian process, such that its realizations
adhere to Maxwell’s equations. We condition this prior on a
single observation of ﬂowing electric current, which leads
to the magnetic ﬁeld circling around the current. All compu-
tations have been performed in Maple using the Involutive
package (Blinkov et al., 2003).
The homogenous Mawell equations are given by the opera-
Figure 1. Taking the squared exponential covariance function for
k in Example 4.1 yields the above smooth mean tangent ﬁeld
on the sphere after conditioning at 4 evenly distributed points on
the equator with two opposite tangent vectors pointing north and
south each. The two visible of these four vectors are displayed
signiﬁcantly bigger.
tor matrix
Mh :=

∂z
−∂y
−∂t
∂x
−∂z
∂x
−∂t
∂y
∂y
∂t
−∂x
∂x
∂z
−∂t −∂y
∂z
∂t
∂y
−∂z
∂x

∂t
∂z
∂y
−∂x
applied to three components of the electric ﬁeld and three
components of the magnetic (pseudo) ﬁeld. We have set all
constants to 1. The right kernel of Mh is zero, in particular
no parametrization exists.
The inhomogeneous Maxwell equations with three addi-
tional components of the electric current and one additional
component of electric ﬂux are given by the 8 × 10 operator
. Using Gr¨obner bases, one
matrix Mi :=
computes the right kernel
04×4−I4
(cid:20)
(cid:21)
Mh


∂x
∂y
∂z
−∂t∂x
−∂t∂y
−∂t∂z
∂2
x + ∂2
y + ∂2
∂t
−∂z
∂y
z − ∂2
∂2
y + ∂2
−∂y ∂x
−∂z ∂x
∂t∂x
∂t
∂z
−∂x
−∂y ∂x
−∂z ∂y
∂t∂y
∂2
x + ∂2
z − ∂2
∂t
−∂y
∂x
−∂z ∂x
−∂z ∂y
y − ∂2
∂2
x + ∂2
∂t∂z
of Mi and veriﬁes that it is a parametrization of the set of
Algorithmic Linearly Constrained Gaussian Processes
> T := E[3];
(cid:20)0
(cid:2)1
(cid:21)
t3 Dt
0(cid:3)
solutions of the inhomogeneous Maxwell equations.
We assume squared exponential covariance functions
k := exp(cid:0) − 1
(cid:0)(x1 − x2)2 + (y1 − y2)2
+(z1 − z2)2 + (t1 − t2)2(cid:1)(cid:1)
and a zero mean function for four uncorrelated parametris-
ing functions (electric and magnetic potential). The result-
ing covariance matrix can be found in Appendix A and for
a demonstration see Figure 2.
Figure 2. We condition the prior in Example 4.2 on an electric
current in z-direction and zero electric ﬂux at the origin x = y =
z = t = 0. The diagram shows the mean posterior magnetic
ﬁeld in the (z, t) = (0, 0)-plane. As expected, it circles around
the point with electric current. This mean ﬁeld has closed form
(cid:0)x2 + y2 − 4(cid:1) exp(cid:0)− 1
(cid:0)x2 + y2(cid:1)(cid:1) ·
(cid:20)−y
(cid:21)
10
Example 4.3. We consider the one-dimensional Weyl
algebra R = R(t)(cid:104)∂t(cid:105). It allows a stronger notion of a basis
than a Gr¨obner basis: the Jacobson form (Jacobson, 1943).
It is similar to the Smith normal form for PID’s, in that
multiplying with invertible matrices on both sides yields
a diagonal matrix. We use the Maple packages Janet and
OreModules (Blinkov et al., 2003; Chyzak et al., 2007).
> with( Janet ): with( OreModules ):
> Alg := DefineOreAlgebra( diff=[ Dt, t
], polynom=[ t ] ):
consider
system
We
dt x(t) = t3u(t) from Example 1.5.7 in (Quadrat,
2010a). The Jacobson form is just(cid:2)1 0(cid:3).
time-varying
control
the
> E := ElementaryDivisors( [
diff(x(t),t)-tˆ3*u(t) ], [ t ], [ x,
u ] ):
> T1 := E[1];
(cid:2)− 1
t3
(cid:3)
> Mult( T1, [[ Dt, -tˆ3 ]], T, Alg );
In particular, after the base change with the matrix T , the
system is free on one generator and parametrizable by the
matrix
(cid:20)0
(cid:21)
The original system is thus parametrizable by
(cid:20)0
(cid:21)
(cid:20) 1
(cid:21)
t3 ∂t
B = T ·
For a parametrizing functions with squared exponential co-
2 (t1 − t2)2) and a
variance functions k(t1, t2) = exp(− 1
(cid:19)
(cid:18)
zero mean function, the covariance function for (x, u) is
(cid:34) 1
t1−t2
(cid:35)
t2−t1
t3
(t2−t1−1)(t1−t2−1)
exp
t3
t3
2t3
(t1 − t2)2
− 1
There are no exceptional points in the domain of the trans-
formed system, but T removes {0} from the domain of the
original system. We restrict ourselves to the set of smooth
functions C∞(R>0, R) deﬁned on positive real numbers.
For a demonstration of these priors see Figures 3 and 4.
x(t)
u(t)
Figure 3. The state function x(t) of the system in Example 4.3 can
be inﬂuenced by assigning an input function u(t). For example
10 , . . . , 5}
setting x(1) = 0 and u(t) = 1
leads to the above posterior mean. This model yields x(5) ≈
t4+1 for t ∈ {1, 11
10 , 12
1.436537, close to(cid:82) 5
t3
t4+1 dt ≈ 1.436551.
A. Covariance matrices
The covariance matrix for the 3-dimensional electric ﬁeld
E, the 3-dimensional magnetic (pseudo) ﬁeld B, the 3-
dimensional electric current C and 1-dimensional electric
ﬂux F in Maxwell’s equations as constructed in Example 4.2
is
KE,E KE,B KE,C KE,F
KB,E KB,B KB,C KB,F
KC,E KC,B KC,C KC,F
KF,E KF,B KF,C KF,F

K := 4k ·
Algorithmic Linearly Constrained Gaussian Processes
x(t)
u(t)
-1
-2
Figure 4. We prescribe a desired behavior for the state x(t) in
Example 4.3 and let the Gaussian process construct a suitable input
u(t). Starting with x(1) = 1 we give u(t) one time step to control
x(t) to zero, e.g., by setting x(t) = 0 for t ∈ { 20
10 , . . . , 5}.
10 , 21
for matrices
KE,E = (2 − (t1 − t2)2)I3 − vT v
KB,B = (2 − vvT )I3 − vT v
KC,C = (4 + 3(t1 − t2)2 − (vvT ))vT v
+((vvT − (t1 − t2)2)2 − 3(t1 − t2)2 − 7vvT + 10)I3
KF,F = (t1 − t2)2(vvT − 3) + (vvT )2 − 11vvT + 18
KB,E = (t1 − t2)(v ∧ v)
KC,E = (t1 − t2)((2 + vvT − (t1 − t2)2)I3 − 2vT v)
KF,E = ((t1 − t2)2 + vvT − 6)v
KC,B = (vvT − (t1 − t2)2 − 4)(v ∧ v)
KF,B =(cid:2)0 0 0(cid:3)

using
for
KF,C = −(t1 − t2)(vvT + (t1 − t2)2 − 8)v
the 3 × 3 identity matrix,
z1 − z2
−(y1 − y2)
:=
−(z1 − z2)
x1 − x2
y2 − y2
y1 − y2
−(x1 − x2)
.
(cid:2)x1 − x2
(cid:3),
z1 − z2
and v ∧ v
I3
:=
Acknowledgments
The authors thanks M. Barakat, S. Gutsche, C. Kaus, D.
Moser, S. Posur, and O. Wittich for discussions concern-
ing this paper, W. Plesken, A. Quadrat, D. Robertz, and E.
Zerz for introducing him to the algebraic background of
this paper, S. Thewes for introducing him to the stochastic
background of this paper, and the authors of (Jidling et al.,
2017) for providing the starting point of this work.
References
Adams, William W. and Loustaunau, Philippe. An introduc-
tion to Gr¨obner bases. Graduate Studies in Mathematics.
American Mathematical Society, 1994.
Barakat, Mohamed. Purity ﬁltration and the ﬁne structure
of autonomy. In Proceedings of the 19th International
Symposium on Mathematical Theory of Networks and Sys-
tems - MTNS 2010, pp. 1657–1661, Budapest, Hungary,
2010.
Bertinet, A. and Agnan, Thomas C. Reproducing Kernel
Hilbert Spaces in Probability and Statistics. Kluwer
Academic Publishers, 2004.
Blinkov, Yuri A., Cid, Carlos F., Gerdt, Vladimir P., Plesken,
Wilhelm, and Robertz, Daniel. The MAPLE Package
JANET: I. Polynomial Systems. II. Linear Partial Dif-
In Proceedings 6th International
ferential Equations.
Workshop on Computer Algebra in Scientiﬁc Computing,
pp. 31–40 and 41–54, 2003. (http://www.mathb.
rwth-aachen.de/Janet).
Buchberger, Bruno. An algorithm for ﬁnding the basis
elements of the residue class ring of a zero dimensional
polynomial ideal. J. Symbolic Comput., 41(3-4):475–
511, 2006. Translated from the 1965 German original by
Michael P. Abramson.
Calandra, Roberto, Peters, Jan, Rasmussen, Carl E., and
Deisenroth, Marc P. Manifold Gaussian processes for
regression. In International Joint Conference on Neural
Networks, pp. 3338–3345, 2016. doi: 10.1109/IJCNN.
2016.7727626.
Chyzak, Fr´ed´eric, Quadrat, Alban, and Robertz, Daniel.
Effective algorithms for parametrizing linear control sys-
tems over Ore algebras. Appl. Algebra Engrg. Comm.
Comput., 16(5):319–376, 2005.
Chyzak, Fr´ed´eric, Quadrat, Alban, and Robertz, Daniel.
OreModules: a symbolic package for the study of mul-
In Applications of time
tidimensional linear systems.
delay systems, volume 352 of Lecture Notes in Con-
trol and Inform. Sci., pp. 233–264. Springer, Berlin,
2007.
(http://www.mathb.rwth-aachen.de/
OreModules).
Decker, Wolfram, Greuel, Gert-Martin, Pﬁster, Gerhard, and
Sch¨onemann, Hans. SINGULAR 4-1-0 — A computer
algebra system for polynomial computations. http:
//www.singular.uni-kl.de, 2016.
Deisenroth, Marc Peter, Fox, Dieter, and Rasmussen,
Carl Edward. Gaussian processes for data-efﬁcient learn-
ing in robotics and control. IEEE Trans. Pattern Anal.
Mach. Intell., 37(2):408–423, 2015.
Dong, Kun, Eriksson, David, Nickisch, Hannes, Bindel,
David, and Wilson, Andrew Gordon. Scalable log de-
terminants for gaussian process kernel learning. 2017.
(arXiv:1711.03481).
Duvenaud, David. Automatic Model Construction with
Gaussian Processes. PhD thesis, University of Cam-
bridge, 2014.
Algorithmic Linearly Constrained Gaussian Processes
Eisenbud, David. Commutative Algebra with a View Toward
Algebraic Geometry, volume 150 of Graduate Texts in
Mathematics. Springer-Verlag, 1995.
Garnett, Roman, Ho, Shirley, and Schneider, Jeff G. Find-
ing galaxies in the shadows of quasars with Gaussian
processes. In Bach, Francis R. and Blei, David M. (eds.),
ICML, volume 37 of JMLR Workshop and Conference
Proceedings, pp. 1025–1033. JMLR.org, 2015.
Gerdt, Vladimir P.
Involutive algorithms for computing
Gr¨obner bases. In Computational commutative and non-
commutative algebraic geometry, volume 196 of NATO
Sci. Ser. III Comput. Syst. Sci., pp. 199–225. 2005.
Grayson, Daniel R. and Stillman, Michael E. Macaulay2,
a software system for research in algebraic geometry.
http://www.math.uiuc.edu/Macaulay2/.
Greuel, G. and Pﬁster, G. A Singular introduction to com-
mutative algebra. Springer-Verlag, 2002. With contri-
butions by Olaf Bachmann, Christoph Lossen and Hans
Sch¨onemann.
Hensman, James, Fusi, Nicol´o, and Lawrence, Neil D. Gaus-
sian processes for big data. In Proceedings of the Twenty-
Ninth Conference on Uncertainty in Artiﬁcial Intelligence,
2013.
Honkela, Antti, Peltonen, Jaakko, Topa, Hande, Chara-
pitsa, Iryna, Matarese, Filomena, Grote, Korbinian, Stun-
nenberg, Hendrik G., Reid, George, Lawrence, Neil
D., and Rattray, Magnus. Genome-wide modeling of
transcription kinetics reveals patterns of rna production
delays. Proceedings of the National Academy of Sci-
ences, 112(42):13115–13120, 2015. doi: 10.1073/pnas.
1420404112.
Lee, Jaehoon, Bahri, Yasaman, Novak, Roman, Schoen-
holz, Samuel S., Pennington, Jeffrey, and Sohl-Dickstein,
Jascha. Deep neural networks as Gaussian processes,
2017. (arXiv:1711.00165).
Macˆedo, Ives and Castro, Rener. Learning divergence-free
and curl-free vector ﬁelds with matrix-valued kernels.
Instituto Nacional de Matematica Pura e Aplicada, Brasil,
Tech. Rep, 2008.
Oberst, Ulrich. Multidimensional constant linear systems.
Acta Appl. Math., 20(1-2):1–175, 1990.
Osborne, Michael A., Garnett, Roman, and Roberts,
Stephen J. Gaussian processes for global optimization. In
3rd international conference on learning and intelligent
optimization (LION3), pp. 1–15, 2009.
Quadrat, Alban. An introduction to constructive al-
In Journ´ees
gebraic analysis and its applications.
Nationales de Calcul Formel, volume 1 of Les cours
du CIRM, pp. 279–469. CIRM, Luminy, 2010a.
(http://ccirm.cedram.org/ccirm-bin/
fitem?id=CCIRM_2010__1_2_281_0).
Quadrat, Alban. Syst`emes et Structures – Une approche
de la th´eorie math´ematique des syst`emes par l’analyse
alg´ebrique constructive. April 2010b. Habilitation thesis.
Quadrat, Alban. Grade ﬁltration of linear functional
systems. Acta Appl. Math., 127:27–86, 2013. doi:
10.1007/s10440-012-9791-2.
Rasmussen, Carl Edward and Williams, Christopher K. I.
Gaussian Processes for Machine Learning (Adaptive
Computation and Machine Learning). The MIT Press,
2006.
Izmailov, Pavel, Novikov, Alexander, and Kropotov,
Dmitry. Scalable Gaussian processes with billions of
inducing inputs via tensor train decomposition, 2017.
(arXiv:math/1710.07324).
Robertz, Daniel. Recent progress in an algebraic anal-
ysis approach to linear systems. Multidimensional
Syst. Signal Process., 26(2):349–388, April 2015. doi:
10.1007/s11045-014-0280-9.
Jacobson, N. The Theory of Rings. Mathematical surveys
and monographs. American Mathematical Society, 1943.
Jaynes, Edwin T. Prior probabilities. IEEE Transactions on
systems science and cybernetics, 4(3):227–241, 1968.
Jaynes, Edwin T. and Bretthorst, G. Larry. Probability
Theory: The Logic of Science. Cambridge University
Press, 2003.
Jidling, Carl, Wahlstr¨om, Niklas, Wills, Adrian, and Sch¨on,
Thomas B. Linearly constrained Gaussian processes.
2017. (arXiv:1703.00787).
Seiler, Werner M. and Zerz, Eva. The inverse syzygy prob-
lem in algebraic systems theory. PAMM, 10(1):633–634,
2010.
Simon-Gabriel, C.-J. and Sch¨olkopf, B. Kernel distribution
embeddings: Universal kernels, characteristic kernels and
kernel metrics on distributions. Technical report, 2016.
(arXiv:1604.05251).
Snelson, Edward, Rasmussen, Carl Edward, and Ghahra-
mani, Zoubin. Warped gaussian processes. In Thrun,
Sebastian, Saul, Lawrence K., and Schlkopf, Bernhard
(eds.), NIPS, pp. 337–344. MIT Press, 2003.
Algorithmic Linearly Constrained Gaussian Processes
Solin, Arno, Kok, Manon, Wahlstr¨om, Niklas, Sch¨on,
Thomas B., and S¨arkk¨a, Simo. Modeling and interpola-
tion of the ambient magnetic ﬁeld by Gaussian processes.
2015. (arXiv:1509.04634).
Sturmfels, Bernd. What is... a Gr¨obner basis? Notices of
the AMS, 52(10):2–3, 2005.
Thewes, Silja, Lange-Hegermann, Markus, Reuber,
Christoph, and Beck, Ralf. Advanced Gaussian Process
Modeling Techniques. In Design of Experiments (DoE)
in Powertrain Development. Expert, 2015.
Titsias, Michalis K. Variational learning of inducing vari-
ables in sparse Gaussian processes. In Artiﬁcial Intelli-
gence and Statistics 12, pp. 567–574, 2009.
Treves, F. Topological Vector Spaces, Distributions and
Kernels. Dover books on mathematics. Academic Press,
1967.
Wahlstr¨om, Niklas, Kok, Manon, Sch¨on, Thomas B., and
Gustafsson, Fredrik. Modeling magnetic ﬁelds using
Gaussian processes. In in Proceedings of the 38th Inter-
national Conference on Acoustics, Speech, and Signal
Processing (ICASSP), 2013.
Wilson, Andrew G. and Adams, Ryan Prescott. Gaussian
process kernels for pattern discovery and extrapolation. In
ICML (3), volume 28 of JMLR Workshop and Conference
Proceedings, pp. 1067–1075. JMLR.org, 2013.
Wilson, Andrew G., Dann, Christoph, and Nickisch, Hannes.
Thoughts on massively scalable Gaussian processes.
2015a. (arXiv:1511.01870).
Wilson, Andrew G., Hu, Zhiting, Salakhutdinov, Rus-
lan, and Xing, Eric P. Deep kernel learning. 2015b.
arXiv:1511.02222).
Zerz, Eva. Topics in multidimensional linear systems theory,
volume 256 of Lecture Notes in Control and Information
Sciences. London, 2000.
Zerz, Eva, Seiler, Werner M, and Hausdorf, Marcus. On the
inverse syzygy problem. Communications in Algebra, 38
(6):2037–2047, 2010.
