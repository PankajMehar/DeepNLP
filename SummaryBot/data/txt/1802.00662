DualMemoryNeuralComputerforAsynchronousTwo-viewSequentialLearningHungLe,TruyenTranandSvethaVenkateshCentreforPatternRecognitionandDataAnalyticsSchoolofInformationTechnology,DeakinUniversity,Geelong,AustraliaEmail:lethai@deakin.edu.auFebruary2,2018AbstractOneofthecoretaskinmulti-viewlearningistocaptureallrelationsamongviews.Forsequentialdata,therelationsnotonlyspanacrossviews,butalsoextendthroughouttheviewlengthtoformlong-termintra-viewandcross-viewinteractions.Inthispaper,wepresentanewmemoryaugmentedneuralnetworkmodelthataimstomodelthesecom-plexinteractionsbetweentwosequentialviewsthatareasynchronous.Ourmodelusestwoneuralencodersforreadingfromandwritingtotwoexternalmemoriesforencodinginputviews.Theintra-viewinteractionsandthelong-termdependenciesarecapturedbytheuseofmemoriesdur-ingthisencodingprocess.Therearetwomodesofmemoryaccessinginoursystem:late-fusionandearly-fusion,correspondingtolateandearlycross-viewinteractions.Inlate-fusionmode,thetwomemoriesaresep-arated,containingonlyview-speciﬁccontents.Intheearly-fusionmode,thetwomemoriessharethesameaddressingspace,allowingcross-memoryaccessing.Inbothcases,theknowledgefromthememoriesﬁnallywillbesynthesizedbyadecodertomakepredictionsovertheoutputspace.Theresultingdualmemoryneuralcomputerisdemonstratedonvariousofex-periments,fromthesyntheticsumoftwosequencestasktothetasksofdrugprescriptionanddiseaseprogressionsinhealthcare.Theresultsshowimprovedperformanceoverbothtraditionalalgorithmsanddeeplearningmethodsdesignedformulti-viewproblems.1IntroductionInmulti-viewlearning,datacanbenaturallypartitionedintodiﬀerentchannelspresentingdiﬀerentviewsofthesamedata.Forexamples,multilingualdocu-mentshaveoneviewforeachlanguage,andimagesofa3Dobjectaretakenfromdiﬀerentviewpoints.Multi-viewsequentiallearningisasub-classofmulti-viewlearningwhereeachviewdataisintheformofsequentialevents,whichcanbe1synchronousorasynchronous.Inthesynchronoussetting,allviewssharethesametimestep.Someproblemsofthistypeincludesvideoconsistingofvisualandaudiostreamsandtextasajointsequenceofwordsandpart-of-speechtags.Synchronousmulti-viewsequentiallearninghasbeenanactivearea[19,22,30].Despitetheirpracticalusages,theseworksmakeassumptionsonthetimestepalignmentsandthustheyareconstrainedbythescopeofsynchronousmulti-viewproblems.Inthiswork,wefocusmoreonasynchronoussequentialmulti-view,thatisthereisnoalignmentamongviewsandthesequencelengthscanvaryacrossviews.Theseoccurwhenthedataiscollectedfromchannelshavingdiﬀerenttimescalesorwecannotkeepthetimeinformationwhenextractingdata.Inhealthcare,forinstance,anelectronicmedicalrecord(EMR)containsinforma-tiononpatient’sadmissions,eachofwhichconsistsofvariousviewssuchasdiagnosis,medicalprocedure,andmedicine.Althoughanadmissionistime-stamped,medicaleventsfromeachviewinsidetheadmissionarenotsyn-chronousanddiﬀerentinlength.Asynchronousmulti-viewdataoftendemonstratesthreetypesofviewin-teractions.Theﬁrsttypeisintra-viewinteractions,thosethatinvolveonlyoneview,representingtheinternaldynamics.Forexamples,eachEMRviewhasspeciﬁcrulesforcodingitsevents,formingdistinctivecorrelationsamongmedicaleventsinsideaparticularview.Thesecondtypeislatecross-viewinter-actions,thosethatspanfrominputviewstooutput,representingthemappingfunctionbetweentheinputsandtheoutputs.Wecallit“late”becausethein-teractionacrossinputviewshappenonlyintheinferenceprocess.Thethirdtypeisearlycross-viewinteractions,thosethataccountforrelationscoveringmultipleinputviewsandhappeningbeforetheinferenceprocess.Forexample,indrugprescriptionproblem,thediagnosisviewisthecauseofthemedicalprocedureview,bothofwhichaﬀecttheoutputwhicharemedicinesforthatpatient.Theinteractionsinsequentialviewsnotonlyspanacrossviewsbutalsoextendthroughoutthelengthofthesequences.Oneexampleinvolvesapatient’sdiseasesincurrentadmissionarerelatedtootherdiseasesortreatmentsfromanadmissionhappenedalongtimeago.Thecomplexityofviewinteractions,togetherwiththeunalignmentandlong-termdependenciesamongviewsposeagreatchallengeinasynchronousmulti-viewproblems.Weproposeanovelmemoryaugmentedneuralnetworkmodel(MANN)solvingtheproblemofasynchronousinteractionsandlong-termdependenciesatthesametime.Ourmodelmakesuseofseveralneuralcontrollersandexternalmemoriestobuildupadualmemoryneuralcomputer.Inourarchitecture,eachinputviewisassignedacontrollerandamemorytomodeltheintra-viewinteractionsinthatparticularview.Ateachtimestep,thecontrollerreadsaninputevent,updatesthememory,andgeneratesanoutputbasedonitscurrenthiddenstateandreadvectorsfromthememory.Correspondingtothetwotypesofcross-viewinteractions,therearetwomodesinourarchitecture:late-fusionandearly-fusionmemories.Inthelate-fusionmode,thememoryspaceforeachviewareseparatedandindependent,thatisthethereisnoinformationexchangebetweenthetwomemoriesduringtheencodingprocess.Thememories’read2valuesareonlysynthesizedtogenerateacross-viewknowledgeinthedecodingphase.Contrasttothelate-fusionmode’s,thememoryaddressingspaceintheearly-fusionmodecanbesharedamongviews.Thatis,theencoderfromoneviewcanaccessandmodifythecontentsoftheotherview’smemory.Thisdesignensurestheinformationissharedacrossviewsviamemoriesaccessingandwehavetoincludecachecomponentstosupportmemoryupdatingprocedureinthiscase.Finally,weapplymemorywrite-protectedmechanisminthedecodingprocesstomaketheinferenceofourmodelmoreeﬃcient.Insummary,ourmaincontributionsare:(i)proposinganoveldualmemoryneuralcomputer(DMNC)tosolvetheasynchronousmulti-viewsequentialprob-lem,(ii)designingourarchitecturetomodelcomprehensivelyviewinteractionsandlong-termdependencies,(iii)demonstratingtheeﬃcacyofourproposedmodelonreal-worldmedicaldatasetsfortheproblemsofdrugprescriptionanddiseaseprogressions.ThesigniﬁcanceofDMNCliesinitsversatilityasourmodelpresentsagenericapproachthatusesexternalmemoriestomodelmulti-viewproblems.2Background2.1RelatedworksMulti-viewlearningisawell-studiedproblem,wheremethodsoftenexploitei-thertheconsensusorthecomplementaryprinciple[29].Astraightforwardap-proachistoconcatenateallmultipleviewsintoonesingleviewtoadapttoconventionalmachinelearningalgorithms,bothforvectorinputs[6,31]orse-quentialinputs[13,25].Anotherapproachisco-training[2,16],aimingtomaximizethemutualagreementondistinctviews.Otherapproacheseitheres-tablishalatentsubspacesharedbymultipleviews[21,28]orperformmultiplekernellearning[23,27].Theseworksaretypicallylimitedtonon-sequentialviews.Morerecently,deeplearningisincreasinglyappliedformulti-viewproblems,especiallywithsequentialdata.Forexamples,LSTM[9]isextendedformulti-viewproblemsin[22]ormultiplekernellearningiscombinedwithconvolutionnetworksin[18].Morerecentmethodsfocusonbuildingdeepnetworkstoextractfeaturesfromeachviewbeforeapplyingdiﬀerentlate-fusiontechniquessuchastensorproducts[30]orcontexualLSTM[19].Allofthesedeeplearningmethodsaredesignedonlyforsynchronoussequentialinputviews.Hence,theapplicationsofthesemethodsmostlyfallintotaggingproblemswheretheoutputisalignedwiththeinputviews.Asfarasweknow,theonlyworkthatcanapplytoasynchronousinputsis[5],inwhichtheauthorsconstructsadualLSTMforfeatureextractionanduseattentionforlate-fusion.Inhealthcare,thereareonlyfewworksthatmakeuseofmulti-viewdata.Amulti-viewmulti-taskmodelisproposedin[15]topredictfuturechronicdiseasesgivenmulti-mediaandmulti-modelobservations.However,thismodelisonlydesignedforsingle-instanceregressionproblems.DeepCare[17]solves3thediseaseprogressionproblembycombiningdiagnosisandinterventionviews.Ittreatseventsineachadmissionasabagandusepoolingstogetthefeaturevectorsforthetwoviewsinanadmission.Thesequentialpropertyofeventsinsideeachadmissionisignoredandthereisnomechanismtomodelcross-viewinteractionsateventlevel.TherearemanyotherworksusingdeeplearningsuchasRETAIN[4],LSTM-DO-TR[10],Dipole[12],Deepr[14],LEAP[32]thatattackdiﬀerentproblemsinhealthcare.However,theyonlyuseoneinputview.Memoryaugmentedneuralnetwork(MANN)isarecentpromisingresearchtopicindeeplearning.MemoryNetworks(MemNNs)[26]andNeuralTuringMachines(NTMs)[7]arethetwoclassesofMANNswhichhavebeenappliedtomanyproblemsincludinghealthcare[20].However,designingaMANNformulti-viewlearningisstillnewandourworkisoneoftheﬁrstattemptstobuildagenericMANNcapableofmodelinginteractionsamongeventsfromdiﬀerentdataviews.ThememoriesusedinourmodelarebasedonthepowerfulDNC[8],thelatestimprovementovertheNTM.SinceDNCisthebuildingblockinourmodel,wewillgivesomepreliminariesofDNCinthenextsubsection.2.2DNCOverviewADNCconsistsofacontroller,whichaccessesandmodiﬁesanexternalmemorymoduleusinganumberofreadandwriteheads.Givensomeinputxt,andasetofRpreviousreadvaluesfrommemoryrt−1=(cid:2)r1t−1,r2t−1,...,rRt−1(cid:3),thecontrollerproducessomekeykt∈RD,whereDisthewordsizeinmemory.ThiskeywillbeusedforlocatingthecurrentreadandwritememoryslotsofamemorymatrixMt∈RN×D,whereNisthenumberofmemorylocations.Thecontent-basedaddressingmechanismisbasedoncosinesimilarity:Cos(Mt(i),kt)=kt·Mt(i)||kt||·||Mt(i)||(1)whichisusedtoproduceacontent-basedread-weightandwrite-weightvectorwhoseelementsarecomputedaccordingtoasoftmaxoversimilarityscorestothekeyofeachmemory’sslot.Inadditiontocontent-basedaddressing,DNCsupportsdynamicmemoryallocationandtemporalmemorylinkageforcomput-ingtheﬁnalwrite-weightwwtandread-weightswrkt.Thememoryisupdatedbyfollowingrule:Mt=Mt−1◦(cid:0)E−wwte>t(cid:1)+wwtv>t(2)whereEisanN×Dmatrixofones,wwt∈[0,1]Nisthewrite-weight,et∈[0,1]Disanerasevector,vt∈RDisawritevector,◦ispoint-wisemultiplication.Thek-threadvaluerktisretrievedusingthisformular:rkt=NXiwrkt(i)Mt(i),1≤k≤R(3)43Methods3.1Two-ViewProblemsFormulationFirst,wewillintroduceagenericformulationofthetwo-viewproblem.LetSi1,Si2denotethetwoinputviewspacesandSdenotestheoutputviewspace.Eachsampleofthetwo-viewproblem(cid:0)Xi1,Xi1,Y(cid:1)consistoftwoinputviews:Xi1=(cid:8)xi11,...,xi1k,...,xi1Lii(cid:9),Xi2=(cid:8)xi21,...,xi2k,...,xi2Li2(cid:9)andoneoutputviewY={y1,...,yk,...,yL},whereeachviewcanhavediﬀerentlength(Li1,Li2,L)andcanbeseenasaset/sequenceofeventsthatbelongstodiﬀerentspaces(xi1k∈Si1,xi2k∈Si2,yk∈S).Eacheventthencanberepresentedbyanone-hotvectorv∈[0,1]kCk,whereCcanbeSi1,Si2orS.Itshouldbenotedthatthisformulationcanbeappliedtomanysituationsincludingvideo-audiounder-standing,image-captioningandothertwo-channeltime-seriessignals.However,herewefocuseﬀortonsolvingthetwo-viewproblemsinhealthcare.Next,wewilldescribeourchosentwo-viewhealthcareproblems.Were-strictthescopetoproblemsofmodelingElectronicMedicalRecord(EMR),whichtypicallycontainsthehistoryofhospitalencounters,includingdiagnosesandinterventionssuchasproceduresanddrugs.Indrugprescriptions,doctorsprescribedrugsafterconsideringdiagnosesandproceduresthatpatientshavetaken.Anotherimportantapplicationismodelingdiseaseprogression,wheredoctormayrefertopatient’shistoryofadmissions(previousdiagnosesandinter-ventions)tohelpdiagnosesthecurrentdiseaseortopredictthefuturediseaseoccurencesofthepatient.ThereareclinicalcodingrulesapplyingtoEMRcodessuchthatdiagnosesare“orderedbypriority”orproceduresfollowtheorderthat“theprocedureswereperformed”1.Besides,althoughmedicalcodesfromdiﬀerentviewsarehighlycorrelated,theyarenotaligned.Forinstances,somediagnosesmaycorrespondtooneprocedureoronediagnosismayresultsinmultiplemedicines.Hence,theseproblemscanbetreatedasasynchronoussequentialtwo-viewlearning.Indrugprescriptioncontext,Si1andSi2correspondtothediagnosisandprocedurespaces,respectively.Scorrespondstothemedicinespace.ThedrugprescriptionobjectiveistoselectanoptimalsubsetofmedicationsfromSbasedondiagnosisandprocedurecodes.Similartodrugprescriptionproblem,wecanformulatethediseaseprogressionsproblemastwosequenceofinputs(diagnosesandinterventions)andonesetofoutputs(nextdiagnoses).Althoughourarchi-tecturecanmodelsequentialoutput,thechoiceofrepresentingoutputassetistofollowcommonpracticeinhealthcarewheretheorderofmedicalsuggestionsisnotreallyimportant.Finally,apatientoftenhasmultipleadmissionrecordsfordiﬀerenthospi-talentries.Thusapatientrecordcanberepresentedas(cid:8)(cid:0)Xi1a,Xi1a,Ya(cid:1)(cid:9)Aa=1,whereAisthenumberofadmissionsthispatientcommits.Therearelong-termtemporaldependenciesbetweenadmissions.Forexample,oncediagnosedwithdiabetes(TypeIorII),theconditionsarepersistentthroughthepatient’slife,1https://mimic.physionet.org/mimictables/5LSTME LSTMi2(Encoder)M2input2hLSTMDLSTME LSTMi1(Encoder)M1 LSTMd(Decoder)Wi1EWEoutputhinput1hRhhWi2EFigure1:DualMemoryNeuralComputer.LSTMi1,LSTMi2arethetwoen-codingcontrollersimplementedasLSTMs.LSTMdisthedecodingcontroller.Thedasharrowsrepresentcross-memoryaccessinginearly-fusionmode.eventhoughitmightnotbecodedateveryvisit.Therefore,inordertopredictYa,wemayneedtoexploitnotonly(cid:0)Xi1a,Xi1a(cid:1)butalso(cid:8)(cid:0)Xi1pa,Xi1pa(cid:1)(cid:9)a−1pa=1.Moredetailsonhowourworkmakesuseofpreviousadmissionsandhandlelong-termdependencieswillbegiveninSection3.4.3.2DualMemoryNeuralComputerWenowpresentourmaincontributiontosolvethegenerictwo-viewproblem–adeepmemoryaugmentedneuralnetworkcalledDualMemoryNeuralComputer(DMNC)(seeFig.1).Ourarchitectureconsistsofthreeneuralcontrollers(twoforencodingandonefordecoding),eachofwhichinteractswithtwoexternalmemorymodules.ThetwomemorymodulesaresimilartothatinDNC[8],thatistheyareequippedwithtemporallinkageanddynamicallocation.ThethreecontrollershavetheirownembeddingmatricesWi1E,Wi2E,WEthatareusedtoprojecttheone-hotrepresentationofeventstoauniﬁedd-dimensionalspace.Weusexi1k,xi2k,yk∈Rdtodenotetheembeddingvectorofxi1k,xi2k,yk,respectively,inwhich:6xi1k=Wi1Exi1k,xi2k=Wi2Exi2k,yk=WEyk,(4)Theembeddingvectorsxi1k,xi2karealwaysusedasinputsoftheencoderswhiletheembeddingvectorykwillonlybeusedasinputofthedecoderiftheoutputviewissequence.Eachencoderwilltransformtheembeddingvectorstoh-dimensionalhiddenvectors.Thenext-stephiddenvectorsoftheencodersarecomputedas:hi1t1+1=LSTMi1(cid:0)(cid:2)xi1t1,ri1t1(cid:3),hi1t1(cid:1),1≤t1<Li1(5)hi2t2+1=LSTMi2(cid:0)(cid:2)xi2t2,ri2t2(cid:3),hi2t2(cid:1),1≤t2<Li2(6)whereri1t1,ri2t2arereadvectorsatcurrenttimestepofeachencodersandLi1,Li2arethelengthofeachinputview.Itshouldbenotedthattimestepineachviewmaybeasynchronousandthelengthsmaybediﬀerent.Inourapplication,sincewetreatinputviewsassequences,weuseLSTMasthecoreoftheencoders2.Usingseparatedencoderforeachviewprocessnaturallyencouragesthein-viewinteractions.Tomodelcross-viewinteractions,weusetwomodesofmemories.Late-fusionmemories:Inthismodeofmemories,ourarchitectureonlymodelslatecross-viewinteractions.Inparticular,ri1t1andri2t2inEq.(5)andEq.(6)arecomputedseparatelyfromdiﬀerentexternalmemorymodules:ri1t1=hri1,1t1,...,ri1,Rt1i=fe1read(cid:0)hi1t1−1,M1(cid:1)(7)ri2t2=hri2,1t2,...,ri2,Rt2i=fe2read(cid:0)hi2t2−1,M2(cid:1)(8)whereM1,M2arethetwomemorymatricescontainingview-speciﬁccontentsandfe1read,fe2readaretworeadfunctionsoftheencoderswithseparatedsetofparameters.Giventhecontrollerhiddenvectors,thereadfunctionsproducethekeyski1t1,ki2t2,eachofwhichisusedtoaddressthecorrespondingmemoryandthencomputethereadvectorsinaDNC’smannerasinEq.(3).Thisdesignensuresthedynamicsofcomputationinoneviewdoesnotaﬀecttheother’sandonlyin-viewcontentsarestoredintheview-speciﬁcmemory.Thismodeisimportantbecauseincertainsituations,allowingwritingexternalcontenttoview-speciﬁcmemorywillmessuptheacquiredknowledgeandobstructthelearningprocess.InSection4.1,wewillshowacasestudythatﬁtswiththissettingandtheempiricalresultswillshowthatrefusingtomodelearlycross-viewinteractionsinthiscaseisnecessarytoachievebetterperformance.Early-fusionmemories:Forsituationswherethereexistsastrongcor-relationbetweenthetwoinputviews,requiringtomodelearlycross-viewin-teractions,weintroduceanothermodeofmemories:early-fusionmode.Inthismode,thetwomemoriessharesthesameaddressingspace,thatistheencoder2Forinputsassets,wecanreplacetheLSTMswithMLPs7Algorithm1Trainingalgorithmforhealthcaredata(setoutput)Require:Trainingset{{(Xi1a,Xi2a,Ya}Aa=1}Nn=11:SampleBsamplesfromtrainingset2:foreachsampleinBdo3:ClearmemoryM1,M24:fora=1,Ado5:(Xi1,Xi2,Y)=(Xi1a,Xi2a,Ya)6:whilet1<Li1ort2<Li2do7:ift1<Li1then8:UseEq.(5)tocalculatehi1t1+19:UseEq.(2)orEq.(13)toupdateM110:UseEq.(7)orEq.(9)toreadM111:t1=t1+112:endif13:ift2<Li2then14:UseEq.(6)tocalculatehi2t2+115:UseEq.(2)orEq.(13)toupdateM216:UseEq.(8)orEq.(10)toreadM217:t2=t2+118:endif19:endwhile20:UseEq.(15)andEq.(16)toreadM1,M221:UseEq.(22)tocalculateby22:Updateθusing∇θLossset(Y,by)23:endfor24:endforfromoneviewcanaccessthememorycontentfromanotherviewandviceversa.Also,thereadfunctionsfereadsharetheparameterset(i.e.shared-weight):ri1t1=hri1,1t1,...,ri1,Rt1i=feread(cid:0)hi1t1−1,[M1,M2](cid:1)(9)ri2t2=hri2,1t2,...,ri2,Rt2i=feread(cid:0)hi2t2−1,[M1,M2](cid:1)(10)Sincethereadvectorsforoneencodercancomefromeithermemories,theencoder’snexthiddenvaluesaredependentonbothviews’memorycontents,whichenablepossibleearlycross-viewinteractionsinthismodeofourarchitec-ture.Memoriesmodiﬁcationwithcachecomponents:Inbothmodes,thetwomemoriesareupdatedeverytimestepbythetwoencoders.Whileinlate-fusionmode,thewritingtotwomemoriesareindependentandcanbeexecutedinparallelusingEq.(2),inearly-fusionmode,thewritingsmustbeexecutedinanalternativemanner.Inparticular,thetwoencoderstaketurnwritingtomemories,oneafteranother,allowingtheexchangeofinformationtohappenat8everytimestep.Doingthiswayisoptimalifthetwoviewsaresynchronousandequalinlengths.Tomakeitworkwithvary-lengthsinputviews,weintroduceanewcomponenttoourarchitecture:cachememorythatliesbetweenthecontrollerandtheexternalmemory.WhiletheoriginalDNCwritesdirectlytheevent’svaluetotheexternalmemory,inearly-fusionmodeofourarchitecture,eachcontrollerintegrateswritevaluesinsideitsowncachememoryctuntilappropriatemomentbeforemovethemtotheexternalmemory.Weintroducegctasalearnablegatetocontrolthedegreeofintegrationbetweencurrentwritevalueandthepreviouscache’scontentasfollows:v0t,gct=MLPc(ht)(11)ct=gct◦ct−1+(1−gct)◦v0t(12)Intheseequations,v0tisthecandidatewritevalue,gctisthecachegate,MLPcisafunctionimplementedbyfeed-forwardneuralnetwork,ctisthecachecontent,and◦istheelement-wisemultiplication.Then,similartoEq.(2),thecachewillbewrittentothememoryusingthefollowingformula:Mt=Mt−1◦(cid:0)E−wwte>t(cid:1)+wwtc>t(13)Weproposethisnewwritingmechanismforearly-fusionmodetoenableoneencodertowaitforanotherwhileprocessinginputevents(inthiscontext,waitingmeanstheencoderstopswritingtomemory).IntheoriginalDNC,ifthewritegategwtisclosetozero,theencoderdoesnotwritetomemoryandthewritevalueatcurrenttimestepwillbelost.However,inourdesign,evenwhenthereisnowriting,thewritevaluesomehowcanbekeptinthecacheifgct<1.Thecacheinaviewmaychoosetoholdanevent’swritevalueinsteadofwritingitimmediatelyatthereadtimestep.Thus,theinformationoftheeventiscompressedinthecacheuntilappropriateoccasion,whichmaybeaftertheappearanceofanothereventfromtheotherview.Thismechanismenablestworelatedeventsindiﬀerenttimestepssimultaneouslyinvolveinbuildingupthememories,whichisessentialtomodelearlycross-viewaccurately.Write-protectedmemories:Inourarchitecture,duringtheinferenceprocess,thedecoderstopswritingtomemories.Wedecidetoaddthisfeaturetoourdesignbecausethedecoderactuallydoesnotreceiveanynewinputwhenitproducesoutput.Writingtomemoriesinthisphasemayberedundantanddeterioratethememorycontents,hamperingtheeﬃciencyofthemodel.3.3InferenceinDMNCInthissection,wewillgivemoredetailontheoperationofthedecoderandtheinferenceprocessinourarchitecture.Becausethedecoderworksdiﬀerentlyfordiﬀerentoutputtypes(setorsequence),wewillpresenttwoversionsofdecoderimplementation.9Outputassequence:Inthissetting,thedecoderingeststheﬁnalstateoftheencodersasitsinitialhiddenstateandgetsthereadvectorsfrombothmemoriestoproduceaprobabilitydistributionovertheoutput:h0=(cid:2)hi1Li1,hi2Li2(cid:3)(14)ri1t=hri1,1t,...,ri1,Rti=fdread(ht−1,M1)(15)ri2t=hri2,1t,...,ri2,Rti=fdread(ht−1,M2)(16)ot=MLPout(cid:0)LSTMd(cid:0)(cid:2)y∗t−1,ri1t,ri2t(cid:3),ht(cid:1)(cid:1)(17)whereri1t,ri2tarereadvectorfromM1,M2,respectively,providedbythedecoder’sreadfunctionfdread,ot∈R|S|istheoutputvectorattimesteptandy∗t−1istheembeddingrepresentationofpredictedoutputy∗tatprevioustimestep.Then,theprobabilityoveroutputattimesteptiscalculatedas:P(cid:0)yt|Xi1,Xi2(cid:1)=softmax(ot)(18)andthepredictedoutputisgivenas:y∗t=argmaxy∈SP(cid:0)yt=y|Xi1,Xi2(cid:1)(19)Thelossfunctioninthiscaseissumofcrossentropyoveralltimestep:Lossseq(Y,P)=−LXt=1logP(cid:0)yt|Xi1,Xi2(cid:1)(20)Outputasset:Inthissetting,thedecoderusesfdreadtoreadfromthememoriesoncetogetthereadvectors.Thenthedecodercombinesthesereadvectorswiththeencoders’ﬁnalhiddenvaluestoproducetheoutputvectorˆy∈R|S|:ˆy=σ(cid:0)MLPout(ri11W1+ri21W2+(cid:2)hi1Li1,hi2Li2(cid:3)W3)(cid:1)(21)Here,thecombinationissimplythelinearweightedsummationwithparame-termatricesW1,W2,W3,MLPoutisthedecoder’soutputfunctionimplementedasasingle-layerfeed-forwardneuralnetworkandσisthesigmoidfunction.Foroutputasset,thelossfunctionismulti-labellossdeﬁnedas:Lossset(Y,by)=−Xyl∈Ylogby+Xyl/∈Ylog(1−byl)(22)Forbothsettings,thedecodermakesuseofbothmemories’contentsandencoders’ﬁnalhiddenvaluestoproducetheoutput.Whilememorycontents10representthelong-termknowledge,theencoder’shiddenvaluesrepresenttheshort-terminformationstoredinsidethecontrollers.Bothsourcesofinforma-tionarecrucialtomodelthelatecross-viewinteractionsandnecessaryforthedecodertomakedecisionsduringitsinferenceprocess.3.4PersistentMemoryModelingMultipleAdmissionsAsmentionedearlierinSection3.1,oneuniquepropertyofhealthcareisthelong-termdependenciesamongadmissions.Therefore,theoutputatthecurrentadmissionYaisdependentonthecurrentandallpreviousadmission’sinputs(cid:8)(cid:0)Xi1pa,Xi1pa(cid:1)(cid:9)apa=1.Therearesomewaystomodelthisproperty.Thesimplestsolutionistoconcatenatethecurrentadmissioninputwithpreviousinputstomakeupthecurrentinputforthemodel.Thismethodcausesdatareplicationandmorepreprocessingoverhead.Anothersolutionistouserecurrentneu-ralnetworktomodelthedependencies.Asin[3,17],theauthorsuseGRUandLSTMwhereeachadmission’sinputistreatedasasetofeventsandisrepresentedbyafeaturevectorbeforefedintotherecurrentnetworks.Inourmemory-augmentedarchitecture,wecanmodelthisdependenciesbyusingthememoriestostoreinformationfrompreviousadmissions.IntheoriginalDNC,thememorycontentisﬂushedeverytimenewdatasample(i.e.newadmission)isfed–thiscertainlylosestheinformationofadmissionhistory.Wemodifythismechanismbykeepingthememoriespersistentduringapatient’sadmissionsprocessing.Thatis,thecontentofmemoriesisbuiltupandmodiﬁedduringthewholehistoryofapatient’sadmissions.Thememoriesareonlyclearedpriortoreadinganewpatient’srecord.Persistentmemoriesinourarchitectureplaytwoimportantroles.First,becausethenumberofeventsacrossadmissionsarelargewhilememorysizesareﬁnite,thememorymoduleslearntocompresseﬃcientlytheinputviews,keepingonlyessentialinformation.Thismakesmemorylook-upsinthedecodingprocessonlylimitedtoaﬁxsizeofchosenknowledge.Thisismorecompactandfocusedthanattentionmechanisms,inwhichthedecoderhastoattendtoalleventsintheinput.Second,eachslotinmemoriescanstoreinformationofanyeventsintheinputviews,whichenablesskip-connectionreferenceindecodingprocess,i.e.thedecodercanjumptoanyinputevents,eventheonesinthefarthestadmission,tolookfornecessaryknowledge.Thewholeprocessoftrainingourdualmemoryneuralcomputerforhealth-caredataissummarizedinAlgorithm1.4ResultsInthissection,wedemonstratetheeﬀectivenessofourproposedmodelDMNConsyntheticandreal-worldtasks.WeuseDMNClandDMNCetodenotelate-fusionandearly-fusionmodesofourmodel,respectively.Thedataforreal-worldproblemsarerealEMRdatasets,somearepublicaccessible.WemakethesourcecodeofDMNCpubliclyavailableathttps://github.com/thaihungle/DMNC.1102000400060008000Step01234Cross entropyDMNC (early-fusion)DMNC (late-fusion)DNCDual-LSTMLSTMWLASFigure2:Traininglossofsumoftwosequencestask.02000400060008000Step0.00.20.40.60.81.0LossDMNC (early-fusion)DMNC (late-fusion)DNCDual-LSTMLSTMWLASFigure3:TrainingAccuracyofSumTask124.1SyntheticTask:SumoftwosequencesWeconductthissyntheticexperimenttoverifyourmodelperformanceandbehavior.Inthisproblem,theinputviewsaretworandomlygeneratedsequenceofnumbers:(cid:8)x11,...,x1L(cid:9),(cid:8)x21,...,x2L(cid:9).EachsequencehasLintegernumbers.Lisrandomlychosenfromrange[1,Lmax]andthenumbersarerandomchosenfromrange[1,50].Theoutputviewisalsoasequenceofintegernumbersdeﬁnedas(cid:8)yi=x1i+x2L+1−i(cid:9)Li=1,inwhichyi∈[2,100].Becausetheoutput’snumberisthesumoftwonumbersfromthetwoinputviews,wenamethetaskassumoftwosequences.Itshouldbenotedthattwoinputnumbersinthesummationdonotsharethesametimestep;hence,theproblemisasynchronous.Tosolvethetask,amodelhastoreadallthenumbersfromthetwoinputsequencesanddiscoverthecorrectpairthatwillbeusedtoproducethesummation.Normalsynchronousmulti-viewmodelscertainlyfailthistaskbecausetheyassumetheinputstobealigned.Inthetrainingphase,wechooseL=8,trainingfor10,000iterationswithminibatchsize=50.Inthetestingphase,weevaluateon2500randomsampleswithLmax=8,Lmax=16,Lmax=20toseethegeneralizationabilityofthemodelsbeyondtherangeinwhichtheyaretrained.Evaluations:thebaselinesforthissynthetictaskarechosenasfollows:•View-concatenatedsequentialmodels:Thisconcatenateseventsininputviewstoformonelongsequence.Thistechniquetransformsthetwo-viewsequentialproblemtonormalsequence-to-sequenceproblem.WepickLSTMandDNCastworepresentativemethodsforthisapproach.•AttentionmodelWLAS[5]:ThishasaLSTMencoderperview,andat-tentionisusedfordecoding,similartothatinmachinetranslation[1].Themodelisappliedsuccessfullyintheproblemofvideosentimentanal-ysis.Tomakeitsuitableforourtasks,wereplacetheencoders’feature-extractionlayersintheoriginalWLASbyanembeddinglayer.Wechoosethismodelasbaselinesinceitsarchitectureissomehowsimilartoours.Thediﬀerenceisthatwemakeuseofexternalmemoriesinsteadofatten-tionmechanism.•DualLSTM:ThismodelistheWLASmodelwithoutattention,thatisonlytheﬁnalstatesofencodersarepassedintothedecoder.Implementations:Forallmodels,embeddingandhiddendimensionsare64and128,respectively.Wordsizeformemory-basedmethodsare64.Memorysizefortheview-concatenatedDNCandDMNCare32and16,respectively.Wedoublethememorysizeforview-concatenatedDNCtoaccountforthefactthatthelengthoftheinputsequenceisnearlydoubleduetoviewconcatenation.Inthiscase,optimalnumberofreadheadsis1.WeuseAdamoptimizerwithdefaultparametersandapplygradientclippingsize=10totrainallmodels.Sinceoutputissequence,weusecross-entropylossfunctioninEq.(20).Theevaluationmetricusedinthistaskisaccuracycomputedasthenumberofcorrectpredictionsoverthelengthofoutputsequence.13ModelAccuracy(%)Lmax=8Lmax=16Lmax=20LSTM41.9924.1218.64DNC37.820.4314.67DualLSTM49.2531.8728.66WLAS48.4635.9830.78DMNCl99.7695.7978.17DMNCe99.0893.0075.84Table1:Sumoftwosequencestasktestresults.#ofpatients34,594#ofadmission42,586#ofdiagnosiscode6,461Averageofdiagnosislength12.19Maxofdiagnosislength39#ofprocedurecode1,881Averageofprocedurelength4.41Maxofprocedurelength35#ofdrugcode300Averageofdruglength37.88Maxofdruglength148Table2:MIMIC-IIIdatastatistics.Results:ThelearningcurvesofthemodelsareplottedinFigs.2and3.ThetestingaverageaccuracyissummarizedinTable1.Asclearlyshown,overalltheproposedmodelsoutperformothermethodsbyahugemarginofabout50%.AlthoughdualLSTMandWLASperformbetterthanview-concatenatedmethods,it’stoohardfornon-memorymethodsto“remember”correctlypairsofinputsforlateroutputsummation.View-concatenatedDNCevenwithdoublememorysizestillfailstolearnthesumrulebecausestoringtwoviews’datainasinglememoryseemtomessuptheinformation,makingthismodelperformworst.BetweentwoversionsofDMNC,late-fusionmodeisbetterperhapsduetotheindependencebetweentwoinputs’numbersequences.Thisisoneoccasionwheretryingtomodelearlycross-interactionsdamagestheperformance.Lastbutnotleast,theslightdropinperformancewhenourproposedmodelsaretestedwithLmax=16showsthatourmodelsreallylearnthesumrule.WhenLmax=20,theinputlengthislongerthanthememorysize,soevenwhenourproposedmodelscanlearnthesumrule,theycannotstoreallinputpairsforlatersummation.However,ourmethodsstillmanagetoperformbetterthananyotherbaseline.14ModelAUCF1P@1P@2P@5DiagnosisOnlyBinaryRelevance82.669.179.977.170.3ClassiﬁerChains66.863.868.366.861.1LSTM84.970.990.886.779.1DNC85.471.490.086.779.8ProcedureOnlyBinaryRelevance81.869.482.680.173.6ClassiﬁerChains63.461.783.780.371.9LSTM83.970.888.186.078.4DNC83.270.488.485.878.7DiagnosisandprocedureBinaryRelevance84.170.381.078.272.3ClassiﬁerChains64.663.084.681.574.2LSTM85.872.191.686.880.5DNC86.472.490.987.480.6DualLSTM85.471.490.687.180.5WLAS[5]86.672.591.988.180.9DMNCl87.473.292.488.982.6DMNCe87.673.492.189.982.5Table3:Mimic-IIIdrugprescriptiontestresults.4.2DrugprescriptiongivendiagnosesandproceduresThedatasetusedforthistaskisMIMIC-III,whichisapubliclyavailabledatasetconsistingofmorethan52kEMRadmissionsfrommorethan46kpatients.Inthistask,wekeepallthediagnosisandprocedurecodesandonlypreprocessthedrugcodesincetherawdrugcode’saveragelengthcanreachhundredsofcodesinanadmission,whichistoolonggiventheamountofdata.Therefore,onlytop300frequentlyusedoftotal4781drugtypesarekept(coveringmorethan70%oftherawdata).TheﬁnalstatisticsofthepreprocesseddataissummarizedinTable2.Evaluations:Wecompareourmethodwiththefollowingbaselines:•Bagofwordsandtraditionalclassiﬁers:Inthisapproach,eachinputviewisconsideredasasetofevents.Thevectorrepresentstheviewisthesumofone-hotvectorsrepresentingtheevents.Theseviewvectorsarethenconcatenatedandpassedintotraditionalclassiﬁers:SVM,LogisticRegression,RandomForest.Tohelptraditionalmethodshandlemulti-labeloutput,weapplytwopopulartechniques:BinaryRelevance[11]andClassiﬁerChains[24].Wewillonlyreportthebestmodelforeachofthetwotechniques,whichareLogisticRegressionandRandomForest,respectively.•View-concatenatedsequentialmodels(LSTM,DNC),DualLSTMand15DiagnosesCalculusOfGallbladderWithOtherCholecystitis,WithObstruction(57411),Vasculardisordersofmalegenitalorgans(60883),AbdominalPain,RightUpperQuadrant(78901),PoisoningByOtherTranquilizers(9695),AcuteMyocardialInfarctionOfOtherInferiorWall,SubsequentEpisodeOfCare(41042),HematomaComplicatingAProcedure(99812),Malignanthypertensiveheartdiseasewithoutheartfailure(40200),Dizzinessandgiddiness(7804),Venous(Peripheral)Insuﬃciency,Unspeciﬁed(45981),HemorrhageOfGastrointestinalTract,Unspeciﬁed(5789)ProceduresCoronaryBypassOfThreeCoronaryArteries(3613),SingleInternalMammary-CoronaryArteryBypass(3615),Extracorporealcirculationauxiliarytoopenheartsurgery(3961),InsertionOfIntercostalCatheterForDrainage(3404),Operationsoncornea:Excisionordestructionoftissueorotherlesionofcornea(114)Top5Ground-truthdrugs(manuallypickedbyexperts)DocusateSodium(DOCU100L),AcetylsalicylicAcid(ASA81),Heparin(HEPA5I),Acetaminophen(ACET325),PotassiumChloride(KCLBASE2)Top5Late-fusionRecommendationsDocusateSodium(DOCU100L),Acetaminophen(ACET325),PotassiumChloride(KCLBASE2),Dextrose(DEX50SY),AcetylsalicylicAcid(ASA81)Top5Early-fusionRecommendationsDocusateSodium(DOCU100L),Neostigmine(NEOSI),Acetaminophen(ACET325),Propofol(PROP100IG),PotassiumChloride(KCLBASE2)Table4:ExampleRecommendedMedicationsbyDMNConMIMIC-IIIdataset.1657411608837890196954104299812402007804459815789Diagnosis codes0.00.10.20.30.40.5Write gate valueLate-fusionEarly-fusionFigure4:M1’sgwtoverdiagnoses.DiagnosiscodesofaMIMIC-IIIpatientislistedalongthex-axis(orderedbypriority)withthey-axisindicatinghowmuchthewritegatewillallowadiagnosiswillbewrittentothememoryM1.WLAS[5]:similartothosedescribedinthesynthetictask.•Single-viewmodels:Toseetheperformancegainswhenmakinguseoftwoinputviews,wealsoreportresultswhenonlyusingoneviewforBinaryRelevance,ClassiﬁerChains,LSTMandDNC.Implementations:Werandomlydividethedatasetintothetraining,val-idationandtestingsetina2/3:1/6:1/6ratio.Fortraditionalmethods,weusegrid-searchingovertypicalrangesofhyper-parameterstosearchforbesthyper-parametervalues.Deeplearningmodels’bestembeddingandhiddendi-mensionsare64and64,respectively.OptimalwordandmemorysizeforDMNCare64and16,respectively.Theview-concatenatedDNCsharesthesameset-tingexceptthememorysizeisdoubleto32memoryslots.Thebestnumberofreadheadsare2.Sincetheoutputinthistaskisset,weusemulti-labellossfunctioninEq.(22)fordeeplearningmethods.Wemeasuretherelativequalityofmodelperformancesbyusingcommonmulti-labelmetrics,AreaUndertheROCCurve(AUC)andF1scores,bothofwhicharemacro-averaged.Similarresultscanbeachievedwhenusingmicro-averagedsowedidnotreportthemhere.Inpractice,precisionatk(P@k)areoftenusedtojudgethetreatmentrecommendationquality.Therefore,wealsoincludethem(k=1,2,5)intheevaluationmetrics.Results:Table3showstheperformancesofexperimentalmodelsonafore-mentionedmeasuremetrics.Wecanseethebeneﬁtofusingtwoinputviewsinsteadofone,whichingeneralhelpsimprovethemodelperformances.Tra-ditionalmethodsclearlyunderperformdeeplearningmethodsperhapsbecausethesemethodsarehardtoscalewhentherearemanyoutputlabelsandthe173613361539613404114Procedure codes0.00.20.40.60.8Write gate valueLate-fusionEarly-fusionFigure5:M2’sgwtoverprocedures.MedicalprocedurecodesofaMIMIC-IIIpatientislistedalongthex-axis(intheorderofexecutions)withthey-axisindicatinghowmuchthewritegatewillallowaprocedurewillbewrittentothememoryM2.inputsinourproblemarenotbag-of-words.Amongdeeplearningmodels,ourproposedonesconsistentlyoutperformothersinalltypeofmeasurements.Ourmethodsdemonstrate1-2%improvementsoverthesecondrunner-upbaselines,whichindicatesthemodelingabilityofmemory-baseddualarchitectureforthisreal-worldtwo-viewproblem.Thelate-fusionmodesseemssuitableforcertaintypeofmetrics,butoverall,theearly-fusionmodeisthewinner,highlightingtheimportanceofmodelingearlycross-viewinteractions.Casestudy:InTable4,weshowanexampleofdrugprescriptionmadeforapatientgivenhiscurrentdiagnosesandprocedures.Thepatienthadseri-ousproblemswithhisbowelasdescribedintheﬁrstfourdiagnoses.Thenextthreediagnosesarealsosevererelatingtohisheartproblemswhiletheremain-ingdiagnosesarelessurgent.Itseemsthatheart-relateddiagnoseslaterledtoheartsurgerieslistedintheprocedurecodes.BothmodesofDMNCpredictcor-rectlythedrugDocusateSodiumusedtocureurgentbowelsymptoms.Relatingtoheartdiseasesandsurgeries,ourmodelspredictcloselytoexpert’schoices.PotassiumChlorideisnecessaryforahealthyheart.AcetaminophenandPropo-folarecommonlyusedduringsurgeries.However,someheartmedicinesuchasHeparinismissedbythetwomodels.Figs.4and5demonstratethe“focus”ofthetwomemoriesondiagnosisandprocedureview,respectively.Thehigherthewritegatevalues,themoreinformationofthemedicalcodeswillbewrittenintothememories.Wecanseebothmodespaylessattentiononlastdiagnosescorrespondingtolessseveresymptoms.Comparedtothelate-fusion,theearly-fusionmodekeepsmoreinformationonprocedures,especiallytheheart-relatedevents.Thismayhelpincreasetheweightonheart-relatedmedicinesandenable18ModelDiabetesMentalP@1P@2P@3P@1P@2P@3DeepCare66.259.653.752.746.940.2WLAS65.960.856.551.848.945.7DMNCl66.561.357.052.749.446.2DMNCe67.661.256.953.650.047.1Table5:Localhospitaltestresults.P@KisprecisionatK.ittoincludeAcetylsalicylicAcid,acommondrugusedafterheartattackinthetoprecommendations.4.3DiseaseprogressiongivenadmissionhistoriesDatausedinthistaskaretwocohortsofdiabetesandmentalEMRscollectedbetween2002-2013fromalargelocalhospitalinAustralia.Thedataisdividedintothreeparts:training,validationandtestingina2/3:1/6:1/6ratio.Sincewewanttopredictthenextdiagnosesforapatientgivenhisorherhistoryofadmission,wepreprocessedthedatasetsbyremovingpatientswithlessthan2admissions,whichendsupwith53,208and52,049admissionsforthetwocohorts.Inthisdataset,proceduresandmedicinesaregroupedintointerventioncodes,togetherwithdiagnosiscodesformingapatient’sadmissionrecord.Thenumberofdiagnosisandinterventioncodesare249and1071,respectively.Moredetailsofthedatasetandpreprocessingstepsaredescribedin[17].DiﬀerentfromMIMIC-III,apatientrecordinthelocalhospital’sdataoftenconsistsofmanyadmissions,whichissuitableforthetaskofdiagnosisprogression.Theaveragelengthandthemaximumlengthofadmissionsperpatientare5.35and253,respectively.Evaluations:Forcomparison,wechoosethesecondbest-runnerinourpreviousexperimentsWLASandthecurrentstate-of-the-artDeepCare[17]asthetwobaselines.Implementations:Weusethevalidationdatasettotunethehyper-parametersofourimplementingmethodsandhavethebestembeddingandhiddendimensionsare20and64,respectively.ThewordandmemorysizeforDMNCarefoundtobe32and32,respectively.Thebestnumberofreadheadsare2.Forperformancemeasurements,weuseP@kmetric(%)tomakeitcomparablewithDeepCare’sresultsreportedin[17].Results:WereportsthetestingresultsofexperimentalmodelsfordiseaseprogressiontaskinTable5.Forbothcohorts,ourproposedmodelsconsistentlyperformbetterthanothermethodsandtheperformancegainsbecomelargerasthenumberofpredictionsincrease.ComparedtoDeepCarethatusespre-trainedembeddingsandtime-intervalsasextrainformation,ourmodelsonlyuserawmedicalcodesandperformbetter.Thisemphasizestheimportanceofmodelingviewinteractionsateventlevel.Thelate-fusionDMNCseemsslightlybetterthantheshared-memoryDMNCindiabetescohort,yetoverall,DMNC19inearly-fusionmodeisthebetterone,whichagainvalidatesitsabilitytomodelalltypesofviewinteractions.5ConclusionsThispaperproposesanovelMANNmodelforasynchronoustwo-viewsequentiallearningcalledDualMemoryNeuralComputer(DMNC).Underourdesign,eachinputviewisassignedaneuralcontrollertoencodeandstoreitseventstoadedicatedmemory.Afterallinputviewsarestored,adecoderwillaccessthememoriesandsynthesizethereadcontentstoproducetheﬁnaloutput.Moreover,ourmodelisequippedwithtwomodesofcomputations,enablingittomodelcomprehensivelyalltypesofviewinteractions.Throughextensiveexperiments,DMNCiscomparedwithvariousbaselinesandconsistentlyshowsbetterperformanceonthreetasks:sumoftwosequences,drugprescriptionanddiseaseprogressions.Futureworkswillfocusongeneralizingourmodeltomulti-inputmulti-outputsettingsandextendingtherangeofapplicationstobiggerproblemssuchasmulti-mediaandmulti-agentsystems.References[1]DzmitryBahdanau,KyunghyunCho,andYoshuaBengio.Neuralma-chinetranslationbyjointlylearningtoalignandtranslate.arXivpreprintarXiv:1409.0473,2014.[2]AvrimBlumandTomMitchell.Combininglabeledandunlabeleddatawithco-training.InProceedingsoftheeleventhannualconferenceonCom-putationallearningtheory,pages92–100.ACM,1998.[3]EdwardChoi,MohammadTahaBahadori,andJimengSun.DoctorAI:PredictingClinicalEventsviaRecurrentNeuralNetworks.arXivpreprintarXiv:1511.05942,2015.[4]EdwardChoi,MohammadTahaBahadori,JimengSun,JoshuaKulas,AndySchuetz,andWalterStewart.RETAIN:AnInterpretablePredictiveModelforHealthcareusingReverseTimeAttentionMechanism.InAd-vancesinNeuralInformationProcessingSystems,pages3504–3512,2016.[5]J.S.Chung,A.Senior,O.Vinyals,andA.Zisserman.Lipreadingsen-tencesinthewild.InIEEEConferenceonComputerVisionandPatternRecognition,2017.[6]AlejandroGonzález,GabrielVillalonga,JiaolongXu,DavidVázquez,JaumeAmores,andAntonioMLópez.Multiviewrandomforestoflocalexpertscombiningrgbandlidardataforpedestriandetection.InIntelligentVehiclesSymposium(IV),2015IEEE,pages356–361.IEEE,2015.20[7]AlexGraves,GregWayne,andIvoDanihelka.Neuralturingmachines.arXivpreprintarXiv:1410.5401,2014.[8]AlexGraves,GregWayne,MalcolmReynolds,TimHarley,IvoDani-helka,AgnieszkaGrabska-Barwińska,SergioGómezColmenarejo,EdwardGrefenstette,TiagoRamalho,JohnAgapiou,etal.Hybridcomputingusinganeuralnetworkwithdynamicexternalmemory.Nature,538(7626):471–476,2016.[9]SeppHochreiterandJürgenSchmidhuber.Longshort-termmemory.Neu-ralcomputation,9(8):1735–1780,1997.[10]ZacharyCLipton,DavidCKale,CharlesElkan,andRandallWetzell.Learningtodiagnosewithlstmrecurrentneuralnetworks.ICLR,2016.[11]OscarLuaces,JorgeDíez,JoséBarranquero,JuanJosédelCoz,andAn-tonioBahamonde.Binaryrelevanceeﬃcacyformultilabelclassiﬁcation.ProgressinArtiﬁcialIntelligence,1(4):303–313,2012.[12]FenglongMa,RadhaChitta,JingZhou,QuanzengYou,TongSun,andJingGao.Dipole:Diagnosispredictioninhealthcareviaattention-basedbidirectionalrecurrentneuralnetworks.InProceedingsofthe23rdACMSIGKDDInternationalConferenceonKnowledgeDiscoveryandDataMin-ing,pages1903–1911.ACM,2017.[13]Louis-PhilippeMorency,RadaMihalcea,andPayalDoshi.Towardsmulti-modalsentimentanalysis:Harvestingopinionsfromtheweb.InProceed-ingsofthe13thinternationalconferenceonmultimodalinterfaces,pages169–176.ACM,2011.[14]PhuocNguyen,TruyenTran,NilminiWickramasinghe,andSvethaVenkatesh.Deepr:AConvolutionalNetforMedicalRecords.JournalofBiomedicalandHealthInformatics,21(1),2017.[15]LiqiangNie,LumingZhang,YiYang,MengWang,RichangHong,andTat-SengChua.Beyonddoctors:futurehealthpredictionfrommultimediaandmultimodalobservations.InProceedingsofthe23rdACMinternationalconferenceonMultimedia,pages591–600.ACM,2015.[16]KamalNigamandRayidGhani.Analyzingtheeﬀectivenessandapplica-bilityofco-training.InProceedingsoftheninthinternationalconferenceonInformationandknowledgemanagement,pages86–93.ACM,2000.[17]TrangPham,TruyenTran,DinhPhung,andSvethaVenkatesh.Predictinghealthcaretrajectoriesfrommedicalrecords:Adeeplearningapproach.JournalofBiomedicalInformatics,69:218–229,May2017.21[18]SoujanyaPoria,ErikCambria,andAlexanderGelbukh.Deepconvolutionalneuralnetworktextualfeaturesandmultiplekernellearningforutterance-levelmultimodalsentimentanalysis.InProceedingsofthe2015confer-enceonempiricalmethodsinnaturallanguageprocessing,pages2539–2544,2015.[19]SoujanyaPoria,ErikCambria,DevamanyuHazarika,NavonilMajumder,AmirZadeh,andLouis-PhilippeMorency.Context-dependentsentimentanalysisinuser-generatedvideos.InProceedingsofthe55thAnnualMeet-ingoftheAssociationforComputationalLinguistics(Volume1:LongPa-pers),volume1,pages873–883,2017.[20]AadityaPrakash,SiyuanZhao,SadidAHasan,VivekVDatla,KathyLee,AshequlQadir,JoeyLiu,andOladimejiFarri.Condensedmemorynetworksforclinicaldiagnosticinferencing.InAAAI,pages3274–3280,2017.[21]N.Quadrianto,A.J.Smola,T.S.Caetano,andQ.V.Le.Estimatingla-belsfromlabelproportions.TheJournalofMachineLearningResearch,10:2349–2374,2009.[22]ShyamSundarRajagopalan,Louis-PhilippeMorency,TadasBaltrusaitis,andRolandGoecke.Extendinglongshort-termmemoryformulti-viewstructuredlearning.InEuropeanConferenceonComputerVision,pages338–353.Springer,2016.[23]AlainRakotomamonjy,FrancisBach,StéphaneCanu,andYvesGrand-valet.Moreeﬃciencyinmultiplekernellearning.InProceedingsofthe24thinternationalconferenceonMachinelearning,pages775–782.ACM,2007.[24]JesseRead,BernhardPfahringer,GeoﬀHolmes,andEibeFrank.Classiﬁerchainsformulti-labelclassiﬁcation.Machinelearning,85(3):333–359,2011.[25]YaleSong,Louis-PhilippeMorency,andRandallDavis.Multi-viewlatentvariablediscriminativemodelsforactionrecognition.InComputerVisionandPatternRecognition(CVPR),2012IEEEConferenceon,pages2120–2127.IEEE,2012.[26]SainbayarSukhbaatar,arthurszlam,JasonWeston,andRobFergus.End-to-endmemorynetworks.InC.Cortes,N.D.Lawrence,D.D.Lee,M.Sugiyama,andR.Garnett,editors,AdvancesinNeuralInformationProcessingSystems28,pages2440–2448.CurranAssociates,Inc.,2015.[27]MarieSzafranski,YvesGrandvalet,andAlainRakotomamonjy.Compositekernellearning.Machinelearning,79(1-2):73–103,2010.[28]MarthaWhite,XinhuaZhang,DaleSchuurmans,andYao-liangYu.Con-vexmulti-viewsubspacelearning.InAdvancesinNeuralInformationPro-cessingSystems,pages1673–1681,2012.22[29]ChangXu,DachengTao,andChaoXu.Asurveyonmulti-viewlearning.arXivpreprintarXiv:1304.5634,2013.[30]AmirZadeh,MinghaiChen,SoujanyaPoria,ErikCambria,andLouis-PhilippeMorency.Tensorfusionnetworkformultimodalsentimentanaly-sis.arXivpreprintarXiv:1707.07250,2017.[31]AmirZadeh,RowanZellers,EliPincus,andLouis-PhilippeMorency.Mul-timodalsentimentintensityanalysisinvideos:Facialgesturesandverbalmessages.IEEEIntelligentSystems,31(6):82–88,2016.[32]YutaoZhang,RobertChen,JieTang,WalterFStewart,andJimengSun.Leap:Learningtoprescribeeﬀectiveandsafetreatmentcombinationsformultimorbidity.InProceedingsofthe23rdACMSIGKDDInternationalConferenceonKnowledgeDiscoveryandDataMining,pages1315–1324.ACM,2017.23
