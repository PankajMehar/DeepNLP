LINKS: A HIGH-DIMENSIONAL ONLINE CLUSTERING METHOD
Philip Andrew Mansﬁeld1 Quan Wang1 Carlton Downey2
Li Wan1
Ignacio Lopez Moreno1
1Google Inc., USA
2Carnegie Mellon University, USA
1 { memes, quanw, liwan, elnota } @google.com
2 cmdowney@cs.cmu.edu
ABSTRACT
We present a novel algorithm, called Links, designed to perform
online clustering on unit vectors in a high-dimensional Euclidean
space. The algorithm is appropriate when it is necessary to clus-
ter data efﬁciently as it streams in, and is to be contrasted with tra-
ditional batch clustering algorithms that have access to all data at
once. For example, Links has been successfully applied to embed-
ding vectors generated from face images or voice recordings for the
purpose of recognizing people, thereby providing real-time identiﬁ-
cation during video or audio capture.
1. INTRODUCTION
Although a wide selection of clustering methods are available [1, 2],
most of them assume concurrent access to all data being clustered.
Our interest is in efﬁciently clustering each datum as it becomes
available, for applications that require unsupervised learning in real
time.
The Links approach is to estimate the probability distribution
of each cluster based on its current constituent vectors, to use those
estimates to assign new vectors to clusters, and to update estimated
distributions with each added vector. The update step includes ﬁx-
ing past cluster assignments where indicated by taking the additional
data into account, although this is primarily to improve the internal
model over time, since in typical online usage scenarios, each clus-
ter assignment is provided once, at the time a new vector is made
available.
Prior work [3] addressing online clustering of unit vectors
employs a small-variance approximation and is applied to low-
dimensional problems such as segmentation of surface normals
in 3D. Our approach is complementary in that it uses a high-
dimensional approximation, and has been applied to problems with
relatively high variance.
Links has been used to cluster CNN-based FaceNet embeddings
[4] and LSTM-based voice embeddings [5]. The results of the latter
experiment are presented in a separate paper [6]. The current paper
focuses on the technical details of the algorithm.
2. MODEL
2.1. Generative model for a cluster
Let X = {xi} be a set of unit-length vectors in RN . They are
conﬁned to the submanifold SN−1, and to determine proximity for
the purpose of clustering these vectors, we will use the natural metric
on this submanifold, which is simply the angle between vectors:
∠(x, x
′) = arccos(x · x
′).
(1)
We address the problem of cluster distributions within this sub-
manifold with the following properties:
1. Each cluster has a center vector µ and its member vectors
x are generated by a probability density that is isotropic in
the sense that it only depends on distance from the center,
x ∼ ρ(x; µ) = f (∠(x, µ)).
2. The function f is the same for every cluster, so that probabil-
ity densities for different clusters are related by isometry.
3. f (θ) decreases exponentially with θ; for example, as a Gaus-
sian suitably normalized on SN−1:
f (θ) ∝ e
θ2
2σ2 .
(2)
This ensures that the distribution is reasonably localized,
since the exponential decrease compensates for a polynomial
factor in the marginal distribution of θ:
ρ(∠(x, µ) = θ) = A(sin θ)N−2f (θ)
(3)
where A is a constant equal to the hypersurface area of SN−2,
A =
(4)
2π(N−1)/2
Γ(cid:0) N−1
2 (cid:1)
4. The prior distribution ρ(µ) for the center of a cluster µ is
constant on SN−1 (no unit vector is preferred).
2.2. Estimated distribution
Given a set X = {xi} chosen randomly from the same cluster, but
without knowledge of the center of the cluster, we would like to
estimate the cluster’s probability distribution. The likelihood of the
center value µ is
L(µ; X) =Yi
ρ(xi; µ)
Pi(∠(xi,µ))2
∝ e
2σ2
(5)
Since the prior ρ(µ) is constant, the posterior ρ(µ|X) is also pro-
portional to the expression in equation 5. The maximum likelihood
(and maximum a posteriori) center is therefore
ˆµ = argmin
µ Xi (cid:0)∠(xi, µ)(cid:1)2
(6)
which is the same as the centroid of the vectors {xi} as deﬁned for a
hypersphere according to [7]. The estimated probability distribution
for the cluster is
ˆρ(x; X) ∝ZSN −1
∝ZSN −1
L(µ; X)ρ(x; µ)dµ
(∠(x,µ))2+Pi(∠(xi,µ))2
(7)
2σ2
dµ.
The probability that a new vector x belongs to the same cluster can
then be estimated as the cumulative amount
strictly increasing function of k, the variance of the estimated distri-
bution decreases with k.
Z{y∈SN −1 | ˆρ(y;X)≥ ˆρ(x;X)}
ˆρ(y; X)dy.
(8)
Similarly, to assess whether two clusters are the same, we de-
termine a threshold on the cosine similarity between their centroids
µc · µ
c ≥ s(k, k′) where, for N ≫ k and N ≫ k′,
2.3. High-dimensional approximation
Our primary interest is in problems with relatively large N . For
example, our typical embedding vectors have N ≥ 128. For large
enough N , the following are true:
Lemma 1 Two randomly chosen vectors x, x′ are almost always
almost perpendicular, i.e.,
s(k, k′) =
q(cid:0)1 + 1
s(cid:18)1 + 1
k tan2 θc(cid:1)(cid:0)1 + 1
k (cid:16) 1
− 1(cid:17)(cid:19)(cid:18)1 + 1
k′ tan2 θc(cid:1)
k′ (cid:16) 1
T 2
T 2
(16)
− 1(cid:17)(cid:19)
P (x · x
′ > δ) < ǫ
(9)
Note that equation 13 is the special case with k′ = 1,
for some positive numbers δ ≪ 1 and ǫ ≪ 1.
Lemma 2 The angle θ between a cluster center and a random vector
from that cluster is almost always almost equal to a global
constant θc, i.e.,
P (|θ − θc| > δ) < ǫ
(10)
for some positive numbers δ ≪ π and ǫ ≪ 1.
Lemma 3 Given two randomly chosen vectors from a cluster with
center µ, their components perpendicular to µ will almost al-
ways be almost perpendicular to each other, i.e.,
P(cid:16)(cid:0)x − (x · µ)µ(cid:1) ·(cid:0)x
′ − (x
′ · µ)µ(cid:1) > δ(cid:17) < ǫ
for some positive numbers δ ≪ 1 and ǫ ≪ 1.
(11)
To assess whether to add a new vector x to an existing cluster
known to include the k vectors {xi}k
i=1, we determine a threshold
x · ˆµ ≥ s(k) on the cosine similarity between the new vector and
the centroid ˆµ of the existing vectors. Using the approximation in
lemmas 2 and 3, and assuming N ≫ k, we can compute vector
(x−cos θcµ)
components in an orthonormal basis including µ,
sin θc
andn 1
sin θc
(xi − cos θcµ)ok
i=1
. This yields
ˆµ =
and a threshold of
pk2 cos2 θc + k sin2 θc
xi
Xi=1
(12)
(13)
s(k) =
k cos2 θc
T 2
pk2 cos2 θc + k sin2 θc
q 1
k +(cid:0)1 − 1
k(cid:1) T 2
where Tc = cos θc, which we call the cluster similarity threshold.
Note that
and
lim
k→∞
ˆµ = µ
lim
k→∞
s(k) = Tc,
(14)
(15)
which conﬁrms that as we accumulate more vectors in a given clus-
ter, the center and cosine similarity threshold of the estimated dis-
tribution approach the center and cosine similarity threshold of the
generative distribution (i.e., the estimate improves). Since s(k) is a
and
s(k, 1) = s(k),
lim
k,k′→∞
s(k, k′) = 1.
(17)
(18)
The latter conﬁrms that the centers estimated from the two sets of
cluster points converge.
3. ALGORITHM
3.1. Online clustering
Each new input vector is assigned to a cluster as soon as it is pro-
duced, with no knowledge of future vectors and no backtracking. A
unique ID for that cluster is returned. The clusterer keeps statisti-
cal information about the vectors received so far. Although it cannot
change a previous answer, it can change the internal representation
of cluster statistics, such as improvements to estimated distributions
as well as cluster splits and merges when indicated by new informa-
tion.
3.2. Internal representation
The Links algorithm’s internal representation is a two-level hierar-
chy: clusters are collections of subclusters, and subclusters are col-
lections of input vectors. The subclusters are represented as nodes in
a graph whose edges join ‘nearby’ nodes (meaning subclusters that
likely belong to the same cluster given the data so far), and clusters
are deﬁned as connected components of the graph. Whereas sub-
clusters are indivisible, clusters can become split along graph edges
in response to changes in subcluster estimated probability distribu-
tions as new data is added. Alternatively, subclusters joined by an
edge can become merged in response to changes.
The reasons for maintaining this two-level hierarchy (rather
than, say, an arbitrary number of levels) are efﬁciency and practi-
cality. It is efﬁcient because the algorithm scales with number of
subclusters rather than number of vectors. It is practical because the
key cluster substructure that can affect future cluster IDs is the set
of potential split points.
3.3. Assessing cluster membership
When a new vector x is available, compute its cosine similarity to
each subcluster centroid ˆµj, and add it to the most-similar subcluster
if the similarity is above a ﬁxed threshold Ts. In other words, let
J = argmax
{x · ˆµj}.
(19)
If
3.6. Hyperparameter Tuning
The similarity thresholds Tc, Ts and Tp need to be tuned to best rep-
resent the data source. This is done by manually labeling a dataset
with cluster IDs, running the clusterer on the data, and adjusting
hyperparameters to improve the accuracy of the output cluster IDs.
Accuracy is simply fraction of correct IDs. Prior to evaluation, the
Hungarian algorithm [8] is used to map a subset of output cluster
IDs bijectively to a subset of ground truth cluster IDs in such a way
that produces the best possible accuracy. For some applications an
alternate objective has been used; for example, one that gives dif-
ferent weights for conﬂating IDs vs. fracturing IDs, to reﬂect the
seriousness of each type of error in practise.
4. ACKNOWLEDGEMENTS
The authors would like to thank Dr. Brian Budge and Dr. Navid
Shiee for help with APIs and evaluation frameworks used in the im-
plementation of the Links algorithm.
x · ˆµJ ≥ Ts
(20)
then add x to subcluster J. Ts, called the subcluster similarity
threshold, is a hyperparameter determining the granularity of cluster
substructure appropriate for the data.
If inequality 20 does not hold, then start a new subcluster con-
taining just x. Next, use the estimated probability distribution of
subcluster J to determine whether to include the new subcluster in
the same cluster as J, by thresholding the cumulative probability in
expression 8. In the high-dimensional approximation, this means the
subcluster is included in the cluster whenever
x · ˆµJ ≥ s(kJ )
(21)
where kJ is the number of vectors in the subcluster J. To a ﬁrst
approximation, s(k) is as given in equation 13. This will be further
reﬁned in section 3.5. If inequality 21 does hold, then add an edge
to the graph joining the new subcluster to subcluster J.
3.4. Updating clusters
When a new vector is added to an existing subcluster, the subclus-
ter’s centroid may change. If this brings it within the subcluster simi-
larity threshold of the centroid of another subcluster currently joined
to the ﬁrst by an edge, then the two are merged. In other words, if
ˆµi · ˆµj ≥ Ts, then nodes i and j are replaced with a single node con-
taining the vectors of both, and with the edge connections of both.
Since the merging process also results in a new subcluster centroid,
this check is continued recursively on affected subclusters.
Next, the edges joining affected nodes are checked for validity.
The edge joining subclusters i and j is removed if the following does
not continue to hold:
ˆµi · ˆµj ≥ s(ki, kj)
(22)
where s(ki, kj) is approximately as given in equation 16, but with
improvements to follow in section 3.5. After severing a cluster in
two by removing an edge, an attempt is made to re-join the two parts
by adding an edge from the affected node to a new partner node
that does satisfy inequality 22. If no such partner is found, then the
cluster remains permanently split.
3.5. Anisotropy
Equations 13 and 16 were used to determine thresholds for member-
ship in the same cluster as a given subcluster, effectively treating the
subcluster’s members as randomly chosen from the cluster and not
correlated with each other. If one were to properly take into account
intra-subcluster correlations, then one consequence is that the limit
in equation 18 would be reduced to a positive number Tp < 1, which
we call the pair similarity maximum,
lim
k,k′→∞
s(k, k′) = Tp,
(23)
whereas the value of s(1, 1), which is T 2
c , would remain unchanged.
Any implicit anisotropy in the cluster distribution, such as an elon-
gation along a preferred axis, will further reduce the value of Tp
without changing s(1, 1). A simple though approximate way to in-
corporate these adjustments into the algorithm is to replace s(k, k′)
and s(k) by the following interpolated versions:
˜s(k, k′) = T 2
c +
˜s(k) = ˜s(k, 1).
Tp − T 2
1 − T 2
c (cid:0)s(k, k′) − T 2
c(cid:1)
(24)
(25)
5. REFERENCES
[1] Brian Everitt, Cluster Analysis, John Wiley & Sons, 2011.
[2] Christian Hennig, Marina Meila, Fionn Murtagh, and Roberto
Rocci, Handbook of Cluster Analysis, Chapman and Hall/CRC,
December 2015.
[3] Julian Straub, Trevor Campbell, Jonathan P. How, and John W.
Fisher, “Small-variance nonparametric clustering on the hyper-
sphere,” in 2015 IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), June 2015, pp. 334–342.
[4] F. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: A uni-
ﬁed embedding for face recognition and clustering,” in 2015
IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2015, pp. 815–823.
[5] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez Moreno,
“Generalized end-to-end loss for speaker veriﬁcation,” arXiv
preprint arXiv:1710.10467, 2017.
[6] Quan Wang, Carlton Downey, Li Wan, Philip Andrew Mans-
“Speaker diarization with
ﬁeld, and Ignacio Lopez Moreno,
lstm,” arXiv preprint arXiv:1710.10468, 2017.
[7] Samuel R. Buss and Jay P. Fillmore, “Spherical averages and
applications to spherical splines and interpolation,” ACM Trans-
actions on Graphics, vol. 20, no. 2, pp. 95–126, 2001.
[8] Harold W. Kuhn, “The hungarian method for the assignment
problem,” Naval Research Logistics Quarterly, vol. 2, pp. 83–
97, 1955.
