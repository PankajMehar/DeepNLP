3D Object Dense Reconstruction
from a Single Depth View
Bo Yang, Stefano Rosa, Andrew Markham, Niki Trigoni, Hongkai Wen*
Abstract—In this paper, we propose a novel approach, 3D-RecGAN++, which reconstructs the complete 3D structure of a given object
from a single arbitrary depth view using generative adversarial networks. Unlike existing work which typically requires multiple views of
the same object or class labels to recover the full 3D geometry, the proposed 3D-RecGAN++ only takes the voxel grid representation of a
depth view of the object as input, and is able to generate the complete 3D occupancy grid with a high resolution of 2563 by recovering
the occluded/missing regions. The key idea is to combine the generative capabilities of autoencoders and the conditional Generative
Adversarial Networks (GAN) framework, to infer accurate and ﬁne-grained 3D structures of objects in high-dimensional voxel space.
Extensive experiments on large synthetic datasets and real-world Kinect datasets show that the proposed 3D-RecGAN++ signiﬁcantly
outperforms the state of the art in single view 3D object reconstruction, and is able to reconstruct unseen types of objects.
Index Terms—3D Reconstruction, Shape Completion, Shape inpainting, Single Depth View, Adversarial Learning, Conditional GAN.
1 INTRODUCTION
T O reconstruct the complete and precise 3D geometry of
an object is essential for many graphics and robotics
applications, from AR/VR [1] and semantic understand-
ing, to robot grasping [2] and obstacle avoidance. Classic
approaches use the off-the-shelf low-cost depth sensing
devices such as Kinect and RealSense cameras to recover
the 3D shape of an object from captured depth images.
Those approaches typically require multiple depth images
from different viewing angles of an object to estimate the
complete 3D structure [3] [4] [5]. However, in practice it is
not always feasible to scan all surfaces of an object before
reconstruction, which leads to incomplete 3D shapes with
occluded regions and large holes. In addition, acquiring and
processing multiple depth views require more computing
power, which is not ideal in many applications that require
real-time performance.
In this paper, we aim to tackle the problem of estimating
the complete 3D structure of an object using a single depth
view. This is a very challenging task, since the partial
observation of the object (i.e. a depth image from one view-
ing angle) can be theoretically associated with an inﬁnite
number of possible 3D models. Traditional reconstruction
approaches typically use interpolation techniques such as
plane ﬁtting, Laplacian hole ﬁlling [6] [7], or Poisson surface
estimation [8] [9] to infer the underlying 3D structure. How-
ever, they can only recover very limited occluded or missing
regions, e.g. small holes or gaps due to quantization arti-
facts, sensor noise and insufﬁcient geometry information.
Interestingly, humans are surprisingly good at solving
such ambiguity by implicitly leveraging prior knowledge.
• Bo Yang, Stefano Rosa, Andrew Markham and Niki Trigoni are with the
Department of Computer Science, University of Oxford, UK.
E-mail:{bo.yang,stefano.rosa,andrew.markham,niki.trigoni}@cs.ox.ac.uk
• Corresponding to Hongkai Wen, who is with the Department of Computer
Science, University of Warwick, UK.
E-mail: hongkai.wen@dcs.warwick.ac.uk
For example, given a view of a chair with two rear legs
occluded by front legs, humans are easily able to guess the
most likely shape behind the visible parts. Recent advances
in deep neural networks and data driven approaches show
promising results in dealing with such a task.
In this paper, we aim to acquire the complete and high-
resolution 3D shape of an object given a single depth view.
By utilizing the high performance of 3D convolutional neu-
ral nets and large open datasets of 3D models, our approach
learns a smooth function to map a 2.5D view to a complete
and dense 3D shape. In particular, we train an end-to-end
model which estimates full volumetric occupancy from a
single 2.5D depth view of an object.
While state-of-the-art deep learning approaches [10] [11]
[2] for 3D shape reconstruction from a single depth view
achieve encouraging results, they are limited to very small
resolutions, typically at the scale of 323 voxel grids. As
a result, the learnt 3D structure tends to be coarse and
inaccurate. However, to increase the 3D shape resolution
without sacriﬁcing recovery accuracy is challenging, as even
a slightly higher resolution would exponentially increase
the search space of potential 2.5D to 3D mapping functions,
resulting in difﬁculties in convergence of neural nets.
Recently, deep generative models achieve impressive
success in modeling complex high-dimensional data dis-
tributions, among which Generative Adversarial Networks
(GANs) [12] and Variational Autoencoders (VAEs) [13]
emerge as two powerful frameworks for generative learn-
ing, including image and text generation [14] [15], and latent
space learning [16] [17]. In the past few years, a number of
works [18] [19] [20] [21] applied such generative models to
learn latent space to represent 3D object shapes, in order to
solve simple discriminative tasks such as new image gener-
ation, object classiﬁcation, recognition and shape retrieval.
In this paper, we propose 3D-RecGAN++, a novel model
that combines a skip-connected 3D autoencoder with adver-
sarial learning to generate a complete and ﬁne-grained 3D
structure conditioned on a single 2.5D view. Particularly, our
model ﬁrstly encodes the 2.5D view to a low-dimensional
latent space vector which implicitly represents general 3D
geometric structures, then decodes it back to recover the
most likely full 3D shape. The rough 3D shape is then
fed into a conditional discriminator which is adversarially
trained to distinguish whether the coarse 3D structure is
plausible or not. The autoencoder is able to approximate the
corresponding shape, while the adversarial training tends
to add ﬁne details to the estimated shape. To ensure the
ﬁnal generated 3D shape corresponds to the input single
partial 2.5D view, adversarial training of our model is based
on a conditional GAN [22] instead of random guessing.
The above novel and efﬁcient network design excels the
competing approaches [2] [11] [23], which either use a single
fully connected layer [2], a low capacity decoder [11], or the
multi-stage and inefﬁcient LSTMs [23] to estimate the full
3D shapes.
Our contributions are as follows:
(1) We propose a novel generative model to reconstruct
the complete and accurate 3D structure using a single ar-
bitrary depth view. Particularly, our model takes a simple
occupancy grid map as input without requiring object class
labels or any annotations, while predicting a compelling
shape within a high resolution of 2563 voxel grid. By
drawing on both autoencoder and GAN, our approach is
end-to-end trainable with high level of generality. To the
best of our knowledge, this is the ﬁrst work that reconstructs
such high resolution of 3D shapes using a single view.
(2) We exploit conditional GAN during training to reﬁne
the 3D shape estimated by the autoencoder. Our contribu-
tion here is that we use the mean value of a latent vector
feature, instead of a single scalar, as the output of the
discriminator to stabilize GAN training.
(3) We conduct extensive experiments for single category
and multi-category object reconstruction, outperforming the
state of the art. Importantly, our approach is also able to
generalize to previously unseen object categories. At last,
our model also performances robustly on real-world dataset
collected by Kinect, after being trained purely on synthetic
datasets.
(4) To the best of our knowledge, there are no good open
datasets which have the ground truth for occluded/missing
parts and holes for each 2.5D view in real world scenarios.
We therefore contribute our real world testing dataset to the
community.
A preliminary version of this work has been published in
ICCV 2017 workshops [24]. Our code and data are available
at: https://github.com/Yang7879/3D-RecGAN-extended
2 RELATED WORK
We review different pipelines for 3D reconstruction or shape
completion. Both conventional geometry based techniques
and the state of the art deep learning based approaches are
covered.
(1) 3D Model/Shape Completion. [25] uses plane ﬁtting
to complete small missing regions, while [26] [27] [28]
[29] [30] apply shape symmetry to ﬁll in holes. Although
these methods show good results, relying on predeﬁned
geometric regularities fundamentally limits the structure
space to hand-crafted shapes. Besides, these approaches are
likely to fail when missing or occluded regions are relatively
big. Another similar ﬁtting pipeline is to leverage database
priors. Given a partial shape input, [31] [32] [33] [34] [35]
[36] try to retrieve an identical or most likely 3D model and
align it with the partial scan. However, these approaches
explicitly assume the database contains identical or very
similar shapes, thus being unable to generalize to novel
objects or categories.
(2) Multiple RGB/Depth Images Reconstruction. Tra-
ditionally, 3D dense reconstruction requires a collection of
RGB images [37]. Geometric shape is recovered by dense
feature extraction and matching [38], or by directly minimiz-
ing reprojection errors [39] from color images. Recently, [40]
[41] [42] [43] [44] [45] leverage deep neural nets to learn the
3D shape from multiple RGB images. However, resolution
of the recovered occupancy shape is usually up to a small
scale of 323. With the advancement of depth sensors, depth
images are also used to recover the object shape. Classic
approaches usually fuse multiple depth images through
iterative closest point (ICP) algorithms [3] [46] [47], while
recent work [48] learns the 3D shape using deep neural nets
from multiple depth views.
(3) Single RGB Image Reconstruction. Predicting a
complete 3D object model from a single view is a long-
standing and extremely challenging task. When reconstruct-
ing a speciﬁc object category, model templates can be used.
For example, morphable 3D models are exploited for face
recovery [49] [50]. This concept was extended to reconstruct
simple objects in [51]. For general and complex object recon-
struction from a single RGB image, recent works [52] [53]
[54] aim to infer 3D shapes using multiple RGB images for
weak supervision. However, the training procedure of [54]
is two stage rather than end-to-end, while [53] uses a sim-
ple autoencoder instead of designing sophisticated learn-
ing frameworks for shape learning, and [52] still requires
3D shape priors for constraints. Shape prior knowedge is
also required in [55] [56] [57]. To recover high resolution
3D shapes, [58] [59] use Octree representation, while [60]
proposed an inverse discrete cosine transform (IDCT) tech-
nique. Lin et al. [61] designed a pseudo-renderer to predict
dense 3D shapes, while [62] sequentially estimates 2.5D
sketches and dense 3D shapes from a single RGB image.
(4) Single Depth View Reconstruction. The task of
reconstruction from a single depth view is to complete
the occluded 3D structures behind the visible parts. 3D
ShapeNets [10] is among the early work using deep neural
nets to estimate 3D shapes from a single depth view. Firman
et al. [63] trained a random decision forest to infer unknown
voxels. Originally designed for shape denoising, VConv-
DAE [1] can also be used for shape completion. To facilitate
robotic grasping, Varley et al. proposed a neural network
to infer the full 3D shape from a single depth view in [2].
However, all these approaches are only able to generate low
resolution voxel grids which are less than 403 and unlikely
to capture ﬁne geometric details. Recent works [11] [64] [23]
[65] can infer higher resolution 3D shapes. However, 3D-
EPN [11] relies on a shape database to synthesize higher
resolution shapes after learning a small 323 voxel grid from
a depth view, while SSCNet [64] requires strong voxel-level
annotations for supervised scene completion and semantic
3
Fig. 2: Overview of the network architecture for training.
Fig. 3: Overview of the network architecture for testing.
After generating training pairs, we feed them into our
network. The ﬁrst part of our network loosely follows the
idea of an autoencoder with the U-net architecture [74].
The skip-connected autoencoder serves as an initial coarse
generator which is followed by an up-sampling module to
further generate a high resolution 3D shape within a 2563
voxel grid. This whole generator aims to learn a correlation
between partial and complete 3D structures. With the super-
vision of complete 3D labels, the generator is able to learn
a function f and infer a reasonable 3D shape given a brand
new partial 2.5D view. In the testing phase, however, the
results tend to be grainy and without ﬁne details.
To address this issue,
in the training phase, the re-
constructed 3D shape from the generator is further fed
into a conditional discriminator to verify its plausibility.
In particular, a partial 2.5D input view is paired with its
corresponding complete 3D shape, which is called the ‘real
reconstruction’, while the partial 2.5D view is paired with
its corresponding output 3D shape from generator, which
is called the ‘fake reconstruction’. The discriminator aims
to discriminate all ‘fake reconstruction’ from ‘real recon-
struction’. In the original GAN framework [12], the task
of the discriminator is to simply classify real and fake in-
put, but its Jensen-Shannon divergence-based loss function
is difﬁcult to converge. The recent WGAN [75] leverages
Wasserstein distance with weight clipping as a loss function
to stabilize the training procedure, whilst the extended work
WGAN-GP [76] further improves the training process using
a gradient penalty with respect to its input. In our 3D-
RecGAN++, we apply WGAN-GP as the loss function of our
conditional discriminator, which guarantees fast and stable
convergence. The overall network architecture for training
is shown in Figure 2, while the testing phase only needs the
well trained generator as shown in Figure 3.
Overall, the main challenge of 3D reconstruction from an
arbitrary single view is to generate new information includ-
ing ﬁlling the missing and occluded regions from unseen
views, while keeping the estimated 3D shape corresponding
to the speciﬁc input 2.5D view. In the training phase, our
3D-RecGAN++ ﬁrstly leverages a skip-connected autoen-
coder together with an up-sampling module to generate
a reasonable ‘fake reconstruction’ within a high resolution
occupancy grid, then applies adversarial learning to reﬁne
the ‘fake reconstruction’ to make it as similar to ‘real recon-
struction’ by jointly updating parameters of the generator.
Fig. 1: t-SNE embeddings of 2.5D partial views and 3D
complete shapes of multiple object categories.
label prediction. Both [23] and [65] are originally designed
for shape inpainting instead of directly reconstructing the
complete 3D structure from a partial depth view. The re-
cent 3D-PRNN [66] predicts simple shape primitives using
RNNs, but the estimated shapes do not have ﬁner geometric
details.
(5) Deep Generative Frameworks. Deep generative
frameworks, such as VAEs [13] and GANs [12], have
achieved impressive success in image super-resolution [67],
image generation [15], text to image synthesis [68], etc.
Recently, [69] [70] [71] [21] applied generative networks for
3D structure generation. However, most of them generate
3D shapes from random noise instead of reconstructing
structures from a speciﬁc single image.
3 3D-RECGAN++
3.1 Overview
Our method aims to estimate a complete and dense 3D
structure of an object, which only takes an arbitrary single
2.5D depth view as input. The output 3D shape is auto-
matically aligned with the corresponding 2.5D partial view.
To achieve this task, each object model is represented in a
high resolution 3D voxel grid. We use the simple occupancy
grid for shape encoding, where 1 represents an occupied cell
and 0 an empty cell. Speciﬁcally, the input 2.5D partial view,
denoted as x, is a 643 occupancy grid, while the output 3D
shape, denoted as y, is a high resolution 2563 probabilistic
voxel grid. The input partial shape is directly calculated
from a single depth image given camera parameters. We use
the ground truth dense 3D shape with aligned orientation
as same as the input partial 2.5D depth view to supervise
our network.
To generate ground truth training and evaluation pairs,
we virtually scan 3D objects from ShapeNet [72]. Figure 1
is the t-SNE visualization [73] of partial 2.5D views and the
corresponding full 3D shapes for multiple general chair and
bed models. Each green dot represents the t-SNE embedding
of a 2.5D view, whilst a red dot is the embedding of the cor-
responding 3D shape. It can be seen that multiple categories
inherently have similar 2.5D to 3D mapping relationships.
Essentially, our neural network is to learn a smooth function,
denoted as f, which maps green dots to red dots as close as
possible in high dimensional space as shown in Equation 1.
The function f is parametrized by convolutional layers in
general.
, where Z = {0, 1}(cid:17)
(1)
(cid:16)
y = f (x)
x ∈ Z 643
encoderdecoderconditional discriminatorU-netinput 2.5D viewtrue full 3D shapereal reconstructionfake reconstructionxconcatconcatlossup-samplingyencoderdecoderU-netinput 2.5D viewup-samplingxyIn the testing phase, given a novel 2.5D view as input, the
jointly trained generator is able recover a full 3D shape with
satisfactory accuracy, while the discriminator is no longer
used.
3.2 Architecture
Figure 4 shows the detailed architecture of our proposed 3D-
RecGAN++. It consists of two main networks: the generator
as in Figure 4a and the discriminator as in Figure 4b.
The generator consists of a skip-connected autoencoder
and an up-sampling module. Unlike the vanilla GAN gen-
erator which generates data from arbitrary latent distribu-
tions, our 3D-RecGAN++ generator synthesizes data from
latent distributions of 2.5D views. Particularly, the encoder
has ﬁve 3D convolutional layers, each of which has a bank of
4x4x4 ﬁlters with strides of 1x1x1, followed by a leaky ReLU
activation function and a max pooling layer with 2x2x2
ﬁlters and strides of 2x2x2. The number of output channels
of max pooling layer starts with 64, doubling at each sub-
sequent layer and ends up with 512. The encoder is lastly
followed by two fully-connected layers to embed semantic
information into a latent space. The decoder is composed
of 5 symmetric up-convolutional layers which are followed
by ReLU activations. Skip-connections between encoder and
decoder guarantee propagation of local structures of the
input 2.5D view. The skip-connected autoencoder is fol-
lowed by the up-sampling module which simply consists of
two layers of up-convolutional layers as detailed in Figure
4a. This simple yet efﬁcient up-sampling module directly
upgrades the output 3D shape to a high resolution of 2563
without requiring complex network design and operations.
It should be noted that without the two fully connected
layers and skip-connections, the vanilla autoencoder would
be unable to learn reasonable complete 3D structures as
the latent space is limited and the local structure is not
preserved. Without the efﬁcient up-sampling module, it is
unable to ﬁnally generate high resolution 3D shapes. Al-
though a more complicated and dedicated network design
could also output 2563 shapes, it would be unlikely to
be effectively trained on a single GPU because of the ex-
tremely high computation consumption for high resolution
3D shape generation. The loss function and optimization
methods are described in Section 3.3.
The discriminator aims to distinguish whether the esti-
mated 3D shapes are plausible or not. Based on the condi-
tional GAN, the discriminator takes both real reconstruction
pairs and fake reconstruction pairs as input. Particularly, it
consists of six 3D convolutional layers, the ﬁrst of which
concatenates the generated 3D shape (i.e. a 2563 voxel grid)
and the input 2.5D partial view (i.e. a 643 voxel grid) which
is reshaped as a 256x256x4 tensor. The reshaping process
is done straightforwardly using Tensorﬂow ‘tf.reshape()’.
Basically, this is to inject the condition information with
a matched tensor dimension, and then leave the network
itself to learn useful features from this condition input. Each
convolutional layer has a bank of 4x4x4 ﬁlters with strides of
2x2x2, followed by a ReLU activation function except for the
last layer which is followed by a sigmoid activation func-
tion. The number of output channels of the convolutional
layers starts with 8, doubling at each subsequent layer and
ends up with 256.
At the early stage of GAN training, the high dimen-
sional real and fake distributions may not overlap, then
the discriminator can separate them perfectly using a single
scalar output, which is theoretically analyzed in [77]. In our
experiments, the original WGAN-GP always crashes in the
early 3 epochs due to the extremely high dimensionality (i.e.
2563 + 643 dimensions). To stabilize it, we propose to use
mean feature (i.e. mean of a vector feature) for discriminator.
As the mean vector feature captures more information from
the input overall, it is more difﬁcult for the discriminator
to easily distinguish whether the mean feature is from
fake or real input. This enables useful information to back-
propagate to the generator. A theoretical study of the mean
feature matching method for GAN is in [78]; mean feature
matching is also applied in [79] to stabilize GAN.
Therefore, our discriminator is to distinguish the distri-
butions of mean feature of fake and real reconstructions,
while the generator is trained to make the two distributions
of mean feature as similar as possible. We apply WGAN-GP
as loss functions for our modiﬁed mean feature matching.
3.3 Objectives
The objective function of our 3D-RecGAN++ includes two
main parts: an object reconstruction loss (cid:96)ae for the genera-
tor; the objective function (cid:96)gan for the conditional GAN.
(1) (cid:96)ae For the generator, inspired by [80], we use mod-
iﬁed binary cross-entropy loss function instead of the stan-
dard version. The standard binary cross-entropy weights
both false positive and false negative results equally. How-
ever, most of the voxel grid tends to be empty, so the net-
work easily gets a false positive estimation. In this regard,
we impose a higher penalty on false positive results than
on false negatives. Particularly, a weight hyper-parameter α
is assigned to false positives, with (1-α) for false negative
results, as shown in Equation 2.
−α ¯yi log(yi)−(1−α)(1− ¯yi) log(1−yi)
(2)
(cid:20)
N(cid:88)
i=1
(cid:96)ae =
(cid:21)
where ¯yi is the target value {0,1} of a speciﬁc ith voxel in
the ground truth voxel grid ¯y, and yi is the corresponding
estimated value (0,1) in the same voxel from the autoen-
coder output y. We calculate the mean loss over the total N
voxels in the whole voxel grid.
(2) (cid:96)gan For the discriminator, we leverage the state of
the art WGAN-GP loss functions. Unlike the original GAN
loss function which presents an overall loss for both real
and fake inputs, we separately represent the loss function
(cid:96)g
gan in Equation 3 for generating fake reconstruction pairs
and (cid:96)d
gan in Equation 4 for discriminating fake and real
reconstruction pairs. Detailed deﬁnitions and derivation of
the loss functions can be found in [75] [76], but we modify
them for our conditional GAN settings.
(cid:96)g
gan = −E(cid:2)D(y|x)(cid:3)
gan = E(cid:2)D(y|x)(cid:3) − E(cid:2)D(¯y|x)(cid:3)
(cid:17)2(cid:21)
(cid:20)(cid:16)(cid:13)(cid:13)∇ ˆyD(ˆy|x)(cid:13)(cid:13)2 − 1
+λE
(cid:96)d
(3)
(4)
5
(a) Generator for 3D shape estimation from a single depth view.
(b) Discriminator for 3D shape reﬁnement.
Fig. 4: Detailed architecture of 3D-RecGAN++, showing the two main building blocks. Note that, although these are shown
as two separate modules, they are trained end-to-end.
where ˆy = ¯y + (1 − )y,  ∼ U [0, 1], x is the input partial
depth view, y is the corresponding output of autoencoder,
¯y is the corresponding ground truth. λ controls the trade-off
between optimizing the gradient penalty and the original
objective in WGAN.
in all epochs. As we do not use dropout or batch normal-
ization, the testing phase is exactly the same as the training
stage. The whole network is trained on a single Titan X GPU
from scratch.
For the generator in our 3D-RecGAN++ network, there
are two loss functions, (cid:96)ae and (cid:96)g
gan, to optimize. As we
discussed in Section 3.1, minimizing (cid:96)ae tends to learn the
overall 3D shapes, whilst minimizing (cid:96)g
gan estimates more
plausible 3D structures conditioned on input 2.5D views. To
minimize (cid:96)d
gan is to improve the performance of discrimi-
nator to distinguish fake and real reconstruction pairs. To
jointly optimize the generator, we assign weights β to (cid:96)ae
and (1− β) to (cid:96)g
gan. Overall, the loss functions for generator
and discriminator are as follows:
(cid:96)g = β(cid:96)ae + (1 − β)(cid:96)g
gan
(cid:96)d = (cid:96)d
gan
(5)
(6)
3.4 Training
We adopt an end-to-end training procedure for the whole
network. To simultaneously optimize both generator and
discriminator, we alternate between one gradient decent
step on discriminator and then one step on generator. For
the WGAN-GP, λ is set as 10 for gradient penalty as in [76]. α
ends up as 0.85 for our modiﬁed cross entropy loss function,
while β is 0.2 for the joint loss function (cid:96)g.
The Adam solver [81] is used for both discriminator and
generator with a batch size of 4. The other three Adam
parameters are set to default values. Learning rate is set
to 0.0001 for the discriminator and 0.0005 for the generator
3.5 Data Synthesis
For the task of 3D dense reconstruction from a single depth
view, obtaining a large amount of training data is an obsta-
cle. Existing real RGB-D datasets for surface reconstruction
suffer from occlusions and missing data and there is no
ground truth of complete and high resolution 2563 3D
shapes for each single view. The recent work 3D-EPN [11]
synthesizes data for 3D object completion, but their map
encoding scheme is the complicated TSDF which is different
from our network requirement.
To tackle this issue, we use the ShapeNet [72] database
to generate a large amount of training and testing data with
synthetically rendered depth images and the corresponding
complete 3D shape ground truth. Particularly, a subset of
object categories is selected for our experiments. For each
category, we generate training data from around 220 CAD
models, while synthesizing testing data from around 40
CAD models. For each CAD model, we create a virtual
depth camera to scan it from 125 different angles, 5 uni-
formly sampled views for each of roll, pitch and yaw space.
For each virtual scan, both a depth image and the corre-
sponding complete 3D voxelized structure are generated
with regard to the same camera angle. That depth image
is simultaneously transformed to a partial 2.5D voxel grid
using virtual camera parameters. Then a pair of partial 2.5D
view and the complete 3D shape is synthesized. Overall,
concat 6431 channel 32364 channels 163128 channels 83256 channels 43512 channels32768 2000 3276843512 channels83256 channels163128 channels32364 channels 64316 channels12838 channels25631 channelconcatconcatconcatconcat43 convlrelu23 maxpool43 convlrelu23 maxpool43 convlrelu23 maxpool43 convlrelu23 maxpoolflattendensereludenserelu43 deconvrelureshape43 deconvrelu43 deconvrelu43 deconvrelu43 deconvrelu43 deconvreluyx256x256x41 channelfake reconstructionreal reconstructionground truth128x128x130          8 channels6431 channely25631 channel256x256x2601 channel43 convlrelu43 convlrelu43 convlrelu43 convlrelu43 convlrelu43 convlreluflatten64x64x65     16 channels32x32x33     32 channels16x16x17     64 channels 8x8x9128 channels 4x4x5256 channelsxloss20480reshape25631 channelup-sampling moduleconcat 6431 channel 32364 channels 163128 channels 83256 channels 43512 channels32768 2000 3276843512 channels83256 channels163128 channels32364 channels 64316 channels12838 channels25631 channelconcatconcatconcatconcat43 convlrelu23 maxpool43 convlrelu23 maxpool43 convlrelu23 maxpool43 convlrelu23 maxpoolflattendensereludenserelu43 deconvrelureshape43 deconvrelu43 deconvrelu43 deconvrelu43 deconvrelu43 deconvreluyx256x256x41 channelfake reconstructionreal reconstructionground truth128x128x130          8 channels6431 channely25631 channel256x256x2601 channel43 convlrelu43 convlrelu43 convlrelu43 convlrelu43 convlrelu43 convlreluflatten64x64x65     16 channels32x32x33     32 channels16x16x17     64 channels 8x8x9128 channels 4x4x5256 channelsxloss20480reshape25631 channelup-sampling modulearound 20K training pairs and 4K testing pairs are gener-
ated for each 3D object category. All data are produced in
Blender.
Besides the large quantity of synthesized data, we also
collect real world data in order to test the proposed network.
We use a Microsoft Kinect camera to manually scan a set of
common objects, such as chairs, tables, etc., from multiple
angles. Then, we use ElasticFusion [47] to reconstruct the
full 3D shapes of the objects, as well as the camera pose
in each scan. The 3D objects are manually segmented from
the background. We then extract ground truth information
by aligning the full 3D objects with the partial 2.5D views.
It should be noted that, due to noise and quantization
artifacts of low-cost RGB-D sensors, and the inaccuracy of
the algorithm, the full 3D ground truth is not 100% accurate.
4 EVALUATION
In this section, we evaluate our 3D-RecGAN++ with com-
parison to the state of the art approaches and an ablation
study to fully investigate the proposed network.
4.1 Metrics
To evaluate the performance of 3D reconstruction, we con-
sider the mean Intersection-over-Union (IoU) between pre-
dicted 3D voxel grids and their ground truth. The IoU for
an individual voxel grid is formally deﬁned as follows:
(cid:80)N
(cid:104)
(cid:80)N
i=1
(cid:2)I(yi > p) ∗ I( ¯yi)(cid:3)
I(cid:0)I(yi > p) + I( ¯yi)(cid:1)(cid:105)
i=1
IoU =
where I(·) is an indicator function, yi is the predicted value
at the ith voxel, ¯yi is the corresponding ground truth, p is the
threshold for voxelization, N is the total number of voxels
in a whole voxel grid. In all our experiments, p is set as
0.5. If the predicted value is over 0.5, it is more likely to be
occupied from the probabilistic aspect. The higher the IoU
value, the better the reconstruction of a 3D model.
4.2 Competing Approaches
We compare against three state of the art deep learning
based approaches for single depth view reconstruction. We
also compare against the generator alone in our network,
i.e. without the GAN, named as 3D-RecAE for short.
(1) 3D-EPN. In [11], Dai et al. proposed a neural network
to reconstruct the 3D shape up to a 323 voxel grid, after
which a high resolution shape is retrieved from an existing
3D shape database. For fair comparison, we only compared
with their neural network performance. Besides, occupancy
grid representation is used for the network training the
testing.
(2) Varley et al. In [2], a network was designed to
complete the 3D shape from a single 2.5D depth view for
robot grasping. The output of their network is a 403 voxel
grid.
(3) Han et al. In [23], a global structure inference network
and a local geometry reﬁnement network are proposed to
complete a high resolution shape from a noisy shape. The
network is not originally designed for single depth view
reconstruction, but its output shape is up to a 2563 voxel
grid and is comparable to our network. For fair comparison,
the same occupancy grid representation is used for their net-
work. It should be noted that their network involves many
convoluted designs, yet the training procedure is extremely
slow and inefﬁcient due to many LSTMs involved.
(4) 3D-RecAE. As for our 3D-RecGAN++, we remove
the discriminator and only keep the generator to infer 3D
complete shape from a single depth view. This comparison
illustrates the beneﬁts from adversarial learning.
4.3 Single-category Results
(1) Results. All networks are separately trained and
tested on four different categories with the same network
conﬁgurations. To fairly compare the IoU between different
approaches, we down sample all results of 2563 voxel grids
to 323 using max pooling with a stride of 8 along the three
axes. Table 1 shows the IoU comparison of all methods on
323 voxel grids, while Table 2 shows the IoU comparison of
[23] and our approaches on higher resolution of voxel grids.
Figure 5 shows the qualitative results of single category re-
construction. In this paper, the meshgrid function in Matlab
is used to plot all 3D shapes for better visualization.
(2) Analysis. The proposed 3D-RecGAN++ signiﬁcantly
outperforms the competing approaches in terms of the IoU
at both lower (323 voxel grids) and higher resolutions
(2563 voxel grids). The 3D shapes generated by our 3D-
RecGAN++ are much more visually compelling than others
in terms of the shape accuracy and the geometrical details.
TABLE 1: Per-category IoU (323 voxel grids).
3D-EPN [11]
Varley et al. [2]
Han et al. [23]
3D-RecAE (ours)
3D-RecGAN++ (ours)
bench
0.758
0.653
0.611
0.800
0.806
chair
0.739
0.619
0.524
0.790
0.793
coach
0.834
0.818
0.505
0.858
0.868
table
0.772
0.678
0.615
0.808
0.821
TABLE 2: Per-category IoU (up to 2563 voxel grids).
Han et al. [23] (643)
Han et al. [23] (1283)
Han et al. [23] (2563)
3D-RecAE (ours) (643)
3D-RecAE (ours) (1283)
3D-RecAE (ours) (2563)
3D-RecGAN++ (ours) (643)
3D-RecGAN++ (ours) (1283)
3D-RecGAN++ (ours) (2563)
bench
0.544
0.492
0.417
0.733
0.669
0.552
0.745
0.683
0.564
chair
0.469
0.432
0.389
0.736
0.693
0.622
0.741
0.699
0.628
coach
0.483
0.469
0.431
0.832
0.812
0.722
0.844
0.825
0.733
table
0.560
0.524
0.476
0.759
0.715
0.644
0.772
0.730
0.659
4.4 Multi-category Results
(1) Results. All networks are also trained and tested on
multiple categories without being given any class labels.
The networks are trained on four categories: {bench, chair,
coach, table}; and then tested separately on an individual
category. Table 3 shows the IoU comparison of all methods
on the resolution of 323 voxel grids, while Table 4 shows
the IoU comparison of [23] and our methods on higher
resolution of voxel grids. Figure 6 shows the qualitative
results of all approaches on multiple categories.
(2) Analysis. The proposed 3D-RecGAN++ signiﬁcantly
outperforms the state of the art by a large margin in all
categories which are trained together on a single model.
7
Fig. 5: Qualitative results of per-category reconstruction from different approaches.
Besides, the performance of our network trained on multiple
categories, does not degrade compared with training the
network on individual categories. This conﬁrms that our
network has enough capacity and capability to learn diverse
features from multiple categories.
TABLE 3: Multi-category IoU (323 voxel grids).
3D-EPN [11]
Varley et al. [2]
Han et al. [23]
3D-RecAE (ours)
3D-RecGAN++ (ours)
bench
0.758
0.681
0.531
0.789
0.806
chair
0.734
0.630
0.512
0.767
0.784
coach
0.843
0.823
0.469
0.850
0.863
table
0.772
0.691
0.596
0.795
0.806
4.5 Cross-category Results
(1) Results. To further investigate the generality of net-
works, we train all networks on {bench, chair, coach, table},
and then test them on another 6 totally different categories:
{airplane, car, faucet, guitar, gun, monitor}. For each of the
6 categories, it has 4625 single arbitrary views from random
selected 37 objects for testing, which is the same data size
as used in the previous {bench, chair, coach, table}. Table 5
Han et al. [23] (643)
Han et al. [23] (1283)
Han et al. [23] (2563)
3D-RecAE (ours) (643)
3D-RecAE (ours) (1283)
3D-RecAE (ours) (2563)
TABLE 4: Multi-category IoU (up to 2563 voxel grids).
table
0.534
0.496
0.458
0.741
0.695
0.623
0.753
0.710
0.638
3D-RecGAN++ (ours) (643)
3D-RecGAN++ (ours) (1283)
3D-RecGAN++ (ours) (2563)
bench
0.453
0.408
0.364
0.719
0.653
0.537
0.741
0.679
0.562
chair
0.451
0.416
0.390
0.710
0.665
0.595
0.730
0.688
0.618
coach
0.442
0.427
0.398
0.825
0.804
0.715
0.838
0.819
0.728
shows the IoU comparison of all approaches on 323 voxel
grids, while Table 6 shows the IoU comparison of [23] and
our approaches on higher resolution of voxel grids. Figure
7 shows the qualitative results of all methods on 6 unseen
categories.
We further evaluate the generality of our 3D-RecGAN++
on a speciﬁc category. Particularly, we conduct four groups
of experiments.
In the ﬁrst group, we train our 3D-
RecGAN++ on bench, then separately test on the remaining
3 categories: {chair, coach, table}. In the second group, the
2.5D input(643)3D-EPN(323)Varley et al.(403)Han et al. (2563)3D-RecAE(2563)3D-RecGAN++(2563)Ground Truth(2563)8
Fig. 6: Qualitative results of multi-category reconstruction from different approaches.
TABLE 5: Cross-category IoU (323 voxel grids).
airplane
faucet guitar gun monitor
3D-EPN [11]
Varley et al. [2]
Han et al. [23]
0.715
0.051
0.585
0.722
0.738
TABLE 6: Cross-category IoU (up to 2563 voxel grids).
car
0.723 0.683 0.753 0.742
0.069 0.025 0.041 0.065
0.414 0.431 0.448 0.401
0.805 0.723 0.762 0.725
0.802 0.731 0.769 0.739
3D-RecAE (ours)
3D-RecGAN++ (ours)
0.667
0.524
0.466
0.670
0.683
airplane car faucetguitar gun monitor
Han et al. [23] (643)
Han et al. [23] (1283)
Han et al. [23] (2563)
3D-RecAE (ours) (643)
3D-RecAE (ours) (1283)
3D-RecAE (ours) (2563)
0.373 0.372 0.356 0.341 0.296 0.517
0.318 0.350 0.319 0.286 0.245 0.470
0.248 0.299 0.284 0.225 0.198 0.414
0.602 0.774 0.678 0.700 0.665 0.661
0.547 0.745 0.645 0.643 0.605 0.614
0.365 0.557 0.520 0.413 0.381 0.529
0.615 0.769 0.687 0.709 0.682 0.680
3D-RecGAN++ (ours) (643)
3D-RecGAN++ (ours) (1283) 0.559 0.740 0.653 0.651 0.625 0.633
3D-RecGAN++ (ours) (2563) 0.379 0.555 0.529 0.424 0.399 0.547
network is trained on chair and separately tested on {bench,
coach, table}. Similarly, another two groups of experiments
are conducted. Basically, this experiment is to investigate
how well our approach learns features from one category
and then generalizes to a different category, and vice versa.
Table 7 shows the cross-category IoU of our 3D-RecGAN++
trained on individual category over 2563 voxel grids.
(2) Analysis. The proposed 3D-RecGAN++ achieves
much higher IoU across the unseen categories than compet-
ing approaches. Our network not only learns rich features
from different object categories, but also is able to generalize
well to completely new types of categories. This implies
that our network may learn geometric relationships such
as lines, planes, curves which are common across various
object categories. It can be also observed that our model
trained on bench tends to be more general than others,
because bench is likely to have more general features to be
learned, while simple categories such as coach are unlikely
to consist of many general features that are shared across
different categories.
4.6 Real-world Experiment Results
(1) Results. Lastly, in order to evaluate the domain adap-
tation capability of the networks, we train all networks on
synthesized data of categories {bench, chair, coach, table},
2.5D input(643)3D-EPN(323)Varley et al.(403)Han et al. (2563)3D-RecAE(2563)3D-RecGAN++(2563)Ground Truth(2563)9
Fig. 7: Qualitative results of cross-category reconstruction from different approaches.
TABLE 7: Cross-category IoU of 3D-RecGAN++ trained on
individual category (2563 voxel grids).
Group 1 (trained on bench)
Group 2 (trained on chair)
Group 3 (trained on coach)
Group 4 (trained on table)
bench
0.482
0.387
0.477
chair
0.510
0.460
0.495
coach
0.549
0.509
0.449
table
0.592
0.549
0.401
and then test them on real-world data collected by a Mi-
crosoft Kinect camera. The real-world data were collected
in different environments, including ofﬁces, homes, and
outdoor university parks, as shown in Figure 8. Compared
to synthesized data, real-world partial 2.5D views are nois-
ier and largely incomplete. For each object, we randomly
selected 20 different 2.5D depth views for testing. Table 8
shows the IoU performance of all approaches using 323
voxel grids, while Table 9 compares the IoU of [23] and
our approaches on higher resolutions. Figure 9 shows some
qualitative results for all methods.
(2) Analysis. There are two reasons why the IoU is
signiﬁcantly lower compared with testing on the synthetic
dataset. First, the ground truth objects obtained from Elastic-
Fusion are empty rather than solid, and are only occupied
on the surface. However, all networks predict dense and
solid voxel grids, so the interior of bulky objects like couches
is not matching. Secondly, the input 2.5D depth view from
real world dataset is noisy and incomplete, due to the
limitation of the RGB-D sensor (e.g., reﬂective surfaces,
outdoor light). In many cases, the input 2.5D view does not
capture the whole object and only contains a small part of
the object, which also leads to failure cases (e.g. the 6th row
in Figure 9) and a lower IoU scores overall. However, our
proposed network is still able to reconstruct reasonable 3D
dense shapes given the noisy and incomplete 2.5D input
2.5D input(643)3D-EPN(323)Varley et al.(403)Han et al. (2563)3D-RecAE(2563)3D-RecGAN++(2563)Ground Truth(2563)10
Fig. 8: Real world objects for reconstruction.
depth views, while the competing algorithms (e.g. Varley et
al.) are not robust to real world noise and unable to generate
compelling results.
TABLE 8: Real-world multi-category IoU (323 voxel grids).
3D-EPN [11]
Varley et al. [2]
Han et al. [23]
3D-RecAE (ours)
3D-RecGAN++ (ours)
bench
0.350
0.043
0.291
0.352
0.347
chair
0.273
0.0.008
0.240
0.295
0.297
coach
0.173
0.038
0.159
0.199
0.201
table
0.246
0.009
0.228
0.268
0.268
TABLE 9: Real-world multi-category IoU (up to 2563 voxel
grids).
Han et al. [23] (643)
Han et al. [23] (1283)
Han et al. [23] (2563)
3D-RecAE (ours) (643)
3D-RecAE (ours) (1283)
3D-RecAE (ours) (2563)
3D-RecGAN++ (ours) (643)
3D-RecGAN++ (ours) (1283)
3D-RecGAN++ (ours) (2563)
bench
0.184
0.117
0.070
0.231
0.145
0.071
0.230
0.147
0.074
chair
0.148
0.092
0.045
0.178
0.096
0.032
0.174
0.097
0.032
coach
0.101
0.067
0.040
0.107
0.055
0.024
0.109
0.057
0.025
table
0.126
0.069
0.031
0.148
0.071
0.024
0.146
0.069
0.023
4.7 Impact of Adversarial Learning
(1) Results. In all above experiments, the proposed 3D-
RecGAN++ tends to outperform the ablated network 3D-
RecAE which does not include the adversarial learning of
GAN part. In all visualization of experiment results, the
3D shapes from 3D-RecGAN++ are also more compelling
than 3D-RecAE. To further investigate how the adversarial
learning improves the ﬁnal 3D results comparing with 3D-
RecAE, we calculate the mean precision and recall from the
above multi-category experiment results. Table 10 shows
the mean precision of 3D-RecGAN++ and 3D-RecAE on
individual categories using the network trained on multiple
categories, while Table 11 shows the mean recall.
(2) Analysis. It can be seen that the results of 3D-
RecGAN++ have much higher precision scores than 3D-
RecAE, which means 3D-RecGAN++ has much less false
positive estimations, while 3D-RecAE tends to estimate
much more false positives. Therefore, the estimated 3D
shapes from 3D-RecAE are likely to be ’fatter’ and ’big-
ger’, while 3D-RecGAN++ tends to output ’thinner’ shapes
with much more shape details being exposed. Both 3D-
RecGAN++ and 3D-RecAE can achieve extremely high re-
call scores (i.e. above 0.9), although 3D-RecGAN++ has
lower recall scores compared with 3D-RecAE. This means
both 3D-RecGAN++ and 3D-RecAE are capable of estimat-
ing almost all of the object shapes without too many false
negatives. In other words, the ground truth 3D shape tends
to be a subset of the estimated shape result.
TABLE 10: Multi-category mean precision (up to 2563 voxel
grids).
3D-RecAE (323)
3D-RecAE (643)
3D-RecAE (1283)
3D-RecAE (2563)
3D-RecGAN++ (323)
3D-RecGAN++ (643)
3D-RecGAN++ (1283)
3D-RecGAN++ (2563)
bench
0.823
0.755
0.688
0.564
0.856
0.798
0.737
0.608
chair
0.802
0.746
0.700
0.626
0.832
0.781
0.739
0.663
coach
0.868
0.843
0.822
0.730
0.888
0.866
0.847
0.751
table
0.825
0.773
0.727
0.651
0.845
0.797
0.755
0.676
TABLE 11: Multi-category mean recall (up to 2563 voxel
grids).
3D-RecAE (323)
3D-RecAE (643)
3D-RecAE (1283)
3D-RecAE (2563)
3D-RecGAN++ (323)
3D-RecGAN++ (643)
3D-RecGAN++ (1283)
3D-RecGAN++ (2563)
bench
0.952
0.939
0.930
0.921
0.934
0.915
0.900
0.887
chair
0.945
0.935
0.929
0.924
0.931
0.918
0.908
0.902
coach
0.978
0.975
0.974
0.972
0.968
0.964
0.961
0.959
table
0.956
0.947
0.940
0.935
0.946
0.934
0.925
0.918
Overall, with regard to experiments on per-category,
multi-category, and cross-category experiments, our 3D-
RecGAN++ outperforms others by a large margin, although
all other approaches can reconstruct reasonable shapes. In
terms of the generality, Varley et al. [2] and Han et al.
[23] are inferior because [2] uses a single fully connected
layers, instead of 3D ConvNets, for shape generation which
is unlikely to be general for various shapes, and [23] applies
LSTMs for shape blocks generation which is inefﬁcient and
11
Fig. 9: Qualitative results of real world objects reconstruction from different approaches.
unable to learn general 3D structures. However, our 3D-
RecGAN++ is superior thanks to the generality of simple
yet efﬁcient 3D autoencoder and the 3D convolutional dis-
criminator. Besides, the 3D-RecAE tends to over estimate the
3D shape, while the adversarial learning of 3D-RecGAN++
is likely to remove the over-estimated parts, so as to leave
the estimated shape to be clearer with more shape details.
5 DISCUSSION
Although our 3D-RecGAN++ achieves the state of the
art performance in 3D object reconstruction from a single
depth view, it has limitations. Firstly, our network takes
the volumetric representation of a single depth view as
input,
instead of taking a raw depth image. Therefore,
a preprocessing of raw depth images is required for our
network. However, in many application scenarios such as
robot grasping, such preprocessing would be trivial and
straightforward given the depth camera parameters. Sec-
ondly, the input depth view of our network only contains a
clean object information without cluttered background. One
possible solution is to leverage an existing segmentation
algorithm such as Mask-RCNN [82] to clearly segment the
target object instance from the raw depth view.
6 CONCLUSION
In this work, we proposed a novel
framework 3D-
RecGAN++ that reconstructs the full 3D structure of an
object from an arbitrary depth view. By leveraging the
generalization capabilities of autoencoders and generative
adversarial networks, our 3D-RecGAN++ predicts dense
and accurate 3D structures with ﬁne details, outperforming
the state of the art in single-view shape completion for
individual object category. We further tested our network’s
ability to reconstruct multiple categories without providing
any object class labels during training or testing, and it
showed that our network is still able to predict precise
3D shapes. Besides, we investigated the network’s recon-
struction performance on unseen categories, our proposed
approach can also predict satisfactory 3D structures. Finally,
our model is robust to real world noisy data and can infer
accurate 3D shapes although the model is purely trained
on synthesized data. This conﬁrms that our network has
the capability of learning general 3D latent features of the
objects, rather than simply ﬁtting a function for the training
datasets, and the adversarial learning of 3D-RecGAN++
learns to add geometric details for estimated 3D shapes. In
summary, our network only requires a single depth view to
recover a dense and complete 3D shape with ﬁne details.
2.5D input(643)3D-EPN(323)Varley et al.(403)Han et al. (2563)3D-RecAE(2563)3D-RecGAN++(2563)Ground Truth(2563)[2]
REFERENCES
[1] A. Sharma, O. Grau, and M. Fritz, “VConv-DAE : Deep Volumetric
Shape Learning Without Object Labels,” ECCV, 2016.
J. Varley, C. Dechant, A. Richardson, J. Ruales, and P. Allen, “Shape
Completion Enabled Robotic Grasping,” IROS, 2017.
[3] R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim,
A. J. Davison, P. Kohli, J. Shotton, S. Hodges, and A. Fitzgibbon,
“KinectFusion: Real-time dense surface mapping and tracking,”
ISMAR, 2011.
[4] M. Nießner, M. Zollh¨ofer, S. Izadi, and M. Stamminger, “Real-time
3D reconstruction at scale using voxel hashing,” ACM Transactions
on Graphics, vol. 32, no. 6, pp. 1–11, 2013.
[5] F. Steinbrucker, C. Kerl, J. Sturm, and D. Cremers, “Large-Scale
Multi-Resolution Surface Reconstruction from RGB-D Sequences,”
ICCV, 2013.
[6] A. Nealen, T. Igarashi, O. Sorkine, and M. Alexa, “Laplacian Mesh
Optimization,” SIGGRAPH, 2006.
[7] W. Zhao, S. Gao, and H. Lin, “A robust hole-ﬁlling algorithm for
triangular mesh,” The Visual Computer, vol. 23, no. 12, pp. 987–997,
2007.
[8] M. Kazhdan, M. Bolitho, and H. Hoppe, “Poisson Surface Recon-
struction,” Symposium on Geometry Processing, 2006.
[9] M. Kazhdan and H. Hoppe, “Screened poisson surface reconstruc-
tion,” ACM Transactions on Graphics, vol. 32, no. 3, pp. 1–13, 2013.
[10] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao,
“3D ShapeNets: A Deep Representation for Volumetric Shapes,”
CVPR, 2015.
[11] A. Dai, C. R. Qi, and M. Nießner, “Shape Completion using 3D-
Encoder-Predictor CNNs and Shape Synthesis,” CVPR, 2017.
[12] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-
Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative Adver-
sarial Nets,” NIPS, 2014.
[13] D. P. Kingma and M. Welling, “Auto-Encoding Variational Bayes,”
ICLR, 2014.
[14] Z. Hu, Z. Yang, X. Liang, R. Salakhutdinov, and E. P. Xing,
“Controllable Text Generation,” ICML, 2017.
[15] T. Karras, T. Aila, S. Laine, and J. Lehtinen, “Progressive Growing
of GANs for Improved Quality, Stability, and Variation,” ICLR,
2018.
[16] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and
P. Abbeel, “InfoGAN: Interpretable Representation Learning by In-
formation Maximizing Generative Adversarial Nets,” NIPS, 2016.
[17] T. D. Kulkarni, W. F. Whitney, P. Kohli, and J. B. Tenenbaum, “Deep
Convolutional Inverse Graphics Network,” NIPS, 2015.
[18] E. Grant, P. Kohli, and M. V. Gerven, “Deep Disentangled Repre-
sentations for Volumetric Reconstruction,” ECCV Workshops, 2016.
[19] R. Girdhar, D. F. Fouhey, M. Rodriguez, and A. Gupta, “Learning
a Predictable and Generative Vector Representation for Objects,”
ECCV, 2016.
[20] H. Huang, E. Kalogerakis, and B. Marlin, “Analysis and synthe-
sis of 3D shape families via deep-learned generative models of
surfaces,” Computer Graphics Forum, vol. 34, no. 5, pp. 25–38, 2015.
[21] J. Wu, C. Zhang, T. Xue, W. T. Freeman, and J. B. Tenenbaum,
“Learning a Probabilistic Latent Space of Object Shapes via 3D
Generative-Adversarial Modeling,” NIPS, 2016.
[22] M. Mirza and S. Osindero, “Conditional Generative Adversarial
Nets,” arXiv, 2014.
[23] X. Han, Z. Li, H. Huang, E. Kalogerakis, and Y. Yu, “High-
Resolution Shape Completion Using Deep Neural Networks for
Global Structure and Local Geometry Inference,” ICCV, 2017.
[24] B. Yang, H. Wen, S. Wang, R. Clark, A. Markham, and N. Trigoni,
“3D Object Reconstruction from a Single Depth View with Adver-
sarial Learning,” ICCV Workshops, 2017.
[25] A. Monszpart, N. Mellado, G. J. Brostow, and N. J. Mitra, “RAPter:
Rebuilding Man-made Scenes with Regular Arrangements of
Planes,” ACM Transactions on Graphics, vol. 34, no. 4, pp. 1–12,
2015.
[26] N. J. Mitra, L. J. Guibas, and M. Pauly, “Partial and Approximate
Symmetry Detection for 3D Geometry,” SIGGRAPH, 2006.
[27] M. Pauly, N. J. Mitra, J. Wallner, H. Pottmann, and L. J. Guibas,
“Discovering structural regularity in 3D geometry,” ACM Transac-
tions on Graphics, vol. 27, no. 3, p. 1, 2008.
[28] I. Sipiran, R. Gregor, and T. Schreck, “Approximate Symmetry
Detection in Partial 3D Meshes,” Computer Graphics Forum, vol. 33,
no. 7, pp. 131–140, 2014.
[29] P. Speciale, M. R. Oswald, A. Cohen, and M. Pollefeys, “A Symme-
try Prior for Convex Variational 3D Reconstruction,” ECCV, 2016.
12
[30] S. Thrun and B. Wegbreit, “Shape from symmetry,” ICCV, 2005.
[31] Y. M. Kim, N. J. Mitra, D.-M. Yan, and L. Guibas, “Acquiring
3D Indoor Environments with Variability and Repetition,” ACM
Transactions on Graphics, vol. 31, no. 6, 2012.
[32] Y. Li, A. Dai, L. Guibas, and M. Nießner, “Database-Assisted Ob-
ject Retrieval for Real-Time 3D Reconstruction,” Computer Graphics
Forum, vol. 34, no. 2, pp. 435–446, 2015.
[33] L. Nan, K. Xie, and A. Sharf, “A Search-Classify Approach for
Cluttered Indoor Scene Understanding,” ACM Transactions on
Graphics, vol. 31, no. 6, pp. 1–10, 2012.
[34] T. Shao, W. Xu, K. Zhou, J. Wang, D. Li, and B. Guo, “An
interactive approach to semantic modeling of indoor scenes with
an RGBD camera,” ACM Transactions on Graphics, vol. 31, no. 6,
pp. 1–11, 2012.
[35] Y. Shi, P. Long, K. Xu, H. Huang, and Y. Xiong, “Data-driven
contextual modeling for 3d scene understanding,” Computers &
Graphics, vol. 55, pp. 55–67, 2016.
[36] J. Rock, T. Gupta, J. Thorsen, J. Gwak, D. Shin, and D. Hoiem,
“Completing 3D Object Shape from One Depth Image,” CVPR,
2015.
[37] R. Hartley and A. Zisserman, Multiple View Geometry in Computer
Vision. Cambridge University Press, 2004.
[38] R. A. Newcombe, S. J. Lovegrove, and A. J. Davision, “DTAM:
Dense Tracking and Mapping in Real-time,” ICCV, 2011.
[39] S. Baker and I. Matthews, “Lucas-Kanade 20 Years On : A Unify-
ing Framework : Part 1,” International Journal of Computer Vision,
vol. 56, no. 3, pp. 221–255, 2004.
[40] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese, “3D-
R2N2: A Uniﬁed Approach for Single and Multi-view 3D Object
Reconstruction,” ECCV, 2016.
[41] X. Di, R. Dahyot, and M. Prasad, “Deep Shape from a Low
Number of Silhouettes,” ECCV, 2016.
[42] Z. Lun, M. Gadelha, E. Kalogerakis, S. Maji, and R. Wang, “3D
Shape Reconstruction from Sketches via Multi-view Convolu-
tional Networks,” 3DV, 2017.
[43] D. J. Rezende, S. M. A. Eslami, S. Mohamed, P. Battaglia, M. Jader-
berg, and N. Heess, “Unsupervised Learning of 3D Structure from
Images,” NIPS, 2016.
[44] A. Kar, C. H¨ane, and J. Malik, “Learning a Multi-View Stereo
Machine,” NIPS, 2017.
[45] M. Ji, J. Gall, H. Zheng, Y. Liu, and L. Fang, “SurfaceNet: An End-
to-end 3D Neural Network for Multiview Stereopsis,” ICCV, 2017.
[46] T. Whelan, J. McDonald, M. Kaess, M. Fallon, H. Johannsson, and
J. J. Leonard, “Kintinuous: Spatially Extended Kinectfusion,” RSS
Workshops, 2012.
[47] T. Whelan, S. Leutenegger, R. F. Salas-moreno, B. Glocker, and A. J.
Davison, “ElasticFusion : Dense SLAM Without A Pose Graph,”
RSS, 2015.
[48] G. Riegler, A. O. Ulusoy, H. Bischof, and A. Geiger, “OctNetFusion:
Learning Depth Fusion from Data,” 3DV, 2017.
[49] V. Blanz and T.Vetter, “Face Recognition based on Fitting a 3D
Morphable Model,” IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, vol. 25, no. 9, pp. 1063–1074, 2003.
[50] P. Dou, S. K. Shah, and I. A. Kakadiaris, “End-to-end 3D face
reconstruction with deep neural networks,” CVPR, 2017.
[51] A. Kar, S. Tulsiani, J. Carreira, and J. Malik, “Category-speciﬁc
object reconstruction from a single image,” CVPR, 2015.
[52] J. Gwak, C. B. Choy, M. Chandraker, A. Garg, and S. Savarese,
“Weakly supervised 3D Reconstruction with Adversarial Con-
straint,” arXiv, 2017.
[53] S. Tulsiani, T. Zhou, A. A. Efros, and J. Malik, “Multi-view Su-
pervision for Single-view Reconstruction via Differentiable Ray
Consistency,” CVPR, 2017.
[54] X. Yan, J. Yang, E. Yumer, Y. Guo, and H. Lee, “Perspective
Transformer Nets: Learning Single-View 3D Object Reconstruction
without 3D Supervision,” NIPS, 2016.
[55] C. Kong, C.-H. Lin, and S. Lucey, “Using Locally Corresponding
CAD Models for Dense 3D Reconstructions from a Single Image,”
CVPR, 2017.
[56] A. Kurenkov, J. Ji, A. Garg, V. Mehta, J. Gwak, C. Choy, and
S. Savarese, “DeformNet: Free-Form Deformation Network for 3D
Shape Reconstruction from a Single Image,” NIPS, 2017.
[57] J. K. Murthy, G. V. S. Krishna, F. Chhaya, and K. M. Krishna,
“Reconstructing Vechicles from a Single Image : Shape Priors for
Road Scene Understanding,” ICRA, 2017.
[58] H. Christian, S. Tulsiani, and J. Malik, “Hierarchical Surface Pre-
diction for 3D Object Reconstruction,” 3DV, 2017.
13
[59] M. Tatarchenko, A. Dosovitskiy, and T. Brox, “Octree Generat-
ing Networks: Efﬁcient Convolutional Architectures for High-
resolution 3D Outputs,” ICCV, 2017.
[60] A. Johnston, R. Garg, G. Carneiro, I. Reid, and A. v. d. Hengel,
“Scaling CNNs for High Resolution Volumetric Reconstruction
from a Single Image,” ICCV Workshops, 2017.
[61] C.-H. Lin, C. Kong, and S. Lucey, “Learning Efﬁcient Point Cloud
Generation for Dense 3D Object Reconstruction,” AAAI, 2018.
[62] J. Wu, Y. Wang, T. Xue, X. Sun, W. T. Freeman, and J. B. Tenenbaum,
“MarrNet: 3D Shape Reconstruction via 2.5D Sketches,” NIPS,
2017.
[63] M. Firman, O. M. Aodha, S. Julier, and G. J. Brostow, “Structured
Prediction of Unobserved Voxels From a Single Depth Image,”
CVPR, 2016.
[64] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser,
“Semantic Scene Completion from a Single Depth Image,” CVPR,
2017.
[65] W. Wang, Q. Huang, S. You, C. Yang, and U. Neumann, “Shape In-
painting using 3D Generative Adversarial Network and Recurrent
Convolutional Networks,” ICCV, 2017.
[66] C. Zou, E. Yumer, J. Yang, D. Ceylan, and D. Hoiem, “3D-PRNN:
Generating Shape Primitives with Recurrent Neural Networks,”
ICCV, 2017.
[67] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham,
A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi,
“Photo-Realistic Single Image Super-Resolution Using a Genera-
tive Adversarial Network,” CVPR, 2017.
[68] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee,
“Generative Adversarial Text to Image Synthesis,” ICML, 2016.
[69] M. Gadelha, S. Maji, and R. Wang, “3D Shape Induction from 2D
Views of Multiple Objects,” 3DV, 2017.
[70] E. Smith and D. Meger, “Improved Adversarial Systems for 3D
Object Generation and Reconstruction,” CoRL, 2017.
[71] A. A. Soltani, H. Huang, J. Wu, T. D. Kulkarni, and J. B. Tenen-
baum, “Synthesizing 3D Shapes via Modeling Multi-View Depth
Maps and Silhouettes with Deep Generative Networks,” CVPR,
2017.
[72] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang,
Z. Li, S. Savarese, M. Savva, S. Song, H. Su, J. Xiao, L. Yi, and
F. Yu, “ShapeNet: An Information-Rich 3D Model Repository,”
arXiv, 2015.
[73] L. v. d. Maaten and G. Hinton, “Visualizing Data using t-SNE,”
Journal of Machine Learning Research, vol. 9, no. Nov, pp. 2579–2605,
2008.
[74] O. Ronneberger, P. Fischer, and T. Brox, “U-Net : Convolutional
Networks for Biomedical Image Segmentation,” MICCAI, 2015.
[75] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein GAN,”
ICML, 2017.
tion,” ICLR, 2015.
ICCV, 2017.
[76] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and
A. Courville, “Improved Training of Wasserstein GANs,” NIPS,
2017.
[77] M. Arjovsky and L. Bottou, “Towards Principled Methods for
Training Generative Adversarial Networks,” ICLR, 2017.
[78] Y. Mroueh, T. Sercu, and V. Goel, “McGAN: Mean and Covariance
Feature Matching GAN,” ICML, 2017.
[79] J. Bao, D. Chen, F. Wen, H. Li, and G. Hua, “CVAE-GAN: Fine-
Grained Image Generation through Asymmetric Training,” ICCV,
2017.
[80] A. Brock, T. Lim, J. M. Ritchie, and N. Weston, “Generative
and Discriminative Voxel Modeling with Convolutional Neural
Networks,” NIPS Workshops, 2016.
[81] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-
[82] K. He, G. Gkioxari, P. Dollar, and R. Girshick, “Mask R-CNN,”
