A Modiﬁed Sigma-Pi-Sigma Neural Network with Adaptive Choice
of Multinomials
Feng Li, Yan Liu, Khidir Shaib Mohamed, and Wei Wu ∗
ABSTRACT
Sigma-Pi-Sigma neural networks (SPSNNs) as a kind of high-order neural networks can provide more
powerful mapping capability than the traditional feedforward neural networks (Sigma-Sigma neural
networks). In the existing literature, in order to reduce the number of the Pi nodes in the Pi layer,
a special multinomial Ps is used in SPSNNs. Each monomial in Ps is linear with respect to each
particular variable σi when the other variables are taken as constants. Therefore, the monomials like
σn
i or σn
i σj with n > 1 are not included. This choice may be somehow intuitive, but is not necessarily
the best. We propose in this paper a modiﬁed Sigma-Pi-Sigma neural network (MSPSNN) with an
adaptive approach to ﬁnd a better multinomial for a given problem. To elaborate, we start from a
complete multinomial with a given order. Then we employ a regularization technique in the learning
process for the given problem to reduce the number of monomials used in the multinomial, and end
up with a new SPSNN involving the same number of monomials (= the number of nodes in the
Pi-layer) as in Ps. Numerical experiments on some benchmark problems show that our MSPSNN
behaves better than the traditional SPSNN with Ps.
Introduction
Sigma-Pi-Sigma neural networks (SPSNNs) [1,4,7,8] as a kind of high-order neural networks can
provide more powerful mapping capability [2-3,5,6] than the traditional feedforward neural networks
(Sigma-Sigma neural networks). In an SPSNN, a Pi layer (denoted by Π layer hereafter) is inserted
in between the two Sigma layers. Each Pi node (Π node) in the Π layer corresponds to a monomial, of
which the variables are the outputs of the Sigma nodes (Σ nodes) of the ﬁrst Sigma layer (Σ1 layer).
Each node in the second Sigma layer (Σ2 layer) implements a linear combination of the outputs of
the Π layer, and therefore represents a multinomial expansion of the output σ = (σ1,··· , σN ) of the
Σ1 layer. Then, the multinomial expansion is processed by an activation function in the Σ2 layer to
give the ﬁnal output of the network.
At the beginning of the development of SPSNN, researchers have realized that it is not a good idea
to include all the possible monomials in the Π layer, i.e., to get a complete multinomial expansion of
the Σ1 layer, since it results in too many Π nodes in the Π layer. In the existing literature, in order
to reduce the number of Π nodes, a special multinomial Ps (called multi-linear multinomial) is used
in SPSNNs. The monomials in Ps are linear with respect to each particular variable σi when taking
the other variables as constants. Therefore, the monomials such as σn
i σj with n > 1 are not
included in Ps. An intuitive idea behind this strategy may be the following: A Π node should receive
at most one signal, rather than two or more signals, from each Σ1 node.
i or σn
But from general numerical approximation point of view, each monomial plays equally important
role for approximating nonlinear mappings by using multinomial. Thus, the special multi-linear
multinomial Ps may not be the best choice for the SPSNN to approximate a particular nonlinear
mapping. To this end, we propose an adaptive approach to ﬁnd a better multinomial for a given
problem. To elaborate, we start from a complete multinomial with a given order. Then we employ
a regularization technique in the learning process for the given problem to reduce the number of
monomials used in the multinomial, and end up with a modiﬁed SPSNN (MSPSNN) involving the
∗W. Wu is the corresponding author (e-mail: wuweiw@dlut.edu.cn).
F. Li, K.S. Mohamed and W. Wu are with the School of Mathematical Sciences, Dalian University of Technology,
Dalian 116024, China.
Y. Liu is with School of Information Science and Engineering, Dalian Polytechnic University, Dalian, China.
same number of monomials (= the number of nodes in the Π layer) as in Ps. In particular, a smoothing
L1/2 regularization term [10,15] is used as an example in our method, which has been successfully
applied for various kinds of neural network regularization.
We divide the learning process of MSPSNN into two phases. The ﬁrst phase is a structural
optimization phase. Here, we insert a regularization term into the usual error function for SPSNN
involving a complete set of multinomials, and perform a usual gradient learning process. In the end,
we delete the Π nodes with smaller Π-Σ2 weights, and obtain a network with the same number of Π
nodes as in Ps.
The second learning phase is a reﬁnement phase. We re-start a gradient learning process for the
network obtained from the ﬁrst learning phase, and use the weights that survived the ﬁrst phase as
the initial weights. The aim of the reﬁnement phase is to make up for the loss caused by the deleted
nodes in the ﬁrst learning phase.
Numerical experiments are performed on some benchmark problems including two approximation
problems and two classiﬁcation problems. It is shown that our new MSPSNN behaves better than
the traditional SPSNN with Ps.
The rest of the paper is arranged as follows. The proposed MSPSNN with smoothing L1/2
In Section 3, Supporting numerical simulations are
regularization term is described in Section 2.
presented. Some conclusions are given in Section 4.
2 MSPSNN method with smoothing L1/2 regularization
2.1 Sigma-Pi-Sigma neural network
An SPSNN is composed of an input layer, two hidden layers of summation node layer (Σ1 layer) and
product node layer (Π layer), and an output layer (Σ2 layer). The numbers of nodes of these layers
are M + 1, N, Q and 1, respectively.
Denote by x = (x0,· · ·, xM )T ∈ RM +1 the input vector, where the M components x0,··· , xM−1
are the “real” input, while xM is an extra artiﬁcial input, ﬁxed to -1. The output vector σ ∈ RN of
Σ1 layer with respect to x can be written as
σ = (σ1,··· , σN ) = (g(w1 · x), g(w2 · x),· · ·, g(wN · x))T ,
(1)
where g(·) is a given nonlinear activation function, wn = (wn0,· · ·, wnM )T ∈ RM +1 (1 ≤ n ≤ N ) is
the weight vector connecting the n-th summation node of Σ1 layer and the input layer, and wn · x
denotes the inner product of wn and x. Here we remark that the component wnM usually represents
the bias of the n-th summation node of Σ1 layer.
In Π layer, Each Π node connects with certain nodes of Σ1 layer, receives signals from these nodes,
and outputs a particular monomial such as
(2)
Denote by ∧q (1 ≤ q ≤ Q) the index set of all the nodes in Σ1 layer that are connected to the q-th Π
node. For instance, let us assume that the above three examples in (2) correspond to the ﬁrst, third
and ﬁfth nodes of Π layer, respectively. Then, we have
σ1, σ1σ2, σ2
1.
∧1 = {1},∧3 = {1, 2},∧5 = {1, 1}.
(3)
The output vector τ = (τ1,··· , τQ)T ∈ RQ of Π layer is computed by
τq =
σi, 1 ≤ q ≤ Q.
(cid:89)
i∈∧q
Here we make a convention that τq =(cid:81)
i∈∧q
σi ≡ 1, when ∧q = φ, i.e., when the q-th Π node is not
Before we concentrate our attention on the choice of ∧q’s, let us describe the output of Σ2 layer.
connected to any node of Σ1 layer. The choice of ∧q’s is our main concern in this paper.
(4)
The output of the single node of Σ2 layer, i.e., the ﬁnal output of the network, is
(5)
where f (·) is another given activation function, and w0 = (w0,1, w0,2,··· , w0,Q)T ∈ RQ is the weight
vector connecting Π layer and Σ2 layer. When the network is used for approximation problems, we
y = f (w0 · τ ),
usually set f (t) = t. On the other hand, when the network is used for classiﬁcation problems, f (t) is
usually chosen to be a Sigmoid function. In both the cases, we can see from (1), (4) and (5) that the
input w0 · τ to Σ2 layer is actually a multinomial expansion of the output values of Σ1 layer, where
the components of τ correspond to the monomials, and the components of w0 are the coeﬃcients,
involved in the multinomial expansion. As comparison, we recall that for the usual feedforward neural
networks, the input to the Σ2 layer is a linear combination of the output values of Σ1 layer.
Now we discuss the choice of ∧q’s in detail and explain the main idea of the paper. For convenience
and clarity, we take the third order multinomial of three variables as an example in this introduction
section. Therefore, we have N = 3, i.e., Σ1 layer has three nodes.
We consider three choices of ∧q’s, resulting in three diﬀerent multinomial expansions: the complete
mutinomial, the partially linear multinomial (the traditional approach), and the adaptive multinomial
(our proposed approach).
The choice of the complete mutinomial means that the input to Σ2 layer is a complete multinomial
as follows:
w0,1 + w0,2σ1 + w0,3σ2 + w0,4σ3 + w0,5σ1σ2 + w0,6σ1σ3 + w0,7σ2σ3
+ w0,8σ2
1 + w0,13σ1σ2
+ w0,14σ3σ2
3 + w0,11σ2σ2
1 + w0,12σ3σ2
2 + w0,10σ2
1 + w0,9σ2
2 + w0,15σ1σ3
3 + w0,16σ3σ2
2 + w0,17σ3
1 + w0,18σ3
2 + w0,19σ3
3 + w0,20σ1σ2σ3.
(6)
We see that there are twenty monomials in the multinomial expansion, corresponding to twenty Π
nodes in Π layer. More generally, when Σ1 layer has N nodes, the number of the monomials is
CN
N +3, which grows very rapidly when N increases. Therefore, the complete multinomial
approach is not a good choice in practice.
complete = C3
The traditional choice in the existing literature is the partially linear multinomial approach:
A partially linear multinomial is linear with respect to each particular variable σi, with the other
variables taken as constants. For instance, the partially linear multinomial corresponds to (6) is
w0,1 + w0,2σ1 + w0,3σ2 + w0,4σ3 + w0,5σ1σ2 + w0,6σ1σ3 + w0,7σ2σ3 + w0,8σ1σ2σ3.
(7)
We see that there are only eight monomials in (7), i.e., only eight nodes left in Π layer. Generally,
when Σ1 layer has N nodes, the number of the monomials is CN
N . Table
1 shows the comparison of CN
linear with diﬀerent N . It can be seen that the diﬀerence
becomes bigger when N increases.
complete and CN
linear = C0
N + C1
N + C2
N + C3
Table 1: Comparison of CN
complete and CN
linear with diﬀerent N .
CN
complete
CN
linear
Diﬀerence
20
12
35
15
20
56
26
30
84
42
42
120
165
64
56
93
72
220
130
90
10
286
176
110
The network structure corresponding to (7) is shown in Fig. 3(a). The corresponding ∧q’s are as
follows:
∧1 = {φ},∧2 = {1},∧3 = {2},∧4 = {3},∧5 = {1, 2},∧6 = {1, 3},∧7 = {2, 3},∧8 = {1, 2, 3}.
(8)
We observe that in (6) and (7), the ﬁrst product node, corresponding to the bias w0,1, does not
connect with any node in the Σ1 layer, so ∧1 = {φ}. We also notice that there are no repeated
indexes in each ∧q in (8).
Our proposed choice is as follows: We start from a complete multinomial with a given order. Then
we employ a regularization technique in the learning process to reduce the number of monomials used
in the multinomial, and end up with a new SPSNN involving the same number of monomials as in
the traditional choice. For instance, in the Example 1 given in Section 4.1 below, a new SPSNN is
obtained with the following multimonial:
w0,1 + w0,2σ1 + w0,3σ2σ3 + w0,4σ1σ2
2 + w0,5σ2σ2
3 + w0,6σ3
1 + w0,7σ3
2 + w0,8σ3
3.
And correspondingly,
∧1 = (∅),∧2 = {1},∧2 = {2, 3},∧3 = {1, 2, 2},∧4 = {2, 2, 3},
∧5 = {2, 3, 3},∧6 = {1, 1, 1},∧7 = {2, 2, 2},∧8 = {3, 3, 3}.
(9)
(10)
We notice that now there are some repeated indexes in six ∧q’s.
2.2 Error function with L1/2 regularization
Let the training samples be {xj, Oj}J
M )T is the j-th input
sample and Oj is its corresponding ideal output. Let yj ∈ R (1 ≤ j ≤ J) be the network output for
the input xj. The aim of the training process is to build up a network such that the errors |yj − Oj|
(1 ≤ j ≤ J) are as small as possible. A conventional square error function with no regularization
term is as follows:
j=1 ⊂ RM +1 × R, where xj = (xj
0,··· , xj
J(cid:88)
J(cid:88)
˜E(W) =
where W = (wT
0 , wT
1 ,··· , wT
N ),
gj(t) =
(yj − Oj)2 =
gj(w0 · τ j),
j=1
j=1
(g(t) − Oj)2, t ∈ R, 1 ≤ j ≤ J.
Let us derive the gradient of the error function ˜E(W). Notice
i ,· · ·,
σj
2 ,··· , τ j
τ j = (τ j
Q)T = (
1 , τ j
σj
i ,
(cid:89)
i∈∧1
(cid:89)
i∈∧2
(cid:89)
i∈∧Q
σj
i )T
(11)
(12)
(13)
(14)
(15)
(16)
(17)
(18)
and
σj = (σj
1, σj
2,··· , σj
N )T = (g(w1 · xj), g(w2 · xj),··· , g(wN · xj))T .
Then, the partial derivative of ˜E(W) with respect to w0,q (1 ≤ q ≤ Q) is
˜Ew0,q (W) =
j(w0 · τ j)τ j
g(cid:48)
q .
J(cid:88)
j=1
Moreover, for 1 ≤ n ≤ N , 0 ≤ m ≤ M and 1 ≤ q ≤ Q, we have
∂τq
∂wnm
i∈∧q\n σi)g(cid:48)(wn · x)xm,
if q (cid:54)= 1, and n ∈ ∧q,
if q = 1, or n /∈ ∧q.
According to (4) and (16), for any 1 ≤ n ≤ N, 0 ≤ m ≤ M , we have
˜Ewnm(W) =
j(w0 · τ j)
g(cid:48)
j(w0 · τ j)
g(cid:48)
Q(cid:88)
(cid:88)
q∈(cid:87)
q=1
w0,q
∂τ j
∂wnm
(cid:89)
w0,q(
i∈∧Q\n
i )g(cid:48)(wn · xj)xj
σj
m,
(cid:40)
((cid:81)
0,
J(cid:88)
J(cid:88)
j=1
j=1
where ∂τ j
∂wnm
denotes the value of
∂τq
∂wnm
with σi = σj
i and x = xj in (16).
The error function with the L1/2 regularization term is
E(W) = ˜E(W) + λ[
|w0,q|1/2 +
|wnm|)1/2].
Q(cid:88)
q=1
N(cid:88)
M(cid:88)
m=0
n=1
The gradient method with L1/2 regularization for training the network is: Starting with an arbitrary
initial value W0, the weights {Wk} are updated iteratively by:
Wk+1 = Wk − (cid:52)Wk.
Here, (cid:52)Wk = ((cid:52)wk
0,1,··· ,(cid:52)wk
0,Q,··· ,(cid:52)wk
10,··· ,(cid:52)wk
J(cid:88)
0,q = −ηEw0,q (Wk) = −η[
(cid:52)wk
N M )T with
g(cid:48)
j(wk
0 · τ j)τ j
q +
j=1
(19)
(20)
λsgn(wk
0,q)
2|wk
0,q|1/2
and
(cid:52)wk
nm = −ηEwnm(Wk)
= −η[
g(cid:48)
j(wk
0 · τ k,j)
J(cid:88)
j=1
(cid:88)
q∈∨n
(cid:89)
i∈∧Q\n
σk,j
wk
0,q(
)g(cid:48)(wk
n · xj)xj
m +
2(|wk
λsgn(wk
nm)
n0| + ··· + |wk
nm|)1/2
]. (21)
Here, 1 ≤ j ≤ J; 1 ≤ n ≤ N ; 0 ≤ m ≤ M ; 1 ≤ q ≤ Q; k = 0, 1,··· ; η > 0 is the learning rate; and
λ > 0 is the regularization parameter.
2.3 Error function with smoothing L1/2 regularization
We note that the usual L1/2 regularization term in (18) is a non-diﬀerentiable function at the origin.
In previous studies [][], it has been replaced by a smoothing function as follows
E(W) = ˜E(W) + λ[
|f (w0,q)|1/2 +
|f (wnm)|)1/2],
N(cid:88)
n=1
M(cid:88)
m=0
Q(cid:88)
(cid:40)|x|,
q=1
(22)
(23)
where f (x) is the following piecewise multinomial function:
f (x) =
It is easy to obtain that
− x4
8a3 + 3x2
4a + 3a
8 ,
if |x| ≥ a,
if |x| < 0.
f (x) ∈ [
3a
, +∞), f(cid:48)(x) ∈ [−1, 1], and f(cid:48)(cid:48)(z) ∈ [0,
2a
].
The gradient of the error function can be written as
EW(W) = (Ew0,1(W), Ew0,2(W),··· , Ew0,Q(W), Ew10(W), Ew11(W),··· , EwNM (W))T ,
(24)
where
Ew0,q (W) =
Ewnm(W) =
J(cid:88)
J(cid:88)
j=1
j=1
q +
j(w0 · τ j)τ j
g(cid:48)
(cid:88)
j(w0 · τ j)
g(cid:48)
q∈∨n
λf(cid:48)(w0,q)
2(f (w0,q))1/2
(cid:89)
w0,q(
i∈∧Q\n
i )g(cid:48)(wn · xj)xj
σj
m +
λf(cid:48)(w0,q)
2(f (wn0) + ··· + f (wnm))1/2
).
Starting from an arbitrary initial value W0, the gradient method with the smoothing L1/2 regular-
ization updates the weights {Wk} iteratively by
Wk+1 = Wk − (cid:52)Wk
(25)
with
and
(cid:52)wk
0,q = −ηEw0,q (Wk) = −η[
J(cid:88)
j=1
g(cid:48)
j(wk
0 · τ k,j)τ j
q +
λf(cid:48)(wk
2(f (wk
0,q)
0,q))1/2
(26)
(cid:52)wk
= −η[
nm = −ηEnm(Wk)
J(cid:88)
0 · τ k,j)
g(cid:48)
j(wk
j=1
(cid:88)
q∈∨n
(cid:89)
i∈∧Q\n
σk,j
wk
0,q(
)g(cid:48)(wk
n · xj)xj
m +
λf(cid:48)(wk
nm)
2(f (wk
n0) + ··· + f (wk
nM ))1/2
],
(27)
where 1 ≤ j ≤ J; 1 ≤ n ≤ N ; 0 ≤ m ≤ M ; 1 ≤ q ≤ Q; k = 0, 1,··· ; η > 0 is the learning rate; and
λ > 0 the regularization parameter.
3 Algorithm
As mentioned in the Introduction, We divide the learning process into two phases: a structural
optimization phase for choosing the structure of the network, followed by a reﬁnement phase for
ﬁnally choosing the weights. Detailed descriptions of these two training phases are given in the
following Algorithms 1 and 2, respectively.
Algorithm 1 Structural optimization
Input. Input the dimension M , the number N of the Σ1 nodes, the number Q of the Π nodes, the
maximum iteration number I, the learning rate η, the regularization parameter λ, and the training
samples {xj, Oj}J
0,Q)T ∈ RQ and
Initialization. Initialize randomly the initial weight vectors w0
w0
n = (w0
Training. For k = 1, 2,· · ·, I do
nM )T ∈ RM +1 (1 ≤ n ≤ N ).
j=1 ⊂ RM +1 × R.
0,1,··· , w0
n1,··· , w0
0 = (w0
n0, w0
Compute the error function (22).
Compute the gradients (26) and (27).
Update the weights wk
0 and wk
n (1 ≤ n ≤ N ) by using (25).
end
Structural optimization.
ˆQ = CN
Output. Output the ﬁnal weight vectors ˆw0 and ˆwn = wI
linear largest weights in absolute value to form a vector ˆw0 = { ˆw1, ˆw2,··· , ˆw ˆQ}.
0 = (wI
n (1 ≤ n ≤ N ).
In the obtained weight vector wI
0,1,··· , w0,Q)T , select the
Algorithm 2 Reﬁnement training
Input. Input the dimension M , the number N of the Σ1 nodes, the number ˆQ of the Π nodes,
the maximum iteration number K, the learning rate η, and the training samples {xj, Oj}J
j=1 ⊂
RM +1 × R.
Initialization. Set w0
Reﬁnement Training. for k = 1, 2,··· , K do
n = ˆwn (1 ≤ n ≤ N ), and λ = 0.
0 = ˆw0 and w0
Compute the error function by (22).
Compute the gradient of the weights (cid:52)wk
Update the weights wk
0 and (cid:52)wk
n (1 ≤ n ≤ N ) by (25).
0 and wk
n (1 ≤ n ≤ N ) by (26) and (27), respectively.
end
Output. Output the ﬁnal weight vectors wK
0 and wK
n (1 ≤ n ≤ N ).
4 Numerical experiments
In this section, the proposed method is performed on four numerical benchmark problems: Mayas’
function problem, Gabor function problem, Sonar problem and the Pima Indians diabetes data clas-
siﬁcation with diﬀerent learning rates.
4.1 Example 1: Mayas’ function approximate
In this example, a network is considered to approximate the Mayas’ function as below:
HM (x, y) = 0.26(x2 + y2) − 0.48xy.
(28)
The training samples of the network are 36 input points selected from an even 6 × 6 grid on −0.5 ≤
x ≤ 0.5 and −0.5 ≤ y ≤ 0.5. Similarly, the test samples are 400 input points selected from 20 × 20
grid on −0.5 ≤ y ≤ 0.5 and −0.5 ≤ y ≤ 0.5.
After performing Algorithms 1 with η = 0.005, λ = 0.0001 and iterationmax = 5000, we select
3, to approximate the complete multinomial. The
eight monomials, 1, σ2σ3, σ1σ2
new structure corresponds to Fig. 3(b). The new weighted linear combination is
2, σ3σ2
2, σ3
2, σ3
1, σ3
w0,1 + w0,2σ1 + w0,3σ2σ3 + w0,4σ1σ2
2 + w0,5σ2σ2
3 + w0,6σ3
1 + w0,7σ3
2 + w0,8σ3
(29)
From Fig. 3(b), the ﬁrst product node, corresponding to the bias w0,1, does not connect with any
node in the Σ1 layer, so ∧1 = φ. And we have
∧1 = ∅,∧2 = {1},∧2 = {2, 3},∧3 = {1, 2, 2},∧4 = {2, 2, 3}...
∧5 = {2, 3, 3},∧6 = {1, 1, 1},∧7 = {2, 2, 2},∧8 = {3, 3, 3}
(30)
Then, we perform Algorithms 2 and use the test samples to evaluate our method. The average
error with diﬀerent parameter η over the 20 tests and the improvement of the performance have
been shown in Table 2. The persuasive comparison shows that the new structure attains the best
eﬀectiveness, i.e., the smallest error. From Fig. 1, we see that the surface of Mayas’ error function in
new structures is monotonically decreasing and converge to 0.
Table 2: Comparison of average error for Mayas’ approximate error function.
Learning Rate Average Old Average New Improvement %
0.001
0.005
0.01
0.05
0.1
0.0042
0.0043
0.0040
0.0039
0.0040
0.0041
0.0040
0.0039
0.0033
0.0035
2.38
6.98
2.5
15.38
12.5
Figure 1: Comparison of error for Mayas approximation problem.
4.2 Example 2: Gabor function approximate
In this example, a MPSPNN is used to approximate the Gabor function as:
(cid:18) x2 + y2
(cid:19)
HG =
2π(0.5)2 exp
2(0.5)2
cos(2π(x + y))
(31)
The training samples of the neural network are 36 input points selected from an evenly 6 × 6 grid
on −0.5 ≤ x ≤ 0.5 and −0.5 ≤ y ≤ 0.5. Similarly, the test samples are 400 input points selected
from 20 × 20 grid on −0.5 ≤ y ≤ 0.5 and −0.5 ≤ y ≤ 0.5. By performing Algorithms 1 with
η = 0.009, λ = 0.0001 and iterationmax = 5000, 1, σ1, σ2σ3, σ1σ2
3 are selected
to approximate the complete multinomial. The new structure corresponds to Fig. 3(c) and the new
weighted linear combination is
2, and σ3
2, σ2σ2
3, σ3
1, σ3
w0,1 + w0,2σ1 + w0,3σ2σ3 + w0,4σ1σ2
2 + w0,5σ2σ2
3 + w0,6σ3
1 + w0,7σ3
2 + w0,8σ3
(32)
and we have
∧1 = ∅,∧2 = {1},∧3 = {2, 3},∧4 = {1, 2, 2},∧5 = {2, 3, 3},∧6 = {1, 1, 1},∧7 = {2, 2, 2},∧8 = {3, 3, 3}
(33)
Then, we perform Algorithms 2 and use the test samples to evaluate our method. The average
error and the improvement of the performance have been shown in Table 3. The results show that
the new structure attains the smallest error. From Fig. 2, we see that the surface of Gabor error
function in new structures is monotonically decreasing and converge to 0, as predicted by Theorem
1.
Table 3: Comparison of average error for Gabor approximate error function.
Learning Rate Average Old Average New Improvement %
0.001
0.005
0.01
0.05
0.1
0.0131
0.0133
0.0130
0.0132
0.0131
0.0075
0.0065
0.0064
0.0063
0.0055
42.75
51.13
50.77
52.27
58.02
Figure 2: Comparison of error for Gabor approximation problem.
4.3 Example 3: Sonar data classiﬁcation
Sonar problem is a well-known benchmark dataset, which aims to classify reﬂected sonar signals
into two categories (metal cylinders and rocks). The related data set comprises 208 input vectors,
each with 60 components. In this example, 4-fold cross validation is used to perform experiments,
that is, 75% samples for training and 25% samples for testing are stochastically selected from the
208 samples. After performing our method, 1, σ3, σ1σ2, σ2
2 are selected to to
approximate the complete multinomial. The new structure corresponds to Fig. 3(d) and the new
weighted linear combination is
3 and σ3
3, σ2σ2
1, σ2σ2
1, σ2
w0,1 + w0,2σ3 + w0,3σ1σ2 + w0,4σ2
1 + w0,5σ2
3 + w0,6σ2σ2
1 + w0,7σ2σ2
3 + w0,8σ3
(34)
Then, we have
∧1 = ∅,∧2 = {2},∧3 = {1, 2},∧4 = {1, 1},∧5 = {3, 3},∧6 = {1, 1, 2},∧7 = {2, 3, 3},∧8 = {2, 2, 2}
(35)
Table 4: Comparison of average classiﬁcation accuracy for sonar problem.
Round Old Train New Train Impprovment% Old Test New Test
Improvement%
Overall
79.42
86.22
85.12
80.77
79.97
82.30
89.26
95.67
87.51
90.87
84.94
89.65
11.67
10.39
2.77
11.77
6.03
8.67
71.16
81.02
78.85
68.52
76.28
75.17
83.18
90.74
81.73
82.85
80.45
83.79
15.58
11.32
3.59
18.93
5.32
10.85
Table 5: Comparison of the best classiﬁcation accuracy for sonar problem.
Round Old Train New Train Impprovment% Old Test New Test
Improvement%
Overall
89.47
94.87
95.62
84.62
88.46
90.61
99.36
99.36
96.15
100.0
100.0
98.97
10.48
4.62
0.55
16.66
12.25
8.82
84.62
89.47
88.46
89.47
79.49
86.30
98.11
95.62
92.31
100.0
88.46
94.90
14.76
6.65
4.26
11.12
10.65
9.49
Table 6: Comparison of the worst classiﬁcation accuracy for sonar problem.
Round Old Train New Train Impprovment% Old Test New Test
Improvement%
Overall
71.79
73.08
73.08
71.79
75.00
72.95
80.77
91.03
78.21
75.00
78.21
80.64
11.77
21.88
6.85
4.37
4.19
10.01
57.69
71.79
61.54
50.0
71.79
62.56
69.23
84.62
65.38
71.79
78.21
73.85
18.18
16.41
6.05
35.78
8.56
16.55
In both structures, 20 trials are carried out for each learning algorithm. In Tables 4, 5 and 6,
we compare average accuracy, best accuracy and worst accuracy of classiﬁcation of both structures,
respectively. In all three tables, it can be seen that new structure is more advantageous than the old
structure. These show that our new structure is better and monotonically decreasing and converge
to 0 during the iterative learning as predicted by Theorem 1.
4.4 Example 4: Pima Indians diabetes data classiﬁcation
To verify the theoretical evaluation of MSPSNNs, we used a Pima Indians Diabetes Database, which
comprises 768 samples with 8 attributes. The dataset is available at UCI machine learning repository
(http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes). 4-fold cross validation is used to
perform our method.
After that, 1, σ1σ2, σ1σ3, σ1σ4, σ2σ3, σ2σ4, σ3σ4 , σ2
1, σ2
2, σ2
3, σ2
4, σ1σ2
2, σ3σ2
2, σ1σ2
3, σ4σ2
3 are selected.
The new structure corresponds to Fig. 5 and the new weighted linear combination is
w0,1 + w0,2σ1σ2 + w0,3σ1σ3 + w0,4σ1σ4 + w0,5σ2σ3 + w0,6σ2σ4 + w0,7σ3σ4 + w0,8σ2
w0,9σ2
2 + w0,13σ3σ2
3 + w0,15σ4σ2
2 + w0,14σ1σ2
2 + w0,10σ2
3 + w0,11σ2
4 + w0,12σ1σ2
1...
(36)
Then, we have
∧1 = ∅,∧2 = {1, 2},∧3 = {1, 3},∧4 = {1, 4},∧5 = {2, 3},∧6 = {2, 4},∧7 = {3, 4},∧8 = {1, 1}...
∧9 = {2, 2},∧10 = {3, 3},∧11 = {4, 4},∧12 = {1, 2, 2},∧13 = {2, 2, 3},∧14 = {1, 3, 3},∧15 = {3, 3, 4}
(37)
The results of comparative analysis experiments using old and new structure for four-order are also
presented, paying particular attention to average error, average best error and average wort correct
classiﬁcation shown in Tables 6, 7 and 8. These lead to verify the theoretical evaluation of SPSNNs
learning with new structure is better and monotonically decreasing and converge to 0 during the
iterative learning as predicted by Theorem 1.
5 Conclusion
In this study, we use the smoothing L1/2 regularization to automatically select some appropriate
terms to approximate the complete Kolmogorov-Gabor Multinomial for the product layer of SP-
SNNs.Numerical experiments are implemented for Mayas’ function problem, Gabor function prob-
lem, Sonar data classiﬁcation and Pima Indians diabetes data classiﬁcation. The numerical results
(a)
(b)
(c)
(d)
Figure 3: (a) Old structure; (b) New structure based on Mayas’ function approximation; (c) New
structure based on Gabor function approximation; (d) New structure based on Sonar problem ap-
proximation.
Table 7: Comparison of average error for Pima Indians problem.
Round Old Train New Train Impprovment% Old Test New Test
Improvement%
Overall
78.76
73.04
84.33
81.01
75.97
78.62
82.53
79.38
84.70
85.81
81.22
82.73
4.67
8.32
0.44
5.75
6.68
5.09
72.31
68.24
79.44
75.86
71.52
73.47
77.57
84.62
65.38
71.79
78.21
77.4
7.02
9.2
0.24
4.18
5.89
7.79
Table 8: Comparison of best error for Pima Indians problem.
Round Old Train New Train Impprovment% Old Test New Test
Improvement%
Overall
88.13
77.01
89.17
87.08
78.75
84.03
89.39
85.97
91.37
90.00
83.76
88.10
1.42
11.0
2.44
3.30
6.17
4.73
76.67
74.58
85.97
81.53
75.63
78.88
786.61
78.75
88.06
83.93
79.70
83.41
12.18
5.44
2.40
2.90
5.24
5.70
demonstrate that the terms, or equivalently the product nodes selected by using smoothing L1/2
regularization have been proved the capability of providing more possibility powerful mapping, which
is diﬀerent from that in old structure.
References
[1] C.K. Li, A sigma-pi-sigma neural network, Neural Process. Lett. 17 (2003), pp. 1-9.
∏ ∏ ∏ ∏ ∏ ∏ ∏ 1 Input layer ∑1 ∏ x ∈ RM ∑2 τ ∈ RQ σ ∈ RN w0,8 w0,1 wNM w11 ∑ ∑ ∑ ∑ x1 xM x2 x3 ∏ ∏ ∏ ∏ ∏ 1 1 ∏ ∏ ∏ ∏ ∏ ∏ 1 Input layer ∑1 ∏ x ∈ RM ∑2 τ ∈ RQ σ ∈ RN w0,8 w0,1 wNM w11 ∑ ∑ ∑ ∑ x1 xM x2 x3 ∏ ∏ ∏ ∏ ∏ 1 1 ∏ ∏ ∏ ∏ ∏ ∏ 1 Input layer ∑1 ∏ x ∈ RM ∑2 τ ∈ RQ σ ∈ RN w0,8 w0,1 wNM w11 ∑ ∑ ∑ ∑ x1 xM x2 x3 ∏ ∏ ∏ ∏ ∏ 1 1 ∏ ∏ ∏ ∏ ∏ ∏ ∏ 1 Input layer ∑1 ∏ x ∈ RM ∑2 τ ∈ RQ σ ∈ RN w0,8 w0,1 wNM w11 ∑ ∑ ∑ ∑ x1 xM x2 x3 ∏ ∏ ∏ ∏ ∏ 1 1 Figure 4: Four-input three-order Old structure
Figure 5: Four-input three-order new structure based on Pima Indians diabetes data classiﬁcation.
Table 9: Comparison of wort error for Pima indians problem.
Round Old Train New Train Impprovment% Old Test New Test
Improvement%
Overall
73.61
63.49
76.04
69.44
73.96
71.31
75.41
73.0
77.08
76.04
79.44
76.19
2.42
13.94
1.36
9.07
7.2
13.0
69.74
61.98
69.44
61.98
69.44
66.52
71.59
66.76
71.40
69.44
71.98
70.23
2.62
7.43
2.78
11.35
3.59
5.43
[2] Y. Liu, Z. Li, D. Yang, K.S. Mohamed, J. Wang, W. Wu, Convergence of batch gradient learning
algorithm with smoothing L1/2 regularization for Sigma-pi-sigma neural networks. Neurocom-
puting 151 (2015), pp. 333-341.
[3] M.M. Gupta, N. Homma, Z.G. Hou, M.G. Solo, I. Bukovsky, Higher order neural networks:
fundamental theory and applications. Artiﬁcial Higher Order Neural Networks for Computer
Science and Engineering: Trends for Emerging Applications pp. 397-422, 2010.
[4] D.E. Rumelhart, G.E. Hinton, R.J. Williams, (1985) Learning internal representations by error
propagation (No. ICS-8506). California Univ San Diego La Jolla Inst for Cognitive Science.
∏ ∏ 1 Input layer ∑1 ∏ x ∈ RM ∑2 τ ∈ RQ σ ∈ RN w0,15 w0,1 wNM w11 ∑ ∑ ∑ ∑ x1 xM x2 x3 ∏ ∏ ∏ ∏ ∏ 1 1 x4 ∑ ∏ ∏ ∏ ∏ ∏ ∏ ∏ ∏ ∏ 1 Input layer ∑1 ∏ x ∈ RM ∑2 τ ∈ RQ σ ∈ RN w0,15 w0,1 wNM w11 ∑ ∑ ∑ ∑ x1 xM x2 x3 ∏ ∏ ∏ ∏ ∏ 1 1 x4 ∑ ∏ ∏ ∏ ∏ ∏ ∏ ∏ [5] G.P. Liu, (2012) Nonlinear identiﬁcation and control: a neural network approach. Springer
Science & Business Media.
[6] M.Fallahnezhad,M.H. Moradi,S. Zaferanlouei, (2011) A hybrid higher order neural classiﬁer for
handling classiﬁcation problems. Expert Systems with Applications, 38(1), 386-393.
[7] A. R. barron, Predicted Squared Error: A criterion for automatic model selection,” Self-
Organizing Methods in Modeling: GMDH Type Algorithms (S.J. Farlow, Ed.), Marcel Dekker,
Inc., NY, Chap.4 (1984), pp. 87-103.
[8] V. S. Stometta and B. A. Hubermann, An improved three-layer back propagation algorithm, in
Proc. IEEE IJCNN, vol. 2, (1987), pp.637-643.
[9] H. Zhang, Y. Tang, X. Liu, (2015) Batch gradient training method with smoothing l0 regular-
ization for feedforward neural networks. Neural Computing and Applications, 26(2), 383-390.
[10] W. Wu, Q.W. Fan, J.M. Zurada et al., Batch gradient method with smoothing L1/2 regularization
for training of feedforward neural networks,Neural Networks 50 (2014) 72-78.
[11] R. Reed, Pruning algorithms-a survey,IEEE Transactionson Neural Networks 8 (1997) 185-204.
[12] S. Shalev-Shwartz, T. Zhang, (2014) Accelerated proximal stochastic dual coordinate ascent for
regularized loss minimization. In International Conference on Machine Learning (pp. 64-72).
[13] J. Tang, S. Alelyani, H. Liu, (2014) Feature selection for classiﬁcation: A review. Data Classiﬁ-
cation: Algorithms and Applications, pp. 37.
[14] S. Scardapane, D. Comminiello, A. Hussain, et al. (2017) Group sparse regularization for deep
neural networks. Neurocomputing 241:81-89.
[15] Z. Xu, H. Zhang, Y. Wang, X. Chang, L1/2 regularization, Science ChinaInformation Sciences,
Vol. 53, No. 6 (2010) pp. 1159-1165.
[16] W. Wu, H. Shao, Z. Li, Convergence of batch BP algorithm with penalty for FNN training, in
Neural Information Processing, (2006) 562-569.
[17] J. Wang, W Wu, J. M. Zurada, 2012, Computational properties and conver-gence analysis of
BPNN for cyclic and almost cyclic learning with penalty, Neural Networks, Vol. 33, pp. 127-135.
[18] A. S. Weigend, D. E. Rumelhart, B. Huberman, Generalization by weight-elimination applied to
currency exchange rate prediction, in Neural Networks, IJCNN 1991 International Joint Confer-
ence on, Seattle, (1991), PP.837-841.
[19] Z. Xu, X. Chang, F. Xu, H. Zhang, L1/2 Regularization: A Thresholding Representation Theory
and a Fast Solver, Neural Networks and Learning Sys-tems, IEEE Transactions on, Vol. 23, No.
7, (2012) pp. 1013-1027.
[20] M. Yuan, Y. Lin, Model selection and estimation in regression with grouped variables, Journal
of the Royal Statistical Society: Series B (Statistical Methodology), Vol. 68 (2006) pp. 49-67.
