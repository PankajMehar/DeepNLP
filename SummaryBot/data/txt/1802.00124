Published as a conference paper at ICLR 2018
RETHINKING THE SMALLER-NORM-LESS-
INFORMATIVE ASSUMPTION IN CHANNEL PRUNING
OF CONVOLUTION LAYERS
Jianbo Ye∗
College of Information Sciences and Technology
Pennsylvania State University
jxy198@ist.psu.edu
James Z. Wang
College of Information Sciences and Technology
Pennsylvania State University
jwang@ist.psu.edu
Xin Lu, Zhe Lin
Adobe Research
{xinl,zlin}@adobe.com
ABSTRACT
Model pruning has become a useful technique that improves the computational
efﬁciency of deep learning, making it possible to deploy solutions on resource-
limited scenarios. A widely-used practice in relevant work assumes that a smaller-
norm parameter or feature plays a less informative role at the inference time. In
this paper, we propose a channel pruning technique for accelerating the compu-
tations of deep convolutional neural networks (CNNs), which does not critically
rely on this assumption. Instead, it focuses on direct simpliﬁcation of the channel-
to-channel computation graph of a CNN without the need of performing a compu-
tational difﬁcult and not always useful task of making high-dimensional tensors of
CNN structured sparse. Our approach takes two stages: the ﬁrst being to adopt an
end-to-end stochastic training method that eventually forces the outputs of some
channels being constant, and the second being to prune those constant channels
from the original neural network by adjusting the biases of their impacting layers
such that the resulting compact model can be quickly ﬁne-tuned. Our approach
is mathematically appealing from an optimization perspective and easy to repro-
duce. We experimented our approach through several image learning benchmarks
and demonstrate its interesting aspects and the competitive performance.
INTRODUCTION
Not all computations in a deep neural network are of equal importance. In the conventional deep
learning pipeline, an expert crafts a neural architecture and trains it against a prepared dataset. The
success of training a deep model often requires trial and error, and such loop usually has little control
on prioritizing the computations happening in the neural network. Recently researchers started to
develop model-simpliﬁcation methods for convolutional neural networks (CNNs), bearing in mind
that some computations are indeed non-critical or redundant and hence can be safely removed from
a trained model without substantially degrading the model’s performance. Such methods not only
accelerate computational efﬁciency but also sometimes alleviate the model’s overﬁtting effects.
Discovering which subsets of the computations of a trained CNN are more reasonable to prune,
however, is nontrivial. Existing methods can be categorized from either the learning perspective or
the computational perspective. From the learning perspective, some methods use a data-independent
approach where the training data does not assist in determining which part of a trained CNN should
be pruned, e.g. He et al. (2017) and Zhang et al. (2016), while others use a data-dependent approach
through typically a joint optimization in generating pruning decisions, e.g., Han et al. (2015) and An-
war et al. (2017). From the computational perspective, while most approaches focused on setting
∗This work was done when Jianbo Ye interned at Adobe in 2017 summer.
Published as a conference paper at ICLR 2018
the dense weights of convolutions or linear maps to be structured sparse, we propose here a method
adopting a new conception to achieve in effect the same goal.
Instead of regarding the computations of a CNN as a collection of separate computations sitting at
different layers, we view it as a network ﬂow that delivers information from the input to the output
through different channels across different layers. We believe saving computations of a CNN is
not only about reducing what are calculated in an individual layer, but perhaps more importantly
also about understanding how each channel is contributing to the entire information ﬂow in the
underlying passing graph as well as removing channels that are less responsible to such process.
Inspired by this new conception, we propose to design a “gate” at each channel of a CNN, controlling
whether its received information is actually sent out to other channels after processing. If a channel
“gate” closes, its output will always be a constant. In fact, each designed “gate” will have a prior
intention to close, unless it has a “strong” duty in sending some of its received information from the
input to follow-up layers. We ﬁnd that implementing this idea in pruning CNNs is unsophisticated,
as will be detailed in Sec 4.
Our method neither introduces any extra parameters to the existing CNN, nor changes its computa-
tion graph. In fact, it only introduces marginal overheads to existing gradient training of CNN. It also
possess an attractive feature that one can successively build multiple compact models with different
inference performances in a single round of resource-intensive training (as were done in our exper-
iments). This eases the process to choose a balanced model to deploy in production. Probably, the
only applicability constraint of our method is that all convolutional layers and fully-connected layer
(except the last layer) in the CNN should be batch normalized (Ioffe & Szegedy, 2015). Given batch
normalization has becomes a widely adopted ingredient in designing state-of-the-art deep learning
models, and many successful CNN models are using it, we believe our approach has a wide scope
of potential impacts.1
In this paper, we start from rethinking a basic assumption widely explored in existing channel prun-
ing work. We point out several issues and gaps in realizing this assumption successfully. Then,
we propose our alternative approach, which work around several numerical difﬁculties. Finally, we
experiment our method across different benchmarks and validate its usefulness and strengths.
2 RELATED WORK
Reducing the size of neural network for speeding up its computational performance at inference
time has been a long studied topic in the community of neural network and deep learning. Pioneer
works include Optimal Brain Damage (LeCun et al., 1990) and Optimal Brain Surgeon (Hassibi
& Stork, 1993). More recent developments focused on either reducing the structural complexity
of a provided network or training a compact or simpliﬁed network from scratch. Our work can be
categorized into the former type. Thus, the literature review below revolves around that reducing the
structural complexity.
To reduce the structural complexity of deep learning models, previous work have largely focused
on sparsifying the weights of convolutional kernels or the feature maps across multiple layers in
a network (Anwar et al., 2017; Han et al., 2015). More recently, some work proposes to impose
structured sparsity on those vector components motivated from the implementation perspective on
specialized hardware (Wen et al., 2016; Zhou et al., 2016; Alvarez & Salzmann, 2016; Lebedev
& Lempitsky, 2016). Yet as argued by authors of (Molchanov et al., 2017), regularization-based
pruning techniques require per layer sensitivity analysis which adds extra computations. Molchanov
et al. (2017) relies on global rescaling of criteria for all layers and does not require sensitivity esti-
mation, a beneﬁcial feature that our approach also has. To our knowledge, it is also much unclear
how widely useful are those work in deep learning. In Section 3, we discuss in details the potential
issues in regularization-based pruning techniques potentially hurting them being widely applicable,
especially for those which regularize high-dimensional tensor parameters or use magnitude-based
pruning methods. Our approach works around the mentioned issues by constraining the anticipated
pruning operations only on batch normalized convolutional layers.
Instead of posing structured
1For convolution layer which is not originally trained with batch normalization, one can still convert it into a
“near equivalent” convolution layer with batch normalization by removing the bias term b and properly setting
σ + , β = b + µ, where σ and µ are estimated from the outputs of the convolution across all training
γ =
samples.
Published as a conference paper at ICLR 2018
sparsity on kernels or feature maps, we enforce sparsity on the scaling parameter γ in batch normal-
ization operator. This blocks the sample-wise information passing through part of the channels in
convolution layer, and in effect implies one can safely remove those channels.
A recent work (Huang & Wang, 2017) used similar technique as ours to remove unimportant residual
modules in ResNet by introducing extra scaling factors to the original network, but some optimiza-
tion subtleties as pointed out in our paper were not well explained. Another recent work called
Network-Slimming (Liu et al., 2017) also aims to sparsify the scaling parameters of batch normal-
ization. But instead of using off-the-shelf gradient learning like theirs, we propose a new algorithmic
approach based on ISTA and rescaling trick, improving robustness and speed of the undergoing opti-
mization. In particular, The work of Liu et al. (2017) was able to prune VGG-A model on ImageNet.
It is unclear how their work would deal with the γ-W rescaling effect and whether their approach
can be adopted to large pre-trained models, such as ResNets and Inceptions. We experimented with
the pre-trained ResNet-101 and compared to most recent work that were shown to work well with
large CNNs. We also experimented with an image segmentation model which has an inception-like
module (pre-trained on ImageNet) to locate foreground objects.
3 RETHINKING THE SMALLER-NORM-LESS-INFORMATIVE ASSUMPTION
In most regularized linear regressions, a large-norm coefﬁcient is often a strong indicator of a highly
informative feature. This has been widely perceived in statistics and machine learning community,
and removing features which have a small coefﬁcient does not substantially affect the regression er-
rors. Therefore, it has been an established practice to use tractable norm to regularize the parameters
in optimizing a model and pick the important ones by comparing their norms after training. How-
ever, this assumption is not unconditional. By using Lasso or ridge regression to select important
predictors in linear models, one always has to ﬁrst normalize each predictor variable. Otherwise,
the result might not be explanatory. For example, ridge regression penalizes more the predictors
which has low variance, and Lasso regression enforces sparsity of coefﬁcients which are already
small in OLS. Such normalization condition for the right use of regularization is often unsatisﬁed
for nonconvex learning. For example, one has to carefully consider two issues outlined below. We
provides these two cases to exemplify how regularization could fail or be of limited usage. There
deﬁnitely exist ways to avoid the speciﬁc failures.
Model Reparameterization. In the ﬁrst case, we show that it is not easy to have ﬁne-grained control
of the weights’ norms across different layers. One has to either choose a uniform penalty in all layers
or struggle with the reparameterization patterns. Consider to ﬁnd a deep linear (convolutional)
network subject to a least square with Lasso: for λ > 0,
n(cid:88)
i=1
E(x,y)∼D(cid:107)W2n ∗ . . . ∗ W2 ∗ W1 ∗ x − y(cid:107)2 + λ
min
{Wi}2n
i=1
(cid:107)W2i(cid:107)1 .
The above formulation is not a well-deﬁned problem because for any parameter set {Wi}2n
can always ﬁnd another parameter set {W (cid:48)
keeping the corresponding l0 norm unchanged by actually setting
i=1, one
i}2n
i=1 such that it achieves a smaller total loss while
i = αWi, i = 1, 3, . . . , 2n − 1 and W (cid:48)
W (cid:48)
i = Wi/α, i = 2, 4, . . . , 2n ,
where α > 1. In another word, for any  > 0, one can always ﬁnd a parameter set {Wi}2n
i=1 (which
is usually non-sparse) that minimizes the ﬁrst least square loss while having its second Lasso term
less than .
We note that gradient-based learning is highly inefﬁcient in exploring such model reparameteriza-
tion patterns. In fact, there are some recent discussions around this (Dinh et al., 2017). If one adopts
a pre-trained model, and augments its original objective with a new norm-based parameter regu-
larization, the new gradient updates may just increase rapidly or it may take a very long time for
the variables traveling along the model’s reparameterization trajectory. This highlights a theoretical
gap questioning existing sparsity inducing formulation and actual computational algorithms whether
they can achieve widely satisfactory parameter sparsiﬁcation for deep learning models.
Published as a conference paper at ICLR 2018
Transform Invariance. In the second case, we show that batch normalization is not compatible
with weight regularization. The example is penalizing l1- or l2-norms of ﬁlters in convolution layer
which is then followed by a batch normalization: at the l-th layer, we let
xl+1 = max{γ · BNµ,σ,(W l ∗ xl) + β, 0},
where γ and β are vectors whose length is the number of channels. Likewise, one can easily
see that any uniform scaling of W l which changes its l1- and l2-norms would have no ef-
fects on the output xl+1. Alternatively speaking, if one is interested in minimizing the weight
norms of multiple layers together, it becomes unclear how to choose proper penalty for each
layer. Theoretically, there always exists an optimizer that can change the weight to one with
inﬁnitesimal magnitude without hurting any inference performance. As pointed by one of the
reviewers, one can tentatively avoid this issue by projecting the weights to the surface of unit ball.
Then one has to deal with a non-convex feasible set of parameters, causing extra difﬁculties in
developing optimization for data-dependent pruning method.
It is also worth noting that some
existing work used such strategy in a layer-by-layer greedy way (He et al., 2017; Zhang et al., 2016).
Based on this discussion, many existing works which claim to use Lasso, group Lasso (e.g. Wen
et al. (2016); Anwar et al. (2017)), or thresholding (e.g. Molchanov et al. (2017)) to enforce param-
eter sparsity have some theoretical gaps to bridge. In fact, many heuristic algorithms in neural net
pruning actually do not naturally generate a sparse parameterized solution. More often, thresholding
is used to directly set certain subset of the parameters in the network to zeros, which can be problem-
atic. The reason is in essence around two questions. First, by setting parameters less than a threshold
to zeros, will the functionality of neural net be preserved approximately with some guarantees? If
yes, then under what conditions? Second, how should one set those thresholds for weights across
different layers? Not every layer contributes equally in a neural net. It is expected that some layers
act critically for the performance but only use a small computation and memory budget, while some
other layers help marginally for the performance but consume a lot resources. It is naturally more
desirable to prune calculations in the latter kind of layers than the former.
In contrast with these existing approaches, we focus on enforcing sparsity of a tiny set of parameters
in CNN — scale parameter γs in all batch normalization. Not only placing sparse constraints on γ
is simpler and easier to monitor, but more importantly, we have two strong reasons:
1. Every γ always multiplies a normalized random variable, thus the channel importance be-
comes comparable across different layers by measuring the magnitude values of γ;
2. The reparameterization effect across different layers is avoided if its follow-up convolution
layer is also batch normalized. In other words, the impacts from the scale changes of γ
parameter are independent across different layers.
Nevertheless, our current work still falls short of a strong theoretical guarantee. But we believe by
working with normalized feature inputs and their regularized coefﬁcients together, one is closer to
a more robust and meaningful approach. Sparsity is not the goal, the goal is to ﬁnd less important
channels using sparsity inducing formulation.
4 CHANNEL PRUNING OF BATCH-NORMALIZED CNN
We describe the basic principle and algorithm of our channel pruning technique.
4.1 PRELIMINARIES
Pruning constant channels. Consider convolution with batch normalization:
xl+1 = max(cid:8)γl · BNµl,σl,l (W l ∗ xl) + βl, 0(cid:9) .
For the ease of notation, we let γ = γl. Note that if some element in γ is set to zero, say, γ[k] = 0,
its output image xl+1
:,:,:,k becomes a constant βk, and a convolution of a constant image channel is
almost everywhere constant (except for padding regions, an issue to be discussed later). Therefore,
we show those constant image channels can be pruned while the same functionality of network is
approximately kept:
Published as a conference paper at ICLR 2018
• If the follow-up convolution layer does not have batch normalization,
xl+2 = max(cid:8)W l+1 ∗ xl+1 + bl+1, 0(cid:9) ,
its values (a.k.a. elements in β) is absorbed into the bias term by the following equation
such that
new := bl+1 + I(γ = 0) · ReLU(β)T sum reduced(W l+1
bl+1
xl+2 ≈ max(cid:8)W l+1 ∗γ xl+1 + bl+1
new, 0(cid:9) ,
:,:,·,·) ,
where ∗γ denotes the convolution operator which is only calculated along channels indexed
by non-zeros of γ. Remark that W ∗ = sum reduced(W:,:,·,·) if W ∗
i,j Wi,j,a,b.
a,b =(cid:80)
(cid:0)W l+1 ∗ xl+1(cid:1) + βl+1, 0(cid:9) ,
• If the follow-up convolution layer has batch normalization,
xl+2 = max(cid:8)γl+1 · BNµl+1,σl+1,l+1
instead its moving average is updated as
new := µl+1 − I(γ = 0) · ReLU(β)T sum reduced(W l+1
µl+1
:,:,·,·) ,
(cid:110)
γl+1 · BNµl+1
new,σl+1,l+1
(cid:0)W l+1 ∗γ xl+1(cid:1) + βl+1, 0
(cid:111)
such that
xl+2 ≈ max
Remark that the approximation (≈) is strictly equivalence (=) if no padding is used in the convolu-
tion operator ∗, a feature that the parallel work Liu et al. (2017) does not possess. When the original
model uses padding in computing convolution layers, the network function is not strictly preserved
after pruning. In our practice, we ﬁne-tune the pruned network to ﬁx such performance degradation
at last. In short, we formulate the network pruning problem as simple as to set more elements in γ
to zero. It is also much easier to deploy the pruned model, because no extra parameters or layers are
introduced into the original model.
To better understand how it works in an entire CNN, imagine a channel-to-channel computation
graph formed by the connections between layers. In this graph, each channel is a node, their infer-
ence dependencies are represented by directed edges. The γ parameter serves as a “dam” at each
node, deciding whether let the received information “ﬂood” through to other nodes following the
graph. An end-to-end training of channel pruning is essentially like a ﬂood control system. There
suppose to be rich information of the input distribution, and in two ways, much of the original input
information is lost along the way of CNN inference, and the useful part — that is supposed to be
preserved by the network inference — should be label sensitive. Conventional CNN has one way
to reduce information: transforming feature maps (non-invertible) via forward propagation. Our
approach introduces the other way: block information at each channel by forcing its output being
constant using ISTA.
ISTA. Despite the gap between Lasso and sparsity in the non-convex settings, we found that
ISTA (Beck & Teboulle, 2009) is still a useful sparse promoting method. But we just need to
use it more carefully. Speciﬁcally, we adopt ISTA in the updates of γs. The basic idea is to project
the parameter at every step of gradient descent to a potentially more sparse one subject to a proxy
problem: let l denote the training loss of interest, at the (t + 1)-th step, we set
γt+1 = min
(1)
where ∇γlt is the derivative with respect to γ computed at step t, µt is the learning rate, λ is the
penalty. In the stochastic learning, ∇γlt is estimated from a mini-batch at each step. Eq. (1) has
closed form solution as
(cid:107)γ − γt + µt∇γlt(cid:107)2 + λ(cid:107)γ(cid:107)1 ,
µt
γt+1 = proxµtλ(γt − µt∇γlt) ,
where proxη(x) = max{|x|− η, 0}· sgn(x). The ISTA method essentially serves as a “ﬂood control
system” in our end-to-end learning, where the functionality of each γ is like that of a dam. When
γ is zero, the information ﬂood is totally blocked, while γ (cid:54)= 0, the same amount of information is
passed through in form of geometric quantities whose magnitudes are proportional to γ.
Published as a conference paper at ICLR 2018
Scaling effect. One can also see that if γ is scaled by α meanwhile W l+1 is scaled by 1/α, that is,
γ := αγ,
W l+1 :=
W l+1
the output xl+2 is unchanged for the same input xl. Despite not changing the output, scaling of γ and
W l+1 also scales the gradients ∇γl and ∇W l+1l by 1/α and α, respectively. As we observed, the
parameter dynamics of gradient learning with ISTA depends on the scaling factor α if one decides
to choose it other than 1.0. Intuitively, if α is large, the optimization of W l+1 is progressed much
slower than that of γ.
4.2 THE ALGORITHM
We describe our algorithm below. The following method applies to both training from scratch or
re-training from a pre-trained model. Given a training loss l, a convolutional neural net N , and
hyper-parameters ρ, α, µ0, our method proceeds as follows:
1. Computation of sparse penalty for each layer. Compute the memory cost per channel
for each layer denoted by λl and set the ISTA penalty for layer l to ρλl. Here
kl
λl =
w · I i
I i
(cid:88)
l(cid:48)∈T (l)
w · kl
h · cl−1 +
w · kl(cid:48)
kl(cid:48)
h · cl(cid:48)
+ I l
w · I l
(2)
 ,
w · I i
w · kl
size of follow-up convolution at layer l(cid:48).
h is the size of input image of the neural network.
h is the kernel size of the convolution at layer l. Likewise, kl(cid:48)
where
• I i
w · kl(cid:48)
• kl
• T (l) represents the set of the follow-up convolutional layers of layer l
• cl−1 denotes the channel size of the previous layer, which the l-th convolution operates
• I l
denotes the channel size of one follow-up layer l(cid:48).
h is the image size of the feature map at layer l.
over; and cl(cid:48)
w · I l
h is the kernel
2. γ-W rescaling trick. For layers whose channels are going to get reduced, scale all γls
in batch normalizations by α meanwhile scale weights in their follow-up convolutions by
1/α.
3. End-to-End training with ISTA on γ. Train N by the regular SGD, with the exception
that γls are updated by ISTA, where the initial learning rate is µ0. Train N until the loss l
plateaus, the total sparsity of γls converges, and Lasso ρ(cid:80)
γl are zero and output the pruned model (cid:101)N by absorbing all constant channels into follow-
5. γ-W rescaling trick. For γls and weights in (cid:101)N which were scaled in Step 2 before training,
6. Fine-tune (cid:101)N using regular stochastic gradient learning.
4. Post-process to remove constant channels. Prune channels in layer l whose elements in
scale them by 1/α and α respectively (scaling back).
up layers (as described in the earlier section.).
l λl(cid:107)γl(cid:107)1 converges.
Remark that choosing a proper α as used in Steps 2 and 5 is necessary for using a large µt · ρ in
ISTA, which makes the sparsiﬁcation progress of γls faster.
4.3 GUIDELINES FOR TUNING HYPER-PARAMETERS
We summarize the sensitivity of hyper-parameters and their impacts for optimization below:
• µ (learning rate): larger µ leads to fewer iterations for convergence and faster progress of
• ρ (sparse penalty): larger ρ leads to more sparse model at convergence. If trained with a
sparsity. But if if µ too large, the SGD approach wouldn’t converge.
very large ρ, all channels will be eventually pruned.
Published as a conference paper at ICLR 2018
• α (rescaling): we use α other than 1. only for pretrained models, we typically choose α
from {0.001, 0.01, 0.1, 1} and smaller α warms up the progress of sparsity.
We recommend the following parameter tuning strategy. First, check the cross-entropy loss and the
regularization loss, select ρ such that these two quantities are comparable at the beginning. Second,
choose a reasonable learning rate. Third, if the model is pretrained, check the average magnitude of
γs in the network, choose α such that the magnitude of rescaled γl is around 100µλlρ. We found
as long as one choose those parameters in the right range of magnitudes, the optimization progress
is enough robust. Again one can monitor the mentioned three quantities during the training and
terminate the iterations when all three quantities plateaus.
There are several patterns we found during experiments that may suggest the parameter tuning has
not been successful. If during the ﬁrst few epochs the Lasso-based regularization loss keeps decreas-
ing linearly while the sparsity of γs stays near zero, one may decrease α and restart. If during the
ﬁrst few epochs the sparsity of γs quickly raise up to 100%, one may decrease ρ and restart. If during
the ﬁrst few epochs the cross-entropy loss keeps at or increases dramatically to a non-informative
level, one may decrease µ or ρ and restart.
5 EXPERIMENTS
5.1 CIFAR-10 EXPERIMENT
We experiment with the standard image classiﬁcation benchmark CIFAR-10 with two different net-
work architectures: ConvNet and ResNet-20 (He et al., 2016). We resize images to 32 × 32 and
zero-pad them to 40 × 40. We pre-process the padded images by randomly cropping with size
32 × 32, randomly ﬂipping, randomly adjusting brightness and contrast, and standardizing them
such that their pixel values have zero mean and one variance.
ConvNet For reducing the channels in ConvNet, we are interested in studying whether one can
easily convert a over-parameterized network into a compact one. We start with a standard 4-layer
convolutional neural network whose network attributes are speciﬁed in Table 1. We use a ﬁxed
learning rate µt = 0.01, scaling parameter α = 1.0, and set batch size to 125.
Model A is trained from scratch using the base model with an initial warm-up ρ = 0.0002 for
30k steps, and then is trained by raising up ρ to 0.001. After the termination criterion are met, we
prune the channels of the base model to generate a smaller network called model A. We evaluate the
classiﬁcation performance of model A with the running exponential average of its parameters. It is
found that the test accuracy of model A is even better than the base model. Next, we start from the
pre-trained model A to create model B by raising ρ up to 0.002. We end up with a smaller network
called model B, which is about 1% worse than model A, but saves about one third parameters.
Likewise, we start from the pre-trained model B to create model C. The detailed statistics and its
pruned channel size are reported in Table 1. We also train a reference ConvNet from scratch whose
channel sizes are 32-64-64-128 with totally 224,008 parameters and test accuracy being 86.3%. The
referenced model is not as good as Model B, which has smaller number of parameters and higher
accuracy.
We have two major observations from the experiment:
(1) When the base network is over-
parameterized, our approach not only signiﬁcantly reduce the number of channels of the base model
but also improves its generalization performance on the test set. (2) Performance degradation seems
unavoidable when the channels in a network are saturated, our approach gives satisfactory trade-off
between test accuracy and model efﬁciency.
ResNet-20 We also want to verify our second observation with the state-of-art models. We choose
the popular ResNet-20 as our base model for the CIFAR-10 benchmark, whose test accuracy is 92%.
We focus on pruning the channels in the residual modules in ResNet-20, which has 9 convolutions
in total. As detailed in Table 2, model A is trained from scratch using ResNet-20’s network structure
as its base model. We use a warm-up ρ = 0.001 for 30k steps and then train with ρ = 0.005. We are
able to remove 37% parameters from ResNet-20 with only about 1 percent accuracy loss. Likewise,
Model B is created from model A with a higher penalty ρ = 0.01.
Published as a conference paper at ICLR 2018
layer
conv1
pool1
conv2
pool2
conv3
pool4
fc
param. size
test accuracy (%)
output
32 × 32
16 × 16
16 × 16
8 × 8
8 × 8
4 × 4
1 × 1
kernel
5 × 5
3 × 3
5 × 5
3 × 3
3 × 3
3 × 3
4 × 4
base
channel
model A model B model C
channel
channel
channel
96
192
192
384
1,986,760
89.0
53
86
67
41
64
52
31
52
40
128
0.001
309,655
89.5
128
0.002
207,583
87.6
127
0.008
144,935
86.0
Table 1: Comparisons between different pruned networks and the base network.
group - block
1-1
1-2
1-3
2-1
2-2
2-3
3-1
3-2
3-3
ResNet-20
model A
model B
channels
param size. = 281,304
test accuracy (%) = 92.0
channels
param size. = 176,596
test accuracy (%) = 90.9
channels
param size. = 90,504
test accuracy (%) = 88.8
16
16
16
32
32
32
64
64
64
12
11
32
28
28
47
34
25
27
18
16
25
Table 2: Comparisons between ResNet-20 and its two pruned versions. The last columns are the
number of channels of each residual modules after pruning.
5.2
ILSVRC2012 EXPERIMENT
We experiment our approach with the pre-trained ResNet-101 on ILSVRC2012 image classiﬁcation
dataset (He et al., 2016). ResNet-101 is one of the state-of-the-art network architecture in ImageNet
Challenge. We follow the standard pipeline to pre-process images to 224×224 for training ResNets.
We adopt the pre-trained TensorFlow ResNet-101 model whose single crop error rate is 23.6% with
about 4.47 × 107 parameters. 2 We set the scaling parameter α = 0.01, the initial learning rate
µt = 0.001, the sparsity penalty ρ = 0.1 and the batch size = 128 (across 4 GPUs). The learning
rate is decayed every four epochs with rate 0.86. We create two pruned models from the different
iterations of training ResNet-101: one has 2.36 × 107 parameters and the other has 1.73 × 107
parameters. We then ﬁne-tune these two models using the standard way for training ResNet-101,
and report their error rates. The Top-5 error rate increases of both models are less than 0.5%. The
Top-1 error rates are summarized in Table 3. To the best of our knowledge, only a few work has
reported their performances on this very large-scale benchmark w.r.t. the Top-1 errors. We compare
our approach with some recent work in terms of models’ parameter size, ﬂops, and error rates. As
shown in Table 3, our model v2 has achieved a compression ratio more than 2.5 while still maintains
more than 1% lower error rates than that of other state of the art models at comparable size of
parameters.
In the ﬁrst experiment (CIFAR-10), we train the network from scratch and allocate enough steps for
both γ and W adjusting their own scales. Thus, initialization of an improper scale of γ-W is not
really an issue given we optimize with enough steps. But for the pre-trained models which were
originally optimized without any constraints of γ, the γs scales are often unanticipated. It actually
takes as many steps as that of training from scratch for γ to warm up. By adopting the rescaling trick
setting α to a smaller value, we are able to skip the warm-up stage and quick start to sparsify γs. For
example, it might take more than a hundred epoch to train ResNet-101, but it only takes about 5-10
epochs to complete the pruning and a few more epochs to ﬁne-tune.
2https://github.com/tensorflow/models/tree/master/slim
Published as a conference paper at ICLR 2018
network
resnet-101 pruned (v2, ours)
resnet-34 pruned (Li et al., 2017)
param size.
resnet-50 pruned (Huang & Wang, 2017) ∼ 1.65 × 107
1.73 × 107
1.93 × 107
2.16 × 107
2.36 × 107
2.5 × 107
4.47 × 107
resnet-101 pruned (v1, ours)
resnet-50
resnet-101
resnet-34
ﬂops
3.03 × 109
3.69 × 109
2.76 × 109
3.64 × 109
4.47 × 109
4.08 × 109
7.8 × 109
error (%)
∼ 26.8
25.44
27.8
26.8
24.73
24.8
23.6
ratio
66%
39%
89%
53%
Table 3: Attributes of different versions of ResNet and their single crop errors on ILSVRC2012
benchmark. The last column means the parameter size of pruned model vs. the base model.
5.3
IMAGE FOREGROUND-BACKGROUND SEGMENTATION EXPERIMENT
As we discussed about the two major observations in Section 5.1, a more appealing scenario is to
apply our approach in pruning channels of over-parameterized model. It often happens when one
adopts a pre-trained network on a large task (such as ImageNet classiﬁcation) and ﬁne-tunes the
model to a different and smaller task (Molchanov et al., 2017). In this case, one might expect that
some channels that were useful in the ﬁrst pre-training task are not quite contributing to the outputs
of the second task.
We describe an image segmentation experiment whose neural network model is composed from an
inception-like network branch and a densenet network branch. The entire network takes a 224× 224
image and outputs binary mask at the same size. The inception branch is mainly used for locating
the foreground objects while the densenet network branch is used to reﬁne the boundaries around
the segmented objects. This model is originally trained on multiple datasets.
In our experiment, we attempt to prune channels in both the inception branch and densenet branch.
We set α = 0.01, ρ = 0.5, µt = 2× 10−5, and batch size = 24. We train the pre-trained base model
until all termination criterion are met, and build the pruned model for ﬁne-tuning. The pruned
model saves 86% parameters and 81% ﬂops of the base model. We also compare the ﬁne-tuned
pruned model with the pre-trained base model across different test benchmark. Mean IOU is used
as the evaluation metric3. It shows that pruned model actually improves over the base model on four
of the ﬁve test datasets with about 2% ∼ 5%, while performs worse than the base model on the most
challenged dataset DUT-Omron, whose foregrounds might contain multiple objects.
test dataset (#images)
MSRA10K (Liu et al., 2011) (2,500)
DUT-Omron (Yang et al., 2013) (1,292)
Adobe Flickr-portrait (Shen et al., 2016) (150)
Adobe Flickr-hp (Shen et al., 2016) (300)
COCO-person (Lin et al., 2014) (50)
param. size
ﬂops
mIOU
83.4%
83.2%
88.6%
84.5%
84.1%
mIOU
85.5%
79.1%
93.3%
89.5%
87.5%
base model
pruned model
1.02 × 107
5.68 × 109
1.41 × 106
1.08 × 109
Table 4: mIOU reported on different test datasets for the base model and the pruned model.
6 CONCLUSIONS
We proposed a model pruning technique that focuses on simplifying the computation graph of a
deep convolutional neural networks. Our approach adopts ISTA to update the γ parameter in batch
normalization operator embedded in each convolution. To accelerate the progress of model pruning,
we use a γ-W rescaling trick before and after stochastic training. Our method cleverly avoids some
possible numerical difﬁculties such as mentioned in other regularization based related work, hence
3https://www.tensorflow.org/api_docs/python/tf/metrics/mean_iou
Published as a conference paper at ICLR 2018
is easier to apply for practitioners. We empirically validate our method through several benchmarks
and show its usefulness and competitiveness in building compact CNN models.
REFERENCES
Jose M Alvarez and Mathieu Salzmann. Learning the number of neurons in deep networks. In Advances in
Neural Information Processing Systems, pp. 2270–2278, 2016.
Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung. Structured pruning of deep convolutional neural networks.
ACM Journal on Emerging Technologies in Computing Systems (JETC), 13(3):32, 2017.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.
SIAM Journal on Imaging Sciences, 2(1):183–202, 2009.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets.
In Proceedings of International Conference on Machine Learning, 2017.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efﬁcient neural
network. In Advances in Neural Information Processing Systems, pp. 1135–1143, 2015.
Babak Hassibi and David G Stork. Second order derivatives for network pruning: Optimal brain surgeon. In
Advances in Neural Information Processing Systems, pp. 164–171, 1993.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016.
Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks.
Proceedings of International Conference on Computer Vision, 2017.
In
Zehao Huang and Naiyan Wang. Data-driven sparse structure selection for deep neural networks. arXiv preprint
arXiv:1707.01213, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In International Conference on Machine Learning, pp. 448–456, 2015.
Vadim Lebedev and Victor Lempitsky. Fast convnets using group-wise brain damage. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 2554–2564, 2016.
Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in Neural Information
Processing Systems, pp. 598–605, 1990.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning ﬁlters for efﬁcient convnets.
In International Conference on Learning Representations, 2017.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and
C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer
vision, pp. 740–755. Springer, 2014.
Tie Liu, Zejian Yuan, Jian Sun, Jingdong Wang, Nanning Zheng, Xiaoou Tang, and Heung-Yeung Shum.
Learning to detect a salient object. IEEE Transactions on Pattern analysis and machine intelligence, 33(2):
353–367, 2011.
Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning efﬁcient
convolutional networks through network slimming. arXiv preprint arXiv:1708.06519, 2017.
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural net-
In International Conference on Learning Representations,
works for resource efﬁcient transfer learning.
2017.
Xiaoyong Shen, Aaron Hertzmann, Jiaya Jia, Sylvain Paris, Brian Price, Eli Shechtman, and Ian Sachs. Au-
tomatic portrait segmentation for image stylization. In Computer Graphics Forum, volume 35, pp. 93–102.
Wiley Online Library, 2016.
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural
networks. In Advances in Neural Information Processing Systems, pp. 2074–2082, 2016.
Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, and Ming-Hsuan Yang. Saliency detection via graph-
based manifold ranking. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 3166–3173, 2013.
10
Published as a conference paper at ICLR 2018
Xiangyu Zhang, Jianhua Zou, Kaiming He, and Jian Sun. Accelerating very deep convolutional networks
for classiﬁcation and detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(10):
1943–1955, 2016.
Hao Zhou, Jose M Alvarez, and Fatih Porikli. Less is more: Towards compact cnns. In European Conference
on Computer Vision, pp. 662–677. Springer, 2016.
Figure 1: Visualization of the number of pruned channels at each convolution in the inception
branch. Colored regions represents the number of channels kept. The height of each bar represents
the size of feature map, and the width of each bar represents the size of channels. It is observed that
most of channels in the bottom layers are kept while most of channels in the top layers are pruned.
11
