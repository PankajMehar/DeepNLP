Matrix completion with deterministic pattern
- a geometric perspective
Yao Xie†
Alexander Shapiro∗
Rui Zhang
School of Industrial and Systems Engineering
Georgia Institute of Technology
Atlanta, GA 30332-0205, USA
Abstract
We consider the matrix completion problem with a deterministic pattern of observed
entries and aim to ﬁnd conditions such that there will be (at least locally) unique
solution to the non-convex Minimum Rank Matrix Completion (MRMC) formulation.
We answer the question from a somewhat diﬀerent point of view and to give a geometric
perspective. We give a suﬃcient and “almost necessary” condition (which we call the
well-posedness condition) for the local uniqueness of MRMC solutions and illustrate
with some special cases where such condition can be veriﬁed. We also consider the
convex relaxation and nuclear norm minimization formulations. Then we argue that the
low-rank approximation approaches are more stable than MRMC and further propose
a sequential statistical testing procedure to determine the rank of the matrix from
observed entries. Finally, numerical examples veriﬁed the validity of our theory.
Keywords: Matrix completion, low rank approximation, transversality, semideﬁnite relax-
ations, hypotheses testing
∗Research of this author was partly supported by NSF grant 1633196 and DARPA EQUiPS program,
†Research of this author was partly supported by NSF grants CCF-1442635, CMMI1538746, and an NSF
grant SNL 014150709.
CAREER Award CCF-1650913.
1 Introduction
Matrix completion (e.g., [4,5,13]) is a fundamental problem in signal processing and machine
learning, which studies the recovery of a low-rank matrix from an observation of a subset of
its entries. It has attracted a lot attention from researchers and practitioners and there are
various motivating real-world applications including recommender systems and the Netﬂix
challenge (see a recent overview in [6]). A popular approach for matrix completion is to ﬁnd
a matrix of minimal rank satisfying the observation constraints. Due to the non-convexity
of the rank function, popular approaches are convex relaxation (see, e.g., [8]) and nuclear
norm minimization. There is a rich literature, both in establishing performance bounds,
developing eﬃcient algorithms and providing performance guarantees. Recently there has
also been new various results for non-convex formulations of matrix completion problem (see,
e.g., [9]).
Existing conditions ensuring recovery of the minimal rank matrix are usually formulated
in terms of missing-at-random entries and under an assumption of the so-called bounded-
coherence (see a survey for other approaches in [6]; we do not aim to give a complete overview
of the vast literature). These results are typically aimed at establishing the recovery with
a high probability. The work [7] studies a related problem: the uniqueness conditions for
minimum rank matrix recovery with random linear measurements of the true matrix (here
the linear measurements correspond to inner product of a measurement mask matrix with
the true matrix, and hence, the observations are diﬀerent from what one has in matrix
completion).
With a deterministic pattern of observed entries, a complete characterization of the
identiﬁable matrix for matrix completion remains an important yet open question: under
what conditions for the pattern, there will be (at least locally) unique solution? Recent
work [12] provides some insights into this problem by studying the so-called completable
problems and establishing conditions ensuring the existence of at most ﬁnitely many rank-r
matrices that agree with all its observed entries. A related work [1] studied this problem
when there is a sparse noise that corrupts the entries.
In this paper, we aim to answer the question from a somewhat diﬀerent point of view
and to give a geometric perspective. In particular, we consider the solution of the Minimum
Rank Matrix Completion (MRMC) formulation, which leads to a non-convex optimization
problem. We address the following questions: (i) Given observed entries arranged according
to a (deterministic) pattern, by solving the MRMC problem, what is the minimum achievable
rank? (ii) Under what conditions, there will be a unique matrix that is a solution to the
MRMC problem? We give a suﬃcient and “almost necessary” condition (which we call the
well-posedness condition) for the local uniqueness of MRMC solutions, and illustrate with
special cases where such condition can be veriﬁed. In addition, we also consider the convex
relaxation and nuclear norm minimization formulations.
We argue that given m observation of an n1 × n2 matrix, if the minimal rank r∗ is less
than R(n1, n2, m) := (n1 + n2)/2 − [(n1 + n2)2/4 − m]1/2, then the corresponding solution is
unstable in the sense that an arbitrary small perturbation of the observed values can make
this rank unattainable. On the other hand if r∗ > R(n1, n2, m), then almost surely the
solution is not (even locally) unique (cf., [18]). This indicates that except in rare occasions,
the MRMC problem cannot have both properties of possessing unique and stable solutions.
Consequently, what makes sense is to try to solve the minimum rank problem approximately
and hence to consider low-rank approximation approaches (such as an approach mentioned
in [6, 20]) as a better alternative to the MRMC formulation.
We also propose a sequential statistical testing procedure to determine the rank of the
matrix from observed entries. Such statistical approach can be useful for many existing low-
rank matrix completion algorithms that require a pre-speciﬁcation of the matrix rank, such
as the alternating minimization approach to solving the non-convex problem by representing
the low-rank matrix as a product of two low-rank matrix factors (see, e.g., [6]).
The paper is organized as follows. In the next section, we introduce the considered setting
and some basic deﬁnitions. In Section 3.1 we discuss rank reducibility from a generic point
of view. Section 3.2 is devoted to studying of the uniqueness of the MRMC solutions. Low-
rank approximations by the least squares method are discussed in Section 4. In Section 3.4
we consider semideﬁnite relaxations of the MRMC problem. Statistical testing of a low-rank
model is discussed in Section 4.2. In Section 5 we present numerical results related to the
developed theory. Finally Section 6 concludes the paper. All proofs are transferred to the
Appendix.
2 Matrix completion and problem setup
Consider the problem of recovering an n1×n2 data matrix of low rank when observing a small
number m of its entries, which are denoted as Mij, (i, j) ∈ Ω. Here Ω ⊂ {1, ..., n1}×{1, ..., n2}
is an index set of cardinality m. This goal can be written as the following optimization
problem referred to as the Minimum Rank Matrix Completion (MRMC),
(2.1)
Let us introduce some necessary deﬁnitions. Consider Ωc := {1, ..., n1} × {1, ..., n2} \ Ω,
rank(Y ) subject to Yij = Mij, (i, j) ∈ Ω.
the complement of the index set Ω, and deﬁne
min
Y ∈Rn1×n2
VΩ :=(cid:8)Y ∈ Rn1×n2 : Yij = 0, (i, j) ∈ Ωc(cid:9) .
VΩc :=(cid:8)Y ∈ Rn1×n2 : Yij = 0, (i, j) ∈ Ω(cid:9) ,
This linear space represents the set of matrices that are ﬁlled with zeros at the locations of
the unobserved entries. Similarly deﬁne
which represents the space of matrices that cannot be determined by the speciﬁed observa-
tions. We denote by M the n1 × n2 matrix with the speciﬁed entries Mij, (i, j) ∈ Ω, and all
other entries equal zero. By this construction, M + VΩc is the aﬃne space of all matrices
that satisfy the observation constraints. Note that M ∈ VΩ and the dimension of the linear
space VΩ is dim(VΩ) = m, while dim(VΩc) = n1n2 − m. By PΩ we denote the projection
onto the space VΩ, i.e., [PΩ(Y )]ij = Yij for (i, j) ∈ Ω and [PΩ(Y )]ij = 0 for (i, j) ∈ Ωc.
We use conventional notations. For a ∈ R we denote by (cid:100)a(cid:101) the least integer that is
greater than or equal to a, and by (cid:98)a(cid:99) the largest integer less than or equal to a. By A ⊗ B
we denote the Kronecker product of matrices (vectors) A and B, and by vec(A) column
vector obtained by stacking columns of matrix A. We use the following matrix identity for
matrices A, B, C of appropriate order
vec(ABC) = (C(cid:62) ⊗ A)vec(B).
(2.2)
By Sp we denote the linear space of p× p symmetric matrices and by writing X (cid:23) 0 we mean
that matrix X ∈ Sp is positive semideﬁnite. By σi(Y ) we denote the i-th largest singular
value of matrix Y ∈ Rn1×n2. By Ip we denote an identity matrix of dimension p.
• We say that a property holds for almost every (a.e) Mij, or almost surely, if the set of
matrices Y ∈ VΩ for which this property does not hold has Lebesgue measure zero in
the space VΩ.
3 Main theoretical results
To gain insights into the identiﬁability issue of matrix completion, we aim to answer the
following two questions: (i) what is achievable minimum rank (the optimal value of problem
(2.1)), and (ii) whether the minimum rank matrix, i.e., the optimal solutions to (2.1), is
unique given a problem set-up.
3.1 Rank reducibility
We denote by r∗ the optimal value of problem (2.1). That is, r∗ is the minimal rank of an
n1 × n2 matrix with prescribed elements Mij, (i, j) ∈ Ω. Clearly r∗ depends on the index
set Ω and values Mij. A natural question what values r∗ can attain. Note that (2.1) is a
non-convex problem and may have multiple solutions. In the literature, the rank-objective
function is often approximated by the nuclear norm function (which is convex), and various
theoretical properties are derived; we will discuss this approach in Section 3.4.
Theorem 3.1 (Upper bound for minimum achievable rank) The following upper bound
always holds
(3.1)
r∗ ≤(cid:6)√
m(cid:7) .
The main idea of the proof is to show that two aﬃne spaces, one representing all matrices
satisfying the observation constraints in (2.1), and another representing all matrices that have
rank less than r, have non-empty intersection.
In a certain generic sense it is possible to give a lower bound for the minimal rank r∗.
Let us consider intersection of a set of low-rank matrices and the aﬃne space of matrices
satisfying the observation constraints. That is, consider the set of n1 × n2 matrices of rank r
Mr :=(cid:8)Y ∈ Rn1×n2 : rank(Y ) = r(cid:9)
(3.2)
and the (aﬃne) mapping AM : VΩc → Rn1×n2 deﬁned as
AM (X) := M + X,
for X ∈ VΩc. As it was pointed before, the image AM (VΩc) = M + VΩc of mapping AM
deﬁnes the space of feasible points of the MRMC problem (2.1). It is well known that Mr
is a smooth, C∞, manifold with
(3.3)
It is said that the mapping AM intersects Mr transverally if for every X ∈ VΩc either
AM (X) (cid:54)∈ Mr, or AM (X) ∈ Mr and the following condition holds
dim(Mr) = r(n1 + n2 − r).
VΩc + TMr(Y ) = Rn1×n2,
(3.4)
where Y := AM (X) and TMr(Y ) denotes the tangent space to Mr at Y ∈ Mr.
By using a classical result of diﬀerential geometry, it is possible to show that for almost
every (a.e.) Mij, (i, j) ∈ Ω, the mapping AM intersects Mr transverally (this holds for every
manifold Mr) (see [18] for a discussion of this result). Transversality condition implies the
following dimensionality condition
dim(VΩc) + dim(TMr(Y )) ≥ dim(Rn1×n2).
In turn the above condition (3.5) can be written as
r(n1 + n2 − r) ≥ m,
or equivalently r ≥ R(n1, n2, m), where
R(n1, n2, m) := (n1 + n2)/2 −(cid:112)(n1 + n2)2/4 − m.
(3.5)
(3.6)
(3.7)
That is, if r < R(n1, n2, m), then the transversality condition (3.4) cannot hold and hence
for a.e. Mij it follows that rank(M + X) (cid:54)= r for all X ∈ VΩc.
Now if AM intersects Mr transverally at AM (X) ∈ Mr (i.e., condition (3.4) holds), then
the intersection AM (VΩc)∩Mr forms a smooth manifold near the point Y := AM (X). When
r > R(n1, n2, m), this manifold has dimension greater than zero and hence the corresponding
rank r solution is not (locally) unique. This leads to the following (for a formal discussion
of these results we can refer to [18]).
Theorem 3.2 (Lower bound and non-uniqueness of solutions) For any index set Ω
of cardinality m and almost every Mij, (i, j) ∈ Ω, the following holds: (i) for every feasible
point Y of problem (2.1) it follows that
(3.8)
(ii) if r∗ > R(n1, n2, m), then problem (2.1) has multiple (more than one) optimal solutions.
rank(Y ) ≥ R(n1, n2, m),
In particular when we have a square matrix n1 = n2 = n, it follows that
R(n, n, m) = n −
n2 − m.
(3.9)
For n1 = n2 = n and small m/n2 we can approximate
R(n, n, m) = n
(cid:16)
1 −(cid:112)1 − m/n2
(cid:17) ≈ m/(2n).
For example, for n1 = n2 = 1000 and m = 20000 we have
m = 141.4 and R(n, n, m) =
10.05, and hence the bounds (3.10) become 11 ≤ r∗ ≤ 142.
It follows from part (i) of Theorem 3.2 that r∗ ≥ R(n1, n2, m) for a.e. Mij. Therefore for
m ≤ min{n1, n2}, generically (almost surely) the minimal rank r∗ is between the bounds
R(n1, n2, m) ≤ r∗ ≤(cid:6)√
m(cid:7) ,
sider data matrix M of the following form M =(cid:0) M1 0
(3.10)
and (2.1) may have unique optimal solution only when r∗ = R(n1, n2, m). Of course such
equality could happen only if R(n1, n2, m) is an integer number. As Example 3.1 below shows,
for any integer r∗ satisfying (3.10) there exists an index set Ω such that the corresponding
MRMC problem attains the minimal rank r∗ for a.e. Mij. In particular this shows that the
lower and upper bounds in (3.10) are tight.
Example 3.1 (Tightness of lower and upper bounds for r∗) For r < min{n1, n2} con-
M2, M3, of the respective order r × r, (n1 − r) × r and (n1 − r) × (n2 − r), represent the
observed entry values. Cardinality m of the corresponding index set Ω is r(n1 + n2 − r), i.e.,
here r = R(n1, n2, m). Suppose that the r × r matrix M1 is nonsingular, i.e., its rows are
linearly independent. Then any row of matrix M2 can be represented as a (unique) linear
combination of rows of matrix M1. It follows that the corresponding MRMC problem has
(unique) solution of rank r∗ = r. In other words, the rank of the completed matrix will be
equal to r (the rank of the sub-matrix M1) and there will be a unique matrix that achieves
this rank. Now suppose that some of the entries of the matrices M2 and M3 are not ob-
served, and hence cardinality of the respective index set Ω is less than r(n1 +n2−r), and thus
r > R(n1, n2, m). In that case the respective minimal rank still is r, provided matrix M1 is
nonsingular, although the corresponding optimal solutions are not unique. In particular, if
(cid:1), i.e., only the entries of matrix M1 are observed, then m = r2 and the minimum
(cid:1) Here, the three sub-matrices M1,
M2 M3
M =(cid:0) M1 0
0 0
rank is r.
3.2 Uniqueness of solutions of the MRMC problem
For a given matrix M ∈ VΩ and the corresponding minimal rank r∗ ≤ R(n1, n2, m) the
question is whether the corresponding solution Y ∗ of rank r∗ is unique. Although, as it was
discussed in the previous section, the set of such matrices M is “thin” (in the sense that it
has Lebesgue measure zero), this question of uniqueness is important, in particular for the
statistical inference (discussed in Section 4.2) when entries Mij are observed with a noise.
Available results, based on the so-called Restricted Isometry Property (RIP) for low-rank
matrix recovery from linear observations and based on the coherence property for low-rank
matrix completion, assert that for certain probabilistic (Gaussian) models such uniqueness
holds with high probability. However for a given matrix M ∈ VΩ it is not clear how to verify
whether the solution is unique. Let us consider the following concept of local uniqueness of
solutions.
Deﬁnition 3.1 We say that an n1×n2 matrix ¯Y is a locally unique solution of problem (2.1)
if PΩ( ¯Y ) = M and there is a neighborhood V ⊂ Rn1×n2 of ¯Y such that rank(Y ) (cid:54)= rank( ¯Y )
for any Y ∈ V, Y (cid:54)= ¯Y .
Note that rank is a lower semicontinuous function of matrix, i.e., if {Yk} is a sequence
of matrices converging to matrix Y , then lim inf k→∞ rank(Yk) ≥ rank(Y ). Therefore local
uniqueness of ¯Y actually implies existence of the neighborhood V such that rank(Y ) >
rank( ¯Y ) for all Y ∈ V, Y (cid:54)= ¯Y , i.e., that at least locally problem (2.1) does not have optimal
solutions diﬀerent from ¯Y .
To explain the main result, we will introduce some constructions associated with the
manifold of low-dimensional matrices. There are several equivalent forms how the tangent
space to the manifold Mr at Y ∈ Mr can be represented. In one way it can be written as
TMr(Y ) =(cid:8)Q1Y + Y Q2 : Q1 ∈ Rn1×n1, Q2 ∈ Rn2×n2(cid:9) .
(3.11)
In an equivalent form this tangent space can be written as
TMr(Y ) =(cid:8)H ∈ Rn1×n2 : F HG = 0(cid:9) ,
(3.12)
where F is an (n1 − r)× n1 matrix of rank n1 − r such that F Y = 0 (referred to as a left side
complement of Y ) and G is an n2× (n2− r) matrix of rank n2− r such that Y G = 0 (referred
to as a right side complement of Y ). We also use the linear space of matrix orthogonal
(normal) to Mr at Y ∈ Mr, denoted by NMr(Y ). A matrix Z is orthogonal to Mr at
Y ∈ Mr if and only if tr(Z(cid:62)Y (cid:48)) = 0 for all Y (cid:48) ∈ TMr(Y ). By (3.11) this means that
Since tr(cid:2)Z(cid:62)(Q1Y + Y Q2)(cid:3) = tr(Y Z(cid:62)Q1)+tr(Z(cid:62)Y Q2) and matrices Q1 and Q2 are arbitrary,
tr(cid:2)Z(cid:62)(Q1Y + Y Q2)(cid:3) = 0, ∀Q1 ∈ Rn1×n1, ∀Q2 ∈ Rn2×n2.
NMr(Y ) =(cid:8)Z ∈ Rn1×n2 : Z(cid:62)Y = 0 and Y Z(cid:62) = 0(cid:9) .
it follows that the normal space can be written as
(3.13)
(3.14)
Deﬁnition 3.2 (Well-posedness condition) We say that a matrix ¯Y ∈ Mr is well-posed
for problem (2.1) if PΩ( ¯Y ) = M and the following condition holds
VΩc ∩ TMr( ¯Y ) = {0}.
(3.15)
Condition (3.15) (illustrated in Figure 1) is a natural condition having a simple geometri-
cal interpretation. Intuitively, it means that the null space of the observation operator does
not have any non-trivial matrix that lies in the tangent space of low-rank matrix manifold.
Hence, there cannot be any local deviation from the optimal solution that still satisﬁes the
measurement constraints. More formally, suppose that condition (3.15) does not hold, i.e.,
there exists nonzero matrix H ∈ VΩc ∩TMr( ¯Y ). This means that there is a curve Z(t) ∈ Mr
starting at ¯Y and tangential to H, i.e., Z(0) = ¯Y and (cid:107) ¯Y + tH − Z(t)(cid:107) = o(t). Of course
if moreover PΩ(Z(t)) = M for all t near 0 ∈ R, then solution ¯Y is not locally unique.
Although this is not guaranteed, i.e., the suﬃcient condition (3.15) may be not necessary
for local uniqueness of the solution ¯Y , violation of this condition implies that solution ¯Y is
unstable in the sense that for some matrices Y ∈ Mr close to ¯Y the distance (cid:107)PΩ(Y ) − M(cid:107)
is of order o((cid:107)Y − ¯Y (cid:107)). This motivates us to introduce the well-posedness condition that
guarantees a matrix to be locally unique solution.
Figure 1: Illustration of well-posedness condition.
Now we can give suﬃcient conditions for local uniqueness:
Theorem 3.3 (Suﬃcient conditions for local uniqueness) Matrix ¯Y ∈ Mr is a lo-
cally unique solution of problem (2.1) if ¯Y is well-posed for (2.1).
Below we present an equivalent form of the well-posedness condition. By Theorem 3.3
we have that if matrix ¯Y ∈ Mr is well-posed, then ¯Y is a locally unique solution of problem
(2.1). Note that condition (3.15) implies that dim(VΩc) + dim(TMr( ¯Y )) ≤ n1n2. That is,
condition (3.15) implies that r(n1 + n2 − r) ≤ m or equivalently r ≤ R(n1, n2, m). By
Theorem 3.2 we have that if r∗ > R(n1, n2, m), then the corresponding optimal solution
!ℳ#$%$%ℳ&$%+()*cannot be locally unique almost surely. Note that since the space VΩ is orthogonal to the
space VΩc, by duality arguments condition (3.15) is equivalent to the following condition
VΩ + NMr( ¯Y ) = Rn1×n2.
(3.16)
By using formula (3.12) it is also possible to write condition (3.15) in the following form
{X ∈ VΩc : F XG = 0} = {0},
(3.17)
where F is a left side complement of ¯Y and G is a right side complement of ¯Y . Recall that
vec(F XG) = (G(cid:62)⊗F )vec(X). Column vector of matrix G(cid:62)⊗F corresponding to component
j ⊗ fi, where fi is the i-th column of matrix F and gj is the j-th
xij of vector vec(X), is g(cid:62)
j ⊗ fi, (i, j) ∈ Ωc, are
row of matrix G. Condition (3.17) means that the column vectors g(cid:62)
linearly independent. We obtain the following result which can be useful for checking the
well-posedness condition.
Theorem 3.4 (Equivalent condition of well-posedness) Matrix ¯Y ∈ Mr is well-posed
for problem (2.1) if and only if for any left side complement F and right side complement G
of ¯Y , the column vectors g(cid:62)
j ⊗ fi, (i, j) ∈ Ωc, are linearly independent.
This in turn implies the following necessary condition for well-posedness of ¯Y ∈ Mr in
terms of the pattern of the index set Ω.
Theorem 3.5 (Necessary condition for well-posedness) If matrix ¯Y ∈ Mr is well-
posed for problem (2.1), then at each row and each column of ¯Y there are at least r elements
of the index set Ω.
However, although necessary, the condition for the index set Ω to have at each row
and each column at least r elements is not suﬃcient to ensure well-posedness as shown by
Theorem 3.6 below. Note that by deﬁnition the matrices F and G are of full rank.
Remark 3.1 (Special case with veriﬁable globally unique solutions) In some rather
special cases it is possible to verify global uniqueness of minimum rank solutions (actually
such conditions work well for the so-called Minimum Rank Factor Analysis discussed in Sec-
tion 3.4). Let ¯Y ∈ Mr be such that PΩ( ¯Y ) = M . Consider an index (k, l) ∈ Ωc. Suppose
that there exist I1 ⊂ {1, ..., n1} \ {k} and I2 ⊂ {1, ..., n2} \ {l} such that |I1| = |I1| = r and
I1 × I2 ⊂ Ω. Consider the r × r submatrix of M corresponding to rows i ∈ I1 and columns
j ∈ I2. Suppose that this submatrix is nonsingular. Then it follows that the minimum
rank r∗ ≥ r. Suppose further that {k} × I2 ∈ Ω and {l} × I1 ∈ Ω. Then for any matrix
Y ∈ Mr such that PΩ(Y ) = M we have that Ykl = Mkl. This follows by observing that the
(r + 1) × (r + 1) submatrix of Y corresponding to rows {k} ∪ I1 and columns {l} ∪ I2 has
rank r and hence zero determinant, and applying Shur complement for the element Ykl. If
this holds for every (k, l) ∈ Ωc, then the uniqueness of the solution ¯Y follows.
Remark 3.2 In Wilson and Worcester [21] was constructed an example of two 6 × 6 sym-
metric matrices of rank 3 with the same oﬀ-diagonal and diﬀerent diagonal elements. If we
deﬁne the index set as Ω := {(i, j) : i (cid:54)= j, i, j = 1, ..., 6}, then this can be viewed as an
R(6, 6, 30) = 6−√
example of two diﬀerent locally unique solutions of rank 3. Note that here m = 30 and
6. That is R(6, 6, 30) > 3 and generically (almost surely) rank cannot be
reduced below r = 4. We will discuss this example further in Section 5.
Note that uniqueness of the minimum rank solution is invariant with respect to per-
mutations of rows and columns of matrix M . This motivates to introduce the following
deﬁnition.
Deﬁnition 3.3 We say that the index set Ω is reducible if by permutations of rows and
columns, matrix M can be represented in a block diagonal form, i.e.,
(cid:21)
(cid:20) M1
0 M2
Otherwise we say that Ω is irreducible.
M =
(3.18)
Theorem 3.6 (Reducible index set) If the index set Ω is reducible, then any minimum
rank solution ¯Y , such that ¯Yij (cid:54)= 0 for all (i, j) ∈ Ωc, is not locally (and hence globally)
unique.
As it was shown in Proposition 3.3, if ¯Y is not locally unique, then it cannot be well-
posed. Therefore if the index set Ω is reducible, then any minimum rank solution, diﬀerent
from M itself, is not well-posed. Of course even if Ω is reducible, it still can happen that in
each row and column there are at least r elements of the index set Ω. That is, the condition
discussed in Theorem 3.5 may be not suﬃcient to ensure the well-posedness property.
3.3 Uniqueness of rank one solutions
In this section we discuss uniqueness of rank one solutions of the MRMC problem (2.1). We
show that in case of the minimum rank one, irreducibility of Ω is suﬃcient for the global
uniqueness. We assume that all Mij (cid:54)= 0, (i, j) ∈ Ω, and that every row and every column
of the matrix M has at least one element Mij. Let ¯Y be a solution of rank one of problem
(2.1), i.e., there are nonzero column vectors v and w such that ¯Y = vw(cid:62) with PΩ( ¯Y ) = M .
Recall that permutations of the components of vector v corresponds to permutations of
the rows of the respective rank one matrix, and permutations of the components of vector
w corresponds to permutations of the columns of the respective rank one matrix. It was
shown in Corollary 3.6 that if the index set Ω is reducible, then solution ¯Y cannot be locally
unique:
Theorem 3.7 (Global uniqueness for rank one solution) Suppose that Ω is irreducible,
Mij (cid:54)= 0 for all (i, j) ∈ Ω, and every row and every column of the matrix M have at least
one element Mij. Then any rank one solution is globally unique.
3.4 Semideﬁnite relaxations and nuclear norm minimization
In this section we consider alternative versions of MRMC problem that have been studied
in literature, including the convex relaxation formulation (see, e.g. [8]) and nuclear norm
minimization, and study solution uniqueness. We also make a connection to existing results
in Factor Analysis.
The MRMC problem (2.1) can be formulated in the following equivalent form
where Ξ ∈ Sp, p = n1 + n2, is symmetric matrix of the form Ξ =(cid:0) 0 M
rank(Ξ + X) subject to Ξ + X (cid:23) 0,
min
X∈WSc
(3.19)
(cid:1). Minimization in
(3.19) is performed over matrices X ∈ Sp which are complement to Ξ in the sense of having
zero entries at all places corresponding to the speciﬁed values Mij, (i, j) ∈ Ω. That is, let
S ⊂ {1, ..., p} × {1, ..., p} be the symmetric index set corresponding to the index set Ω, i.e.,
(i, j) ∈ S when 1 ≤ i ≤ j ≤ n1, if and only if (i, j + n2) ∈ Ω; and if (i, j) ∈ S, then
(j, i) ∈ S. By Sc ⊂ {1, ..., p} × {1, ..., p} we denote the symmetric index set complement of
S. Deﬁne
M(cid:62) 0
WS := {X ∈ Sp : Xij = 0, (i, j) ∈ Sc} and WSc := {X ∈ Sp : Xij = 0, (i, j) ∈ S}.
We consider a more general minimum rank problem of the form (3.19) in that we allow the
index set S to be a general symmetric subset of {1, ..., p} × {1, ..., p}, with a given matrix
Ξ ∈ WS. Note that WS ∩ WSc = {0} and WS + WSc = Sp.
As a heuristic it was suggested in [8] to approximate problem (3.19) by the following
problem
tr(X) subject to Ξ + X (cid:23) 0.
min
X∈WSc
(3.20)
Together with the minimum rank problem (3.19) we consider the following SDP problem
tr(CX) subject to Ξ + X (cid:23) 0,
min
X∈WSc
(3.21)
for some matrix C ∈ WSc. Of course problem (3.20) is a particular case of problem (3.21)
for C := Ip. We have the following result (cf., [17]).
Theorem 3.8 (Global uniqueness of trace minimization) Suppose that the matrix C ∈
WSc is positive deﬁnite. Then for a.e. Ξ ∈ WS problem (3.21) has unique optimal solution.
Recall that by saying that a property holds for a.e. Ξ ∈ WS we mean that it holds for all Ξ
in WS except for a subset of WS of Lebesgue measure zero.
The classical Minimum Rank Factor Analysis (MRFA) can be viewed as a particular
case of problem (3.19) with WSc being the space Dp of p × p diagonal matrices, and given
symmetric matrix Ξ of oﬀ diagonal elements. It is possible to show that generically (i.e., for
a.e. Ξ) the reduced rank of the MRFA problem is bounded (cf., [14]):
rank(Ξ + X) ≥ 2p + 1 − √
8p + 1
, ∀X ∈ Dp.
(3.22)
10
In Factor Analysis the respective minimum trace problem of the form (3.20) is called the
Minimum Trace Factor Analysis (MTFA). A relation between MRFA and MTFA problems
is discussed in [14, 15]. In Factor Analysis the setting of Remark 3.1 can be used to show
that in a certain generic sense, MRFA solution is unique if the respective minimal rank is
less than p/2 (we can refer to [3], and references therein, for a discussion of uniqueness of
MRFA solutions).
Consider the SDP problem (3.21), and assume that the matrix C ∈ WSc is positive
deﬁnite. The (Lagrangian) dual of problem (3.21) is the problem
tr(CX) − tr[Λ(Ξ + X)].
(3.23)
For Λ = C − Θ, with Θ ∈ WS, problem (3.23) can be written (note that tr(CΞ) = 0 for
Ξ ∈ WS) as
min
X∈WSc
max
Λ(cid:23)0
tr(ΘΞ) subject to C − Θ (cid:23) 0.
max
Θ∈WS
(3.24)
Note that for both problems (3.23) and (3.24) the Slater condition holds, and hence there is
no duality gap between these problems, and both problems have nonempty bounded sets of
optimal solutions.
Optimality conditions (necessary and suﬃcient) for problem (3.21) are
C = Pτ (Λ),
(Ξ + X)Λ = 0,
Λ (cid:23) 0, Ξ + X (cid:23) 0, X ∈ WSc.
(3.25)
(3.26)
(3.27)
Now suppose that ¯X ∈ WSc is such that Ξ + ¯X (cid:23) 0 and rank(Ξ + ¯X) = r < p. Let E be
a p × (p − r) matrix of rank p − r such that (Ξ + ¯X)E = 0. By the optimality conditions
(3.25)–(3.27) we have that ¯X is an optimal solution of the SDP problem (3.21) if and only
if the following condition holds: there exists Z ∈ Sp−r
+ such that Pτ (EZE(cid:62)) = C. Equations
Pτ (EZE(cid:62)) = C can be viewed as a system of dim(Wτ ) equations with (p − r)(p − r + 1)/2
unknowns (nonduplicated elements of matrix Z ∈ Sp−r). When r is “small” and consequently
(p − r)(p − r + 1)/2 > dim(Wτ ), it is likely that this system will have a solution Z (cid:23) 0, and
hence ¯X is an optimal solution of problem (3.21). This of course is a heuristic argument and
a more careful analysis is needed.
We can also view this by adjusting weight matrix C to the considered matrix Ξ + ¯X by
choosing Z (cid:23) 0 and deﬁning C := Pτ (EZE(cid:62)). For such C the corresponding SDP problem
has ¯X as an optimal solution. Note that although matrix EZE(cid:62) is positive semideﬁnite
when Z (cid:23) 0, there is no guarantee that the corresponding matrix Pτ (EZE(cid:62)) is positive
semideﬁnite.
Next we consider the nuclear norm minimization problem. Consider the following (con-
vex) problem
where (cid:107) · (cid:107)∗ is the nuclear norm (cid:107)Y (cid:107)∗ = (cid:80)min{n1,n2}
(cid:107)X + M(cid:107)∗,
min
X∈VΩc
σi(Y ). For the nuclear norm its dual
norm is given by σ1(·) and the respective unit ball of the dual norm can be written as
i=1
(3.28)
11
B =(cid:8)Q ∈ Rn1×n2 : λ1(Q(cid:62)Q) ≤ 1(cid:9) , where λ1(Q(cid:62)Q) is the largest eigenvalue of matrix Q(cid:62)Q.
It follows that (cid:107)Y (cid:107)∗ is equal to the optimal value of the following SDP problem
tr(Q(cid:62)Y ) s.t.
max
Q∈Rn1×n2
The Lagrangian dual of this problem is
Q(cid:62) In2
(cid:20) In1 Q
(cid:20) Λ1
− 1
2Y (cid:62)
(cid:21)
(cid:23) 0.
(cid:21)
− 1
2Y
Λ2
(cid:23) 0.
min
Λ1∈Sn1 , Λ2∈Sn2
tr(Λ1 + Λ2) s.t.
index set S being the symmetric index set corresponding to the index set Ω and Ξ :=(cid:0) 0 M
It follows that here problem (3.28) can be written in the form (3.20) with p = n1 + n2, the
M(cid:62) 0
(the coeﬃcient −1/2 can be absorbed into X). Then by Theorem 3.8 we have that almost
surely (i.e., for a.e. Mij) the nuclear norm minimization problem has unique optimal solution.
(cid:1)
4 Low-rank matrix approximation
In this section, we discuss a formulation of the MRMC based on ﬁnding a low-rank ap-
proximation with noisy and incomplete observations of matrix entries. We refer to it as the
“Low-Rank Matrix Approximation” (LRMA) problem. Compared with the formulation of
exact low rank recovery, the LRMA is more realistic in the presence of noise. We discuss
properties of the LRMA problem and then introduce a statistical test procedure for choosing
the true rank of the matrix.
4.1 LRMA and its properties
By Theorem 3.2 we have that if the minimal rank r∗ is less than R(n1, n2, m), then the
corresponding solution is unstable in the sense that an arbitrary small perturbation of the
observed values Mij can make this rank unattainable. On the other hand if r∗ > R(n1, n2, m),
then almost surely the solution is not (even locally) unique. This indicates that except in
rare occasions, problem (2.1) of exact rank minimization cannot have both properties of
possessing unique and stable solutions. Consequently, what makes sense is to try to solve
the minimum rank problem approximately. That is consider the problem
min
Y ∈Rn1×n2 , X∈VΩc
F (X + M, Y ) subject to rank(Y ) = r,
(4.1)
where M ∈ VΩ is a given data matrix, and F (A, B) is a discrepancy between matrices
A, B ∈ Rn1×n2. For example if F (A, B) := (cid:107)A − B(cid:107)2
i,j Y 2
ij,
being the Frobenius norm, then (4.1) becomes the least squares problem
F with (cid:107)Y (cid:107)2
F = tr(Y (cid:62)Y ) = (cid:80)
(cid:88)
min
Y ∈Mr
(i,j)∈Ω
12
(Mij − Yij)2 .
(4.2)
The least squares approach although is natural, is not the only one possible. For example, in
the statistical approach to Factor Analysis the discrepancy function is based on the Maximum
Likelihood method and is more involved.
We discuss below the least squares approach. There are several questions related to
formulation of the approximation problem (4.2). The ﬁrst question is how to solve problem
(4.2) numerically. In general problem (4.2) is not convex and could be diﬃcult to solve.
Proposition 4.1 (Necessary condition for LRMA) The following are necessary condi-
tions for Y ∈ Mr to be an optimal solution of problem (4.2)
(PΩ(Y ) − M )(cid:62)Y = 0 and Y (PΩ(Y ) − M )(cid:62) = 0.
(4.3)
Remark 4.1 We can view the least squares problem (4.2) from the following point of view.
Consider function
φ(Y, Θ) := 1
2tr[(PΩ(Y ) − Θ)(cid:62)(PΩ(Y ) − Θ)],
(4.4)
(4.5)
with Θ ∈ VΩ viewed as a parameter. Deﬁne
(Yij − Mij)2 = 1
f (Y ) := 1
(cid:88)
(i,j)∈Ω
2tr[(PΩ(Y ) − M )(cid:62)(PΩ(Y ) − M )],
Hence, the problem (4.2) consists of minimization of f (Y ) subject to Y ∈ Mr. Note that
for Θ = M we have f (·) = φ(·, M ), where f (·) is deﬁned in (4.5). Let ¯Y ∈ Mr be such
that φ( ¯Y , Θ0) = 0 for some Θ0 ∈ VΩ, i.e., PΩ( ¯Y ) = Θ0. A suﬃcient condition for ¯Y to be a
locally unique solution of problem (2.1), at M = Θ0, is
tr(cid:2)PΩ(H)(cid:62)PΩ(H)(cid:3) > 0, ∀H ∈ TMr( ¯Y ) \ {0}.
(4.6)
The above condition means that if H ∈ TMr( ¯Y ) and H (cid:54)= 0, then PΩ(H) (cid:54)= 0. In other
words this means that the kernel
Ker(PΩ) := {H ∈ TMr( ¯Y ) : PΩ(H) = 0}
is {0}. Since PΩ(H) = 0 for any H ∈ VΩc, it follows that: condition (4.6) is equivalent to
the suﬃcient condition (3.15) of Proposition 3.3. That is, condition (4.6) means that matrix
¯Y is well-posed for problem (2.1).
Assuming that condition (4.6) (or equivalently condition (3.15)) holds, by applying the
Implicit Function Theorem to the ﬁrst order optimality conditions of the least squares prob-
lem (4.2) we have the following result.
Proposition 4.2 Let ¯Y ∈ Mr be such that PΩ( ¯Y ) = Θ0 for some Θ0 ∈ VΩ and suppose
that the well posedness condition (3.15) holds. Then there exist neighborhoods V and W of
¯Y and Θ0, respectively, such that for any M ∈ W ∩ VΩ there exists unique Y ∈ V ∩ Mr
satisfying the optimality conditions (4.3).
13
(4.3). Then if PΩ( ¯Y ) is suﬃciently close to M (i.e., the ﬁt(cid:80)
The above proposition implies the following. Suppose that we run a numerical procedure
which identiﬁes a matrix ¯Y ∈ Mr satisfying the (necessary) ﬁrst order optimality conditions
(i,j)∈Ω (Yij − Mij)2 is suﬃciently
small) and condition (3.15) holds at ¯Y , then we can say that f (Y ) > f ( ¯Y ) for all Y (cid:54)=
¯Y in a neighborhood of ¯Y . That is, ¯Y solves the least squares problem at least locally.
Unfortunately it is not clear how to quantify the “suﬃciently close” condition, and this does
not guarantee global optimality of ¯Y unless ¯Y is the unique minimum rank solution.
4.2 Statistical test for rank selection
The second important question for LRMA is what rank r to use for a considered data
matrix M ∈ VΩ. By the above discussion it will be natural to take some value of r less than
R(n1, n2, m), since otherwise we will not even have locally unique solution. Can the ﬁt of
Y ∈ Mr to X + M , and hence the choice of r, be tested in some statistical sense? In this
section we discuss statistical testing of value of the “true” minimal rank when the entries of
the data matrix M are observed with noise.
In order to proceed we assume the following model with noisy and possibly biased obser-
vations of a subset of matrix entries. There is a (population) value Y ∗ of n1 × n2 matrix of
ij, (i, j) ∈ Ω,
rank r < R(n1, n2, m) and Mij are viewed as observed (estimated) values of Y ∗
based on a sample of size N . The observed values are modeled as
ij + N−1/2∆ij + εij, (i, j) ∈ Ω,
(4.7)
where Y ∗ ∈ Mr and ∆ij are some (deterministic) numbers. The random errors εij are
assumed to be independent of each other and such that N 1/2εij converge in distribution to
ij, (i, j) ∈ Ω. The additional terms N−1/2∆ij in (4.7)
normal with mean zero and variance σ2
represent a possible deviation of population values from the “true” model and are often
referred to as the population drift or a sequence of local alternatives (we can refer to [11]
for a historical overview of invention of the local alternatives setting). This is a reasonably
realistic model motivated by many real applications.
Deﬁnition 4.1 We say that the model is globally identiﬁed (at Y ∗) if ¯Y ∈ Rn1×n2 of
rank( ¯Y ) ≤ r and PΩ( ¯Y ) = PΩ(Y ∗) imply that ¯Y = Y ∗, i.e., Y ∗ is the unique solution
of the respective matrix completion problem. Similarly it is said that the model is locally
identiﬁed if this holds for all such ¯Y in a neighborhood of Y ∗, i.e., Y ∗ is a locally unique
solution.
Mij = Y ∗
Consider the following weighted least squares problem
wij (Mij − Yij)2 ,
(cid:88)
min
Y ∈Mr
(i,j)∈Ω
for some weights wij > 0, (i, j) ∈ Ω. (Of course, if wij = 1, (i, j) ∈ Ω, then problem (4.8)
coincides with the least squares problem (4.2).) We have the following standard result about
consistency of the least squares estimates.
14
(4.8)
Proposition 4.3 Suppose that the model is globally identiﬁed at Y ∗ ∈ Mr and values Mij,
(i, j) ∈ Ω, converge in probability to the respective values Y ∗
ij as the sample size N tends
to inﬁnity. Then an optimal solution ˆY of problem (4.8) converges in probability to Y ∗ as
N → ∞.
Consider the following weighted least squares test statistic
TN (r) := N min
Y ∈Mr
wij (Mij − Yij)2 ,
(4.9)
(cid:88)
(i,j)∈Ω
(cid:88)
(i,j)∈Ω
(cid:88)
(i,j)∈Ω
(cid:88)
(i,j)∈Ω
(cid:0)Y ∗
ij with ˆσ2
where wij := 1/ˆσ2
ij converge in probability
ij as N → ∞). Recall that the respective condition of form (3.15), or equivalently (4.6),
to σ2
is suﬃcient for local identiﬁablity of Y ∗. The following asymptotic results can be compared
with similar results in the analysis of covariance structures (cf., [19]).
ij being consistent estimates of σ2
ij (i.e., ˆσ2
Proposition 4.4 (Asymptotic properties of test statistic) Consider the noisy obser-
vation model (4.7). Suppose that the model is globally identiﬁed at Y ∗ ∈ Mr and Y ∗ is
well-posed for problem (2.1). Then the test statistic TN (r) converges in distribution to non-
central chi square with degrees of freedom df r = m − r(n1 + n2 − r) and the noncentrality
parameter
δr = min
H∈TMr (Y ∗)
ij (∆ij − Hij)2 .
σ−2
(4.10)
Note that the optimal (minimal) value of the weighted least squares problem (4.8) can
be approximated by
min
H∈TMr (Y ∗)
wij (Eij − Hij)2 + RN ,
(4.11)
with Eij := N−1/2∆ij + εij and the error term RN = o ((cid:107)M − PΩ(Y ∗)(cid:107)2) being of stochastic
order RN = op(N−1). Hence, the noncentrality parameter, given in (4.10), can be approxi-
mated as
δr ≈ N min
Y ∈Mr
wij
ij + N−1/2∆ij − Yij
(4.12)
(cid:1)2
That is, the noncentrality parameter is approximately equal to N times the ﬁt to the “true”
model of the alternative population values Y ∗
ij + N−1/2∆ij under small perturbations of order
O(N−1/2).
Remark 4.2 The above asymptotic results are formulated in terms of the “sample size N ”
suggesting that the observed values are estimated from some data. This allows to formulate
mathematically precise convergence results. One can take a more pragmatic point of view
that when there is a “small” random noise in the observed values, the respective test statistics
properly normalized with respect to magnitude of that noise have approximately a noncentral
chi square distribution.
15
The asymptotics of the test statistic TN (r) depends on r and also on the cardinality m of
the index set Ω. Suppose now that more observations become available at additional entries
of the matrix. That is we are testing now the model with a larger index set Ω(cid:48), of cardinality
m(cid:48), such that Ω ⊂ Ω(cid:48).
In order to emphasize that the test statistic also depends on the
corresponding index set we add the index set in the respective notations. Note that if Y ∗ is
a solution of rank r for both sets Ω and Ω(cid:48) and the model is globally (locally) identiﬁed at
Y ∗ for the set Ω, then the model is globally (locally) identiﬁed at Y ∗ for the set Ω(cid:48). Note
also that if the regularity condition (3.15) holds at Y ∗ for the smaller model (i.e.
for Ω),
then it holds at Y ∗ for the larger model (i.e. for Ω(cid:48)). The following result can be proved in
the same way as Theorem 4.4 (cf., [19]).
Proposition 4.5 Consider index sets Ω ⊂ Ω(cid:48) of cardinality m = |Ω| and m(cid:48) = |Ω(cid:48)|, and the
noisy observation model (4.7). Suppose that the model is globally identiﬁed at Y ∗ ∈ Mr and
condition (3.15) holds at Y ∗ for the smaller model (and hence for both models). Then the
statistic TN (r, Ω(cid:48)) − TN (r, Ω) converges in distribution to noncentral χ2 with df r,Ω(cid:48) − df r,Ω =
m(cid:48)−m degrees of freedom and the noncentrality parameter δr,Ω(cid:48)−δr,Ω, and TN (r, Ω(cid:48))−TN (r, Ω)
is asymptotically independent of TN (r, Ω).
It is also possible to give asymptotic distribution of solutions of problem (4.8). Suppose
now that the assumptions of Theorem 4.4 hold with all ∆ij in equation (4.7) being zeros.
Let ˆYN be a solution of problem (4.8), i.e.,
ˆYN ∈ arg min
Y ∈Mr
(cid:88)
wij
(i,j)∈Ω
(cid:16)
Y ∗
ij + εij
(cid:124) (cid:123)(cid:122) (cid:125)
Mij
−Yij
(cid:17)2
Then N 1/2( ˆYN − Y ∗) converges in distribution to a random matrix A(Z), where Z ∈ VΩ is
a random matrix with entries Zij ∼ N (0, σ2
ij), (i, j) ∈ Ω, having normal distribution and
independent of each over, and A(Z) is the optimal solution of the problem
ij (Zij − Hij)2 .
σ−2
(4.13)
(cid:88)
min
H∈TMr (Y ∗)
(i,j)∈Ω
Note that A(·) is a linear operator and hence A(Z) has a multivariate normal distribution
with zero means. Note also that (compare with (7.5))
rank(A(Z)) = dim(cid:0)PΩ(TMr(Y ∗)(cid:1) = r(n1 + n2 − r).
(4.14)
5 Numerical experiments
In this section we give a summary of numerical experiments illustrating the presented theory.
For a more extensive discussion of these results we refer to the supplementary material at
https://github.com/ruizhang-ray/Matrix-Completion.
16
5.1 An example of 6×6 matrix considered in [21]
As it was pointed in Remark 3.2, in [21] was given an example of two diﬀerent locally
unique solutions of rank r∗ = 3 for a 6 × 6 matrix with the index set Ω corresponding to its
oﬀ-diagonal elements. The matrix M in that example is

M =
0.56 0.16 0.48 0.24 0.64
0.20 0.66 0.51 0.86
0.18 0.07 0.23
0.72
0.41
0.56
0.16 0.20
0.48 0.66 0.18
0.24 0.51 0.07 0.30
0.64 0.86 0.23 0.72 0.41
0.3

The two rank 3 solutions are given by diagonal matrices:
D1 = Diag(0.64, 0.85, 0.06, 0.56, 0.50, 0.93),
D2 = Diag(0.425616, 0.902308, 0.063469, 0.546923, 0.386667, 0.998).
Two numerical procedures - the soft-thresholded SVD [10] and a nuclear norm minimiza-
tion [2], were applied to this example. Both procedures didn’t recover any of these two
solutions; the soft-thresholded SVD converged to another solution matrix and the nuclear
norm minimization produced the following diagonal matrix solution
D3 = Diag(0.4369, 0.7625, 0.0520, 0.5302, 0.1926, 0.9555),
with the respective rank r = 4 and smaller nuclear norm than in the above rank 3 solutions
and the solution produced by the soft-thresholded SVD procedure. Note that here both
optimal solutions are well posed, and yet these numerical procedures could not recover any
one of them. It is not clear how typical this example, of diﬀerent locally optimal solutions,
is. Recall that generically the nuclear norm minimization problem possesses unique optimal
solution. However it is not clear how well it approximates the ‘true’ minimal rank solution
when it is observed with a noise.
5.2 Comparison of LRMA and nuclear norm minimization
In ﬁgure 2, ﬁgure 3 and ﬁgure 4, there is a comparison between the performances of LRMA
(using least square) and nuclear norm minimization when true matrices are observed with
normal noise. We generate Y ∗, an n1 × n2 matrix of rank r∗, by uniformly generated an
n1 × r∗ matrix U , an n2 × r∗ matrix V and an r∗ × r∗ diagonal matrix D and setting
Y ∗ = ˜U D ˜V T , where ˜U and ˜V are orthonormalization of U , V respectively. We uniformly
sample Ω, where |Ω| = m. Observation matrix M is generated by Mij = Y ∗
ij + εij, (i, j) ∈ Ω,
where εij ∼ N (0, σ2N−1). Least square approximation is solved by a soft-threshholded SVD
solver with setting no nuclear norm regularization. The algorithm stops when either relative
change in the Frobenius norm between two successive estimates, (cid:107)Y (t+1) − Y t(cid:107)F /(cid:107)Y (t)(cid:107)F , is
17
ij and Y nm
less than some tolerance, denoted as tol or the iteration is larger than some number, denoted
as it. Nuclear norm minimization is solved via TFOCS [2]. The solutions of least square
approximation and nuclear norm minimization are denoted as Y ls and Y nm respectively.
In the following, we will compare this two methods by looking into the absolute value of
ij − Y ∗
Y ls
In Figure 2, we compare solutions in the situation that well-posedness condition is sat-
isﬁed. In this experiment, n1 = 40, n2 = 50, r∗ = 10, m = 1000, σ = 5, N = 50 and Ω is
sampled until well-posedness condition is satisﬁed. Convergence parameter tol = 10−20 and
it = 50000. Least square approximation recover the true matrix Y ∗ with less error than the
nuclear norm minimization.
ij − Y ∗
ij.
Figure 2: Absolute errors of two methods when well-posedness condition is sat-
isﬁed. The picture on the left is the structure of Ω with Ω denoted by yellow and Ωc denoted
by blue. The picture in the middle is the absolute error between true matrix and the completed
ij|. The picture on the right is the absolute error
matrix by least square approximation, |Y ls
between true matrix and the completed matrix by nuclear norm minimization, |Y nm
ij|. True
matrix Y ∗ ∈ R40×50, rank(Y ∗) = 10, |Ω| = 1000, ∆ij = 0, εij ∼ N (0, 52
50 ) and observation matrix
ij + εij, (i, j) ∈ Ω. For the error, blue denotes small error and yellow denotes large error.
Mij = Y ∗
ij − Y ∗
ij − Y ∗
In Figure 3, we compare solutions in the situation that the necessary condition for well-
posedness is violated. In this experiment, n1 = 70, n2 = 40, r∗ = 11, m = 1300, σ = 5, N =
50 and Ω is sampled until the necessary condition for well-posedness is violated. Convergence
parameter tol = 10−16 and it = 50000. Least square approximation fails to recover the true
matrix at the rows (the row 3, 6, 30, 46, 50) violating the necessary condition for well-
posedness. Nuclear norm minimization can only recover with larger error than least square
approximation.
In Figure 4, we compare solutions in the situation that Ω is reducible. In this experiment,
n1 = 40, n2 = 50, r∗ = 10, m = 1000, σ = 5, N = 50 and Ω = {(i, j) ∈ {1··· 20} ×
{1··· 20} ∪ {21··· 40} × {21··· 50}}. Convergence parameter tol = 10−20 and it = 50000.
In this situation, both methods can’t recover the true matrix Y ∗ due to unidentiﬁability.
18
Figure 3: Absolute errors of two methods when necessary condition of well-
posedness is violated. The picture on the left is the structure of Ω. The picture in the
middle is the absolute error between true matrix and the completed matrix by least square ap-
proximation, |Y ls
ij|. The picture on the right is the absolute error between true matrix and
ij|. True matrix Y ∗ ∈ R70×40,
the completed matrix by nuclear norm minimization, |Y nm
ij + εij, (i, j) ∈ Ω. The
rank(Y ∗) = 11, |Ω| = 1300, ε ∼ N (0, 52
necessary condition for well-posedness is violated, i.e. the numbers of observations are less than
11, at row 3, 6, 30, 46, 50. Color scheme is the same as in Figure 2.
50 ) and observation matrix Mij = Y ∗
ij − Y ∗
ij − Y ∗
Figure 4: Absolute errors of two methods when Ω is reducible. The picture on the
left is the structure of Ω. The picture in the middle is the absolute error between true matrix
and the completed matrix by least square approximation, |Y ls
ij|. The picture on the right is
the absolute error between true matrix and the completed matrix by nuclear norm minimization,
|Y nm
ij − Y ∗
50 ) and observation
ij + εij, (i, j) ∈ Ω. Ω is reducible. Only two diagonal block matrices M1 ∈ R20×20
matrix Mij = Y ∗
and M2 ∈ R20×30 are observed. Note that the necessary condition of well-posedness is satisﬁed in
this situation. Color scheme is the same as in ﬁgure 2.
ij|. True matrix Y ∗ ∈ R40×50, rank(Y ∗) = 10, |Ω| = 1000, εij ∼ N (0, 52
ij − Y ∗
19
5.3 Numerical veriﬁcation of asymptotic theorems
N (r)}200
N (r, Ω(cid:48)) − T (k)
N (r, Ω)}200
ij , (i, j) ∈ Ω, where ε(k)
Figure 6 is the Q-Q plot of {T (k)
An idealized model, when a “true” solution of low rank is observed with a noise, was in-
troduced in Section 4.2 (see equation (4.7)) and asymptotically chi-square test statistics for
testing the “true” minimal rank were suggested. Below are discussed numerical experiments
validating these asymptotics.
We generate Y ∗, an n1 × n2 matrix of rank r∗, by uniformly generated an n1 × r∗ matrix
U , an n2 × r∗ matrix V and an r∗ × r∗ diagonal matrix D and setting Y ∗ = ˜U D ˜V T , where
˜U and ˜V are orthonormalization of U , V respectively. We uniformly sample Ω, where
|Ω| = m. To verify the asymptotics, observation matrix M is generated repeatedly by
ij ∼ N (0, σ2N−1). In each instance the corresponding
ij = Y ∗
ij + ε(k)
M (k)
statistic T (k)
N (r) is computed using equation (4.9). Least square approximation is solved by a
soft-threshholded SVD solver with setting no nuclear norm regularization and the algorithm
stops when either relative change in the Frobenius norm between two successive estimates,
(cid:107)Y (t+1) − Y t(cid:107)F /(cid:107)Y (t)(cid:107)F , is less than some tolerance, denoted as tol or the iteration is larger
than some number, denoted as it.
Figure 5 is the Q-Q plot of {T (k)
k=1 against the corresponding chi-square distribution.
In this experiment, n1 = 40, n2 = 50, r∗ = 11, m = 1000, σ = 5, N = 400 and Ω is
sampled until well-posedness condition is satisﬁed. Convergence parameter tol = 10−20 and
it = 50000. From the result, we can see TN (r) follows the central chi-square distribution
with degree of freedom df r = m − r(n1 + n2 − r) = 131 as proved in Theorem 4.4.
k=1 against the corresponding chi-
square distribution. In this experiment, n1 = 40, n2 = 50, r∗ = 11, m = 996, σ = 5, N = 400,
m(cid:48) = |Ω(cid:48)| = 1001 and Ω is sampled until well-posedness condition is satisﬁed (Notes that Ω(cid:48)
also satisﬁed well-posedness condition since Ω(cid:48)C ⊂ ΩC). Convergence parameter tol = 10−20
and it = 50000. From the result, we can see TN (r, Ω(cid:48))− TN (r, Ω) follows a central Chi-square
distribution with degree of freedom df r,Ω(cid:48) − df r,Ω = m(cid:48) − m = 5 as proved in Theorem 4.5.
Figure 7 and 8 shows a possible procedure that determines the true rank r∗ by sequential
chi-square tests. In this experiment, n1 = 40, n2 = 50, r∗ = 9, m = 1000, σ = 5, N = 400,
and Ω is sampled until well-posedness condition is satisﬁed. Convergence parameter tol =
10−12 and it = 50000. Only one observation matrix M is generated in this experiment. Then
the least square approximation are sequentially solved by speciﬁed r from 1 to (cid:100)R(n1, n2, m)(cid:101)
and for each TN (r), p-value of the corresponding central chi-square distribution (with degree
of freedom df r = m − r(n1 + n2 − r)) can be computed to determine whether r should be
accepted. It shows that all the r < r∗ is rejected and the ﬁrst r accepted is the true r∗, which
happens with high probability in the experiments as showed in the supplementary material.
The performed numerical results justify the asymptotic theorems and show that although
the least square approximation can not be solved as computationally eﬃciently as the nu-
clear norm minimization, current algorithms can still give a satisfactory statistical inference
under the assumption of model (4.7). Numerical experiments, shown in the supplementary
material, also validate the noncentral chi-square asymptotics of the considered statistics un-
der the assumption of the population drift with the additional terms representing a deviation
20
Figure 5: Q-Q plot of TN (r) against
quantiles of chi-square distribution.
Y ∗ ∈ R40×50, rank(Y ∗) = 11,
|Ω| =
1000. Observation matrix M is generated
ij , (i, j) ∈ Ω, where
200 times, M (k)
ij ∼ N (0, 52
ε(k)
N (r)
is computed as equation 4.9. By Theorem
4.4, {T (k)
N (r)} follows central chi-square with
df r = m − r(n1 + n2 − r) = 131.
ij = Y ∗
400 ). For each M (k), T (k)
ij +ε(k)
Figure 6: Q-Q plot of TN (r, Ω(cid:48)) −
TN (r, Ω) against quantiles of chi-
Y ∗ ∈ R40×50,
square distribution.
|Ω| = 996,
rank(Y ∗) = 11,
where Ω ⊂ Ω(cid:48). Observation matrix M(cid:48)
(cid:48)(k)
ij =
and M are generated 200 times, M
ij, (i, j) ∈
Y ∗
ij + ε(k)
Ω, where ε(k)
50 ). For each time,
N (r, Ω(cid:48)) and T (k)
T (k)
N (r, Ω) are computed. By
N (r, Ω)} fol-
N (r, Ω(cid:48)) − T (k)
Theorem 4.5, {T (k)
lows central chi-square with df r,Ω(cid:48) − df r,Ω =
m(cid:48) − m = 5 .
ij , (i, j) ∈ Ω(cid:48) and Mij = M(cid:48)
|Ω(cid:48)| = 1001,
ij ∼ N (0, 52
of population values from the “true” model.
6 Conclusion
In this paper, we have examined the non-convex matrix completion from a geometric view-
point and established a suﬃcient and “almost necessary” condition for local uniqueness of
solutions. Our characterization assumes deterministic patterns, and the results are general.
From this study, we found that the minimum rank matrix completion (MRMC) tends to
lead to unstable or non-unique solutions and thus the alternative low-rank matrix approxi-
mation (LRMA) is a better and more stable approach. We proposed a new statistical test
for rank selection, based on observed entries, which can be useful for many practical matrix
completion algorithms.
21
Figure 8: p-value for the chi-square
rank test. The experiment is the same as
the one in Figure 7. The red dash line is
p-value = 0.05. This plot is equivalent to
Figure 7.
Figure 7: Sequential test for rank se-
Y ∗ ∈ R40×50, rank(Y ∗) = 9,
lection.
|Ω| = 1000, εij ∼ N (0, 52
400 ) and observa-
ij + εij, (i, j) ∈ Ω. Least
tion matrix Mij = Y ∗
square approximation are solved sequentially
by speciﬁed r from 1 to (cid:100)R(n1, n2, m)(cid:101) and
corresponding TN (r) is computed sequen-
tially. The grey region is the conﬁdence in-
terval with α = 0.05. The red point indicates
the corresponding TN (r) is not in the conﬁ-
dence interval while the blue point indicates
the corresponding TN (r) is in the conﬁdence
interval. The data is plotted in log scale.
7 Appendix
m is an integer.
Proof of Theorem 3.1 It suﬃces to verify (3.1) for the cases when
m. Let V ∈ Rn1×r and W ∈ Rn2×r be matrices of rank r, with respective
Consider r :=
1 × r row vectors v1, ..., vn1 and w1, ..., wn2. Consider the following system of equations
j = 0, (i, j) ∈ Ω, with unknowns given by elements of matrix Z ∈ Rr×r. This is a
viZw(cid:62)
linear homogeneous system with m = r2 equations and r2 unknowns, which can be written as
(wj ⊗ vi)vec(Z) = 0, (i, j) ∈ Ω. Let matrices V and W be chosen in such a way that the row
vectors wj ⊗ vi, (i, j) ∈ Ω, are linearly independent. Then this system of linear equations has
22
only one solution Z = 0. Consider the linear space of matrices L := {V ZW (cid:62) : Z ∈ Rr×r}.
Since vectors wj ⊗ vi, (i, j) ∈ Ω, are linearly independent we have that dim(L) = r2, and by
the above construction L ∩ VΩc = {0}. On the other hand dim(VΩc) + dim(L) = n1n2 and
hence L + VΩc = Rn1×n2. It follows that for any matrix M ∈ VΩ the linear (aﬃne) spaces
M + VΩc and L have a nonempty intersection. Note that M + VΩc represents the set of all
matrices that satisfy the observation constraints. Also note that for every matrix Y ∈ L its
rank(Y ) ≤ r. It follows that if r2 = m, then it is possible to ﬁnd an n1 × n2 matrix of rank
r ≤ √
m with the prescribed elements Mij, (i, j) ∈ Ω.
Proof of Theorem 3.3 We argue by a contradiction. Suppose that there is a sequence
{Yk} ⊂ Mr converging to ¯Y such that PΩ(Yk) = M . Note that it follows that Yk − ¯Y ∈ VΩc.
By passing to a subsequence if necessary we can assume that (Yk − ¯Y )/tk, where tk :=
(cid:107)Yk − ¯Y (cid:107), converges to some H ∈ VΩc (of course it is assumed that Yk (cid:54)= ¯Y ). We have that
Yk = ¯Y + tkH + o(tk) and hence H ∈ TMr( ¯Y ). It remains to note that H (cid:54)= 0 to obtain the
required contradiction with condition (3.15).
Proof of Theorem 3.5 Suppose that in row i ∈ {1, ..., n1} there are less than r elements
of Ω. This means that the set σi := {j : (i, j) ∈ Ωc} has cardinality greater than n2 − r. Let
F be a left side complement of ¯Y and G be a right side complement of ¯Y . Since rows gj of
G are of dimension 1× (n2 − r), we have then that vectors gj, j ∈ σi, are linearly dependent,
i.e.,(cid:80)
j∈σi
λjgj = 0 for some λj, not all of them zero. Then
(cid:80)
j ⊗ fi) =(cid:0)(cid:80)
j∈σi
λj(g(cid:62)
j∈σi
(cid:1)(cid:62) ⊗ fi = 0.
λjgj
(7.1)
j ⊗ fi, (i, j) ∈ Ωc, to be linearly independent.
This contradicts the condition for vectors g(cid:62)
Similar arguments can be applied to the columns of matrix ¯Y . (cid:3)
Proof of Theorem 3.6 Suppose that Ω is reducible. Then by making permutations of
rows and columns if necessary, it can be assumed that M has the block diagonal form as in
1 , M2 = V2W (cid:62)
(3.18). Let ¯Y be a respective minimum rank solution. That is M1 = V1W (cid:62)
(cid:1) being n1 × r and n2 × r matrices of rank r.
(cid:1) and W =(cid:0) W1
2 and
1 are zero matrices. By changing V1 to αV1 and W1 to α−1W1 for α (cid:54)= 0, we change
. Since ¯Yij (cid:54)= 0, (i, j) ∈ Ωc, it cannot happen that V1W (cid:62)
V2W (cid:62)
and ¯Y = V W (cid:62) with V =(cid:0) V1
(cid:16) M1 V1W (cid:62)
(cid:17)
(cid:16) M1
Note that ¯Y =
V2W (cid:62)
matrix ¯Y to matrix
1 M2
V2
. This shows solution ¯Y is not locally unique.
αV1W (cid:62)
(cid:17)
W2
α−1V2W (cid:62)
1 M2
Proof of Theorem 3.7 Suppose that Ω is irreducible. Consider a rank one solution ¯Y =
vw(cid:62) with respective vectors v = (v1, ..., vn1)(cid:62) and w = (w1, ..., wn2)(cid:62). We can assume that
v1 is ﬁxed, say v1 = 1. Consider an element M1j1, (1, j1) ∈ Ω, in the ﬁrst row of matrix M .
Since M1j1 = v1wj1, it follows that the component wj1 of vector w is uniquely deﬁned. Next
consider element Mi1,j1, (i1, j1) ∈ Ω. Since Mi1j1 = vi1wj1, it follows that the component
vi1 of vector v is uniquely deﬁned. We proceed now iteratively. Let ν ⊂ {1, ..., n1} and
ω ⊂ {1, ..., n2} be index sets for which the respective components of vectors v and w are
already uniquely deﬁned. Let j (cid:54)∈ ω be such that there is (i, j(cid:48)) ∈ Ω with j(cid:48) ∈ ω and hence
wj(cid:48) is already uniquely deﬁned. Since Mij = viwj and Mij(cid:48) = viwj(cid:48), it follows that wj is
23
uniquely deﬁned and j can be added to the index set ω. If such column j does not exist,
take row i (cid:54)∈ ν such that there is (i(cid:48), j) ∈ Ω with i(cid:48) ∈ ν. Then vi is uniquely deﬁned and
hence i can be added to ν. Since Ω is irreducible, this process can be continued until all
components of vectors v and w are uniquely deﬁned.
Proof of Proposition 4.1 Consider function deﬁned in (4.5). The diﬀerential of f (Y ) can
be written as
df (Y ) = tr[(PΩ(Y ) − M )(cid:62)dY ].
Therefore if Y ∈ Mr is an optimal solution of the least squares problem (4.2), then
∇f (Y ) = PΩ(Y ) − M is orthogonal to the tangent space TMr(Y ). By (3.14) this implies
optimality conditions (4.3) .
Proof of Proposition 4.2 Consider function φ deﬁned in (4.4), and the problem of mini-
mization of φ(Y, Θ) subject to Y ∈ Mr with Θ viewed as a parameter. Locally for Y near
¯Y ∈ Mr the manifold Mr can be represented by a system of K = n1n2−dim(Mr) equations
gi(Y ) = 0, i = 1, ..., K, for an appropriate smooth mapping g = (g1, ..., gK). That is, the
above optimization problem can be written as
min φ(y, θ) subject to gi(y) = 0, i = 1, ..., K,
(7.2)
where with some abuse of the notation we write this in terms of vectors y = vec(Y ) and
θ = vec(Θ). Note that the mapping g is such that the gradient vectors ∇g1(¯y), ...,∇gK(¯y)
are linearly independent.
First order optimality conditions for problem (7.2) are
∇yL(y, λ, θ) = 0, g(y) = 0,
(7.3)
where L(y, λ, θ) := f (y, θ) + λ(cid:62)g(y) is the corresponding Lagrangian. For θ = θ0 this system
has solution ¯y and the corresponding vector ¯λ = 0 of Lagrange multipliers. We can view
(7.3) as a system of (nonlinear) equations in z = (y, λ) variables.
Jacobian matrix (cid:0) H G
(cid:1) of the system (7.3) at (y, λ) = (¯y, ¯λ), where H := ∇yyφ(¯y, θ0) is
We would like now to apply the Implicit Function Theorem to this system of equations
to conclude that for all θ near θ0 it has unique solution near ¯z = (¯y, ¯λ). Consider the
the Hessian matrix of the objective function and G := ∇g(¯y) = [∇g1(¯y), ...,∇gK(¯y)]. We
need to verify that this Jacobian matrix is nonsingular. This is implied by condition (3.15),
which is equivalent to condition (4.6). Indeed suppose that
G(cid:62) 0
(cid:20) H G
G(cid:62) 0
(cid:21)(cid:20) v
(cid:21)
= 0,
(7.4)
for some vectors v and u of appropriate dimensions. This means that Hv + Gu = 0 and
G(cid:62)v = 0. It follows that v(cid:62)Hv = 0. Condition G(cid:62)v = 0 means that v is orthogonal to
the tangent space TMr(¯y).
It follows then by condition (4.6) that v = 0. Then Gu = 0
and hence, since G has full column rank, it follows that u = 0. Since equations (7.4) have
24
only zero solution, it follows that this Jacobian matrix is nonsingular. Now by implying the
Implicit Function Theorem to the system (7.3) we obtain the required result. This completes
the proof.
Proof of Proposition 4.4 Note that under the speciﬁed assumptions, Mij − Y ∗
ij are of
stochastic order Op(N−1/2). We have by Proposition 4.3 that an optimal solution of problem
(4.8) converges in probability to Y ∗. By the standard theory of least squares (e.g., [16, Lemma
2.2]) we can write the following local approximation near Y ∗ as (4.11). It follows that the
limiting distribution of TN (r) is the same as the limiting distribution of N times the ﬁrst
term in the right hand side of (4.11). Note that N 1/2w1/2
ij Eij converges in distribution to
normal with mean σ−1
ij ∆ij and variance one. It follows that the limiting distribution of N
times the ﬁrst term in the right hand side of (4.11), and hence the limiting distribution
of TN (r), is noncentral chi-square with degrees of freedom ν = m − dim (PΩ(L)) and the
noncentrality parameter δr. Recall that dimension of the linear space L is equal to the sum
of the dimension of its image PΩ (L) plus the dimension of the kernel Ker(PΩ). It remains
to note that condition (3.15) means that Ker(PΩ) = {0} (see Remark 4.1), and hence
dim (PΩ(L)) = dim (L) = r(n1 + n2 − r).
(7.5)
This completes the proof.
References
[1] Morteza Ashraphijuo, Vaneet Aggarwal, and Xiaodong Wang. On deterministic sam-
pling patterns for robust low-rank matrix completion. IEEE Signal Processing Letter,
accepted, 2017.
[2] Stephen R Becker, Emmanuel J Cand`es, and Michael C Grant. Templates for convex
cone problems with applications to sparse signal recovery. Mathematical programming
computation, 3(3):165, 2011.
[3] P.A. Bekker and J.M.F. Ten Berge. Generic global indentiﬁcation in factor analysis.
Linear Algebra and Its Applications, 264:255–263, 1997.
[4] Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex opti-
mization. Foundations of Computational Mathematics (FOCS), 9(6):717–772, 2009.
[5] Emmanuel J Cand`es and Terence Tao. The power of convex relaxation: Near-optimal
matrix completion. IEEE Trans. Info. Theory, 56(5):2053–2080, 2010.
[6] M. Davenport and J. Romberg. An overview of low-rank matrix recovery from incom-
plete observations. IEEE Journal of Selected Topics in Signal Processing, 10(4):608–622,
2016.
25
[7] Y. C. Eldar, D. Needell, and Y. Plan. Uniqueness conditions for low-rank matrix recov-
ery. Applied and Computational Harmonic Analysis, 33(2):309–314, Sept. 2012.
[8] M. Fazel. Matrix rank minimization with applications. Ph.D. thesis, Stanford Univer-
sity, 2002.
[9] Y.-P. Hsieh, Y.-C. Kao, R. Mahabadi, Y. Alp, and A. Kyrillidis. A non-euclidean
gradient descent framework for non-convex matrix factorization. submitted, 2017.
[10] Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algo-
rithms for learning large incomplete matrices. J. Machine Learning Research, 11:2287–
2322, 2010.
[11] D.A. McManus. Who invented local power analysis? Econometric Theory, 7:265–268,
1991.
[12] D. Pimentel-Alarcon, N. Boston, and R. D. Nowak. A characterization of deterministic
sampling patterns for low-rank matrix completion. IEEE Journal of Selected Topics in
Signal Processing, 10(4):623–636, 2016.
[13] Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank
solutions of linear matrix equations via nuclear norm minimization. SIAM Review,
52(3):471–501, 2010.
[14] A. Shapiro. Rank reducibility of a symmetric matrix and sampling theory of minimum
trace factor analysis. Psychometrika, 47:187–199, 1982.
[15] A. Shapiro. Weighted Minimum Trace Factor Analysis. Psychometrika, 47:243–264,
1982.
[16] A. Shapiro. Asymptotic distribution of test statistics in the analysis of moment struc-
tures under inequality constraints. Biometrika, 72:133–144, 1985.
[17] A. Shapiro. Extremal Problems on the Set of Nonnegative Deﬁnite Matrices. Linear
Algebra and Its Applications, 67:7–18, 1985.
[18] A. Shapiro.
Statistical
inference of semideﬁnite programming. Technical report,
Georgia Institute of Technology, 2017. Preprint posted on Optimization Online,
http://www.optimization-online.org/DB HTML/2017/01/5842.html.
[19] J.H. Steiger, A. Shapiro, and M.W. Browne. On the multivariate asymptotic distribution
of sequential chi-square statistics. Psychometrika, 50:253–254, 1985.
[20] J. A. Tropp, A. Yurtsever, M. Udell, and V. Cevher. Practical sketching algorithms for
low-rank matrix approximation. SIAM J. Matrix Anal. Appl., 38(4):1454–1485, Dec.
2017.
26
[21] E.B. Wilson and J. Worcester. The resolution of six tests into three general factors.
Proc. Nat. Acad. Sci. U.S.A., 25:73–77, 1939.
27
