A NOTION OF STABILITY FOR K-MEANS CLUSTERING*
BY T. LE GOUIC AND Q. PARIS
Aix Marseille Univ, CNRS, Centrale Marseille, I2M, Marseille, France
& National Research University Higher School of Economics
Moscow, Russia
In this paper, we deﬁne and study a new notion of stability for
the k-means clustering scheme building upon the notion of quan-
tization of a probability measure. We connect this notion of stabil-
ity to a geometric feature of the underlying distribution of the data,
named absolute margin condition, inspired by recent works on the
subject.
CONTENTS
1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1 Quantization and the k-means clustering scheme . . . . . . . . . . . . . . . . . . . . . .
1.2 Risk bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 Stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2 Stability results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1 Absolute margin condition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Comparing notions of stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
3 Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
3.1 Proof of Theorem 2.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
3.2 Proof of Proposition 2.7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
A.1 Proofs for Remark 1.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
A.2 Proof for Remark 2.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
A.3 A consistency result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B Stability of a learning problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
Author’s addresses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
A Technical results
*The study has been funded by the Russian Academic Excellence Project 5-100.
Keywords and phrases: Clustering, k-means, Stability
2
T. LE GOUIC AND Q. PARIS
1. Introduction. Unsupervised classiﬁcation consists in partitioning a data set into a se-
ries of groups (or clusters) each of which may then be regarded as a separate class of ob-
servations. This task, widely considered in data analysis, enables, for instance, practition-
ers, in many disciplines, to get a ﬁrst intuition about their data by identifying meaningful
groups of observations. The tools available for unsupervised classiﬁcation are various. De-
pending on the nature of the problem, one may rely on a model based strategy modeling
the unknown distribution of the data as a mixture of known distributions with unknown
parameters. Another approach, model-free, is embodied by the well known k-means clus-
tering scheme. This paper focuses on the stability of this clustering scheme.
1.1. Quantization and the k-means clustering scheme. The k-means clustering scheme
prescribes to classify observations according to their distances to chosen representatives.
This clustering scheme is strongly connected to the ﬁeld of quantization of probability
measures and this paragraph shortly recalls how these concepts interact. Suppose our data
modeled by n i.i.d. random variables X1,..., Xn, taking their values in some metric space
(E,d), and with same distribution P as (and independent of) a generic random variable X .
Let k ≥ 1 be an integer ﬁxed in advance, representing the prescribed number of clusters,
and deﬁne a k-points 1 quantizer as any mapping q : E → E such that2 |q(E)| = k. Denoting
c1,...,ck the values taken by q, the sets {x ∈ E : q(x) = c j }, 1 ≤ j ≤ k, partition the space E
into k subsets (or cell) and each point c j (called indifferently a center, a centroid or a code
point) stands as a representative of all points in its cell. Given a quantizer q, associated
data clusters are deﬁned, for all 1 ≤ j ≤ k, by
C j (q) := {x ∈ E : q(x) = c j }∩ {X1,..., Xn}.
The performance of this clustering scheme is naturally measured by the average square
distance, with respect to P, of a point to its representative. In other words, the risk of q
(also referred to as its distortion) is deﬁned by
R(q) :=(cid:90)
(1.1)
d(x, q(x))2 dP(x).
Quantizers of special interest are nearest neighbor (NN) quantizers, i.e. quantizers such
that, for all x ∈ E,
q(x) ∈ argmin
c∈q(E)
d(x,c).
The interest for these quantizers relies on the straightforward observation that for any
) ≤ R(q). Hence, at-
quantizer q, an NN quantizer q
(E) satisﬁes R(q
tention may be restricted to NN quantizers and any optimal quantizer
such that q(E) = q
(cid:48)
(cid:48)
(cid:48)
(1.2)
q (cid:63) ∈ argmin
R(q),
to be k-points quantizers.
1The integer k is supposed ﬁxed throughout the paper and all quantizers considered below are supposed
2For a set A, notation |A| refers to the number of elements in A.
ON THE GEOMETRY OF K-MEANS CLUSTERING
(where q ranges over all quantizers k-points quantizers) is necessarily an NN quantizer.
We will denote Qk the set of all k-points NN quantizers and, unless mentionned explicitly,
all quantizers involved in the sequel will be considered as members of Qk. For q ∈ Qk, the
value of its risk is entirely described by its image. Indeed, if q ∈ Qk takes values c1,...,ck,
then
(1.3)
Denoting c = {c1,...,ck}, referred to as a codebook, we’ll often denote by R(c) the right
hand side of (1.3) with a slight abuse of notation.
min
1≤j≤k
d(x,c j )2 dP(x).
R(q) =(cid:90)
V j (c) :=(cid:169) x ∈ E : ∀(cid:96) ∈ {1,...,k}, d(x,c j ) ≤ d(x,c(cid:96))(cid:170).
A few additional considerations, relative to NN-quantizers, will be useful in the paper.
Given c = {c1,...,ck}, denote V j (c) the set of points in E closer to c j than to any other c(cid:96),
that is
These sets do not partition the space E since, for i (cid:54)= j , the set Vi (c)∩V j (c) is not necessarily
empty. A Voronoi partition of E relative to c is any partition W1,...,Wk of E such that, for
all 1 ≤ j ≤ k, W j ⊂ V j (c) up to relabeling. For instance, given q ∈ Qk with image c, the
sets W j = q
−1(c j ), 1 ≤ j ≤ k, form a Voronoi partition relative to c. We call frontier of the
Voronoi diagram generated by c the set
F (c) :=(cid:91)
i(cid:54)=j
Vi (c)∩V j (c).
(1.4)
Given an optimal quantizer q (cid:63) with image c(cid:63) = {c (cid:63)
as the center condition, states that for all 1 ≤ j ≤ k,and provided |suppP| ≥ k,
1 ,...,c (cid:63)
k }, a remarkable property, known
(1.5)
P(V j (c(cid:63))) > 0 and c (cid:63)
∈ argmin
c∈E
V j (c(cid:63))
d(x,c)2 dP(x).
(cid:90)
From now on, the probability measure P will be supposed to have a support of more than
k points.
We end this subsection by mentioning that computing an optimal quantizer requires the
knowledge of the distribution P. From a statistical point of view, when the only informa-
tion available about P consists in the sample X1,..., Xn, reasonable quantizers are em-
pirically optimal quantizers, i.e. NN quantizers associated to any codebook ˆc = { ˆc1,..., ˆck}
satisfying
(1.6)
ˆc ∈ argmin
c={c1,...,ck }
ˆR(c) where
min
1≤j≤k
d(Xi ,c j )2.
In other words, empirically optimal quantizers minimize the risk associated to the empir-
ical measure
n(cid:88)
i=1
ˆR(c) = 1
n(cid:88)
δXi .
i=1
Pn := 1
4
T. LE GOUIC AND Q. PARIS
The computation of empirically optimal centers is known to be a hard problem, due in
particular to the non-convexity of c (cid:55)→ (cid:98)R(c), and is usually performed by Lloyd’s algorithm
for which convergence guarantees have been obtained recently by Lu and Zhou (2016) in
the context where P is a mixture of sub-gaussian distributions.
1.2. Risk bounds. The performance of the k-means clustering scheme, based on the no-
tion of risk, has been widely studied in the literature. Whenever (E,|.|) is a separable Hilbert
space, the existence of an optimal codebook, i.e. of c(cid:63) = {c (cid:63)
1 ,...,c (cid:63)
k } such that
R(c(cid:63)) = R (cid:63) =
inf
c={c1,...,ck }
R(c),
is well established (see, e.g, Theorem 4.12 in Graf and Luschgy, 2000), provided E|X|2 <
+∞. In this same context, works of Pollard (1981, 1982a) and Abaya and Wise (1984) imply
that R(ˆc) → R (cid:63) almost surely as n goes to +∞, where ˆc is as in (1.6). The non-asymptotic
performance of the k-means clustering scheme has also received a lot of attention and has
been studied, for example, by Chou (1994); Linder et al. (1994); Bartlett et al. (1998); Linder
(2000, 2001); Antos (2005); Antos et al. (2005) and Biau et al. (2008). For instance Biau et al.
(2008) prove that in a separable Hilbert space, and provided |X| ≤ L almost surely, then
(cid:112)
ER(ˆc)− R (cid:63) ≤ 12kL2/
n,
for all n ≥ 1. A similar result is established in Cadre and Paris (2012) relaxing the hypothesis
of bounded support by supposing only the existence of an exponential moment for X . In
the context of a separable Hilbert space, Levrard (2015) establishes a stronger result under
some conditions involving the quantity p(t) deﬁned as follows.
DEFINITION 1.1 ( Levrard, 2015 ). Let M be the set of all c(cid:63) = {c (cid:63)
R (cid:63). For t ≥ 0, let
k } such that R(c(cid:63)) =
1 ,...,c (cid:63)
p(t) := sup
c(cid:63)∈M
P(F (c(cid:63))t ),
(1.7)
where, for any set A ⊂ E, the notation At stands for the t-neighborhood of A in E deﬁned by
At = {x ∈ E : d(x, a) ≤ t} and where F (c(cid:63)) is deﬁned in (1.4).
For any codebook c = (c1,...,ck), P(F (c)t ) corresponds to the probability mass of the fron-
tier of the associated Voronoi diagram inﬂated by t (see Figure 1). Under some slight re-
strictions and supposing p(t) does not increase too rapidly with t, it appears that the ex-
cess risk is of order O(1/n) as described below.
THEOREM 1.2 ( Proposition 2.1 and Theorem 3.1 in Levrard, 2015 ). Suppose that (E,|.|) is
a (separable) Hilbert space. Denote
− c (cid:63)
P(V j (c(cid:63))).
(1) Suppose that P(x : |x| ≤ L) = 1 for some L > 0. Then B > 0 and pmin > 0.
| and pmin =
B = inf
inf
c(cid:63)∈M ,1≤j≤k
c(cid:63)∈M ,i(cid:54)=j
|c (cid:63)
ON THE GEOMETRY OF K-MEANS CLUSTERING
(2) Suppose in addition that there exists r0 > 0 such that, for all 0 < t ≤ r0,
where p(t) is as in (1.7). Then, for all x > 0, and any ˆc minimizing the empirical risk as in
(1.6),
p(t) ≤ B pmin
128L2 t,
R(ˆc)− R (cid:63) ≤ C (k + x)L2
with probability at least 1− e
(and explicit) characteristics of P.
−x, where C > 0 denotes a constant depending on auxiliary
Fig 1: For k = 5, the ﬁgure represents k centers in the Euclidean plane.
The black solid lines deﬁne the frontier of the associated Voronoi diagram.
The light-green area, inside the red dashed lines, corresponds to the t-
neighborhood of this frontier for some small t.
1.3. Stability. For a quantizer q ∈ Qk, the risk R(q) describes the average square distance
of a point x ∈ E to its representative q(x) whenever x is drawn from P. The risk of q char-
acterizes therefore an important feature of the clustering scheme based on q and deﬁning
optimality of q in terms of the value of its risk appears as a reasonable approach. However,
an important though simple observation is that the excess risk R(q)−R(q (cid:63)), for an optimal
quantizer q (cid:63), isn’t well suited to describe the geometric similarity between the clusterings
based on q and q (cid:63). For one thing, there might be several optimal codebooks. Also, even
in the context where there is a unique optimal codebook, quite different conﬁgurations
T. LE GOUIC AND Q. PARIS
of centers c may give rise to very similar values of the excess risk R(c)− R(c(cid:63)). This obser-
vation relates to the difference between estimating the optimal quantizer and learning to
perform as well as the optimal quantizer and is relevant in a more general context as brieﬂy
discussed in Appendix B below. Basically, the idea of stability we are referring to consists
in identifying situations where having centers c with small excess risk guarantees that c
isn’t far from an optimal center c(cid:63) geometrically speaking. We formalize this idea below.
DEFINITION 1.3. Consider a function F : Qk × Qk → (cid:82)+. The clustering problem discussed
in subsections 1.1 and 1.2 is called (F, φ)-stable if, for any optimal quantizer q (cid:63), for any
auxiliary quantizer q,
(1.8)
F (q (cid:63), q) ≤ φ(R(q)− R(q (cid:63))).
We say that the clustering problem is strongly stable for F , if φ is linear.
Note ﬁrst that, for some chosen F , the notion of stability deﬁned above characterizes a
property of the underlying distribution P. Here, properties of the function F are deliber-
ately unspeciﬁed as, in practice, F can be chosen in order to encode very different proper-
ties, of more or less geometric nature. An important property of this notion is that stable
clustering problem are such that ε-minimizers of the risk are close to the optimal quan-
tizer (in the sense of F ). See 2.5 for such result.
tion (called algorithm) A :(cid:83)
REMARK 1.4. This notion differs from the notion of algorithm stability, as studied in Ben-
David et al. (2006) and Ben-David et al. (2007). Their notion of stability is deﬁned for a func-
n E n → Qk that gives a quantizer for any sample {X1,..., Xn}.
Then the stability of A is deﬁned as
Stab(A,P) = lim
n→∞ED(A({X1,..., Xn}), A({Y1,...,Yn})),
where the Xi ’s and Yi ’s are i.i.d. random variables of common law P and D is a (pseudo)metric
on Qk. In this context, an algorithm is said to be stable for P if Stab(A,P) = 0. According to
this deﬁnition, any constant algorithm A = q is stable. A notable difference, is that our no-
tion of stability includes a notion of consistency. Indeed, since q (cid:55)→ R(q) is continuous (for a
proper choice of the metric on Qk), then our notion of stability measures (if and) at which
rate q → q (cid:63) whenever R(q) → R (cid:63). Thus, we focus only on the behaviour of algorithms A
such that R(A({X1,..., Xn})) → R (cid:63).
A ﬁrst rather obvious choice for F is given by
F1(q (cid:63), q) := minσ max
1≤j≤k
d(c (cid:63)
j ,cσ(j )),
(1.9)
if q(E) = {c1,...,ck} and q (cid:63)(E) = {c (cid:63)
mutations σ of {1,...,k} (see Figure 3).
1 ,...,c (cid:63)
k } and where the minimum is taken over all per-
ON THE GEOMETRY OF K-MEANS CLUSTERING
REMARK 1.5. Note that F1(q (cid:63), q) does not always coincide with the Hausdorff distance
dH (c(cid:63),c) between c = {c1,...,ck} and c(cid:63) = {c (cid:63)
k }. Indeed, Figure 2 presents a conﬁg-
uration of codebooks c and c(cid:63) that have small Hausdorff distance but deﬁne NN quantizers
q and q (cid:63) with large F1(q (cid:63), q). However, it may be seen that inequality
1 ,...,c (cid:63)
dH (c(cid:63),c) ≤ F1(q (cid:63), q)
always holds and that, provided
dH (c(cid:63),c) < 1
|c (cid:63)
− c (cid:63)
|,
min
i(cid:54)=j
we obtain dH (c(cid:63),c) = F1(q (cid:63), q). The proof of these statements is reported in Appendix A.1.
Fig 2: In this simple case, where k=3, the set of black dots and the set of
white dots have small Hausdorff distance but deﬁne two NN quantizers, say
q1 and q2, for which F1(q1, q2) is large.
Whenever (E,|.|) is Euclidean, it follows from the previous remark and Pollard (1982b) that,
provided the optimal codebook c(cid:63) is unique,
F1(q (cid:63), ˆq) −→
a.s.,
n→+∞ 0,
when ˆq is any quantizer minimizing the empirical risk ˆR. In Levrard (2015), under the
conditions of Theorem 1.2, it is proven that for any optimal quantizer q (cid:63), and any q ∈ Qk
such that q(E) ⊂ {x : |x| ≤ L},
(R(q)− R(q (cid:63))),
F1(q (cid:63), q)2 ≤ pmin
(cid:112)
provided F1(q (cid:63), q) ≤ Br0/4
2M which proves in this case (a local version of) the stability
of the clustering scheme for F1 (constants are deﬁned in Theorem 1.2).
In the same spirit, when E = Rd and for a measure P with bounded support, Rakhlin and
(cid:48)
(cid:112)
n are optimal
Caponnetto (2007) show that F1(qn, q
(cid:48)
quantizers for empirical measures Pn and P
n whose supports differ by at most o(
n)
points. In addition, their Lemma 5.1 shows that, for P with bounded support,
n) → 0 as n → ∞ whenever qn and q
(cid:48)
dH (c(cid:63),c) ≤ C E[||X − q(X )|2−|X − q (cid:63)(X )|2|]
d+2 ,
for some constant C > 0. Note that, since E[||X −q(X )|2−|X −q (cid:63)(X )|2|] ≥ R(q)−R(q (cid:63)), our
main result (Theorem 2.3) improves this inequality under suitable conditions discussed
below.
8
T. LE GOUIC AND Q. PARIS
While F1 captures distances between representatives of the two quantizers, it is however
totally oblivious to the amount of wrongly classiﬁed points. From this point of view, a more
interesting quantity is described by
(1.10)
F2(q (cid:63), q) := minσ P
V j (c(cid:63))∩Vσ(j )(c)
(cid:34)(cid:195) k(cid:91)
j=1
(cid:33)c(cid:35)
where the minimum is taken over all permutations σ of {1,...,k} (see Figure 3). This quan-
tity measures exactly the amount of points that are misclassiﬁed by q compared to q
regarding P.
In the present paper, we study a related quantity, of geometric nature, deﬁned simply as
the average square distance between a quantizer q and an optimal quantizer q (cid:63), i.e.
(1.11)
d(q(x), q (cid:63)(x))2 dP(x).
F(q (cid:63), q)2 :=(cid:90)
As discussed later in the paper (see Subsection 2.2), this quantity may be seen as an inter-
mediate between F1 and F2 incorporating both the notion of proximity of the centers and
the amount of misclassiﬁed points. The general concern of the paper will be to establish
conditions under which the clustering scheme is strongly stable for this function F2.
Fig 3: The image of q (cid:63) (resp. q) is represented by the black (resp. blue dots).
The quantity F1(q (cid:63), q) corresponds to the length of the longest pink seg-
ments in the ﬁrst (left) ﬁgure. The quantity F2(q (cid:63), q) is the P measure of the
light green area in the second (right) ﬁgure.
ON THE GEOMETRY OF K-MEANS CLUSTERING
2. Stability results.
In this section, we present our main results. In the sequel, we restrict
ourselves to the case where E is a (separable) Hilbert space with scalar product 〈.,.〉 and
associated norm |.|. For any E-valued random variable Z , we’ll denote
(cid:107)Z(cid:107)2 := E|Z|2,
for brevity.
2.1. Absolute margin condition. We ﬁrst address the issue of characterizing the stability
of the clustering scheme in terms of the function F deﬁned in (1.11). The next deﬁnition
plays a central role in our main result.
DEFINITION 2.1 ( Absolute margin condition ). Suppose that(cid:82) |x|2dP(x) < +∞ and let q (cid:63)
be an optimal k-points quantizer of P. For λ ≥ 0, deﬁne
A(λ) =(cid:169)x ∈ E : q (cid:63)(x + λ(x − q (cid:63)(x))) = q (cid:63)(x)(cid:170).
Then, P is said to satisfy the absolute margin condition with parameter λ0 > 0, if both the
following conditions hold:
1. P(A(λ0)) = 1.
2. For any random variable Y such that (cid:107)X − Y (cid:107) ≤ (1+ λ0)(cid:107)X − q(X )(cid:107), the application
q ∈ Qk (cid:55)→ (cid:107)Y − q(Y )(cid:107)2
has a unique minimizer qλ.
) for λ(cid:48) ≤ λ. Letting c(cid:63) = q (cid:63)(E), the ﬁrst point of
Note that A(0) = E and that A(λ) ⊂ A(λ(cid:48)
this deﬁnition states that the neighborhood E \ A(λ0) of the frontier F (c(cid:63)) is of probability
zero (see Figure 4). The next remark discusses the geometry of the set A(λ), involved in the
previous deﬁnition, in comparison with the sets F (c(cid:63))t used in Deﬁnition 1.1. In partic-
ular, it follows from the following remark that, for appropriate 0 < t1 < t2, the set E \ A(λ)
satisﬁes
F (c)t1 ⊂ (E \ A(λ0)) ⊂ F (c)t2.
REMARK 2.2. Let c = {c1,...,ck} ⊂ E. Denote
m(c) = min
i(cid:54)=j
|ci − c j| and M(c) = max
i(cid:54)=j
|ci − c j|.
For all λ ≥ 0 and t > 0, let
A(λ) := {x ∈ E : q(x + λ(x − q(x))) = q(x)} and B(t) := E \ F (c)t .
Then the following statements hold.
10
T. LE GOUIC AND Q. PARIS
Fig 4: The ﬁgure represents an optimal codebook (the black dots) for a
distribution P, the frontier of the associated Voronoi diagram (the solid
black line) and, for a ﬁxed value of λ > 0, the set A(λ) (the light blue area).
For points x in A(λ) (blue dots), the ﬁgure represents the associated point
x + λ(x − q (cid:63)(x)) (tip of the arrows) which, by deﬁnition of A(λ), belongs to
A(λ).
ON THE GEOMETRY OF K-MEANS CLUSTERING
11
1. For all 0 < t < M(c)/2,
2. For all λ > 0
(cid:181)
2t
(cid:182)
M(c)− 2t
(cid:182)
(cid:181) m(c)λ
2(1+ λ)
B(t) ⊂ A
A(λ) ⊂ B
We are now in position to state the main result of this paper.
THEOREM 2.3. Suppose that(cid:82) |x|2dP(x) < +∞. Let q (cid:63) be an optimal quantizer for P and
suppose that P satisﬁes the absolute margin condition 2.1 with parameter λ0 > 0. Then, for
any q ∈ Qk, it holds that
F(q (cid:63), q)2 ≤ 1+ λ0
λ0
(R(q)− R(q (cid:63))).
REMARK 2.4. The above theorem states that the clustering scheme is strongly stable for F
provided the absolute margin condition holds. Here, we brieﬂy argue that this result is op-
timal in the sense that strong stability requires that both hypotheses of the absolute margin
condition 2.1 hold in general.
1. The following example shows that the ﬁrst point of the absolute margin condition
cannot be dropped. Take P uniform on [−1,1]× [−1/2,1/2] and ﬁx k = 2. Then the
ﬁrst point of the absolute margin condition is clearly not satisﬁed. The codebook
c(cid:63) = {(−1/2,0),(1/2,0)}
deﬁnes the unique optimal quantizer. For ε > 0, consider now
cε = {(−1/2, ε),(1/2,−ε)}.
Then it can be checked through straightforward computations that F(q (cid:63), qε) = ε and
that R(qε)− R(q (cid:63)) ≤ ε2, so that there exists no λ > 0 for which inequality
F(q (cid:63), qε)2 ≤ 1+ λ
(R(qε)− R(q (cid:63)))
holds for all ε > 0.
2. If there is not uniqueness of an optimal quantizer of P, then the result clearly cannot
hold. Although, this uniqueness property does not sufﬁce. To illustrate this statement,
suppose P is deﬁned by P = (µ1 + µ2)/2 where µ1 is uniform on [−1;1]× {1} and µ2 is
uniform on [−1;1]× {−1}. For k = 2, the codebook
c(cid:63) = {(0,1),(0,−1)}
deﬁnes the unique optimal quantizer for P. The distribution P satisﬁes the ﬁrst point
of the absolute margin condition for any λ > 0, but fails to satisﬁes the second point
for large λ. In particular, it follows from details in the proof of Theorem 2.3 that the
desired inequality cannot hold for large λ.
12
T. LE GOUIC AND Q. PARIS
An interesting consequence of Theorem 2.3 holds in the context of empirical measures
for which the absolute margin condition always holds. Consider a sample X1,..., Xn com-
posed of i.i.d. variables with distribution P and let
n(cid:88)
i=1
Pn = 1
δXi .
The next result ensures that an ε-empirical risk minimizer (i.e. a quantizer qε such that
Rn(qε) ≤ infq Rn(q)+ ε) is at a distance (in terms of F) at most ε(1+ λ)/λ to an empirical
risk minimizer for some λ depending only on Pn.
COROLLARY 2.5. Let ε > 0. Let Pn be the empirical measure of a measure P, associated
with sample {X1,..., Xn}. Suppose Pn has a unique optimal quantizer ˆq. Then Pn satisﬁes
the absolute margin condition for some λn > 0. In addition, if qε ∈ Qk satisﬁes
n(cid:88)
i=1
then
n(cid:88)
|Xi − qε(Xi )|2 ≤ ε+ 1
|Xi − ˆq(Xi )|2,
i=1
F( ˆq, qε)2 ≤ 1+ λn
ε.
λn
The last result follows easily from Theorem 4.2 in Graf and Luschgy (2000) (stating that
Pn(F (ˆc)) = 0, for ˆc = ˆq(E), and thus Pn(A(λ)) = 1 for some λ > 0) and from Theorem 2.3.
The proof is therefore omitted for brevity. The interpretation of this corollary is that any al-
gorithm producing a quantizer q with small empirical risk ˆR(q) will be, automatically, such
that F( ˆq, q) is small (and again, provided uniqueness of ˆq) if λn is large. λn deﬁned by the
absolute margin condition, provides thus a key feature for stability of the k-means. The
nice feature of the previous result is that λn is of course independent of the ε-minimizer
qε. However, an important remaining question, of large practical value, is to lower bound
λn with large probability to assess the size of the coefﬁcient (1+ λn)/λn. This is left for
future research.
2.2. Comparing notions of stability. This subsection describes the relationships existing
between the function F involved in our main result, with the two functions F1 and F2 men-
tioned earlier in section 1.3. Below, we restrict attention to the case where there is a unique
optimal quantizer q (cid:63). Comparing F and F2 can be done straightforwardly. Let
m = inf
i(cid:54)=j
|c (cid:63)
− c (cid:63)
| and M = sup
i(cid:54)=j
|c (cid:63)
− c (cid:63)
|.
Observe that, for F1(q (cid:63), q) small enough, the permutation reaching the minimum in the
deﬁnitions of F1 and F2 is the same and can be assumed to be the identity without loss of
ON THE GEOMETRY OF K-MEANS CLUSTERING
13
generality. Then, it follows that, for F1(q (cid:63), q) small enough,
P(Vi (c(cid:63))∩V j (c))|c (cid:63)
F(q (cid:63), q)2 = k(cid:88)
≤ k(cid:88)
i=1
≤ F1(q (cid:63), q)2+ F2(q (cid:63), q)(F1(q (cid:63), q)+ M)2,
− ci|2+(cid:88)
P(Vi (c(cid:63))∩Vi (c))|c (cid:63)
− c j|2
i ,j=1
i(cid:54)=j
P(Vi (c(cid:63))∩V j (c))(|c (cid:63)
− c j|+ M)2
and similarly, when m ≥ F1(q (cid:63), q),
F(q (cid:63), q)2 ≥ k(cid:88)
− ci|2+ k(cid:88)
i(cid:54)=j=1
P(Vi (c(cid:63))∩Vi (c))|c (cid:63)
i=1
≥ F2(q (cid:63), q)(m − F1(q (cid:63), q))2.
P(Vi (c(cid:63))∩V j (c))(m −|c (cid:63)
− c j|)2
This two inequalities imply that F2 and F2 are comparable whenever F1 is small enough.
Comparing F1 and F requires more effort, although one inequality is also quite straight-
forward. Recall the notation pmin = infi P(Vi (c(cid:63))). Suppose again that the optimal permu-
tation in the deﬁnition of F1 is the identity. Then, remark that F1(q (cid:63), q) ≤ m/2, implies
|c (cid:63)
− c j|, for all i , j . Thus, in this case,
− ci| ≤ |c (cid:63)
F(q (cid:63), q)2 = E|q (cid:63)(X )− q(X )|2
i ,j=1
= k(cid:88)
≥ k(cid:88)
i=1
≥ pminF1(q (cid:63), q)2.
P(Vi (c(cid:63))∩V j (c))|c (cid:63)
k(cid:88)
P(Vi (c(cid:63))∩V j (c))|c (cid:63)
j=1
− c j|2
− ci|2
In view of providing a more detailled result, we deﬁne the function p (cid:63), similar in nature to
the function p introduced by Levrard (2015) and deﬁned in 1.1.
DEFINITION 2.6. For a metric space (E,d) and a probability measure P on E, let X be a
random variable of law P. Denote q (cid:63) an optimal quantizer of P with image c(cid:63) = {c (cid:63)
1 ,...,c (cid:63)
k }
and ∂Vi (c(cid:63)) the frontier of the Voronoi cell associated to ci . Then, for all t > 0, we let
(cid:169)md(X , ∂Vi (c(cid:63))) ≤ 2d(X , q (cid:63)(X ))t + 2t 2(cid:170)(cid:33)
(cid:195) k(cid:91)
i=1
p (cid:63)(t) := P
where m = infi(cid:54)=j |c (cid:63)
− c (cid:63)
|.
While p(t) corresponds to the probability of the t-inﬂated frontier of the Voronoi cells
(deﬁned in Deﬁnition 1.1), p (cid:63)(t) corresponds to a similar object in which the inﬂation of
14
T. LE GOUIC AND Q. PARIS
the frontier gets larger as the points go further from their representant in the codebook
c(cid:63). These two functions can thus differ signiﬁcantly, in general. However, since m/4 ≤
d(X , q (cid:63)(X )) for X such that d(X , ∂Vi (c(cid:63))) < m/4, it follows that
p(t) ≤ p (cid:63) (2t),
whenever 0 < t < m/4. And when the probability measure P has its support in a ball of
diameter R > 0, it can be readily seen that for all t > 0
p (cid:63)(t) ≤ p(cid:161)m
−1(cid:163)2Rt + 2t 2(cid:164)(cid:162).
If the support of P is not contained in a ball, the comparison is not as straightforward.
We can now state the last comparison inequality.
PROPOSITION 2.7. Under the same setting as in the Deﬁnition 2.6,
F(q (cid:63), q)2 ≤ F1(q (cid:63), q)2+ p (cid:63)(F1(q (cid:63), q))(M + F1(q (cid:63), q))2
A consequence of this proposition and the result of Levrard (2015) recalled in Theorem 1.2
is the following
COROLLARY 2.8. Under the conditions of Theorem 1.2,
F(q (cid:63), ˆq)2 = O
for any empirical risk minimizer ˆq.
(cid:181)
(cid:182)+ p (cid:63)
(cid:181) 1
(cid:181) 1(cid:112)
(cid:182)(cid:182)
3. Proofs. This section gathers the proofs of the main results of the paper. Additional
proofs are postponed to the appendices.
3.1. Proof of Theorem 2.3. Recall that E is a Hilbert space with scalar product 〈.,.〉, norm
|.| and that, for an E-valued random variable Z with square integrable norm, we denote
(cid:107)Z(cid:107)2 = E|Z|2 for brevity. For λ > 0, set
xλ = x + λ(x − q (cid:63)(x)).
As E is a Hilbert space, we have for all y, z ∈ E and all t ∈ [0,1],
|t y + (1− t)z|2 = t|y|2+ (1− t)|z|2+ t(1− t)|y − z|2.
Now for all x ∈ E, any quantizer q ∈ Qk and any λ > 0, using the previous inequality with
y = xλ− q(x), z = q (cid:63)(x)− q(x) and t = (1+ λ)
−1, it follows that
|q (cid:63)(x)− q(x)|2 = 1+ λ
≤ 1+ λ
(|x − q(x)|2−|x − q (cid:63)(x)|2)+ |xλ− q (cid:63)(x)|2−|xλ− q(x)|2
(|x − q(x)|2−|x − q (cid:63)(x)|2)+ |xλ− q (cid:63)(x)|2−|xλ− q(xλ)|2
ON THE GEOMETRY OF K-MEANS CLUSTERING
15
where the last inequality follows from the fact that q is a nearest neighbor quantizer. Inte-
grating this inequality with respect to P, we obtain
(3.1)
where we have denoted
F(q (cid:63), q)2 ≤ 1+ λ
(R(q)− R(q (cid:63)))+ 1
cq(λ),
cq(λ) := (cid:107)Xλ− q (cid:63)(X )(cid:107)2−(cid:107)Xλ− q(Xλ)(cid:107)2.
Observe that λ (cid:55)→ cq(λ) is continuous. Now, deﬁne
c∞(λ) := sup
cq(λ),
where the supremum is taken over all k-points quantizers q ∈ Qk. The function λ (cid:55)→ c∞(λ)
is increasing and satisﬁes obviously c∞(λ) ≥ cq (cid:63)(λ) ≥ 0, for all λ > 0. To prove the theorem,
we will show that c∞(λ0) ≤ 0, whenever P satisﬁes the absolute margin condition with
paramater λ0 > 0. To that aim, we provide two auxiliary results.
LEMMA 3.1. Suppose there exists R > 0 such that P(B(0,R)) = 1. For all λ > 0, denote qλ
any quantizer such that cqλ(λ) = c∞(λ) and denote q (cid:63) an optimal quantizer of the law of
X . Assume that there exists λ0 > 0, s.t. with probability one, ∀1 ≤ j ≤ k,
Then, for all 0 < λ1 < λ0, there exists ε > 0 such that for all 0 < λ < λ1, if F1(qλ, q (cid:63)) < ε, then
q (cid:63)(X ) = q (cid:63)(Xλ0).
q (cid:63) = qλ.
PROOF OF LEMMA 3.1. Set qλ(E) = {c1,...,ck} and {c (cid:63)
k } = q (cid:63)(E). Suppose without loss
of generality that the optimal permutation in the deﬁnition of F1 is the identity. The as-
sumption implies that, with probability one, for each 1 ≤ i ≤ k, on the event q (cid:63)(X ) = c (cid:63)
i ,
the inequality |Xλ0
|2 ≤ |Xλ0
1 ,...,c (cid:63)
− c (cid:63)
− c (cid:63)
|2 holds, or equivalently
− c (cid:63)
〉 ≤ |c (cid:63)
− c (cid:63)
i ,c (cid:63)
|2.
2(1+ λ0)〈X − c (cid:63)
(3.2)
However,
|Xλ0
|Xλ0
− ci|2 ≤ |Xλ0
so that |Xλ0
− c j|2 if
− ci|2 =(1+ λ0)2|X − c (cid:63)
− c j|2 =(1+ λ0)2|X − c (cid:63)
|2+|c (cid:63)
|2+|c (cid:63)
− ci|2+ 2(1+ λ0)〈X − c (cid:63)
− c j|2+ 2(1+ λ0)〈X − c (cid:63)
i ,c (cid:63)
i ,c (cid:63)
− ci〉
− c j〉
2(1+ λ0)〈X − c (cid:63)
i ,c j − ci〉 ≤ |c (cid:63)
− c j|2−|c (cid:63)
− ci|2.
This means that q (cid:63) and qλ share the same cells on the support of P. Thus,
on the event q (cid:63)(X ) = c (cid:63)
i . As a result,
(cid:195) k(cid:91)
i=1
|Xλ− ci|2 < |Xλ− c j|2,
= 1.
(cid:33)
(cid:170)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)X − λc (cid:63)
(cid:175)(cid:175)2
(cid:170)(cid:175)(cid:175)X − c (cid:63)
(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)2
+ ci
1+ λ
{q (cid:63)(X ) = c (cid:63)
i }∩ {qλ(Xλ) = ci }
k(cid:88)
k(cid:88)
E1(cid:169)q (cid:63)(X )=c (cid:63)
E1(cid:169)q (cid:63)(X )=c (cid:63)
≥ (1+ λ)2
= (cid:107)Xλ− q (cid:63)(X )(cid:107)2,
i=1
i=1
(cid:107)Xλ− qλ(X )(cid:107)2 = (1+ λ)2
16
T. LE GOUIC AND Q. PARIS
Since (3.2) holds, for all λ1 < λ0, there exists therefore ε = ε(λ0, λ1,R,max{|c (cid:63)
such that, if F1(q (cid:63), q) < ε, then for all λ ≤ λ1,
− c (cid:63)
| : i (cid:54)= j })
(3.3)
where inequality (3.3) follows from the center condition (1.5). Therefore, since qλ mini-
i i.e. q (cid:63) = qλ;
mizes(cid:107)Xλ−q(Xλ)(cid:107)2 amongst NN quantizers, (3.3) is an equality, so that ci = c (cid:63)
since (cid:107)Xλ − X(cid:107) = (1+ λ)(cid:107)X − q (cid:63)(X )(cid:107) ≤ (1+ λ0)(cid:107)X − q (cid:63)(X )(cid:107) implies from asbolute margin
condition that qλ is unique.
LEMMA 3.2. Suppose X satisﬁes the conditions of Lemma 3.1. Denote
λ− = max{λ : c∞(λ) > 0}.
If λ− < λ0, then, then there exists q (cid:54)= q (cid:63) such that
) = cq(λ−
c∞(λ−
) = cq (cid:63)(λ−
).
PROOF OF LEMMA 3.2. Suppose that there exists only one quantizer qλ− such that c∞(λ−
cqλ− (λ−
) =
. Therefore, by continuity of λ (cid:55)→ cqλ− (λ),
). By deﬁnition of λ−
, cqλ− (λ) ≤ 0 for λ < λ−
) = 0,
cqλ− (λ−
and thus; by previous Lemma 3.1, qλ− = q (cid:63), since cq (cid:63)(λ−
, denote
by qλ any quantizer such that cqλ(λ) = c∞(λ), which exists by Lemma A.1. Then by Lemma
A.1, F1(qλ, qλ−) → 0 as λ → λ−
, so that for all λ− λ− > 0 small enough, Lemma 3.1 applies
and states qλ = qλ− = q (cid:63); which contradicts the deﬁnition of λ−
The absolute margin condition for λ0 (Deﬁnition 2.1) implies that λ−
3.2 is greater than λ0. Therefore, c∞(λ0) = 0, and thus (3.1) gives
) = 0. Now, for all λ > λ−
deﬁned in Lemma
F(q (cid:63), q)2 ≤ 1+ λ0
λ0
(R(q)− R(q (cid:63))).
Finally, by a continuity argument, the result still holds without the assumption P(|X| ≤
R) = 1.
ON THE GEOMETRY OF K-MEANS CLUSTERING
17
3.2. Proof of Proposition 2.7. The following proof borrows some arguments from the proof
|. Take 1 ≤ i , j ≤ k and con-
of Lemma 4.2 of Levrard (2015). Recall that m = infi(cid:54)=j |c (cid:63)
sider the hyperplane
| = |x − c (cid:63)
i ,j := {x ∈ E : |x − c (cid:63)
h(cid:63)
− c (cid:63)
|}.
Then, for all x ∈ Vi (c(cid:63)),
d(x,h(cid:63)
i ,j ) =
(3.4)
− c (cid:63)
〉|
− c (cid:63)
〉|
|〈c (cid:63)
|〈c (cid:63)
+ c (cid:63)
+ c (cid:63)
2|c (cid:63)
− 2x,c (cid:63)
− c (cid:63)
− 2x,c (cid:63)
2m
|x − c (cid:63)
|2−|x − c (cid:63)
2m
|2
Without loss of generality, suppose now for simplicity that the permutation σ achieving
the minimum in the deﬁnition of F1(q (cid:63), q) is the identity, σ(j ) = j , so that
F1(q (cid:63), q) = max
|c (cid:63)
− ci|.
Then, it follows that for x ∈ Vi (c(cid:63))∩V j (c),
|x − c (cid:63)
|2−|x − c (cid:63)
|2 ≤ (|x − c j|+|c j − c (cid:63)
≤ (|x − ci|+|c j − c (cid:63)
≤ (|x − c (cid:63)
|+|ci − c (cid:63)
|(|ci − c (cid:63)
= 2|x − c (cid:63)
≤ 4|x − c (cid:63)
|F1(q (cid:63), q)+ 4F1(q (cid:63), q)2.
|)2−|x − c (cid:63)
|2
|2
|)2−|x − c (cid:63)
|+|c j − c (cid:63)
|)2−|x − c (cid:63)
|)+ (|ci − c (cid:63)
|+|c j − c (cid:63)
|2
|+|c j − c (cid:63)
|)2
(3.5)
Thus, using the fact that, for all x ∈ Vi (c(cid:63)), we have
d(x, ∂Vi (c(cid:63))) = min
i(cid:54)=j
i ,j ),
we deduce from the previous observations that, for all i (cid:54)= j ,
d(x,h(cid:63)
Vi (c(cid:63))∩V j (c) ⊂(cid:169)x ∈ E : md(x, ∂Vi (c(cid:63))) ≤ 2|x − q (cid:63)(x)|F1(q (cid:63), q)+ 2F1(q (cid:63), q)2(cid:170).
(cid:91)
Vi (c(cid:63))∩V j (c) ⊂(cid:169)x ∈ E : md(x, ∂Vi (c(cid:63))) ≤ 2|x − q (cid:63)(x)|F1(q (cid:63), q)+ 2F1(q (cid:63), q)2(cid:170).
The right hand side being independent of j , we obtain in particular,
j(cid:54)=i
18
T. LE GOUIC AND Q. PARIS
i ,j=1
− c j|2
Therefore,
P(Vi (c(cid:63))∩V j (c))|c (cid:63)
E|q (cid:63)(X )− q(X )|2 = k(cid:88)
= k(cid:88)
= k(cid:88)
i=1
i(cid:54)=j,i=1,j=1
≤ F1(q (cid:63), q)2+ p (cid:63)(F1(q (cid:63), q))(F1(q (cid:63), q)+ M)2
P(Vi (c(cid:63))∩Vi (c))|c (cid:63)
P(Vi (c(cid:63))∩Vi (c))|c (cid:63)
k(cid:88)
k(cid:88)
− ci|2+
− ci|2+
i(cid:54)=j,i=1,j=1
i=1
P(Vi (c(cid:63))∩V j (c))|c (cid:63)
− c j|2
P(Vi (c(cid:63))∩V j (c))(|c (cid:63)
− c j|+ M)2
which shows the desired result.
APPENDIX A: TECHNICAL RESULTS
A.1. Proofs for Remark 1.5. Recall that, for any two sets A, B ⊂ E, their Hausdorff dis-
tance is deﬁned by
dH (A,B) := inf{ ε > 0 : A ⊂ B ε and B ⊂ Aε },
where Aε = {x ∈ E : d(x, A) ≤ ε}. The fact that dH (c(cid:63),c) ≤ F1(q (cid:63), q) then follows easily from
deﬁnitions. Now, to prove the second statement, observe that, in the context of the ﬁnite
sets c and c(cid:63), the inﬁmum in the deﬁnition of δ := dH (c(cid:63),c) is attained so that, for any
∈ B(ci , δ) = {x ∈ E : |x − ci| ≤ δ}.
i ∈ {1,...,k}, there exists some j ∈ {1,...,k} such that c (cid:63)
Now suppose that
δ < 1
min
i(cid:54)=j
|c (cid:63)
− c (cid:63)
|.
Then, the balls B(ci , δ) are necessarily disjoint and therefore contain one and only one
element of c(cid:63), denoted c (cid:63)
σ(i ). As a result,
F1(q (cid:63), q) ≤ max
|ci − c (cid:63)
σ(i )
| = δ,
where the last equality follows by construction. This implies the desired result.
A.2. Proof for Remark 2.2. Let x ∈ E and denote ci = q(x). First, it may be checked that
assumption d(x, F (c)) > ε holds if, and only if,
∀ j (cid:54)= i :
〈x − ci ,c j − ci〉
< |c j − ci|
− ε.
(A.1)
Similarly, observe that q(xλ) = q(x) if and only if, for j (cid:54)= i , we have |xλ − ci| < |xλ − c j|.
Using the deﬁnition of xλ, this last condition may be equivalently written, for all j (cid:54)= i , as
|c j − ci|
(1+ λ)2|x − ci|2 < |x − c j|2+ 2λ〈x − ci , x − c j〉+ λ2|x − ci|2
= (1+ λ2)|x − ci|2+ 2〈x − ci , λ(x − c j )+ ci − c j〉+|ci − c j|2.
(A.2)
ON THE GEOMETRY OF K-MEANS CLUSTERING
19
After simpliﬁcation in (A.2), we therefore obtain that q(xλ) = q(x) if, and only if,
(A.3)
∀ j (cid:54)= i :
0 < |ci − c j|− 2(1+ λ)
〈x − ci ,c j − ci〉
|c j − ci|
The result now easily follows from combining (A.1) and (A.3).
A.3. A consistency result. The next result is adaptated from Theorems 4.12 and 4.21 in
Graf and Luschgy, 2000.
LEMMA A.1. Suppose X ∈ L2(P). Then, letting q (cid:63) be an optimal k-points quantizer for the
distribution of X and denoting Xλ = X + λ(X − q (cid:63)(X )), the following statements hold.
1. For any λ ≥ 0, there exists a k-points NN quantizer qλ such that
(cid:107)Xλ− qλ(Xλ)(cid:107)2 = min
(cid:107)Xλ− q(Xλ)(cid:107)2,
where the minimum is taken over all k-points quantizers.
2. For all λ0 ≥ 0, if qλ0 is unique,
F1(qλ, qλ0) = 0.
lim
λ→λ0
PROOF OF LEMMA A.1. We state the result for a measure with bounded support and refer
to Graf and Luschgy (2000) for unbounded case.
1. Let qn be a sequence of quantizers such that
(cid:107)Xλ− qn(X )(cid:107)2 → inf
(cid:107)Xλ− q(X )(cid:107)2,
as n → ∞. Since balls in E are weakly compact, the centers qn(E) = {cn
k } weakly
converge to some limit {c1,...,ck} up to a subsequence. Denote q0(Xλ) a limit of a
weakly converging subsequence of qn(Xλ), realizing the limit liminf(cid:107)X − qn(Xλ)(cid:107)2
then, by Fatou Lemma,
1 ,...,cn
liminf(cid:107)Xλ− qn(Xλ)(cid:107)2 ≥ Eliminf|Xλ− qn(Xλ)|2
= (cid:107)Xλ− q0(Xλ)(cid:107)2+ Eliminf|q0(Xλ)− qn(Xλ)|2
≥ inf
(cid:107)Xλ− q(Xλ)(cid:107)2+ Eliminf|q0(Xλ)− qn(Xλ)|2,
which shows that q0 realizes the minimum of infq (cid:107)Xλ− q(X )(cid:107)2.
2. Similarly, for any sequence λn → λ0 as n → ∞, qλn (Xλn ) has a weak limit q0(Xλ0) (up
to subsequence). Then,
(cid:107)Xλ0
n→∞ (cid:107)Xλn
− qλ0(Xλ0)(cid:107)2 = liminf
≥ liminf
n→∞ (cid:107)Xλn
≥ Eliminf|Xλn
≥ (cid:107)Xλ0
≥ (cid:107)Xλ0
− qλ0(Xλn )(cid:107)2
− qλn (Xλn )(cid:107)2
− qλn (Xλn )|2
− q0(Xλ0)(cid:107)2+ Eliminf|q0(Xλ0)− qλn (Xλn )|2
− qλ0(Xλ0)(cid:107)2+ Eliminf|q0(Xλ0)− qλn (Xλn )|2.
20
T. LE GOUIC AND Q. PARIS
The last inequality holds because qλ0 is optimal. This shows Eliminf|q0(X )−qn(X )|2 =
0, and since qλ0 is supposed to be unique q0 = qλ0 and therefore, every subsequence
converges to the same limit qλ0; so F1(qλ, qλ0) → 0.
APPENDIX B: STABILITY OF A LEARNING PROBLEM
In this section, we brieﬂy argue that the problem considered in the paper, while of special
interest in the context of unsupervised learning, ﬁnds a natural extension in a more gen-
eral framework of learning theory, namely the context of contrast minimization. Let Z be
a measurable space equipped with a probability distribution P and let T be a given set of
parameters. Suppose given a sample Z1,..., Zn of i.i.d. variables with common distribution
P. Given a contrast function
C : Z × T → (cid:82)+,
consider the problem of designing a data driven t, based on the sample Z1,..., Zn, achiev-
ing a small value of the risk function
R(t) :=(cid:90)
C (z, t)dP(z).
This general problem, known as contrast minimization, is a classical way to unify the su-
pervised and unsupervised learning approaches as illustrated in the next example.
EXAMPLE B.1. Classical examples include the following.
• Supervised learning. The supervised learning problem corresponds to the contrast
minimization problem where Z = X × Y , where T is a class of candidate functions
t : X → Y and where, for a given loss function (cid:96) : Y 2 → (cid:82)+, the contrast is
C ((x, y), t) = (cid:96)(y, t(x)).
• Unsupervised learning. The unsupervised learning problem discussed earlier in the
present paper corresponds to the contrast minimization problem where Z is a metric
space (E,d), where T is the set Q of all k-points quantizers, for a given integer k, and
where the contrast function is
(B.1)
C (x, q) = d(x, q(x))2.
Given the general problem of contrast minimization, formulated above, one may naturally
extend the question discussed in the present paper by considering the following notion of
stability.
DEFINITION B.2. Consider a function F : T 2 → (cid:82)+ and an increasing function φ : (cid:82)+ → (cid:82)+.
Then, the contrast minimization problem is called (F, φ, ε)-stable if, for any t (cid:63) minimizing
the risk on T ,
F (t (cid:63), t) ≤ ε ⇒ F (t (cid:63), t) ≤ φ(R(t)− R(t (cid:63))).
ON THE GEOMETRY OF K-MEANS CLUSTERING
21
Our main result, Theorem 2.3, proves the stability of the contrast minimization problem
for the contrast function deﬁned in (B.1). The following result proves the stability of the
supervised learning problem for a strongly convex loss function.
EXAMPLE B.3. Consider the supervised learning problem described in the example above.
Suppose there exists α > 0 such that, for all y ∈ Y , the function u ∈ Y (cid:55)→ (cid:96)(y,u) is α-strongly
convex. Then, for any convex class T of functions t : X → Y and any t (cid:63) minimizing the risk
on T , we have
(cid:90)
for any t ∈ T , where µ is the marginal of P on X . In particular, for all ε > 0, this learning
problem is (ε, φ)-stable for the L2(µ) metric with φ(u) = 2
(t − t (cid:63))2 dµ ≤ 4
(R(t)− R(t (cid:63))),
(cid:112)
(cid:112)
α.
u/
REFERENCES
E.A. Abaya and G.L. Wise. Convergence of vector quantizers with applications to optimal quantization. SIAM
Journal of Applied Mathematics, 44:183–189, 1984.
A. Antos. Improved minimax bounds on the test and training distortion of empirically designed vector quan-
tizers. IEEE Transactions on Information Theory, 51:4022–4032, 2005.
A. Antos, L. Györﬁ, and A. György. Improved convergence rates in empirical vector quantizer design. IEEE
Transactions on Information Theory, pages 4013–4022, 2005.
P.L. Bartlett, T. Linder, and G. Lugosi. The minimax distorsion redundancy in empirical quantizer design.
IEEE Transactions on Information Theory, 44:1802–1813, 1998.
S. Ben-David, U. Von Luxburg, and D. Pál. A sober look at clustering stability. In International Conference on
Computational Learning Theory, pages 5–19. Springer, 2006.
S. Ben-David, D. Pál, and H. U. Simon. Stability of k-means clustering. In International Conference on Com-
putational Learning Theory, pages 20–34. Springer, 2007.
G. Biau, L. Devroye, and G. Lugosi. On the performance of clustering in hilbert spaces. IEEE Transactions on
Information Theory, 54:781–790, 2008.
B. Cadre and Q. Paris. On Hölder ﬁelds clustering. Test, 21:301–316, 2012.
P.A. Chou. The distorsion of vector quantizers trained on n vectors decreases to the optimum at oP (1/n).
IEEE Transactions on Information Theory, pages 457–457, 1994.
S. Graf and H. Luschgy. Foundations of quantization for probability distributions. Springer-Verlag, New-York,
2000.
C. Levrard. Nonasymptotic bounds for vector quantization in hilbert spaces. The Annals of Statistics, 43(2):
592–619, 2015.
T. Linder. On the training distortion of vector quantizers. IEEE Transactions on Information Theory, pages
1617–1623, 2000.
T. Linder. Learning-theoretic methods in vector quantization. Lecture Notes for the Advanced School on the
Principle of Nonparametric Learning, Udine, Italy, July 9-13, 2001.
T. Linder, G. Lugosi, and K. Zeger. Rates of convergence in the source coding theorem, in empirical quantizer
design, and in universal lossy source coding. IEEE Transactions on Information Theory, 40:1728–1740,
1994.
Y. Lu and H.H. Zhou.
Statistical and computational guarantees of lloyd’s algorithm and its variants.
arXiv:1612.02099, 2016.
D. Pollard. Strong consistency of k-means clustering. The Annals of Statistics, 9:135–140, 1981.
D. Pollard. A central limit theorem for k-means clustering. The Annals of Probability, 10:199–205, 1982a.
D. Pollard. Quantization and the method of k-means. IEEE Transactions on Information Theory, 28:1728–
1740, 1982b.
A. Rakhlin and A. Caponnetto. Stability of k-means clustering. In Advances in neural information processing
systems, pages 1121–1128, 2007.
22
T. LE GOUIC AND Q. PARIS
E-MAIL: thibaut.le_gouic@math.cnrs.fr
NATIONAL RESEARCH UNIVERSITY
HIGHER SCHOOL OF ECONOMICS (HSE)
FACULTY OF COMPUTER SCIENCE
SCHOOL OF DATA ANALYSIS AND ARTIFICIAL INTELLIGENCE
MOSCOW, RUSSIA
E-MAIL: qparis@hse.ru
