VR Goggles for Robots:
Real-to-sim Domain Adaptation for Visual Control
Jingwei Zhang∗1, Lei Tai∗2, Yufeng Xiong1, Ming Liu2, Joschka Boedecker1, Wolfram Burgard1
1 University of Freiburg, 2 The Hong Kong University of Science and Technology
Abstract
This paper deals with the reality gap from a novel
perspective, targeting transferring Deep Reinforce-
ment Learning (DRL) policies learned in simulated
environments to the real-world domain for visual
control tasks. Instead of adopting the common so-
lutions to the problem by increasing the visual ﬁ-
delity of synthetic images output from simulators
during the training phase, this paper seeks to tackle
the problem by translating the real-world image
streams back to the synthetic domain during the de-
ployment phase, to make the robot feel at home. We
propose this as a lightweight, ﬂexible, and efﬁcient
solution for visual control, as 1) no extra trans-
fer steps are required during the expensive training
of DRL agents in simulation; 2) the trained DRL
agents will not be constrained to being deployable
in only one speciﬁc real-world environment; 3) the
policy training and the transfer operations are de-
coupled, and can be conducted in parallel. Besides
this, we propose a conceptually simple yet very ef-
fective shift loss to constrain the consistency be-
tween subsequent frames, eliminating the need for
optical ﬂow. We validate the shift loss for artis-
tic style transfer for videos and domain adaptation,
and validate our visual control approach in real-
world robot experiments. A video of our results
is available at: https://goo.gl/b1xz1s.
1 Introduction
Pioneered by the Deep Q-network [Mnih et al., 2015] and
followed up by various extensions and advancements [Mnih
et al., 2016; Lillicrap et al., 2015; Schulman et al., 2015;
Schulman et al., 2017], Deep Reinforcement Learning (DRL)
algorithms show great potential in solving high-dimensional
real-world robotics sensory control tasks. However, DRL
methods typically require several millions of training sam-
ples, making them infeasible to train directly on real robotic
systems. As a result, DRL algorithms are generally trained in
simulated environments, then transferred to and deployed in
real scenes. However, the reality gap, also referred to as the
∗indicates equal contribution.
domain shift, namely the noise pattern, texture, lighting con-
dition discrepancies, etc., between synthetic renderings and
real sensory readings, imposes major challenges for gener-
alizing the sensory control policies trained in simulation to
reality.
In this paper, we focus on visual control tasks, where au-
tonomous agents perceive the environment with their onboard
cameras, and execute commands based on color image read-
ing streams. A natural way and also the typical choice in the
recent literature of dealing with the reality gap for visual con-
trol, is by increasing the visual ﬁdelity of the simulated im-
ages [Bousmalis et al., 2017], by matching the distribution of
synthetic images to that of the real ones [Tobin et al., 2017],
and by gradually adapting the learned features and represen-
tations from the simulated domain to the real-world domain
[Rusu et al., 2017]. These sim-to-real methods, however,
inevitably have to add preprocessing steps for each individ-
ual training frame to the already expensive learning pipeline
of DRL control policies; also, the complete policy training
phase has to be conducted again for each visually different
real-world scene. Attempts have also been made in computer
graphics to directly increase the quality of the simulators, to
make the synthetically rendered images more visually realis-
tic; however, the rendering for detailed and realistic texture
and modality often adds to the computational burden.
This paper attempts to tackle the reality gap in the visual
control domain from a novel perspective, with the aim of
adding minimal extra computational burden to the learning
pipeline. We cope with the reality gap only during the ac-
tual deployment phase of agents in real-world scenarios, by
adapting the real camera streams to the synthetic modality, so
as to translate the unfamiliar or unseen features of real images
back into the simulated style, which the agents have already
learned how to deal with during training in the simulators.
Compared to other sim-to-real methods bridging the reality
gap, our proposed real-to-sim approach, which we refer to as
VR Goggles, has several appealing properties:
• First of all, our approach is highly lightweight. It does
not add any extra processing burden to the training phase
of DRL policies.
• Secondly, our proposed method is highly ﬂexible and
efﬁcient. Since we decouple the policy training and the
adaptation operations, the preparations for transferring
the polices from simulation to the real world can be con-
ducted in parallel with the training of DRL control poli-
cies. From each visually different real-world environ-
ment that we expect to deploy the agent in, we just need
to collect several (typically on the order of 2000) im-
ages, and train a model of VR Goggles for each of them.
More importantly, we do not need to retrain or ﬁnetune
the visual control policy for new environments.
As an additional contribution, we propose a new shift
loss, which enables us to generate consistent synthetic image
streams without information to impose temporal constraints,
or even sequential training data. We show that shift loss is a
promising and much cheaper alternative to the constraints im-
posed by optical ﬂow, and we demonstrate its effectiveness in
artistic style transfer for videos and domain adaptation.
2 Related Works
2.1 Domain Adaptation
Domain adaptation, also referred to as image-to-image trans-
lation, targets translating images from a source domain into
a target domain. We here focus on the most general unsu-
pervised methods that require the least manual effort and are
applicable to robotics control tasks.
CycleGAN [Zhu et al., 2017a]
introduced a cycle-
consistent loss to enforce an inverse mapping from the tar-
get domain to the source domain on top of the source to
target mapping.
It does not require paired data from the
two domains of interest, and shows convincing results for
relatively simple data distributions containing few semantic
types. However, in terms of translating between more com-
plex data distributions containing many more semantic types,
its results are not as satisfactory, in that permutations of se-
mantics often occur. CyCADA [Hoffman et al., 2017] added
a semantic constraint on top, to enforce a match between the
semantic map of the translated image and that of the input.
However, the semantic loss was not added in its experiments
on large datasets due to memory limitations.
Following the observation that several most recent and ad-
vanced robotics simulators do provide semantic ground truth,
and the semantic segmentation literature is quite mature (e.g.,
[Chen et al., 2017]), we adopt the semantic constraint from
CyCADA into our method. We are able to include the seman-
tic loss calculation with special conﬁgurations (Sec. 4.2).
2.2 Domain Adaptation for DRL
DRL approaches have been adopted into robotics control
tasks such as manipulation and navigation. Below we review
the recent literature with an emphasis on works taking the re-
ality gap into consideration.
For manipulation, [Bousmalis et al., 2017] bridged the re-
ality gap by adapting synthetic images to the realistic do-
main during the training phase. However, this addition of
an adaptation step before every training iteration can greatly
slow down the whole learning pipeline. [Tobin et al., 2017]
proposed to ramdomise the texture of objects, lighting condi-
tions, and camera positions during training, in the hope that
the learned model will generalize naturally to real-world sce-
narios. However, such randomization unfortunately cannot
be satisﬁed at a low cost by most of the popular robotic simu-
lators. Moreover, there is no guarantee that these randomized
simulations can cover the visual modality of a random real-
world scene. [Rusu et al., 2017] deals with the reality gap
by progressively adapting the learned features and represen-
tations of a model trained in simulation to that of the realistic
domain. This method, however, still needs to go through an
expensive control policy training phase for each visually dif-
ferent real-world scenario.
Artistic style transfer for videos works on video sequences
instead of individual frames. It targets generating temporally
consistent stylizations for sequential frames. [Ruder et al.,
2017] provides a key observation that: a trained stylization
network with a total downsampling factor of s (e.g., s = 4
for a network with 2 convolutional layers of stride 2), is shift
invariant to shifts equal to the multiples of s pixels, but can
output signiﬁcantly different stylizations otherwise. This un-
For navigation, where autonomous agents are expected to
encounter sensor readings of environments at a much larger
scale than manipulation, the reality gap has not been directly
dealt with in the literature of learning-based visual control to
the best of our knowledge. Some works, however, chose spe-
cial setups to circumvent the reality gap. For example, 2D
Lidar [Tai et al., 2017; Zhang et al., 2017b] and depth images
[Zhang et al., 2017a; Tai et al., 2018] are sometimes chosen
as the sensor modality for transferring the navigation policies
to the real world, since the discrepancies between the simu-
lated domain and the real-world domain for them are smaller
than those for color images. [Zhu et al., 2017b] conducted
real-world experiments with visual inputs. But in their se-
tups, the real-world scene is highly visually similar to their
simulated environment, which is a condition that can rarely
be met in practice.
In this paper, we mainly consider domain adaptation for
visual navigation tasks using DRL, which has not yet been
considered in the literature. We believe the adaptation for
navigation is much more challenging than for manipula-
tion, since navigation agents usually work in environments
at much larger scales with more complexities than the con-
ﬁned workspace for manipulators. We believe our proposed
real-to-sim method can be naturally adopted in manipulation.
An important aspect of domain adaptation, within the con-
text of dealing with the reality gap for DRL, is the consis-
tency between subsequent frames, which has not yet been
considered in any of the aforementioned adaptation meth-
ods. As a method for solving sequential decision making, the
consistency between the subsequent input frames for DRL
agents can be critical for the successful fulﬁllment of their ﬁ-
nal goals. Apart from the solutions for solving the reality gap
for DRL, the general domain adaptation literature also lacks
works considering sequential frames instead of single frames.
Therefore, we look to borrow techniques from other re-
search ﬁelds that successfully extend single-frame algorithms
to the video domain, among which the most applicable meth-
ods are those from the artistic style transfer literature.
2.3 Artistic Style Transfer for Videos
Artistic style transfer is a technique for transfering the artistic
style of artworks to photographs [Johnson et al., 2016].
Figure 1: The VR Goggles pipeline. We depict the computation of the losses LGANX, LcycY , LsemY and LshiftX. We show both outdoor and
indoor scenarios for demonstration, where the adaptation for the outdoor scene is trained with the semantic loss Lsem (since its simulated
domain CARLA has ground truth semantic labels to train a segmentation network fX), and the indoor one without (since its simulated domain
Gazebo does not provide semantic ground truth). The components marked in red are those involved in the ﬁnal deployment phase: a real
sensor reading is captured (y ∼ preal), then passed through the Goggles module (generator GX) to be translated into the simulated domain X,
where the DRL agents were originally trained; the translated image ˆx is then fed to DRL agents, to output control commands for autonomous
vehicles. For clarity, we skip the counterpart losses LGANY , LcycX, LsemX and LshiftY .
desired property (of not being shift invariant) causes the out-
put of the trained network to change signiﬁcantly for even
very small changes in the input, which leads to temporal in-
consistency (under the assumption that only relatively lim-
ited changes would appear in subsequent frames of the in-
coming sequential data). However, their solution of adding
temporal constraints between generated subsequent frames,
is rather expensive, as it requires optical ﬂow as input during
deployment. [Huang et al., 2017] incorporated the temporal
constraint into the single-frame artistic style transfer pipeline
and is a relatively cheap solution. However, we believe that
constraining optical ﬂow on single input images is not well-
deﬁned. We suspect that the improved temporal consistency
in [Huang et al., 2017] is actually due to the inexplicitly im-
posed consistency constraints for regional shifts by optical
ﬂow. We validate this suspicion in our experiments (Sec. 4.1).
We believe that the fundamental problem causing the in-
consistency (the shift variance) can be solved by an additional
constraint of shift loss, which we will introduce in Sec. 3.4.
We show that the shift loss enables us to constrain the consis-
tency between generated subsequent frames, without the need
for the relatively expensive optical ﬂow constraint. We argue
that for a network that has been properly trained to learn a
smooth function approximation, small changes in the input
should also result in small changes in the output.
3 Methods
3.1 Problem formulation
We consider visual data sources from two domains: X, con-
taining sequential frames {x0, x1, x2,···} (e.g., synthetic
images output from a simulator; x ∼ psim, where psim de-
notes the simulated data distribution), and Y, containing se-
quential frames {y0, y1, y2,···} (e.g., real camera readings
from the onboard camera of a mobile robot; y ∼ preal, where
preal denotes the distrbution of the real sensory readings). We
emphasize that, although we require our method to generate
consistent outputs for sequential inputs, we do not need the
training data to be sequential; we formalize it in this way only
because some baseline methods have this requirement.
As we have discussed, DRL agents are typically trained in
the simulated domain X, while they are expected to perform
tasks in the real-world domain Y. And as we have discussed
before, we choose to tackle this problem by translating the
images from real domain images to the synthetic domain dur-
ing deployment.
In the following we introduce the details
of our approach for performing domain adaptation. Also to
cope with the sequential nature of the incoming data streams,
we introduce a technique for constraining the consistency of
the translated subsequent frames.
3.2 CycleGAN Loss
To achieve this, we ﬁrst build on top of CycleGAN [Zhu et al.,
2017a], which learns two generative models to map between
domains: GY : X → Y, with its discriminator DY, and
GX : Y → X, with its discriminator DX, via training two
GANs simultaneously:
LGANY (GY, DY; X, Y) = Ey [log DY(y)]
LGANX (GX, DX; Y, X) = Ex [log DX(x)]
(1)
+ Ex [log(1 − DY(GY(x)))] ,
(2)
+ Ey [log(1 − DX(GX(y)))] ,
in which GY learns to generate images GY(x) matching
those from domain Y, while GX tries translating y to im-
ages from domain X. Following CycleGAN, we also add the
cycle consistency loss to constrain those mappings:
LcycY (GX, GY; Y) = Ey [||GY(GX(y)) − y||1] ,
LcycX (GY, GX; X) = Ex [||GX(GY(x)) − x||1] .
(3)
(4)
3.3 Semantic Loss
Since our translation domains of interest are between syn-
thetic images and real-world sensor images, we take advan-
tage of the fact that many recent robotic simulators provide
ground truth semantic labels and add the semantic constraint
inspired by CyCADA [Hoffman et al., 2017].
X∼psimGXGYˆYLcycYLsemYLshiftXfXfXˆX[x→·,y→·]GXY[x→·,y→·]LGANXDXtrainedinXY∼prealindoorˆXoutdooroutdoorindoorindooroutdoorAssuming that for images from domain X, the ground truth
semantic information SX is available, a semantic segmenta-
tion network fX can be easily obtained by minimizing the
cross-entropy loss, denoted CrossEnt(SX, fX(X)).
We further assume that the ground truth semantic for do-
main Y is lacking (which is the case for most real scenarios),
meaning that fY is not easily accessible.
In this case, we
provide ”semi” semantic supervision to the training agents.
After fX for semantic segmentaion of domain X is ob-
tained, ”semi” semantic supervision for the generators can be
incorporated, by imposing consistency between the seman-
tic map of the input and that of the generated output. This
semantically consistent image translation can be achieved by
minimizing the following losses (we use fX to also generate
”semi” semantic labels for domain Y):
LsemY (GY; X, fX) = CrossEnt(fX(X), fX(GY(X))) (5)
LsemX (GX; Y, fX) = CrossEnt(fX(Y), fX(GX(Y))) (6)
3.4 Shift Loss for Consistent Generation
literature for image-to-image
Different from the current
translation or domain adaptation, our model is additionally
expected to output consistent images for sequential input
data. Although by adding Lsem, the semantics of the con-
secutive outputs are constrained, inconsistences and artifacts
still occur quite often. Moreover, in cases where ground truth
semantics are unavailable from either domain, the sequential
outputs are even less constrained, which could potentially
lead to inconsistent DRL policy outputs. To constrain the
consistency even in these situations, following the discussion
from Sec. 2.3, we introduce shift loss below.
For an input image x, we use x[x→i,y→j] to denote the re-
sult of a shift operation: shifting x along the X axis by i
pixels, and j pixels along the Y axis. We sometimes omit
y → 0 or x → 0 in the subscript if the image is only shifted
along the X or Y axis.
According to [Ruder et al., 2017], a trained stylization net-
work is shift invariant to shifts of multiples of s pixels (s
represents the total downsampling factor of the network), but
can output signiﬁcantly different stylizations otherwise. This
causes the output of trained network to change greatly for
even very small changes in the input. We thus propose to add
a conceptually simple yet direct and effective shift loss:
LshiftY (GY;X) = E
(cid:104)(cid:12)(cid:12)(cid:12)(cid:12)GY(x)[x→i,y→j] − GY(x[x→i,y→j])
(cid:12)(cid:12)(cid:12)(cid:12)2
(cid:104)(cid:12)(cid:12)(cid:12)(cid:12)GX(y)[x→i,y→j] − GX(y[x→i,y→j])(cid:12)(cid:12)(cid:12)(cid:12)2
LshiftX (GX;Y) = E
y, i,j∼u(1,s−1)
x, i,j∼u(1,s−1)
(cid:105)
(cid:105)
(7)
(8)
where u denotes the uniform distribution.
Shift loss constrains the shifted output to match the output
of the shifted input, regarding the shifts as image-scale move-
ments. under the assumption that only limited regional move-
ment would appear in subsequent input frames, shift loss ef-
fectively smoothes the mapping function for small regional
movements, restricting the changes in its outputs for subse-
quent inputs frames. It can be regarded as a cheap alternative
for imposing consistency constraints on small movements,
eliminating the need for the relatively expensive optical ﬂow
information, which is crucial for meeting the requirement of
real-time control in robotics.
3.5 Full Objective
Our full objective for learning VR Goggles (Fig. 1) is:
L(GY, GX, DY, DX; X, Y, fX) =
(9)
LGANY (GY, DY; X, Y) + LGANX(GX, DX; Y, X)
LcycY (GX, GY; Y) + LcycX (GY, GX; X)(cid:1)
+ λcyc
+ λsem (LsemY (GY; X, fX) + LsemX (GX; Y, fX))
+ λshift (LshiftY (GY; X) + LshiftX (GX; Y)) ,
where λcyc, λsem and λshift controls the weighting for each
loss. This corresponds to solving the following optimization:
DY,DX L(GY, GX, DY, DX). (10)
G∗Y, G∗X = arg min
max
GY,GX
(cid:0)
4 Experiments
4.1 Artistic Style Transfer for Videos
To evaluate our method, we ﬁrstly conduct experiments for
artistic style transfer for video sequences, to validate the ef-
fectiveness of shift loss on constraining consistency for se-
quential frames. We collect a training dataset of 98 HD video
footage sequences (from VIDEVO1, containing 2450 frames
in total); the Sintel2 sequences are used for testing, as their
ground-truth optical ﬂow is available. We compare the per-
formance of the models trained under the following setups:
• FF [Johnson et al., 2016]: Conanical feed forward style
transfer trained on single frames;
• FF+ﬂow [Huang et al., 2017]: FF trained on sequential
images, with optical ﬂow added for imposing temporal
constraints on subsequent frames.
shift loss, as discussed in Sec. 3.4.
• Ours: FF trained on single frames, with an additional
We do not compare our method with that of [Ruder et al.,
2017], as they require optical ﬂow as input during deploy-
ment. This is relatively expensive for our target application
of real-time control.
Implementation-wise, we use the pretrained VGG-19 as the
loss network, relu2 2 as the content layer, relu1 2, relu2 2,
relu3 2 and relu4 2 as the style layers. We set the weight for
each loss as follows: 1e5 for content, 2 for style, 1e-7 for spa-
tial regularization, 10 for optical ﬂow, and 100 for shift. The
downsampling factor s for our transformer network is 4; we
use the same transformer network architecture and style im-
ages as in [Johnson et al., 2016]. Shifts are uniformly sam-
pled from [1, s − 1] for every training frame.
As a proof of concept, we begin our evaluation by compar-
ing the setups on their ability to generate shift invariant styl-
izations. In particular, for each image x in the testing dataset,
we generate 4 more test images by shifting the original image
along the X axis by 1, 2, 3, 4 pixels respectively, and pass
all 5 frames (x, x[x→1], x[x→2], x[x→3], x[x→4]) through the
trained network to examine the consistency of the generated
images (Fig. 2).
1https://www.videvo.net/ 2http://sintel.is.tue.mpg.de/
Figure 3: Temporal error maps between generated stylizations for
subsequence input frames. 1st row: input frames; 2nd ∼ 4th row:
temporal error maps (with the corresponding stylizations shown on
top) of outputs from FF, FF+ﬂow, and Ours. We here choose a very
challenging style (mosaic) for temporal consistency, as it contains
many ﬁne details, with tiny tiles laid over the original image in the
ﬁnal stylizations. Yet, Ours achieves very high consistency.
lower temporal loss with the shift loss constraint.
4.2 Domain Adaptation for Outdoor Scenarios
Next we validate the shift loss in the ﬁeld of domain adapta-
tion, ﬁrstly in outdoor urban street scenarios (where we col-
lect synthetic domain images X ∼ psim from the CARLA sim-
ulator [Dosovitskiy et al., 2017], and realistic domain images
Y ∼ preal from the RobotCar dataset [Maddern et al., 2017]).
We compare the following three setups:
• CyCADA [Hoffman et al., 2017]: CycleGAN with se-
mantic constraints, trained on single frames;
• CyCADA+ﬂow: CyCADA with the temporal constraint
as in [Huang et al., 2017], trained on sequential frames;
• Ours: CyCADA with shift loss, trained on single frames;
we refer to this as VR Goggles.
Table 1: Temporal loss comparison between FF, FF+ﬂow and Ours.
This metric is part of the optimization objective of FF+ﬂow, while
optical ﬂow is never provided to Ours; yet our method is able to
achieve lower temporal loss on the evaluated Sintel sequences.
Figure 2: Shift-invariance evaluation, comparing between FF,
FF+ﬂow and Ours. We shift an input image x along the X axis by
1, 2, 3, 4 pixels respectively and feed all 5 frames through the net-
works trained via FF, FF+ﬂow and Ours, and show the generated
stylizations. We mark the most visible differences in small circles
and dim the rest of the generated images. As is discussed in [Ruder
et al., 2017], FF generates almost identical stylizations for x and
x[x→4] (because 4 is a multiple of the total downsampling factor of
the trained network), but those for x[x→1],x[x→2],x[x→3] differ sig-
niﬁcantly. FF+ﬂow improves the shift-invariance, but we suspect
the improvement is due to the inexplicit consistency constraint on
regional shifts imposed by optical ﬂow. Ours, is able to generate
shift-invariant stylizations with the proposed shift loss.
The results shown in Fig. 2 validate the discussion from
[Ruder et al., 2017], since the stylizations for x and x[x→4]
from FF are almost identical (s = 4 for the trained network),
but differ signiﬁcantly otherwise. FF-ﬂow improves the in-
variance by a limited amount; Ours method is capable of gen-
erating consistent stylizations for shifted input frames, with
the shift loss directly reducing the shift variance.
We continue by evaluating the consistency of the stylized
In Fig. 3, we show the temporal
sequential input frames.
error maps, the same metric as in [Huang et al., 2017], of two
stylized consecutive frames for each method. Ours (bottom
row) achieves the highest temporal consistency.
Furthermore, we evaluate the temporal loss computed us-
ing the ground truth optical ﬂow for the Sintel sequences (Ta-
ble 1). Although the temporal loss is part of the optimization
objective of FF-ﬂow, and our method does not have access
to any optical ﬂow information, Ours is still able to achieve
x[x→1]xx[x→2]x[x→3]x[x→4]FFABOursABFF+ﬂowABmosaiclamuse0.1190.0930.0860.1320.1080.1100.1270.1040.0980.1150.0890.0830.1240.0950.087FFOursFF+ﬂow0.1220.0960.0900.1130.0910.0890.1130.0850.0780.1520.1300.1270.1250.0990.0920.1540.1310.1300.1230.0970.0900.1350.1120.1120.1380.1080.1070.1210.1000.0920.1320.1060.0940.1290.1040.0940.1140.0900.0910.1270.0960.0830.1320.1020.101FFOursFF+ﬂowambush5bamboo1market6temple2sleeping2shaman3alley2bamboo2alley1sleeping1Figure 5: Real-world visual control experiment. A DRL agent is
trained in a simulated ofﬁce environment, that is able to navigate
to chairs based on visual input. Without retraining or ﬁnetuning
the DRL policy, our proposed VR Goggles enables the mobile robot
to directly deploy this policy in real ofﬁce environments, achieving
100% success rate in a set of real-world experiments. We refer to
the attached video for details of the real-world experiments.
Speciﬁcally, we begin by training a DRL agent in a simu-
lated ofﬁce environment (A3C [Mnih et al., 2016], 20k roll-
outs, 8 parallel CPU threads), to accomplish the task of navi-
gating to chairs based purely on its front-facing color camera
readings; the agent obtained a reward of −0.005 for a step
cost, −0.05 for collision, and 1 for reaching the target. Then,
we deploy the trained DRL policy onto a real robot in a real-
world ofﬁce, and compare the following adaptations:
• NoGoggles: Feed the sensor readings directly to the
trained DRL policy;
• CycleGAN [Zhu et al., 2017a]: Use CycleGAN to trans-
late the real sensory inputs to the synthetic domain be-
fore feeding to the DRL policy; since the synthetic do-
main here (Gazebo) does not provide ground truth se-
mantics, we drop the semantic constraint Lsem;
the VR Goggles to translate the input images.
• Ours: Use models trained by CycleGAN + shift loss as
We use the same network conﬁguration as in Sec. 4.2, except
that here the input images are of size 360 × 640.
We show in the attached video that, without domain adap-
tation, directly deploying the DRL policy fails completely in
the real-world tasks; our proposed method achieves the high-
est success rate (0%, 60% and 100% for NoGoggles, Cycle-
GAN and Ours respectively) due to the quality and consis-
tency of the translated streams. The control cycle runs in real-
time at 13Hz on a Nvidia Jetson TX2. In the video, we also
show that VR Goggles can easily train a new model for a new
type of chair without any adjustment to the previously trained
control policy. We limited the velocity of the robot due to
the camera exposure time, since motion blur can greatly in-
ﬂuence the adaptation quality. We leave it as future work to
evaluate on more compatible platforms.
5 Conclusions
To conclude, we tackle the reality gap when deploying DRL
visual control policies trained in simulation to the real world,
by translating the real image streams back to the synthetic
domain during the deployment phase. Due to the sequential
nature of the incoming sensor streams for control tasks, we
propose shift loss to increase the consistency of the translated
subsequent frames. We validate the shift loss in both artistic
Figure 4: Comparison of the translated images for sequential input
frames for the different approaches. 1st row: two subsequent input
frames from the realistic domain, with several representative images
from the simulated domain shown in between; 2nd ∼ 4th row: out-
puts from CyCADA, CyCADA+ﬂow and Ours. Our method is able
to output consistent subsequent frames and eliminate artifacts. We
adjust the brightness of some zoom-ins for visualization purposes.
We pretrain the segmentation network fX using Deeplab
[Chen et al., 2017]. It is worth mentioning that the original
CyCADA paper did not use the semantic constraint in their
experiments due to memory issues. We are able to incorpo-
rate semantic loss calculation, by cropping the input images.
Actually, a naive random crop would highly likely lead to se-
mantic permutations; so we crop inputs of the two domains
in the same training iteration from the same random position,
and our empirical results show that this greatly stabilizes the
adaptation. The input images are of size 450 × 800, we train
the network with 256 × 256 crops. We use the same network
architecture as in CycleGAN, and train for 50 epochs with a
learning rate of 2e − 4, as we observe no performance gain
training for longer iterations.
4, we show a comparison of the subsequent
frames generated by the three approaches. Our method again
achieves the highest consistency and eliminates more artifacts
due to the smoothness of the learned model.
In Fig.
4.3 Domain Adaptation for Indoor Scenarios with
Real-world Robotic Experiments
Finally, we conduct domain adaptation for indoor ofﬁce en-
vironments (X ∼ psim rendered from a self-built Gazebo
[Koenig et al., 2004] world and Y ∼ preal captured from a
real ofﬁce, using a RealSense R200 camera mounted on a
Turtlebot3 Wafﬂe). We validate our proposed method of us-
ing the VR Goggles to facilitate the transfer of polices trained
in the simulated domain to the realistic domain, with a set of
real-world robotic experiments.
CycleGANNoGogglesVRGogglesDRLpolicytrainedinGazebo0%60%100%Successratestyle transfer for videos, and domain adaptation. In the end,
we successfully verify our domain adaptation method for vi-
sual control through a set of real-world robot experiments.
Several future works can be conducted based on our
method. For example, training DRL agents on more com-
plicated tasks, and in more complicated simulated environ-
ments that provide ground truth semantic labels, such as the
newly released MINOS [Savva et al., 2017]. Also, since in
this paper we have mainly focused on learning based visual
navigation, applying our method to manipulation tasks would
be an interesting direction.
References
[Bousmalis et al., 2017] Konstantinos Bousmalis, Alex Ir-
pan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey, Mri-
nal Kalakrishnan, Laura Downs, Julian Ibarz, Peter Pastor,
Kurt Konolige, et al. Using simulation and domain adapta-
tion to improve efﬁciency of deep robotic grasping. arXiv
preprint arXiv:1709.07857, 2017.
[Chen et al., 2017] LC Chen, G Papandreou, I Kokkinos,
K Murphy, and AL Yuille. Deeplab: Semantic image seg-
mentation with deep convolutional nets, atrous convolu-
tion, and fully connected crfs. IEEE transactions on pat-
tern analysis and machine intelligence, 2017.
[Dosovitskiy et al., 2017] Alexey Dosovitskiy, German Ros,
Felipe Codevilla, Antonio Lopez, and Vladlen Koltun.
In Conference
Carla: An open urban driving simulator.
on Robot Learning, pages 1–16, 2017.
[Hoffman et al., 2017] Judy Hoffman, Eric Tzeng, Tae-
sung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko,
Alexei A Efros, and Trevor Darrell. Cycada: Cycle-
consistent adversarial domain adaptation. arXiv preprint
arXiv:1711.03213, 2017.
[Huang et al., 2017] Haozhi Huang, Hao Wang, Wenhan
Luo, Lin Ma, Wenhao Jiang, Xiaolong Zhu, Zhifeng Li,
and Wei Liu. Real-time neural style transfer for videos. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 783–791, 2017.
[Johnson et al., 2016] Justin Johnson, Alexandre Alahi, and
Li Fei-Fei. Perceptual losses for real-time style transfer
In European Conference on Com-
and super-resolution.
puter Vision, pages 694–711. Springer, 2016.
[Koenig et al., 2004] Nathan Koenig, B A, and Andrew
Howard. Design and use paradigms for gazebo, an open-
In Intelligent Robots and
source multi-robot simulator.
Systems, 2004.(IROS 2004). Proceedings. 2004 IEEE/RSJ
International Conference on, volume 3, pages 2149–2154.
IEEE, 2004.
[Lillicrap et al., 2015] Timothy P Lillicrap, Jonathan J Hunt,
Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
Continuous con-
David Silver, and Daan Wierstra.
arXiv preprint
trol with deep reinforcement learning.
arXiv:1509.02971, 2015.
[Maddern et al., 2017] Will Maddern, Geoff Pascoe, Chris
Linegar, and Paul Newman. 1 Year, 1000km: The Oxford
RobotCar Dataset. The International Journal of Robotics
Research (IJRR), 36(1):3–15, 2017.
[Mnih et al., 2015] Volodymyr Mnih, Koray Kavukcuoglu,
David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidje-
land, Georg Ostrovski, et al. Human-level control through
deep reinforcement learning. Nature, 518(7540):529,
2015.
[Mnih et al., 2016] Volodymyr Mnih, Adria Puigdomenech
Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asyn-
chronous methods for deep reinforcement learning.
In
International Conference on Machine Learning, pages
1928–1937, 2016.
[Ruder et al., 2017] Manuel Ruder, Alexey Dosovitskiy, and
Thomas Brox. Artistic style transfer for videos and spher-
ical images. arXiv preprint arXiv:1708.04538, 2017.
[Rusu et al., 2017] Andrei A Rusu, Matej Veˇcer´ık, Thomas
Roth¨orl, Nicolas Heess, Razvan Pascanu, and Raia Had-
sell. Sim-to-real robot learning from pixels with progres-
sive nets. In Conference on Robot Learning, pages 262–
270, 2017.
[Savva et al., 2017] Manolis Savva, Angel X Chang, Alexey
Dosovitskiy, Thomas Funkhouser, and Vladlen Koltun.
Minos: Multimodal indoor simulator for navigation in
complex environments. arXiv preprint arXiv:1712.03931,
2017.
[Schulman et al., 2015] John Schulman, Sergey Levine,
Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust
In International Conference
region policy optimization.
on Machine Learning, pages 1889–1897, 2015.
[Schulman et al., 2017] John Schulman, Filip Wolski, Pra-
fulla Dhariwal, Alec Radford, and Oleg Klimov. Prox-
arXiv preprint
imal policy optimization algorithms.
arXiv:1707.06347, 2017.
[Tai et al., 2017] Lei Tai, Giuseppe Paolo, and Ming Liu.
Virtual-to-real deep reinforcement learning: Continuous
control of mobile robots for mapless navigation. In 2017
IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS), pages 31–36, Sept 2017.
[Tai et al., 2018] Lei Tai, Jingwei Zhang, Ming Liu, and
Wolfram Burgard. Socially-compliant navigation through
raw depth inputs with generative adversarial imitation
learning. In Robotics and Automation (ICRA), 2018 IEEE
International Conference on, May 2018.
[Tobin et al., 2017] Josh Tobin, Rachel Fong, Alex Ray,
Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel.
Domain randomization for transferring deep neural net-
In Intelligent
works from simulation to the real world.
Robots and Systems (IROS), 2017 IEEE/RSJ International
Conference on, pages 23–30. IEEE, 2017.
[Zhang et al., 2017a] Jingwei Zhang, Jost Tobias Springen-
berg, Joschka Boedecker, and Wolfram Burgard. Deep
reinforcement learning with successor features for navi-
gation across similar environments. In 2017 IEEE/RSJ In-
ternational Conference on Intelligent Robots and Systems
(IROS), pages 2371–2378, Sept 2017.
[Zhang et al., 2017b] Jingwei Zhang, Lei Tai,
Joschka
Boedecker, Wolfram Burgard, and Ming Liu. Neural slam.
arxiv preprint. arXiv preprint arXiv:1706.09520, 3, 2017.
[Zhu et al., 2017a] Jun-Yan Zhu, Taesung Park, Phillip Isola,
and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In Proceed-
ings of the IEEE Conference on Computer Vision and Pat-
tern Recognition, pages 2223–2232, 2017.
[Zhu et al., 2017b] Yuke Zhu, Roozbeh Mottaghi, Eric
Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, and Ali
Farhadi. Target-driven visual navigation in indoor scenes
In Robotics and Au-
using deep reinforcement learning.
tomation (ICRA), 2017 IEEE International Conference on,
pages 3357–3364. IEEE, 2017.
