Interpretable Deep Convolutional Neural Networks
via Meta-learning
Xuan Liu∗, Xiaoguang Wang∗†, Stan Matwin∗‡
∗Institute for Big Data Analytics
Faculty of Computer Science, Dalhousie University, Halifax, NS, Canada
Email: xuan.liu@dal.ca
†Alibaba Group, Hangzhou, China
‡Institute of Computer Science
Email: xiaoguang.wxg@alibaba-inc.com
Polish Academy of Sciences, Warsaw, Poland
Email: stan@cs.dal.ca
Abstract—Model
interpretability is a requirement in many
applications in which crucial decisions are made by users
relying on a model’s outputs. The recent movement for “al-
gorithmic fairness” also stipulates explainability, and therefore
interpretability of learning models. And yet the most successful
contemporary Machine Learning approaches, the Deep Neural
Networks, produce models that are highly non-interpretable. We
attempt to address this challenge by proposing a technique called
CNN-INTE to interpret deep Convolutional Neural Networks
(CNN) via meta-learning. In this work, we interpret a speciﬁc
hidden layer of the deep CNN model on the MNIST image
dataset. We use a clustering algorithm in a two-level structure
to ﬁnd the meta-level training data and Random Forest as base
learning algorithms to generate the meta-level test data. The
interpretation results are displayed visually via diagrams, which
clearly indicates how a speciﬁc test instance is classiﬁed. Our
method achieves global interpretation for all the test instances
without sacriﬁcing the accuracy obtained by the original deep
CNN model. This means our model is faithful to the deep CNN
model, which leads to reliable interpretations.
Index Terms—interpretability, Meta-learning, deep learning,
Convolutional Neural Network, TensorFlow, big data
I. INTRODUCTION
With the fast development of sophisticated machine learning
algorithms, artiﬁcial intelligence has been gradually penetrat-
ing a number of of brand new ﬁelds with unprecedented speed.
One of the outstanding problems hampering further progress
is the interpretability challenge. This challenge arises when
the models build by the machine learning algorithms are to
be used by humans in their decision making, particularly
when such decisions are subject to legal consequences and/or
administrative audits. For human decision makers operating
in those circumstances, to accept the professional and legal
responsibility ensuing from decisions assisted by machine
to comprehend the models. This is
learning,
generally true for areas like criminal
justice, health care,
terrorism detection, education system and ﬁnancial markets.
To trust the model, decision makers need to ﬁrst under-
stand the model’s behavior, and then evaluate and reﬁne the
model using their domain knowledge. Even for areas like
book or movie recommendations [1] and automated aids [2],
is critical
it
explanations for a recommendation and an error made could
increase the trust and reliance on these systems. Furthermore,
the European General Data Protection Regulation, forthcoming
in June, 2018, stipulates the explainability of all automatically
made decisions concerning individuals, and that includes the
decisions made with or assisted by machine learning models.
Hence, there is a growing demand for interpretability of the
machine learning algorithms.
it
In this paper, we deﬁne interpretability of a model as
the ability to provide visual or textual presentation of the
connections between input features and the output predictions.
To realize the goal of interpretability, there are usually two
approaches. One is to design an algorithm that is inherently
interpretable, while achieving competitive accuracy of a com-
plex model. The examples are Decision Trees [3], Decision
Lists [4], and Decision Sets [5], etc. The disadvantage of this
approach is that there is a trade off between interpretability
and accuracy:
is not easy to learn an interpretable (so
presumably simple) model expressing a complex process with
very high accuracy. The other approach which does not
sacriﬁce accuracy takes the opposite approach: it ﬁrst builds a
highly accurate model without worrying about interpretabilty,
and subsequently uses a separate set of re-representation
techniques to assist the user in understanding the behavior
of the algorithm. One of the techniques could be to use the
aforementioned relatively simple and interpretable algorithms
to explain the behavior of a complex model and the reasons
why a given classiﬁer, treated as a black box, classiﬁes a
given instance in a particular way, e.g. LIME [6], BETA [7],
TREPAN [11].
Deep learning methods have been lately very successful in
image processing and natural language processing. It could
be categorized as a representation learning approach [12],
which learns reﬁned features that could improve a model’s
generalization ability. Deep learning, however, is highly non-
interpretable.
In this paper we are reporting a work in progress where
we try to interpret the inner mechanisms of deep learning.
Our method: CNN-INTE is inspired by [8]. We design and
implement a tool that helps the user understand how the hidden
layers in a deep CNN model work to classify examples. And
the results are expressed in graphs which indicates sequential
separations of the true class and the hypothesis. The main
contributions of our method is as follows:
• Compared to LIME [6] which only provides local in-
terpretation in speciﬁc regions of the feature space,
our method provides global interpretation for any test
instances in the whole feature space.
• Compared to models which apply inherently interpretable
algorithms, e.g. [5], our method has the advantage of
not compromising the accuracy of the model
to be
interpreted. This produces more reliable interpretation.
• In contrary to [6] and [7] which treat the model to be in-
terpreted as black box, we interpret the inner mechanisms
of deep CNN models.
• The experiments are implemented in the TensorFlow [9]
platform, which makes our model scalable to big datasets
more easily. Scalability is an issue pointed out as future
work in [6] and [7] but not realized yet.
II. RELATED WORK
To resolve the problems for “trusting a prediction” and
“trusting a model”, two methods are proposed in [6] to explain
individual predictions and understand a model’s behavior
respectively: Local Interpretable Model-agnostic Explanations
(LIME) and Submodular Pick LIME (SP-LIME). The main
idea for LIME is to use inherently interpretable models g
to interpret complex models f
locally. They designed an
objective function to minimize the unfaithfulness (when g is
approximating f
in a local area) and the complexity of g.
Although it was stated in their paper that in the objective
function g could be any interpretable models, they set g as
sparse linear models in their paper. Based on the individual
explanations generated by LIME, they design an submodular
pick algorithm: SP-LIME to explain the model as a whole
by picking a number of representative and non-redundant
instances.
It was suggested in [10] that coverage, precision and
effort should be used to evaluate the results of the model
interpretation. Although LIME achieves high precision and
low effort, the coverage is not clear. In other words, LIME
is able to explain why a speciﬁc prediction is made using
the weights of the local model g, but can’t indicate to what
local region the explanation is faithful. To solve this problem,
the Anchor Local Interpretable Model-Agnostic Explanations
method (aLIME) was introduced in [10]. In aLIME, the if-then
rules are used instead of using the weights in a linear model to
explain a speciﬁc prediction (as was executed in LIME). The
idea is based on the Decision Sets algorithm from [5]. These
if-then rules are easy to comprehend and has good coverage.
It was pointed out that there is a trade off between inter-
pretability and accuracy for machine learning algorithms [5].
In terms of inherently interpretable models, rule-based models,
e.g. Decision Trees and Decision Lists are often preferred, as
they can ﬁnd a balance between these two factors. Decision
lists are usually considered more interpretable than decision
trees, as they use the if-then-else statements with a hierar-
chy structure. But this structure reduces to some extent the
interpretability, as to interpret an additional rule all previous
rules should be reasoned about. Also new rules down the list
are applied to much narrow feature spaces, which makes the
multi-class classiﬁcation difﬁcult where the minority classes
deserves equally good rules. This motivates the proposal of the
Decision Sets algorithm in [5], which produces the isolated if-
then rules, where each rule could be an independent prediction.
To realize this, an objective function takes into account both
interpretability (expressed by precision and recall of rules)
and accuracy (expressed by size, length, cover and overlap).
They showed that solving the objective function is a NP-
hard problem, and ﬁnds near-optimal solutions of it. However,
Decision set’ accuracy only approaches random forest, and its
expressive power just catches up with decision tree.
Another model agnostic explanation approach is
the
Black Box Explanations through Transparent Approximations
(BETA), introduced in [7]. Different from LIME which aims
for local interpretation, BETA is a framework which attempts
to produce global interpretation for any classiﬁer which are
treated as black box classiﬁers. Based on their previous work
on Decision Sets, the authors designed a framework with two
level decision sets to taking into account ﬁdelity (faithfulness
to the black box model), unambiguity (single and deterministic
explanations for each instance), interpretability (complexity
minimized) and interactivity (user speciﬁed explorations of the
feature’s subspace). In this two level structure, the outer if-then
rules are the “neighborhood descriptors” and the inner if-then
rules are “decision logic rules” (how the black box model
labels an instance under the outer if-then rules). Similar to
[5], an objective function is built and near-optimal solutions
are found.
III. METHODOLOGY
Our methodology could be classiﬁed as post-hoc interpre-
tation [14], where a trained model is given and the main task
is to interpret it. This method is close to the second approach
mentioned in the fourth paragraph of the introduction section,
but is also different in many ways. First, the model to be
interpreted here is not treated as a black box as we directly
interpret the hidden layers of a deep CNN. Second, compared
to LIME [6] which only has local interpretability, our method
achieves global
interpretability. Similar to LIME, we also
provide qualitative interpretation with graphs to visualize the
results. As our method interprets deep CNN via Meta-learning,
we ﬁrst brieﬂy introduces deep CNN, meta-learning and then
discuss our framework in details.
A. Deep Convolutional Neural Network
This section introduces the deep CNN model we are going
to interpret. As we implement our program using TensorFlow,
we use its TensorBoard function to draw the structure of the
deep CNN we construct in Fig. 1. Deep CNN is now the most
advanced machine learning algorithm for image classiﬁcation.
It takes advantage of the two-dimensional structure of the input
images. It uses a set of ﬁlters to ﬁlter the pixels of the raw
input images to generate higher level representations to be
learnt by the model in order to improve the performance.
There are three major components of deep CNN: convolu-
tional layer, pooling layer and fully connected layer (same as
in regular neural networks). A deep CNN model is usually
a stack of these layers. In the convolutional layer, a ﬁlter is
used to compute dot products between the pixels of the input
image at speciﬁc position and the values of the ﬁlter, producing
one single value in the output feature map. The convolution
operation is completed after the ﬁlter is slided across the width
and height of the input image. Following the convolutional
layer, an activation function, often a rectiﬁed linear unit
(ReLU) [15], is applied to inject nonlinearities into the model
and speed up the training process. Following ReLU is the
pooling layer which is a non-linear down-sampling layer. A
common algorithm for pooling is the max pooling algorithm.
In this algorithm, each sub-region of the previous feature map
is turned into a single maximum value in this region. Max
pooling reduces computation and controls overﬁtting. In order
to calculate the predicted class, after performing max pooling,
the feature map needs to be ﬂattened and feed into a fully
connected layer. In the last layer: the output layer, a softmax
classiﬁer is applied for prediction.
The structure of the deep CNN model we designed is
illustrated in Fig. 1, “Placeholder” represents for the interface
to input the training data. “Reshape” is needed ﬁrst to convert
the input one-dimensional image data into two dimensional
data. In our experiment, we use the MNIST dataset [16].
The 784 input features are converted into a two-dimensional
28 × 28 image. Our model has two series of a convolutional
layer followed by a pooling layer: “conv1”-“pool1”-“conv2”-
“pool2”, which are followed by one fully connected layer
“fc1”. As a fully connected network is susceptible to suffer
from overﬁtting, the “dropout” operation [17] applied after
“fc1” aims to reduce it. In this operation, a probability
parameter p is set to keep a speciﬁc neuron with probability p
(or drop it with probability 1-p). The “Adam optimizer” [18],
rather than a standard Stochastic Gradient Descent optimizer
is used to train the model via modifying the variables and
reducing the loss. “fc2” is the output layer with 10 neurons:
each represents the class 0-9.
B. Meta-learning
Meta-learning is an ensemble learning method which learns
from the results of the base classiﬁers. It has a two-level
structure, where the algorithms used in the ﬁrst level are called
base-learners and the algorithm in the second level is called
meta-learner. The base-learners are trained on the original
training data. The meta-learner is trained by the predictions of
the base classiﬁers and the true class of the original training
data. When training the meta-learner, the “Class-combiner”
strategy [13] is applied here, where the predictions includes
just
the predicted class (instead of all classes, as in the
“Binary-class-combiner”).
Fig. 1. Structure of a deep CNN model generated by TensorFlow’s Tensor-
Board .
To understand the meta-learning algorithm intuitively, Fig. 2
illustrates a simpliﬁed training process for meta-learning [19].
The numbers 1, 2, 3, 4 represents the four steps of training.
In the 1st step, the base learning algorithms 1 to m are trained
on the training data. In the 2nd step, a validation dataset is
used to test the trained classiﬁers 1 to m. In the 3rd step,
the predictions generated in step 2 and the true labels of the
validation dataset are used to train a meta-learner. Finally, in
the 4th step, a meta-classiﬁer is produced and the whole meta-
learning training process is completed.
Once the training process is accomplished, the test process
is much easier to execute. Fig. 3 presents a simpliﬁed test
process [19]. In the 1st step, the test data is applied on the base
classiﬁers to generate predictions which combined with the
true labels of the test data comprises the meta-level test data
in 2nd step. In the 3rd step, the ﬁnal predictions are generated
by testing the meta-level classiﬁer with the predictions in the
2nd step and the accuracy could be calculated.
Fig. 2. Meta-learning training process.
Fig. 4. CNN-INTE training process.
Fig. 3. Meta-learning test process.
C. Framework
Our framework is named as CNN-INTE which stands for
Convolutional Neural Network Interpretation. It is similar to
meta-learning, but different in a few ways. In this work, we
interpret the ﬁrst fully connected layer “fc1” of the deep CNN
model illustrated in Fig. 1.
The training process is shown in Fig. 5. In the 1st step, the
original training data is used to train a CNN model. In the
2nd step, the parameters generated in the 1st step are used
to calculate the values for the activations of the ﬁrst fully
connected layer: fc1. In the 3rd step, a clustering algorithm is
used to cluster the data generated in step 2 into a number of
groups which we deﬁne as factors henceforth. In the 4th step,
the data belonging to each of the factors are clustered again
generating a number of clusters each assigned a unique ID. In
the 5th step, these IDs are grouped together as the training
features in the meta-level, using the labels of the original
training data as label for the meta-learner. In the 6th step,
the features of the original training data and the IDs (set as
labels) in step 4 are used to train a number of random forests
[21].
Assume the training data T has N numbers of instances and
layer “fc1” has H neurons. The labels of the training data are
Ty = {l1, l2, ..., lN}. Once the deep CNN model is trained,
for each training instance ti, we calculate the activations at
each hidden neuron on this layer. Hence, we obtain a matrix
S with size H × N. To construct the meta-level training data,
we use a clustering algorithm to cluster the matrix along the
hidden layer axis into several factors F = {f1, f2, ..., fk}.
Then within each of the factors, we cluster the data again,
this time along the axis of the instances. The clustering results
are the IDs each instance belongs to. For instance, if there 10
clusters, after the second level clustering each instance will
have an ID between 0-9. All the IDs combined with the true
labels of the training data builds up the meta-level training
data.
To present the technical details of the CNN-INTE training
process, we provide the pseudo code in Algorithm 1. Line 1-3
is the initialization of the algorithm. In line 4, the activations
S are clustered into K factors, where K is the number of
clusters set in the clustering algorithm C l. In lines 5-7 the
same clustering algorithm C l is applied on all the factors to
generate K sets of ID numbers. Lines 8-9 uses the generated
ID numbers and the true labels of the original training data to
train the meta-learner: C m. Till now, the training process is not
done yet. We still need to generate the base models to be used
in the test process. Lines 10-12 uses the features of the original
training data and the ID numbers to train K base models.
The output of the training process would be the meta-lever
classiﬁer: ˜M and K base models: B = {M1, M2,··· , MK}.
Algorithm 1 CNN-INTE Training Process
1: Input: activations: S;
training data: T ; Meta learning
algorithm: C m; Clustering algorithm: C l; Base learning
algorithms: {C1, C2,··· , CK}
2: E = ∅
3: Scv = ∅
4: {f1, f2,··· , fK} = C l(S)
5: for k = 1··· K do
IDsk = C l(fk)
6:
7: end for
8: Scv = {IDs1, IDs2,··· , IDsK, Ty}
9: ˜M = C m(Scv)
10: for k = 1··· K do
11:
12: end for
13: E = ({M1, M2,··· , MK} , ˜M )
14: Output: Ensemble E
Mk = Ck(Tx, IDsk)
Fig. 5 is a toy example that illustrates the above process.
In this example, there are 5 hidden neurons and 6 training
instances. We set the number of clusters for both the ﬁrst and
second level clustering as 3. Hence, the matrix S with size
6 × 5 is ﬁrst clustered into 3 factors {f1, f2, f3} horizontally.
For each factor, the activations are again clustered into three
clusters vertically, e.g. F1 is clustered into {C11, C12, C13}. If
we set the ID numbers for these cluster as {0, 1, 2}, then the
corresponding ID numbers for t1 to t6 in factor f1 according
to Fig. 5 are {0, 0, 1, 1, 2, 2}. Hence, the meta-level training
features are expressed as0

0 1 1
0 0 1
1 1 1
This data combined with the corresponding training labels of
the original training data is used to train the meta-learner.
Here the meta-learner we used is the Decision Tree [3], an
inherently interpretable algorithm. Its tree structure provides
an excellent visual explanation of the predictions.
Fig. 5. Toy example for the generation of Meta-level training data.
The test process of the meta-model is exactly the same
as the meta-learning test process, which is shown in Fig. 6.
In the test process, we use the original test data to test the
base classiﬁers generated in the meta-level training process to
obtain the meta-level test data’s features. The base-learner we
applied is random forest. The number of base models is equal
to the number of factors. Hence, we have K base models:
B = {M1, M2, ..., MK}. In the toy example, there are three
factors which lead to three base models. The training data for
the ﬁrst base model corresponding to F 1 would be
(t1 − l1)
(t2 − l2)
(t3 − l3)
(t4 − l4)
(t5 − l5)
(t6 − l6)
Here ti−li represents for the features of each original training
instance. Once we obtain the K base models, we can use the
original test data to test them to produce the meta-level test
data. These data are then feed into the trained decision tree
model to interpret individual test predictions.
Fig. 6. CNN-INTE test process.
IV. EXPERIMENTS
The dataset we use is the MNIST database of handwritten
digits from 0 to 9 [16]. We extracted 55,000 examples (the
original dataset has 60,000 examples for training) as the
training data and 10,000 examples as the test data. Each of
the examples represents for the 28 × 28 images with pixels
ﬂattened as 784 features. The experiments are performed on
the TensorFlow platform.
A. Experimental Setup
First of all, we need to train a nice deep CNN model. We
ﬁrst reshape the input training data into 55000 images each
with size 28 × 28. Training all the data on every epoch is
expensive, which requires lots of resources of the computer
and may lead to the termination of the program. Here we
apply stochastic training: on the ﬁrst epoch, we select a mini-
batch of the training data and perform optimization on this
batch; once we loop through all the batches, we randomize
the training data and start a new epoch. In our experiment, we
set the epoch e = 1000, batch size b = 50. Stochastic training
is cheap and achieves similar performance to using the whole
training data in every epoch. For each mini-batch, in the ﬁrst
convolutional layer, we apply 32 ﬁlters (or kernels) each with
size 5×5, which generates 32 feature maps. In the ﬁrst pooling
layer we apply ﬁlters with size 2 × 2. The stride size is set as
2. The second convolutional layer use 64 ﬁlters with the same
size as the ﬁrst convolutional layer. The second pooling layer
has the same parameters as the previous one. Immediately after
this pooling layer is the ﬁrst fully connected layer. We set the
number of neurons for this layer as 128. To reduce overﬁtting
we also set the dropout [17] parameter d = 0.5, which means
a neuron’s output has 50% probability to be dropped. The
last layer is the second fully connected layer (or the “readout
layer”), which has 10 neurons with each neuron outputs the
probability of the corresponding digits 0-9. The test accuracy
of this trained CNN model on the test data is 93.9%.
Now comes the key part for setting up interpretation. we
deﬁne interpretability of a model as the ability to provide
visual or textual presentation of the connections between input
features and the output predictions. We ﬁrst feed the trained
fully connected layer f c1 with the original training data, which
would produce a data S with size of 128 × 55000. We then
cluster S into several factors. The clustering algorithm we
applied is the k-means algorithm [20]. The number of factors
is equal to the number of clusters which we set as 8 in this
level. Hence, S is now turned into a list F = {f1, f2, ..., f8}
with size 8 × 55000 having each row representing the data
belonging to each factor. In the second level clustering, for
each factor in F we use the k-means algorithm again to cluster
them into a number of clusters. We set the number as 10 in our
experiment as the number of classes for the original training
data is 10. Hence each cluster will be assigned a unique ID
number between 0 and 9. Then we use the IDs belonging
to each training instances and the true labels of the original
training data to train a decision tree algorithm. Due to the
limitation of the space, we are unable to show the structure
of the trained decision tree here. We set the maximum depth
of the decision tree as 5. Although deeper decision tree would
generate better accuracy, it makes it harder to interpret with
too many tree levels.
To obtain the test data for decision tree, we ﬁrst use the
original training data’s feature as features and the IDs for each
factor in F as labels to train the corresponding random forest
algorithm [21], generating 8 base models. For random forest,
we set the number of trees as 20 and the maximum nodes as
2000. Finally we use the original test data to test the 8 trained
base models. The generated predictions become the features
of meta-level test data with sizes of 10000 × 8. Using the
meta-level test data on the trained decision tree produces an
accuracy of 92.8% with tree depth=5. This value is comparable
to the test accuracy on the trained deep CNN model: 93.9%.
It should be noted that the decision tree’s accuracy could be
further improved by increasing the depth of tree and tuning
other related parameters.
B. Experimental Results
To interpret the deep CNN model’s behavior on the test data,
we intend to use diagrams generated by our tool: CNN-INTE
to examine individual predictions on the test data. Hence,
we provide qualitative interpretations visually. We arbitrarily
selected two test instances that were correctly classiﬁed by the
decision tree and one test instance that was wrongly classiﬁed.
It should be noted that this tool could be used on any test
instances globally and not just limited to the three cases we
provide. The details of the selected test instances are shown
in Table I. Here “f0-f7” represents the features of the meta-
level test data, “label” is the test label in the original test data,
“pred” is the prediction generated by the decision tree on the
meta-level test data. “True1” and “True2” represents for the
two correctly classiﬁed instances and “Wrong1” is the wrongly
classiﬁed instance.
INSTANCES SELECTED FROM THE META-LEVEL TEST DATA
TABLE I
Features and labels
True1
True2
Wrong1
f0
f1
f2
f3
f4
f5
f6
f7
label
pred
In order to examine the classiﬁcation process visually, we
check each feature values according to the trained structure
of the decision tree and plot the graphs of the activations
corresponding to the true label and the hypothesis. The in-
terpretation result for instance “True1” is shown in Fig. 7. As
the true label for this instance is 3, all other classes could be
regarded as hypothesis and this is why there are no graphs for
“Hypothesis: 3” in Fig. 7. Each row represents the examination
of the feature values corresponding to different factors in
different levels of the trained decision tree, e.g. the ﬁrst row
represents the root level of the decision tree. Since we set the
depth of the decision tree as 5, there are 5 rows in all. Each
column stands for the query of if the test instance belongs to
the corresponding hypothesis over the nodes visited.
Take the column of “Hypothesis:0” as an example, the goal
is to ﬁnd if the label of the test instance is 0. In the 1st
row we extract the activations corresponding to “f6” which
satisﬁes the condition that f6 (cid:54) 4.5 (this is determined by the
trained decision tree) and draw a graph between activations
that belongs to label=0 (hypothesis) and label=3 (true). Then
we check the graph to evaluate if the data corresponding to
the true class could be separated from the hypothesis. The
answer is no because the hypothesis represented as blue points
overlaps with the true class shown as red points. Hence, we
need to query the trained decision tree further. The values of
the factors we need to check is: f6 (cid:54) 2.5 for 2nd row; f6 (cid:54) 0.5
for 3rd row; f7 (cid:54) 0.5 for 4th row; f1 (cid:54) 0.5 for 5th row. In
this process, we noticed that in the 4th row the true class and
the hypothesis class are successfully separated as only the red
points corresponding to the true label are left. Therefore, we
don’t need to examine further and that’s why the graph for the
5th row is not displayed. We highlight the graph with green
rectangles if the ﬁnal results are separable and red vice versa.
The same idea is applied on other hypothesis. We also draw
the graphs for instances “True2” and “Wrong1” in Fig. 8 and
Fig. 9 respectively.
V. CONCLUSION AND FUTURE WORK
In this work, we present an interpretation tool CNN-INTE,
which interprets a hidden layer of a deep CNN model: to
ﬁnd out how the learned hidden layer classiﬁes new test
instances. Although we just show the results for the ﬁrst
fully connected layer before the read-out layer, the approach
could be applied on any hidden layers. The interpretation
is realized by ﬁnding the relationships between the original
training data and the trained hidden layer “fc1” via meta-
learning. We used two-level k-means clustering algorithm to
ﬁnd the meta-level training data and random forests as base
models for generating meta-level test data. The visual results
generated by our program clearly indicate why a test instance
is truly or wrongly classiﬁed by checking if there are any
overlaps of the corresponding activations. For future work, we
plan to initiate quantiﬁcation of the interpreted results. In our
experiments, one of the things we ﬁnd tricky is the setting of
the number of clusters for the k-means algorithm. In the future,
we plan to replace the k-means algorithm with DBSCAN [22]
Fig. 7. Example of a correctly classiﬁed test instance: True1.
which doesn’t need specifying the number of clusters. As
stated in [5], “decision sets” seems to be a better option than
decision tree as a inherently interpretable algorithm, so we also
plan to replace decision tree with decision sets. Last but not
least, it would be quite meaningful to apply this tool on real
world applications where interpretations are demanded either
between the training data and the hidden layer or between the
hidden layer and the predictions.
ACKNOWLEDGMENT
The authors acknowledge the support of the Province of
Nova Scotia, of Dalhousie University, and of the the Natural
Sciences and Engineering Research Council of Canada under
the CREATE program grant.
REFERENCES
[1] J. L. Herlocker, J. A. Konstan, and J. Riedl, “Explaining collaborative
ﬁltering recommendations,” In Proceedings of the 2000 ACM conference
on Computer supported cooperative work, pp. 241–250,December 2000.
[2] M. T. Dzindolet, S. A. Peterson, R. A. Pomranky, L. G. Pierce, and H. P.
Beck. “The role of trust in automation reliance,” Int. J. Hum.-Comput.
Stud., vol.58, no.6, pp.697–718, 2003.
[3] J. Ross Quinlan, C4. 5: programs for machine learning, Elsevier, 2014.
[4] R. L. Rivest, “Learning decision lists,” Machine learning, vol.2, no.3,
pp.229–246, 1987.
[5] H. Lakkaraju, S. H. Bach, and J. Leskovec, “interpretable decision sets:
A joint framework for description and prediction,” In Proceedings of the
22nd ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, pp. 1675–1684, ACM, August, 2016.
[6] M. T.Ribeiro, S.Singh and C.Guestrin, “Why should i trust you? :
Explaining the predictions of any classiﬁer,” In Proceedings of the 22nd
ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, pp. 1135–1144, ACM, August, 2016.
[7] H. Lakkaraju, E.Kamar, R.Caruana and J.Leskovec, “Interpretable &
Explorable Approximations of Black Box Models,” KDD’17 workshop,
2017.
[8] J. J.Thiagarajan, B.Kailkhura, P. Sattigeri
, and K. N.Ramamurthy,
“TreeView: Peeking into Deep Neural Networks Via Feature-Space Par-
titioning,” 30th Conference on Neural Information Processing Systems
(NIPS), 2016.
[9] M.Abadi,et al. “TensorFlow: A System for Large-Scale Machine Learn-
ing,” In OSDI, Vol. 16, pp. 265–283, 2016.
[10] M. T.Ribeiro, S.Singh and C.Guestrin, “Nothing Else Matters: Model-
Agnostic Explanations By Identifying Prediction Invariance,” 30th Con-
ference on Neural Information Processing Systems (NIPS), 2016.
[11] M. Craven and J. W. Shavlik, “Extracting tree-structured representations
of trained networks,” In Advances in neural information processing
systems, pp. 24–30, 1996.
[12] I. Goodfellow, Y. Bengio and A. Courville, Deep learning, MIT press,
2016.
[13] P. K. Chan and S. J. Stolfo, “Experiments on multistrategy learning by
meta-learning,” In Proceedings of the second international conference
on information and knowledge management, pp. 314–323, ACM, De-
cember, 1993.
[14] G. Montavon, W. Samek, and K.R. M¨uller, “Methods for interpreting and
understanding deep neural networks,” Digital Signal Processing, 2017.
[15] V. Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted boltz-
mann machines,” In Proceedings of the 27th international conference on
machine learning (ICML-10), pp. 807–814, 2010.
[16] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” Proceedings of the IEEE, 86(11), pp.
2278–2324, November, 1998.
[17] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-
dinov, “Dropout: A simple way to prevent neural networks from overﬁt-
ting,” The Journal of Machine Learning Research, 15(1), pp. 1929–1958,
2014.
[18] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
arXiv preprint arXiv:1412.6980, 2014.
[19] X. Liu, X. Wang, S. Matwin, and N. Japkowicz, “Meta-learning for
large scale machine learning with MapReduce,” In Big Data, 2013 IEEE
International Conference on, pp. 105–110, IEEE, October, 2013.
Fig. 8. Example of a correctly classiﬁed test instance: True2.
[20] J. A. Hartigan and M. A. Wong, “Algorithm AS 136: A k-means
clustering algorithm,” Journal of the Royal Statistical Society, Series
C (Applied Statistics), 28(1), pp. 100-108, 1979.
[21] A. Liaw and M. Wiener, “Classiﬁcation and regression by randomFor-
est,” R news, 2(3), pp. 18–22, 2002.
[22] M. Ester, H. P. Kriegel, J. Sander, and X. Xu, “A density-based algorithm
for discovering clusters in large spatial databases with noise,” In Kdd,
Vol. 96, No. 34, pp. 226-231, August, 1996.
Fig. 9. Example of a wrongly classiﬁed test instance: Wrong1.
