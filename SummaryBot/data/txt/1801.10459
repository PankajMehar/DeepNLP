Pretraining Deep Actor-Critic Reinforcement Learning Algorithms
With Expert Demonstrations
Xiaoqin Zhang, Huimin Ma
Dept. of EE, Tsinghua University
xiaoqin-15@mails.tsinghua.edu.cn, mhmpub@tsinghua.edu.cn
Abstract
Pretraining with expert demonstrations have been
found useful in speeding up the training process of
deep reinforcement learning algorithms since less
online simulation data is required. Some people use
supervised learning to speed up the process of fea-
ture learning, others pretrain the policies by imitat-
ing expert demonstrations. However, these meth-
ods are unstable and not suitable for actor-critic
reinforcement learning algorithms. Also, some
existing methods rely on the global optimum as-
sumption, which is not true in most scenarios. In
this paper, we employ expert demonstrations in a
actor-critic reinforcement learning framework, and
meanwhile ensure that the performance is not af-
fected by the fact that expert demonstrations are not
global optimal. We theoretically derive a method
for computing policy gradients and value estima-
tors with only expert demonstrations. Our method
is theoretically plausible for actor-critic reinforce-
ment learning algorithms that pretrains both policy
and value functions. We apply our method to two
of the typical actor-critic reinforcement learning al-
gorithms, DDPG and ACER, and demonstrate with
experiments that our method not only outperforms
the RL algorithms without pretraining process, but
also is more simulation efﬁcient.
1 Introduction
Deep reinforcement learning is a general method that have
been successful in solving complex control problems. Mnih
et al. in [Mnih et al., 2015] combined Q learning with deep
neural networks and proved to be successful in image based
Atari games.
Policy gradient methods have been proved signiﬁcantly ef-
ﬁcient in both continuous control problems ([Sutton et al.,
1999], [Silver et al., 2014], [Heess et al., 2015]) and discrete
control problems ([Silver et al., 2016], [Wang et al., 2016]).
Among policy gradient methods, actor-critic algorithms are at
the heart of many signiﬁcant advances in reinforcement learn-
ing ([Bhatnagar et al., 2009], [Degris et al., 2012], [Lillicrap
et al., 2015], [Mnih et al., 2016]). These algorithms estimate
state-action value functions independently, and proved to be
efﬁcient in policy optimization.
However, an enormous number of online simulation data is
required for deep reinforcement learning. Hence we attempt
to learn from expert demonstrations and decrease the amount
of online data required in deep reinforcement learning algo-
rithms.
One of the representative method of learning from ex-
pert demonstrations is inverse reinforcement learning. Ng
et al. proposed the ﬁrst inverse reinforcement learning algo-
rithm [Ng and Russell, 2000], which recovers reward function
based on the assumption that the expert policy is the global
optimal policy. From recovered reward function, Abbeel et
al. are able to propose apprenticeship learning ([Abbeel and
Ng, 2004]) to train a policy with expert demonstrations and a
simulation environment that does not output reward. Appren-
ticeship learning inspired many similar algorithms ([Syed and
Schapire, 2008], [Syed et al., 2008], [Piot et al., 2014], [Ho
et al., 2016]), Ho et al.
[Ho and Ermon, 2016a] proposed
a imitation learning method that merges inverse reinforce-
ment learning and reinforcement learning, hence imitate the
expert demonstrations with generative adversarial networks
(GANs).
These algorithms proved successful in solving MDP\R
([Abbeel and Ng, 2004]). However, MDP\R is different from
original MDP since MDP\R environments do not output task
based reward data. And for this reason, inverse reinforce-
ment based algorithms attempt to assume the expert demon-
strations to be global optimal and imitate the expert demon-
strations.
In order to learn from expert demonstrations for
MDP, alongside with state-of-the-art reinforcement learning
algorithms, different frameworks are required.
There are some prior work that attempt to make use of ex-
pert demonstrations for reinforcement learning algorithms.
[Lakshminarayanan et al., 2016]
Lakshminarayanan et al.
proposed a training method for DQN based on the assump-
tion that expert demonstrations are global optimal, thus pre-
train the state-action value function estimators.
Cruz Jr et al. [Cruz Jr et al., 2017] focused on feature ex-
tracting for high dimensional, especially image based simu-
lation environments, and proposed a framework for discrete
control problems that pretrains the neural networks with clas-
siﬁcation tasks using supervised learning. The purpose of this
pretraining process is to speed up the training process by try-
ing to extract features of high dimensional states. However,
this work is only suitable for image based, discrete action en-
vironments, and ignored the fact that expert demonstrations
perform better than current learned policies.
The ﬁrst published version of AlphaGo [Silver et al., 2016]
is one of the most important work that pretrains the neural
networks with human expert demonstrations. In this work, a
policy network and a value network is used. The value net-
work is trained with on-policy reinforcement learning, and
the policy network is pretrained with expert demonstrations
using supervised learning, then trained with policy gradient.
This work and [Cruz Jr et al., 2017] are quite similar, the role
of expert demonstrations is to speed up the feature extraction,
and to give policy a warm start. The fact that expert demon-
strations perform better is not fully used, and the framework
is not extensive enough for other problems and other rein-
forcement learning algorithms.
In this paper, we propose an extensive framework that pre-
trains actor-critic reinforcement learning algorithms with ex-
pert demonstrations, and use expert demonstrations for both
policy functions and value estimators. We theoretically derive
a method for computing policy gradient and value estimators
with only expert demonstrations. Experiments show that our
method improves the performance of baseline algorithms on
both continuous control environments and high-dimensional-
state discrete control environments.
2 Background and Preliminaries
In this paper, we deal with an inﬁnite-horizon discounted
Markov Decision Process (MDP), which is deﬁned by the tu-
ple {S, A, P, r, ρ0, γ}. In the tuple, S is a ﬁnite set of states,
A is a ﬁnite set of actions, P : S × A × S → R is the
transition probability distribution, r : S → R is the reward
function, ρ0 : S → R is the probability distribution of initial
state S0, and γ ∈ (0, 1) is the discount factor.
A stochastic policy πs : S × A → R returns the probabil-
ity distribution of actions based on states, and a deterministic
policy πd : S → A returns the action based on states. In this
paper, we deal with both stochastic policies and deterministic
policies, and a ∼ π(s) means a ∼ πs(a|s) or a = πd(s)
respectively. Thus the state-action value function Qπis:
Qπ(st, at) = Est+1,at+1,...
γτ r(st+τ )
The deﬁnitions of the value function V π and the advantage
function Aπ are:
(cid:34) ∞(cid:88)
(cid:34) ∞(cid:88)
τ =0
(cid:35)
(cid:35)
V π(st) = Eat,st+1,...
γτ r(st+τ )
Aπ(st, at) = Qπ(st, at) − V π(st)
τ =0
And let η(π) denote the discounted reward of π:
(cid:35)
(cid:34) ∞(cid:88)
t=0
η(π) = Es0,a0,...
γtr(st)
For future convenience, let dπ(s) denote the limiting dis-
tribution of states:
dπ(s) = lim
t→∞ P r(st = s)
where in all of the deﬁnitions above:
s0 ∼ ρ0(s0), at ∼ π(st), st+1 ∼ P (st+1|st, at)
The goal of actor-critic reinforcement learning algorithms
is to maximize the discounted reward, η(π), to obtain the op-
timal policy, where we use a parameterized policy πθ. While
estimating η(π) or ∇θη(πθ) based on simulated samples,
many algorithms use a state-action value estimator Qw, to
estimate the state-value function Qπ for policy function πθ.
One typical deterministic actor-critic algorithm DDPG
(Deep Deterministic Policy Gradient) [Lillicrap et al., 2015]
uses estimator Qw = ˆQπ to estimate the gradient of
an off-policy deterministic discounted reward ηβ(πθ) =
s∈S dβ(s)V π(s) [Degris et al., 2012], where β is the roll-
(cid:80)
out policy:
∇θηβ(π) ≈ Est∼dβ (s)
= Est∼dβ (s)
(cid:2)∇θQw(s, a)|s=st,a∼πθ(st)
(cid:3)
(cid:2)∇aQw(s, a)|s=st,a∼πθ(st)π(cid:48)
(cid:3)
Where Qw is updated with sampled data from π using Bell-
man equation, π(cid:48)
θ = ∇θπθ(s)|s=st.
Another off-policy algorithm that has Qw as an estimator
of policy πθ is ACER (Actor-Critic with Experience Replay)
[Wang et al., 2016] that optimizes stochastic policy. The
algorithm maximizes off-policy deterministic discounted re-
ward ηβ(πθ) as well, and modiﬁes the off-policy policy gra-
dient ˆgacer = ∇θηβ(π) to:
ˆgacer = ¯ρt∇θ log πθ(at|st)(cid:2)Qret(st, at) − V w(st)(cid:3)
(cid:110)
∇θ log πθ(a|st)Aw(st, a)
+ Ea∼πθ(st)
Where Aw(st, a) = Qw(st, a) − V w(st),
¯ρt =
; [x]+ = x if x > 0 and is zero otherwise;
min
β(a,st) ; st ∼ dβ(st)
V w(st) = Ea∼πθ(st)(st, a); ρt(a) = π(a,st)
and at ∼ β(st); Qret is the Retrace estimator of Qπ [Munos
et al., 2016], which can be expressed recursively as follows:
c, π(at,st)
β(at,st)
(cid:21)
(cid:32)(cid:20) ρt(a) − c
(cid:111)
ρt(a)
(cid:33)
Qret(st, at) = rt + γ ¯ρt+1δQ(st+1, at+1) + γV w(s+1)
where
δQ(st+1, at+1) = Qret(st+1, at+1) − Qw(st+1, at+1)
In ACER, state-action value function is updated using Qret
as target, with gradient gQ:
gQ = (Qret(st, at) − Qw(st, at))∇wQw(st, at)
In this paper, we will apply our methods with expert
demonstrations to DDPG and ACER.
3 Expert Based Pretraining Methods
Suppose there exists an expert policy π∗ that performs better
than π. We deﬁne perform better with the following straight-
forward constraint:
η(π∗) (cid:62) η(π)
(1)
The deﬁnition of perform better above is based on the fact
that the goal of actor-critic RL algorithms is to maximize
η(π). Here the expert policy π∗ is different from that of IRL
[Ng and Russell, 2000], imitation learning [Ho and Ermon,
2016b] or LfD [Hester et al., 2017], since π∗ here is not the
optimum policy of the MDPs.
of (s, a) pairs, {(st, at)}t=0,1,2,..., sampled from π.
Here we deﬁne a demonstration of a policy π as a sequence
Actor-critic RL algorithms tend to optimize η(πθ) as the
target. Thus pretraining procedures for these algorithms need
to estimate η(πθ) as the optimization target using expert
demonstrations. Also, from deﬁnition (1), we need to esti-
mate η(π∗) as well.
However, With only demonstrations of expert policy π∗
and a black-box simulation environment, η(π∗) and η(πθ)
cannot be directly estimated. Hence we introduce Theorem
1 (see [Schulman et al., 2015] and [Kakade and Langford,
2002]).
Theorem 1. For two policies π and π∗:
η(π∗) − η(π) = Es∗
0 ,a∗
0 ,...∼π∗
γtAπ(s∗
t , a∗
t )
(2)
(cid:35)
Proof. (See also [Schulman et al., 2015] and [Kakade and
Langford, 2002]) Note that
Aπ(s, a) = Es(cid:48)∼P (s(cid:48)|s,a) [r(s) + γV π(s(cid:48)) − V π(s)]
(cid:34) ∞(cid:88)
t=0
(cid:35)
we have:
Es∗
0 ,a∗
0 ,...∼π∗
=Es∗
0 ,a∗
0 ,...∼π∗
(cid:34) ∞(cid:88)
(cid:34) ∞(cid:88)
t=0
γtAπ(s∗
t , a∗
t )
γt(r(st) + γV π(st+1) − V π(st))
(cid:35)
(cid:35)
(cid:34) ∞(cid:88)
t=0
γtr(st)
t=0
0∼ρ0 [V π(s0)] + Es∗
0 ,a∗
0 ,...∼π∗
= − Es∗
= − η(π) + η(π∗)
For many actor-critic RL algorithms like DDPG and
ACER, policy optimization is based on accurate estima-
tions of state-action value functions or value functions of the
learned policy πθ. Typically, those algorithms use data sam-
pled from πθ, {(st, at, rt)}t=0,1,2,..., to estimate Qπ and V π.
The estimating processes usually need a large amount of sim-
ulations to be accurate enough.
Combine Theorem 1 with constraint (1), we have:
(cid:34) ∞(cid:88)
t=0
(cid:35)
Es∗
0 ,a∗
0 ,...∼π∗
γtAπ(s∗
t , a∗
t )
(cid:62) 0
(3)
This result links state-action value functions with expert
demonstrations, allowing us to apply constraint (1) while
training state-action value functions. This constraint is for
value estimators, like Qw and V w. When value estimators
are not accurate enough, constraint (3) would not be satis-
ﬁed. Hence if an algorithm update value estimators under
constraint (3), the estimators would be more accurate, and in
result improve the policy optimizing process.
Another pretraining process is policy optimization using
expert demonstrations. Like most actor-critic algorithms, we
suppose advantage function Aπ(s, a) is already known while
conducting policy optimization. Then we can estimate the up-
date step with expert demonstrations and estimations of value
functions.
Considering Theorem 1, we estimate he policy gradient as
the following:
∇θη(πθ)
=∇θ(η(πθ) − η(π∗))
= − ∇θEs∗
0 ,...∼π∗
0 ,a∗
(cid:34) ∞(cid:88)
(cid:35)
γtAπ(s∗
t , a∗
t )
(4)
t=0
Equation (4) provides an off-policy policy optimization
procedure with data only from expert demonstrations. It turns
out that perform better is not a must in this procedure for ex-
pert policy π∗.
Recently, people like to propose sample efﬁcient RL algo-
rithms, like ACER and Q-Prop [Gu et al., 2017], since RL al-
gorithms need a large amount of simulation time while train-
ing. With expert demonstrations, since there is no reward
data, we cannot conduct sample efﬁcient policy optimization
processes. However, when we update policies with (4), no
simulation time is needed. We call the situation simulation ef-
ﬁcient, which means the algorithms may need a large amount
of data, but need few simulation data while training.
Note that sample efﬁcient algorithms are all simulation ef-
ﬁcient algorithms, all of these methods intend to decrease the
simulation time.
In this paper, we evaluate our method by
how simulation efﬁcient it is.
In this section, we found two pretraining methods for actor-
critic RL algorithms, namely (3) and (4). Both of them
are based on Theorem 1. The theorem connects policy dis-
counted reward η(πθ) and expert demonstration data, requir-
ing no reward data from expert trajectories. Equation (3)
gives a constraint of value function estimators based on the
deﬁnition of perform better, and equation (4) provides an off-
policy method to optimize policy function regardless of how
expert demonstrations perform.
4 Algorithms with Expert Demonstrations
Theorem 1 provides a way to satisfy constraint (1) and update
policies πθ with demonstrations of expert policy π∗, and does
not need reward data sampled from π∗. In this section, we
organize the results in Section 3 in a more piratical way, then
we apply the pretraining methods to two of the typical actor-
critic RL algorithms, DDPG and ACER.
These actor-critic RL algorithms use neural networks
Qw(s, a) to estimate the state-action value functions of pol-
icy, Qπ(s, a), where π is the is the current learned policy
while training, which is a parameterized function, πθ, always
in the form of artiﬁcial neural networks.
For pretraining processes based on Theorem 1, we need
an estimator of advantage function for policy πθ, Aπ(s, a).
Based on parameterized policy and state-action value func-
tion estimator Qw, we obtain the advantage function estima-
tor Aw,θ:
Aw,θ(s∗
t , a∗
V w,θ(s∗
t , a∗
t ) = Qw(s∗
t ) = Ea∼πθ(s)Qw(s∗
t ) − V w,θ(s∗
t )
t , a)
(5)
(6)
Figure 1: Example screenshots of MuJoCo simulation environments
that we attend to experiment on with DDPG as baseline. The tasks
are: HalfCheetah (left), Hopper (middle), and Walker2d (right).
Considering the training processes of DDPG and ACER, at
the beginning of the processes the policies are nearly random
and estimators Qw(s, a) are not accurate, since there is little
data from simulation. Therefore if there exist some expert
demonstrations that perform better than initial policies, we
can introduce the data using constraint (3), in order to obtain
a more accurate estimator Qw(s, a).
then Qw(s, a) is accurate
enough for the fact that π∗ performs better. Hence we update
the estimator with expert demonstrations with the following
gradient, in which [x]+ = x if x > 0, otherwise is zero:
If constraint (3) is satisﬁed,
(cid:34)
Q = ∇w
g∗
Es∗
0 ,a∗
0 ,...∼π∗
γtAw,θ(s∗
t , a∗
t )
(cid:34) ∞(cid:88)
t=0
(cid:34) ∞(cid:88)
(cid:35)(cid:35)
(7)
(cid:35)
From equation (4), we optimize policy with expert demon-
strations. Since expert demonstrations do not contain reward
data, we can update policy parameters with a simple policy
gradient:
π = −∇θEs∗
g∗
0 ,a∗
0 ,...∼π∗
γtAw,θ(s∗
t , a∗
t )
(8)
t=0
For the reason that π∗ is not the optimal policy of the
MDPs, we only train with expert demonstrations for a lim-
ited period of time at the beginning of the training process,
to guarantee π∗ performs better than πθ, hence we call the
process pretraining.
ACER, we add gradients g∗
of the algorithms:
To pretrain actor-critic RL algorithms like DDPG and
π to the original gradients
Q and g∗
Q = gQ + λQg∗
gpre
π = gπ + λπg∗
gpre
(9)
(10)
Q and gpre
Where gQ and gπ are original gradients of baseline actor-
critic RL algorithms, andgpre
are pretraining gradi-
ents for estimator Qw and parameterized policy function πθ
respectively while pretraining. We introduce expert demon-
strations to the base algorithms instead of replacing them,
since the state-action value functions are estimated with the
baseline algorithms and gradient g∗
Q only makes Qw satisfy
constraint (1).
4.1 Pretraining DDPG
DDPG is a representative off-policy actor-critic deterministic
RL algorithm. The algorithm is for continuous action space
MDPs, and optimizes the policy using off-policy policy gra-
dient.
Two neural networks are used in DDPG at the same time.
One is named critic network, which is the state-action value
function estimator Qw, and the other is named actor network,
which is the parameterized policy πθ. Since it is an algorithm
for deterministic control, the input of the actor network is a
state of MDPs, and the output is the corresponding action.
Two neural networks are trained simultaneously, with gra-
dients gQ and gπ respectively. gQ is based on Bellman equa-
tion, and gπ is the off-policy policy gradient.
In order to introduce expert demonstrations for pretraining
critic network and actor network, we apply (9) and (10) to
pretrain the two neural networks.
Note that for a deterministic policy πθ, equation (6) be-
comes V w,θ(s) = Qw(s, πθ(s)).
4.2 Pretraining ACER
ACER is an off-policy actor-critic stochastic RL algorithm,
which modiﬁes the policy gradient to make the process sam-
ple efﬁcient. ACER solves both discrete control problems and
continuous control problems.
For discrete control problems, a double-output convolu-
tional neural work (CNN) is used in ACER. One output is
a softmax policy πθ, and the other is Qw values. Although θ
and w share most of the parameters, they are updated sepa-
rately with different gradients.
For stochastic control problems, a new structure named
Stochastic Dueling Networks (SDNs) is used for value func-
(cid:80)n
tion estimation. The network outputs a deterministic value es-
timation V w(s), and a stochastic state-action value estimation
Qw,θ(s, a) ∼ V w(s) + Aw(s, a) − 1
i=1 Aw(s, ˙a)| ˙a∼πθ.
t ) −
Hence equation (5) becomes Aw,θ(s∗
t ) = Qw,θ(s∗
V w(s∗
t ).
t , a∗
t , a∗
In ACER, gradient gπ is the modiﬁed policy gradient, and
gQ is based on Retrace. Both of the gradients are explained
in Section 2.
but in this paper, we compute pretraining gradients g∗
g∗
π directly with expert demonstrations.
Policy gradient is estimated using trust region in ACER,
Q and
Figure 2: Results of pretraining based on DDPG. The ﬁgures each is a different task, and they are respectfully experimented on HalfCheetah
(left), Hopper (middle) and Walker2d (right), The vertical dashed black lines represent the points when pretraining end, and the horizontal
dashed brown lines represent the average episode reward of expert demonstrations. The transparent blue and red lines are original training
results, and the opaque lines are smoothed lines with sliding windows.
5 Experiments
We test our algorithms based on DDPG and ACER on various
environments, in order to investigate how simulation efﬁcient
the pretraining methods are. The baselines are DDPG and
ACER without pretraining.
Because of the existence of [x]+, g∗
Q deﬁned in (7) could
be inﬁnity sometimes. Hence we clip the gradient during pre-
training. We set λQ and λπ = 1 in equations (9) and (10).
The expert policies that generate expert demonstrations are
policies trained with baseline algorithms, i.e. DDPG and
ACER.
With DDPG as baseline, we apply our algorithm to low
dimensional simulation environments using the MuJoCo
physics engine [Todorov et al., 2012], and test on tasks with
action dimensionality are: HalfCheetah (6D), Hopper (3D),
and Walker2d (6D). These tasks are illustrated in Figure 1.
All the setups with DDPG as baseline share the same net-
work architecture that compute policies and estimate value
functions referring to [Lillicrap et al., 2015]. Adam [Kingma
and Ba, 2014] is used for learning parameters and the learn-
ing rate of actor network and critic network are respectively
10−3 and 10−4. For critic network, L2 weight decay of 10−2
is used with γ = 0.99. Both actor network and critic network
have 2 hidden layers with 400 and 300 units respectively.
The results of our pretraining method based on DDPG are
illustrated in Figure 2. In the ﬁgures, the horizontal dashed
brown lines represent the average episode reward of expert
demonstrations. It is obvious that the expert demonstrations
are not global optimal demonstrations, and in order to guar-
antee the expert policies perform better than learned policies,
the pretraining process stops early with 30000 training steps
and 60000 simulation steps.
As shown in Figure 2, it is obvious that DDPG with our
pretraining method outperforms initial DDPG. Results on
HalfCheetah (Figure 2 left) is representative and clear, pre-
training process gives training a warm start, and after pre-
training stops, the performance drops because of the new
learning gradient. However, after pretraining, DDPG learns
faster than the baseline, hence it outperforms initial DDPG.
Although the results of DDPG are unstable on Hopper (Fig-
ure 2 middle) and Walker2d (Figure 2 right), smoothed results
Figure 3: Example screenshots of Atari simulation environments
that we attend to experiment on with ACER as baseline. The tasks
from left to right are: AirRaid, Breakout, Carnival, CrazyClimber
and Gopher.
indicate that DDPG with pretraining processes learns faster
than DDPG.
With ACER as baseline, we apply our algorithm to image
based Atari games. We only tested on discrete control prob-
lems with ACER, and the environments we tested on are: Air-
Raid, Breakout, Carnival, CrazyClimber and Gopher. The en-
vironments are illustrated in Figure 3.
The experiment settings are similar to [Wang et al., 2016],
The double-output network consists of a convolutional layer
with 32 8× 8 kernels with stride 4, a convolutional layer with
64 4 × 4 kernels with stride 2, a convolutional layer with 64
3 × 3 kernels with stride 1, followed by a fully connected
layer with 512 units. The network outputs a softmax policy
and state-action value Q for every action.
Because of the limitation of memory, each thread of ACER
only have a replay memory of 5000 frames, which is the only
different setting from [Wang et al., 2016]. Entropy regular-
ization with weight 0.001 is also adopted, and the discount
factor γ = 0.99, importance weight truncation c = 10. Trust
region updating is used as described in [Wang et al., 2016],
and all the settings of trust region update remain the same.
ACER without trust region update is not tested in this paper.
The results of our pretraining method based on ACER with
trust region update is illustrated in Figure 4. All of the envi-
ronments are image based Atari games. All the lines have the
same meaning as Figure 2, and it is obvious that ACER with
pretraining process outperforms initial ACER.
Unlike DDPG, the performance of learned policies does
not fall after pretraining process ends. This is because for
0246810Training Steps /1050100020003000400050006000RewardDDPGDDPG+Pretrain0246810Training Steps /10505001000150020002500300035000246810Training Steps /10501000200030004000Figure 4: Results of pretraining based on ACER with trust region update. Similar to Figure 2, the vertical dashed black lines are the points
when pretraining end, and the horizontal dashed brown lines are the average episode reward of expert demonstrations. The transparent red
and blue lines represent the original training results, and the opaque ones are smoothed results with sliding windows.
Q deﬁned in (7) is always zero, and g∗
stochastic discrete control, a random policy and a random
state-action value estimator always satisﬁes constraint (1),
hence g∗
π deﬁned in (8)
is policy gradient based on expert demonstrations, similar to
original gπ from baseline ACER, therefore the performance
of learned policies does not fall after pretraining.
Note that learning with expert demonstrations use the same
amount of simulation steps as baseline algorithms, our pre-
training method is more simulation efﬁcient than baselines.
6 Conclusion
In this work, we propose an extensive method that pretrains
actor-critic reinforcement learning methods. Based on The-
orem 1, we design a method that takes advantage of expert
demonstrations. Our method does not rely on the global op-
timal assumption of expert demonstrations, which is one of
the key differences between our method and IRL algorithms.
Our method pretrains policy function and state-action value
estimators simultaneously with gradients (9) and (10). With
experiments based on DDPG and ACER, we demonstrate that
our method outperforms the raw RL algorithms.
One limitation of our framework is that it has to estimate
the advantage function for expert demonstrations, and the
framework is not suitable for algorithms like A3C [Mnih et
al., 2016] and TRPO [Schulman et al., 2015] that only main-
tain a value estimator V w(s). On the other hand, the fact
that expert demonstrations perform better is not considered
during pretraining of policies (Equation (8)). We left these
extensions in our future work.
References
[Abbeel and Ng, 2004] Pieter Abbeel and Andrew Y Ng.
Apprenticeship learning via inverse reinforcement learn-
ing. In Proceedings of the twenty-ﬁrst international con-
ference on Machine learning, page 1. ACM, 2004.
[Bhatnagar et al., 2009] Shalabh Bhatnagar, Richard Sutton,
Mohammad Ghavamzadeh, and Mark Lee. Natural actor-
critic algorithms. Automatica, 45(11), 2009.
[Cruz Jr et al., 2017] Gabriel V Cruz Jr, Yunshu Du, and
Matthew E Taylor.
Pre-training neural networks with
human demonstrations for deep reinforcement learning.
arXiv preprint arXiv:1709.04083, 2017.
[Degris et al., 2012] Thomas Degris, Martha White, and
Richard S Sutton. Off-Policy Actor-Critic.pdf. Icml, 2012.
[Gu et al., 2017] Shixiang Gu, Timothy Lillicrap, Zoubin
Ghahramani, Richard E Turner, and Sergey Levine. Q-
Prop : Sample-Efﬁcient Policy Gradient with An Off -
Policy Critic. ICLR, pages 1–13, 2017.
[Heess et al., 2015] Nicolas Heess, Gregory Wayne, David
Silver, Tim Lillicrap, Tom Erez, and Yuval Tassa. Learn-
ing continuous control policies by stochastic value gradi-
ents. In Advances in Neural Information Processing Sys-
tems, pages 2944–2952, 2015.
[Hester et al., 2017] Todd Hester, Matej Vecerik, Olivier
Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Andrew
0.00.20.40.60.81.0Training Steps /107020406080100RewardAirRaidACER+pretrainACER0.00.20.40.60.81.0Training Steps /107051015202530Breakout0.00.20.40.60.81.0Training Steps /10710152025303540Carnival0.00.20.40.60.81.0Training Steps /1070255075100125150175RewardCrazyClimber0.00.20.40.60.81.0Training Steps /1070100200300400500Gopherence on Machine Learning and Knowledge Discovery in
Databases, pages 549–564. Springer, 2014.
[Schulman et al., 2015] John Schulman, Sergey Levine,
Michael Jordan, and Pieter Abbeel. Trust Region Policy
Optimization. Icml-2015, page 16, 2015.
[Silver et al., 2014] David Silver, Guy Lever, Nicolas Heess,
Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In Proceedings
of the 31st International Conference on Machine Learning
(ICML-14), pages 387–395, 2014.
[Silver et al., 2016] David Silver, Aja Huang, Chris J Maddi-
son, Arthur Guez, Laurent Sifre, George Van Den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Veda Pan-
neershelvam, Marc Lanctot, et al. Mastering the game of
go with deep neural networks and tree search. Nature,
529(7587):484–489, 2016.
[Sutton et al., 1999] Richard S. Sutton, David Mcallester,
Satinder Singh, and Yishay Mansour. Policy Gradient
Methods for Reinforcement Learning with Function Ap-
proximation. Advances in Neural Information Processing
Systems 12, pages 1057–1063, 1999.
[Syed and Schapire, 2008] Umar Syed
and Robert E
Schapire. A game-theoretic approach to apprenticeship
In Advances in neural information processing
learning.
systems, pages 1449–1456, 2008.
[Syed et al., 2008] Umar Syed, Michael Bowling,
and
Robert E Schapire. Apprenticeship learning using linear
In Proceedings of the 25th international
programming.
conference on Machine learning, pages 1032–1039. ACM,
2008.
[Todorov et al., 2012] Emanuel Todorov, Tom Erez, and Yu-
val Tassa. Mujoco: A physics engine for model-based
control. In Intelligent Robots and Systems (IROS), 2012
IEEE/RSJ International Conference on, pages 5026–5033.
IEEE, 2012.
[Wang et al., 2016] Ziyu Wang, Victor Bapst, Nicolas Heess,
Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, and
Nando de Freitas. Sample efﬁcient actor-critic with expe-
rience replay. arXiv preprint arXiv:1611.01224, 2016.
Sendonaris, Gabriel Dulac-Arnold, Ian Osband, John Aga-
piou, et al. Learning from demonstrations for real world
reinforcement learning. arXiv preprint arXiv:1704.03732,
2017.
[Ho and Ermon, 2016a] Jonathan Ho and Stefano Ermon.
In Advances
Generative adversarial imitation learning.
in Neural Information Processing Systems, pages 4565–
4573, 2016.
[Ho and Ermon, 2016b] Jonathan Ho and Stefano Ermon.
Generative Adversarial Imitation Learning. In Nips, pages
4565–4573, 2016.
[Ho et al., 2016] Jonathan Ho, Jayesh Gupta, and Stefano
Ermon. Model-free imitation learning with policy opti-
mization. In International Conference on Machine Learn-
ing, pages 2760–2769, 2016.
[Kakade and Langford, 2002] Sham Kakade and John Lang-
ford. Approximately Optimal Approximate Reinforce-
ment Learning. Proceedings of the 19th International
Conference on Machine Learning, pages 267–274, 2002.
[Kingma and Ba, 2014] Diederik Kingma and Jimmy Ba.
arXiv
Adam: A method for stochastic optimization.
preprint arXiv:1412.6980, 2014.
[Lakshminarayanan et al., 2016] Aravind
Lakshmi-
narayanan, Sherjil Ozair, and Yoshua Bengio. Reinforce-
ment Learning with Few Expert Demonstrations. Neural
Information Processing Systems - Workshop on Deep
Learning for Action and Interaction, 2016.
[Lillicrap et al., 2015] Timothy P. Lillicrap,
Jonathan J.
Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yu-
val Tassa, David Silver, and Daan Wierstra. Continuous
control with deep reinforcement learning. arXiv preprint
arXiv:1509.02971, pages 1–14, 2015.
[Mnih et al., 2015] Volodymyr Mnih, Koray Kavukcuoglu,
David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidje-
land, Georg Ostrovski, et al. Human-level control through
deep reinforcement learning. Nature, 518(7540):529–533,
2015.
[Mnih et al., 2016] Volodymyr Mnih, Adria Puigdomenech
Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asyn-
chronous methods for deep reinforcement learning.
In
International Conference on Machine Learning, pages
1928–1937, 2016.
[Munos et al., 2016] R´emi Munos, Tom Stepleton, Anna
Harutyunyan, and Marc G. Bellemare. Safe and Efﬁcient
Off-Policy Reinforcement Learning. arXiv, (Nips), 2016.
[Ng and Russell, 2000] Andrew Ng and Stuart Russell. Al-
gorithms for inverse reinforcement learning. Proceedings
of the Seventeenth International Conference on Machine
Learning, 0:663–670, 2000.
[Piot et al., 2014] Bilal Piot, Matthieu Geist, and Olivier
Pietquin. Boosted bellman residual minimization han-
In Joint European Confer-
dling expert demonstrations.
