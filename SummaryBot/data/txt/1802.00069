Leveraging Adiabatic Quantum Computation for Election
Forecasting
Maxwell Henderson1, John Novak2, and Tristan Cook1
1QxBranch
2Standard Cognition
Abstract
Accurate, reliable sampling from fully-connected
graphs with arbitrary correlations is a diﬃcult prob-
lem. Such sampling requires knowledge of the prob-
abilities of observing every possible state of a graph.
As graph size grows, the number of model states be-
comes intractably large and eﬃcient computation re-
quires full sampling be replaced with heuristics and
algorithms that are only approximations of full sam-
pling. This work investigates the potential impact
of adiabatic quantum computation for sampling pur-
poses, building on recent successes training Boltz-
mann machines using a quantum device. We inves-
tigate the use case of quantum computation to train
Boltzmann machines for predicting the 2016 Presi-
dential election.
Introduction
As the results of the 2016 US Presidential Election
were ﬁnalized, it was clear that the majority of pro-
fessional polling groups, many of whom had pre-
dicted the probability of a Clinton victory to be well
over 90%, were had signiﬁcantly overestimated their
predictions ([1, 2, 3])). While it could be argued that
the underlying models were correct and that the par-
ticular result was just a very rare event, post-mortem
analyses have revealed ﬂaws that led to large pre-
diction biases. According to multiple post-election
analyses, it was concluded that a leading cause of er-
ror in the majority of election forecasting models was
a lack of correlation between individual states pre-
dictions ([4, 5, 6, 7]). Uncorrelated models, though
much simpler to build and train, cannot capture the
more complex behavior of a fully-connected system.
To capture these higher-order relationships, a fully-
connected graphical model would be ideal. While
these models are more powerful, practical roadblocks
have prevented their widespread adoption due to dif-
ﬁculties in implementation using classical computa-
tion. However, recent studies have shown that quan-
tum computing is a competitive alternative when
generating such networks ([8, 9, 10, 11, 12]).
Quantum machine learning (QML) is a blossom-
ing ﬁeld. As summarized in the comprehensive re-
view of QML in [13], machine learning applications
from support vector machines to principal compo-
nent analysis are being reimagined on various quan-
tum devices. One of the most exciting research ar-
eas within QML is deep quantum learning, which fo-
cuses on the impact quantum devices and algorithms
can have on classical deep neural networks (DNNs)
and graphical models. A particular class of DNNs is
the Boltzmann machine (BM), which is an incred-
ibly powerful fully-connected graphical model that
can be trained to learn arbitrary probability distri-
butions. A downside of these networks is that BMs
are incredibly costly to train, a fact that has limited
their practical application. This large computational
training cost has drawn attention to the implemen-
tation of quantum computation to help train such
networks. BMs realized on quantum devices (partic-
ularly adiabatic quantum devices such as those pro-
duced by D-Wave Systems ([9]) may possess inherent
beneﬁts compared to those realized on classical de-
vices. Research groups have realized various forms
of BMs (fully-connected BM, restricted Boltzmann
machines (RBMs), and Helmholtz machines) trained
using quantum computation, and this research has
shown quantum computation can be used to eﬀec-
tively train neural networks for image recognition
tasks ([9, 11, 12]).
In this work, we will leverage the power of adia-
batic quantum computation to eﬃciently train fully-
connected BMs for the novel purpose of election
modeling. Additionally, we have systematically ex-
plored a number of the assumptions underlying the
approach of using adiabatic quantum computers
(AQC) to model BMs, and we have demonstrated
that for most systems of interest (such as this one)
the approach does appear to be valid. We believe the
methods proposed in this paper could bring an in-
teresting new factor into the conversation of election
forecasting at large, one in which quantum compu-
tation could play a future role.
2 Methodology
2.1 Modeling Boltzmann Machines
with AQC
In this work, we will be generating fully-connected
BMs trained using a D-Wave 2X 1152 qubit quan-
tum device using the general method described in
[9]. While the methodology for training both RBMs
and BMs using a D-Wave machine have been laid
out in previous papers ([9, 11]), we will brieﬂy re-
view the logic and methodology here.
A BM is a fully-connected graph of (N ) binary
units (neurons). These neurons can be either “visi-
ble” (directly model some aspect of a data distribu-
tion) or “hidden” (not tied to any particular aspect
of the data distribution and used only for captur-
ing features from the data distribution). Each net-
work has 2N possible states, and the probability of
sampling a particular state s = (s1, ..., sN ) from the
model is
p(s) =
(1)
e−E(s)
wherein Z it the well-known partition function and
E is an energy function deﬁned as
E(s) = −(cid:88)
bisi − (cid:88)
si∈s
si,sj∈s
Wijsisj,
(2)
wherein b represent the linear “bias” on each unit
and W represents the “weight” of the coupling be-
tween two units (b and W will be referred to as our
“model parameters”). To properly train the net-
work, we need to adjust the model parameters so
that the model distribution produced by repeatedly
sampling the model is as close as possible to the un-
derlying data distribution; more precisely, we want
to maximize the log-likelihood, L, of the data dis-
tribution. To calculate the model parameters for
maximizing L, we use the familiar gradient descent
method and learning rate η to get model parameter
update equations
(3)
∆Wij =
(cid:17)
(cid:16)(cid:104)sisj(cid:105)D − (cid:104)sisj(cid:105)M
(cid:16)(cid:104)si(cid:105)D − (cid:104)si(cid:105)M
(cid:17)
∆bi =
(4)
In equations 3 and 4, the values inside (cid:104)∗(cid:105) represent
expectation values over the data (D) and model (M )
distributions. The model would be perfectly trained
if ﬁrst ((cid:104)si(cid:105)) and second ((cid:104)sisj(cid:105)) order moments were
identical for both the data and model distribution.
To properly adjust the model parameters we need
to calculate expectation values over the model it-
self. Getting the “true” values would thus require a
calculation for all 2N possible states of the model,
which is clearly intractable as the system size in-
creases. These particular calculations are where the
use of quantum computation is ideal, and we see a
potential for a speedup in our overall training algo-
rithm.
The quantum devices produced by D-Wave Sys-
In
tems perform a quantum annealing algorithm.
theory, this algorithm can leverage quantum eﬀects
to take an initial quantum system that is in a well-
known ground state and transform this into a ﬁnal
Hamiltonian - one in which the system should still
be in the ground state (assuming the annealing pro-
cess was slow enough, as well as many other factors
discussed elsewhere [14]). The original use case of
this algorithm lies in the fact that if you can prop-
erly map a computationally diﬃcult problem of in-
terest into this ﬁnal Hamiltonian, then measuring
the ground state of this ﬁnal Hamiltonian should
produce the optimal solution to the original prob-
lem. However, this use case has been elusive at scale;
as shown in the research of [15], which focuses on
fundamental limitations of quantum devices at ﬁ-
nite temperatures. Even taking some optimistic as-
sumptions (such as perfect, instant thermalization),
as the system (problem) size grows, the probabil-
ity of measuring the optimal (ground) state of the
system decreases exponentially. Rather than return-
ing the ground state solution, repeatedly measuring
from such a device returns a Boltzmann distribution
of energies.
While these results prove challenging for using
such hardware for optimization, it presents an ideal
opportunity for training BMs. At a high level, in-
stead of trying to calculate (cid:104)si(cid:105)M and (cid:104)sisj(cid:105)M di-
rectly, we can instead map our network onto the D-
Wave quantum device. By obtaining a ﬁnite num-
ber of samples from the hardware device, the goal
is to generate better approximations of (cid:104)si(cid:105)M and
(cid:104)sisj(cid:105)M than classical heuristics. This method seems
all the more natural as the form of the Hamiltonian
H of the D-Wave device is
H(S) = − (cid:88)
hiSi − (cid:88)
JijSiSj,
(5)
Si∈S
Si,Sj∈S
which is the same functional form as the BM en-
ergy in equation 2. In this equation, S is the vector
of qubit spin states, hi are the bias terms on each
qubit, and Jij are the (anti)ferromagnetic couplings
between the qubits. By mapping the model param-
eters of a BM to the hardware parameters of a D-
Wave device and making a set of measurements of
the device, one can use these measurements to con-
struct approximations of (cid:104)si(cid:105)M and (cid:104)sisj(cid:105)M . Ad-
vantages have been shown in using fully-connected
BM on QC devices because using the methods of
[9], the eﬀective temperature of the device does not
have to be taken into account. Equation 1 is a spe-
cial case of the more general representation; rather
than raising the exponential to −E(s), the more gen-
eral expression raises its to −E(s)β, where β is the
“eﬀective” temperature of the system (parameter re-
lated to temperature of the system). If β = 1 then
we arrive at equation 1, but in general when using
a quantum device one will not know the eﬀective
temperature beforehand, which can experience large
ﬂuctuations between measurements. While this can
be problematic for training RBMs using quantum
annealers, and requires diﬀerent techniques to esti-
mate this parameter ([10, 8, 11]), fully connected
BMs do not require these additional calculations for
eﬀective training ([9]).
Though the structure of the BM graph to embed
on the device is fully connected, we are in practice
limited by the graph structure physically realized
in the hardware. The adiabatic quantum device we
used for this research was a D-Wave 2X, which has
1,152 qubits connected in a Chimera graph archi-
tecture consisting of 8 qubit cells arranged as K4,4
bipartite graphs. The qubits within each cells are
cross connected, and each cell is connected to four
adjacent cells (with the exception of cells along the
boundaries) as shown in Figure 1. To properly map
the BM energy function of (2) to the device, the
graph minor-embedding problem must be solved; we
need a hardware embedding which uses a chain of
multiple physical qubits to realize a single logical
qubit in the problem Hamiltonian of (5). Using the
same method as [9], we ﬁnd embeddings using the
embedding heuristic provided by D-Wave’s API and
resolve discrepancies of the qubit chains using ma-
jority vote (a post-processing step of the measure-
ments).
Figure 1: Four bipartite cells of a Chimera graph
architecture showing how the cells interconnect. In
each cell there are four horizontal and four verti-
cal qubits, colored alternately blue and burnt orange.
Within cells, where two qubits overlap they are cou-
pled by means of a Josephson junction, indicated by
green circles. Each qubit can be coupled to two ad-
dtional qubits from adjacent cells, also by means of
Josephson junctions, indicated by light green circles.
2.2 Quantum Boltzmann Machines
for Forecasting Elections
The methodology outlined in section 2.1 lays out
our approach for training fully-connected BM using
a D-Wave quantum device. This section will detail
our procedure for implementing these networks to
forecast elections.
In this research, we study the
US Presidential election, and each binary unit in
the BM represents a single US state. The winner
of a particular election simulation is determined by
the candidate with the most electoral college votes.
Each US state has a particular number of electoral
college votes to award to a candidate (2 + an integer
which scales as a function of the state’s population),
and these votes are awarded entirely to one candi-
date (winner-take-all). We assert that each sample
returned from a fully-connected BM will in eﬀect
be a simulation of a US presidential election. Each
sample from the BM returns a binary vector, where
each entry in the vector corresponds to the vot-
ing results of a particular US state. These individ-
ual state voting results are mapped to a particular
candidate/party (i.e., 1 = Democrat, 0 = Republi-
can). To determine the election simulation outcome,
we weight each of these US state outcomes accord-
ing to their net weight in the national vote (each
state’s electoral votes). The winner of each simu-
lation (sample) is determined by the sum of each
party’s overall national vote, which is calculated us-
ing the binary results (from sample) and national
weight (electoral votes) for each state.
The goal is to train the BM which is being sam-
pled from so that the ﬁrst and second order mo-
ment terms of the model distribution approach those
of the data distribution. This training process has
already been discussed in section 2.1, and in this
section we will expand on how we determined the
ﬁrst and second order moment terms for the data
distribution of our election model. The ﬁrst order
moment terms represent the probability that each
state will vote for a particular candidate. As an
example, if we believe that there is a 80% chance
that the Democratic candidate wins Colorado, then
the ﬁrst order moment for the binary variable as-
signed to represent Colorado should be equal to 0.8.
To determine all the ﬁrst order moments for each
state in our model, we use the current time-averaged
polling results made publicly available on FiveThir-
tyEight ([16]). We obtain a projected vote share
for both candidates for each day that data is avail-
able (6 months before, and including, November 8th
2016). These projected vote shares are then used
as input to a sigmoidal model (same model used
by FiveThirtyEight [17]) which rightly assumes that
elections are stochastic, and that the result for each
state/country follows a probabilistic rather than de-
terministic curve based on the popular-vote / pro-
jected vote share margin. This method for convert-
ing a popular-vote margin to a probability of victory
is shown for a particular state in Figure 2.
Figure 2: Model for interpreting projected vote share
to probabilities. A. Plot of Maine’s polling projec-
tions over time, where the solid lines are the time-
averaged projected vote share for both candidates,
and the dashed lines are the resulting probabilities
of victory for each candidate, calculated using the
best ﬁt function shown in B.
Given the underlying projected vote shares for
each state and the best ﬁt function shown in Fig-
ure 2B, calculating the ﬁrst order moment terms
for each state is straightforward. Calculating the
second order terms, the eﬀective “correlations” be-
tween states, is much more diﬃcult. These correla-
tions express the likelihood that two states will end
up with the same (or diﬀerent) voting result in an
election. States that vote the same are more cor-
related (higher second order moment), and states
that don’t have a lower correlation (lower second or-
der moment). These correlations are inﬂuenced by a
plethora of demographic (race, age, education), geo-
graphic, and additional factors. Professional model-
ers (such as those at FiveThirtyEight) have com-
plex methodologies for determining these correla-
tions; however, a rigorous analysis of these corre-
lations is outside the scope of this particular work.
We used data obtained publicly, which is suﬃcient
to validate the general approach of our model.
To calculate the second order moment terms, we
use one source of data and make two particular
ansatz. The data source we use is presidential elec-
tion results from the last 11 US Presidential elec-
tions. This data contains the date and results per
state for each election. Our ﬁrst ansatz is that if we
consider two states, these states should have higher
correlations (second order terms) if they had voted
similarly in previous elections. This correlation is
agnostic towards which candidate was voted for in
each of these previous elections; the only important
factor for the two states in question is if the vote
was for the same candidate or a diﬀerent one. The
second ansatz is that in terms of weighting previ-
ous election results, more recent elections are more
relevant. This means recent elections increase corre-
lations between two states more than those that hap-
pened longer ago. We assume a linear relationship
between time and importance. The raw correlations
(cid:104)sisj(cid:105)Draw between states i and j are calculated as
follows:
n=1:11 n(cid:0)2injn − in − jn + 1(cid:1)
(cid:80)
(cid:104)sisj(cid:105)Draw =
(cid:80)
n=1:11 n
(6)
wherein n refers to a particular election year in the
set [1968, 1972, ... , 2008, 2012] (higher n is more re-
cent) and in and jn are the results for election n for
both respective states. We then enforce a hard con-
straint that second order correlations should never
contradict ﬁrst order moments (which are calculated
directly from current polling data). This is accom-
plished by ﬁnally calculating the second order mo-
ments between states i and j as
(cid:104)sisj(cid:105)D = (cid:104)sisj(cid:105)Draw min((cid:104)si(cid:105)D,(cid:104)sj(cid:105)D).
(7)
We have now a methodology for mapping election
forecasting models, speciﬁcally the 2016 US Pres-
idential election, to BM by deﬁning mathematical
models for calculating both ﬁrst and second order
data distribution terms.
In the following section,
we validate that this approach holds true for small,
nonexistent countries and then attempt to simulate
a “real time” forecast for 2016 Presidential election
using quantum-trained BMs.
2.3 Caveats and limitations
In section 2.1 and 2.2, we reviewed the methodology
for training fully-connected BM with a D-Wave ma-
chine as well as describe our approach for mapping
election forecast models to the (to be trained) BM.
While this work uses this approach as described, a
few caveats and limitations deserve some additional
attention here.
2.3.1 Hardware constraints
The hardware size limitations of the D-Wave 2X does
not allow us to fully embed a 50 state model as well
as the DC province, which are the 51 fundamen-
tal voting blocks for the US Presidential election.
Using the virtual full-yield Chimera capability of-
fered by D-Wave, which uses a combination of the
quantum device hardware in tandem with classical
software for simulating missing qubits and couplers,
we were able to embed 49 states and omitted DC
and Maryland. These were omitted because they
were ranked as the most “deﬁnite” by model stan-
dards (both were approaching 100% likelihood to
vote Democrat), as well as geographically adjacent.
2.3.2 Assert that all states are winner-take-
all
While the US Presidential election is winner-take-
all at the state level, two states are exceptions to
this rule: Maine and Nebraska. Instead of winner-
take-all, these states award delegates by district. To
simplify the model and ﬁt within the hardware con-
straints, we treat these states as winner-take-all re-
gardless. This decision was made for three reasons.
First, the primary purpose of this paper is to val-
idate the overall election methodology for model-
ing such elections using QC-trained neural networks;
such state speciﬁc rules fall outside this scope of
this work. Second, these states have small weight
(electoral college votes) in the broader election, so
treating them as winner-take-all has a reduced ef-
fect compared to a much larger state under the same
voting system. Third, in the future we could treat
the provinces as individual states themselves, each
awarding electoral college votes with a winner-take-
all system. However, due to our limitation already
expressed in the previous issue, this experiment will
be left to future studies on a larger quantum device.
2.3.3
Inability to model national errors in
same model
The strength in models with correlations as de-
scribed here is simple; they can account for a form
of error that is inaccessible to independent models.
However, there are also two other primary types of
error that we would want our ﬁnal model to consider:
national and state-speciﬁc errors. Both of these er-
ror arise from the fact that polling is never perfect;
there are always voting blocks that are under or over-
represented based on the types of people that are
both polled and respond to the poll. National error
arises from the fact that all states could have system-
atically missed a particular type of voting block in a
similar, characteristic manner. This leads to errors
that aﬀect each state in a similar way. State-speciﬁc
error is the same concept, but on a state-by-state
level. The latter (as discussed in future results sec-
tion) can be addressed naturally by the nature of the
QC-training algorithm; however the former cannot.
Since we wish to emulate the best possible model, in-
corporating state-speciﬁc, state, and national errors,
we choose to create a meta-model which aggregates
results from several diﬀerent models build on the as-
sumption of diﬀerent national error. In our case, we
take 25 equally-spaced samples from a t-distribution
with 10 degrees of freedom; this is the same distri-
bution and degrees of freedom used for national and
state-speciﬁc error used by FiveThirtyEight ([16]).
These points deﬁne the national errors we use to
train 25 diﬀerent models. For instance, one national
model may have a national error favoring Clinton
by 1% point, while another might favor Trump by
1%. These national errors are ultimately incorpo-
rated into the ﬁrst order moment terms for each
state, leading to models which are slightly biased
towards either candidate to a relative degree. The
average of these 25 models is calculated after simu-
lating each model independently, and weighing the
results of each by the probability of occurrence for
each national error.
2.3.4 Limited time windows of D-Wave ac-
cess
In a production environment, it would be ideal to
produce updates to forecasts daily (or sometimes
several times a day) for particularly high-proﬁle elec-
tions. These updates occur as new polls come in,
changing the particular predictions for each state,
and thus ultimately the national results. Applying
our proposed methodology could assuredly be used
for these purposes, but a limiting factor for simu-
lating this daily forecast over 6 months is access to
the D-Wave quantum device. Due to limited access
time to run experiments on the D-Wave device, and
that we have to simulate multiple error models (as
explained in section 2.3.3), we choose to only model
every 2 weeks of data rather than daily. This allowed
us to generate an appropriate number of simulations
for these days across all national error models.
3 Results
3.1 Eﬀect of Chain Length
As mentioned in section 2.1, using a well posed
Hamiltonian and the right environmental variables,
an AQC should theoretically be capable of ﬁnding
the ground state of the Hamiltonian.
In practice,
thermal ﬂuctuations, environmental interactions, in-
suﬃciently short annealing times, and a plethora
of other physical and engineering challenges result
in a low probability of measuring the ground state,
but instead some other low energy (potentially near-
optimal) state. This is especially true for larger
Hamiltonian systems, as shown in [15]; for ﬁnite-
temperature AQCs, as the system size increases, the
probability of measuring a non-optimal low energy
state approaches 1.
In contrast, if we wish to use
an AQC as a sampling engine for sampling from
BMs, we can potentially face a diﬀerent set of ob-
stacles when using small embeddings (system sizes).
In terms of using an AQC for machine learning pur-
poses, returning a distribution of low energy solu-
tions rather than the optimal conﬁguration drives
the learning process, as the ﬁrst and second order
statistics of these measurements determines the up-
date terms for the model. At small physical em-
bedding size, the probability of measuring the op-
timal state increases signiﬁcantly, and at very con-
cise embedding sizes the probability of measuring
the ground state energy can approach 1. This be-
havior is that of a Hopﬁeld network, which is a
BM at T = 0. Unlike a BM, the Hopﬁeld net-
work can only return ground state energy solutions.
This would imply that our training algorithm as de-
scribed in section 2 would not work for such a sys-
tem. Each time model updates in equations 3 and 4
are made, the energy function of equation 2 changes
as well. This new energy function would lead to
new ground state solutions, which in turn could have
completely diﬀerent model parameters. While train-
ing a BM leads to model updates “smoothly” guid-
ing the model parameters ((cid:104)si(cid:105)M , (cid:104)sisj(cid:105)M ) towards
the data distribution ((cid:104)si(cid:105)D, (cid:104)sisj(cid:105)D), slight changes
in the model parameters of a Hopﬁeld network can
completely change the ground state solutions, lead-
ing to chaotic model parameter updates.
Figure 3: Training results for arbitrary Boltzmann
machines realized on D-Wave device using (A) 1x
and (B) 2x embedding qubit embedding chains. In
each subplot, the horizontal red lines are the respec-
tive target values.
One potential way to mitigate these eﬀects is to
deliberately increase the size of the qubit chains for
embedding the problem. For optimization purposes,
the goal would be to ﬁnd the minimum chain length
for embedding the problem Hamiltonian onto the
physical device. By keeping the embedding chains
minimal, the system size is as small as possible which
increases the chance of measuring an optimal ground
state. The opposite should be true as well:
the
more we increase the chain lengths for embedding
the logical qubits onto the hardware, the more low
energy states become available to system, increasing
the probability that the system will transition away
from the ground state during the annealing process.
This should enable one to properly train BM for any
number of nodes, given that the qubit chain lengths
are long enough. By validating this assertion, we
can argue that our approach here for using AQC
for realizing BMs for election modeling could be ap-
plied to any sized system, as well as validate that our
particular experiments are in a regime where proper
learning is possible.
To test this hypothesis, we performed experiments
with fully connected graphs of size 5 through 9, em-
bedded with three diﬀerent embeddings of various
chain lengths, and studied how well we could train
the systems to reproduce activation probability dis-
tributions deﬁned by graphs with arbitrary ﬁrst and
second order terms. For each run, the activation (hi)
and correlation (Ji,j) probabilities were selected ran-
domly such that for node i the activation probability
hi ∈ (0, 1) and for two nodes i and j, the correlation
probability Ji,j = ci,jhihj where ci,j is the correla-
tion strength and ci,j ∈ (0, 1).
Three embeddings were used for each graph: a
maximally concise embedding, an embedding de-
rived from a maximally concise graph of twice as
many nodes (denoted by “2x”), and an embedding
derived from a maximally concise graph of three
times as many nodes (denoted by “3x”). The de-
cision to approach the problem in this way was done
because the D-Wave API has been set up for op-
timization problems, and as such the hardware em-
bedding functions in general attempt to return max-
imally concise embeddings. The 2x and 3x embed-
dings returned from the API were for graphs of 2x
and 3x the size of the problem graph, so they were
reduced to the correct size by joining the physical
qubits representing pairs (in the case of 2x) and
triples (in the case of 3x) of logical qubits (usually
represented by chains of physical qubits) into single
logical qubits of chains of physical qubits 2x and 3x
times as long as in the original embedding. An ex-
ample of training by the shortest (1x) and medium
(2x) chain lengths are shown in Figure 3.
For all the subgraphs of Figure 3, the x-axis of
each graph is the number of completed iterations in
the training algorithm while the y-axis is the acti-
vation probability when sampling the graph multi-
ple times. The graphs on the diagonal are single
node activation probabilities(ﬁrst order moments)
and the oﬀ-diagonal graphs are the two node cor-
relations (second order moments).
In Figure 3A,
the activation probabilities fail to converge to the
desired values, indicating that the qubit chains are
not allowing suﬃcient degrees of freedom for the sys-
tem to model a Boltzmann machine. However, using
the same network but with the 2x embedding qubit
chains, the network was able to converge over time
towards the target ﬁrst and second order moment
values. In Table 1, we show the root mean squared
error (RMSE) for training iterations 191-200 for dif-
ferent QC-trained networks at diﬀerent embedding
chain lengths.
Nodes Chain
1x
2x
3x
1x
2x
3x
RMSE
0.437 ± 0.072
0.106 ± 0.036
0.060 ± 0.038
0.149 ± 0.101
0.038 ± 0.028
0.045 ± 0.034
Table 1: RMSE for QC-trained networks at diﬀer-
ent embedding chain lengths. As the networks grow
larger, the chain length diﬀerences grows more negli-
gible as chains are naturally getting longer to satisfy
the embedding.
Given the current D-Wave qubit connectivity
graph, as the problem size grows larger, the average
embedding chain length similarly grows. As most
studies embed as large a problem as possible onto
the device, this has naturally led to longer chain
lengths in previous research. As future hardware
improvements are made and shorter qubit chains are
feasible (through increased connectivity), it may be-
come important to validate that the individual log-
ical qubits are properly learning the respective tar-
get terms. The lengthening technique shown here
could provide a simple but eﬃcient tool for ensuring
Boltzmann-like behavior for all nodes in the logical
graph without having to perturb any of the individ-
ual energy scalings.
3.2 Modeling the Presidential Elec-
tion
The primary experiment we conducted was to at-
tempt to simulate a “real-time election model fore-
cast” using QC-trained Boltzmann machines. Start-
ing on the date 2016-06-08 and continuing until elec-
tion day 2016-11-08, we trained multiple fully con-
nected Boltzmann machines using the D-Wave adi-
abatic device.
Figure 4: Summed error as a function of training
iterations for one national error model. The small
spikes of error occur deep into the training process
are simply an artifact of the updates in the ﬁrst or-
der moments that happen at 2 week (25 iteration)
intervals.
As mentioned in section 2.3.3 and 2.3.4, due to
limitations we retrained the network every two weeks
rather than daily, and used 25 diﬀerent networks to
model diﬀerent national errors (derived from a t-
distribution with 10 degrees of freedom). The net-
works starting on 2016-06-08 were initialized with
small, random coeﬃcients and then subsequently
trained for 150 iterations each. Then, at each 2 week
interval, the ﬁrst order moment terms were updated
and trained for an additional 25 iterations. The
changes to the ﬁrst order moments were small, so
fewer training iterations were necessary to converge
to a stable summed error (sum of squared ﬁrst and
second moment errors) across the networks. This
led to 400 total training iterations per national er-
ror (150 for 2016-06-08 + 25*10 for the next ten
Figure 5: Comparing 2016 Presidential election forecasting results from QC-trained methodology to those
of FiveThirtyEight. QC-trained networks each had a national bias towards Clinton (CB), Trump (TB), or
neither candidate.
two-week updates). An example of the training er-
ror for a particular national error model is shown in
Figure 4.
Knowing from section 3.1 that our qubit chains
are suﬃciently long enough to learn properly, the
training error results of Figure 4 are to be expected.
Similar plots were observed across all national error
models, as this translates into nothing more than
scaling the ﬁrst order moment terms. We can then
take the samples from these networks at diﬀerent
iterations as our election forecasting simulation re-
sults. We choose to take samples for the last 10
iterations of each forecasting date (this would be it-
erations 141-150 for 2016-06-08 and 16-25 for the
next 10 forecasting dates). This allows us to sam-
ple from the network once it has reached a general
steady-state in terms of summed training error. As
discussed in section 2.2, each logical qubit is mapped
to a particular state and each sample is equivalent to
an election forecast. To determine which candidate
“won” a particular sample, we simply map the qubit
results back to the particular state it represents, and
add each state’s number of electoral votes to the can-
didate that state voted for in the sample. Since the
democratic candidate was the heavy favorite in most
election models, we choose to express our forecast-
ing results in terms of the probability of a Clinton
victory. In this way, each sample results in a partic-
ular candidate winning (270 electoral votes or more)
or losing (we combined ties into this category for
simplicity, although an exact tie is very unlikely).
For our experiments, we took 1,000 samples from
the D-Wave device at every iteration for each na-
tional error model. This gave us 10,000 samples for
each national error model for each forecasting date
(10 training iterations, 1,000 samples per training
iteration). The probability of a Clinton victory for
each national error model was simply the sum of
the individual samples which were won by Clinton
divided by the number of total samples (10,000 in
our case, per national error model and time step).
Finally, to get an average election forecast as a func-
tion of time (shown in Figure 5), we calculated the
weighted arithmetic mean across all national error
models for each forecasting date. The weights for
each national error were deﬁned as the t-distribution
probability density function of each national error
(t-distribution with 10 degrees of freedom).
As evidenced in Figure 5, the QC-trained network
results followed trends similar to the trends of the
professional FiveThirtyEight forecasts. The overall
probabilities of the diﬀerent national error networks
also follows naturally; networks that had a national
error in favor of Clinton increased the probability
of a Clinton victory, and networks with a national
error in favor of Trump decreased the probability
of a Clinton victory. The largest apparent diﬀer-
ence between the QC-trained models was the overall
probability of a Clinton victory. While the Aver-
age result line of the QC-trained networks follows a
very similar pattern to the predictions of FiveThir-
tyEight, the QC-trained results are almost uniformly
20% lower. This result in no way says the quan-
tum methodology is “better”, but rather highlights
the diﬀerences in the overall approach. It is likely
that these results are mostly dependent on the un-
derlying diﬀerences in how we calculated our sec-
ond order moments terms between the states. An
interesting future study would be to replicate the
quantum-training protocol described here but using
second order moments driven by demographic data
of the individual state inhabitants.
An important factor forecasters also desire from
a forecasting model is to know which states are the
most important for predicting a particular outcome.
A straightforward approach is to generate a vector
for each state (1 = state voted Democrat, 0 = state
voted Republican) and a similar vector for the out-
come (1 = Democratic victory, 0 = Republican vic-
tory) of each simulation for the date November 8,
2016. Then, we can calculate the Pearson correla-
tion coeﬃcient between the two vectors and take the
absolute value of these correlations. Table 2 shows
the 10 states with the highest and lowest correlation
coeﬃcients. As expected, states that leaned heavily
Democratic or Republican had very low correlation
coeﬃcients; regardless of the outcome of the election,
states like Illinois and Nebraska were virtual locks for
the Democratic and Republican candidates, respec-
tively. Similarly, the states with the highest corre-
lation coeﬃcients contained many of the most con-
tested states in the election. FiveThirtyEight’s fore-
casts have a similar “tipping-point chance” metric
which they deﬁne as “the probability that a state will
provide the decisive vote in the Electoral College”
([16]). On election day, 7 out of the 10 states they
ranked as the highest tipping-point chance states
were similarly in the list of 10 most correlated states
in Table 2 (the diﬀerences: FiveThirtyEight included
Virginia, Minnesota, and Wisconsin, while ours in-
cluded New Hampshire, Iowa, and Arizona).
Finally, we take into consideration the individual
state errors observed in our QC-trained models. As
mentioned previously, modelers (such as FiveThir-
tyEight) will apply some degree of noise for individ-
State
Ohio
Florida
Nevada
New Hampshire
Pennsylvania
Iowa
Michigan
North Carolina
Colorado
Arizona
Illinois
Nebraska
Alabama
Oklahoma
California
West Virginia
Delaware
Oregon
Idaho
Arkansas
Correlation coeﬃcients
0.204
0.163
0.178
0.167
0.155
0.152
0.145
0.137
0.130
0.127
0.002
0.004
0.005
0.006
0.008
0.008
0.008
0.009
0.015
0.016
Table 2: Pearson correlation coeﬃcients for the 10
states most (top) and least (bottom) correlated with
the election forecasting results.
ual states, such as adding in state-speciﬁc error from
sampling. It would be useful to know how the natu-
ral sampling of the quantum device during training
lends itself to state-speciﬁc error. For each iteration
that we used for determining the national averages,
we calculated the diﬀerence between the target (cid:104)s(cid:105)D
and current model output (cid:104)s(cid:105)M . If this diﬀerence is
negative, this would be a state-speciﬁc error in fa-
vor of the Democratic candidate, and vice versa a
positive value translates to error beneﬁting the Re-
publican candidate. By taking all these errors per
state, we can form diﬀerent state-speciﬁc error dis-
tributions per state. These distributions vary con-
siderably, depending on the underlying target (cid:104)s(cid:105)D
value, as evidenced in Figure 6.
At the extremes, we see that the error distribu-
tions of states leaning heavily Democratic or Repub-
lican are asymmetrical. This occurs naturally, due
to (cid:104)s(cid:105)M being bound between 0 and 1. If (cid:104)s(cid:105)D ≈ 0
(state leaning heavily Republican), all error will be
biased in the negative direction; similarly, states
with (cid:104)s(cid:105)D ≈ 1 (state leaning heavily Democrat) will
have positively-biased error distributions. For swing
states, we see a much more uniform spread of error,
which shows that in the absence of bounds ((cid:104)s(cid:105)D ≈
10
0 or 1), the error tends to be equally distributed.
Figure 6: Example distributions of state-speciﬁc
error for states leaning heavily Republican (top),
Democratic (bottom), or swing states (middle).
One interesting ﬁnding was that heavily-learning
Democratic states seemed to have longer error dis-
tribution tails compared to the heavily-leaning Re-
publican states. As seen in Figure 6, while almost all
the probability mass of Alabama’s error distribution
is contained within the range -5 to 5, a substantial
amount of California’s error distribution falls out-
side these bounds. This phenomena can introduce
an amount of bias in favor of one particular candi-
date. One potential mitigation technique for deal-
ing with this issue is taking the average of multiple
gauges ([11]), some of which could “ﬂip” the mea-
surement value (ﬂip Republican = 1, Democrat =
0). Additionally, some interesting new techniques
using “shimming” ([18]) have been shown to reduce
overall qubit error. In future work, it would be an in-
teresting topic to explore the evolution of individual
logical qubit error distributions in QC-trained Boltz-
mann machines by using shimming techniques (re-
ducing error) or introducing random noise (increas-
ing error) on a per-qubit basis.
4 Conclusions
In this work, we have showed an initial implementa-
tion of QC-trained Boltzmann machines, which can
be employed for the diﬃcult task of sampling from
correlated systems, an essential problem at the heart
of many applications such as election forecast mod-
eling. We validated that this approach successfully
learned various data distributions based on state
polling results during the 2016 US Presidential cam-
paign, and these QC-trained models generated fore-
casts that had similar structural properties and out-
comes compared to a best in class election modeling
group. While quantum computers and samplers are
an emerging technology, we believe this application
area could be of near-term interest. This methodol-
ogy could be an interesting technique to bring to the
broader conversation of modeling in future election
forecasts.
References
[1] Survey ﬁnds Hillary Clinton has ‘more than
99% chance’ of winning election over Donald
Trump, http://www.independent.co.uk/, 2016.
[2] The Huﬃngton Post Presidential Forecast,
http://elections.huﬃngtonpost.com/2016/fore-
cast/president, 2016.
[3] Key model predicts big election win for Clinton,
http://money.cnn.com/2016/11/01/news/econ-
omy/hillary-clinton-win-forecast-moodys-
analytics/index.html, 2016.
[4] How the polls, including ours, missed trump’s
http://www.reuters.com/article/us-
victory,
usa-election-polls-idUSKBN1343O6, 2016.
[5] Analysis:
Early
thoughts
went wrong with
http://www.wbur.org/politicker/2016/11/09/
pollster-early-thoughts, 2016.
election
the
about what
polls,
[6] The
devil
in
the
polling
data,
https://www.quantamagazine.org/why-nate-
silver-sam-wang-and-everyone-else-were-wrong-
part-2-20161111/, 2016.
11
[7] Epic fail,
http://www.economist.com/news/
united-states/21710024-how-mid-sized-error-
led-rash-bad-forecasts-epic-fail, 2016.
[8] J. E. Dorband,
ArXiv e-prints
(2016),
1606.06123.
[9] M. Benedetti, J. Realpe-G´omez, R. Biswas, and
A. Perdomo-Ortiz, Phys. Rev. X 7, 041052
(2017).
[10] M. Benedetti, J. Realpe-G´omez, R. Biswas, and
A. Perdomo-Ortiz, Phys. Rev. A (2016).
[11] S. H. Adachi and M. P. Henderson, ArXiv e-
prints (2015), 1510.06356.
[12] M. Benedetti,
A. Perdomo-Ortiz,
1708.09784.
J. Realpe-G´omez,
and
ArXiv e-prints (2017),
[13] J. Biamonte et al., Nature 549, 195 (2017).
[14] E. Farhi et al., Science 292, 472 (2001).
[15] T. Albash, V. Martin-Mayor, and I. Hen, Phys.
Rev. Lett. 119, 110502 (2017).
[16] A
user’s
guide
to
election
ﬁvethirtyeight’s
forecast,
general
2016
https://ﬁvethirtyeight.com/features/a-users-
guide-to-ﬁvethirtyeights-2016-general-election-
forecast/, 2016.
[17] FiveThirtyEight,
Election update: Clin-
forecast,
ton’s big lead means a steadier
https://ﬁvethirtyeight.com/features/election-
update-clintons-big-lead-means-a-steadier-
forecast/, 2016.
[18] S. Adachi, Qubit bias measurement and correc-
tion, D-Wave Users Conference, 2017.
12
