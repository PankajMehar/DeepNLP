Approximate Inference via Weighted Rademacher Complexity
Jonathan Kuck
Computer Science Department
Stanford University
jkuck@cs.stanford.edu
Ashish Sabharwal
Seattle, WA
Stefano Ermon
Stanford University
Allen Institute for Artiﬁcial Intelligence
Computer Science Department
ashishs@allenai.org
ermon@cs.stanford.edu
Abstract
Rademacher complexity is often used to characterize the learn-
ability of a hypothesis class and is known to be related to the
class size. We leverage this observation and introduce a new
technique for estimating the size of an arbitrary weighted set,
deﬁned as the sum of weights of all elements in the set. Our
technique provides upper and lower bounds on a novel gener-
alization of Rademacher complexity to the weighted setting
in terms of the weighted set size. This generalizes Massart’s
Lemma, a known upper bound on the Rademacher complexity
in terms of the unweighted set size. We show that the weighted
Rademacher complexity can be estimated by solving a ran-
domly perturbed optimization problem, allowing us to derive
high-probability bounds on the size of any weighted set. We
apply our method to the problems of calculating the parti-
tion function of an Ising model and computing propositional
model counts (#SAT). Our experiments demonstrate that we
can produce tighter bounds than competing methods in both
the weighted and unweighted settings.
Introduction
A wide variety of problems can be reduced to computing the
sum of (many) non-negative numbers. These include calculat-
ing the partition function of a graphical model, propositional
model counting (#SAT), and calculating the permanent of
a non-negative matrix. Equivalently, each can be viewed as
computing the discrete integral of a non-negative weight func-
tion. Exact summation, however, is generally intractable due
to the curse of dimensionality (Bellman 1961).
As alternatives to exact computation, variational methods
(Jordan et al. 1998; Wainwright, Jordan, and others 2008)
and sampling (Jerrum and Sinclair 1996; Madras 2002) are
popular approaches for approximate summation. However,
they generally do not guarantee the estimate’s quality.
An emerging line of work estimates and formally bounds
propositional model counts or, more generally, discrete in-
tegrals (Ermon et al. 2013a; Chakraborty, Meel, and Vardi
2013; Ermon et al. 2014; Zhao et al. 2016). These approaches
reduce the problem of integration to solving a small number
of optimization problems involving the same weight function
but subject to additional random constraints introduced by
a random hash function. This results in approximating the
Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
#P-hard problem of exact summation (Valiant 1979) using
the solutions of NP-hard optimization problems.
Optimization can be performed efﬁciently for certain
classes of weight functions, such as those involved in the
computation of the permanent of a non-negative matrix.
If instead of summing (permanent computation) we max-
imize the same weight function, we obtain a maximum
weight matching problem, which is in fact solvable in poly-
nomial time (Kuhn 1955). However, adding hash-based con-
straints makes the maximum matching optimization prob-
lem intractable, which limits the application of random-
ized hashing approaches (Ermon et al. 2013c). On the
other hand, there do exist fully polynomial-time random-
ized approximation schemes (FPRAS) for non-negative per-
manent computation (Jerrum, Sinclair, and Vigoda 2004;
Bezáková et al. 2006). This gives hope that approximation
schemes may exist for other counting problems even when
optimization with hash-based constraints is intractable.
We present a new method for approximating and bound-
ing the size of a general weighted set (i.e., the sum of the
weights of its elements) using geometric arguments based
on the set’s shape. Our approach, rather than relying on
hash-based techniques, establishes a novel connection with
Rademacher complexity (Shalev-Shwartz and Ben-David
2014). This generalizes geometric approaches developed for
the unweighted case to the weighted setting, such as the work
of Barvinok (1997) who uses similar reasoning but with-
out connecting it with Rademacher complexity. In particular,
we ﬁrst generalize Rademacher complexity to weighted sets.
While Rademacher complexity is deﬁned as the maximum
of the sum of Rademacher variables over a set, weighted
Rademacher complexity also accounts for the weight of each
element in the set. Just like Rademacher complexity is related
to the size of the set, we show that weighted Rademacher
complexity is related to the total weight of the set. Further, it
can be estimated by solving multiple instances of a maximum
weight optimization problem, subject to random Rademacher
perturbations. Notably, the resulting optimization problem
turns out to be computationally much simpler than that re-
quired by the aforementioned randomized hashing schemes.
In particular, if the weight function is log-supermodular,
the corresponding weighted Rademacher complexity can be
estimated efﬁciently, as our perturbation does not change
the original optimization problem’s complexity (Orlin 2009;
Bach and others 2013).
Our approach most closely resembles a recent line of work
involving the Gumbel distribution (Hazan and Jaakkola 2012;
Hazan, Maji, and Jaakkola 2013; Hazan et al. 2016; Balog et
al. 2017; Mussmann and Ermon 2016; Mussmann, Levy, and
Ermon 2017). There, the Gumbel-max idea is used to bound
the partition function by performing MAP inference on a
model where the unnormalized probability of each state is
perturbed by random noise variables sampled from a Gumbel
distribution. While very powerful, exact application of the
Gumbel method is impractical, as it requires exponentially
many independent random perturbations. One instead uses
local approximations of the technique.
Empirically, on spin glass models we show that our tech-
nique yields tighter upper bounds and similar lower bounds
compared with the Gumbel method, given similar computa-
tional resources. On a suite of #SAT model counting instances
our approach generally produces comparable or tighter upper
and lower bounds given limited computation.
Background
Rademacher complexity is an important tool used in learn-
ing theory to bound the generalization error of a hypothesis
class (Shalev-Shwartz and Ben-David 2014).
Deﬁnition 1. The Rademacher complexity of a set A ⊆ Rn
is deﬁned as:
R(A) :=
Ec
sup
a∈A
ciai
(1)
where Ec denotes expectation over c, and c is sampled uni-
formly from {−1, 1}n.
As the name suggests, it is a measure of the complexity of
set A (which, in learning theory, is usually a hypothesis class).
It measures how “expressive” A is by evaluating how well we
can “ﬁt” to a random noise vector c by choosing the closest
vector (or hypothesis) from A. Intuitively, Rademacher com-
plexity is related to |A|, the number of vectors in A, another
crude notion of complexity of A. However, it also depends
on how vectors in A are arranged in the ambient space Rn.
A central focus of this paper will be establishing quantitative
relationships between R(A) and |A|.
A key property of Rademacher complexity that makes it
extremely useful in learning theory is that it can be estimated
using a small number of random noise samples c under mild
conditions (Shalev-Shwartz and Ben-David 2014). The result
follows from McDiarmid’s inequality:
Proposition 1 (McDiarmid, 1989). Let X1, ..., Xm ∈ X
be independent random variables. Let f : X m (cid:55)→ R be a
function that satisﬁes the bounded differences condition that
∀i ∈ {1, ..., m} and ∀x1, ..., xm, x(cid:48)
i ∈ X :
|f (x1, ..., xi, ..., xm) − f (x1, ..., x(cid:48)
(cid:35)
(cid:34)
n(cid:88)
i=1
Then for all  > 0
(cid:104)(cid:12)(cid:12)f (X1, ..., Xm) − E(cid:2)f (X1, ..., Xm)(cid:3)(cid:12)(cid:12) ≥ 
Pr
i, ..., xm)| ≤ di.
(cid:32) −22(cid:80)
(cid:105) ≤ exp
(cid:33)
j d2
McDiarmid’s inequality says we can bound, with high
probability, how far a function f of random variables may
deviate from its expected value, given that the function does
not change much when the value of a single random variable
is changed. Because the function in Eq. (1) satisﬁes this
property (Shalev-Shwartz and Ben-David 2014), we can use
Eq. (1) to bound R(A) with high probability by computing
the supremum for only a small number of noise samples c.
Problem Setup
In this section we formally deﬁne our problem and intro-
duce the optimization oracle central to our solution. Let
w : {−1, 1}n → [0,∞) be a non-negative weight function.
We consider the problem of computing the sum
(cid:88)
Z(w) =
w(x).
x∈{−1,1}n
Many problems, including computing the partition func-
tion of an undirected graphical model, where w(x) is the
unnormalized probability of state x (see Koller and Fried-
man (2009)), propositional model counting (#SAT), and com-
puting the permanent of a non-negative matrix can be reduced
to calculating this sum. The problem is challenging because
explicit calculation requires summing over 2n states, which
is computationally intractable in cases of interest.
Due to the general intractability of exactly calculating
Z(w), we focus on an efﬁcient approach for estimating Z(w)
which additionally provides upper and lower bounds that hold
with high probability. Our method depends on the following
assumption:
Assumption 1. We assume existence of an optimization ora-
cle that can output the value
{(cid:104)c, x(cid:105) + log w(x)}
x∈{−1,1}n
δ(c, w) = max
(2)
for any vector c ∈ {−1, 1}n and weight function w :
{−1, 1}n → [0,∞).
Note that throughout the paper we simply denote log2
as log, loge as ln, and assume log 0 = −∞. Assump-
tion 1 is reasonable, as there are many classes of models
where such an oracle exists. For instance, polynomial time
algorithms exist for ﬁnding the maximum weight match-
ing in a weighted bipartite graph (Hopcroft and Karp 1971;
Jonker and Volgenant 1987). Graph cut algorithms can be ap-
plied to efﬁciently maximize a class of energy functions (Kol-
mogorov and Zabin 2004). More generally, MAP inference
can be performed efﬁciently for any log-supermodular weight
function (Orlin 2009; Chakrabarty, Jain, and Kothari 2014;
Fujishige 1980). Our perturbation preserves the submodular-
ity of − log w(x), as (cid:104)c, x(cid:105) can be viewed as n independent
single variable perturbations, so we have an efﬁcient opti-
mization oracle whenever the original weight function is
log-supermodular. Further, notice that this is a much weaker
assumption compared with the optimization oracle required
by randomized hashing methods (Chakraborty, Meel, and
Vardi 2013; Ermon et al. 2014; Zhao et al. 2016).
If an approximate optimization oracle exists that can ﬁnd
a value within some known bound of the maximum, we can
modify our bounds to use the approximate oracle. This may
improve the efﬁciency of our algorithm or extend its use to
additional problem classes. For the class of log-supermodular
distributions, approximate MAP inference is equivalent to
performing approximate submodular minimization (Jegelka,
Lin, and Bilmes 2011).
We note that even when an efﬁcient optimization oracle
exists, the problem of exactly calculating Z(w) is gener-
ally still hard. For example, polynomial time algorithms ex-
ist for ﬁnding the maximum weight perfect matching in a
weighted bipartite graph. However, computing the permanent
of a bipartite graph’s adjacency matrix, which equals the
sum of weights for all perfect matchings or Z(w), is still #P-
complete(Jerrum, Sinclair, and Vigoda 2004). A fully poly-
nomial randomized approximation scheme (FPRAS) exists
(Jerrum, Sinclair, and Vigoda 2004; Bezáková et al. 2006),
based on Markov chain Monte Carlo to sample over all per-
fect matchings. However, the polynomial time complexity
of this algorithm suffers from a large degree, limiting its
practical use.
Our approach for estimating the sum Z(w) = (cid:80)
Weighted Rademacher Bounds on Z(w)
x w(x)
is based on the idea that the Rademacher complexity of a
set is related to the set’s size. In particular, Rademacher
complexity is monotonic in the sense that R(A) ≤ R(B)
whenever A ⊆ B. Note that monotonicity does not hold for
|A| ≤ |B|, that is, R(A) is monotonic in the contents of A
but not necessarily in its size. We estimate the sum of arbi-
trary non-negative elements by generalizing the Rademacher
complexity in deﬁnition 2.
Deﬁnition 2. We deﬁne the weighted Rademacher complexity
of a weight function w : {−1, 1}n → [0,∞) as
R(w) := Ec
max
x∈{−1,1}n
{(cid:104)c, x(cid:105) + log w(x)}
(3)
for c sampled uniformly from {−1, 1}n.
In the notation of Eq. (2), the weighted Rademacher com-
plexity is simply R(w) = Ec[δ(c, w)]. For a set A ⊆
{−1, 1}n, let IA : {−1, 1}n → {0, 1} denote the indicator
weight function for A, deﬁned as IA(x) = 1 ⇐⇒ x ∈ A.
Then R(IA) = R(A), that is, the weighted Rademacher com-
plexity is identical to the standard Rademacher complexity
for indicator weight functions. For a general weight function,
the weighted Rademacher complexity extends the standard
Rademacher complexity by giving each element (hypothesis)
its own weight.
Algorithmic Strategy
The key idea of this paper is to use the weighted Rademacher
complexity R(w) to provide probabilistic estimates of Z(w),
the total weight of w.
This is a reasonable strategy because as we have seen
before, for an indicator weight function IA : {−1, 1}n →
{0, 1}, R(IA) reduces to the standard Rademacher complex-
ity R(A), and Z(IA) = |A| is simply the cardinality of
the set. Therefore we can use known quantitative relation-
ships between R(A) and |A| from learning theory to estimate
(cid:20)
(cid:21)
k(cid:88)
i=1
|A| = Z(IA) in terms of R(A) = R(IA). Although not for-
mulated in the framework of Rademacher complexity, this is
the strategy used by Barvinok (1997).
Here, we generalize these results to general weight func-
tions w and show that it is, in fact, possible to use R(w) to
obtain estimates of Z(w). This observation can be turned into
an algorithm by observing that R(w) is the expectation of a
random variable concentrated around its mean. Therefore, as
we will show in Proposition 2, a small number of samples suf-
ﬁces to reliably estimate R(w) (and hence, Z(w)) with high
probability. Whenever w is ‘sufﬁciently nice’ and we have
access to an optimization oracle, the estimation algorithm is
efﬁcient.
Algorithm 1 Rademacher Estimate of log Z(w)
Inputs: A positive integer k and weight function w :
{−1, 1}n → [0,∞).
Output: A number ¯δk(w) which approximates log Z(w) =
log
(cid:16)(cid:80)
(cid:17)
x∈{−1,1}n w(x)
1. Sample k vectors c1, c2, . . . , ck independently and uni-
formly from {−1, 1}n.
2. Apply the optimization oracle of assumption 1 to each
vector c and compute the mean
¯δk(w) =
max
x∈{−1,1}n
{(cid:104)ci, x(cid:105) + log w(x)}.
3. Output ¯δk(w) as an estimator of R(w) and thus log Z(w).
Bounding Weighted Rademacher Complexity
The weighted Rademacher complexity is an expectation over
optimization problems. The optimization problem is deﬁned
by sampling a vector, or direction since all have length
n,
uniformly from {−1, 1}n and ﬁnding the vector x that is
most aligned (largest dot product) after adding log w(x).
Our ﬁrst objective is to derive bounds on the weighted
Rademacher complexity in terms of the sum Z(w).
We begin with the observation that it is impossible to de-
rive bounds on the Rademacher complexity in terms of set
size that are tight for sets of all shapes. To gain intuition, note
that in high dimensional spaces the dot product of any par-
ticular vector and another chosen uniformly at random from
{−1, 1}n is close to 0 with high probability. The distribution
of weight vectors throughout the space may take any geomet-
ric form. One extreme conﬁguration is that all vectors with
large weights are packed tightly together, forming a Ham-
ming ball. At the other extreme, all vectors with large weights
could be distributed uniformly through the space. As Figure 1
illustrates, a large set of tightly packed vectors and a small set
of well-distributed vectors will both have similar Rademacher
complexity. Thus, bounds on Rademacher complexity that
are based on the underlying set’s size fundamentally cannot
always be tight for all distributions. Nevertheless, the lower
and upper bounds we derive next are tight enough to be useful
in practice.
Lemma 2. For any λ > 0, γ > 0, and weight functions
w, wγ : {−1, 1}n → [0,∞) with wγ(x) = w(x)γ, the
weighted Rademacher complexity of wγ is upper bounded by
R(wγ) ≤ 1
(cid:40)
with
log w∗(λ, γ) + λ
λγ − 1
log Z(w) +
(4)
wmax = maxx w(x),
wmin = minx{w(x) : w(x) > 0},
(λ, γ) =
Note that for an indicator weight function we recover the
if λγ ≥ 1
if λγ ≤ 1
(cid:113) 2 log Z(w)
bound from Massart’s Lemma by setting λ =
and γ = 1.
Corollary 2.1. For sufﬁciently large γ and
(cid:115)
λ =
2 log Z(w)
wmax
we recover the bound wmax ≤ Z(w) from Lemma 2.
Lemma 2 holds for any λ > 0 and γ > 0. In general we
set γ = 1 and optimize over λ to make the bound as tight as
possible, comparing the result with the trivial bound given
by Corollary 2.1. More sophisticated optimization strategies
over λ and γ could result in a tighter bound. Please see the
appendix for further details and proofs.
Bounding the Weighted Sum Z(w)
With our bounds on the weighted Rademacher complexity
from the previous section, we now present our method for
efﬁciently bounding the sum Z(w). Proposition 2 states that
we can estimate the weighted Rademacher complexity using
the optimization oracle of assumption 1.
Proposition 2. For c ∈ {−1, 1}n sampled uniformly at ran-
dom, the bound
R(w) −
6n ≤ δ(c, w) ≤ R(w) +
6n
(5)
holds with probability greater than .95.
Proof. By applying Proposition 1 to the function fw(c) =
δ(c, w), and noting the constant di = 2, we have
(cid:104)|δ(c, w) − R(w)| ≥
(cid:105) ≤ e−3 ≤ .05.
6n
This ﬁnishes the proof.
To bound Z(w) we use our optimization oracle to solve
a perturbed optimization problem, giving an estimate of the
weighted Rademacher complexity, R(w). Next we invert the
bounds on R(w) (Lemmas 1 and 2) to obtain bounds on
Z(w). We optimize the parameters λ and β (from equations
1 and 2) to make the bounds as tight as possible. By applying
our optimization oracle repeatedly, we can reduce the slack
introduced in our ﬁnal bound when estimating R(w) (by
Lemma 2) and arrive at our bounds on the sum Z(w), stated
(cid:80)
in the following theorem.
Theorem 1. With probability at least 0.95, the sum Z(w) =
x∈{−1,1}n w(x) of any weight function w : {−1, 1}n →
[0,∞) is bounded by the outputs of algorithms 2 and 3 as
ψLB < log Z(w) < ψU B.
Figure 1: Illustration mapping a set of vectors in high di-
mensional space {−1, 1}n to the unit circle. Red regions
correspond to regions of space that have a large dot prod-
uct with some vector in the set. Left: when the size of a
set is small, very few regions have a large dot product with
any vector in the set, so the Rademacher complexity will be
small. Right: when a large set of vectors is tightly packed
in a small region of space, the Rademacher complexity will
remain relatively small. In both left and right ﬁgures we have
similar (small) Rademacher complexities, yet different set
sizes. This illustrates why tight bounds on the set size based
on Rademacher complexity are difﬁcult to achieve.
Lower bound. To lower bound the weighted Rademacher
complexity we adapt the technique of (Barvinok 1997) for
lower bounding the standard Rademacher complexity. The
high level idea is that the space {−1, 1}n can be mapped to
the leaves of a binary tree. By following a path from the root
to a leaf, we are dividing the space in half n times, until we
arrive at a leaf which corresponds to a single element (with
some ﬁxed weight). By judiciously choosing which half of
the space (branch of the tree) to recurse into at each step we
derive the bound in Lemma 1, whose proof is given in the
appendix.
Lemma 1. For any β ∈ (0, 1/2), the weighted Rademacher
complexity of a weight function w : {−1, 1}n → [0,∞) is
lower bounded by
R(w) ≥ log w
n log (1 − β) + log Z(w) − log w∗(β)
(β) +
(cid:16) 1−β
(cid:17)
log
(cid:40)
with
(β) =
wmax = maxx w(x),
wmin = minx{w(x) : w(x) > 0},
if β ≥ 1/3
if β ≤ 1/3
Upper bound.
In the unweighted setting, a standard up-
per bound on the Rademacher complexity is used in learn-
ing theory to show that the Rademacher complexity of a
small hypothesis class is also small, often to prove PAC-
learnability. Massart’s Lemma (see (Shalev-Shwartz and
Ben-David 2014), lemma 26.8) formally upper bounds the
Rademacher complexity in terms of the size of the set. This
result is intuitive since, as we have noted, the dot product
between any one vector x ∈ {−1, 1}n is small with most
other vectors c ∈ {−1, 1}n. Therefore, if the set is small the
Rademacher complexity must also be small.
Adapting the proof technique of Massart’s Lemma to the
weighted setting we arrive at the following bound:
2. If log wmin was provided as input and λ ≤ 1,
2. If wmax was provided as input, calculate
ln Z(w) = Eγ
max
x∈{−1,1}n
{ln w(x) + γ(x)}
where w∗ =
(cid:19)
(cid:18) 1 − βopt
(cid:26)wmin,
βopt
wmax,
if βopt < 1
if βopt > 1
Algorithm 2 Rademacher Lower Bound for log Z(w)
Inputs: The estimator ¯δk(w) output by algorithm 1, k used
to compute ¯δk(w), and optionally wmin and wmax.
Output: A number ψLB which lower bounds log Z(w).
1. If log wmin was provided as input, calculate
k − log wmin
λ =
¯δk(w) −(cid:113) 6n
(¯δk(w) −(cid:113) 6n
ψLB =
3. Otherwise,
+ log wmin.
k − log wmin)2
2n
ψLB = ¯δk(w) −
(cid:114) 6n
− n
4. Output the lower bound max{ψLB, log wmax}.
Experiments
The closest line of work to this paper showed that the partition
function can be bounded by solving an optimization problem
perturbed by Gumbel random variables (Hazan and Jaakkola
2012; Hazan, Maji, and Jaakkola 2013; Hazan et al. 2016;
Kim, Sabharwal, and Ermon 2016; Balog et al. 2017). This
approach is based on the fact that
(cid:21)
(cid:40)
(cid:40)
where all 2n random variables γ(x) are sampled from the
Gumbel distribution with scale 1 and shifted by the Euler-
Mascheroni constant to have mean 0. Perturbing all 2n states
with IID Gumbel random variables is intractable, leading the
authors to bound ln Z(w) by perturbing states with a combi-
nation of low dimensional Gumbel perturbations. Speciﬁcally
the upper bound
ln Z(w) ≤ ΘU B = Eγ
max
x∈{−1,1}n
ln w(x) +
γi(xi)
(Hazan et al. 2016) and lower bound
(cid:41)(cid:35)
(cid:41)(cid:35)
n(cid:88)
n(cid:88)
i=1
i=1
ln Z(w) ≥ ΘLB = Eγ
max
x∈{−1,1}n
ln w(x) +
γi(xi)
(Balog et al. 2017, p. 6) hold in expectation, where γi(x) for
i = 1, . . . , n are sampled from the Gumbel distribution with
scale 1 and shifted by the Euler-Mascheroni constant to have
mean 0.
To obtain bounds that hold with high probability using
Gumbel perturbations we calculate the slack term (Hazan et
al. 2016, p. 32)
(cid:33)2
(cid:114) 1
ln
2k
(cid:40)
n max
(cid:114) 32
(cid:41)(cid:41)
ln
ln
(cid:32)
(cid:40)
g = min
1 +
(cid:20)
(cid:34)
(cid:34)
Algorithm 3 Rademacher Upper Bound for log Z(w)
Inputs: The estimator ¯δk(w), k used to compute ¯δk(w), and
optionally wmin and wmax.
Output: A number ψU B which upper bounds log Z(w).
1. If wmin was provided as input, calculate
(cid:113) 6n
k − log wmin
(cid:113) 6n
k − log wmax
¯δk(w) +
βmin =
¯δk(w) +
βmax =
3. Set the value
βopt =

βmin,
βmax,
2 ,
3 ,
if 0 < βmin < 1
if 1
3 < βmax < 1
if 1
2 < βmax
otherwise
4. Output the upper bound ψU B:
(a) If βopt = 1
(b) If βopt = 1
(c) Otherwise,
3, ψU B = ¯δk(w) +
2, ψU B = n + log wmax.
(cid:113) 6n
k + n log(cid:0) 3
(cid:1).
ψU B = nβopt log
−n log (1 − βopt)+log w
giving upper and lower bounds θU B = ΘU B + g and θLB =
n that hold with probability 1 − α where k samples
ΘLB − g
are used to estimate the expectation bounds.
We note the Gumbel expectation upper bound takes nearly
the same form as the weighted Rademacher complexity, with
two differences. The perturbation is sampled from a Gum-
bel distribution instead of a dot product with a vector of
Rademacher random variables and, without scaling, the two
bounds are naturally written in different log bases.
We experimentally compare our bounds with those ob-
tained by Gumbel perturbations on two models. First we
bound the partition function of the spin glass model from
(Hazan et al. 2016). For this problem the weight function
is given by the unnormalized probability distribution of the
spin glass model. Second we bound the propositional model
counts (#SAT) for a variety of SAT problems. This problem
falls into the unweighted category where every weight is
either 0 or 1, speciﬁcally every satisfying assignment has
weight 1 and we bound the total number of satisfying assign-
ments.
the question of how many assignments x to the underlying
boolean variables result in F evaluating to true. Our weight
function is given by w(x) = 1 if F (x) evaluates to true, and
0 otherwise.
We performed MAP inference on the perturbed problem
using the weighted partial MaxSAT solver MaxHS (Davies
2013). Ground truth was obtained for a variety of models1 us-
ing three exact propositional model counters (Thurley 2006;
Sang et al. 2004; Oztok and Darwiche 2015)2. Table 1 shows
bounds that hold with probability .95 and k = 1. While the
Gumbel lower bounds are always trivial, we produce non-
trivial lower bounds for several model instances. Our upper
bounds are generally comparable to or tighter than Gumbel
upper bounds.
Analysis
Our bounds are much looser than those computed by random-
ized hashing schemes (Chakraborty, Meel, and Vardi 2013;
Ermon et al. 2013d; Ermon et al. 2013b; Zhao et al. 2016),
but also require much less computation (Ermon et al. 2013c;
Achim, Sabharwal, and Ermon 2016). While our approach
provides polynomial runtime guarantees for MAP inference
in the spin glass model after random perturbations have been
applied, randomized hashing approaches do not. For proposi-
tional model counting, we found that our method is computa-
tionally cheaper by over 2 orders of magnitude than results
reported in Zhao et al. (2016). Additionally, we tried reducing
the runtime and accuracy of randomized hashing schemes
by running code from Zhao et al. (2016) with f values of 0,
.01, .02, .03, .04, and .05. We set the maximum time limit
to 1 hour (while our method required .01 to 6 seconds of
computation for reported results). Throughout experiments
on models reported in Table 1 our approach still generally re-
quired orders of magnitude less computation and also found
tighter bounds in some instances.
Empirically, our lower bounds were comparable to or
tighter than those obtained by Gumbel perturbations on both
models. The weighted Rademacher complexity is generally
at least as good an estimator of log Z as the Gumbel up-
per bound, however it is only an estimator and not an upper
bound. Our upper bound using the weighted Rademacher
complexity, which holds in expectation, is empirically weaker
than the corresponding Gumbel expectation upper bound.
However, the slack term needed to transform our expecta-
tion bound into a high probability bound is tighter than the
corresponding Gumbel slack term. Since both slack terms
approach 0 in the limit of inﬁnite computation (k = ∞, the
number of samples used to estimate the expectation bound),
this can result in a trade-off where we produce a tighter upper
bound up to some value of k, after which the Gumbel bound
becomes tighter.
1The models used in our experiments can be downloaded from
http://reasoning.cs.ucla.edu/c2d/results.html
counts were
2Precomputed model
downloaded
from
https://sites.google.com/site/marcthurley/sharpsat/benchmarks/
collected-model-counts
Figure 2: Bounds for a 7x7 spin glass model with k = 5 (for
both methods), that hold with probability .95. Our bounds
and estimator are scaled to match Gumbel log base e bounds.
Spin Glass Model
Following (Hazan et al. 2016), we bound the partition func-
tion of a spin glass model with variables xi ∈ {−1, 1} for
i = 1, 2, . . . , n, where each variable represents a spin. Each
spin has a local ﬁeld parameter θi which corresponds to its
local potential function θi(xi) = θixi. We performed experi-
ments on grid shaped models where each spin variable has 4
neighbors, unless it occupies a grid edge. Neighboring spins
interact with coupling parameters θi,j(xi, xj) = θi,jxixj.
The potential function of the spin glass model is
θ(x1, x2, . . . , xn) =
θixi +
θi,jxixj,
(cid:88)
i∈V
(cid:88)
(i,j)∈E
(cid:88)
i∈V
(cid:88)
(i,j)∈E
 .
with corresponding weight function
w(x) = exp
θixi +
θi,jxixj
We compare our bounds on a 7x7 spin glass model. We sam-
pled the local ﬁeld parameters θi uniformly at random from
[−1, 1] and the coupling parameters uniformly at random
from [0, c) with c varying. Non-negative coupling parameters
make it possible to perform MAP inference efﬁciently us-
ing the graph-cuts algorithm (Kolmogorov and Zabin 2004;
Greig, Porteous, and Seheult 1989). We used the python
maxﬂow module wrapping the implementation from Boykov
and Kolmogorov (2004).
Figure 2 shows bounds that hold with probability .95,
where all bounds are computed with k = 5. For this value
of k, our approach produces tighter upper bounds than using
Gumbel perturbations. The crossover to a tighter Gumbel per-
turbation upper bound occurs around k ≈ 15. Lower bounds
are equivalent, although we note it is trivial to recover this
bound by simply calculating the largest weight over all states.
Propositional Model Counting
Next we evaluate our method on the problem of proposi-
tional model counting. Given a boolean formula F , this poses
Model Name
log-1
log-2
log-3
log-4
tire-1
tire-2
tire-3
tire-4
ra
rb
sat-grid-pbl-0010
sat-grid-pbl-0015
sat-grid-pbl-0020
sat-grid-pbl-0025
sat-grid-pbl-0030
c432
c499
c880
c1355
c1908
c2670
#Variables
#Clauses
939
1337
1413
2303
352
550
577
812
1236
1854
110
240
420
650
930
196
243
417
555
751
1230
3785
24777
29487
20963
1038
2001
2004
3222
11416
11324
191
436
781
1226
1771
514
714
1060
1546
2053
2876
ln(Z)
47.8
24.2
26.4
65.3
20.4
27.3
26.1
32.3
659.2
855.9
54.7
125.4
220.4
348.3
502.4
25.0
28.4
41.6
28.4
22.9
161.5
¯δ1(w)
64.5 (20.8)
48.6 (20.7)
49.9 (22.3)
106.0 (26.6)
30.7 (11.2)
42.1 (14.2)
36.9 (17.1)
55.0 (17.4)
621.1 (15.5)
857.2 (12.6)
51.6 (4.9)
120.2 (6.6)
215.5 (9.0)
338.8 (9.4)
482.6 (13.0)
42.7 (5.8)
58.5 (6.2)
83.1 (8.4)
79.8 (12.2)
87.8 (12.2)
260.0 (14.6)
ψU B
θU B
ψLB
438.0 (46.2)
485.7 (60.3)
503.9 (65.3)
830.2 (77.7)
198.5 (17.6)
283.9 (27.7)
280.5 (36.1)
384.7 (38.9)
856.7 (0.0)
1285.1 (0.0)
76.2 (0.0)
166.4 (0.0)
291.1 (0.0)
450.5 (0.0)
644.6 (0.0)
135.3 (1.0)
168.2 (0.4)
281.1 (4.7)
342.9 (14.2)
427.7 (19.3)
812.8 (10.8)
426.5 (43.0)
464.0 (45.1)
478.2 (42.3)
676.9 (58.8)
249.6 (23.7)
310.2 (29.6)
316.5 (29.1)
383.3 (35.3)
1100.9 (45.7)
1387.5 (43.9)
176.3 (13.6)
310.3 (18.8)
472.3 (26.5)
667.8 (33.1)
893.3 (36.7)
212.6 (18.2)
243.8 (16.6)
332.9 (21.3)
368.6 (28.7)
419.1 (32.8)
701.4 (39.6)
0.5 (0.6)
0.3 (0.4)
0.4 (0.4)
0.4 (0.5)
0.3 (0.4)
0.3 (0.4)
0.4 (0.6)
0.3 (0.3)
184.1 (10.1)
239.3 (7.7)
7.6 (2.2)
26.6 (3.8)
56.2 (5.6)
97.0 (6.2)
144.1 (8.7)
1.4 (0.8)
3.2 (1.2)
4.2 (1.4)
2.3 (1.3)
1.8 (0.9)
23.7 (3.4)
θLB
-0.3 (0.0)
-0.3 (0.0)
-0.3 (0.0)
-0.2 (0.0)
-0.5 (0.1)
-0.4 (0.1)
-0.4 (0.1)
-0.3 (0.0)
0.3 (0.0)
0.2 (0.0)
-0.5 (0.1)
-0.1 (0.1)
0.0 (0.1)
0.2 (0.1)
0.2 (0.0)
-0.5 (0.1)
-0.4 (0.1)
-0.3 (0.1)
-0.3 (0.1)
-0.3 (0.0)
-0.1 (0.0)
Table 1: Empirical comparison of our estimate of (¯δ1(w)) and bounds on (ψ) propositional model counts against bounds based
on Gumbel perturbations (θ). The mean over 100 runs is shown with the standard deviation in parentheses. Bounds hold with
probability .95 and k = 1 for both methods. Tighter bounds are in bold. Meta column descriptions, left to right: model name and
information, natural logarithm of ground truth model counts and our estimator, upper bounds, and lower bounds.
Conclusion
We introduced the weighted Rademacher complexity, a novel
generalization of Rademacher complexity. We showed that
this quantity can be used as an estimator of the size of a
weighted set, and gave bounds on the weighted Rademacher
complexity in terms of the weighted set size. This allowed
us to bound the sum of any non-negative weight function,
such as the partition function, in terms of the weighted
Rademacher complexity. We showed how the weighted
Rademacher complexity can be efﬁciently approximated
whenever an efﬁcient optimization oracle exists, as is the
case for a variety of practical problems including calculating
the partition function of certain graphical models and the per-
manent of non-negative matrices. Experimental evaluation
demonstrated that our approach provides tighter bounds than
competing methods under certain conditions.
In future work our estimator R(w) and bounds on Z(w)
may be generalized to other forms of randomness. Rather
than sampling c uniformly from {−1, 1}n, we could conceiv-
ably sample each element ci from some other distribution,
such as the uniform distribution over [−1, 1], a Gaussian,
or Gumbel. Our bounds should readily adapt to continuous
uniform or gaussian distributions, although derivations may
be more complex in general. As another line of future work,
the weighted Rademacher complexity may be useful beyond
approximate inference to learning theory.
Acknowledgments
We gratefully acknowledge funding from Ford, FLI and NSF
grants #1651565, #1522054, #1733686. We also thank Tri
Dao, Aditya Grover, Rachel Luo, and anonymous reviewers.
References
[2016] Achim, T.; Sabharwal, A.; and Ermon, S. 2016. Be-
yond parity constraints: Fourier analysis of hash functions for
inference. In International Conference on Machine Learning,
2254–2262.
[2013] Bach, F., et al. 2013. Learning with submodular
functions: A convex optimization perspective. Foundations
and Trends R(cid:13) in Machine Learning 6(2-3):145–373.
[2017] Balog, M.; Tripuraneni, N.; Ghahramani, Z.; and
Weller, A. 2017. Lost relatives of the Gumbel trick.
In
34th International Conference on Machine Learning, 371–
379.
[1997] Barvinok, A. I. 1997. Approximate counting via
random optimization. Random Structures and Algorithms
11(2):187–198.
[1961] Bellman, R. E. 1961. Adaptive control processes: a
guided tour. Princeton university press.
[2006] Bezáková, I.; Štefankoviˇc, D.; Vazirani, V. V.; and
Vigoda, E. 2006. Accelerating simulated annealing for the
permanent and combinatorial counting problems. In Proceed-
ings of the seventeenth annual ACM-SIAM symposium on
Discrete algorithm, 900–907.
[2004] Boykov, Y., and Kolmogorov, V. 2004. An experimen-
tal comparison of min-cut/max-ﬂow algorithms for energy
minimization in vision. IEEE transactions on pattern analy-
sis and machine intelligence 26(9):1124–1137.
[2014] Chakrabarty, D.; Jain, P.; and Kothari, P. 2014. Prov-
able submodular minimization using wolfe’s algorithm. In
Advances in Neural Information Processing Systems, 802–
809.
[2013] Chakraborty, S.; Meel, K. S.; and Vardi, M. Y. 2013. A
scalable approximate model counter. In International Confer-
ence on Principles and Practice of Constraint Programming,
200–216. Springer.
[2013] Davies, J. 2013. Solving MAXSAT by Decoupling
Optimization and Satisfaction. Ph.D. Dissertation, University
of Toronto.
[2013a] Ermon, S.; Gomes, C.; Sabharwal, A.; and Selman,
B. 2013a. Taming the curse of dimensionality: Discrete
In Proceedings
integration by hashing and optimization.
of the 30th International Conference on Machine Learning
(ICML-13), 334–342.
[2013b] Ermon, S.; Gomes, C. P.; Sabharwal, A.; and Selman,
B. 2013b. Embed and project: Discrete sampling with univer-
sal hashing. In Advances in Neural Information Processing
Systems (NIPS), 2085–2093.
[2013c] Ermon, S.; Gomes, C. P.; Sabharwal, A.; and Selman,
B. 2013c. Optimization with parity constraints: From binary
codes to discrete integration. In Proc. of the 29th Conference
on Uncertainty in Artiﬁcial Intelligence (UAI).
[2013d] Ermon, S.; Gomes, C. P.; Sabharwal, A.; and Selman,
B. 2013d. Taming the curse of dimensionality: Discrete
integration by hashing and optimization. In Proc. of the 30th
International Conference on Machine Learning (ICML).
[2014] Ermon, S.; Gomes, C.; Sabharwal, A.; and Selman,
B. 2014. Low-density parity constraints for hashing-based
discrete integration. In International Conference on Machine
Learning, 271–279.
[1980] Fujishige, S. 1980. Lexicographically optimal base of
a polymatroid with respect to a weight vector. Mathematics
of Operations Research 5(2):186–196.
[1989] Greig, D. M.; Porteous, B. T.; and Seheult, A. H. 1989.
Exact maximum a posteriori estimation for binary images.
Journal of the Royal Statistical Society. Series B (Method-
ological) 271–279.
[2012] Hazan, T., and Jaakkola, T. S. 2012. On the partition
function and random maximum a-posteriori perturbations. In
Langford, J., and Pineau, J., eds., Proceedings of the 29th
International Conference on Machine Learning (ICML-12),
991–998. New York, NY, USA: ACM.
[2016] Hazan, T.; Orabona, F.; Sarwate, A. D.; Maji, S.; and
Jaakkola, T. 2016. High dimensional inference with ran-
dom maximum a-posteriori perturbations. arXiv preprint
arXiv:1602.03571.
[2013] Hazan, T.; Maji, S.; and Jaakkola, T. 2013. On sam-
pling from the Gibbs distribution with random maximum
a-posteriori perturbations. In Advances in Neural Informa-
tion Processing Systems, 1268–1276.
[1971] Hopcroft, J. E., and Karp, R. M. 1971. A n5/2 algo-
rithm for maximum matchings in bipartite graphs. In Switch-
ing and Automata Theory, 1971., 12th Annual Symposium on,
122–125. IEEE.
[2011] Jegelka, S.; Lin, H.; and Bilmes, J. A. 2011. On
fast approximate submodular minimization. In Advances in
Neural Information Processing Systems, 460–468.
[1996] Jerrum, M., and Sinclair, A. 1996. The markov chain
monte carlo method: an approach to approximate counting
and integration. Approximation algorithms for NP-hard prob-
lems 482–520.
[2004] Jerrum, M.; Sinclair, A.; and Vigoda, E. 2004. A
polynomial-time approximation algorithm for the permanent
of a matrix with nonnegative entries. Journal of the ACM
(JACM) 51(4):671–697.
[1987] Jonker, R., and Volgenant, A. 1987. A shortest aug-
menting path algorithm for dense and sparse linear assign-
ment problems. Computing 38(4):325–340.
[1998] Jordan, M. I.; Ghahramani, Z.; Jaakkola, T. S.; and
Saul, L. K. 1998. An introduction to variational methods
for graphical models. NATO ASI SERIES D BEHAVIOURAL
AND SOCIAL SCIENCES 89:105–162.
[2016] Kim, C.; Sabharwal, A.; and Ermon, S. 2016. Exact
sampling with integer linear programs and random perturba-
tions. In Proc. 30th AAAI Conference on Artiﬁcial Intelli-
gence.
[2009] Koller, D., and Friedman, N. 2009. Probabilistic
graphical models: principles and techniques. MIT press.
[2004] Kolmogorov, V., and Zabin, R. 2004. What energy
functions can be minimized via graph cuts? IEEE transac-
tions on pattern analysis and machine intelligence 26(2):147–
159.
[1955] Kuhn, H. W. 1955. The hungarian method for the
assignment problem. Naval Research Logistics (NRL) 2(1-
2):83–97.
[2002] Madras, N. N. 2002. Lectures on monte carlo methods,
volume 16. American Mathematical Soc.
[1989] McDiarmid, C. 1989. On the method of bounded
differences. Surveys in combinatorics 141(1):148–188.
[2016] Mussmann, S., and Ermon, S. 2016. Learning and
inference via maximum inner product search. In International
Conference on Machine Learning, 2587–2596.
[2017] Mussmann, S.; Levy, D.; and Ermon, S. 2017. Fast
amortized inference and learning in log-linear models with
randomly perturbed nearest neighbor search. UAI.
[2009] Orlin, J. B. 2009. A faster strongly polynomial time
algorithm for submodular function minimization. Mathemat-
ical Programming 118(2):237–251.
[2015] Oztok, U., and Darwiche, A. 2015. A top-down
compiler for sentential decision diagrams. In IJCAI, 3141–
3148.
[2004] Sang, T.; Bacchus, F.; Beame, P.; Kautz, H. A.; and
Pitassi, T. 2004. Combining component caching and clause
learning for effective model counting. In SAT.
[2014] Shalev-Shwartz, S., and Ben-David, S. 2014. Un-
derstanding machine learning: From theory to algorithms.
Cambridge university press.
[2006] Thurley, M. 2006. sharpsat-counting models with
advanced component caching and implicit bcp. In SAT, 424–
429.
[1979] Valiant, L. G. 1979. The complexity of enumera-
tion and reliability problems. SIAM Journal on Computing
8(3):410–421.
[2008] Wainwright, M. J.; Jordan, M. I.; et al. 2008. Graphi-
cal models, exponential families, and variational inference.
Foundations and Trends R(cid:13) in Machine Learning 1(1–2):1–
305.
[2016] Zhao, S.; Chaturapruek, S.; Sabharwal, A.; and Ermon,
S. 2016. Closing the gap between short and long xors for
model counting. In AAAI, 3322–3329.
Appendix
We present formal proofs of our bounds on the sum Z(w) of any non-negative weight function w : {−1, 1}n → [0,∞). For
readability we occasionally restate results from the main paper. The format of our proof is as follows. First we bound the
weighted Rademacher complexity, R(w), by the output of our optimization oracle (δ(c, w), described in Assumption 1), which
we refer to as the slack bound. Next we lower bound the sum Z(w) by R(w) and apply our slack bound to obtain a lower
bound on Z(w) in terms of δ(c, w). Similarly, we upper bound the sum Z(w) by R(w) and apply our slack bound to obtain an
upper bound on Z(w) in terms of δ(c, w). Finally we tighten the bounds by repeatedly applying our optimization oracle.
Slack Bound
We use McDiarmid’s bound (Proposition 1) to bound the difference between the output of our optimization oracle (δ(c, w),
described in Assumption 1) and its expectation, which is the weighted Rademacher complexity R(w). For the function
the constant di = 2 in McDiarmid’s bound, giving
fw(c) = δ(c, w) = max
x∈{−1,1}n
P [|δ(c, w) − R(w)| ≥ ] ≤ exp
P [|δ(c, w) − R(w)| ≥
6n] ≤ exp
{(cid:104)c, x(cid:105) + log2 w(x)},
(cid:33)
(cid:32) −22(cid:80)
(cid:33)
j c2
≤ .05.
(cid:32)−2(
4n
6n)2
By choosing c ∈ {−1, 1}n uniformly at random we can say with probability greater than .95 that
R(w) −
6n ≤ δ(c, w) ≤ R(w) +
6n.
(6)
Lower Bound
In this section we lower bound the sum Z(w) by R(w) and apply our slack bound to obtain a lower bound on Z(w) in terms of
δ(c, w). We extend the Massart lemma (Shalev-Shwartz and Ben-David 2014, lemma 26.8) to the weighted setting by accounting
for the log2 w(x) weight term in the weighted Rademacher complexity. Our lower bound on Z(w) is given by the following
Lemma:
Lemma 3. For c ∈ {−1, 1}n sampled uniformly at random, the following bound holds with probability greater than .95:
Proof. We begin by upper bounding R(w) in terms of Z(w). Deﬁne c ∈ {−1, 1}n generated uniformly at random and
x ∈ {−1, 1}n. For any λ > 0, γ > 0, and weight functions w, wγ : {−1, 1}n → [0,∞) with wγ(x) = w(x)γ we have
(cid:40) (δ(c,w)−√
δ(c, w) − √
6n−log2 wmin)2
2n
6n − n
2 ,
log2 Z(w) ≥
(cid:104)
Ec
λ(cid:104)c, x(cid:105) + λγ log2 w(x)
max
(cid:105)
+ log2 wmin,
if δ(c,w)−√
otherwise
6n−log2 wmin
≤ 1
(cid:104)
R(wγ) = Ec
Ec
(cid:34)
Ec
log2
log2 max
max
(cid:104)
(cid:105)
2λ((cid:104)c,x(cid:105)+γ log2 w(x))(cid:105) ≤
(cid:104)c, x(cid:105) + γ log2 w(x)
(cid:35)
(cid:88)
(cid:35)
(cid:34)(cid:88)
2λ((cid:104)c,x(cid:105)+γ log2 w(x))
2λ((cid:104)c,x(cid:105)+γ log2 w(x))
Jensen≤
log2
Ec
where we have used Jensen’s inequality. By the linearity of expectation and independence between elements ci in a random
vector c,
(cid:88)
(cid:16)
(cid:104)
2λ(cid:104)c,x(cid:105)(cid:105)(cid:17)
R(wγ) ≤ 1
log2
2λγ log2 w(x)Ec
log2
2λγ log2 w(x)
i=1
n(cid:89)
Eci
(cid:2)2λcixi(cid:3)(cid:33)
(cid:32)
(cid:88)
Using Lemma A.6 from (Shalev-Shwartz and Ben-David 2014),
(cid:32)
(cid:88)
λ2||x||2
(cid:19)
R(wγ) ≤ 1
log2
(cid:18)
(cid:88)
2λγ log2 w(x)2
log2
2λγ log2 w(x)
2λxi + 2−λxi
log2
λ2 n
(cid:33)
log2
≤ 1
(cid:33)
(cid:32)
(cid:88)
log2
2λγ log2 w(x)
(cid:32)(cid:88)
n(cid:89)
i=1
(λxi)2
(cid:33)
(cid:33)
λn
2λγ log2 w(x)
2λγ log2 w(x)
(cid:32)(cid:88)
λn
log2
(cid:33)
λn
w(x)w(x)λγ−1
(cid:88)
(cid:33)
n(cid:89)
(cid:32)
i=1
(cid:32)(cid:88)
(cid:33)
log2
w(x)λγ
log2
max
w(x)
λn
log2 Z(w) +
log2 max
(cid:32)
Next,
where
R(wγ) ≤ 1
(cid:8)w(x)λγ−1(cid:9)(cid:88)
w∗(λ, γ) =
log2 Z(w) +
(cid:26)wmax = maxx w(x),
wmin = minx{w(x) : w(x) > 0},
if λγ ≥ 1
if λγ ≤ 1
{wλγ−1
max , wλγ−1
min } + λ
log2 w∗(λ, γ) + λ
λγ − 1
(7)
Note that for λγ = 1 we have two valid inequalities that hold for either choice of w∗(λ, γ). Having bounded the weighted
Rademacher complexity in terms of Z(w), we now apply the slack bound from equation 6 and have that with probability greater
than .95
(8)
This upper bound on δ(w) holds for any λ > 0, so we could jointly optimize over λ and γ to make the bound as tight as possible.
However, this is non-trivial because changing γ changes the weight function we supply to our optimization oracle. Instead we
generally set γ = 1 and optimize over only λ. At the end of this section we derive another bound with a different choice of γ.
This bound is trivial to derive, but illustrates that other choices of γ could result in meaningful bounds.
log2 Z(w) +
6n.
Rewriting the bound in Equation 8 with γ = 1 and w∗(λ) = w∗(λ, 1) we have
log2 w∗(λ, γ) + λ
δ(c, w) ≤ 1
λγ − 1
(9)
so the optimal value of λ that makes our bound as tight as possible occurs at the maximum of the quadratic function
− (λ − 1) log2 w∗(λ) − λ2 n
+ λ(δ(c, w) −
h(λ) = −λ2 n
− λ(log2 w∗(λ) − (δ(c, w) −
h(cid:48)(λ) = −λn − log2 w∗(λ) +
6n) ≤ log2 Z(w),
(cid:17)
(cid:16)
6n)) + log2 w∗(λ)
δ(c, w) −
6n
h(cid:48)(cid:48)(λ) = −n,
Where the stated derivatives are valid for λ (cid:54)= 1, as w∗(λ) is piecewise constant with a discontinuity at λ = 1. The maximum
of h(λ) must occur at λ = −∞, +∞, 1, or the value of λ that makes h(cid:48)(λ) = 0. By inspection the maximum does not occur
at λ = ±∞, so the maximum will occur at h(cid:48)(λ) = 0 or else λ = 1 if the derivative is never zero. We have h(cid:48)(λ) = 0 at
λ = (δ(c, w) − √
(cid:17)
6n − log2 w∗(λ))/n. Rearranging equation 9 we have
+ log2 w∗(λ) ≤ log2 Z(w).
Depending on the value of δ(c, w) we have 3 separate regimes for the optimal lower bound on Z(w).
6n − log2 w∗(λ)
δ(c, w) −
−λ2 n
(cid:16)
+ λ
6n + log2 wmin: In this case h(cid:48)(λ) = 0 at λ = (δ(c, w) − √
6n − log2 wmin)/n (note λ < 1 so that
1. δ(c, w) < n +
w∗(λ) = wmin) and the optimal lower bound is
(δ(c, w) − √
6n − log2 wmin)2
2n
+ log2 wmin ≤ log2 Z(w).
We require that λ > 0, but note that we can discard our bound and recompute with a new c if we ﬁnd that λ < 0
for our computed value of δ(c, w), as this can only happen with low probability when our slack bound is violated
and we have estimated R(w) poorly with δ(c, w). Note that R(w) = Ec
Ec
(cid:2)maxx∈{−1,1}n{(cid:104)c, x(cid:105) + log2 wmin}(cid:3) = n + log2 wmin, so R(w) − log2 wmin = n > 0.
(cid:2)maxx∈{−1,1}n{(cid:104)c, x(cid:105) + log2 w(x)}(cid:3) ≥
2. n +
6n + log2 wmin < δ(c, w) < n +
lower bound of
3. δ(c, w) > n +
6n + log2 wmax: In this case h(cid:48)(λ) is never zero, so at λ = 1 we have the optimal
δ(c, w) −
≤ log2 Z(w),
6n − n
6n + log2 wmax: This case cannot occur because δ(c, w) ≤ n + log2 wmax by deﬁnition.
We now illustrate how alternative choices of γ could result in meaningful bounds. From Equation 7 we have
(cid:20)
max
x∈{−1,1}n
Ec
(cid:21)
(cid:20)
{(cid:104)c, x(cid:105) + γ log2 w(x)}
R(wγ) ≤ 1
≤ 1
(cid:104)c, x(cid:105) + γ log2
(cid:26)
Ec
max
x∈{−1,1}n
log2 Z(w) +
log2 Z(w) +
w(x)
w∗(λ, γ)
λγ − 1
λγ − 1
≤ 1
(cid:27)(cid:21)
log2 w∗(λ, γ) + λ
log2 w∗(λ, γ) + λ
log2
Z(w)
w∗(λ, γ)
+ λ
For sufﬁciently large γ (γ ≥ 1
large γ,
λ) we have w∗(λ, γ) = wmax, which makes log2
w(x)
w∗(λ,γ) ≤ 0 for all x. Further, for sufﬁciently
(for all c) and when a single element x ∈ {−1, 1}n has the unique largest weight (w(x) > w(y)∀x (cid:54)= y)
arg max
x∈{−1,1}n
(cid:104)c, x(cid:105) + γ log2
w(x)
w∗(λ, γ)
= arg max
x∈{−1,1}n
{w(x)}
(cid:27)
(cid:27)(cid:21)
w(x)
w∗(λ, γ)
= 0.
(cid:26)
Ec
max
x∈{−1,1}n
(cid:26)
(cid:20)
(cid:114)
(cid:26)
Therefore, for sufﬁciently large γ and λ =
2 log2
Z(w)
wmax
(cid:20)
Ec
max
x∈{−1,1}n
(cid:104)c, x(cid:105) + γ log2
(cid:104)c, x(cid:105) + γ log2
(cid:27)(cid:21)
, we have
w(x)
w∗(λ, γ)
≤ 1
log2
Z(w)
w∗(λ, γ)
+ λ
(cid:115)
0 ≤
Z(w)
2n log2
wmax
wmax ≤ Z(w).
(cid:114)
λ or γ ≤ 1
λ. Note that we have used λ =
λ requires a bound on Z(w). We leave joint optimization over λ and γ for future work.
λ or alternatively we would make γ as big as possible subject to γ ≤ 1
This bound is trivial, however we picked γ poorly. To tighten this bound we would make γ as small as possible subject to
γ ≥ 1
gauranteeing γ ≥ 1
Upper Bound
In this section we upper bound the sum Z(w) by R(w) and apply our slack bound to obtain an upper bound on Z(w) in
terms of δ(c, w). Our proof technique is inspired by (Barvinok 1997), who developed the method for bounding the sum Z(w)
of a weight function with values of either 0 or 1. We generalize the proof to the weighted setting for any weight function
w : {−1, 1}n → [0,∞). The principle underlying the method is that by dividing the space {−1, 1}n in half n times we arrive at
a single weight. By judiciously choosing which half of the space to recurse into at each step we can bound upper bound Z(w).
Deﬁne the j dimensional space Ij = {x : x ∈ {−1, 1}j} for j ≥ 1 and I0 = {(0)}. For any vector x ∈ Ij−1 with j ≥ 2,
deﬁne x+, x− ∈ Ij as
For the single element (0) ∈ I0, deﬁne (0)+ = (1) and (0)− = (−1).
x+ = (x1, x2, . . . , xj−1, 1), x− = (x1, x2, . . . , xj−1,−1).
Given the weight function wj : Ij → [0,∞) (in this section we explicitly write wj to denote that the weight function has a j
Z(w)
wmax
, so
2 log2
dimensional domain while w implicitly denotes wn), deﬁne the weight functions
j−1 : {−1, 1}j−1 → [0,∞), with w+
w+
j−1(x) = wj(x+)
and
We have split the weights of our original weight function between two new weight functions, each with j−1 dimensional domains
(two disjoint half spaces of our original j dimensional domain). Now we relate the expectation R(wj) to the expectations
R(w+
Lemma 4. For j ≥ 1 one has
j−1 : {−1, 1}j−1 → [0,∞), with w−
w−
j−1) in Lemmas 4 and 5.
j−1(x) = wj(x−)
j−1) and R(w−
R(w+
j−1),R(w−
j−1) ≤ R(wj)
Proof. Given x, c ∈ Ij−1 we have
(cid:104)c, x(cid:105) + log2 w+
j−1(x) =
((cid:104)c+, x+(cid:105) + log2 wj(x+)) + ((cid:104)c−, x+(cid:105) + log2 wj(x+))
≤ maxy∈Ij {(cid:104)c+, y(cid:105) + log2 wj(y)} + maxy∈Ij {(cid:104)c−, y(cid:105) + log2 wj(y)}
δ(c+, wj) + δ(c−, wj)
(10)
This inequality holds for any x, so we can maximize the left hand side of Equation 10 over x and get
δ(c, w+
j−1) = max
x∈Ij−1
Now we average over c ∈ Ij−1 and get
j−1(x)(cid:9) ≤ δ(c+, wj) + δ(c−, wj)
(cid:8)(cid:104)c, x(cid:105) + log2 w+
(cid:88)
(cid:88)
2j−1
c∈Ij−1
≤ 1
2j−1
R(w+
j−1) =
δ(c, w+
j−1)
δ(c+, wj) + δ(c−, wj)
(cid:88)
j−1) ≤ R(wj) follows the same structure.
c∈Ij
2j
c∈Ij−1
δ(c, wj) = R(wj)
The proof for R(w−
Lemma 5. For j ≥ 1 one has
Proof. Let c, x ∈ Ij−1. Then
R(w−
j−1) + R(w+
j−1)
≤ R(wj) − 1
(cid:104)c, x(cid:105) + log2 w+
j−1(x) = (cid:104)c+, x+(cid:105) − 1 + log2 w+
j−1(x)
= (cid:104)c+, x+(cid:105) + log2 wj(x+) − 1
≤ δ(c+, wj) − 1
The inequality (cid:104)c, x(cid:105) + log2 w+
j−1(x) ≤ δ(c+, wj) − 1 holds for any x. Maximizing over x we get
δ(c, w+
j−1) ≤ δ(c+, wj) − 1.
Similarly,
and maximizing over x we get
Therefore
(cid:104)c, x(cid:105) + log2 w−
j−1(x) = (cid:104)c−, x−(cid:105) − 1 + log2 w−
j−1(x)
= (cid:104)c−, x−(cid:105) + log2 wj(x−) − 1
≤ δ(c−, wj) − 1,
δ(c, w−
j−1) ≤ δ(c−, wj) − 1.
δ(c, w−
j−1) + δ(c, w+
j−1)
≤ δ(c−, wj) + δ(c+, wj)
− 1
and averaging over c ∈ Ij−1 we get
R(w−
j−1) + R(w+
j−1)
≤ 1
2j−1
2j−1
j−1)
c∈Ij−1
δ(c, w−
j−1) + δ(c, w+
(cid:88)
(cid:19)
(cid:18) δ(c−, wj) + δ(c+, wj)
(cid:88)
 − 1 = R(wj) − 1.
(cid:88)
δ(c, wj)
− 1
c∈Ij−1
2j
c∈Ij
j−1) and R(w−
Equipped with our relations between the expectation R(wj) and the expectations R(w+
j−1) from Lemmas 4
and 5, we are prepared to upper bound Z(wn) by R(wn). To understand our strategy it is helpful to view the weight function
wj : {−1, 1}j → [0,∞) as a binary tree with 2j leaf nodes, each corresponding to a weight. The weight functions w+
j−1 and
w−
j−1 correspond to subtrees whose root nodes are the two children of the root node in the complete tree representing wj. Our
strategy is to recursively divide the original weight function wn in half, picking one of the two subtrees based on their relative
sizes (as measured by the sum of each subtree’s weights and their weighted Rademacher complexities). Eventually we arrive at
a leaf, corresponding to a single weight. This allows us to relate the weighted Rademacher complexity of the original weight
function (or the entire tree) to Z(wn) and the weight of this single leaf. The problem of choosing which subtree to pick at each
step based on their relative sizes is computationally intractable. To avoid this difﬁculty we do not explicitly ﬁnd the leaf, but
instead conservatively pick either the largest or smallest weight in the entire tree, which gives us an upper bound on Z(wn) in
terms of R(wn). After upper bounding Z(wn) by R(wn) we apply our slack bound to obtain an upper bound on Z(wn) in terms
of δ(c, wn).
Lemma 6. Assuming Z(w) > 0, for c ∈ {−1, 1}n sampled uniformly at random, the following bound holds with probability
greater than .95:
6n − log2 wmin
6n − log2 wmax
(cid:1) ≈ δ(c, w) +
(cid:1) − n log2 (1 − βopt) + log2 wmin,
(cid:1) − n log2 (1 − βopt) + log2 wmax,
6n + .58n,
if 0 < βopt < 1
if 1
3 < βopt < 1
if βopt = 1
if βopt = 1
log2 Z(w) ≤
where

(cid:16) 1−βopt
(cid:16) 1−βopt
βopt
log2
βopt
log2
δ(c, w) +
log2 wmax + n,
6n + n log2
(cid:17)(cid:0)δ(c, w) +
(cid:17)(cid:0)δ(c, w) +
(cid:0) 3

δ(c,w)+
δ(c,w)+
2 ,
3 ,
βopt =
6n−log2 wmin
6n−log2 wmax
6n−log2 wmin
if 0 < δ(c,w)+
6n−log2 wmax
3 < δ(c,w)+
if 1
6n−log2 wmax
2 < δ(c,w)+
if 1
otherwise
< 1
< 1
Proof. Let β be a parameter, to be set later, such that 0 < β ≤ 1/2. We construct a sequence of weight functions
wn, wn−1, . . . , w1, w0 where wj : {−1, 1}j → [0,∞). Starting with our original weight function wn, we use two rules
to decide whether wj−1 = w−
j−1 or wj−1 = w+
Rule 1: Given wj, if min{(cid:80)
Rule 2: Given wj, if min{(cid:80)
y w+
y w+
j−1(y),(cid:80)
j−1(y),(cid:80)
j−1.
y w−
y w−
j−1(y)} < β(cid:80)
(cid:40)(cid:88)
j−1(y)} ≥ β(cid:80)
(cid:8)R(w+
w+
wj−1 = arg max
j−1
j−1,w
w+
wj−1 = arg min
j−1
j−1,w
w+
(cid:88)
y wj(y), we let
j−1(y),
w−
j−1(y)
y wj(y), we let
j−1),R(w−
j−1)(cid:9)
(cid:41)
Note that R(w0) = log2 wn(x) for some x ∈ In. That is, after dividing our original space with 2n states in half n times, we
are left with a single state. Now, given that Z(w) > 0, rule 1 guarantees wn(x) > 0. As proof by contradiction, assume that
wn(x) = 0. This requires that for some for some integer i (with 0 < i ≤ n) we have 0 <(cid:80)
following rule 1 makes this impossible.
y wi−1(y), but
Our ﬁrst step is to relate the weighted Rademacher complexity R(wn) to the ﬁnal leaf weight wn(x) based on the number of
y wi(y) and 0 =(cid:80)
times we use rule 2 when dividing the original tree. Every time we use rule 2
R(wj−1) = min
w+
j−1,w
j−1
{R(w+
j−1),R(w−
j−1)}.
By Lemma 5, R(wj) is at least as large as the average of R(w−
minimum of R(w−
R(wj) ≥ R(wj−1) regardless of whether we use rule 1 or 2. Let m be the number of times we have used Rule 2, then
j−1) plus one, making it at least as large as the
j−1) plus one. Therefore R(wj) ≥ R(wj−1) + 1 whenever we use rule 2. By lemma 4 we have
j−1) and R(w+
j−1) and R(w+
z∈Ij
R(w) = R(wn) ≥ R(w0) + m = log2 wn(x) + m.
Now we relate the number of times we use rule 2 to the sum Z(wn). Observe that(cid:80)
(cid:80)
wj(z); therefore if we have used rule 1 we must have(cid:80)
y wj−1(y) ≥ (1 − β)(cid:80)
of the two subtrees. If we have used rule 2 then(cid:80)
y wj−1(y) ≥ β(cid:80)
(cid:18) β
w0(y) ≥ (1 − β)n−m βm (cid:88)
subtrees both carry at least the fraction β of the total weight. Therefore
wn(y) = (1 − β)n
(cid:88)
wn(x) =
j−1(y) +(cid:80)
(cid:19)m
w−
j−1(y) =
z wj(z) because we picked the largest
z wj(z), because by deﬁnition we use rule 2 when the two
y∈Ij−1
y∈Ij−1
w+
1 − β
y∈I0
y∈In
Taking logarithms
(cid:16) 1−β
(cid:17)
Note that log2
(cid:19)
(cid:18) β
1 − β
log2 wn(x) ≥ n log2 (1 − β) + m log2
+ log2 Z(wn)
−m log2
≥ n log2 (1 − β) + log2 Z(wn) − log2 wn(x)
(cid:18) β
(cid:18) 1 − β
1 − β
(cid:19)
(cid:19)
≥ n log2 (1 − β) + log2 Z(wn) − log2 wn(x).
m log2
> 0 for 0 < β < 1/2 so
m ≥ n log2 (1 − β) + log2 Z(wn) − log2 wn(x)
(cid:16) 1−β
(cid:17)
log2
n log2 (1 − β) + log2 Z(wn) − log2 wn(x)
Z(wn).
(11)
(12)
(13)
(14)
and by combining Equations 11 and 13 we have
R(w) ≥ log2 wn(x) +
Applying the slack bound from Equation 6 we have3
log2
(cid:17)
(cid:16) 1−β
(cid:16) 1−β
(cid:17)
log2
δ(c, w) ≥ log2 wn(x) +
n log2 (1 − β) + log2 Z(wn) − log2 wn(x)
6n
with probability greater than .95.
Now we can choose β to optimize this bound. Note that the bound in Equation 14 contains the term wn(x), however it is
computationally intractable to explicitly ﬁnd this leaf following the procedure outlined above. Instead we conservatively use
either the smallest or largest weight depending on the value of β. If these weights cannot be computed we may alternatively pick
β = 1/3 to eliminate wn(x) from the bound. Depending on the value of β we have 4 cases outlined below:
1. When β = 1/3, the quantity log2
(cid:16) 1−β
(cid:17)
δ(c, w) ≥ log2 Z(wn) − n log2(3/2) −
= 1 and
6n.
3This bound is scale invariant; if we scale the weight function by a constant a so that w(cid:48)(x) = aw(x) then R(w(cid:48)) = log2 a + R(w),
log2 Z(w(cid:48)
n) = log2 a + log2 Z(wn), and log2 w(cid:48)
n(x) = log2 a + log2 wn(x) so the bound remains unchanged.
2. When 0 < β < 1
3, the quantity 1 −
log2( 1−β
β )
> 0 and
δ(c, w) ≥ n log2 (1 − β) + log2 Z(wn)
3. When 1
3 < β < .5, the quantity 1 −
log2( 1−β
β )
< 0 and
δ(c, w) ≥ n log2 (1 − β) + log2 Z(wn)
(cid:16) 1−β
(cid:17)
(cid:16) 1−β
(cid:17)
log2
log2
(cid:32)
1 −
+ log2 wmin
(cid:32)
1 −
+ log2 wmax
(cid:17)(cid:33)
(cid:17)(cid:33)
(cid:16) 1−β
(cid:16) 1−β
log2
log2
6n
6n.
4. When β = .5, the quantity 1−β
β = 0 and we recover the trivial bound log2 Z(wn) ≤ log2 wmax + n from equation 12.
To choose the best value for β we minimize the upper bound on log2 Z(wn) with respect to β. The upper bound on log2 Z(wn)
is
(cid:18) 1 − β
(cid:19)(cid:16)
δ(c, w) +
(cid:17) − n log2 (1 − β) + log2 w∗(β)
(cid:19)
6n − log2 w∗(β)
(cid:18) 1 − β
− n log2 (1 − β) + log2 w∗(β),
= a log2
log2 Z(wn) ≤ L(β) = log2
with
3. Differentiating we
a = δ(c, w) +
where w∗(β) = wmin = minx{w(x) : w(x) > 0} for β < 1
get
(cid:16)− 1−β
L(cid:48)(β) =
(cid:17)
β2 − 1
(1 − β)2
The ﬁrst derivative has a root at β = a
L(cid:48)(cid:48)(β) =
aβ
(cid:16)− 1−β
(cid:16)− 1−β
6n − log2 w∗(β),
(cid:17)
3 and w∗(β) = wmax = maxx w(x) for β > 1
β2 − 1
(cid:17)
1 − β
β2 − 1
1 − β
n ) = n3
(cid:16) 2(1−β)
β3 + 2
β2
1 − β
(1 − β)2
1 − β
(cid:17)
n, with L(cid:48)(cid:48)( a
a(n−a). By deﬁnition the only meaningful values of β are
0 < β ≤ .5, so either 0 < a < n/2 and the second derivative is positive making this root a minimum or a
n is outside our
valid range for β and the minimum occurs at an endpoint of the range. Note that for a > 0 we have limβ→0 L(β) = ∞,
so the minimum never occurs at the endpoint β = 0 when a > 0. If a ≤ 0 then our slack bound has been violated and
we have estimated R(w) poorly with δ(c, w), so it is appropriate to sample a new c and recompute δ(c, w). (To see this,
note that R(w) = Ec
R(w) − log2 wmin = n > 0.) Also, lim→0 L( 1
3 − ) (at β = 1/3 the term w∗(β) doesn’t appear in the
bound so it doesn’t matter whether w∗(β) = wmin or w∗(β) = wmax). Assuming we have sampled c such that a > 0, this
means the optimal value of β that minimizes our upper bound is:
(cid:2)maxx∈{−1,1}n{(cid:104)c, x(cid:105) + log2 wmin}(cid:3) = n + log2 wmin, so
(cid:2)maxx∈{−1,1}n{(cid:104)c, x(cid:105) + log2 w(x)}(cid:3) ≥ Ec
3 + ) = lim→0 L( 1
δ(c,w)+
δ(c,w)+

(cid:17)(cid:0)δ(c, w) +
(cid:17)(cid:0)δ(c, w) +
(cid:0) 3
6n + n log2
6n−log2 wmin
6n−log2 wmax
6n−log2 wmin
if 0 < δ(c,w)+
6n−log2 wmax
3 < δ(c,w)+
if 1
6n−log2 wmax
2 < δ(c,w)+
if 1
otherwise
< 1
< 1
6n − log2 wmin
6n − log2 wmax
(cid:1) ≈ δ(c, w) +
(cid:1) − n log2 (1 − βopt) + log2 wmin,
(cid:1) − n log2 (1 − βopt) + log2 wmax,
6n + .58n,
βopt =
2 ,
3 ,
Our upper bound on Z(wn) is given by

(cid:16) 1−βopt
(cid:16) 1−βopt
βopt
log2
βopt
log2
δ(c, w) +
log2 wmax + n,
log2 Z(wn) ≤
if 0 < βopt < 1
if 1
3 < βopt < 1
if βopt = 1
if βopt = 1
Tightening the Slack Bound We can improve our high probability bounds on δ(c, w) in terms of Z(w) by generating k
independent vectors c1, c2, . . . , ck ∈ {−1, 1}n, applying the optimization oracle from Assumption 1 to each, and taking the
mean ¯δk(w) = (δ(c1, w) + ··· + δ(ck, w))/k. This gives the bounds
log2 w∗(β)+
n log2 (1 − β) + log2 Z(w) − log2 w∗(β)
log2 w∗(λ, γ)+λ
(cid:114) 6n
(cid:114) 6n
log2 Z(w)+
λγ − 1
≤ ¯δk(w) ≤ 1
In the last two sections we inverted these bounds and optimized over β and λ to obtain high probability bounds on Z(w) in terms
of δ(c, w). This process is unchanged, with the exceptions that we replace δ(w, c) (computed from a single c ∈ {−1, 1}n) with
¯δk(w) and the term
w(cid:48)(c1, ..., ck) = w(c1) × ··· × w(ck) for c1, . . . , ck ∈ {−1, 1}n. The sum of this new function’s weights is
Proof: recall the weight function w : {−1, 1}n → [0,∞). Let’s deﬁne a new weight function w(cid:48) : {−1, 1}nk → [0,∞) as
6n with
k .
log2
(cid:17)
(cid:16) 1−β
(cid:113) 6n
(cid:88)
w(cid:48)(x)
(cid:88)
(cid:88)
x1∈{−1,1}n
Z(w(cid:48)) =
··· (cid:88)
w(x1) × ··· × (cid:88)
xk∈{−1,1}n
x∈{−1,1}kn
w(x1) × ··· × w(xk)
w(xk)
xk∈{−1,1}n
x1∈{−1,1}n
Also note that the largest and smallest non-zero weights in w(cid:48) are w(cid:48)
c(cid:48) = (c1, .., ck). The value δ(w(cid:48), c(cid:48)) for our new weight function is now
max = wk
= Z(w)k.
max and w(cid:48)
min = wk
min. Deﬁne c(cid:48) ∈ {−1, 1}nk as
δ(w(cid:48), c(cid:48)) =
max
{log2 w(x1) + (cid:104)c1, x1(cid:105)} + ··· + max
x(cid:48)∈{−1,1}nk
xk∈{−1,1}n
= max
x1∈{−1,1}n
{log2 w(cid:48)(x(cid:48)) + (cid:104)c(cid:48), x(cid:48)(cid:105)}
{log2 w(xk) + (cid:104)ck, xk(cid:105)}
= k¯δk(w).
We lower bound ¯δk(w) by applying the bound from Equation 14 to w(cid:48) (recalling that either w∗(β) = wmin or w∗(β) = wmax)
and ﬁnd that
¯δk(w) =
δ(w(cid:48), c(cid:48))
≥ log2 w∗(β)k
nk log2 (1 − β) + log2 Z(w)k − log2 w∗(β)k
= log2 w∗(β) +
n log2 (1 − β) + log2 Z(w) − log2 w∗(β)
k log2
(cid:17)
(cid:16) 1−β
(cid:17)
(cid:16) 1−β
log2
Similarly, for the upper bound on ¯δk(w) we apply the bound from Equation 8 to w(cid:48) and ﬁnd that
nk
log2 w∗(λ, γ) + λ
log2 w∗(λ, γ)k + λ
λγ − 1
log2 Z(w)k +
log2 Z(w) +
δ(w(cid:48), c(cid:48))
λγ − 1
¯δk(w) =
≤ 1
(cid:18) 1
6nk
(cid:114) 6n
(cid:19)
(cid:114) 6n
6nk
