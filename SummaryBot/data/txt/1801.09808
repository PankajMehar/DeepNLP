The Intriguing Properties of Model Explanations
Maruan Al-Shedivat
Carnegie Mellon University
alshedivat@cs.cmu.edu
Avinava Dubey
Carnegie Mellon University
akdubey@cs.cmu.edu
Eric P. Xing
Carnegie Mellon University
epxing@cs.cmu.edu
Abstract
Linear approximations to the decision boundary of a complex model have become
one of the most popular tools for interpreting predictions. In this paper, we study
such linear explanations produced either post-hoc by a few recent methods or
generated along with predictions with contextual explanation networks (CENs).
We focus on two questions: (i) whether linear explanations are always consistent or
can be misleading, and (ii) when integrated into the prediction process, whether and
how explanations affect performance of the model. Our analysis sheds more light
on certain properties of explanations produced by different methods and suggests
that learning models that explain and predict jointly is often advantageous.
Introduction
Model interpretability is a long-standing problem in machine learning that has become quite acute with
the accelerating pace of widespread adoption of complex predictive algorithms. There are multiple
approaches to interpreting models and their predictions ranging from a variety of visualization
techniques [1–3] to explanations by example [4, 5]. The approach that we consider in this paper
thinks of explanations as models themselves that approximate the decision boundary of the original
predictor but belong to a class that is signiﬁcantly simpler (e.g., local linear approximations).
Explanations can be generated either post-hoc or alongside predictions. A popular method, called
LIME [6], takes the ﬁrst approach and attempts to explain predictions of an arbitrary model by
searching for linear local approximations of the decision boundary. On the other hand, recently
proposed contextual explanation networks (CENs) [7] incorporate a similar mechanism directly into
deep neural networks of arbitrary architecture and learn to predict and to explain jointly. Here, we
focus on analyzing a few properties of the explanations generated by LIME, its variations, and CEN.
In particular, we seek answers to the following questions:
1. Explanations are as good as the features they use to explain predictions. We ask whether and
how feature selection and feature noise affect consistency of explanations.
2. When explanation is a part of the learning and prediction process, how does that affect perfor-
mance of the predictive model?
3. Finally, what kind of insight we can gain by visualizing and inspecting explanations?
2 Methods
We start with a brief overview of the methods compared in this paper: LIME [6] and CENs [7]. Given
a dataset of inputs, x ∈ X , and targets, y ∈ Y, our goal is to learn a predictive model, f : X (cid:55)→ Y. To
explain each prediction, we have access to another set of features, z ∈ Z, and construct explanations,
gx : Z (cid:55)→ Y, such that they are consistent with the original model, gx(z) = f (x). These additional
features, z, are assumed to be more interpretable than x, and are called interpretable representation
in [6] and attributes in [7].
Interpretable ML Symposium, 31st Conference on Neural Information Processing Systems (NIPS 2017), Long
Beach, CA, USA.
2.1 LIME and Variations
Given a trained model, f, and an instance with features (x, z), LIME constructs an explanation, gx,
as follows:
gx = argmin
g∈G L(f, g, πx) + Ω(g)
(1)
where L(f, g, πx) is the loss that measures how well g approximates f in the neighborhood deﬁned
by the similarity kernel, πx : Z (cid:55)→ R+, in the space of additional features, Z, and Ω(g) is the penalty
on the complexity of explanation. Now more speciﬁcally, Ribeiro et al. [6] assume that G is the class
of linear models:
(2)
and deﬁne the loss and the similarity kernel as follows:
gx(z) := bx + wx · z
πx(z(cid:48)) := exp(cid:8)
−D(z, z(cid:48))2/σ2(cid:9)
(3)
L(f, g, πx) :=
πx(z(cid:48)) (f (x(cid:48)) − g(z(cid:48)))2 ,
where the data instance is represented by (x, z), z(cid:48) and the corresponding x(cid:48) are the perturbed features,
D(z, z(cid:48)) is some distance function, and σ is the scale parameter of the kernel. Ω(g) is further chosen
to favor sparsity of explanations.
(cid:88)
z(cid:48)∈Z
2.2 Contextual Explanation Networks
K(cid:88)
k=1
LIME is a post-hoc model explanation method. This means that it justiﬁes model predictions by
producing explanations which, while locally correct, are never used to make the predictions in the ﬁrst
place. Contrary to that, CENs use explanations as the integral part of the learning process and make
predictions by applying generated explanations. Now more formally, CENs construct the predictive
model f : X ×Z (cid:55)→ Y via a composition: given x, an encoder, eθ : X (cid:55)→ G, produces an explanation
g which is further applied to z to make a prediction. In other words:
(4)
f (x, z) := gx(z), where gx := eθ(x)
In [7] we introduced a more general probabilistic framework that allows to combine different
deterministic and probabilistic encoders with explanations represented by arbitrary graphical models.
To keep our discussion simple and concrete, here we assume that explanations take the same linear
form (2) as for LIME and the encoder maps x to (bx, wx) as follows:
bx := αθ(x)(cid:62)B, wx := αθ(x)(cid:62)W, where
α(k)
θ (x) = 1,∀k : α(k)
θ (x) ≥ 0
(5)
In other words, explanation (bx, wx) is constrained to be a convex combination of K components
from a global learnable dictionary, D := (B, W ), where the combination weights, αθ(x), also called
attention, are produced by a deep network. Encoder of such form is called constrained deterministic
map in [7] and the model is trained jointly w.r.t. (θ, B, W ) to minimize the prediction error.
3 Analysis
Both LIME and CEN produce explanations in the form of linear models that can be further used
for prediction diagnostics. Our goal is to understand how different conditions affect explanations
generated by both methods, see whether this may lead to erroneous conclusions, and ﬁnally understand
how jointly learning to predict and to explain affects performance.
We use the following 3 tasks in our analysis: MNIST image classiﬁcation1, sentiment classiﬁcation
of the IMDB reviews [8], and poverty prediction for households in Uganda from satellite imagery
and survey data [9]. The details of the setup are omitted in the interest of space but can be found
in [7], as we follow exactly the same setup.
3.1 Consistency of Explanations
Linear explanation assign weights to the interpretable features, z, and hence strongly depend their
quality and the way we select them. We consider two cases where (a) the features are corrupted with
additive noise, and (b) selected features are incomplete. For analysis, we use MNIST and IMDB data.
1http://yann.lecun.com/exdb/mnist/
(a)
(b)
Fig. 1: The effect of feature quality on explanations. (a) Explanation test error vs. the level of the noise added to
the interpretable features. (b) Explanation test error vs. the total number of interpretable features.
We train baseline deep architectures (CNN on MNIST and LSTM on IMDB) and their CEN variants.
For MNIST, z is either pixels of a scaled down image (pxl) or HOG features (hog). For IMDB, z is
either a bag of words (bow) or a topic vector (tpc) produced by a pre-trained topic model.
The effect of noisy features. In this experiment, we inject noise2 into the features z and ask LIME
and CEN to ﬁt explanations to the noisy features. The predictive performance of the produced
explanations on noisy features is given on Fig. 1a. Note that after injecting noise, each data point has
a noiseless representation x and noisy ˜z. Since baselines take only x as inputs, their performance
stays the same and, regardless of the noise level, LIME “successfully” overﬁts explanations—it is
able to almost perfectly approximate the decision boundary of the baselines using very noisy features.
On the other hand, performance of CEN gets worse with the increasing noise level indicating that
model fails to learn when the selected interpretable representation is low quality.
The effect of feature selection. Here, we use the same setup, but instead of injecting noise into z,
we construct ˜z by randomly subsampling a set of dimensions. Fig. 1b demonstrates the result. While
performance of CENs degrades proportionally to the size of ˜z, we see that, again, LIME is able to ﬁt
explanations to the decision boundary of the original models despite the loss of information.
These two experiments indicate a major drawback of explaining predictions post-hoc: when con-
structed on poor, noisy, or incomplete features, such explanations can overﬁt the decision boundary of
a predictor and are likely to be misleading. For example, predictions of a perfectly valid model might
end up getting absurd explanations which is unacceptable from the decision support point of view.
3.2 Explanations as a Regularizer
In this part, we compare CENs with baselines in terms of performance. In each task, CENs are
trained to simultaneously generate predictions and construct explanations. Overall, CENs show very
competitive performance and are able to approach or surpass baselines in a number of cases, especially
on the IMDB data (see Table 1). This suggests that forcing the model to produce explanations along
with predictions does not limit its capacity.
(a)
(b)
Fig. 2: (a) Training error vs. iteration (epoch or batch) for baselines and CENs. (b) Validation error for models
trained on random subsets of data of different sizes.
Additionally, the “explanation layer” in CENs affects the geometry of the optimization problem and
causes faster and better convergence (Fig. 2a). Finally, we train the models on subsets of data (the
size varied from 1% to 20% for MNIST and from 2% to 40% for IMDB) and notice that explanations
play the role of a regularizer which strongly improves the sample complexity (Fig. 2b).
2We use Gaussian noise with zero mean and select variance for each signal-to-noise ratio level appropriately.
−30−20−10010SNR,dB141664Testerror(%)MNISTCNNLIME-pxlCEN-pxlCEN-hog−20−10010SNR,dB8163264IMDBLSTMLIME-bowCEN-bowCEN-tpc050100Featuresubsetsize(%)050100Testerror(%)MNISTCNNLIME-pxlCEN-pxlCEN-hog050100Featuresubsetsize(%)2040IMDBLSTMLIME-bowCEN-bowCEN-tpc01020Epochnumber0.250.51.0Trainerror(%)MNISTCNNCEN-pxlCEN-hog05001000Batchnumber204060IMDBLSTMCEN-tpc51015Trainingsetsize(%)510Validationerror(%)MNISTCNNCEN-pxlCEN-hog2040Trainingsetsize(%)10203040IMDBLSTMCEN-bowCEN-tpcTable 1: Performance of the models on classiﬁcation tasks (averaged over 5 runs; the std. are on the order of the
least signiﬁcant digit). The subscripts denote the features on which the linear models are built: pixels (pxl),
HOG (hog), bag-or-words (bow), topics (tpc), embeddings (emb), discrete attributes (att).
MNIST
IMDB
Satellite
Model Err (%) Model Err (%) Model Acc (%) AUC (%)
LRpxl
LRhog
CNN
MoEpxl
MoEhog
CENpxl
CENhog
8.00 LRbow
2.98 LRtpc
0.75 LSTM
1.23 MoEbow
1.10 MoEtpc
0.76 CENbow
0.73 CENtpc
13.3 LRemb
17.1 LRatt
13.2 MLP
13.9 MoE
12.2 CEN
(cid:63)6.9 VCEN
(cid:63)7.8
62.5
75.7
77.4
77.9
81.5
83.4
68.1
82.2
78.7
85.4
84.2
84.6
(cid:63)Best previous results for similar LSTMs: 8.1% (supervised) and 6.6% (semi-supervised) [10].
3.3 Visualizing Explanations
Finally, we showcase the insights one can get from explanations produced along with predictions.
Particularly, we consider the problem of poverty prediction for household clusters in a Uganda from
satellite imagery and survey data. The x representation of each household cluster is a collection
of 400 × 400 satellite images; z is represented by a vector of 65 categorical features from living
standards measurement survey (LSMS). The goal is binary classiﬁcation of households in Uganda
into poor and not poor. In our methodology, we closely follow the original study of Jean et al. [9] and
use a pretrained VGG-F network for embedding the images into a 4096-dimensional space on top of
which we build our contextual models. Note that this datasets is fairly small (642 points), and hence
we keep the VGG-F frozen to avoid overﬁtting. We note that quantitatively, by conditioning on the
VGG features of the satellite imagery, CENs are able to signiﬁcantly improve upon the sparse linear
models on the survey features only (known as the gold standard in remote sensing techniques).
After training CEN with a dictionary of size 32, we discover that the encoder tends to sharply select
one of the two explanations (M1 and M2) for different household clusters in Uganda (see Fig. 3a
and also Fig. 4a in appendix). In the survey data, each household cluster is marked as either urban
or rural; we notice that, conditional on a satellite image, CEN tends to pick M1 for urban areas
and M2 for rural (Fig. 3b). Notice that explanations weigh different categorical features, such as
reliability of the water source or the proportion of houses with walls made of unburnt brick, quite
differently. When visualized on the map, we see that CEN selects M1 more frequently around the
major city areas, which also correlates with high nightlight intensity in those areas (Fig. 3c,3d). High
performance of the model makes us conﬁdent in the produced explanations (contrary to LIME as
discussed in Sec. 3.1) and allows us to draw conclusions about what causes the model to classify
certain households in different neighborhoods as poor.
(a)
(b)
(c)
(d)
Fig. 3: Qualitative results for the Satellite dataset: (a) Weights given to a subset of features by the two models
(M1 and M2) discovered by CEN. (b) How frequently M1 and M2 are selected for areas marked rural or urban
(top) and the average proportion of Tenement-type households in an urban/rural area for which M1 or M2 was
selected. (c) M1 and M2 models selected for different areas on the Uganda map. M1 tends to be selected for
more urbanized areas while M2 is picked for the rest. (d) Nightlight intensity of different areas of Uganda.
M1M2Water:UnreliableWatersrc:PublictapWalls:UnburntbricksRoof:Thatch,StrawIswaterpayedVegetationHaselectricityNightlightintensity0.9-0.4-0.6-1.20.3-0.20.50.2-0.3-0.3-0.10.4-0.2-0.8-0.7-0.7−0.8−0.40.00.40.80.30.40.50.60.7Timesmodelselected(%)M1M2RuralUrban0.10.20.30.4HHtype:Tenement(%)M1M2AruaGuluKampala(capital)IgangaMasakaKaseseUganda:ContextualModelsM1M2AruaGuluKampala(capital)IgangaMasakaKaseseUganda:NightlightIntensity0%100%References
[1] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks:
Visualising image classiﬁcation models and saliency maps. arXiv preprint arXiv:1312.6034,
2013.
[2] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding
neural networks through deep visualization. arXiv preprint arXiv:1506.06579, 2015.
[3] Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by invert-
ing them. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 5188–5196, 2015.
[4] Rich Caruana, Hooshang Kangarloo, JD Dionisio, Usha Sinha, and David Johnson. Case-based
explanation of non-case-based learning methods. In Proceedings of the AMIA Symposium, page
212, 1999.
[5] Been Kim, Cynthia Rudin, and Julie A Shah. The bayesian case model: A generative approach
for case-based reasoning and prototype classiﬁcation. In Advances in Neural Information
Processing Systems, pages 1952–1960, 2014.
[6] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why Should I Trust You?: Explaining
the predictions of any classiﬁer. In Proceedings of the 22nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, pages 1135–1144. ACM, 2016.
[7] Maruan Al-Shedivat, Avinava Dubey, and Eric P Xing. Contextual explanation networks. arXiv
preprint arXiv:1705.10301, 2017.
[8] Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher
Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Human Language Technologies-Volume 1,
pages 142–150. Association for Computational Linguistics, 2011.
[9] Neal Jean, Marshall Burke, Michael Xie, W Matthew Davis, David B Lobell, and Stefano
Ermon. Combining satellite imagery and machine learning to predict poverty. Science, 353
(6301):790–794, 2016.
[10] Rie Johnson and Tong Zhang. Supervised and semi-supervised text categorization using lstm for
region embeddings. In Proceedings of The 33rd International Conference on Machine Learning,
pages 526–534, 2016.
A Appendix
(a) Full visualization of explanations M1 and M2 learned by CEN on the poverty prediction task.
(b) Correlation between the selected explanation and the value of a particular survey variable.
Fig. 4: Additional visualizations for the poverty prediction task.
B Details on Consistency of Explanations
We provide a detailed description of the experimental setup used for our analysis in Section 3.1.
M1M216HHtype:BQ15Iswaterpayed14Waterusagep/day13Dist.towatersrc.12Num.ofrooms11Avg.dist.toroad10Avg.dist.tomarket09Avg.vegetationdec.08Avg.vegetationinc.07Vegetation06Avg.percipitation05Avg.temperature04Hasgenerator03Haselectricity02Isurban01Nightlightintensity-0.9-0.7-0.3-0.3-0.10.10.30.4-0.3-0.10.40.40.10.00.10.2-0.4-0.2-0.10.4-0.20.3-0.0-0.1-0.10.1-0.2-0.8-0.0-0.6-0.7-0.7M1M232Roof:Wood,Planks31Roof:Tin30Roof:Tiles29Roof:Thatch,Straw28Roof:Other27Roof:Mud26Roof:Ironsheets25Roof:Concrete24Roof:Asbestos23HHtype:Uniport22HHtype:Tenement21HHtype:Sharedhouse20HHtype:Other19HHtype:Privatehouse18HHtype:Privateapt17HHtype:Hut-0.7-0.4-0.20.0-0.6-0.30.50.20.30.50.60.5-0.5-0.4-0.5-0.3-0.1-0.3-0.0-0.0-0.6-0.6-0.6-0.7-0.5-0.6-0.2-0.3-0.4-0.70.30.5M1M248Floor:Stone47Floor:Other46Floor:Mosaic/tiles45Floor:Cowdung44Floor:Earth43Floor:Cement42Floor:Bricks41Walls:Stone40Walls:Unburntbricks39Walls:Timber38Walls:Thatch,Straw37Walls:Other36Walls:Mud,poles35Walls:Cementblocks34Walls:Brickw/mud33Walls:Brickw/cement-0.9-1.1-0.5-0.5-0.9-0.90.30.40.3-0.1-0.8-0.8-0.00.0-0.30.60.3-0.20.0-0.9-0.20.4-0.10.10.30.4-0.7-1.0-0.3-0.3-0.6-0.7M1M264Water:Unreliable63Water:Contribution62Water:Badtaste61Water:Unprotect.OK60Water:Longqueues59Water:Faraway58Watersrc:Vendortruck57Watersrc:Unprotectedwell56Watersrc:River/lake/pond55Watersrc:Rainwater54Watersrc:Publictap53Watersrc:Protectedwell52Watersrc:Privatetap51Watersrc:Other50Watersrc:Gravityﬂow49Watersrc:Bore-hole0.9-0.40.3-0.9-0.9-0.4-0.50.2-0.6-0.30.00.2-0.9-0.30.10.3-0.70.2-0.8-0.9-0.6-1.2-0.1-0.1-1.1-1.0-0.8-0.80.70.80.40.2−0.8−0.40.00.40.80123456789101112131415161718192021222324252627282930310.00.1Correlation323334353637383940414243444546474849505152535455565758596061626364Featurenumber−0.050.000.050.10Correlation-0.10.1
