Sensitivity Sampling Over Dynamic Geometric Data Streams
with Applications to k-Clustering
Zhao Song∗
Lin F. Yang∗
zhaos@g.harvard.edu
Harvard University & UT-Austin
lin.yang@princeton.edu
Princeton University
Peilin Zhong∗
peilin.zhong@columbia.edu
Columbia University
Abstract
Sensitivity based sampling is crucial for constructing nearly-optimal coreset for k-means /
median clustering.
In this paper, we provide a novel data structure that enables sensitivity
sampling over a dynamic data stream, where points from a high dimensional discrete Euclidean
space can be either inserted or deleted. Based on this data structure, we provide a one-pass
coreset construction for k-means clustering using space (cid:101)O(k poly(d)) over d-dimensional geo-
metric dynamic data streams. While previous best known result is only for k-median[BFL+17],
which cannot be directly generalized to k-means to obtain algorithms with space nearly linear
in k. To the best of our knowledge, our algorithm is the ﬁrst dynamic geometric data stream
algorithm for k-means using space polynomial in dimension and nearly optimal in k.
We further show that our data structure for maintaining coreset can be extended as a uniﬁed
approach for a more general classes of k-clustering, including k-median, M-estimator clustering,
and clusterings with a more general set of cost functions over distances. For all these tasks, the
space/time of our algorithm is similar to k-means with only poly(d) factor diﬀerence.
∗Work done while the author was visiting IBM Almaden and hosted by David P. Woodruﬀ.
Contents
1 Introduction
1.1 Related Works
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 Problem Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 Our Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.4 Our Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
(cid:101)O(k2)-Space Algorithm with Chen’ Framework . . . . . . . . . . . . . . . . .
(cid:101)O(k)-Space Algorithm (Nearly Optimal) with Sensitivity-based Sampling . .
1.4.1
1.4.2
1.4.3 Max-Cut and Average-Distance . . . . . . . . . . . . . . . . . . . . . . . . . .
1.5 Meta Algorithms and a Roadmap . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.6 Concluding Remark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.7 Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
References
A Notation
B Preliminaries
B.1 Deﬁnitions of Dynamic Streaming Model for k-Clustering Problems . . . . . . . . . .
B.2 Deﬁnitions of k-means Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B.3 Deﬁnitions of M-estimator Clustering . . . . . . . . . . . . . . . . . . . . . . . . . .
B.4 Basic Probability Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B.5 Tools from Previous Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
C Why Do Previous Techniques Fail?
D Coreset Construction for k-means Based on Chen’s Framework
D.1 Deﬁnitions and Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
D.2 Recursive Partition Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
D.3 Bounding the Close Parts
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
D.4 Bounding the Far Parts
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
D.5 Our Coreset Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
E Coreset Over the Dynamic Data Stream
E.1 Streaming Coreset Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
E.2 The Dynamic Point-Cell Storing Data Structure . . . . . . . . . . . . . . . . . . . . .
E.3 Main Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
F General Clustering Problem
F.1 M-Estimator Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
F.2 Improvements Over k-median . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
G Applications
G.1 A Dynamic Streaming Approximation to Max-CUT . . . . . . . . . . . . . . . . . . .
G.2 A Dynamic Streaming Approximation to Average Distance . . . . . . . . . . . . . . .
10
12
12
13
13
14
20
20
20
20
21
21
22
22
23
24
26
28
30
33
35
35
36
38
43
43
45
45
46
46
H (cid:101)O(k) Space Algorithm Based on Sensitivity Sampling
H.1 Deﬁnitions and Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
H.2 Reducing Original Problem to Important Points . . . . . . . . . . . . . . . . . . . . .
H.3 Sampling Scores of Important Points . . . . . . . . . . . . . . . . . . . . . . . . . . .
H.4 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
47
47
49
51
1
Introduction
Clustering is one of the central problems in modern research of computation, including algorithmic
design, machine learning and many more. Among all the clustering methods, k-means is one of the
most important approaches for clustering geometric datasets. The ﬁrst k-means algorithm dates
back to the 1950s. Over the last half of the century, varies works have studied this problem (see
[Jai10] for a complete survey). It also inspired tremendous other variants of clustering methods,
e.g. aﬃnity propagation, mean-shift,spectral clustering, mini batch k-means and many more (e.g.,
[Scu10, CM02, SM00, MS01, NJW02, VL07]). This problem has also been studied in diﬀerent
settings and computational models, e.g. distributed computing, parallel computing, map-reduce,
streaming setting and even quantum computing. k-means clustering has been successfully used in
various research topics, e.g., database, data-mining, computer vision, geostatistics, agriculture, and
astrophysics (e.g., [SM00, GKL+17, LIY+15, MS01]).
In 2004, Indyk [Ind04] introduced the model for dynamic geometric data streams, in which a set
of geometric points from a d-dimensional discrete space [∆]d are updated dynamically, for some large
∆, i.e., the data stream is of the form insert(p) and delete(p) for p ∈ [∆]d. For a ﬁxed dimension
d, Frahling and Sohler [FS05] develop the ﬁrst eﬃcient streaming (1 + )-approximation algorithms
for k-means, k-median, and as well as other geometric problems over dynamic data streams. In
their paper, they propose an algorithm to maintain a coreset of size k−O(d) for obtaining a (1 ± )
approximation of k-clustering, including k-means and k-median. A coreset for clustering is a small
weighted point set that summarizes the original dataset. Solving the clustering problem over the
coreset provides an approximate solution for the original problem. It is one of the fundamental tool
for solving k-means clustering problem.
A very recent work by Braverman, Frahling, Lang, Sohler & Yang [BFL+17] provides a data
structure for maintaining a coreset for k-median in dynamic stream using space polynomial in d.
However their technique cannot be extended to k-means, since the technique relies heavily on the
p∈P dist(p, C), where dist(·,·)
is the Euclidean metric and C is a set of points. Later we show a hard example (in Section C) that
the error based on their design is unbounded for k-means, which is a form of sum of squares, i.e.,
p dist2(p, C). In this paper, we ﬁrst show that by combining the coreset framework of [Che09]
together with the grid structure constructed in [BFL+17] gives a data structure for maintaining a
k-means coreset over dynamic stream that use space polynomial in d. Such a data structure uses
fact that k-median cost function is a form of sum of absolutes, i.e.,(cid:80)
(cid:80)
space (cid:101)O(k2 poly(d)). Although the space is not optimal in k, but it is already the ﬁrst (to the best
of our knowledge) of its kind obtaining space polynomial in d.
To obtain an algorithm with space nearly optimal in k (i.e., linear in k), new idea has to
be introduced.
In 2011, [FL11] introduced a revolutionary coreset framework for constructing
coreset in batch-setting and insertion-only stream. Their coreset framework is by sampling data
points based on the “sensitivity” of each point. It is deﬁned as the maximum percentage change of
the cost function over all possible clustering solutions after removing the point from the dataset.
Very recently, [BFL16] improves upon this framework and gives a better insertion-only streaming
algorithm. One of our major contributions is to show that the sensitivity-based sampling scheme
is achievable even in dynamic update streams. Hence we obtain another algorithm that is not only
space nearly optimal in k but also polynomial in dimension d.
We further show that our data structure for maintaining coreset can be extended as a uniﬁed
approach for a more general classes of k-clustering, including k-median, M-estimator clustering,
and clusterings with a more general set of cost functions over a distance. For all these tasks, the
space/time of our algorithm is similar to k-means with only a poly(d) factor diﬀerence. Here an M-
estimator represents a speciﬁc function over the distance, e.g., the Huber norm [Hub64] is speciﬁed
by a parameter τ > 0, and its measure function H is given by H(x) = x2/2τ, if |x| ≤ τ and
H(x) = |x| − τ /2 otherwise. Recently, M-estimators attracted interests in a variety of computing
ﬁelds (e.g., [NYWR09, CW15b, CW15a, SWZ17b]).
1.1 Related Works
It is well known that ﬁnding the optimal solution for k-means is NP-hard even for k = 2 or
d = 2 [DFK+04, ADHP09, MNV09, Vat09]. The most success algorithm used in practice is Lloyd’s
algorithm, which is also known as “the” k-means method [Llo82]. Since k-means can’t be solved
in polynomial time, there has several works trying to understand the “local search method” for
k-means. Kanungo et al.
[KMN+02] proved that a very simple local search heuristic is able to
obtain a polynomial-time algorithm with approximation ratio 9 +  for any ﬁxed  > 0 for k-
means in Euclidean space. Very recently, two groups improved the ratio to 1 +  independently,
Friggstad, Rezapour and Salavatipour [FRS16] showed that, for any error parameter  > 0, the
local search algorithm that considers swaps of up to dO(d) · (1/)O(d/) centers at a time will produce
a solution using exactly k centers whose cost is at most (1 + )-factor greater than the optimum
solution. Cohen-Addad, Klein and Mathieu [CAKM16] proved that the number of swapped centers
is poly(1/). A large number of works [IKI94, Mat00, BHPI02, DLVKKR03, HPM04] proposed
(1 + )-approximation algorithm with ineﬃcient running time in Rd.
There is a line of works targeting insertion-only streaming k-means or k-median. For example,
[BS80, GMMO00, COP03, BDMO03, AHPV04, HPM04, HPK05, Che09, FL11, FS12, AMR+12,
BFL16] and many others have developed and improved streaming algorithms for computing a so-
lution for k-means and k-median approximately. Recent years, there have been lots of interest in
dynamic streaming algorithms for other settings, e.g. [BYJK+02, FKM+05, Bas08, KL11, AGM12a,
AGM12b, GKP12, GKK12, AGM12b, BKS12, CMS13, CMS13, AGM13, McG14, BGS15, BHNT15,
BS16, ACD+16, ADK+16, BWZ16, KLM+17, SWZ17a, SWZ17b].
In addition, there are many
works related to k-median or k-means in diﬀerent settings, e.g., [IP11, BIP+16, BCMN14, CCGG98].
1.2 Problem Setting
In this section, we formally deﬁne the problem of interests. We consider about the streaming setting,
where the space of an algorithm is limited, i.e., we cannot store all the input points. In the dynamic
data stream setting, we allow points to be deleted. Formally, let Q denote a set of points on high
dimensional grids [∆]d1, initialized as an empty set. At time t, we observe a tuple (pt, opt) where
pt ∈ [∆]d and opt ∈ {insertion, deletion} meaning we insert a point to or delete a point from Q.
Note that some of the points we observed in the stream might not belong to Q at the end of the
stream since deletion is allowed. After one-pass of the data-stream, we want to output a small
multi-set of points (i.e., of size o(|Q|)) S (associated with some weight-function w to each point)
which summarizes the the important properties of the ground-truth points set Q. Also we require
our algorithm to have a ﬁxed memory budget at any update time. This rules out the naïve approach
of storing Q in memory explicitly. Formally speaking, we require S to be an  k-means coreset for
Q, which satisﬁes,
∀Z ⊂ [∆]d,|Z| = k : (1 − ) cost(Q, Z) ≤ costw(S, Z) ≤ (1 + ) cost(Q, Z),
1We restrict the setting to be on the integer grid for the sake of representation simplicity. Our algorithm and
analysis can be trivially extended to non-integer grid setting.
where
(cid:88)
p∈Q
min
z∈Z
cost(Q, Z) =
dist2(p, z) and costw(S, Z) =
(cid:88)
p∈Q
wp dist2(p, z)
min
z∈Z
⊂ [∆]d with |C∗
| = k such that the function cost(C) :=(cid:80)
For other diﬀerent clustering objectives, e.g., M-estimator clustering, our problem setting is roughly
the same except the dist2(·) function is changed to the corresponding function.
1.3 Our Results
k-Means For a discrete geometric point set P ⊂ [∆]d, the k-means problem is to ﬁnd a set
C∗
p∈P dist2(p, C) is minimized, where
dist(p, C) = minc∈C dist(p, c) describing the minimal distance from p to a point in C. We de-
velop the ﬁrst (1 + )-approximation algorithm for the k-means clustering problem in dynamic data
streams that uses space polynomial in the dimension d. To the best of our knowledge, all previ-
ous algorithms for k-means in the dynamic streaming setting required space exponentially in the
dimension. Formally, our main theorem states,
Theorem 1.1 (k-means). Fix , δ ∈ (0, 1/2), k, ∆ ∈ N+, let L = log ∆. There is a data struc-
ture supporting insertions and deletions of a point set Q ⊂ [∆]d, maintaining a weighted set S
with positive weights for each point, such that with probability at least 1 − δ, at any time of the
(cid:101)O(−2k2 · log2(1/δ) · poly(d, L)) bits in the worst case2. For each update of the input, the algo-
stream, S is an -coreset for k-means of size −2k2 · log(1/δ) · poly(d, L). The data structure uses
rithm needs poly(d, 1/, L, log k) time to process. After one pass, it outputs the coreset in time
poly(d, k, L, 1/, log 1/δ).
Note that, for the k-means problem, [CEM+15] shows that one can always do the random
projection to reduce d to O(k/2). However since random projection does not preserve the grid-
like structure, it remains a caveat to use random projection method in dynamic geometric stream.
Nevertheless, the most interesting setting would be that when d ≤ O(k/2) and d (cid:29) log k. The
theorem is restated in Theorem E.5 in Section E and the proof is presented therein.
We further present another one-pass algorithm with diﬀerent trade-oﬀs in k and . This algo-
rithm has space nearly linear in k while still polynomial in d. The dependence on k is ensentially
nearly optimal (for ﬁxed  and d)! The guarantee of our result is presented in the following theorem.
Theorem 1.2. Fix , δ ∈ (0, 1/2), k, ∆ ∈ N+, let L = log ∆. There is a data structure supporting
insertions and deletions of a point set Q ⊂ [∆]d, maintaining a weighted set S with positive weights
k-means of size k· log(1/δ)· poly(d, L, 1/),3 The data structure uses (cid:101)O(k· log2(1/δ)· poly(d, L, 1/))
for each point, such that with probability at least 1−δ, at any time of the stream, S is an -coreset for
bits in the worst case. For each update of the input, the algorithm needs poly(d, 1/, L, log k) time
to process. After one pass, it outputs the coreset in time poly(d, k, L, 1/, log 1/δ).
M-estimator Clustering and More General Cost Functions Further we show that our
algorithm and analysis on k-means can be extended to general functions (including M-estimator)
over distances, i.e., a function that satisﬁes approximately sub-additivity: there is a ﬁxed constant
C > 0,
2For any function f, we deﬁne (cid:101)O(f ) to be f · logO(1)(f ).
∀x, y ≥ 0, M (x + y) ≤ C · (M (x) + M (y)).
3The exact dependence of  is determined by the coreset framework in [FL11] and [BFL16], we show that −3
dependence is suﬃcient. See the Section H for details.
Notice that the above equation automatically holds when M (·) is a non-decreasing function satis-
fying
∀x > 0, M (x) > 0 and ∀c > 0, M (cx) ≤ f (c)M (x)
where f (c) > 0 is a bounded function. In this case, we are aiming to solve
(cid:88)
z∈Q
min
Z⊂[∆]d:|Z|≤k
min
z∈Z
M (dist(q, z))
for a given point set Q.
Although M-estimator clustering problem has been studied by [FS12, BFL16], their algorithm
can only be applied in insertion only streaming model. We show the ﬁrst dynamic streaming algo-
rithms for this problem. Our data structure maintains a coreset with size similar to k-means, with
only slightly diﬀerent dependence on the dimension d. More precisely, if we extend the algorithm
and the analysis of Theorem 1.1 to the M-estimator clustering setting, then we can get an algorithm
with success probability at least 1 − δ which can output an -coreset for M-estimator clustering
the worst case, where the poly(d) factor depends on the M-estimator function. For each update of
the input, the algorithm needs poly(d, 1/, L, log k) time to process and outputs the coreset in time
poly(d, k, L, 1/, log 1/δ). We can also generalize the algorithm and the analysis of Theorem 1.2 to the
M-estimator clustering setting. In this case, we can get a coreset of size k · log(1/δ)· poly(d, L, 1/),
In Section F, we show the details of how to generalize the algorithm and analysis of Theorem 1.1
of size −2k2 · log(1/δ) · poly(d, L). The data structure uses (cid:101)O(−2k2 · log2(1/δ) · poly(d, L)) bits in
and the data structure uses (cid:101)O(k · log2(1/δ) · poly(d, L, )) bits in the worst case.
to M-estimator clustering setting.
Maintaining Approximate Solutions for Max-Cut and Average Distance We also show
that our data-structure can be used to maintain an approximate solution for the Max-Cut and
Average-Distance problems, where the ﬁrst problem asks to ﬁnd a cut of the streaming point set Q,
such that the sum of distances of all possible pairs across the cut are maximized; the later problem
asks to estimate average distance over all pairs. Our data structure supports maintaining a 1/2-
approximation to Max-Cut over all times of the stream, and estimating the cost of the cut up to
(1±) factor. For Average-Distance, our data structure supports maintaining a (1±)-approximation
over all times of the stream. Furthermore, our data structure maintains approximate solutions for
the generalized version of these problems, i.e., the M-estimator over distances. The data structure
for these problems uses space polynomial in 1/, d, log ∆, log 1/δ, where δ is the failure probability.
The formal results are presented in Section G.
(cid:101)O(k2)-Space Algorithm with Chen’ Framework
1.4 Our Techniques
1.4.1
We ﬁrst show a data structure design whose underlying framework is based on [Che09]. Denote
Xk = {C ⊂ [∆]d : |C| ≤ k} as the set of all k-sets. For a ﬁnite point set P ⊂ [∆]d, an -coreset for
k-means of P is a weighted point set S ⊂ [∆]d such that
where cost(S, C) =(cid:80)
∀C ∈ Xk : | cost(P, C) − cost(S, C)| ≤  cost(P, C)
s∈S ws dist(s, C)2, where ws is the weight of point s. Our coreset algorithm
can be viewed as a combination of several techniques in both clustering algorithms and dynamic
streaming algorithms. From a high level, we apply an oblivious grid structure over the point sets as
used in [FS05] and [BFL+17] to form an implicit partition of the point sets. This partition satisﬁes
the crucial property as required by the coreset framework of Chen [Che09]. We then build a dynamic
streaming algorithm to simulate the random sampling of Chen’s framework. These combinations
are highly non-trivial. We highlight several diﬃculties and how we resolve it in this paper.
• Firstly, a naïve application of the grid structure in [FS05] gives a coreset with size exponentially
depending on d. To resolve this problem, we randomly shift the grid struture and show that
the exponential dependence on d becomes polynomial.
• Secondly, in Chen’s framework, points are stored in memory, one can do straightforward
sampling from the dataset. However, for us, the memory of the algorithm is limited and
points can even be deleted. We do not know any of the sampling parameters before the
stream coming. But these parameters are crucial to implement Chen’s framework. We resolve
this problem by applying a clever data structure, which is similar to the K-set data structure
as in [Gan05]. This data structure has a small memory budget, and can output the set of
points under insertions and deletions if the size of ﬁnal point set is smaller than the memory
budget. Without knowing any parameters of the true dataset, we guess polylogarithmically
many possibilities of all the parameters in Chen’s framework. We show that if the guessed
parameters are close to the true parameters, the data sample is guaranteed to be smaller than
the memory budget. This set of points form a coreset of the dataset and will be output by
the sampling data structure.
Next we elaborate the details of each framework that we have been applying. We start with the
introduction of Chen’s coreset framework.
for some t ≤ αk and(cid:80)t
Chen’s Coreset Framework The bottom level of our algorithm is the framework of Chen
[Che09]. As shown in Figure 1, the core idea is to ﬁnd a partition of the dataset P = P1∪ P2∪ . . . Pt,
i=1 |Pi| diam(Pi)2 ≤ β OPT, where OPT is the optimal cost of k-means on P .
ple (cid:101)O(β2k/(2)) points uniformly at random from each part Pi such that the error of estimating the
We denote such a partition as an (α, β)-partition. If an (α, β)-partition is known, then one can sam-
cost of the contribution of Pi is |Pi| diam(Pi)2/β. More formally, denote Xk = {C ⊂ [∆]d : |C| ≤ k}
as the set of all k centers and Si as the set of samples for Pi. Assigning each point in Si with a
weight |Pi|/|Si|, then by a Hoeﬀding bound, we expect, with high probability,
Notice that size of the coreset is (cid:101)O(αk2β2).
Combining the samples of each part, we obtain that S = ∪t
(cid:12)(cid:12) cost(Pi, C) − cost(Si, C)
∀C ∈ Xk :
(cid:12)(cid:12) ≤ |Pi| diam(Pi)2/β.
i=1Si is a coreset for k-means of P .
Obtaining An (α, β)-partition in a Dynamic Stream In an oﬄine setting, i.e., the case that
all points are stored in memory, obtaining an (α, β)-partition is quite straightforward, i.e., by using
Indyk’s (α, β)-bi-criterion algorithm [Ind00a]. But it becomes challenging once the algorithm has
limited memory and points can be deleted. We overcome this diﬃculty by applying a similar grid
structure as used in [FS05] and [BFL+17]. As shown in Figure 1(b), we build log ∆ many grids over
the dataset. Each higher layer reﬁnes its parent layer by splitting each cell into 2d many sub-cells.
We stress that the grid structure is oblivious to the point set. Because of this crucial property, we are
able to insert and delete points from the point set, and the grids stay intacted. Similar to [FS05],
one can show that for each level i, the number of cells containing more than O(OPT·4i/∆2/k)
Figure 1: (a) The coreset framework of Chen [Che09]. The point set is partitioned in to a set of
sets, which are called a (α, β)-partition. Points are then sampled from each partition. See texts
for details. (b)The grid structure over the point set. From top to bottom, we have four levels of
grids. Each higher level partition a cell in the parent level into 2d many sub-cells, where d is the
dimension of d.
points is bounded by 2O(d) · k, independent of the number of points in P . We shall call these cells as
heavy cells, which also form a tree. Notice that the grid structure has only log ∆ levels, the number
of heavy cells is O(log ∆· 2O(d) · k). Since each heavy cell has diameter √d· ∆/2i and the non-heavy
children of a heavy cell in level i contains at most O(2d · OPT·4i/∆2/k) points, thus in each level,
the
(number of points in non-heavy children) · (diameter of the heavy cell)2
is bounded by
O(2d OPT·4i/∆2) · (√d · ∆/2i)2 = O(2d OPT /k).
Applying the deﬁnition of the (α, β)-partition, we then show that these non-heavy children of heavy
cells form a (α, β)-partition, here α = O(2d·log ∆) and β = O(2d·log ∆). The majority of their work
algorithm does not apply Chen’s framework, and thus their ﬁnally -coreset is of size (cid:101)O(k/O(d))
is showing how a dynamic streaming algorithm can maintain such a partition. We stress that their
and the algorithm uses (cid:101)O(k2/O(d)) space.
Removing Exponential Dependence on d In the last paragraph, we show that an (α, β)-
partition can be implicitly obtained by simply building a grid structure over the dataset. However,
the size of this partition is of exponential in d. To get rid of this exponential dependence, we apply
a random shift to the grid structure, as shown in Figure 2. We now explain why a random shift
brings down the dependence on d. As shown in the last paragraph, a heavy cell is deﬁned by the
number of points in it. Let us ﬁx an optimal solution of k-means, i.e., a set of k-centers z1, z2, . . . , zk.
A random shift grantees that, with high probability, any of the zis is “far” away from a boundary
of a cell. Conditioning on this event, for the cells that do not contain an optimal center, every
point in it contributes to the cost at least the distance of the optimal center to the boundary of a
cell, which is “far”. Therefore, we are able to bound the number of cells containing no center but
containing too many points . With the k cells containing a optimal center, the total number of
P1P2P3P4P5P6Figure 2: Random shift of a grid brings down the number of heavy cells. In the left panel, we have
a worst case alignment of points and grids that many cells contain lots of points. In the right panel,
after the random shift, only two cells are containing many points.
“heavy cell”s is bounded by O(α · k), where α = O(d · log ∆). With additional tricks, we show an
implicit (α, β)-partition can be constructed from the heavy cells, where β = (poly(d log ∆)).
Sample Maintainance and Rejection Data Structure With the random-shifted grid struc-
ture, we aim to sample points from the (α, β)-partition, as it does in Chen’s construction. However,
without knowing the parameters of the point sets, i.e., the optimal cost OPT, we have to guess its
value. We guess logarithmically many possibilities of OPT, i.e., oi = 2i for i = 1, 2, . . . , log(d∆d).
For each guess, we run an sampler and attempt to get a coreset. Notice that, all these guesses and
sampler based on Chen’s construction are oblivious to the input point set. Later we show that if
the guess oi ≤ OPT, then the above mentioned samples (with the sampling probability and heavy
cells deﬁned by oi) form an -coreset to the point set. However if oi/ OPT is too small, the coreset
is too large. We construct a new dynamic set-point sampler data structure that has a ﬁxed memory
budget and rejects large data samples but preserve data samples with a small number of points.
With this data structure, we can pick the smallest oi, whose resulting samples are preserved by the
data structure. As a result, these samples are the correct coreset.
Now we elaborates the details of the data structure. This data structure supports insertions
and deletions over pairs of the form (C, p), where C represents a set and p ∈ [∆]d is point. The
data structure supports querying all the points in sets with small number of points, at any time of
the stream. Suppose there are at most α non-empty sets and at most θ sets containing at least β
points and the rest of the sets containing γ points. Then with a memory budget of [Θ(θβ + γ)], the
algorithm is able to maintain the number of points in each cell, and the γ points in cells with less
than β points. The high level idea is to use two level of pair-wise hash functions. In the ﬁrst level,
we hash the ID of the set (i.e., the name or the coordinate if the set is a cell) to a universe [Θ(α)]
and the point to a universe [Θ(β)], then we hash the pair of hash values of the set ID and the point
to a universe [Θ(θβ + γ)]. It can be shown that if a point c from a set C with less than β points,
then the third hash value is unique to all other pairs, with at least constant probability. Use this
unique hash value, plus a sanity checker based on parity we can recover each bit of the point and
the ID of the set. By repeating log 1/δ times, we can recover all the γ points from sets with less
than β points.
randomlyshiftheavycellonlysmallnumberofcellscontainingmanypoints1.4.2
(cid:101)O(k)-Space Algorithm (Nearly Optimal) with Sensitivity-based Sampling
Next we show our techniques for improving the space complexity from (cid:101)O(k2) to (cid:101)O(k). The high
level idea of our data structure is to simulate a sensitivity-based sampling using an oblivious linear
sketch. Our sampling scheme takes the advantages of most of the data structures constructed in
the last section. To begin, we ﬁrst brieﬂy review the coreset framework of [FL11] and [BFL16].
In [FL11] and [BFL16],
A Brief Review of the Coreset Framework in [FL11] and [BFL16]
they have proposed a framework called “sensitivity” sampling. Since techniques are similar for
distance functions other than (cid:96)2
2, we take k-means for an example. Let Q be the set of points, let
X k ⊂ [∆]d×k be the set of all possible k-centers. Then the sensitivity of a point q ∈ Q is deﬁned as
s(q) = max
Z∈X k
dist2(q, Z)/
dist2(p, Z)
(cid:18)(cid:88)
p∈Q
(cid:19)
the point q in Q. It can be easily shown that(cid:80)
Namely, s(q) denotes the maximum possible change in the cost function of any k-set Z after removing
each point gets a probability s(cid:48)(q)/((cid:80)
q s(q) ≈ k. Suppose one designs a uniform upper
bound s(cid:48)(·) such that ∀q ∈ Q : s(cid:48)(q) ≥ s(q), and denote a probability distribution D over Q as
and let R be a set of i.i.d. samples sampling from D with |R| (cid:38) (cid:80)
p∈Q s(cid:48)(p)). The framework in [FL11] and [BFL16] shows that,
q∈Q s(cid:48)(q)/2 and assign each
point q in Q a weight (|R|s(cid:48)(q))−1, then R is with high probability a (1 ± ) k-means coreset for
theory. If(cid:80)
q∈Q s(q), then a (cid:101)O(k)-sized coreset is constructed.
Q. The proof is by a establishing connection between the VC-dimension theory and the coreset
q∈Q s(cid:48)(q) is not too diﬀerent from(cid:80)
Vaguely speaking, the reason that sensitivity sampling removes a k factor from our k2-construction
is by constructing a new functional space (from the the (cid:38) k samples), whose VC-dimension is O(d)
instead of O(kd).
Sensitivity Sampling Over Dynamic Data Stream Analogously to the framework described
in the last section, we describe an algorithm that simulates the sensitivity sampling over the dynamic
stream. As we have shown, to correctly assign the sampling probability, all we need is an upper
bound on the sensitivity. For each point p ∈ [∆]d, if p is in the set Q at the end of the stream, then
it must have a corresponding sensitivity. Denote its true sensitivity as s(p). We want to design a
“good” upper bound s(cid:48)(p) of the sensitivity of each point p such that
1. the sum of s(cid:48)(p) is not too large;
2. we are able to i.i.d. sample m points from the ﬁnal undeleted points such that each point is
chosen with probability proportional to s(cid:48)(p) in each sample, where the magnitude of m is
inﬂuenced by the sum of s(cid:48)(p);
3. at the end of the stream, we are able to approximate s(cid:48)(p) for each given point p.
The ﬁrst property ensures that m will not be too large which means that the size of the coreset
is small. The second property ensures that we can ﬁnally obtain the points in the coreset by
implementing such sampling procedure. The third property shows that we can obtain the weights
of the points in the coreset.
In the previous section, we gave the concepts of “heavy cell” and the (α, β) partition based on the
random shifted grid. Precisely, if a cell in the ith level which contains at least Ti number of points,
then we say the cell is “heavy” where Ti is a threshold parameter which is set to be Θ( d2
(∆/2i)2 )
k · OPT
10
in our work. As discussed in the previous section, all the non-“heavy” cells whose parent cell is
“heavy” formed a partition. We call such non-“heavy” cell as a partition cell. Since the cells formed
a partition, it is easy to see, for each undeleted point, it must belong to a unique partition cell.
Furthermore, if the unique partition cell which contains point p is in the ith level of the grid, then
we say p is a partition point in the ith level. Then we show that all the points which are partition
points in the ith level have a universal sensitivity upper bound which is Θ(d3/Ti). Furthermore, if
p∈Q s(cid:48)(p)
we set the sensitivity upper bound of p as s(cid:48)(p) = Θ(d3/Ti), it is easy to argue that (cid:80)
cannot be too large. The reason is that
(cid:88)
p∈Q
(cid:48)
(p) =
(# of partition points in the ith level) · Θ(d3/Ti)
(# of heavy cells in the ith level) · Θ(d3)
level i
(cid:88)
(cid:88)
= (# of heavy cells) · Θ(d3)
≤ α · k · Θ(d3),
level i
Since (cid:80)
where the ﬁrst inequality follows by that each level i heavy cell contains at least Ti number of points.
The second inequality follows by the last section, and α is as the same as mentioned in the last
section.
p∈Q s(cid:48)(p) is not large, we know that the size of the coreset will be small. Also notice
that when we know a point p is a partition point in the ith level, then we already know s(cid:48)(p). By
last section, we know that we can have a streaming algorithm which can ﬁnd out all the heavy cells
which means that we can know the shape of the whole partition at the end of the stream. Thus, for
any given point p, we can determine its partition cell, and thus know the s(cid:48)(p) which means that
for each point in the coreset, we could calculate its weight. Now, the problem remaining is to get
m i.i.d. sample points, and for each sample, each point p is chosen with probability proportional to
s(cid:48)(p).
A challenge to implement the sampling procedure is that the sampling scheme here is diﬀerent
from the sampling procedure described in the previous section. Note that in the previous section,
the sampling scheme is that we independently determine each point should be chosen or not, but
here we need to get m independent samples where each sample is a point in Q, and the probability
that p is chosen is proportional to s(cid:48)(p). We cannot directly use the sampling scheme described in
the previous section. However we can handle this issue by two-stage sampling. A good property of
our s(cid:48)(p) is that all the partition points in the ith level have the same s(cid:48)(p). To sample a point p
with probability proportional to s(cid:48)(p) is equivalent to ﬁrstly sample a level i, where each level j is
j = s(cid:48)(p)
sampled with probability proportional to (# of partition points in the jth level)·s(cid:48)
for point p which is a partition point in the jth level; then we uniformly sample a partition point in
the ith level. To implement the ﬁrst sampling stage, we just need to know the number of partition
points in each level. Fortunately, we can achieve this by using the streaming algorithm described
by the last section. Now let us focus on the second stage. In this stage, we want to implement
the uniform sampling oracle over all the partition points in the ith level. We cannot apply the
traditional (cid:96)0 sampler here since we do not have any information of the partition points in each
level at the beginning of the stream. To achieve our goal, we should be more carefully. We ﬁrstly
subsample all the points in the ith level, we just use the streaming data structure described in the
last section to maintain all the survived points. We show that by using the data structure we are
able to recover all the survived partition points. Then for all the survived partition points in the ith
level, we then uniformly choose a survivor as the output. A potential problem is that if in the ith
level, the number of partition points is small, then it is highly possible that none of the partition
j where s(cid:48)
11
Algorithm 1 A Meta-Algorithm for Point Sampling From a Dynamic Data Stream
procedure PointSampler(P )
Let O = {1, 2, 4, 8,··· , 2(cid:100)d log(d∆)(cid:101)
Choose randomly shifted |L| layers grids
Create |O| independent KSet instances (with limited memory budget) for each layer l ∈ L
For each o, l, create a set of hash functions Hl,o, each h ∈ Hl,o is a function maps [∆]d → {0, 1}
for each update of a point p ∈ ∆d in the data-stream do
},L = {−1, 0, 1,··· ,(cid:100)log ∆(cid:101)}.
(cid:46) reads points set P in the data-stream
Create |Hl,o| copies of p as the form (p, i) for each hi ∈ Hl,o
for (o, l) ∈ O × L and hi ∈ Ho,l do
end for
Update (o, l)-th sketch: if hi(p) = 1 then KSeto,l.update(p, i)
end for
Choose the smallest o∗
∈ O such that all the {KSeto∗,l}l∈L succeed.
Output the sampled point sets and grid cells given by {KSeto∗,l}l∈L
end procedure
point can be survived. But this will not introduce a problem since if the number of partition points
in the ith level is small, the probability that level i is chosen in the ﬁrst stage is small. More
(cid:17)
(cid:16)(cid:80)
precisely, suppose we need m i.i.d. samples in total, then with high probability the number of times
that the ﬁrst sampling stage samples level i is about m · (# of partition points in the ith level) ·
s(cid:48)
j(# of partition points in the jth level) · s(cid:48)
(cid:17)
(cid:16)(cid:80)
. Suppose for each level i, we prepared 1.1m
i/
many uniform sampling oracles, we just need to guarantee that the number of success oracle is at
least m·(# of partition points in the ith level)·s(cid:48)
j(# of partition points in the jth level) · s(cid:48)
then it is guaranteed to have suﬃcient many samples. In our work, we show that, for the uniform
sampling oracle in level i, we just need the drop probability to be poly(d, −1, log(∆), log(1/δ))· 1
it is enough to achieve our goal. Thus, we have enough success uniform samplers for each level.
i/
kTi
1.4.3 Max-Cut and Average-Distance
We use our coreset data structure to obtain solutions for max-cut and average distance over a
dynamic dataset. The basic idea is to use the 1-means coreset as a proxy for estimating pairwise
distance. We reduce the Max-Cut instance and Average-Distance instance to a instance of estimating
the distance of a point to a subset of the original point set. Hence 1-means coreset is suﬃcient for
this case.
1.5 Meta Algorithms and a Roadmap
All our algorithms share a similar meta-structure to sample points from a dynamic data stream.
The meta-algorithm is presented in Algorithm 1. This meta algorithm is an oblivious linear sketch
over the input data set (i.e., the algorithm does not need to know the actual data set). If we view
the data set as a binary vector in {0, 1}∆d, then our meta-algorithm converts the vector linearly
into Θ(d log ∆) binary vectors in {0, 1}∆d and Θ(d log ∆) vectors in Z∆d, i.e., it gives Θ(d log ∆)
level of points samples and counts of the number of points in grid cells of each level. These output
vectors are very sparse, and hence can be stored in limited memory budget.
In the meta algorithm, we ﬁrst build random shifted grids G−1, G0, G1,··· , GL where L = log ∆
and each grid Gi reﬁnes its parent by splitting each cell into 2d cells evenly. Then for each pair
of (o, l) where o ∈ {1, 2, 4,··· , 2(cid:100)log(d·∆d+1)(cid:101)
}, l ∈ {−1, 0,··· , L}, we choose a set of random hash
12
Pr[hi(p) = 1] is deﬁned according to the speciﬁc tasks (i.e., for the (cid:101)O(k2) and (cid:101)O(k) algorithms, we
functions Hl,o,,δ, where each hi ∈ Hl,o,,δ is a function hi : [∆]d → {0, 1} such that ∀p ∈ [∆]d,
chose diﬀerent sampling probability), and we initialize a point maintainer KSet (see Algorithm 2).
Each KSet data structure has a limited memory budget. The KSet data structure can succeed
only if the number of points and grid cells sampled (by the hash functions hi) is under the memory
budget (otherwise, the information stored by KSet is treated as garbage). When we scan the
stream, for each insert/delete operation, suppose the point is p, we ﬁrst check whether hi(p) = 1. If
it is 0, we just ignore this operation. Otherwise, for each level l ﬁnd the cell C ∈ Gl which contains
p, and for each o, we use the information of C, p and the operation type (ins/del) to update the
point maintainer which corresponds to (o, l). We then build our coreset from the succeeded KSet
instances with the smallest parameter o.
We provide some basic notation in Section A. Section B states some deﬁnitions and useful
tools from previous works.
In Section D, explain how to construct a coreset using randomized
grid structure and hash functions over the universe [∆]d. In Section E, we show how to maintain
the coreset in a dynamic stream. Section F presents the general result for M-estimator and also
improvement compared to previous k-median result. Section G shows how to extend our k-means
to some other geometric problems. We provide improved one-pass algorithm for k-means based on
sensitivity sampling in Section H.
1.6 Concluding Remark
In this paper we present two algorithm for obtaining -coreset for the k-means problem in high-
dimensional dynamic geometric data streams. Our ﬁrst algorithm is a one-pass algorithm and takes
space (cid:101)O(k2−2 poly(d, log ∆)) based on Chen’s framework [Che09]. Our second algorithm is a one-
pass algorithm and takes space (cid:101)O(k poly(d, log ∆, 1/)) based on sensitivity sampling. Both of the
algorithm take space polynomial in d. To the best of our knowledge, these are the ﬁrst results
for obtianing k-means coreset using space polynomial in d. In particular, our second algorithm is
nearly optimal with regard to parameter k. Furthermore, the coresets output by our algorithms
consist only positive weighted points. One can run her favorite oﬄine algorithms to obtain the
desired approximated solutions. Both our algorithm can be extended to a much general set of cost
functions, e.g., the M-estimator.
1.7 Acknowledgments
The authors would like to thank Alexandr Andoni, Vladimir Braverman, Lijie Chen, Kenneth
Clarkson, Harry Lang, Cameron Musco, Christopher Musco, Vasileios Nakos, Jelani Nelson, Eric
Price, Aviad Rubinstein, Chen Shao, Christian Sohler, Cliﬀord Stein, Huacheng Yu, Zhengyu Wang,
David P. Woodruﬀ, and Hongyang Zhang for useful discussions.
LY would like to thank Professor Vladimir Braverman for generous support.
13
References
[ACD+16]
[ADHP09]
[ADK+16]
[AGM12a]
[AGM12b]
[AGM13]
Ittai Abraham, Shiri Chechik, Daniel Delling, Andrew V Goldberg, and Renato F
Werneck. On dynamic approximate shortest paths for planar graphs with worst-
case costs.
In Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium
on Discrete Algorithms (SODA), pages 740–753. Society for Industrial and Applied
Mathematics, 2016.
Daniel Aloise, Amit Deshpande, Pierre Hansen, and Preyas Popat. Np-hardness of
euclidean sum-of-squares clustering. Machine learning, 75(2):245–248, 2009.
Ittai Abraham, David Durfee, Ioannis Koutis, Sebastian Krinninger, and Richard
Peng. On fully dynamic graph sparsiﬁers. In 2016 IEEE 57th Annual Symposium on
Foundations of Computer Science (FOCS), pages 335–344. IEEE, 2016.
Kook Jin Ahn, Sudipto Guha, and Andrew McGregor. Analyzing graph structure
via linear measurements. In Proceedings of the twenty-third annual ACM-SIAM sym-
posium on Discrete Algorithms, pages 459–467. Society for Industrial and Applied
Mathematics, 2012.
Kook Jin Ahn, Sudipto Guha, and Andrew McGregor. Graph sketches: sparsiﬁcation,
spanners, and subgraphs. In Proceedings of the 31st ACM SIGMOD-SIGACT-SIGAI
symposium on Principles of Database Systems, pages 5–14. ACM, 2012.
Kook Jin Ahn, Sudipto Guha, and Andrew McGregor. Spectral sparsiﬁcation in
dynamic graph streams. In Approximation, Randomization, and Combinatorial Op-
timization. Algorithms and Techniques, pages 1–10. Springer, 2013.
[AHPV04]
Pankaj K Agarwal, Sariel Har-Peled, and Kasturi R Varadarajan. Approximating
extent measures of points. Journal of the ACM (JACM), 51(4):606–635, 2004.
[AMR+12] Marcel R Ackermann, Marcus Märtens, Christoph Raupach, Kamil Swierkot, Chris-
tiane Lammersen, and Christian Sohler. Streamkm++: A clustering algorithm for
data streams. Journal of Experimental Algorithmics (JEA), 17:2–4, 2012.
[Bas08]
[BCMN14]
[BDMO03]
[BFL16]
[BFL+17]
Surender Baswana. Streaming algorithm for graph spanners-single pass and constant
processing time per edge. Information Processing Letters, 106(3):110–114, 2008.
Sayan Bhattacharya, Parinya Chalermsook, Kurt Mehlhorn, and Adrian Neumann.
New approximability results for the robust k-median problem. In Scandinavian Work-
shop on Algorithm Theory, pages 50–61. Springer, 2014.
Brain Babcock, Mayur Datar, Rajeev Motwani, and Liadan O’Callaghan. Maintain-
ing variance and k-medians over data stream windows. In Proceedings of the twenty-
second ACM SIGMOD-SIGACT-SIGART symposium on Principles of database sys-
tems, pages 234–243. ACM, 2003.
Vladimir Braverman, Dan Feldman, and Harry Lang. New frameworks for oﬄine and
streaming coreset constructions. arXiv preprint arXiv:1612.00889, 2016.
Vladimir Braverman, Gereon Frahling, Harry Lang, Christian Sohler, and Lin F
Yang. Clustering high dimensional dynamic data streams. In ICML. https://arxiv.
org/pdf/1706.03887, 2017.
14
[BGS15]
[BHNT15]
[BHPI02]
[BIP+16]
[BKS12]
[BR94]
[BS80]
[BS16]
[BWZ16]
[BYJK+02]
[CAKM16]
Surender Baswana, Manoj Gupta, and Sandeep Sen. Fully dynamic maximal match-
ing in o(log n) update time. SIAM Journal on Computing, 44(1):88–113, 2015.
Sayan Bhattacharya, Monika Henzinger, Danupon Nanongkai, and Charalampos
Tsourakakis. Space-and time-eﬃcient algorithm for maintaining dense subgraphs
on one-pass dynamic streams. In Proceedings of the forty-seventh annual ACM sym-
posium on Theory of computing, pages 173–182. ACM, 2015.
Mihai B¯adoiu, Sariel Har-Peled, and Piotr Indyk. Approximate clustering via core-
sets. In Proceedings of the thiry-fourth annual ACM symposium on Theory of com-
puting, pages 250–257. ACM, 2002.
Arturs Backurs, Piotr Indyk, Eric Price, Ilya Razenshteyn, and David P Woodruﬀ.
Nearly-optimal bounds for sparse recovery in generic norms, with applications to
k-median sketching. In Proceedings of the Twenty-Seventh Annual ACM-SIAM Sym-
posium on Discrete Algorithms, pages 318–337. SIAM, 2016.
Surender Baswana, Sumeet Khurana, and Soumojit Sarkar. Fully dynamic ran-
domized algorithms for graph spanners. ACM Transactions on Algorithms (TALG),
8(4):35, 2012.
Mihir Bellare and John Rompel. Randomness-eﬃcient oblivious sampling. In Foun-
dations of Computer Science, 1994 Proceedings., 35th Annual Symposium on, pages
276–287. IEEE, 1994.
Jon Louis Bentley and James B Saxe. Decomposable searching problems i. static-to-
dynamic transformation. Journal of Algorithms, 1(4):301–358, 1980.
Aaron Bernstein and Cliﬀ Stein. Faster fully dynamic matchings with small approxi-
mation ratios. In Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium
on Discrete Algorithms, pages 692–711. Society for Industrial and Applied Mathe-
matics, 2016.
Christos Boutsidis, David P Woodruﬀ, and Peilin Zhong. Optimal principal compo-
nent analysis in distributed and streaming models. In Proceedings of the 48th Annual
ACM SIGACT Symposium on Theory of Computing (STOC), pages 236–249. ACM,
https://arxiv.org/pdf/1504.06729, 2016.
Ziv Bar-Yossef, TS Jayram, Ravi Kumar, D Sivakumar, and Luca Trevisan. Counting
distinct elements in a data stream. In International Workshop on Randomization and
Approximation Techniques in Computer Science, pages 1–10. Springer, 2002.
Vincent Cohen-Addad, Philip N Klein, and Claire Mathieu. Local search yields ap-
proximation schemes for k-means and k-median in euclidean and minor-free metrics.
In Foundations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium
on, pages 353–364. IEEE, 2016.
[CCGG98] Moses Charikar, Chandra Chekuri, Ashish Goel, and Sudipto Guha. Rounding via
trees: deterministic approximation algorithms for group steiner trees and k-median.
In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages
114–123. ACM, 1998.
15
[CEM+15]
[Che09]
[CM02]
[CMS13]
[COP03]
[CW15a]
[CW15b]
[DFK+04]
Michael B Cohen, Sam Elder, Cameron Musco, Christopher Musco, and Madalina
Persu. Dimensionality reduction for k-means clustering and low rank approxima-
tion.
In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory
of Computing (STOC), pages 163–172. ACM, https://arxiv.org/pdf/1410.6801,
2015.
Ke Chen. On coresets for k-median and k-means clustering in metric and euclidean
spaces and their applications. SIAM Journal on Computing, 39(3):923–947, 2009.
Dorin Comaniciu and Peter Meer. Mean shift: A robust approach toward feature
space analysis.
IEEE Transactions on pattern analysis and machine intelligence,
24(5):603–619, 2002.
Michael S Crouch, Andrew McGregor, and Daniel Stubbs. Dynamic graphs in
the sliding-window model. In European Symposium on Algorithms, pages 337–348.
Springer, 2013.
Moses Charikar, Liadan O’Callaghan, and Rina Panigrahy. Better streaming algo-
rithms for clustering problems. In Proceedings of the thirty-ﬁfth annual ACM sym-
posium on Theory of computing, pages 30–39. ACM, 2003.
Kenneth L Clarkson and David P Woodruﬀ. Input sparsity and hardness for robust
subspace approximation. In 2015 IEEE 56th Annual Symposium on Foundations of
Computer Science (FOCS), pages 310–329. IEEE, https://arxiv.org/pdf/1510.
06073, 2015.
Kenneth L Clarkson and David P Woodruﬀ. Sketching for m-estimators: A uniﬁed
approach to robust regression.
In Proceedings of the Twenty-Sixth Annual ACM-
SIAM Symposium on Discrete Algorithms (SODA), pages 921–939. SIAM, 2015.
Petros Drineas, Alan Frieze, Ravi Kannan, Santosh Vempala, and V Vinay. Clustering
large graphs via the singular value decomposition. Machine learning, 56(1):9–33,
2004.
[DLVKKR03] W Fernandez De La Vega, Marek Karpinski, Claire Kenyon, and Yuval Rabani.
In Proceedings of the thirty-ﬁfth
Approximation schemes for clustering problems.
annual ACM symposium on Theory of computing, pages 50–58. ACM, 2003.
[FIS05]
[FKM+05]
[FL11]
[FRS16]
Gereon Frahling, Piotr Indyk, and Christian Sohler. Sampling in dynamic data
streams and applications. In Symposium on Computational Geometry 2005, pages
142–149, 2005.
Joan Feigenbaum, Sampath Kannan, Andrew McGregor, Siddharth Suri, and Jian
Zhang. On graph problems in a semi-streaming model. Theoretical Computer Science,
348(2-3):207–216, 2005.
Dan Feldman and Michael Langberg. A uniﬁed framework for approximating and
clustering data. In Proceedings of the 43rd ACM Symposium on Theory of Computing,
STOC 2011, San Jose, CA, USA, 6-8 June 2011, pages 569–578, 2011.
Zachary Friggstad, Mohsen Rezapour, and Mohammad R Salavatipour. Local search
yields a ptas for k-means in doubling metrics. In Foundations of Computer Science
(FOCS), 2016 IEEE 57th Annual Symposium on, pages 365–374. IEEE, 2016.
16
[FS05]
[FS12]
[Gan05]
[GKK12]
Gereon Frahling and Christian Sohler. Coresets in dynamic geometric data streams.
In Proceedings of the thirty-seventh annual ACM symposium on Theory of computing
(STOC), pages 209–217. ACM, 2005.
Dan Feldman and Leonard J Schulman. Data reduction for weighted and outlier-
resistant clustering. In Proceedings of the twenty-third annual ACM-SIAM sympo-
sium on Discrete Algorithms, pages 1343–1354. Society for Industrial and Applied
Mathematics, 2012.
Sumit Ganguly. Counting distinct items over update streams. In International Sym-
posium on Algorithms and Computation, pages 505–514. Springer, 2005.
Ashish Goel, Michael Kapralov, and Sanjeev Khanna. On the communication and
streaming complexity of maximum bipartite matching. In Proceedings of the twenty-
third annual ACM-SIAM symposium on Discrete Algorithms, pages 468–485. SIAM,
2012.
[GKL+17]
Shalmoli Gupta, Ravi Kumar, Kefu Lu, Benjamin Moseley, and Sergei Vassilvitskii.
Local search methods for k-means with outliers. Proceedings of the VLDB Endow-
ment, 10(7):757–768, 2017.
[GKP12]
Ashish Goel, Michael Kapralov, and Ian Post. Single pass sparsiﬁcation in the stream-
ing model with edge deletions. arXiv preprint arXiv:1203.4900, 2012.
[GMMO00]
Sudipto Guha, Nina Mishra, R. Motwani, and L. O’Callaghan. Clustering data
streams. In FOCS, pages 359–366, 2000.
[HPK05]
[HPM04]
[Hub64]
[IKI94]
[Ind00a]
[Ind00b]
[Ind04]
Sariel Har-Peled and Akash Kushal. Smaller coresets for k-median and k-means
clustering. In Proceedings of the twenty-ﬁrst annual symposium on Computational
geometry, pages 126–134. ACM, 2005.
Sariel Har-Peled and Soham Mazumdar. On coresets for k-means and k-median
clustering. In Proceedings of the thirty-sixth annual ACM symposium on Theory of
computing, pages 291–300. ACM, 2004.
Peter J. Huber. Robust estimation of a location parameter. The Annals of Mathe-
matical Statistics, 35(1):73–101, 1964.
Mary Inaba, Naoki Katoh, and Hiroshi Imai. Applications of weighted voronoi dia-
grams and randomization to variance-based k-clustering. In Proceedings of the tenth
annual symposium on Computational geometry, pages 332–339. ACM, 1994.
Piotr Indyk. High-dimensional computational geometry. PhD thesis, stanford univer-
sity, 2000.
Piotr Indyk. Stable distributions, pseudorandom generators, embeddings and data
stream computation. In Foundations of Computer Science, 2000. Proceedings. 41st
Annual Symposium on, pages 189–197. IEEE, 2000.
Piotr Indyk. Algorithms for dynamic geometric problems over data streams.
In
Proceedings of the thirty-sixth annual ACM symposium on Theory of computing, pages
373–380. ACM, 2004.
17
[IP11]
[Jai10]
[KL11]
[KLM+17]
[KMN+02]
[LIY+15]
[Llo82]
[LNNT16]
[Mat00]
[McG14]
[MNV09]
[MS01]
[NJW02]
Piotr Indyk and Eric Price. K-median clustering, model-based compressive sensing,
and sparse recovery for earth mover distance. In Proceedings of the forty-third annual
ACM symposium on Theory of computing, pages 627–636. ACM, 2011.
Anil K Jain. Data clustering: 50 years beyond k-means. Pattern recognition letters,
31(8):651–666, 2010.
J. Kelner and A. Levin. Spectral sparsiﬁcation in the semi-streaming setting.
Symposium on Theoretical Aspects of Computer Science (STACS), 2011.
In
Michael Kapralov, Yin Tat Lee, CN Musco, CP Musco, and Aaron Sidford. Sin-
gle pass spectral sparsiﬁcation in dynamic streams. SIAM Journal on Computing,
46(1):456–477, 2017.
Tapas Kanungo, David M Mount, Nathan S Netanyahu, Christine D Piatko, Ruth
Silverman, and Angela Y Wu. A local search approximation algorithm for k-means
clustering.
In Proceedings of the eighteenth annual symposium on Computational
geometry, pages 10–18. ACM, 2002.
Zaoxing Liu, Nikita Ivkin, Lin Yang, Mark Neyrinck, Gerard Lemson, Alexander Sza-
lay, Vladimir Braverman, Tamas Budavari, Randal Burns, and Xin Wang. Streaming
algorithms for halo ﬁnders. In e-Science (e-Science), 2015 IEEE 11th International
Conference on, pages 342–351. IEEE, 2015.
Stuart Lloyd. Least squares quantization in pcm. IEEE transactions on information
theory, 28(2):129–137, 1982.
Kasper Green Larsen, Jelani Nelson, Huy L Nguyen, and Mikkel Thorup. Heavy hit-
ters via cluster-preserving clustering. In Foundations of Computer Science (FOCS),
2016 IEEE 57th Annual Symposium on, pages 61–70. IEEE, https://arxiv.org/
pdf/1604.01357, 2016.
Jiri Matoušek. On approximate geometric k-clustering. Discrete & Computational
Geometry, 24(1):61–84, 2000.
Andrew McGregor. Graph stream algorithms: a survey. ACM SIGMOD Record,
43(1):9–20, 2014.
Meena Mahajan, Prajakta Nimbhorkar, and Kasturi Varadarajan. The planar k-
means problem is np-hard. In International Workshop on Algorithms and Computa-
tion, pages 274–285. Springer, 2009.
Marina Meila and Jianbo Shi. A random walks view of spectral segmentation.
2001.
.,
Andrew Y Ng, Michael I Jordan, and Yair Weiss. On spectral clustering: Analysis and
an algorithm. In Advances in neural information processing systems, pages 849–856,
2002.
[NYWR09]
Sahand Negahban, Bin Yu, Martin J Wainwright, and Pradeep K Ravikumar. A
uniﬁed framework for high-dimensional analysis of m-estimators with decomposable
regularizers.
In Advances in Neural Information Processing Systems, pages 1348–
1356, 2009.
18
[Scu10]
[SM00]
[SWZ17a]
[SWZ17b]
[Vat09]
David Sculley. Web-scale k-means clustering. In Proceedings of the 19th international
conference on World wide web (WWW), pages 1177–1178. ACM, 2010.
Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation.
Transactions on pattern analysis and machine intelligence, 22(8):888–905, 2000.
IEEE
Zhao Song, David P Woodruﬀ, and Peilin Zhong. Low rank approximation with
entrywise (cid:96)1-norm error. In Proceedings of the 49th Annual Symposium on the Theory
of Computing (STOC). ACM, https://arxiv.org/pdf/1611.00898, 2017.
Zhao Song, David P Woodruﬀ, and Peilin Zhong. Relative error tensor low rank
approximation. arXiv preprint arXiv:1704.08246, 2017.
Andrea Vattani. The hardness of k-means clustering in the plane. In Manuscript,
volume 617. http://cseweb.ucsd.edu/avattani/papers/kmeans_hardness.pdf,
2009.
[VL07]
Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing,
17(4):395–416, 2007.
19
A Notation
For an n ∈ N+, let [n] denote the set {1, 2,··· , n}. Given integers m ≤ n, we denote [m, n] =
For any function f, we deﬁne (cid:101)O(f ) to be f · logO(1)(f ). In addition to O(·) notation, for two
{m, m + 1, m + 2, . . . , n} as the integer interval.
functions f, g, we use the shorthand f (cid:46) g (resp. (cid:38)) to indicate that f ≤ Cg (resp. ≥) for an
absolute constant C. We use f (cid:104) g to mean cf ≤ g ≤ Cf for constants c, C.
B Preliminaries
B.1 Deﬁnitions of Dynamic Streaming Model for k-Clustering Problems
In this section, we give the deﬁnition of the input form and the computational model.
Deﬁnition B.1 (Dynamic streaming model for k-clustering). Let P ⊂ [∆]d initially be an empty set.
In the dynamic streaming model, there is a stream of update operations such that the qth operation
has the form (opq, pq) where opq ∈ {ins, del}, pq ∈ [∆]d which means that the set P should add a
point pq or should remove the point pq. An algorithm is allowed 1-pass/2-pass over the stream. At
the end of the stream, the algorithm stores some information regarding P. The space complexity of
an algorithm in this model is deﬁned as the total number of bits required to describe the information
the algorithm stores during the stream.
B.2 Deﬁnitions of k-means Clustering
For representation simplicity, we restrict the dataset to be from the discrete space [∆]d for some
large integer ∆ and d. The extension of our results to Rd is straightforward. We will discuss this
issue more in the concluding remarks. We use dist(·,·) as the Euclidean distance in space [∆]d, i.e.,
for any p, q ∈ [∆]d,
dist(p, q) = (cid:107)p − q(cid:107)2 ,
where (cid:107)·(cid:107) is the (cid:96)2 norm. We further extend the distance deﬁnition to set and point, and set to set.
Formally, let sets P, Q ⊂ [∆]d and point p ∈ [∆]d, then
dist(p, q) and
dist(P, Q) = min
dist(p, q).
dist(p, Q) = dist(Q, p) = min
q∈Q
p∈P,q∈Q
We also denote diam(Q) diameter of Q, i.e., the largest distance of any pair of points in Q.
Deﬁnition B.2 (k-means clustering). Given an input point set Q ⊂ [∆]d, the k-means clustering
problem is to ﬁnd a set of k points Z ⊂ [∆]d, such that the following objective is minimized.
(cid:88)
q∈Q
cost(Q, Z) =
dist2(q, Z).
Each point of Z is also called a center. Note that a set Z deﬁnes a partition of the point set Q by
assigning each point q ∈ Q to the closest center in Z (ties are broken arbitrarily). We use OPT to
denote the minimum cost of the k-means problem.
Deﬁnition B.3 (Coreset for k-means). Given an input point set Q ⊂ [∆]d. An -coreset S of
Q is a multiset, usually of smaller size, and summarizes the important structures of Q. The so-
lution of the optimization problem on S is an approximate solution on Q. Formally, let S =
20
{(s1, w1), (s2, w2), . . . ,} be an -coreset for Q, where each si ∈ [∆]d and wi ∈ R is the weight of si.
Then S satisﬁes
(cid:12)(cid:12) cost(S, Z) − cost(Q, Z)
(cid:12)(cid:12) ≤  cost(Q, Z),
∀Z ⊂ [∆]d,|Z| = k :
where
cost(S, Z) :=
wi dist2(si, Z).
(cid:88)
si∈S
(cid:88)
q∈Q
Deﬁnition B.4 (Coreset for k-means in dynamic stream). Given a point set P ⊂ [∆]d described
by a dynamic stream, an error parameter  ∈ (0, 0.5), and an fail probability parameter δ ∈ (0, 1),
the goal is to design an algorithm in the dynamic streaming model (Deﬁnition B.1) which can with
probability at least 1 − δ output an -coreset for k-means (Deﬁnition B.3) with minimal space.
B.3 Deﬁnitions of M-estimator Clustering
Our coreset framework can also be extended to arbitrary clustering of using M-estimators.
Deﬁnition B.5 (M-estimator Clustering). We deﬁne function M : R → R to be a M-estimator.
We deﬁne
costM (Q, Z) =
min
z∈Z
M (dist(q, z)).
The goal of a M-estimator clustering to solve the following optimization problem.
min
Z⊂[∆]d:|Z|≤k
costM (Q, Z).
Deﬁnition B.6 (Coreset for M-estimator). Given an input point set Q ⊂ [∆]d. An -coreset S
of Q is a multiset, usually of smaller size, and summarizes the important structures of Q. The
solution of the optimization problem on S is an approximate solution on Q. Formally, let S =
{(s1, w1), (s2, w2), . . . ,} be an -coreset for Q, where each si ∈ [∆]d and wi ∈ R is the weight of si.
Then S satisﬁes that ∀Z ⊂ [∆]d with |Z| = k,
holds, where costM (S, Z) :=(cid:80)
(cid:12)(cid:12) costM (S, Z) − costM (Q, Z)
(cid:12)(cid:12) ≤  costM (Q, Z),
si∈S wi minz∈Z M (dist(si, z)).
Deﬁnition B.7 (Coreset for M-estimator clustering in dynamic stream). Given a point set P ⊂ [∆]d
described by a dynamic stream, an error parameter  ∈ (0, 0.5), and an failure probability parameter
δ ∈ (0, 1), the goal is to design an algorithm in the dynamic streaming model (Deﬁnition B.1)
which can with probability at least 1 − δ output an -coreset for k-center M-estimator clusetering
(Deﬁnition B.6) with minimal space.
dom variables. Suppose that |Xi| ≤ b almost surely, for all i ∈ [n]. Let σ2 denote (cid:80)n
B.4 Basic Probability Tools
Lemma B.8 (Bernstein inequality [HPM04]). Let X1, X2,··· , Xn be independent zero-mean ran-
j ].
j=1 E[X 2
Then for all positive t,
(cid:34) n(cid:88)
i=1
Pr
Xi > t
(cid:35)
≤ exp(cid:0)
21
(cid:1).
t2
2σ2 + 2bt/3
B.5 Tools from Previous Work
Theorem B.9 ([LNNT16]). Given parameters k ≥ 1,  ∈ (0, 1/2), δ ∈ (0, 1/2). There is a random-
ized (one-pass) algorithm that uses O((k + 1/2) log n/δ · log m) bits of space, requires O(log n) time
per update, needs O(k + 1/2) poly(log n) decoding time. For all i ∈ [n], let fi ∈ [− poly(n), poly(n)]
denote the frequency of i at the end of the data stream. Without loss of generality, we assume that
f1 ≥ f2 ≥ ··· ≥ fn. The algorithm is able to output a set H with size O(k + 1/2) such that, with
probability 1 − δ,
Property (II) : for all i ∈ [n], if f 2
Property (I) : for all (i,(cid:98)fi) ∈ H, f 2
Property (III) : for all (i,(cid:98)fi) ∈ H, |(cid:98)fi − fi| ≤ ((cid:80)n
j , then (i,(cid:98)fi) ∈ H ;
j /k − 2(cid:80)n
j /k + 2(cid:80)n
j ;
j=k+1 f 2
j=k+1 f 2
j=k+1 f 2
j )
2 .
i ≥
i ≥
(cid:80)n
(cid:80)n
j=1 f 2
j=1 f 2
C Why Do Previous Techniques Fail?
[FIS05] is one of the early works using sampling procedure to solve dynamic streaming
[FIS05]
geometric problem. They show that it is possible to use point samples over a dynamic point
set as a subroutine to solve several geometric problem, e.g. Euclidean Minimum Spanning Tree.
However, they only show how to implement the uniform sampling by using counting distinct elements
and subsampling procedure as subroutines.
In this work, we require assigning points diﬀerent
“importance”. The bottom level sampling scheme of ours is similar to theirs, but ours requires a
much complicated framework to implement the importance sampling over the point sets.
a set of centers, and P be the point sets then cost(Z, P ) =(cid:82) ∞
[Ind04] This paper uses a critical observation to estimate the cost of k-median, that is: let Z be
a summation with logarithmic levels, i.e.,(cid:82) ∞
|P − B(Z, r)|dr =(cid:80)∞
|P − B(Z, r)|dr, where B(Z, r) is
union of neighborhoods (of radius r) of every point in Z. Then this integration is approximated by
i=0 |P − B(Z, ri+1)|(ri+1 − ri),
where ri = O((1 + )i). The critical part is to estimate |P − B(Z, ri−1)|. This paper constructed a
counting data structure based on grids with side length O(ri). Then every input point is snapped
to a grid point. To obtain suﬃciently accurate counts for each |P − B(Z, ri−1)|, the data structure
needs to query |Z|/d many grid points per Z. Such a data structure is implemented using pair-wise
independent hash functions, and uses memory O(|Z|/O(d)).
Notice that this paper gives only an approximation of the cost. It is not a coreset. Therefore,
to obtain the k-median solution, an exhaustive search is used. Furthermore, their technique fails to
extend to k-means, which lacks the integration formula of the cost function.
Why Does [FS05] require exponential dependence in d, (1/)d? The grid structure of
our paper is inspired by [FS05]. In [FS05], they ﬁrst build a deterministic quadtree-like structure:
Consider a big cube containing the entire data set. This cube is treated as the root cell of the
tree. Going down a level, they partition the parent cell into 2d subcells. Then each leaf of the
tree contains at most one single point of the dataset. Notice that the cell side length decrease
geometrically as the level increase. They mark a cell as “heavy” if the cell containing enough points
that moving all points in the cell to the center of the cell incurs too much error in the cost of an
optimal k-means/median solution. Since the side length (or diameter) of cells decreases as level
increases, the number of points required to have this eﬀect becomes larger. Eventually, all cells are
non-heavy after some level. As such we also have a tree of heavy cells. For each heavy cell, the
coreset is constructed by assigning each point in the non-heavy children to its center. It turns out
if we want an epsilon-approximated coreset, the threshold of the non-heavy cell is exponential in d,
22
i.e., each non-heavy cell in level i cannot contain more than (cid:101)O(O(d) · OPT /2i) points, where OPT
is the optimal cost. This small threshold gives (cid:101)O(1/O(d)) many heavy cells.
Why do the previous insertion-only streaming coreset construction algorithms fail for
dynamic stream model? Many of the previous insertion only streaming coreset construction
algorithms (e.g., [FS12]) heavily depend on a merge-reduce technique, i.e.
read some points in
the stream, construct a coreset, then read another part, construct a new coreset, and merge the
two coresets. They repeat this procedure until the stream ends. This technique works well in
insertion only streaming model. But once some points get deleted, the framework fails immediately.
Though [BFL16] shows a new framework other than merge-reduce, their algorithm relies on a non-
deleting structure of the data stream as well.
Why Do [BFL+17] fail to extend to k-means? Though some k-median coreset construction
techniques can be easily extended to k-means coreset construction (see e.g. [BFL16, FS12]), their
construction can only be implemented in insertion only streaming model.
p∈P (dist(ci
p∈P (dist(ci
p, z), where each ci
, z)− dist(cL−2
p, z) − dist(ci−1
[BFL+17] showed a k-median coreset construction in dynamic streaming model. But their
construction cannot be extended to k-means coreset construction directly. Their k-median algorithm
heavily relies on writing the cost of each point as a telescope sum. For example, we consider the
of the cell in the i-th level containing p. Therefore, the total 1-median cost(cid:80)
1-median problem.
let z be a candidate center point and p ∈ P be a point, then dist(p, z) =
dist(p, z)− dist(cL−1
, z) + dist(cL−1
set P on z can be split into L pieces, i.e.,(cid:80)
p is the center
, z) +···− dist(c0
p∈P dist(p, z) of point
p, z)−dist(ci−1
, z)) is(cid:80)
then the estimator to(cid:80)
, z)) for each i ∈ [L]. [BFL+17]
estimates the cost of each L pieces by sampling points, i.e., let Si be the samples in the i-th level,
p, z) − dist(ci−1
p where
p∈Si(dist(ci
p is the probability that the point p is sampled in the samples Si. A crucial observation is that we
ζ i
have | dist(ci, z)−dist(ci−1, z)| ≤ ∆/2i – the cell size of level i which is independent from the location
inequality to show the high concentration of the estimator(cid:80)
of z. Since this nice upper bound on | dist(ci, z)−dist(ci−1, z)|, [BFL+17] then can apply Bernstein’s
only (cid:101)O(1/2) samples per level. But this framework does not work for 1-means even though one can
still write the telescope sum structure as(cid:80)
p with
estimator as(cid:80)
, z)), and we can still setup the
p, z) − dist2(ci−1
, z)| is not upper
p∈Si(dist2(ci
bounded by the cell size. Instead, the magnitude is depending on the location z. For example, it
p, z) − dist2(ci−1
, z)| ≥ ∆. If we apply the Bernstein’s inequality here,
can be as large as | dist2(ci
points per level, one may need (cid:101)Ω(2i) samples, which can be as large as ∆d. Thus no space saving
then since the upper bound of | dist2(ci
, z)| is larger than ∆, if we still sample the
is possible.
p, z)− dist2(ci−1
p. But | dist2(ci
p, z) − dist2(ci−1
p, z) − dist2(ci−1
p, z)− dist(ci−1
p(dist2(ci
, z))/ζ i
p∈Si(dist(ci
, z))/ζ i
, z))/ζ i
D Coreset Construction for k-means Based on Chen’s Framework
To formally introduce our coreset construction, we here explain the high level outline. We impose a
randomly shifted grid structure over the universe [∆]d. This is done independently of dataset. The
grid structure can be stored in memory with only negligible amount of space. We then prove there
are good properties of the randomly shifted grid, for a ﬁxed point set. Next we show how to extract
a coreset using the help of the grid and hash functions over the universe. After that we formally
prove our construction.
23
center point Z, the total cost can be written as a telescope sum(cid:80)
Figure 3: Telescope sum [BFL+17] fails for k-means. In the k-median problem, for a ﬁxed set of
p, Z)−dist(ci−1
, Z)). For
(cid:80)
, p) which is independent
each piece, | dist(ci
from the choice of Z. However, in the k-means problem, the telescope sum of the total cost is
p, Z)2−dist(ci−1
, Z)2|
, Z)2). For each piece, the upper bound of | dist(ci
may depend on the location of Z, and it can be at least ∆ in the worst case.
p∈P (dist(ci
, Z)| is always upper bounded by dist(ci−1
p, Z)2−dist(ci−1
p∈P (dist(ci
p, Z)−dist(ci−1
D.1 Deﬁnitions and Properties
Without loss of generality, we assume the dataset is from [∆]d and ∆ = 2L for some positive integer
L. Otherwise we can enlarge ∆ to the closest power of 2. The space [∆]d is partitioned by the
grid structure recursively as follows. The ﬁrst level of the grid contains a single cell, which is taken
as the entire space. For each higher level, we reﬁne our partition by splitting each cell into 2d
equal square sub-cells. In the ﬁnest level, i.e., the L-th level, each cell contains a single point. We
further randomly shift the boundary of the grids to achieve certain properties, which we will show.
Formally, our grid structure is deﬁned as follows.
Deﬁnition D.1 (Grids and cells). Let g0 = ∆. Let v be a vector chosen uniformly at random from
[0, g0 − 1]d. Partition the space {1, 2,··· , ∆}d into a regular Cartesian grid G0 with side-length
g0 and translated so that a vertex of this grid falls on v. Each cell C ⊂ [∆]d of this grid can be
expressed as
[v1 + n1g0, v1 + (n1 + 1)g0) × ··· × [vd + ndg0, vd + (nd + 1)g0) ⊂ [∆]d
for some (n1, n2,··· , nd) ∈ Zd. (Note that each cell is cartesian product of batch of continuous
interval)
For i ≥ 1, we deﬁne the regular grid Gi as the grid with side-length gi = g0/2i aligned such that
each cell of Gi−1 contains 2d cells of Gi. The ﬁnest grid is GL where L = (cid:100)log2 ∆(cid:101); the cells of this
grid therefore have side-length at most 1 and thus contain at most a single input point.
Each grid forms a partition of the point-set Q. There is a d-ary tree such that each vertex at
depth i corresponds to a cell in Gi and this vertex has 2d children which are the cells of Gi+1 that
it contains. For convenience, we deﬁne G−1 as the entire dataset and it contains a single cell.
Center Cells Next, we show that the randomly shifted grid structure has very good properties.
First, we ﬁx an optimal k-set Z∗ = {z∗
k} ⊂ [∆]d as the optimal k-means solution for the
(cid:88)
input dataset Q, i.e.,
2, . . . , z∗
1, z∗
dist2(q, Z) = OPT .
cost(Q, Z
) =
min
Z⊂[∆]d:|Z|=k
q∈Q
24
z∆k-mediantelescopesumd(p,z)=d(p,z)−d(c2p,z)+d(c2p,z)−d(c1p,z)+d(c1p,z)−d(c0p,z)+d(c0p,z)where|d(p,z)−d(c2p,z)|≤d(p,c2p)where|d(c2p,z)−d(c1p,z)|≤d(c2p,c1p)where|d(c1p,z)−d(c0p,z)|≤d(c1p,c0p)k-meanstelescopesumd2(p,z)=d2(p,z)−d2(c2p,z)+d2(c2p,z)−d2(c1p,z)+d2(c1p,z)−d2(c0p,z)+d2(c0p,z)but|d2(p,z)−d2(c2p,z)|≥∆but|d2(c2p,z)−d2(c1p,z)|≥∆but|d2(c1p,z)−d2(c0p,z)|≥∆c0pc1pc2ppThen we call a cell C in level Gi a center cell if it is close to some centers in Z∗. Formally
dist(C, Z
) = min
q∈C,z∗∈Z∗ dist2(q, z
) ≤
gi
2d
(1)
where gi = 2−i∆ is the side length of the cell and d is the dimension of the space. We show that
there are only a small number of center cells.
Lemma D.2. Let ξ denote the even that the number of center cells of all grids is upper bounded by
3kL/ρ. Then event ξ holds with probability at least 1 − ρ.
Proof. Fix an i and consider a grid Gi. For each optimal center z∗
j , denote Xj,α the indicator
random variable for the event that the distance to the boundary in dimension α of grid Gi is at
most gi/(2d). Since in each dimension, if the center is close to a boundary, it contributes a factor
at most 2 to the total number of center cells. It follows that the number of cells that have distance
at most gi/(2d) to z∗
j is at most
(cid:80)d
N = 2
α=1 Xj,α.
We denote Yj,α to be 2Xj,α, then
E[N ] = E
(cid:35)
Yj,α
(cid:34) d(cid:89)
α=1
d(cid:89)
α=1
E[Yj,α].
Using Pr[Xj,α = 1] ≤ (2gi/(2d))/gi = 1/d, we obtain
E[Yj,α] ≤ E[1 + Xj,α] = 1 + E[Xj,α] ≤ 1 + 1/d.
Thus E[N ] =(cid:81)d
α=1 E[Yj,α] ≤ (1 + 1/d)d ≤ e. Thus the expected number of center cells in a single
grid is at most (1 + 1/d)dk ≤ ek ≤ 3k. By linearity of expectation, the expected number of center
cells in all grids is at most ekL. By Markov’s inequality, the probability that we have more than
3kL/ρ center cells in all grids is at most ρ.
Heavy Cells The property of small number of center cells allows us to bound the number of cells
that containing too many points. Next we introduce the notion of heavy cells, in which there are a
large number of input points. Ideally, we would use OPT, the optimal cost of the k-means solution
to deﬁne our heavy cells. However, OPT is not known to us. We use an arbitrary guess o to OPT
to deﬁne the heavy cells.
Deﬁnition D.3 (Heavy Cell). Let o > 0 be ﬁxed number. For each i ∈ {0, 1,··· , L − 1}, a cell in
Gi is called o-heavy if it contains at least Ti(o) = d2
3k points. The cell in G−1 is always heavy,
g2
and no cell in GL is called heavy. Here we call Ti(·) the threshold function in level i. o is omitted
if it is clear from the context.
i · ρo
Since the side length gi is decreasing as i increases, the thresholding function Ti(o) is increasing
as i increases. In particular, Ti+1(o)/Ti(o) = 4 for any o > 0 and i ∈ {0, 1,··· , L − 1}. Therefore,
we have the following lemma.
Lemma D.4. For 0 ≤ i ≤ L, if a cell C in Gi is heavy, then its parent cell C(cid:48) in Gi−1 must be
heavy. If a cell C(cid:48) in Gi−1 is not heavy, then any of its children cells in Gi cannot be heavy.
25
Figure 4: Example of o-Partition
The next lemma bounds the number of heavy cells.
Lemma D.5. If the number of center cells is at most 3kL/ρ, then the number of o-heavy cells is
at most 16kL OPT /(ρo).
Proof. For a non-center heavy cell C, the contribution of the input points in C to the optimal
k-means cost is at least g2
4d2 Ti(o) ≥ oρ/(12k), since each of them is of distance at least gi/(2d) to
an optimal center in Z∗. Thus there are at most 12k OPT /(oρ) many non-center heavy cells in a
grid. In total there are at most 12kL OPT /(oρ) + 1 many non-center heavy cells, where the +1
term comes from G−1. Since there are at most 3kL/ρ many center cells, the total number of heavy
cells is at most
1 + (3kL/ρ)(1 + 4 OPT /o) ≤ 16kL OPT /(ρo).
The heavy cells allows us to construct the coreset. The number of heavy cells essentially gives
us a bound of how many samples we need obtain from the original dataset.
D.2 Recursive Partition Approach
Recall that in our coreset construction, we ﬁrst partition the input datasets into L + 1 disjoint
subsets. And then we sample our points separately from these partitions. To prove Theorem D.14,
we ﬁrst reﬁne the partition conceptually as follows.
Deﬁnition D.6 (o-Partition). Suppose parameter o > 0. Let Q ⊂ [∆]d be the input set. For each
level 0 ≤ i ≤ L we deﬁne a set of sets P i as follows. Initialize P i as an empty set. For each heavy
cell C in Gi−1, we group the non-heavy children cells of C in the following manner. First note
26
that Ti(o) = 4Ti−1(o), so by non-heaviness each child cell contains at most 4Ti−1(o) points of Q.
If all the non-heavy children together contain less than Ti(o) points of Q, we take all the points of
Q containing in them as a single set, and add it to P i. Otherwise, we make groups such that each
group of these non-heavy children cells contains at least Ti(o) points and at most 3Ti(o) points of
Q. We put the points of Q in each of these groups as a set and add it to P i. Our full partition of
Q is deﬁned as P = P 0 ∪P 1 ∪ . . . ∪ P L.
For each i ∈ {0, 1,··· , L}, we write set Pi in the following way,
Pi = {Pi,1,Pi,2,··· ,Pi,|Pi|}.
For each j ∈ [|Pi|], if set Pi,j contains at least Ti(o) points, then say set Pi,j is a heavy set. We use
|Pi,j| to denote the number of points of Q that contained in partition Pi,j.
Remark D.7. We remark that the algorithm and coreset construction does not need to know and
ﬁnd such a partition. The partition we given here is only for analysis purposes. Also note that
for each i ∈ {0, 1,··· , L}, the sets in P i give a partition for Qi, i.e., the set of points falling in
non-heavy children of a heavy cell.
Claim D.8 (Upper bound |Pi,j|). For each i ∈ {0, 1,··· , L}, we write set Pi in the following way,
Pi = {Pi,1,Pi,2,··· ,Pi,|Pi|}.
Then for each j ∈ [|Pi|], we have |Pi,j| ≤ 3Ti(o).
Proof. It directly follows by Deﬁnition D.6.
Fact D.9. For a partition P of points set Q, we have
L(cid:88)
|Pi|(cid:88)
L(cid:88)
|P| =
|Pi|, and
|Pi,j| = |Q|
i=0
i=0
j=1
Deﬁnition D.10. We say a partition P(d, g) is a α, β partition if
|P| ≤ αk
and
|Pi,j|dg2
i−1 ≤ β OPT .
Proof. Without loss of generality, we assume L = log ∆ is suﬃciently large, i.e., L ≥ 9. We ﬁrst
bound the cardinality of P. For the partition sets, we split them into three groups. The ﬁrst group
are these sets containing a center cell. Since there are at most 3kL/ρ center cells, thus the number
of such sets is bounded by 3kL/ρ.
The second group are these non-heavy sets, i.e., these sets containing less than Ti points. The
number of these sets are bounded by the heavy cells since each heavy cell can produce at most one
of such a set. Lemma D.5 shows that the number of heavy cells is at most 16kL OPT /(ρo).
27
|P| ≤ αk and
|Pi,j|dg2
i−1 ≤ β OPT
Lemma D.11. Given parameters o, α, β with 0 < o ≤ OPT, α = 36L OPT /(ρo), β = 120d3L, and
conditioned on the number of center cells is at most 3kL/ρ. Then the o-partition P satisﬁes
L(cid:88)
|Pi|(cid:88)
i=0
j=1
L(cid:88)
|Pi|(cid:88)
i=0
j=1
The last group are the remaining sets. Each of these sets contain at least Ti−1 many points by
construction. For any point in such a set, its distance to any center is at least gi/(2d). Thus the
contribution to OPT is at least Tig2
i /(4d2). Thus the number of such sets per level is bounded by
12k OPT /(ρo). In total, we can upper bound |P| in the following sense,
|P| ≤ 16kL OPT /(ρo) + 12kL OPT /(ρo) ≤ 30kL OPT /(ρo).
(2)
Next, we show
L(cid:88)
|Pi|(cid:88)
i=0
j=1
|Pi,j|(gi−1√d)2 ≤
3Ti(o)(gi−1√d)2
L(cid:88)
L(cid:88)
i=0
|Pi|(cid:88)
|Pi|(cid:88)
j=1
4d3oρ/k
i=0
j=1
≤ |P| · 4d3oρ/k
≤ 120d3L OPT .
where the ﬁrst step follows by |Pi,j| ≤ 3Ti(o) (Claim D.8), the second step follows by Ti(o) ≤ 4d3o
3k ,
the last step follows by Eq. (2).
D.3 Bounding the Close Parts
Given a set Z of k points, we partition each P i for 0 ≤ i ≤ L into r = O(log(∆√d)) parts based on
their distance to Z. Formally, let
i (Z) = {P ∈ P i : dist(P, Z) ≤ 2√dgi−1}.
P 0
and for 0 < j ≤ r
(3)
(4)
i (Z) = {P ∈ P i : 2j√dgi−1 < dist(P, Z) ≤ 2j+1√dgi−1}.
P j
Since the distance is upper bounded by ∆√d, thus r = (cid:100)log(∆√d)(cid:101) is suﬃcient to partition P i. It
is also easy to see that for any i ∈ [0, L] and j ≥ 1 we have
max
p∈P,P⊂P j
i (Z)
dist(p, Z) ≤ (2j+1 + 1)√dgi−1.
Based on this deﬁnition, we ﬁrst bound the error in P 0
Lemma D.12 (Cost of close parts). Suppose we are given an input point set Q, its (α, β)-partition
P 0 ∪P 1 ∪ . . . ∪ P L and an arbitrary ﬁxed set of k points Z ⊂ [∆]d. For each 0 ≤ i ≤ L, let P 0
i (Z)
be deﬁned in (3) as the partition sets that is in distance √dgi−1 to Z. Let P 0
i denote all the points
i with sampling probability ζ
in P 0
and weight 1/ζ. If
i be the independent sample of points in P 0
i (Z) of the coreset in the next lemma.
i (Z). Let multiset S0
(cid:20) 256
ζ ≥ min
2 ·
β2
kL · Ti(o) · ln
(cid:18) 2
(cid:19)
(cid:21)
, 1
for some , δ ∈ (0, 1/2) and o ∈ (0, OPT], then, with probability at least 1 − δ,
i |, kL · Ti(o)) · dg2
2β · max(|P 0
i , Z) − cost(S0
i , Z)
i−1.
(cid:12)(cid:12) cost(P 0
(cid:12)(cid:12) ≤
28
Proof. First we recall that
cost(P 0
i , Z) =
(cid:88)
p∈P 0
d(p, Z)2.
Let Ip be the indicator that a point p ∈ P 0
cost(S0
i , Z) =
is sampled. Then
Ip
d(p, Z)2.
(cid:88)
p∈Pj
The diﬀerence can be written as,
| cost(P 0
i , Z) − cost(S0
i , Z)| ≤
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:88)
p∈P 0
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .
(1 −
Ip
)d(p, Z)2
We have that
and for each p ∈ P 0
i ,
(cid:88)
p∈P 0
 = 0
(1 −
Ip
)d(p, Z)2
(cid:20)
(1 −
Ip
)2d(p, Z)4
(cid:21)
(1 − ζ)d(p, Z)4
16d2g4
i−1
Note that, |(1− Ip
(cid:88)
compute σ2 and b as follows,
ζ )d(p, Z)2| ≤ d(p, Z)2/ζ. In order to apply Bernstein’s inequality, Lemma B.8, we
(cid:20)
d(p, Z)2
16d2g4
(cid:21)
i−1
(1 −
Ip
)2d(p, Z)4
≤ |P 0
i |
and b = max
p∈P 0
4dg2
i−1
σ2 =
p∈P 0
If |P 0
i | ≥ kLTi(o), we choose t = 
i−1 and calculate
By Bernstein’s inequality, we have that
t2
i |dg2
i |2 · ζ
256β2
2β · |P 0
4σ2 = |P 0
(cid:18)
(cid:19)
d(p, Z)2
Ip
1 −
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:88)
p∈P 0
Pr
(cid:19)
and
t2
4bt/3
3|P 0
i | · ζ
32β
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) > t
 ≤ 2 exp
(cid:18)
(cid:18)
− min
≤ exp
≤ δ.
29
2σ2 + 2bt/3
t2
(cid:20) t2
t2
4bt/3
4σ2 ,
(cid:21)(cid:19)
If |P 0
i | ≤ kLTi(o), we choose t = 
kLTi(o)
|P 0
i |
t2
4σ2 =
Thus
Pr
We thus conclude the proof.
2β · kLTi(o)dg2
kLTi(o)2 · ζ
(cid:19)
(cid:18)
(cid:88)
256β2
Ip
1 −
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
p∈P 0
i−1 and calculate
t2
and
3kLTi(o) · ζ
32β
4bt/3
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) > t
 ≤ δ.
d(p, Z)2
D.4 Bounding the Far Parts
The goal of this section is to prove Lemma D.13. We show that the error of the coreset S over each
P j
i for i ∈ [0, L], j ∈ [1, r] is still bound.
Lemma D.13 (Cost of far parts). Suppose we are given an input point set Q, its (α, β)-partition
P 0 ∪P 1 ∪ . . . ∪ P L and an arbitrary ﬁxed set of k points Z ⊂ [∆]d. For each 0 ≤ i ≤ L and
j ∈ [r] where r = (cid:100)log(∆√d)(cid:101), let P j
i (Z) be deﬁned in (4) as the partition sets that is in dis-
tance (2j−1√dgi−1, 2j√dgi−1] to Z. Let P j
i be the
independent sample of points in P j
i denote all the points in P j
i with sampling probability ζ and weight 1/ζ. If
i (Z). Let multiset Sj
ζ ≥ min
(cid:19)
(cid:21)
, 1
2 ·
β2
Ti(o) · ln
(cid:20) 256
(cid:18) 2
(cid:12)(cid:12) ≤  · cost(P j
for some , δ ∈ (0, 1/2) and o ∈ (0, OPT], then, with probability at least 1 − δ,
2
β2 cost(Q, Z).
i , Z) − cost(Sj
(cid:12)(cid:12) cost(P j
i , Z) +
i , Z)
Proof. The proof is similar to that of Lemma D.12 except that we need to consider the partition
sets containing too few number of points. We additionally partition P j
i,2(Z) and
denote P j
i,1(Z)∪P j
i (Z) = P j
i,2(Z) such that
i,1 and P j
i,2 as the points in P j
∀P ∈ P j
i,1 and Sj
i,1(Z),|P| ≥
i,1(Z) and P j
Ti(o)
and ∀P
β2
(cid:88)
cost(P j
i,r, Z) =
d(p, Z)2.
(cid:48)
∈ P j
i,2(Z),|P
(cid:48)
| <
Ti(o)
β2 .
Similarly we denote Sj
that
i,2 as the sampled points in each set. For each r ∈ {0, 1}, we recall
Let Ip be the indicator that a point p ∈ P j
cost(Sj
i,r, Z) =
i,r is sampled. Then
Ip
d(p, Z)2.
p∈P j
i,b
(cid:88)
p∈Pi,r
30
The diﬀerence can be written as,
| cost(P j
i,r, Z) − cost(Sj
i,b, Z)| ≤
We have that
 (cid:88)
p∈P j
i,r
(1 −
Ip
)d(p, Z)2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:88)
p∈P j
i,r
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .
(1 −
Ip
)d(p, Z)2
 = 0
and for each p ∈ P j
i,r,
(cid:20)
(1 −
(cid:21)
Ip
)2d(p, Z)4
(1 − ζ)d(p, Z)4
24jd2g4
i−1
In order to apply Bernstein’s inequality, Lemma B.8, we compute σ2 and b as follows,
Note that, |(1 − Ip
ζ )d(p, Z)2| ≤ d(p, Z)2/ζ.
(cid:20)
(cid:88)
σ2 =
and
p∈P j
i,r
(cid:21)
(1 −
Ip
)2d(p, Z)4
≤ |P j
i,r|
24jd2g4
i−1
b = max
p∈P j
i,r
d(p, Z)2
(2j+1 + 1)2dg2
i−1
Case 1. r = 1.
For r = 1, we consider two cases:
Case 1a. |P j
i,1| ≥ |P j
By Bernstein’s inequality, we have
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:88)
p∈P 0
i,1
Pr
Since dist(P, Z) ≥ 2j−1dgi−1 for any P ∈ P j
i,1, with probability at least 1 − δ, we have
i−1 and compute,
(cid:21)(cid:19)
t2
4bt/3
≤ δ.
i,1| · ζ
64β
(cid:18)
t2
and
i,1|2 · ζ
256β2
8|P j
i,1 |Ti(o), then we choose t = 
i,1|22jdg2
4bt/3 ≥ |P j
4σ2 = |P j
t2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) > t
 ≤ 2 exp
(cid:20) t2
(cid:18)
(cid:19)
(cid:12)(cid:12)(cid:12) ≤  cost(P j
i,1, Z) − cost(Sj
− min
d(p, Z)2
i,1, Z)
1 −
Ip
4σ2 ,
i,1, Z).
(cid:12)(cid:12)(cid:12)cost(P j
(cid:113)
i,1| < |P j
≤ |P j
|P j
i,1| · |P j
8β ·
i,1 |Ti(o), then we choose
i,1 | · Ti(o) · 22j · dg2
i−1 ≤
8|P j
i,1|22jdg2
i−1.
31
Case 1b.
| P j
i,1 |Ti(o)
β2
t =
We compute,
t2
4σ2 = |P j
By Bernstein’s inequality, we have that
i,1 |Ti(o) ·  · ζ
64β
(cid:113)
|P j
i,1||P j
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) > t

t2
4bt/3
(cid:19)
and
(cid:18)
i,1 |Ti(o)2 · ζ
256β2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:88)
(cid:18)
p∈P j
Pr
i,1
Ip
1 −
(cid:20) t2
d(p, Z)2
(cid:21)(cid:19)
t2
4bt/3
− min
4σ2 ,
≤ 2 exp
≤ δ.
(cid:12)(cid:12)(cid:12)cost(P j
(cid:12)(cid:12)(cid:12) ≤  cost(P j
i,1, Z).
i,1, Z) − cost(Sj
i,1, Z)
Thus with probability at least 1 − δ, we have
Case 2. r = 2.
Lastly, we consider b = 2. Notice that each of these set contains at most Ti(o)/β2 points. We
now choose t = 
8β2|P j
i,2 |Ti(o)22jdg2
2bt/3 =
i−1, and compute
8β2|P j
i,2 |Ti(o) · 24jd2g4
i−1 ·
ζ ≥
12
σ2
which implies
(cid:1)
(cid:1)
t2
2σ2 + 2bt/3
t2
 16bt + 2bt/3
t
18b
2 · ζ · Ti(o)
144
exp(cid:0)
≤ exp(cid:0)
≤ exp(−
≤ exp(−
(cid:19)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤
where the second step follows by 16 + 2/3 ≤ 18. By taking a union bound, with probability 1 − δ,
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:18)
(cid:88)
p∈P j
i,2
Ip
1 −
d(p, Z)2
8β2|P j
i,2 |Ti(o)22jdg2
i−1.
By the construction, each part P ∈ P j
dist(P, Z) ≥ 2j√dgi−1 for j ≥ 1, thus dist(CP ∩ Q, Z) ≥ 2j−1√dgi−1. Therefore
i,2, it corresponding to a unique heavy cell CP in Gi−1. Since
8β2|P j
i,2 |Ti(o) · 22jdg2
i−1 ≤
8β2
4|CP ∩ Q| · 4 · 22(j−1)dg2
i−1 ≤
2
β2 cost(Q, Z).
(cid:88)
P∈P j
i,2
This concludes the proof.
32
Figure 5: (a) We show an example of partitioning the set of points from top to bottom. There are
four levels in total. The top level is the ﬁrst level, and the bottom level is the fourth level. The
number of blocks is decreasing, when the levels is increasing. Each blue bock is a heavy cell. In the
ﬁrst level, there is a single blue block. In the second level, we partition it into four cells. Only two
of them are heavy (blue) cells. In the third level, we partition the each of heavy cells (in the second
level) into four cells. One has one heavy (blue) cell, and the other has two (blue) heavy cells. (b)We
show an example of sampling points from those non-heavy (green) cells. In the second level, there
are two green cells, we sample 16 (red) points from each of them. In the third level, there are ﬁve
green cells, we sample 8 (red) points from each of them. In the fourth (bottom) level, there are 12
green cells, we sample 4 (red) points from each of them.
D.5 Our Coreset Construction
To explain our coreset construction, we ﬁrst deﬁned a probability of sampling in each level Gi, for
i ∈ [0, L], as follows,
(cid:20) 9 × 256
2
(144d3L)2
Ti(o)
· ln
πi(o, , δ) := min
(cid:18) 2(L + 1)∆dk
(cid:19)
(cid:21)
, 1
(5)
where , δ ∈ (0, 1/2) and o > 0 are ﬁxed parameters. We now deﬁne L + 1 sets of weighted samples
as follows. Denote the input point set Q ⊂ [∆]d. For 0 ≤ i ≤ L, let Ci ⊂ Gi be the set of non-heavy
cells, whose parent cell in Gi−1 is o-heavy. Let Qi be the set of points falling in a cell of Ci. Notice
that by Lemma D.4, Q0, Q1, . . . , QL are disjoint sets and form a partition of Q.
Coreset Construction Let Si be a multiset, obtained by sampling each point q in Qi indepen-
dently with probability πi(o, , δ). Each point in Si has weight 1/(πi(o, , δ)). Then our weight set
is obtained as
S(o, , δ) = S0 ∪ S1 ∪ . . . ∪ SL.
(6)
33
We omit (o, , δ) if it is clear from the context.
Notice that our coreset set construction is determined by the parameter o,  and δ. We show in
the next theorem the construction above gives a -coreset for k-means on Q.
Theorem D.14. Suppose 0 < o ≤ OPT, and we given a point set Q ∈ [∆]d. Let S(o, , δ) be
multiset deﬁned in (6). Then conditioning on event ξ holds, where we only have ekL/ρ center cells,
with probability at least 1 − δ, S(o, , δ) is an -coreset of k-means on Q and
(cid:19)
−2k2d7L4 log(1/δ) · OPT·o
(cid:20) 256
Proof. Let (cid:48) = /3. Take the sampling probability
β2
Ti(o) · ln
(cid:18) 2(L + log d)2∆dk
E[|S|] = O(
−1).
(cid:21)
, 1
ζ = π(o, , δ) = min
(cid:48)2 ·
Let S = S(o, , δ) be the obtained multiset.
(4) for i ∈ [0, L] and j ∈ [r] where r = (cid:100)log(∆√d)(cid:101). Let P j
falling in a part of P j
For any ﬁxed set Z ⊂ [∆]d of k points, let P j
i . Let Sj
i (Z) be the subsets of partitions deﬁned in (3) and
i ⊂ Q be the corresponding point sets
i be the corresponding sampled set of P j
i . Notice that
S = ∪i∈[0,L],j∈[r]Sj
i .
By Lemma D.12, Lemma D.13 and with a union bound over all possible i ∈ [0, L] and j ∈ [r], with
probability at least 1 − δ
∆dk ,
Conditioning on the above inequalities, we have
(cid:48)
(cid:18)
2β · |P 0
i | · Ti(o) · dg2
i−1
(cid:88)
(cid:48)
|cost(S, Z) − cost(Q, Z)| ≤
cost(P j
i∈[0,L]
i , Z) +
i∈[0,L],j∈[r]
(cid:19)
2(cid:48)
β2 cost(Q, Z)
By Lemma D.11,
thus
(cid:88)
i∈[0,L]
|P 0
i | ≤
12eLk
OPT
(cid:48)
2β · |P 0
i | · Ti(o) · dg2
i−1 ≤
(cid:48)
(cid:48)
2β · 144d3L OPT = 
OPT .
Since β = 144d3L, therefore
Hence have
(L + 1)(L + log d/2)/β2 ≤
(cid:88)
2(cid:48)
β2 cost(Q, Z) ≤ 
(cid:48)
i∈[0,L],j∈[r]
cost(Q, Z).
34
and
i , Z)
(cid:12)(cid:12)cost(S0
(cid:12)(cid:12) cost(P j
i , Z) − cost(P 0
(cid:12)(cid:12) ≤
i , Z)(cid:12)(cid:12) ≤ 
i , Z) − cost(Sj
(cid:88)
(cid:48)
(cid:48)
2β · |P 0
i | · Ti(o) · dg2
i−1
· cost(P j
i , Z) +
2(cid:48)
β2 cost(Q, Z).
Overall, we have
(cid:48)
|cost(S, Z) − cost(Q, Z)| ≤ 3
cost(Q, Z).
Notice that 3(cid:48) = . Since there are at most ∆dk possible Z, by a union bound, we have for all
Z ⊂ [∆]d with |Z| ≤ k,
|cost(S, Z) − cost(Q, Z)| ≤  cost(Q, Z).
Next we bound the number of points sampled. By Lemma D.11, there are at most
|P| ≤ αk =
12ekL
OPT
parts in the o-partition. Each parts contains at most 3Ti(o) points. Therefore, in expectation,
(cid:88)
i∈[0,L]
12ekL
E(|S|) =
|P i | · 3Ti(o) · πi(o, , δ)
OPT
(cid:18) k2d7L4
9 × 256
(cid:18) 1
(cid:19)
2
(cid:19)
· (3 × 144d3L)2 · ln
OPT
= O
log
2
(cid:18) 2(L + 1)∆dk
(cid:19)
Remark D.15. The coreset can be constructed independently and oblivious to the point set, i.e.,
using hash functions ho,i : [∆]d → {0, 1}, such that the probability of ho,i(p) = 1 is exactly πi(o, , δ).
Later, we pick the set of points with hash value 1, when it is available. Then we construct the coreset
as in 6.
Observing that in our proof, the only requirement to bound the cost of partition. Thus we have
the following proposition.
Proposition D.16. Suppose 0 < o ≤ OPT, and we given a point set Q ∈ [∆]d. Let S(o, , δ) be mul-
tiset deﬁned in (6). Then conditioning on event that there exists an o-partition (see Deﬁnition D.6)
satisfying
|P | ≤ αk
|P| · Ti(o) · dg2
i−1 ≤ d3L OPT
and (cid:88)
(cid:88)
i∈[0,L]
P∈P i
then with probability at least 1 − δ, S(o, , δ) is an -coreset of k-means on Q and
E[|S|] = O(
−2k2d7L3α log 1/δ).
E Coreset Over the Dynamic Data Stream
E.1 Streaming Coreset Construction
As a ﬁrst step, we modify our coreset construction slightly such that an streaming algorithm can
maintain the coreset construction. The only diﬃculty is that we cannot obtain the number of points
in a cell exactly. We overcome this diﬃculty by modifying our heavy cell deﬁnitions as follows.
Deﬁnition E.1 ((, o)-Heavy Cell Scheme). Fixing a number  ∈ [0, 1], a set of input points Q ⊂
[∆]d and a grid structure G = {G−1, G0, . . . , GL} over [∆]d deﬁned in Deﬁnition D.1. We call a
procedure an (o, )-heavy cell scheme if it satisﬁes,
35
1. ∀C ∈ G−1, C is o-heavy; ∀C ∈ GL, C is not heavy;
2. for i ∈ [0, L], if a cell C ∈ Gi with |C ∩ Q| ≥ Ti(o), then C is identiﬁed as o-heavy;
3. for i ∈ [−1, L], if a cell C with |C ∩ Q| < (1 − )Ti(o), then C is identiﬁed as not heavy;
4. for i ∈ [−1, L − 1] a cell C ∈ Gi is identiﬁed not heavy, then all its children cells C(cid:48)
As one can easily verify that the heavy cells in Deﬁnition D.1 is given by a (o, 0)-heavy cell
is identiﬁed not heavy;
∈ Gi+1
scheme. In particular, we have the following theorem.
Theorem E.2. Suppose 0 < o ≤ OPT, 0 ≤  ≤ 1, and we are given a point set Q ∈ [∆]d. Let
S(o, , δ) be multiset deﬁned in (6) using heavy cells deﬁned by any (o, )-heavy cell scheme. Then
conditioning on event ξ holds, where we only have ekL/ρ center cells, with probability at least 1− δ,
S(o, , δ) is an -coreset of k-means on Q and
−1).
E[|S|] = O(
−2k2d7L4 log(L/δ) · OPT·o
E.2 The Dynamic Point-Cell Storing Data Structure
In this section, we introduce a data structure that maintains the coreset in a dynamic data stream.
To begin, we ﬁrst introduce the K-Set data structure, which we use as a sub-routine in our algorithm.
Lemma E.3 (K-Set). Given parameter M ≥ 1, N ≥ 1, k ≥ 1, δ ∈ (0, 1/2). There is a data structure
that requires O(k(log M +log N ) log(k/δ)) bits and is able to process a data stream S(which contains
both insertion and deletion). The data structure processes each stream operation (i,±) (i ∈ [N ]) in
O(log(k/δ)) time. For each time t, let Mt denote the sum of the frequency of each the element. Let
M = maxt Mt. At each point of time t, it supports an operation Query, if the number of distinct
elements is ≤ k, then with probability at least 1− δ it returns the set of items and the frequencies of
each items. It returns ∅ otherwise.
Our coreset critically relies on the data structure deﬁned in Algorithm 2.
It deﬁnes a data
structure that supports inserting and deleting cell-point pairs. For example, each input is of the
form (C, p, op), where C is a cell in the grid, identiﬁed as its ID (including the information of its level
and coordinates), p ∈ C and op is + or − representing insertion or deletion. It has the following
guarantees.
Lemma E.4. There is a data structure (procedure SampleStream+ in Algorithm 2) that supports
insertion and deletions on cell-point pairs from [2L∆d] × [∆]d and a query operation. At any time
point, the query operation returns either of the follow two results,
(1) C, f and S = {SC : ∀C ∈ C}, where C is all the cells undeleted, f is a function encoding the
number of points in a cell in C, and ∀C ∈ C, SC is the set of points in C if |C| ≤ β;
(2) ∅.
If conditioning on the following three events,
(I) there are at most α non-empty cells;
(II) there are at most θ cells that each of them contains more than β points;
(III) the number of points in all the other cells that each containing most β points is at most γ,
then the algorithm outputs ∅ with probability at most δ. The algorithm uses
bits of space in the worst case.
O[(θβ + γ) log(∆d) · log((θβ + γ)/δ)].
36
Proof. We ﬁrst show that conditioning on the three events, our algorithm outputs the set of points
with high probability. We then show that if the three events do not happen, the output of the
algorithm always matches the requirement of the statement.
Let C denote the set of all cells, let P denote the set of all points. We also identify each
cell C ∈ C by its coordinate. We deﬁne hash function h : C → [a1 · α], g : [P ] → [a2 · β] and
f : [a1 · α] × [a2 · β] → [a3(θβ + γ)], where constant a1, a2, a3 ≥ 1(will be decided later) and h, g, f
are pair-wise independence function. For each w ∈ [a3(θβ + γ)], j ∈ [2 log(∆d)] and z ∈ {0, 1}, we
deﬁne counter
counter(w, j, z) ∈ {0, 1}.
We initialize the all the entries of the counter to be 0. Note the number of counters is 2(a3(θβ +
γ) log(∆d)). For each update (C, p, operation), we update the update the counter in the following
way,
counter(f (h(C), g(p)), j, (C, p)j) ← counter(f (h(C), g(p)), j, (C, p)j) ± 1 mod 2,∀j ∈ [2 log(∆d)],
where (C, p)j is the j-th bit of the pair (C, p), “+” is corresponding to insertion, and “−” is corre-
sponding to deletion. We also initialize an K-Set structure to store at most α cells, and update the
cell C to it.
For each w ∈ [a3(θβ + γ)] we deﬁne counter
size(w) ∈ {0, 1,··· , ∆d}.
We initialize the all the entries of the counter to be 0. Note the number of counters is a3(θβ + γ).
For each update (C, p, operation), we update the counter in the following way,
size(f (h(c), g(p))) ← size(f (h(c), g(p))) ± 1,
where “+” is corresponding to insertion, and “−” is corresponding to deletion.
For a ﬁxed cell C, for any cell C(cid:48)
(cid:54)= C, we have
(cid:48)
[h(C) = h(C
XC(cid:48) = 0 otherwise. We deﬁne X =(cid:80)
We deﬁne random boolean variable XC(cid:48) such that XC(cid:48) = 1 if h(C(cid:48)) = h(C) and b is non-empty;
C(cid:48) /∈C \{C} xC(cid:48), and compute E[X] = Θ(1/a1). By Markov’s
Pr
h∼H
)] = Θ(1/(a1α))
inequality,
Pr[X ≥ 1] ≤ E[X]/1 ≤ Θ(1/a1).
By choosing a1 to be suﬃciently large constant, we obtain with probability at least 0.99, there is
no non-empty cell C(cid:48)
∈ C \{C}, satisfying that h(C(cid:48)) = h(C).
For a ﬁxed x ∈ [a1α], conditioned on only one non-empty cell c is hashed into this bin and c has
at most β points. We consider a ﬁxed y ∈ [a2β], suppose there is a point p such that g(p) = y. For
any other point p(cid:48)(belong to the same cell C), since g is a pair-wise independence hash function,
then we have
(cid:48)
[g(p) = g(p
Pr
g∼G
)] = Θ(1/(a2β))
37
Y =(cid:80)
We deﬁne random variable Yp(cid:48) such that Yp(cid:48) = 1 if g(p) = g(p(cid:48)); Yp(cid:48) = 0 otherwise. We deﬁne
p(cid:48) /∈C∩P\{p}, and compute E[Y ] ≤ Θ(1/a2). By Markov’s inequality,
Pr[Y ≥ 1] ≤ E[Y ]/1 ≤ Θ(1/a3).
By choosing a3 to be suﬃciently large constant, we obtain with probability at least 0.99, there is
no p(cid:48)
∈ P ∩ C\{p}, satisfying that g(p) = g(p(cid:48)).
Moreover, there are at most (θβ + γ) distinct pairs (c, p). Repeating the above argument, we
observe that for give pair (h(C), g(p)), with probability at least 0.99, no other pairs (h(C(cid:48)), g(p(cid:48)))
has the same hash value f (h(C), g(p)).
We say an entry (C, p) is good, if the cell C is non-empty and contains at most β points. We
can show for any good entry (C, p), one can recover point p with probability at least 0.9. Since the
total number of good pairs is at most γ, thus repeating the above procedure Θ(log(γ/δ)) times, we
have for any good entry (C, p), we can recover point p with probability at least 1 − δ/γ. By taking
a union bound over at most γ pairs, we have with probability at least 1 − δ/2, we can recover all
the good entries (C, p). By taking a union with the event that the K-Set data-structure is working
correctly (with probability at least 1 − δ/2), we obtain the probability 1 − δ.
It remains to show that if any of the three events not happening our algorithm never outputs an
incorrect result. If the number of cells is greater than α, then either the K-set structure fails, or we
know the number of cells are greater than α, thus the algorithm always outputs ∅. If the remaining
events does not happen or there are two good pairs collide in their hash values, the algorithm can
always detect it by the counters. Thus the points recovered are always from the stream. A proper
pruning detects any possible error.
Overall, for a single repeat, the number of bits used by hash function is O(log(∆d)), the number
of bits used by counter counteri() is O(a3(θβ + γ)) log(∆d), the number of bits used by counter
sizei() is O(a3(θβ + γ)) log(∆d). The number of bits for K-Set is O(α log(∆d) log(α/δ)). Putting it
all together, th number of bits used over all repeats is
O((θβ + γ) log(∆d) · log((θβ + γ)/δ)).
E.3 Main Algorithm
Theorem E.5 (k-means). Fix , δ ∈ (0, 1/2), k, ∆ ∈ N+, let L = log ∆. There is a data structure
supporting insertions and deletions of a point set P ⊂ [∆]d, maintaining a weighted set S with
positive weights for each point, such that with probability at least 1− δ, S is an -coreset for k-means
of size
The data structure uses
−2k2d7L4 log(1/δ)),
O(
(cid:101)O(
−2k2d8L5 · (dkL + log(1/δ)) · log(1/δ))
bits in the worst case. For each update of the input, the algorithm needs poly(d, 1/, L, log k) time
to process and outputs the coreset in time poly(d, k, L, 1/, 1/δ, log k) after one pass of the stream.
38
Figure 6
Proof. Let the point set at the end of the insertion and deletion operations be Q ⊂ [∆]d. Without
loss of generality, we assume the optimal cost for k-means is at most OPT and OPT > 0. Indeed
we if OPT = 0, then the set Q contains at most k points. Thus we can always use a simple K-set
structure to dynamically maintain k points. The correctness of such an algorithm is guaranteed by
Lemma E.3. Further we observe that OPT ≥ 1, since each pair of points is of distance at least
1. To show our data structure, we ﬁrst pick a constant number ρ > 0 and use it as the failure
probability. Later we show we can boost it to arbitrarily small δ > 0. Furthermore, we prove our
theorem by showing a aδ failure probability, where a is some absolute constant. We can easily boost
the probability by only losing a constant factor in space.
The Data Structure Let ρ > 0 be a constant, i.e., take ρ = 0.01. Our data structure ﬁrst
initializes a randomized grid structure G−1, G0, G1, . . . , GL over [∆]d, as in Deﬁnition D.1. Then we
guess logarithmic many possible values for OPT, i.e., let O = {1, 2, 4, 8, . . . , poly log(∆d+2d)}. For
each guess of o, we initialize a SampleStream+ data structure SSo with parameters α, β, γ, θ, δ,
where α, β, γ, θ to be determined.
For each o and level i ∈ [−1, L], we initialize a new hash function ho,i : [∆]d → {0, 1} such that
∀p ∈ [∆]d : Pr[ho,i = 1] = πi(o, , δ) for some , δ ∈ (0, 0.5), where πi(·,·,·) is deﬁned in (5). For
representation simplicity, we ﬁrst assume that ho,is are fully independent hash functions. Later we
show that we can de-randomize them by using limited independence hash functions.
For each o ∈ O and i ∈ [−1, L], and each point p in operation, let Ci(p) be the cell containing p
in Gi. We also identify Ci(p) with its ID. For all i ∈ [−1, L], we ﬁrst compute its hash value ho,i(p),
if ho,i(o) = 1, we update (Ci(p), p) to the data structure SSo with the corresponding operations on
39
Ch:C→[a1α]h:C→[a1α]Pg:[P]→[a2β]g:[P]→[a2β]f:[a1α]×[a2β]→[a3(θβ+γ)]f:[a1α]×[a2β]→[a3(θβ+γ)]wsize(w)counter(w,∗,0)counter(w,∗,1)Choose hi : C → [a1α] to be pair-wise independent hash function
Choose gi : [P ] → [a2β] to be pair-wise independent hash function
Choose fi : [a1α] × [a2β] → [a3(θβ + γ)] to be pair-wise independent hash function
Initialize counter : counteri(w, j, z) ← 0,∀w ∈ [a3(θβ + γ)], j ∈ [2 log(∆d)], z ∈ {0, 1}
Initialize counter : sizei(w) ← 0,∀w ∈ [a3(θβ + γ)]
(cid:46) C, p the cell and the point, op ∈ {−, +}
(cid:46) Hash cell into bins
(cid:46) Hash point into bins
(cid:46) Update counter
(cid:46) Update counter
Si,C ← ∅, x ← hi(C)
for y ∈ [bβ] do
w ← fi(x, y)
if sizei(w) == 1 and ∀j ∈ [2 log(∆d)], counter(w, j, 0) + counter(w, j, 1) = 1 then
end for
end procedure
procedure Update(C, p, op)
procedure Init(∆, d, α, β, γ, θ, δ)
KS.Update(C, op)
for i = 1 → Rrepeats do
for j = 1 → [log ∆d] do
a1 ← Θ(1), a2 ← Θ(1), a3 ← Θ(1), Rrepeats ← Θ(log((θβ + γ)/δ))
Initialize K-Set data-structure : KS.Init(α, ∆d, δ/α)
C ← the set of all cells (i.e., the L + 2 levels of grids).
for i = 1 → Rrepeats do
x ← hi(C)
y ← gi(p)
w ← fi(x, y)
z ← j-th bit of the bit representation of (C, p)
counteri(w, j, z) ← counteri(w, j, z) op 1 mod 2
sizei(w) ← sizei(w) op 1
Algorithm 2 Better Data Structure with Small Number of Bits
1: procedure SampleStream+
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35:
36:
37:
38:
39:
40:
41:
42:
43:
44:
45:
46:
47:
48:
49:
50:
51:
52:
53:
54: end procedure
Si ← ∅
if KS.Check() (cid:54)= Fail or KS.size ≤ α then
end for
S ← Merge(S1, S2,··· , SRepeats)
if ∃C ∈(cid:101)C,(cid:101)f (C) ≤ β : |SC| (cid:54)= (cid:101)f (C) then
return (cid:101)C,(cid:101)f , S
return ∅
(C, p) ← BitToNumber(counter(w,∗,∗)),
Si,C ← Si,C ∪ {(C, p)}
(cid:101)C,(cid:101)f ← KS.Query()
for c ∈ (cid:101)C do
end procedure
procedure Query()
for i = 1 → Rrepeats do
else
end for
return ∅
end if
S ← S ∪ Si
else
end if
end procedure
40
end for
end for
end if
end for
p. This completes the description of the data structure. The complete data structure is presented
in Algorithm 2.
Retrieve The Coreset Next we describe how to retrieve the coreset from the data maintained in
the above data structure. First we ﬁnd the smallest o∗
∈ O such that SSo∗ does not return ∅. From
SSo∗, we obtain a set of cells (cid:101)C together with a function (cid:101)f that encodes the number of points in
each cell in(cid:101)C. We also obtain a set SC for each C ∈(cid:101)C ∩ Gi that is either ∅ or SC = C ∩ Qo,i, where
Qo,i = {q ∈ Q : ho,i(q) = 1}. For each C ∈(cid:101)C, we estimate the number of points in C, |C ∩ Q|, by
(cid:101)f (C)(1 + /2)/πi(o, , δ). With this, we identify all the heavy cells in(cid:101)C by following Deﬁnition D.14.
After this, we initialize our multiset S = ∅. For each non-heavy cell C in (cid:101)C, we assign a weight
1/πi(o, , δ) to each of point in SC. After this, add all the points in SC to S. This completes the
description of retrieving the coreset. The complete data structure is presented in Algorithm 3.
Correctness with Constant Probability We ﬁrst show that by chosen a constant ρ, i.e.,
for a ρ > 0, if the event ξ happens, i.e., there are at most ekL/ρ center cells in the grid, then
with probability at least 1 − δ, our data structure maintains an -coreset S for k-means. Notice
that Pr[ξ] ≥ 1 − ρ. To show the correctness of the data structure, we show that there exists an
1 ≤ o ≤ OPT, the output S is an -coreset for k-means on Q.
Heavy Cell Identiﬁcation First, for an 1 ≤ o ≤ OPT, we show that with probability at least
1 − δ, for all i ∈ [0,−L] and for all C ∈ Gi, the estimator of points in a cell,
(cid:98)co,i = |Qo,i ∩ C|/π(o, , δ) · (1 + /2),
together with the operations in Deﬁnition D.14 give an (o, )-heavy cell scheme (Deﬁnition E.1) by
identifying heavy cells from level −1 to L and removing those heavy cells with parent cells identiﬁed
as non-heavy.
Here
is deﬁned in (5). Indeed it is suﬃce to show that for all heavy cells C ∈ Gi, |C ∩ Qo,i|/π(o, , δ) is
(cid:18)
an (1 ± /2)-approximation to |Qo,i ∩ C|. Observing that
2 · (d3L)2 ·
E [|C ∩ Qo,i|] ≥ O
(cid:19)(cid:19)
dkL + log
by the Chernoﬀ bound and an union bound over all heavy cell cells, we obtain the desired precision
of estimation.
Correct Parameters of SampleStream In this section, we show that there exists an 1 ≤ o ≤
OPT such that with probability at least 1 − δ, an SampleStream instance SSo returns the set
of points in non-heavy cells together with all the estimates of the number of points in heavy cells.
To show this, it is suﬃce to show that an OPT /2 ≤ o ≤ OPT satisﬁes the above statement with
probability at least 1 − δ.
Firstly, we show that with probability at least 1 − δ/4 there are at most α (to be determined)
non-empty cells in all grids containing a sampled point (i.e., ho,i(p) = 1).
41
(cid:18)
(cid:19)(cid:19)
πi(o, , δ) = O
(d3L)2
Ti(o) ·
dkL + log
(cid:18) 1
2 ·
(cid:18) 1
Indeed, for each point in a non center cell, it contributes to the OPT with at least g2
i /(4d2) (see
Deﬁnition (1)). Therefore, there are at most
OPT
g2
i /(4d2)
4d2
g2
OPT = 4Ti(o) ·
ek
ρ ·
OPT
points in non-center cells per level of grid, where Ti(o) = d2
ek is the thresholding function. Let
g2
Q(cid:48)
o,i be the set of points in non-center cells of level i with ho,i value 1, i.e., the set of sampled points
from non-center cells. Therefore,
i · ρo
E [|Qo,i|] = 4Ti(o) ·
ek
ρ ·
OPT
· πi(o, , δ) = O
dkL + log
where we use the fact that OPT /o ≤ 2. By the Chernoﬀ bound and a union bound over all grids,
there are at most
(cid:19)(cid:19)
(cid:18) k
(cid:18)
2 · (d3L)2 ·
(cid:19)(cid:19)
points from non-center cells being sampled. Hence there are at most this number of non-center cells
containing a sampled point. Together with the number of center cells, there are at most
cells containing a sample point. Next we pick
(cid:18) kL
(cid:18) kL
(cid:18) 1
2 · (d3L)2 ·
dkL + log
α = O
2 · (d3L)2 ·
dkL + log
β = Θ
2 · (d3L)2 ·
dkL + log
(cid:19)(cid:19)
(cid:19)(cid:19)
(cid:18)
(cid:18)
(cid:18)
(cid:18) kL
(cid:19)
(7)
(8)
By the Chernoﬀ bound and a union bound over all cells, with probability at least 1 − δ/4, we
have that each non-heavy cell containing at most β/2 sample points. Next, since that are at most
O(kL/ρ) heavy cells, we set
θ = Θ
(9)
Then there are at most θ cells containing more than β sample points. Lastly, we pick γ = α as an
upper bound on the number of points in cells with at most β sample points. As such, conditioning
on the above events, by Lemma E.4, with probability at least 1 − δ, the data structure SSo outputs
the desired set of points and the cells with counts. And by Theorem E.2, the multiset is an -coreset
for k-means on Q.
Boost Probability from 1 − ρ − O(δ) to 1 − O(δ) The probability boosting procedure is by
δ ) times and take the output
independently repeating the above procedure in parallel u = O(log 1
as follows. For each r ∈ [u], suppose in the r-th repeat, the output guess (if exists) of OPT is or,
the SampleStream structure is SSr
or, the multiset is Sr and the number of heavy cells returned
r, i.e,
by SSr
estimates the number of points in non-heavy children of a heavy cell, and divide Ti(o) to obtain α(cid:48)(cid:48)
r.
Consider all the repeats with
r. Based on the heavy cell scheme, we also estimate the number of parts as α(cid:48)(cid:48)
or is α(cid:48)
for some large constant a1. Let the set of repeats satisfying this requirement be R.
(cid:48)(cid:48)
r ≤ a1kL
42
Let r∗ = arg minr∈R or. Then the ﬁnal output is taken as Sr∗.
To show that the overall failure probability of such an operation is at most O(δ), the ﬁrst claim is
that, if ∃or ≤ OPT and α(cid:48)(cid:48)
r ≤ a1kL, then in the repeat r, with probability at least 1− δ, the number
of parts in an or-partition (see Deﬁnition D.6) is at most O(kL). This simply follows from the
Chernoﬀ bound, since with probability at least 1 − δ, the number of points in a set with more than
Ti(o) points can be accurately estimated. Thus it is straight forward to verify that this partition
satisﬁes the requirement of Proposition D.16. Thus, by Proposition D.16, with probability at least
1 − δ the output multiset Sr is a -coreset for k-means on Q. Therefore, if there exists a guess
or ≤ OPT in the output sequence, then for the minimum, or∗
≤ OPT and with probability at least
1 − O(δ) we obtain the desired coreset.
Next, we show that there exists an r ∈ [u], and or ≤ OPT. Since each repeat is independent,
then with probability at least 1 − O(δ), there exists an r with at most ekL/ρ centers cells the grid
of the r-th repeat. Conditioning on this event, the r-th repeat outputs the multiset with or ≤ OPT
with probability at least 1 − O(δ). This concludes the probability boosting.
Space Bound and Random Bits Lastly we show the space algorithm. Each SampleStream
data structure uses
α log(
) · dL
= O
2 · (d3L)2 · dL ·
dkL + log
log(kLd/) + log log
bits of space. For all O(d log ∆) repeats, the overall space complexity is
(cid:16)
(cid:101)O
(cid:18) kd2L3
(cid:17)
(cid:18) kL
(cid:18)
(cid:18)
(cid:19)
(cid:19)
= (cid:101)O
(cid:18)
(cid:19)
(cid:18) kd8L5
(cid:18)
(cid:19)(cid:19)
(cid:19)
log
(cid:19)
· (d3L)2 ·
2
dkL + log
· log
2
dkL + log
Lastly for the independent hash function we use, we can de-randomize them by the method in
[FS05] by using z-wise independent hash functions for suﬃciently large z, or use a pseudo-random
generator as it in [Ind00b] and then apply a auxiliary algorithm argument as it in [BFL+17]. In
either case, one can show that the space for the random bits is not dominating. This completes the
proof.
F General Clustering Problem
F.1 M-Estimator Clustering
Our framework can be extended easily to other clustering problems, e.g., we consider the following
(cid:88)
M-Estimator clustering problem, ﬁxing Q ⊂ [∆]d,
(10)
min
Z⊂[∆]d:|Z|≤k
p∈Q
M (dist(p, Z))
Here M (·) is non-decreasing function satisﬁes
∀x > 0, M (x) > 0 and ∀c > 0, M (cx) ≤ f (c)M (x)
where f (c) > 0 is a bounded function.
43
Algorithm 3 Data Structure
1: procedure SampleKMeans
2:
3:
4:
5:
6:
7:
8:
procedure Init(k, ∆, d, , δ, δ)
O ← {1, 2,··· , poly(d, ∆d)}
ρ ← 0.01,
Construct G−1, G0, . . . , GL
for o ∈ O do
for i = 0 → L − 1 do
(cid:46) grids deﬁned in Section D.1 using parameter ρ
Construct hash function ho,i : [∆]d → {0, 1} s.t. Pr
ho,i∼H
α, θ, γ and β are determined in (7), (8) and (9)
RecordPointso ← SampleStream.Init(∆, d, α, β, γ, θ, δ)
end for
[ho,i(p) = 1] = πi(o, , δ)
(cid:46) op ∈ {−, +}
(cid:46) So = {So,0, So,1,··· , S0,L−1}
end for
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32: end procedure
return S
end for
end procedure
procedure Update(p, op)
for o ∈ O do
for i = 0 → L − 1 do
if ho,i(p) = 1 then
c ←Cell(p) in Gi
RecordPointso.Update(c, p, op)
end if
end for
end procedure
procedure Query()
for o ∈ O do
end for
o∗
(cid:101)Co,(cid:101)f o, So ← RecordPointso.Query()
S ← PointsToCoreset((cid:101)Co∗
← arg mino∈O{So,i (cid:54)= ∅,∀i ∈ {0, 1,··· , L − 1}}
,(cid:101)f o∗
, So∗
end procedure
Theorem F.1 (M-Estimator Clustering). Fix , δ ∈ (0, 1/2), k, ∆ ∈ N+, let L = log ∆. There is a
data structure supporting insertions and deletions of a point set P ⊂ [∆]d, maintaining a weighted
set S with positive weights for each point, such that with probability at least 1 − δ, S is an -coreset
for M-estimator clustering problem deﬁned in (10) of size
−2k2dηL4 log(1/δ)),
O(
where η is an absolute constant depending only on M. The data structure uses
(cid:101)O(
−2k2dηL5 · (dkL + log(1/δ)) · log(1/δ))
bits in the worst case. For each update of the input, the algorithm needs poly(d, 1/, L, log k) time
to process and outputs the coreset in time poly(d, k, L, 1/, 1/δ, log k) after one pass of the stream.
44
Proof. We can simply repeat the proof for k-means. In the proof, we need to modify heavy cell
deﬁnition, i.e., the thresholding function Ti(o),
Ti(o) =
M (gi/(2d)) ·
In the proof of Theorem D.14, we need to modify β to be,
L · f (√d)
(cid:32)
β = Θ
(cid:33)
ρ · o
ek
f (1/(2d))
The rest of the proof follows by replacing dist2(·) with M (dist(·)). We can verify that only the
dependence on d changes.
Improvements Over k-median
F.2
In this section, we show that our newly developed techniques can improve over the k-median con-
struction in [BFL+17]. In particular, we have the following guarantee.
Theorem F.2 (k-median). Fix , δ ∈ (0, 1/2), k, ∆ ∈ N+, let L = log ∆. There is a data structure
supporting insertions and deletions of a point set P ⊂ [∆]d, maintaining a weighted set S with
positive weights for each point, such that with probability at least 1 − δ, S is an -coreset for k-
median of size
The data structure uses
−2k · poly(d, L) · log(1/δ).
(cid:101)O(
−2k · poly(d, L) · log(1/δ))
bits in the worst case. For each update of the input, the algorithm needs poly(d, 1/, L, log k) time
to process and outputs the coreset in time poly(d, k, L, 1/, 1/δ, log k) after one pass of the stream.
Proof. Using the same construction in [BFL+17]. The improvements can be stated as follows.
Instead of take a union over the each level for union bounding the center cells, we bound the overall
center cells. This saves an L factor in the space. However doing so can only be achieved by using
our newly constructed SampleStream procedure, which additionally saves L factor.
Furthermore, we boost the failure probability from ρ to arbitrary δ > 0 using the procedure in
proof Theorem E.5.
Remark F.3. Using our newly developed technique, we save space complexity for O(1/δ) factor,
comparing with that in [BFL+17]. Furthermore, our newly developed stream sampling algorithm
also saves space up to poly(dL) factors.
G Applications
In this section, we show our dynamic data structure can be used to approximately maintain a
solution for many problems in the dynamic streaming setting.
45
G.1 A Dynamic Streaming Approximation to Max-CUT
In this section, we show that our coreset construction can be used to obtain a 1/2-approximation
of Max-CUT of the form as
M (dist(p, q))
(11)
(cid:88)
(cid:88)
p∈C1
q∈C2
max
C1∪C2
where M (·) is an M-estimator and C1 and C2 partition the streaming point set P ⊂ [∆]d. Formally,
we obtain the following result.
Theorem G.1 (Max-CUT). Fix , δ ∈ (0, 1/2), ∆ ∈ N+, let L = log ∆. There is a data struc-
O(cid:0) 1
(cid:1) bits in the worst case. For each update of the input, the algorithm needs
ture supporting insertions and deletions of a point set P ⊂ [∆]d, and outputs a 1/2-solution with
cost estimation up to a (1 ± ) factor with probability at least 1 − δ. The data structure uses
poly(d, 1/, L) time to process and outputs the coreset in time poly(d, L, 1/, 1/δ) after one pass of
the stream.
Proof. To solve Max-CUT, we simply use a random solution, i.e. use a hash function to maintain
two random cuts C1 and C2. Let OPTCUT be the optimal value for Max-CUT. It is a standard
result that,
2 · poly(d, L) · log 1
(cid:88)
(cid:88)
p∈C1
q∈C2
(cid:88)
(cid:88)
p(cid:48)∈S1
(cid:88)
(cid:88)
q(cid:48)∈S2
p(cid:48)∈S1
q(cid:48)∈S2
(cid:48)
M (dist(p
(cid:48)
, q
OPTCUT
M (dist(p, q))
 ≥
(cid:88)
(cid:88)
= (1 ± /2)2 (cid:88)
p(cid:48)∈S1
p∈C1
)) = (1 ± )
)) = (1 ± /2)
q∈C2
(cid:88)
(cid:88)
(cid:88)
q∈C2
M (dist(p
(cid:48)
, q))
M (dist(p, q)).
To approximate the cost, we use the coreset to obtain two (/2)-coresets for 1-M-clustering on both
C1 and C2. Let the coresets be S1 and S2. Then we show
M (dist(p
(cid:48)
(cid:48)
, q
M (dist(p, q)).
Indeed,
q∈C2
as desired. The probability boosting for the solution is standard.
p∈C1
G.2 A Dynamic Streaming Approximation to Average Distance
For similar proof of the Max-CUT, we obtain a dynamic estimation for average distance,
(cid:88)
|Q| − 1
p,q∈Q
M (dist(p, q))
where M (·) is an M-estimator.
Theorem G.2 (Average Distance). Fix , δ ∈ (0, 1/2), ∆ ∈ N+, let L = log ∆. There is a
O(cid:0) 1
(cid:1) bits in the worst case. For each update of the input, the algorithm needs
data structure supporting insertions and deletions of a point set P ⊂ [∆]d, and outputs a (1 ± )
approximation to the average distance of P , with probability at least 1 − δ. The data structure uses
poly(d, 1/, L) time to process and outputs the coreset in time poly(d, L, 1/, 1/δ) after one pass of
the stream.
2 · poly(d, L) · log 1
46
Proof. The proof is similar to that of Theorem G.1 by constructing a corset for 1-M-Clustering on
Q.
H (cid:101)O(k) Space Algorithm Based on Sensitivity Sampling
H.1 Deﬁnitions and Preliminaries
Deﬁnition H.1 (γ-important partition). Given a set of points Q ⊂ [∆]d, and parameter γ ∈
(0, 1/10). Let Pi,j be the partition sets deﬁned for each level i ∈ [−1, L]. Let P denote the partition,
We deﬁne sets R, R ⊂ {0, 1,··· , L} as follows
i
(cid:12)(cid:12)(cid:12)(cid:12) |Pi|(cid:88)
j=1
R =
|Pi,j| ≤ γ · Ti(o)
P = ∪L
i=0Pi = ∪L
i=0 ∪
|Pi|
j=1 {Pi,j}.
 , and R =
i
(cid:12)(cid:12)(cid:12)(cid:12) |Pi|(cid:88)
j=1
 .
|Pi,j| > γ · Ti(o)
We deﬁne important partition and non-important partition as follows
P I = ∪i /∈RPi and P N = ∪i∈RPi.
Let QI denote the set of γ-important points of Q that contained in γ-important partition P I. Let
QN denote the set of γ-non-important points of Q that contained in γ-non-important partition P N.
If in the context, the value of γ is clear, we sometimes will omit γ, and call QI as important
points, and call P I as important partition.
Theorem H.2 (Oﬀ-line Algorithm, [FL11]). Given a set of points Q ⊂ [∆]d, for each point p ∈ Q,
let s(p) denote the sensitivity of point p which is deﬁned as follows
s(p) =
max
Z∈[∆]d,|Z|=k
(cid:80)
dist2(p, Z)
q∈Q dist2(q, Z)
for each sample, it chooses point p with probability s(cid:48)(p)/t(cid:48) where s(cid:48)(p) ∈ [s(p), 1], t(cid:48) =(cid:80)
Let A denote a sampling procedure deﬁned in the following way: it repeats m independent samples;
p∈Q s(cid:48)(p);
and for each sample, if some point p got chosen, we associated weight w(p) = t(cid:48)/(ms(cid:48)(p)) to that
point. Let (S, w(·)) denote the set of points and weights outputted by A. If m ≥ Ω(t(cid:48)−2(log |Q| log t(cid:48)+
log(1/δ))), then with probability at least 1 − δ, (S, w(·)) is an (1 ± )-coreset of size m for Q.
H.2 Reducing Original Problem to Important Points
In this section, we show that if we want to output a coreset for the original point set, we only need
to ﬁnd the coreset for those important points. It means that we can drop all the points which are
in the “non-important” level.
Lemma H.3. Given a set of points Q ⊂ [∆]d associated with partition P deﬁned as follows,
Let P N ⊂ Q denote the set of non-important points associated with partition P N deﬁned as follows,
P = P0 ∪ P1 ∪ ··· ∪ PL.
P N = ∪i∈RPi,
47
(cid:80)|Pi|
j=1 |Pi,j| ≤ γTi(o)} and γ ≤ /(200Ld3ρ). Let QI denote the set of points that
where R = {i |
contained by non-important partitions P I. Then for all Z ⊂ [∆]d with |Z|, we have
(I) cost(Q, Z) ≥ cost(QI , Z)
(II) cost(Q, Z) ≤ (1 + ) cost(QI , Z)
Proof. Proof of (I). It is trivially true, because QN is a subset of Q.
Proof of (II). We consider a i which satisﬁes
|Pi|(cid:88)
j=1
|Pi,j| ≤ γ · Ti(o),
and ﬁx a j ∈ [|Pi|]. Let c(Pi,j) denote the cell in (i − 1)-th level which contains all the points in
partition Pi,j. For each point p ∈ Pi,j, let Ni,j denote the set of important points (⊂ QI) that
contained by cell c(Pi,j), there exists a point q ∈ Ni,j satisﬁes that
dist2(p, Z) ≤ 2 dist2(p, q) + 2 dist2(q, Z)
≤ 2dg2
≤ 2dg2
i−1 + 2 dist2(q, Z)
i−1 + 2
(cid:88)
|Ni,j|
q∈Ni,j
dist2(q, Z)
(12)
where the ﬁrst step follows by triangle inequality, the second step follows by deﬁnition of the grids,
the last step follows by an averaging argument.
According to deﬁnition of Ni,j,
|Ni,j| ≥ Ti−1(o) − γTi(o) ≥
Ti−1(o).
48
We can lower bound cost(Q, Z) in the following sense,
cost(Q, Z) = cost(QI , Z) + cost(QN , Z)
= cost(QI , Z) +
dist2(p, z)
≤ cost(QI , Z) + 2
|Pi,j| ·
i−1 +
|Ni,j|
dist2(q, Z)
≤ cost(QI , Z) + 2
|Pi,j| ·
i−1 +
Ti−1(o)
q∈Ni,j
dist2(q, Z)
≤ cost(QI , Z) + 2
|Pi,j| ·
i−1 +
Ti−1(o)
dist2(q, Z)
≤ cost(QI , Z) + 2
i−1 +
Ti−1(o)
dist2(q, Z)
≤ cost(QI , Z) + 4LγTi(o) ·
Ti−1(o)
q∈QI
dist2(q, Z)
(cid:19)
dg2
= cost(QI , Z) + 4LγTi(o) ·
≤ cost(QI , Z) + 4Lγ(d3ρo/(3k) + 8 cost(QI , Z))
≤ cost(QI , Z) + 4Lγ(d3ρ cost(Q, Z)/(3k) + 8 cost(QI , Z)).
cost(QI , Z)
Ti−1(o)
i−1 +
j=1
p∈Pi,j
j=1
i∈R
i∈R
(cid:88)
|Pi|(cid:88)
(cid:88)
(cid:88)
|Pi|(cid:88)
(cid:88)
|Pi|(cid:88)
|Pi|(cid:88)
(cid:88)
(cid:88)
i∈R
j=1
i∈R
j=1
γTi(o) ·
i∈R
dg2
dg2
dg2
dg2
dg2
(cid:18)
i−1 +
q∈Ni,j
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
q∈QI
q∈QI





(cid:80)|Pi|
where the second step follows by deﬁnition of cost, the third step follows by Eq. (12), the fourth
step follows by |Ni,j| ≥ Ti−1(o)/2, the ﬁfth step follows by Ni,j ⊂ QI, the sixth step follows by
i=1 |Pi,j| ≤ γTi(o), the seventh step follows by |R| ≤ L + 1 ≤ 2L, the ninth step follows by
Ti(o) = 4Ti−1(o) and Ti(o) = d2ρo/(3g2
It implies that
i k), and the last step follows by o ≤ OPT ≤ cost(Q, Z).
cost(Q, Z)
cost(Q\QN , Z) ≤
1 + 32Lγ
1 − 4Lγd3ρ/(3k) ≤
1 + /4
1 − /4 ≤ 1 + ,
where the second step follows by γ ≤ /(200Ld3ρ), and the last step follows by γ < 1/2.
Therefore, we complete the proof.
H.3 Sampling Scores of Important Points
In this section, we focus on the sensitivity of important points. We ﬁrst give a good upper bound of
sensitivity of important points. And then we show the sum of the upper bound o the sensitivities
is small.
Lemma H.4. Given a point set Q associated with partition P and parameter o ∈ (0, OPT]. For
each p ∈ Q, there must exist a unique partition Pi,j that contains p. Let QI ⊂ Q denote the set of
49
γ-important points, where γ ≤ /(200Ld3ρ). Then we have, for all Z ⊂ [∆]d with |Z| = k, for all
p ∈ QI,
(cid:80)
dist2(p, Z)
q∈QI dist2(q, Z) ≤ 60
d3
Ti(o)
Proof. Fix a p ∈ QI. Let Pi,j denote the partition that contains this point. Let c(Pi,j) denote
the cell in i − 1-th level which contains all the points in partition Pi,j. Let Ni,j denote the set of
non-important points belong to cell c(Pi,j). It is easy to observe that
(cid:48)
∃p
∈ Ni,j, dist2(p
(cid:48)
, Z) ≤
|Ni,j|
q∈Ni,j
dist2(q, Z)
(13)
(cid:88)
(cid:80)
+ 2
dist2(p, p(cid:48))
q∈QI dist2(q, Z)
q∈Ni,j dist2(q, Z)
dist2(p(cid:48), Z)
q∈QI dist2(q, Z)
i−1
dg2
q∈QI dist2(q, Z)
q∈QI dist2(q, Z)
q∈QI dist2(q, Z)
dg2
i−1
q∈QI dist2(q, Z)
q∈QI dist2(q, Z)
(cid:80)
(cid:80)
+ 2
+ 2
We have
(cid:80)
dist2(p, Z)
q∈QI dist2(q, Z) ≤ 2
≤ 2
≤ 2
= 2
≤ 2
≤ 2
≤ 2
(cid:80)
(cid:80)
(cid:80)
(cid:80)
(cid:80)
|Ni,j|
|Ni,j|
|Ni,j|
|Ni,j|
|Ni,j|
|Ni,j|
(cid:80)
+ 2
+ 4
+ 16
dg2
i−1
q∈QI dist2(q, Z)
dg2
i−1
OPT
dg2
OPT
+ 48
d3ρo
Ti(o)k OPT
+ 48
d3ρo
Ti(o)k OPT
+ 48
d3
Ti(o)
≤ 10
≤ 10
≤ 60
Ti(o)
Ti(o)
d3
Ti(o)
where the ﬁfth step follows by (cid:80)
where the ﬁrst step follows by triangle inequality, the second step follows by Eq. (13) and p(cid:48)
∈ c(Pi,j),
q∈QI dist2(q, Z) ≥ (1 − ) OPT ≥ OPT /2 (By Lemma H.3), the
sixth step follows by g2
3k , the eighth step follows
by Ti(o) = 4Ti−1(o) ≤ 5|Ni,j| ( according o-partition and γ-important partition, see Deﬁnition D.6
and Deﬁnition H.1), the ninth step follows by ρ ∈ (0, 1), 1/k ≤ 1, o ≤ OPT.
i , the seventh step follows by Ti(o) = d2
g2
i−1 ≤ 4g2
ρo
Thus, we complete the proof.
Lemma H.5. Given a point set Q associated with partition P of parameter o ∈ (0, OPT]. Let
QI ⊂ Q denote the set of γ-important points, where γ ≤ /(200Ld3ρ). For each important level
50
i ∈ R, let(cid:80)|Pi|
p∈QI
Proof.
j=1 |Pi,j| = ai · Ti(o). Then we have, for all Z ⊂ [∆]d with |Z| = k, for all p ∈ QI,
(cid:88)
i∈R
ai · 3.
(cid:88)
(cid:80)
dist2(p, Z)
q∈QI dist2(q, Z) ≤ 60d3 ·
(cid:88)
(cid:88)
(cid:88)
(cid:80)
(cid:88)
dist2(p, Z)
q∈QI dist2(q, Z)
Pi,j
p∈Pi,j∩QI
p∈QI
Ti(o)
d3
d3
60
|Pi,j| · 60
Ti(o)
Pi,j
≤60d3 ·
(cid:88)
i∈R
ai · 3.
where the ﬁrst inequality follows by Lemma H.4.
H.4 Algorithm
The goal of this Section is to prove Theorem H.6,
Theorem H.6 (k-means, linear in k). Fix , δ ∈ (0, 1/2), k, ∆ ∈ N+, and let L = log ∆. There is a
data structure supporting insertions and deletions of a point set P ⊂ [∆]d, maintaining a weighted
set S with positive weights for each point, such that with probability at least 1 − δ, S is an -coreset
for k-means of size
The data structure uses
−3k poly(d, L, log(1/δ)).
−3k poly(d, L, log(1/δ))
bits in the worst case. For each update of the input, the algorithm needs poly(d, 1/, L, log k) time
to process and outputs the coreset in time poly(d, k, L, 1/, 1/δ, log k) after one pass of the stream.
Remark H.7. For each level i, there are three kinds of points, (1) points in partition, (2) points in
heavy cell (these points belong to the partition of some level j ∈ {i + 1,··· , L}), (3) points in non-
partition and non-heavy cell (these points belong to the partition of some level j ∈ {0, 1,··· , i− 1}).
We ﬁrst introduce some auxiliary concentration results.
Theorem H.8 ([BR94]). Let k be an even integer, and let X be the sum of n k-wise independent
random variables taking values in [0, 1]. Let µ = E[X] and a > 0. Then we have
(cid:20)
(cid:21)
(cid:18) kµ + k2
(cid:19)k/2
a2
Pr
|X − µ| > a
≤ 8 ·
51
Algorithm 4 Sensitivity-Based Sampling
1: procedure Oracle()
2:
3:
4:
procedure Init(i, o, Gi)
(cid:46) Gi is the i-th grid
m ← c max(d3Lk−3(Ld + log(1/δ)), d3Lk−2(dL log(dLk) + log(1/δ))) for some constant c
RecordPoints ← SampleStream+.Init(∆, d, c1kL + c3mL, c2m/k, c3mL, c1kL, δ/(d∆d)c4) (cid:46)
c1, c2, c3, c4 are four constants
for j = 1 → 100m do
Prhj∼H [hj(p) = 1] = 1/Ti(o)/k
[∆]d → {0, 1} s.t.
(cid:46) Ti(o) is given in Deﬁnition D.3
(log(1/δ))-wise independent hash hj
independent
Construct
5:
6:
end procedure
procedure Update(c, p, op)
for j = 1 → 100m do
if hj(p) = 1 then
c ← Cell(p) in Gi
RecordPoints.Update(c, (p, j), op)
end if
end for
end for
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35:
36: end procedure
end procedure
procedure Query(j(cid:48))
(cid:101)C,(cid:101)f , S1, S2, . . . , S100m ← RecordPoints.Query() (cid:46) Sj contains the samples of the form (p, j)
(cid:98)Sj ← ∅
for C ∈ (cid:101)C do
Return Fail if RecordPoints returns Fail
Identify heavy cells using the samples from S1, S2, . . . , S100m
(cid:46) Partition cell: a cell is not heavy, but its parent is heavy; The criterion is based on (cid:101)f (C)
(cid:98)Sj ← (cid:98)Sj ∪ Sj,C
(cid:46) Sj,C denotes the set of samples of Sj containing in C
if C is a partition cell in this level then
end if
end for
end procedure
procedure EstNumPts( )
Let j denotes the j(cid:48)-th non-empty (cid:98)Sj(cid:48). If no such j(cid:48) exists, return Fail
return A uniform sample from (cid:98)Sj
Denote the same (cid:98)Sj as in Query
return | ∪j (cid:98)Sj| · Ti(o) · k/(100m)
Return 0 if any step in Query returns Fail
end procedure
Lemma H.9. Let π ∈ [0, 1] be a ﬁxed value. Suppose we have m independent (4 log(1/δ))-wise hash
functions h1, h2, . . . , hm : [∆]d → {0, 1} with that ∀i ∈ [m], p ∈ [∆]d : Pr[hi(p) = 1] = π. Let set
S ⊂ [∆]d and denote hi(S) = {s | hi(s) (cid:54)= 0, s ∈ S}. For each i ∈ [m], s ∈ S, we deﬁne Xi,s to be
hi(s). Let X =(cid:80)m
s∈S Xi,s. Then with probability at least 1 − δ,
(cid:80)
i=1
(cid:12)(cid:12)X − mπ|S|
(cid:12)(cid:12) ≤ mπ|S|
provided m ≥ c log δ(1/δ)
π|S|2
for some suﬃciently large constant c ≥ 4.
52
O ← {1, 2, 4, 8, 16··· , poly(d, ∆d)}
ρ ← 0.01,
Construct G−1, G0,··· , GL
Initialize oracleo,l ← oracle.Init(l, o, Gi)
procedure Init(k, ∆, d, , δ)
Algorithm 5
1: procedure SampleKMeansLinear()
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15: end procedure
end procedure
procedure Update(p, op)
for l = 0 → L do
end for
oracleo,l.Update(p,op)
for o ∈ O do
end procedure
end for
(cid:46) grids deﬁned in Section D.1 using parameter ρ
(cid:46) Algorithm 4
Ao,i, ﬂago,i ← Oracleo,i.EstNumPts()
So,i ← ∅
end for
γ ← /(200Ld3ρ)
(cid:46) Important levels
R ← {i | Ao,i > γ · Ti(o)}
m ← c max(d3Lk−3(Ld + log(1/δ)), d3Lk−2(dL log(dLk) + log(1/δ))) for some constant c
for j = 1 → m do
Ao,i
Sample a level i ∈ R with probability
q, ﬂag ← Oracleo,i.Query(j)
if ﬂag (cid:54)= Fail then
i(cid:48)∈R Ao,i(cid:48)/Ti(cid:48)(o))/(m/Ti(o))
(cid:80)
i(cid:48)∈R Ao,i(cid:48)
for o ∈ O do
procedure Query()
for i = 0 → L do
Algorithm 6
1: procedure SampleKMeansLinear()
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26: end procedure
So,i ← So,i ∪ (q, wq)
So,i ← ∅
break
wq ← ((cid:80)
end for
o∗
return So
end procedure
end if
else
end for
← arg mino∈O{So,i (cid:54)= ∅, ﬂago,i (cid:54)= Fail,∀i ∈ {0, 1,··· , L − 1}}
(cid:46) Algorithm 4
(cid:46) weights of i-th level
(cid:46) So = So,0 ∪ So,1 ∪ ··· ∪ S0,L−1
Proof. It is obvious that
m(cid:88)
(cid:88)
i=1
s∈S
X =
Xi,s =
m(cid:88)
(cid:88)
i=1
s∈S
hi(s) =
m(cid:88)
i=1
|hi(S)|.
Then we have each Xi,s is a (log(1/δ))-wise independent random variable. Since m ≥ c log(1/δ)
π|S|2
, we
53
have that
Let k = 4 log(1/δ). Thus by Lemma H.8, with probability at least
mπ|S| ≥ c log(1/δ) and 2mπ|S| ≥ ck.
we have
1 − 8((kµ + k2)/(mπ|S|)2)k/2 ≥ 1 − (c/2)k/2 ≥ 1 − δ,
(cid:12)(cid:12)X − mπ|S|
(cid:12)(cid:12) ≤ mπ|S|
provided c ≥ 4.
Lemma H.10. Let k ≥ 1 be some ﬁxed value. Suppose we have 100m independent pairwise hash
functions h1, h2, . . . , h100m : [∆]d → {0, 1} with that ∀i ∈ [100m], p ∈ [∆]d : |S|· Pr[hi(p) = 1] = 1/k.
i=1 1|hi(S)|>0. Then with
probability at least 1 − δ,
Let set S ⊂ [∆]d and denote hi(S) = {s : hi(s) (cid:54)= 0, s ∈ S}. Let X =(cid:80)m
X > m/k,
provided m/k ≥ c log(1/δ) for some suﬃciently large constant c.
Proof. Denote, for j = 1, 2, . . . , 10m/k,
Yj =
i=10(j−1)k+1
|hi(S)|.
Thus E[Yj] = 10 and Var[Yj] ≤ 10. Thus by Chebyshev’s inequality, we have
Pr[|Yj − 10| ≥ 5] ≤ 10/25 = 0.4.
Thus E[X] ≥ 4m/k. Denote
Xj =
i=10(j−1)k+1
1|hi(S)|>0.
j=1 |Pi,j| = ai · Ti(o). Conditioned on StreamSampling+
Then each Xj is an independent random variable. The lemma follows from applying Chernoﬀ bound
over(cid:80) Xj.
Claim H.11. For each i ∈ R, let (cid:80)|Pi|
doesn’t fail, then with probability at least 1 − δ, for all o, i, the number of non-empty oracleo,i.(cid:98)S1,
oracleo,i.(cid:98)S1, ··· , oracleo,i.(cid:98)S100m is at least mai/k.
Notice that the expectation of |oracleo,i.(cid:98)Sj| = ai/k. Since mai/k ≥ c(cid:48) log(∆d/δ) for a suﬃcient
at least 1 − δ, the number of non-empty oracleo,i.(cid:98)S1, oracleo,i.(cid:98)S1, ··· , oracleo,i.(cid:98)S100m is at
Claim H.12. For each i ∈ R, let(cid:80)|Pi|
o∗, with probability at least 1 − δ,(cid:80)
Proof. Since i ∈ R, and γ = /(200Ld3ρ). we know that ai ≥ /(200Ld3ρ) by Deﬁnition H.1.
large constant c(cid:48), then by Lemma H.10 and taking union bound over all o, i, we have with probability
j=1 |Pi,j| = ai · Ti(o). For the smallest valid o ∈ (0, OPT], i,e,
i(cid:48)∈R ai(cid:48) > 100k.
least mai/k.
10jk(cid:88)
10jk(cid:88)
54
Proof. Let us ﬁx an o ∈ O, i ∈ {0,··· , L − 1}.
E[memory size for oracleo,i]
100m
kTi(o) · (#partition points + #non-partition and non-heavy points)
100m
kTi(o)
(cid:33)
(cid:88)
(cid:32)
c2m
k · #heavy cells in level i
ai(cid:48)Ti(cid:48)(o)
L(cid:88)
ai · Ti(o) +
i(cid:48)<i
c2m
k · #heavy cells in level i
i=0
i=0
ai
ai +
100m
(cid:33)
k · (100 + c2)
c2m
k · #heavy cells in level i
(cid:32) L(cid:88)
If o is the smallest and(cid:80)L
i=0 ai ≤ 100k, then let o(cid:48) = o/2. Since the probability that each point
is sampled blow up by twice, the expected memory only blows up by twice. Since m is large enough,
with probability at least 1 − δ, the actual memory used by o(cid:48) is at most twice of the expectation of
used memory size. It means o(cid:48) is a smaller valid guess, thus it leads to a contradiction to o is the
smallest valid guess.
Claim H.13. Given parameters γ ∈ (0, 1/10) and the smallest valid guess o ∈ (0, OPT]. Let
j=1 |Pi,j| = ai · Ti(o). Let
R = {i ∈ {0, 1,··· , L} |
m denote the number of samples sampled from all the levels (Algorithm 6). Then with probability at
least 1 − δ, the number of samples sampled from level i is at most mai/(50k).
Proof. Recall that m is number of samples in Algorithm 6. We can compute the expectation of
number samples choosing level i,
j=1 |Pi,j| > γ · Ti(o)}. For each i ∈ R, let (cid:80)|Pi|
(cid:80)|Pi|
E[#samples choosing level i] = m ·
= m ·
(cid:20)
Pr
∃o, i, #samples choosing level i ≥ m ·
(cid:80)
(cid:80)
(cid:80)
ai(cid:80)
i(cid:48)∈R
i(cid:48)∈R ai(cid:48)
p∈Pi d3/Ti(o)
p∈Pi(cid:48) d3/Ti(cid:48)(o)
(cid:21)
ai(cid:80)
i(cid:48)∈R ai(cid:48) ·
≤ δ,
Since m is suﬃciently large, by using Chernoﬀ bound and taking union bound over all o, i, we have
Further using Claim H.12, we have
ai(cid:80)
i(cid:48)∈R ai(cid:48) ·
m ·
2 ≤
mai
50k
Thus, with probability 1 − δ, for all o, i, the number of samples sampled from level i is at most
mai/(50k).
55
Lemma H.14. At the end of one-pass of stream, for each ﬁxed o ≤ OPT, for each i ∈ {0, 1,··· , L},
we know Pi is important or not. If Pi is γ-important, then we can output Ao,i such that
|Pi|(cid:88)
j=1
|Pi|(cid:88)
j=1
(1 − )
|Pi,j| ≤ Ao,i ≤ (1 + )
|Pi,j|
with probability 1 − δ.
Proof. Note that in the algorithm each point is copied 100m times. And in the ith level, each point
is sampled with probability 1/(Ti(o)k). Since each heavy cell has at least Ti(o) number of points,
then according to Lemma H.9, the number of points in the heavy cell which are sampled is at least
(1 − )100m/k. Thus, at the end of the stream, we are able to identify the heavy cells which means
that for each given point, we are able to determine whether it is a partition point, a heavy cell point
or a non-partition non-heavy cell point in the ith level.
Now consider a level i which is important, which means that the number of partition points
is at least γTi(o). Recall that γ = /(200Ld3ρ). Then since m is suﬃciently large, we can apply
Lemma H.9 again such that the number of partition points in the level i which are sampled is in the
range (1±)100m/(kTi(o))·
j=1 |Pi,j|.
j=1 |Pi,j| ≤ Ao,i ≤ (1+)(cid:80)|Pi|
(cid:80)|Pi|
j=1 |Pi,j|. Thus, we have (1−)(cid:80)|Pi|
(cid:80)|Pi|
j=1 |Pi,j| > γ · Ti(o)}. For each i ∈ R, let (cid:80)|Pi|
probability at least 1 − δ,(cid:80)
Proof. We prove by contradiction. Suppose(cid:80)
i∈R ai ≤ O(kL).
Claim H.15. Given parameters γ ∈ (0, 1/10) and a valid guess o ∈ (0, OPT]. Let R = {i ∈
j=1 |Pi,j| = ai · Ti(o). Then with
{0, 1,··· , L} |
i∈R ai ≥ ω(kL). According to Lemma H.9, the number
of partition points stored in our data structure will be at least ω(kL) which contradicts to the total
space used by our data structure.
Now in the following, we give the whole proof of our main theorem.
proof of Theorem H.6. It is easy to see the total space used is small: we actually maintained |O| ×
L number of oracles, and by Lemma E.4, each oracle uses space at most k−3 poly(L, d, log k,
log d, log(1/δ)) bits.
Now, let us look at the correctness. Firstly, we can argue that the algorithm will output a valid
o∗ with o∗ < OPT . Let o ∈ [OPT /2, OPT], we look at level i, the total number of non-heavy cell
i /(2d)2) where the ﬁrst term is all the non-heavy cell points
points is at most 3kL/ρ·Ti(o)+OPT /(g2
which are in the center cells, and the second term is all the non-heavy cell points which are not
i /(2d)2) ≤ 24kTi(o) then we have that the total number of non-
in the center cells. Since OPT /(g2
heavy cell points in the level i is at most 27kLTi(o). Thus, due to Lemma H.9, the total number
of sampled non-heavy cell points is at most O(mL). Furthermore, due to Lemma D.5, the total
number of heavy cells is at most O(kL). Thus, all the oracles will not FAIL. Thus, the algorithm
will choose an o∗ which is at most OPT .
Since o∗ is a valid guess, then according to Claim H.15, Lemma H.5 and Theorem H.2, m samples
is enough to get a good coreset. In the following of the proof, our goal is to prove that our m samples
is actually good.
According to Lemma H.14, we can identify all the important levels. And according to Lemma H.3,
we only need to show that these m samples provide a coreset of the important points.
56
According to Lemma H.14, we can estimate the number of partition points in each important
level up to a approximation factor within 1 ± . Thus, we can sample each important level i with
probability proportional to
(cid:80)
(cid:80)
(cid:80)
p∈Pi d3/Ti(o)
i(cid:48)∈R
p∈Pi(cid:48) d3/Ti(cid:48)(o)
up to a factor 1 ± . Now the only thing we remaining to prove is that we can implement uniform
sampling over all the partition points for each important level. Since o∗ is the smallest valid guess
of o, Claim H.13 shows that the number of uniform samples needed from level i is upper bounded
by mai/(50k). Due to Claim H.11, the number of uniform samples the corresponding oracle can
provided is at least mai/k. Thus, we can get enough uniform samples.
Thus, we are able to sample m i.i.d. samples such that each point p is chosen with probability
proportional to
(cid:80)
i(cid:48)∈R
(1 ± )
(cid:80)
d3/Ti(o)
p∈Pi(cid:48) d3/Ti(cid:48)(o)
where p is the partition point in the level i. By applying Lemma H.4, Theorem H.2 and Lemma H.3,
we complete the proof of the correctness.
57
