
Imagine we are interested in setting up a classiﬁcation program for deciding
whether a given word belongs to a certain language. This can be seen as a
mission in supervised machine learning, where the machine experiences labeled
data about the target language. The label is 1 if the datum is contained in
the language and 0 otherwise. The machines task is to infer some rule in order
to generate words in the language of interest and thereby generalize from the
training samples. Inductive Inference provides a model and diﬀerent performance
measures, allowing to abstract from likewise important questions concerning
details of the implemantation and instead focuses on what general properties of
the learning process can be achieved.
More formally, according to Gold [1967] the learner is modelled by a com-
putable function, successively receiving sequences incorporating more and more
data. Thereby, it possibly updates the current description of the target language
(his hypothesis). Learning is considered successful, if after some ﬁnite time the
2
Martin Aschenbach, Timo K¨otzing, and Karen Seidel
learner’s hypotheses yield good enough approximations to the target language.
The original and most common learning success criterion is called explanatory
(Ex-)learning or (Lim-)learning in the limit and additionally requires that the
learner ﬁnally settles on exactly one correct hypothesis, which precisely recog-
nizes the words in the language to be learned. Later on, allowing a vacillation
between ﬁnitely or inﬁnitely many descriptions and admitting for ﬁnitely many
anomalies, i.e., deviations between the described language and target language,
have been considered, see for example Case [2016].
Analyzing diﬀerent measures of learning success answers natural questions re-
garding the learning process: Will learning power reduce if only inconsistent
hypotheses are allowed to be changed? Can learning proceed strongly mono-
tonically, that is, every hypothesis is a subset of all later hypotheses? Are we
successful on more collections of languages, in case we only demand monotonoc-
ity for the sets of correctly inferred words? How does this relate to requiring a
cautious learner, which never guesses supersets of later hypotheses?
Osherson, Stob, and Weinstein [1986] analyze several restrictions for learning
from informant and mention that cautious learning, which forbids to ever con-
jecture a strict subset of an earlier conjecture, is a restriction to learning power;
we extend this statement with our Proposition 22.
Furthermore, they consider a version of conservativeness where mind changes
(changes of the hypothesis) are only allowed if there is positive data contradict-
ing the current hypothesis, which they claim to restrict learning power. In this
paper, we stick to the deﬁnition of B¯arzdi¸nˇs [1977] and Blum and Blum [1975],
who deﬁne conservativeness for learning from informant such that mind changes
are allowed also when there is negative data contradicting the current hypoth-
esis, which is arguably the more natural deﬁnition in the case of learning from
informants. While the work of Lange and Zeugmann [1994] considers variously
restricted learning of indexable families, i.e., sets of languages for which there is
a uniform decision procedure, we also deal with arbitrary collections of recur-
sively enumerable sets. See also Lange, Zeugmann, and Zilles [2008] for a survey
on learning indexable families.
For purely learning from positive information, so-called texts, there are en-
tire maps displaying the pairwise relations of diﬀerent learning restrictions, see
K¨otzing and Palenta [2014] and Jain, K¨otzing, Ma, and Stephan [2016]. With
this paper we give an equally informative map for the case of learning from in-
formant. To derive this, in Section 3 normal forms and a regularity property for
learning from informants are provided. Especially, it is shown that learners can
be assumed total and the presentation of the informant to be canonical, that is,
presenting the data and their labels following the common well-ordering of N.
Already Gold [1967] was interested in these normal forms and proved that they
can be assumed without loss of generality in the basic setting, whereas our re-
sults apply to all so called delayable learning success criteria and may be helpful
to generalize insights, that are so far bounded to Lim-learning.
In Section 4 we proceed to analyze the set of delayable learning restrictions,
which share the common feature that hypotheses can be delayed without violat-
Learning Families of Formal Languages from Informants
ing the learning restriction (by contrast, a hypothesis which is consistent now
might not be consistent later due to new data, so the restriction of consistency
is not delayable). We show that the promiment requirement of conservativeness
does not restrict learning power, even when paired with (strong) decisiveness, re-
quiring the learner not to return to an abandoned conjecture. This is essential for
understanding the relations between the diﬀerent delayable learning restictions,
when Lim-learning from informants, since with it a number of other learning
criteria are also non-restrictive. The proof is an intricate simulation argument,
combining diﬀerent techniques in the ﬁeld.
We complete the map of delayable learning restrictions by showing that the afore-
mentioned learning restrictions of cautiousness, avoiding hypotheses for proper
subsets of previously guesses, and of monotonicity, never removing correct data
from a conjecture, are incomparable in terms of learning power. This yields all
pairwise relations of learning criteria, being displayed in Figure 1 and Figure 2.
In Section 5 we generalize another result by Gold [1967], namely Lim-learning
from texts, where negative information is only implicit in the absence of certain
data, to be harder than Lim-learning from informants. Relations between these
presentation modi have been investigated by Lange and Zeugmann [1993], who
focussed on the interdependencies when considering diﬀerent learning restriction
concerning monotonicity. We show that their observation of strongly monotonic
Lim-learnability from informants implying Lim-learnability from texts is bound
to indexable families by providing a collection of recursive languages separating
these learning criteria. Learning from text has been extensively studied, including
many learning success criteria and other variations, see Jain, Osherson, Royer,
and Sharma [1999] or Case [2016].
The observation of every indexable family being Lim-learnable from informants
and thus also behaviorally correct learnable, where convergence of the hypothe-
ses only needs to be semantic, fails when arbitrary collections of recursively enu-
merable sets are in consideration. This follows from results of B¯arzdi¸nˇs [1974]
and Case and Smith [1983], as observed in Section 2. Further, in Section 6 we
prove that it still holds in case all delayable semantic restrictions are required.
Additionally, a strict hierarchy by allowing for an increasing ﬁnite number of
anomalies as well as not more learning power by allowing any vacillation between
ﬁnitely many correct hypotheses, are provided in these two sections, respectively.
Finally, we adress open questions arising from the paper and other challenges
related to our investigations.
Informant Learning: Restrictions and Success
We let N denote the natural numbers including 0 and write 8 for an inﬁnite
cardinality. Moreover, for a function f we write dompfq for its domain and
ranpfq for its range. If we deal with (a subset of) a cartesian product, we are
going to refer to the projection functions to the ﬁrst or second coordinate by
pr1 and pr2, respectively. For sets X, Y and a P N we write X “a Y , if X
equals Y with a anomalies, i.e., |pXzY q Y pY zXq| ď a, where |.| denotes the
4
Martin Aschenbach, Timo K¨otzing, and Karen Seidel
cardinality function. In this spirit we write X “˚ Y , if there exists some a P N
such that X “a Y , i.e., pXzY qYpY zXq is ﬁnite. Further, Xăø denotes the ﬁnite
sequences over X, i.e., all functions from some n P N to X and X ø stands for the
countably inﬁnite sequences over X, i.e., all functions from N to X. Additionally,
Xďø :“ Xăø Y X ø denotes the set of all countably ﬁnite or inﬁnite sequences
over X. For every f P Xďø and t P N, we let frts :“ tps, fpsqq | s ă tu denote
the restriction of f to t. Finally, for sequences σ, τ P Xăø their concatenation is
τ and we write σ Ď τ , if σ is an initial segment of τ , i.e., there is
denoted by σ
some t P N such that σ “ τrts. In our setting, we typically have X “ N ˆ t0, 1u.
We denote the set of all partial functions f : dompfq Ď Nˆt0, 1u Ñ N and total
functions f : Nˆt0, 1u Ñ N by P and R, respectively.
As far as possible, notation and terminology on the learning theoretic side fol-
low Jain, Osherson, Royer, and Sharma [1999], whereas on the computability
theoretic side we refer to Odifreddi [1999], Rogers [1967] and K¨otzing [2009].
(cid:97)
2.1 The Backbone of Learning Restrictions
Now we come to introduce the basic deﬁnitions in Gold-Style learning theory
important in this paper. Let L Ď N. If L is recursively enumerable, i.e., there is
a (partial) computable function f such that dompfq “ L, we call L a language.
In case L is even recursive, i.e., its characteristic function is computable, we
say it is a recursive language. Moreover, we call L Ď PowpNq a collection of
languages, if every L P L is a language. If every L P L is a recursive language, L
is called a collection of recursive languages. In case there exists an enumeration
tLξ | ξ P Ξu of L, where Ξ Ď N is recursive and a computable function f with
ranpfq Ď t0, 1u such that x P Lξ ô fpx, ξq “ 1 for all ξ P Ξ and x P N, we say
L is an indexable family of recursive languages.
Further, we ﬁx a programming system ϕ as introduced in Royer and Case [1994].
Brieﬂy, in the ϕ-system, for a natural number p, we denote by ϕp the par-
tial computable function with program code p. We also call p an index for
Wp :“ dompϕpq. In reference to a Blum complexity measure, for all p, t P N,
p Ď Wp the recursive set of all natural numbers less or equal
we denote by W t
to t, on which the machine executing p halts in at most t steps. We are going
to make use of these recursive sets in the proof of our essential Proposition 19,
showing that conservativeness does not restrict (explanatory) Lim-learning from
informants. Moreover, by s-m-n we refer to a well-known recursion theoretic ob-
servation, which gives nice ﬁnite and inﬁnite recursion theorems, like Case’s
Operator Recursion Theorem ORT.
In the context of language learning, Gold [1967], in his seminal paper, distin-
guished two major diﬀerent kinds of information presentation. The focus of this
paper is one of these methods of presenting the language to the learner, namely
so called informants. Intuitively, for any natural number x an informant for a
language L answers the question whether x P L in ﬁnite time. More precisely, for
every natural number x the informant I has either px, 1q or px, 0q in its range,
where the ﬁrst is interpreted as x P L and the second as x R L, respectively. In
Learning Families of Formal Languages from Informants
order to grasp this more formally, we will have to deal with ﬁnite and inﬁnite
sequences of pairs px, iq, where x P N and i P t0, 1u. For f P pNˆt0, 1uqďø let
pospfq :“ ty P N | Dx P N : pr1pfpxqq “ y ^ pr2pfpxqq “ 1u and
negpfq :“ ty P N | Dx P N : pr1pfpxqq “ y ^ pr2pfpxqq “ 0u
denote the sets of all natural numbers, about which f gives some positive or
negative information, respectively. As in Lange, Zeugmann, and Zilles [2008]
according to B¯arzdi¸nˇs [1977] and Blum and Blum [1975] for A Ď N we deﬁne
Conspf, Aq
:ô pospfq Ď A ^ negpfq Ď NzA
and say f is consistent with A or f is compatible with A in case Conspf, Aq is
true.
Deﬁnition 1. Let L be a language. We call every function I : N Ñ Nˆt0, 1u
such that pospIq Y negpIq “ N and pospIq X negpIq “ ∅ an informant. Further,
we denote by Inf the set of all informants and the set of all informants for the
language L is deﬁned as
InfpLq :“ tI P Inf | pospIq “ Lu.
It is immediate, that negpIq “ NzL for every I P InfpLq. If the informant I for
every time t P N reveals information about t itself, for short pr1pIptqq “ t, we
call I a canonical informant or according to Gold [1967] methodical informant.
The learner, which can also be thought of as a scientist trying to infer a theory
or a machine making a guess on the basis of evaluated data, is modelled by a
(partial) computable function M : dompMq Ď pNˆt0, 1uqăø Ñ N.
In the following we introduce the fundamental notion of the learning sequence
and clarify what successful learning means.
Deﬁnition 2. Let M be a learner and L a language. Further, let I P InfpLq be
an informant for L presented to M .
(i) We call h “ phtqtPN P Nø, where ht :“ MpIrtsq for all t P N, the learning
sequence of M on I. Thus h is the sequence of M ’s hypotheses by observing
I.
(ii) For a P NYt˚u and b P Ną0 Yt˚,8u we say that M learns L from I with a
b -learns L
bpM, Iq, if there is a time t0 P N such that for
anomalies and vacillation number b in the limit, for short M Lima
from I, or even shorter Lima
all t ě t0 we have
Wht “a L and |t ht | t ě t0 u| ď b.
The intuition behind the latter is that, sensing I, M eventually only vacillates
between b-many hypotheses, where the case b “ ˚ stands for eventually ﬁnitely
many diﬀerent hypotheses. In convenience with the literature, we ommit the
6
Martin Aschenbach, Timo K¨otzing, and Karen Seidel
superscript 0 and the subscript 1. Lim-learning, also known as explanatory
(Ex-)learning, is the most common deﬁnition for successful learning and cor-
responds to the notion of identiﬁability in the limit by Gold [1967], where the
learner eventually decides on one correct hypotheses. On the other end of the
hierarchy of convergence criteria is behaviorally correct learning, for short Bc- or
Lim8-learning, which only requires the learner to be eventually correct, but al-
lows inﬁnitely many syntactically diﬀerent hypotheses in the limit. Behaviorally
correct learning was introduced by Osherson and Weinstein [1982]. The general
b -learning for a P N Y t˚u and b P Ną0 Y t˚u was ﬁrst men-
deﬁnition of Lima
tioned by Case [1999]. In our setting, we also allow b “ 8 and subsume all Lima
under the notion of a convergence criterion, since they determine in which semi-
topological sense the learning sequence needs to have L as its limit, in order to
succeed in learning L.
In the following we review so-called learning restrictions, i.e., further potential
properties of the learning sequence being investigated in this paper. Learning re-
strictions incorporate certain desired properties of the learners’ behavior relative
to the information being presented. For this, we employ the notion of consistency,
stated above.
Deﬁnition 3. Let M be a learner and I P Inf an informant. As before we
denote by h “ phtqtPN P Nø the learning sequence of M on I. We write
(i) ConspM, Iq (Angluin [1980]), if M is consistent on I, i.e., for all t
ConspIrts, Whtq.
(ii) ConvpM, Iq (Angluin [1980]), if M is conservative on I, i.e., for all s, t
with s ď t
ConspIrts, Whsq ñ hs “ ht.
(iii) DecpM, Iq (Osherson, Stob, and Weinstein [1982]), if M is decisive on I,
i.e., for all r, s, t with r ď s ď t
Whr “ Wht ñ Whr “ Whs .
(iv) CautpM, Iq (Osherson, Stob, and Weinstein [1986]), if M is cautious on I,
i.e., for all s, t with s ď t
(cid:32)Wht Ĺ Whs.
(v) WMonpM, Iq (Jantke [1991],Wiehagen [1991]), if M is weakly monotonic
on I, i.e., for all s, t with s ď t
ConspIrts, Whsq ñ Whs Ď Wht .
(vi) MonpM, Iq (Jantke [1991],Wiehagen [1991]), if M is monotonic on I, i.e.,for
all s, t with s ď t
Whs X pospIq Ď Wht X pospIq.
Learning Families of Formal Languages from Informants
(vii) SMonpM, Iq (Jantke [1991],Wiehagen [1991]), if M is strongly monotonic
on I, i.e., for all s, t with s ď t
Whs Ď Wht.
(viii) NUpM, Iq (Baliga, Case, Merkle, Stephan, and Wiehagen [2008]), if M is
non-U-shaped on I, i.e., for all r, s, t with r ď s ď t
Whr “ Wht “ pospIq ñ Whr “ Whs.
(ix) SNUpM, Iq (Case and Moelius [2011]), if M is strongly non-U-shaped on
I, i.e., for all r, s, t with r ď s ď t
Whr “ Wht “ pospIq ñ hr “ hs.
(x) SDecpM, Iq (K¨otzing and Palenta [2014]), if M is strongly decisive on I,
i.e., for all r, s, t with r ď s ď t
Whr “ Wht ñ hr “ hs.
The following lemma states the implications between almost all of the above
deﬁned learning restrictions, which form the foundation of our research. Figure
1 on page 23 includes the resulting backbone, which is slightly diﬀerent from
the one for learning from exclusively positive information, since WMon does
not necessarily imply NU in the context of learning from informants. There the
implications are represented as black lines from bottom to top. For the backbone
of learning from positive information consult K¨otzing and Palenta [2014].
Lemma 4. Let M be a learner and I P Inf an informant. Then
(i) ConvpM, Iq implies SNUpM, Iq and WMonpM, Iq.
(ii) SDecpM, Iq implies DecpM, Iq and SNUpM, Iq.
(iii) SMonpM, Iq implies CautpM, Iq, DecpM, Iq, MonpM, Iq and WMonpM, Iq.
(iv) DecpM, Iq and SNUpM, Iq imply NUpM, Iq.
(v) WMonpM, Iq does not imply NUpM, Iq.
Proof. Verifying the claimed implications is straightforward. In order to verify
(v), consider L “ 2N. Fix p, q P N such that Wp “ 2N Y t1u and Wq “ 2N and
deﬁne the learner M for all σ P Nˆt0, 1uăø by
Mpσq “
if 1 P negpσq ^ 2 R pospσq;
otherwise.
p,
q,
In order to prove WMonpM, Iq for every I P InfpLq, let I be an informant for
L and sIpxq :“ mintt P N | pr1pIptqq “ xu, i.e., sIp1q and sIp2q denote the ﬁrst
occurance of p1, 0q and p2, 1q in ranpIq, respectively. Then we have for all t P N
2N Y t1u,
2N,
Wht “
if sIp1q ă t ď sIp2q;
otherwise.
8
Martin Aschenbach, Timo K¨otzing, and Karen Seidel
We have Whs “ WMpIrssq “ 2N Y t1u as well as 1 P negpIrtsq for all s, t P N
with sIp1q ă s ď sIp2q and t ą sIp2q. Therefore, (cid:32)ConspIrts, Whsq because
of negpIrtsq Ę NzWhs. We obtain WMonpM, Iq since whenever s ď t in N
are such that ConspIrts, Whsq, we know that Whs “ 2N Y t1u can only hold if
likewise sIp1q ă t ď sIp2q and hence Wht “ 2N Y t1u, which yields Whs Ď Wht.
Furthermore, if Whs “ 2N all options for Wht satisfy Whs Ď Wht. Otherwise, in
case M observes the canonical informant I for L, we have Wh0 “ Wh1 “ 2N,
Wh2 “ 2N Y t1u and Wht “ 2N for all t ą 2, which shows (cid:32)NUpM, Iq.
2.2 Learning Sequences and Success of Simulated Learners
From deriving the backbone of the hierarchy of delayable learning restrictions
when learning languages from informants, we now come to introduce general
properties of learning restrictions and learning success criteria, which allow gen-
eral observations, not bound to the setting of (explanatory) Lim-learning.
Deﬁnition 5. Let T :“ P ˆ Inf denote the whole set of possible learners and
informants. We denote by
∆ :“ t Caut, Cons, Conv, Dec, SDec, WMon, Mon, SMon, NU, SNU, Tu
the set of admissible learning restrictions and by
Γ :“ t Lima
b | a P N Y t˚u ^ b P Ną0 Y t˚,8uu
the set of convergence criteria. Further, if
β P t nč
i“0
δi X γ | n P N,@i ď npδi P ∆q and γ P Γ u Ď P ˆ Inf ,
we say that β is a learning success criterion.
Note that every convergence criterion is indeed a learning success criterion by
letting n “ 0 and δ0 “ T, where the latter stands for no restriction. In the liter-
ature convergence criteria are also called identiﬁcaton criteria and then denoted
by I or ID.
Let us now sum up and introduce the notation of a learning criterion. In order
to observe whether one way of learning is more powerful than another one, we
are going to compare diﬀerent settings, always denoted in the form rαInf βs or
rαTxtβs, where Inf and Txt may have indices. Clearly, we distinguish the mode
of information presentation, namely, whether the learner observes the language
as solely positive information, i.e. a text, Txt, or an informant, Inf . We also
sometimes refer to results on learning collections of recursive functions, in which
a text for the graph of the respective function is presented to the learner. We
denote the associated learning criteria in the form rαFnβs, where again indices
to Fn are allowed.
Learning Families of Formal Languages from Informants
Secondly, a learning criterion speciﬁes, what successful learning means. This
information is provided at position β, where the learning restrictions to meet
are denoted in alphabetic order, followed by a convergence criterion.
Last but not least, at position α, we restrict the set of admissible learners by re-
quiring for example totality. The properties stated at position α are independent
of learning success. Note that it is also conventional to require M ’s hypothesis
sequence to fulﬁll certain learning restrictions, not asking for the success of the
learning process.
Deﬁnition 6. Let α be a property of partial computable functions from the set
pNˆt0, 1uqăø to N and β a learning success criterion. We denote by rαInf βs
the set of all collections of languages that are β-learnable from informants by
a learner M with the property α. In case the learner only needs to succeed on
canonical informants, we denote the corresponding set of collections of languages
by rαInf canβs. The notations rαTxtβs and rαFnβs are deﬁned similarly.
In the case of learning from solely positive information, there have been plenty
of investigations on the relation of diﬀerent success criteria, e.g., on the relation
of rTxtLimbs and rTxtNULimbs for all b P Ną0 Y t˚,8u by Carlucci, Case,
Jain, and Stephan [2008], Baliga, Case, Merkle, Stephan, and Wiehagen [2008]
and Fulk, Jain, and Osherson [1994] as summed up in Case [2016].
Moreover, K¨otzing and Palenta [2014] and Jain, K¨otzing, Ma, and Stephan [2016]
give the picture, of how the learning restrictions in Deﬁnition 3 relate, when
learning languages from texts with possibly restricting attention to set-driven or
iterative learners.
More interesting for understanding the power of learning from informants are
results from function learning, as, by the next lemma, collections of functions
separating two convergence criteria in the associated setting yield a separating
collection for the respective convergence criteria, when learning languages from
informants.
In the following we make use of a computable bijection x. , .y : NˆN Ñ N with its
computable inverses π1, π2 : N Ñ N such that x “ xπ1pxq, π2pxqy for all x P N.
Lemma 7. For f P R let Lf :“ txx, fpxqy | x P Nu denote the language
encoding its graph. Let a P NYt˚u and b P Ną0 Yt˚,8u. Then for every F Ď R
F P rFnLima
bs ô LF :“ t Lf | f P F u P rInf Lima
bs.
Proof. Let a, b and F be as stated. First, assume there is a learner M on function
sequences such that F P FnLima
bpMq. In order to deﬁne the learner M1 acting on
informant sequences and returning W -indices, we employ the following procedure
for obtaining a W -code Gppq for Lϕp , when given a ϕ-code p:
Given input n, interpreted as xx, yy, let the program encoded by p run
on x “ π1pnq. If it halts and returns y “ π2pxq, then halt; otherwise
loop.
10
Martin Aschenbach, Timo K¨otzing, and Karen Seidel
The learner M1 acts on σ P pNˆt0, 1uqăø by
M1pσq :“ GpMpdecodeppospσqqqq,
where decodeppospσqq denotes the from σ uniformly computable sequence τ with
τpiq “ pπ1pniq, π2pniqq for all i ă |pospσq| “ |τ|, where pniqiă|pospσq| denotes the
bpM1q as
enumeration of pospσq according to σ. By construction, LF P Inf Lima
G preserves the number of anomalies.
For the other claimed direction let M be a learner on informant sequences with
bpMq. As above we employ a computable function that for every
LF P Inf Lima
f P R transforms a W -index p for Lf into a ϕ-index Hppq such that ϕHppq “ f .
Thereby, we interpret each natural number i as xxu, vy, ty and check whether ϕp
halts on xu, vy in at most t steps of computation. If so, we check whether u is
the argument x we want to compute fpxq for and in case the answer is yes, we
return v.
Given input x, for i “ 0 till 8 do the following: If Φppπ1piqq ď π2piq and
π1pπ1piqq “ x, then return π2pπ1piqq; otherwise increment i.
Deﬁne the learner M1 on σ P pNˆNqăø by
M1pσq :“ HpMpˆσqq,
pxxπ1piq, π2piqy, 1q
pxxπ1piq, π2piqy, 0q
where we transform σ “ ppx0, fpx0qq, . . . ,px|σ|´1, fpx|σ|´1qqq into an informant
sequence ˆσ of length ˆ|σ| :“ maxtj | @i ă j π1piq ă |σ|u by letting
if σpπ1piqq “ pxπ1piq, π2piqq
ˆσpiq :“
for all i ă ˆt. Note that for every f P R and every T P Txtpfq by letting
IT :“
Trts, we obtain an informant for Lf . We show pxx, fpxqy, 1q P IT for
every x P N and leave the other details to the reader. Let x P N and s minimal,
such that px, fpxqq P ranpTrssq, i.e., xs´1 “ x. Further, let t be such that s ď ˆt.
ITpxs ´ 1, fpxqyq “ y
Then clearly
Trtspxs ´ 1, fpxqyq “ pxx, fpxqy, 1q.
otherwise
tPN
Again, the claim follows, since H preserves the number of anomalies.
With this we obtain a hierarchy of learning restrictions.
Proposition 8. Let b P t1,8u. Then
(i) for all a P N holds rInf Lima
bs Ĺ rInf Lima`1
bs,
bs Ĺ rInf Lim˚
(ii)
(iii) rInf Lim˚s Ĺ rInf Lim8s
Proof. By Lemma 7 this results transfer from the corresponding observations for
function learning by B¯arzdi¸nˇs [1974] and Case and Smith [1983].
aPNrInf Lima
s,
Learning Families of Formal Languages from Informants
11
rInf Lims Ĺ . . . Ĺ rInf Limas Ĺ rInf Lima`1s Ĺ . . .
rInf Limas Ĺ rInf Lim˚s
Ĺ rInf Lim8s Ĺ . . . Ĺ rInf Lima8s Ĺ rInf Lima`18 s Ĺ . . .
rInf Lima8s Ĺ rInf Lim˚
8s.
In particular, we have
aPN
aPN
The lemma obviously also holds when considering TxtLima
b -learning languages,
where the construction of the text sequence from the informant sequence is
folklore.
The next deﬁnition provides a properties of learning restrictions playing a cen-
tral role in most of our proofs, since it applies to almost all of the learning
restrictions introduced in Deﬁnition 3. The analog for learning from text has
been introduced in K¨otzing and Palenta [2014] and a generalization is studied in
K¨otzing, Schirneck, and Seidel [2017], where the relations between rTxtδLims
and rTxtδLim8s for diﬀerent δ P ∆ are investigated, respectively.
Deﬁnition 9. Denote the set of all unbounded and non-decreasing functions by
S, i.e.,
S :“ t s : N Ñ N | @x P NDt P N : sptq ě x and @t P N : spt ` 1q ě sptqu.
Then every s P S is a so called admissible simulating function.
A predicate β Ď P ˆ Inf is delayable, if for all s P S, all I, I1 P Inf and all
partial functions M, M1 P P holds: Whenever we have pospI1rtsq Ě pospIrsptqsq,
negpI1rtsq Ě negpIrsptqsq and M1pI1rtsq “ MpIrsptqsq for all t P N, from βpM, Iq
we can conclude βpM1, I1q.
The name refers to tricks in order to delay mind changes of the learner which
were used to obtain polynomial computation times for the learners hypothesis
updates as discussed by Pitt [1989] and Case and K¨otzing [2009]. Moreover, it
should not be confused with the notion of δ-delay by Akama and Zeugmann
[2008], which allows satisfaction of the considered learning restriction δ steps
later than in the un-δ-delayed version.
In order to give an intuition, think of β as a learning restriction or learning
success criterion and imagine M to be a learner. Then β is delayable if and
only if it carries over from M together with an informant I to all learners M1
and informants I1 representing a delayed version of M on I. More concretely,
as long as the learner M1 conjectures hsptq “ MpIrsptqsq at time t and has,
in form of I1rts, at least as much data available as was used by M for this
hypothesis, M1 with I1 is considered a delayed version of M with I. Note, that the
simulating function’s unboundedness in particular guarantees pospIq “ pospI1q
and negpIq “ negpI1q.
The next result guarantees that arguing with the just deﬁned properties covers
all of the considered learning restrictions but consistency.
12
Martin Aschenbach, Timo K¨otzing, and Karen Seidel
Lemma 10. (i) Let δ P ∆. Then δ is delayable if and only if δ ‰ Cons.
(ii) Every convergence criterion γ P Γ is delayable.
(iii) The intersection of ﬁnitely many delayable predicates on P ˆ Inf is again
i“0 δi X γ with
delayable. Especially, every learning success criterion β “
δi P ∆ztConsu for all i ď n and γ P Γ , β is delayable.
Proof. We approach piq by showing, that Cons is not delayable. To do so, con-
2 uqq
sider s P S with sptq :“ t t
and I1pxq :“ px, 12Npxqq, where 12N stands for the characteristic function of all
even natural numbers. By s-m-n there are learners M and M1 such that for all
σ P pNˆt0, 1uqăø
2u, I, I1 P Inf deﬁned by Ipxq :“ pt x
2 u, 12Npt x
WMpσq “ tx P N | px even ^ x ď t
WM1pσq “ tx P N | px even ^ x ď t
uq _ px odd ^ x ą t
uq _ px odd ^ x ą t
|σ|
|σ|
|σ|
|σ|
uqu
uqu.
Further, ConspM, Iq is easily veriﬁed since for all t P N
pospIrtsq “ tx P N | x even ^ x ď t t ´ 1
uu Ď WMpIrtsq
negpIrtsq “ tx P N | x odd ^ x ď t t ´ 1
uqu Ď NzWMpIrtsq
but on the other hand (cid:32)ConspM1, I1q since for all t ą 2
pospI1rtsq “ tx P N | x even ^ x ă tu
Ę tx P N | px even ^ x ď t t
uq _ px odd ^ x ą t t
uqu “ WM1pI1rtsq.
The remaining proofs for piq and piiq are straightforward. Basically, for Dec,
SDec, SMon and Caut, the simulating function s being non-decreasing and
M1pI1rtsq “ MpIrsptqsq for all t P N would suﬃce, while for NU, SNU and
Mon one further needs that the informants I and I1 satisfy pospIq “ pospI1q.
The proof for WMon and Conv to be delayable, requires all assumptions, but
s’s unboundedness. Last but not least, in order to prove that every convergence
criterion γ “ Lima
delayed variants, one essentially needs both characterizing properties of s and of
course M1pI1rtsq “ MpIrsptqsq. Finally, piiiq is obvious.
b , for some a P N Y t˚u and b P Ną0 Y t˚,8u, carries over to
3 Normal Forms: Canonical Informants and Totality
To facilitate smooth proofs, in this section we discuss normal forms for learning
from informants. First we consider the notion of set-drivenness, which restricts
the set of admissible learners to those not considering the order of presentation
or number of occurances of a certain datum. In Lemma 12 we show for delayable
learning success criteria, that every collection of languages that is learnable from
Learning Families of Formal Languages from Informants
13
canonical informants is also learnable set-drivenly from arbitrary informants.
Moreover, in Lemma 14 we observe that only considering total learners does not
alter the learnability of a collection of languages in case of a delayable learning
success criterion. Further along the line, we provide a regularity property of
learners, called syntactic decisiveness, for Lim-learning in Lemma 18.
Deﬁnition 11 (Wexler and Culicover [1980]). A learner M is set-driven,
if for all σ, τ P Nˆt0, 1uăø
p pospσq “ pospτq ^ negpσq “ negpτqq ñ Ipσq “ Ipτq.
Intuitively, M is set-driven, if it does not care about the order in which the
information is presented or whether there are repetitions. Sch¨afer-Richter [1984]
and Fulk [1985] showed that set-drivenness is a restriction when learning only
from positive information and also the relation between the learning restrictions
diﬀer as observed by K¨otzing and Palenta [2014].
In the next Lemma we observe that, by contrast, set-drivenness is not a re-
striction in the setting of learning from informants. Concurrently, we generalize
Gold [1967]’s observation, stating that considering solely canonical informants
to determine learning success does not give more learning power, to arbitrary
delayable learning success criteria.
Lemma 12. Let β be a delayable learning success criterion. Then every lan-
guage collection L that is β-learnable by a learner from canonical informants
can also be β-learned by a set-driven learner from arbitrary informants, i.e.,
rInf canβs “ rSdInf βs
Proof. Clearly, we have rInf canβs Ě rSdInf βs. For the other inclusion, let L be
β-learnable by a learner M from canonical informants. Let L P L and I1 P InfpLq.
For every f P pNˆt0, 1uqďø, thus especially for I1 and all its initial segments, we
deﬁne sf P S for all t for which frts is deﬁned, by
sfptq “ suptx P N | @w ă x : w P pospfrtsq Y negpfrtsqu,
i.e., the largest natural number x such that for all w ă x we know, whether
w P pospfrtsq. In the following f will either be I1 or one of its initial segments,
which in any case ensures pospfrtsq Ď L for all appropriate t. By construction, sf
is non-decreasing and if we consider an informant I P Inf , since pospIqYnegpIq “
N, sI is also unbounded. In order to employ the delayability of β, we deﬁne an
operator Σ : pNˆt0, 1uqďø Ñ pNˆt0, 1uqďø such that for every f P pNˆt0, 1uqďø
in form of Σpfq we obtain a canonically sound version of f . Σpfq is deﬁned on
all t ă sfp|f|q in case f is ﬁnite and on every t P N otherwise by
Σpfqptq :“
if pt, 0q P ranpfq;
pt, 0q,
pt, 1q, otherwise.
14
Martin Aschenbach, Timo K¨otzing, and Karen Seidel
Intuitively, in Σpfq we sortedly and without repetitions sum up all information
contained in f up to the largest initial segment of N, f without interruption
informs us about. For a ﬁnite sequence σ the canonical version Σpσq has length
sσp|σ|q. Now consider the set-driven learner M1 deﬁned by
M1pσq “ MpΣpσqq.
Since I :“ ΣpI1q is a canonical informant for L, we have βpM, Iq. Moreover, for
all t P N holds pospIrsI1ptqsq Ď pospI1rtsq and negpIrsI1ptqsq Ď negpI1rtsq by the
deﬁnitions of sI1 and of I using Σ. Finally,
M1pI1rtsq “ MpΣpI1rtsqq “ MpΣpI1qrsI1ptqsq “ MpIrsI1ptqsq
and the delayability of β yields βpM1, I1q.
Therefore, while considering delayable learning from informants, looking only at
canonical informants already yields the full picture also for set-driven learners.
We will make use of this reduction in other proofs.
Note that the construction of the canonical sound version ΣpIq for an informant
I corresponds to the construction of the corresponding one-one text T1´1 for an
arbitrary text T , as repeatedly employed in K¨otzing, Schirneck, and Seidel [2017].
Clearly, a similar result can be obtained, when learning recursive functions from
their graphs being presented in the canonical or an arbitrary order.
The next proposition answers the arising question, whether Lemma 12 also holds,
when requiring the non-delayable learning restriction of consistency, negatively.
In the following let K :“ t p P N | ϕpppqÓu denote the halting problem.
Proposition 13. The collection of languages
L :“ t2K Y 2pK Y txuq ` 1 | x P Nu
is consistently, conservatively, strongly decisively and strongly monotonically
Lim-learnable from canonical informants. Further, L is not consistently Lim-
learnable from arbitrary informants. For short we have
L P rInf canConsConvSDecSMonLimszrInf ConsLims.
Proof. Let p : N Ñ N such that Wppxq “ 2K Y 2pK Y txuq ` 1 for every x P N
and let k be a W -index for 2K Y 2K ` 1. Consider the learner M deﬁned by
Mpσq “
ppxq,
k,
if x with 2x P negpσq and 2x ` 1 P pospσq exists;
otherwise.
for every σ P pNˆt0, 1uqăø. Clearly, M conservatively, strongly decisively and
strongly monotonically Lim-learns L from informants and on canonical infor-
mants for languages in L it is consistent.
Learning Families of Formal Languages from Informants
15
Now, assume there is a learner M such that L P Inf ConsLimpMq. By Lemma 16
there is a locking sequence σ for 2K Y 2K ` 1. By s-m-n there is a computable
function
χpxq “
if Mpσq “ Mpσ
1,
0, otherwise.
(cid:97)p2x ` 1, 1qq;
By the consistency of M on L, we immediately obtain that χ is the characteristic
function for K, a contradiction.
Note, that there must not be an indexable family witnessing the diﬀerence stated
in the previous proposition, since every indexable family is consistently and con-
servatively Lim-learnable by enumeration. Further, the connatural observation
rConsFnLims Ĺ rConsFncanLims by Jantke and Beick [1980], when learning
collections of recursive functions, may be helpful to reprove this result with a
generalization of Lemma 7 to consistent learning.
Gold [1967] further introduces request informants for M and L. As the name
already suggests, there is an interaction between the learner and the informant
in the sense that the learner decides, about which natural number the informant
should inform it next. His observation rInf Lims “ rInf canLims “ rInf reqLims
seems to hold true when facing arbitrary delayable learning success criteria, but
fails in the context of the non-delayable learning restriction of consistency.
Moreover, by the following lemma, in most of the proofs in the remainder of this
paper we are going to consider only total learners.
Lemma 14. Let β be a delayable learning success criterion. Then every lan-
guage collection β-learnable by a learner from informants can also be β-learned
by a total learner from informants, i.e.,
rInf βs “ rRInf βs.
Proof. Let L P rInf βs and M be a learner witnessing this. Without loss of
generality we may assume that ∅ P dompMq. We deﬁne the total learner M1 by
letting sM : pNˆt0, 1uqăø Ñ N,
σ ÞÑ supts P N | s ď |σ| and M halts on σrss after at most |σ| stepsu
and
M1pσq :“ MpσrsMpσqsq.
The convention supp∅q “ 0 yields that sM is total and it is computable, since for
M only the ﬁrst |σ|-many steps have to be evaluated on σ’s ﬁnitely many initial
segments. One could also employ a Blum complexity measure here. Hence, M1
is a total computable function.
In order to observe, that M1 Inf β-learns L, let L P L and I be an informant for
L. By letting sptq :“ sMpIrtsq, we clearly obtain an unbounded non-decreasing
16
Martin Aschenbach, Timo K¨otzing, and Karen Seidel
function, hence s P S. Moreover, for all t P N from sptq ď t immediately follows
pospIrsptqsq Ď pospIrtsq, negpIrsptqsq Ď negpIrtsq as well as
M1pIrtsq “ MpIrsMpIrtsqsq “ MpIrsptqsq.
By the delayability of β and with I1 “ I, we ﬁnally obtain βpM1, Iq.
In the following we transfer an often employed observation by Blum and Blum
[1975] to the setting of learning from informants and generalize it to all conver-
gence criteria introduced in Deﬁnition 2.
Deﬁnition 15. Let M be a learner, L a language and a P N Y t˚u as well as
b P Ną0 Y t˚,8u. We call σ P pNˆt0, 1uqăø a Lima
b -locking sequence for M on
L, if Conspσ, Lq and
˘˘˘
DD Ď N p |D| ď b ^ @τ P pNˆt0, 1uqăø
Conspτ, Lq ^ Mpσ
WMpσ(cid:97)τq “a L ^ Mpσ
τq P D
``
τqÓ
(cid:97)
(cid:97)
Further, a locking sequence for M on L is a Lim-locking sequence for M on L.
Intuitively, the learner M is locked by the sequence σ onto the language L in the
sense that no presentation consistent with L can circumvent M guessing admis-
sible approximations to L and additionally all guesses based on an extension of
σ are captured by a ﬁnite set of size at most b.
Note that the deﬁnition implies MpσqÓ, WMpσq “a L and Mpσq P D.
Lemma 16. Let M be a learner, a P NYt˚u, b P Ną0Yt˚,8u and L a language
Lima
b -identiﬁed by M . Then there is a Lima
b -locking sequence for M on L.
Proof. This is a straightforward contradictory argument. Without loss of gen-
erality M is deﬁned on ∅. Assume for every σ with Conspσ, Lq, MpσqÓ and
WMpσq “a L and for every ﬁnite D Ď N with at most b elements there exists a
sequence τ D
σ P pNˆt0, 1uqăø with
σ qÓ ^
τ D
σ , Lq ^ Mpσ
(cid:97)
Conspτ D
Let IL denote the canonical informant for L. We obtain an informant for L on
which M does not Lima
b -converge by letting
(cid:97)
σ q R D
τ D
σ q “a L _ Mpσ
(cid:32)WMpσ(cid:97)τ D
I :“
σ0 :“ ILr1s,
(cid:97)
n τ Dn
σn
i q | maxt0, n´b`1u ď i ď nu we collect M ’s
σn, with
nPN
(cid:97)
ILpn ` 1q
σn`1 :“ σ
for all n P N, where in Dn :“ t Mpσ´
at most b-many last relevant hypotheses. Since I is an informant for L by having
interlaced the canonical informant for L, the learner M Lima
b -converges on I.
n0 Ď Irts we have Wht “a L. Then
Therefore, let n0 be such that for all t with σ´
certainly t Mpσ´
i q | n0 ď i ď n0 ` bu has cardinality b ` 1, a contradiction.
Learning Families of Formal Languages from Informants
17
Obviously, an appropriate version also holds when learning from text is consid-
ered.
Before we determine the relations between the introduced learning restrictions
for (explanatory) Lim-learning from informants, we introduce a further beneﬁ-
cial property, requiring a learner never to syntactically return to an abandoned
hypothesis.
Deﬁnition 17 (K¨otzing and Palenta [2014]). Let M be a learner, L a lan-
guage and I an informant for L. We write
SynDecpM, Iq, if M is syntactically decisive on I, i.e.,
@r, s, t : pr ď s ď t ^ hr “ htq ñ hr “ hs.
The following easy observation shows that this variant of decisiveness can always
be assumed in the setting of Lim-learning from informants. This is employed in
the proof of Proposition 19.
Lemma 18. Every language collection Lim-learnable from informants can also
be syntactically decisively Lim-learned from informants, i.e.,
rInf Lims “ rInf SynDecLims
Proof. Since obviously rInf SynDecLims Ď rInf Lims, it suﬃces to show that
every Inf Lim-learnable collection of languages is also Inf SynDecLim-learnable.
For, let L P rInf Lims and M witnessing this. In the deﬁnition of the learner
M1, we make use of a one-one computable padding function pad : NˆN Ñ N
such that Wp “ dompϕpq “ dompϕpadpp,xqq “ Wpadpp,xq for all p, x P N. Now,
consider M1 deﬁned by
M1pσq :“
padpMpσq,|σ|q,
M1pσq,
if Mpσ´q ‰ Mpσq;
otherwise
M1 behaves almost like M with the crucial diﬀerence, that whenever M performs
a mind change, M1 semantically guesses the same language as M did, but syn-
tactically its hypothesis is diﬀerent from all former ones. The padding function’s
deﬁning property and the assumption that M Inf Lim-learns L immediately
yield the Inf SynDecLim-learnability of L by M1.
Note that SDec implies SynDec, which is again a delayable learning restriction
and therefore also afsoep. Thus, in the proof of Lemma 18 we could have also
restricted our attention to canonical informants.
4 Relations between Delayable Learning Restrictions
In order to reveal the relations between the delayable learning restrictions in
(explanatory) Lim-learning from informants, in Proposition 19 we acquire that
18
Martin Aschenbach, Timo K¨otzing, and Karen Seidel
conservativeness and strongly decisiveness do not restrict informant learning.
After this, Propositions 20 and 22 provide that cautious and monotonic learning
are incomparable, implying that both these learning settings are strictly stronger
than strongly monotonic learning and strictly weaker than unrestricted learning.
The overall picture is summarized in Figure 1 and stated in Theorem 24.
Proposition 19. Every collection of languages Lim-learnable from methodical
informants can also be conservatively and strongly decisively Lim-learned by a
total, set-driven learner from informants, i.e.,
rInf Lims “ rRSdInf ConvSDecLims
Especially,
rInf Lims “ rInf ConvSDecLims
Proof. Obviously rInf Exs Ě rRSdInf ConvSDecExs and by the Lemmas 12,
14 and 18 it suﬃces to show rRInf SynDecExs Ď rInf mConvSDecExs.
Now, let L P rRInf SynDecExs and M a learner witnessing this. In particular,
M is total and on informants for languages in L we have that M never returns
to a withdrawn hypothesis. For every set X and t P N, let Xrts denote the
methodical informant sequence of the ﬁrst t elements of X.
We want to deﬁne a learner M1 which uses hypotheses ppσq; these hypotheses
should mimic the hypotheses Mpσq, but suitably poisoned, i.e., modiﬁed such
that, if σ is a locking sequence, then ppσq is codes the same language as Mpσq.
However, if σ is not a locking sequence, then the hypothesis should no include
data that we want to change our mind on. In order to do so formally in a
computable way, we use the following deﬁnitions.
For any given σ, D Ď W s
Mpσq and s, we let
zspσ, Dq “ mintz ď s | D Ď W z
Mpσqu.
For any given σ, D0 and s, we let‹
Qpσ, D0, sq “ tD Ď W s
Mpσq | maxpDq ă minpW s
MpσqzDq and D0 Ă D and
Mpσq “ MpW s
Mpσqrzspσ, Dqsqu.
Intuitively, given σ, ppσq should enumerate more and more elements enumerated
by Mpσq. If we have currently enumerated a set D0 and consider a time bound
of s, then Q gives all the candidate sets we can use for extending D0. Note that,
for all σ, D0 and s, Qpσ, D0, sq is totally ordered by Ď.
We consider the following auxiliary sets for all σ.
A0pσq “ pospσq;
$’&’%W s
@s P N : As`1pσq “
Mpσq,
maxĎ Qpσ, Aspσq, sq,
Aspσq,
Mpσq ‰ H;
if negpσq X W s
else if Qpσ, Aspσq, sq ‰ H;
otherwise.
‹ We suppose minH “ 8 for convenience.
Learning Families of Formal Languages from Informants
19
Furthermore, using s-m-n, we deﬁne p for all σ such that
sPN
Wppσq “
Aspσq.
$’&’%ppσq,
Finally, we deﬁne our new learner M1 such that
M1pσq “
ppσq,
M1pσ´q, otherwise.
if |σ| “ 0;
else if Mppσ´q1q ‰ Mpσq ^ (cid:32)Conspσ, A|σ|ppσ´q1qq;
That is, M1 follows the mind changes of M once a suitably inconsistent hypoth-
esis has been seen. All hypotheses of M by are poisoned in a way to ensure that
we can decide inconsistency.
Claim 1: Let L be a language Inf Ex-learned by M . Then M1 Inf Ex-learns L.
Let t be minimal such that, for all t1 ě t, MpLrtsq “ MpLrt1sq. Thus, MpLrtsq
is a correct hypothesis for L, we denote by e.
Case 1: M1 does not make a mind change after t.
Then M1 converged already before that mind change of M , so there is t0 ă t
minimal with, for all t1 ě t0, e1 :“ M1pLrt0sq “ M1pLrt1sq. From the deﬁnition of
M1 we get, for all t1 ě t, ConspLrt1s, At1pLrt0sqq. Thus, We1 contains all elements
of L and no other, i.e., We1 “ L as desired.
Case 2: M1 makes a mind change after t.
Let t1 ě t be the time of that mind change. Clearly, M1 will converge to
ppLrt1sq, denoted by e1. We get We1 Ď L immediately. Suppose now there is
a minimal element x P LzWe1. Then We1 is ﬁnite and equal to some AspLrt1sq
(since the next added elements would necessarily contain x by deﬁnition). Let
y “ maxpAspLrt1sq Y txuq and let s1 ą s be large enough such that Lry ` 1s
e ry ` 1s, i.e., all element of L up to (and including) y are enumerated
equals W s1
by time s1. Let z :“ zs1pLrt1s, pospLry ` 1sqq be the time window considered in
the third condition of pospLry` 1sq P QpLrt1s, AspLrt1sq, s1q, for any s1 ě s1. Let
e rzs “ Lrzs, i.e., all elements within the
s2 ě s1 be large enough such that W s2
time window are enumerated. Then pospLry ` 1sq P QpLrt1s, AspLrt1sq, s2q, as
M is converged after t ă t1 on L, a contradiction to x (which is in pospLry` 1sq)
not being included in We1.
Claim 2: Let L be a language Inf Ex-learned by M . Then M1 in conservative
on L.
Let t be such that M1pLrtsq ‰ M1pLrt ` 1sq. Let e1 :“ M1pLrtsq and let t0 ď t
be minimal such that M1pLrt0sq “ e1. From the mind change of M1 we get that
there was a mind change of M , i.e.
MpLrt0sq ‰ MpLrt ` 1sq.
Suppose, by way of contradiction, We1 is consistent with Lrt ` 1s, i.e.
ConspLrt ` 1s, We1q.
(1)
(2)
20
Martin Aschenbach, Timo K¨otzing, and Karen Seidel
From the fact that M1 made a mind change we get
(cid:32)ConspLrt ` 1s, At`1pLrt0sqq.
(3)
If there was an element x P At`1pLrt0sq which is listed negatively in Lrt ` 1s,
then x P At`1pLrt0sq Ď We1, contradicting Equation (2). In particular, we obtain
negpLrt1sq X WMpLrt1sq “ H from the ﬁrst case in the deﬁnition of A.
Thus, to satisfy Equation (3), there is x such that
x P pospLrt ` 1sqzAt`1pLrt0sq.
(4)
Since pospLrt ` 1sq Ď We1 from Equation (2), there is t1 ą t ` 1 minimal such
that pospLrt ` 1sq Ď At1pLrt0sq. This implies
pospLrt ` 1sq P QpLrt ` 1s, At1´1pLrt0sq, t1q.
Thus, zt1pLrt0s, pospLrt ` 1sqq ă t ` 1, using the mind change of M , see Equa-
tion (1). We now get
pospLrt ` 1sq P QpLrt ` 1s, AtpLrt0sq, t ` 1q,
leading to Lrt ` 1s Ď At`1pLrt0sq, a contradiction to Equation (4).
Finally, M1 behaves strongly decisively on the methodical informant for L since
M1 is consistent after a mind change and only makes a mind change when in-
consistent (i.e., M1 is conservative).
The auxiliary sets transfer the technique of poisoning a conjecture introduced in
K¨otzing and Palenta [2014] and tailor it to this special setting.
The next two propositions show that monotonic and cautious learning are in-
comparable on the level of indexable families. In the ﬁrst proposition the learner
can even be assumed cautious on languages it does not identify. Thus, according
to Deﬁnition 6 we write this success independent property of the learner on the
left side of the mode of presentation.
Proposition 20. The indexable family
L :“ t2X Y p2pNzXq ` 1q | X Ď N ﬁnite or X “ Nu
is Lim-learnable by a cautious learner from informants. Further, L is not mono-
tonically Lim8-learnable from informants. For short we have
L P rCautInf LimszrInfMonLim8s.
Particularly, rInfMonLims Ĺ rInf Lims.
Proof. We ﬁrst show L R rInfMonLim8s. Let M be a Inf Lim8-learner for L.
Further, let I0 be the methodical informant for L0 :“ 2N P L. Then there exists
Learning Families of Formal Languages from Informants
21
t0 such that WMpI0r2t0sq “ 2N. Moreover, consider the methodical informant I1
for
L1 :“ 2t0, . . . , t0u Y p2pNzt0, . . . , t0uq ` 1q P L
and let t1 ą t0 such that WMpI1r2t1sq “ L1. Similarly, we let I2 be the methodical
informant for
L2 :“ 2t0, . . . , t0, t1 ` 1u Y p2pNzt0, . . . , t0, t1 ` 1uq ` 1q P L
and choose t2 ą t1 with WMpI2r2t2sq “ L2. Since 2pt1 ` 1q P pL0 X L2qzL1 and
by construction I2r2t0s “ I0r2t0s as well as I2r2t1s “ I1r2t1s, we obtain
2pt1 ` 1q R WMpI2r2t1sq X L2
2pt1 ` 1q P WMpI2r2t0sq X L2
and
and therefore M does not learn L2 monotonically from I2.
Let us now adress L P rCautInf Lims. Fix p P N such that Wp “ 2N. Further, by
s-m-n there is a computable function q : N Ñ N with WqpxXyq “ X Yp2NzXq` 1,
where xXy stands for a canonical code of the ﬁnite set X. We deﬁne the learner
M for all σ P Nˆt0, 1uăø by
Mpσq “
p,
qpxpospσq X 2Nyq, otherwise.
if pospσq Ď 2N;
Intuitively, M guesses 2N as long as no odd number is known to be in the
language L to be learned. If for sure L ‰ 2N, then M assumes that all even
numbers known to be in L so far are the only even numbers therein.
It is easy to verify that M is computable and by construction it learns L. For
establishing the cautiousness, let L be any language, I an informant for L and
s ď t. Furthermore, assume WMpIrssq ‰ WMpIrtsq. In case pospIrssq Ę 2N, we
have x P ppospIrtsq X 2Nq with x R ppospIrssq X 2Nq and therefore as desired
WMpIrtsqzWMpIrssq ‰ ∅. Then again, pospIrssq Ď 2N implies WMpIrssq “ 2N and
thus again WMpIrtsqzWMpIrssq ‰ ∅.
Corollary 21. There exists an indexable family cautiously Lim-learnable from
informants, but not strictly monotonically Lim-learnable from informants. In
particular,
rInf SMonLims Ĺ rInf CautLims.
The following proposition extends the overservation of Osherson, Stob, and We-
instein [1986] for cautious learning to restrict learning power.
Proposition 22. The indexable family
L :“ tNzX | X Ď N ﬁniteu
is monotonically Lim-learnable from informants. Further, L is not cautiously
behaviorally correct learnable from informants. For short we have
L P rInfMonLimszrInf CautLim8s.
Particularly, rInf CautLims Ĺ rInf Lims.
22
Martin Aschenbach, Timo K¨otzing, and Karen Seidel
Proof. In order to approach L R rInf CautLim8s, let M be a GInf Lim8-
learner for L and I0 the methodical informant for N. Moreover, let t0 be such that
WMpI0rt0sq “ N. Let I1 be the methodical informant for L1 :“ Nztt0 ` 1u. Since
M learns L1, there is t1 ą t0 such that WMpI1rt1sq “ L1. We have I1rt0s “ I0rt0s
and hence M is not cautiously learning L1 from I1.
By s-m-n there is a computable function p : N Ñ N such that for all ﬁnite sets
X holds WppxXyq “ NzX, where xXy denotes a canocical code for X as already
employed in the proof of Proposition 20. We deﬁne the learner M by letting for
all σ P Nˆt0, 1uăø
Mpσq “ ppxnegpσqyq.
The corresponding intuition is that M includes every natural number in its guess,
not explicitally excluded by σ. Clearly, M learns L and behaves monotonically
on L, since for every D Ď N ﬁnite, every informant I for NzD and every t P N,
we have WMpIrtsq Ě NzD and therefore WMpIrtsq X NzD “ NzD.
This reproves the following result by Lange, Zeugmann, and Kapur [1996].
Corollary 23. There exists an indexable family monotonically Lim-learnable
from informants, but not strictly monotonically Lim-learnable from informants.
In particular,
rInf SMonLims Ĺ rInfMonLims.
We sum up the preceding results in the next theorem and also represent them
in Figure 1, in which black lines denote the backbone given by Lemma 4. The
therein claimed proper inclusions were already stated in the Propositions 20 and
22 as well as the Corollaries 21 and 23 thereafter.
Theorem 24. All
learning restrictions introduced in deﬁnition 3 but Caut,
Mon and SMon do not restrict fully informed Lim-learning from informants.
Further, for this kind of learning Caut and Mon are incomparable. For short
we have
(i) @δ P tConv, Dec, SDec, WMon, NU, SNUu : rInf δLims “ rInf Lims
(ii) rInfMonLims K rInf CautLims
Proof. The ﬁrst part is an immediate consequence of Proposition 19 and so is
the second part of the Propositions 20 and 22.
5 Outperforming Learning from Texts
In the following we relate informant learning to the more prominent concept of
learning from solely positive information, i.e., texts.
Learning Families of Formal Languages from Informants
23
Fig. 1. Relations between delayable learning restrictions in full-information (explana-
tory) Lim-learning of languages from informants.
Deﬁnition 25. Let L be a language. The set of all texts for the language L is
TxtpLq :“ t T P pN Y t#uqø | cntpTq “ Lu,
where cntpTq “ ranpTqzt#u is the content of T .
Thus, a text is just an enumeration of the language, since the pause symbol #
is interpreted as no new information but necessary for the empty language. In
case L is presented to the learner M in form of a text, M will never know for
sure that some natural number x is not in L. The learner, which was originally
thought of as a child while language acquisition, is in this setting modelled by a
(partial) computable function M : dompMq Ď Năø Ñ N.
Already Gold [1967] observed rTxtLims Ĺ rInf Lims and lateron Lange and
Zeugmann [1993] further investigated the interdependencies when considering
the diﬀerent monotonicity learning restrictions. For instance, they showed that
there exists an indexed family L P rInfMonLimszrTxtLims ‰ ∅ and in
contrast that for indexed families Inf SMonLim-learnability implies TxtLim-
learnability. We show that this inclusion fails on the level of families of recursive
languages.
InfLimSdInfLimTNUDecSMonMonWMonCautSDecSNUConv24
Martin Aschenbach, Timo K¨otzing, and Karen Seidel
Proposition 26. The class of recursive languages
L :“ t2pL Y txuq Y 2L ` 1 | L is recursive ^ WminpLq “ L ^ x ě minpLqu
is strongly monotonically Lim-learnable from informants. Moreover, L is not
Lim-learnable from texts, i.e.,
L P rInf SMonLimszrTxtLims.
Proof. Let pm denote an index for 2Wm Y 2Wm ` 1 and pm,x an index for
2pWmYtxuqY2Wm`1. The learner M will look for the minimum of the possible
L-generating language of the presented recursively enumerable set and moreover
try to detect the exception x, in case it exists. Thus, it checks for all m such
that 2m P pospσq or 2m ` 1 P pospσq whether for all k ă m holds 2k P negpσq
or 2k ` 1 P negpσq. In case m has this property relative to σ, we write σL
minpmq.
Further, M tries to ﬁnd x such that 2x P pospσq and 2x ` 1 P negpσq and we
excpxq that x is as wished. Consider the learner M deﬁned by
abbreviate by σL
$’&’%indp∅q,
pm,
pm,x,
Mpσq “
if there is no m with σL
if σL
if σL
minpmq and there is no x with σL
minpmq and σL
excpxq.
minpmq;
excpxq;
for all σ P pNˆt0, 1uqăø. Clearly, M strongly monotonically Lim-learns L.
To observe L R rTxtLims, assume there exists M such that L P TxtLimpMq.
By s-m-n there exists e P N such that for all i P N
Aσpiq “ t k P N | Mpσq ‰ Mpσ
Bσpiq “ t k P N | Mpσq ‰ Mpσ
(cid:97)p2e ` 4iqkqu;
(cid:97)p2e ` 4i ` 2qkqu;
σ0 “ p2e, 2e ` 1q;
σi,
(cid:97)
i p2e ` 4iqinfpAσipiqq(cid:97)p2e ` 4i ` 1q,
(cid:97)
i p2e ` 4i ` 2qinfpBσipiqq(cid:97)p2e ` 4i ` 3q,
tn | 2n ` 1 P ranpσiqu.
if Aσipiq “ Bσipiq “ ∅
or i ą 0 ^ σi´1 “ σi;
if Aσipiq ‰ ∅
^ infpAσipiqq ď infpBσipiqqq;
if Bσipiq ‰ ∅
^ infpBσipiqq ă infpAσipiqq;
$’’’’’’’’&’’’’’’’’%
iPN
σi`1 “
We “
We is recursive, because it is either ﬁnite or we can decide it along the construc-
tion of the σi. Thus, 2WeY2We`1 P L. If for some index i holds σi`1 “ σi, then
M fails to learn either 2pWeYte`2iuqY2We`1 or 2pWeYte`2i`1uqY2We`1.
On the other hand, if there is no such i, by letting T :“
iPN σi we obtain a
text for 2We Y 2We ` 1, on which M performs inﬁnitely many mindchanges.
Note that the learner witnessing the SMonLim-learnability of L is also conser-
vative and strongly decisive, which was not stated because of Theorem 24.
Learning Families of Formal Languages from Informants
25
6 Duality of the Vacillatory Hierarchy
After having investigated the relations between the diﬀerent delayable learning
restrictions in the setting of Lim-learning from informants and its relation to
learning from texts, we link it to other convergence criteria. In Proposition 8 we
already observed a hierarchy, when varying the number of anomalies and will
now show that allowing the learner to vacillate between ﬁnitely many correct
hypothesis in the limit does not give more learning power. On the contrary, only
requiring semantic convergence, i.e., allowing inﬁnitely many correct hypotheses
in the limit, does allow to learn more collections of languages even with an
arbitrary semantic learning restriction at hand.
As every indexable family of recursive languages is Lim-learnable from infor-
mants by enumeration, the vacillatory hierarchy collapses for such collections
bs for all
of languages, i.e., for all indexable families L we have L P rInf Lima
a P N Y t˚u and b P Ną0 Y t˚,8u. In contrast, we strengthen Proposition 8
(iii) by separating Inf Lim- and Inf Lim8-learning at the level of families of
recursive languages, even when requiring the Lim8-learning sequence to meet
all introduced delayable semantic learning restrictions.
Proposition 27. The collection of recursive languages
L :“ tL Y txu | L Ď N is recursive ^ WminpLq “ L ^ x ě minpLqu
is strongly monotonically Lim8-learnable from informants. Moreover, L is not
Lim-learnable from informants, i.e.,
L P rInf SMonLim8szrInf Lims.
Especially, for every δ P tCaut, Dec, Mon, SMon, WMon, NUu holds
L P rInf δLim8szrInf Lims.
Proof. By the Lemmas 4 and 12 it suﬃces to show
L P rInf canSMonLim8szrInf canLims.
By s-m-n there are p : Nˆt0, 1uăø ˆ N Ñ N and a learner M such that for all
σ P Nˆt0, 1uăø and x P N
Wppσ,xq “ Wminppospσqq Y txu and
$’&’%indp∅q,
minppospσqq,
ppσ, xq,
Mpσq “
if pospσq “ ∅;
else if pospσqzW
else if x “ minppospσqzW
|σ|
minppospσqq “ ∅;
|σ|
minppospσqqq;
26
Martin Aschenbach, Timo K¨otzing, and Karen Seidel
where indp∅q refers to the canonical index for the empty set. For every L in
Inf canSMonLim8pMq, let L Y txu P L with L Ď N recursive, WminpLq “ L
and x ě minpLq and let I be the methodical informant for LYtxu. Then for all
t ą minpLq we have WminppospIrtsqq “ WminpLq “ L. Further, let m be minimal
such that ty P L | y ă xu Ď W m
minpLq. Since x ě minpLq the construction yields
for all t P N
$’&’%∅,
Wht “
L,
L Y txu, otherwise.
if t ď minpLq;
else if minpLq ď t ă maxtx ` 1, mu;
This can be easily veriﬁed, since in case y P L we have L “ L Y tyu and shows
the Inf canSMonLim8-learnability of L by M .
In order to approach L R rInf canLims, assume to the contrary that there is a
learner M that Inf canLim-learns L. We are going to deﬁne a recursive language
L with WminpLq “ L helpful for showing that not all of L is Inf canLim-learned
by M . In order to do so, for every methodical σ P Nˆt0, 1uăø we deﬁne sets
σ stand for the methodical informant of pospσq, whereas
σ, A1
A0
σ we collect all t ą |σ|
σ denotes the methodical informant of pospσqYt|σ|u. In A0
I 1
for which M ’s hypothesis on I 0
σ we
σrts makes a guess diﬀerent from Mpσq.
capture all t ą |σ| such that M on I 1
This reads as follows
σrts is diﬀerent from Mpσq. Similarly, in A1
σ Ď N. For this let I 0
σ :“ t t P N | t ą |σ| ^ MpI 0
A0
σ :“ t t P N | t ą |σ| ^ MpI 1
A1
σrtsq ‰ Mpσqu,
σrtsq ‰ Mpσqu.
Note that for every t ą |σ|
σrts “ σ
I 0
σrts “ σ
I 1
(cid:97)pp|σ|, 0q,p|σ|`1, 0q, . . . ,pt ´ 1, 0qq,
(cid:97)pp|σ|, 1q,p|σ|`1, 0q, . . . ,pt ´ 1, 0qq.
By s-m-n there exists p P N such that (we use the convention infpHq “ 8)
@i P N : σi`1 “
σ0 “ pp0, 0q, . . . ,pp ´ 1, 0q,pp, 1qq,
σi “ A1
if A0
if infpA0
σiqs,
σiqs, otherwise;
σirminpA0
I 0
σirminpA1
I 1
pospσiq.
Wp “
$’&’%σi,
iPN
σi “ H;
σiq ď infpA1
σiq;
Learning Families of Formal Languages from Informants
27
By construction p “ minpWpq and Wp is recursive, which immediately yields
L :“ Wp P L. Further, for every i P N from σi ‰ σi`1 follows Mpσiq ‰ Mpσi`1q.
Aiming at a contradiction, let I be the methodical informant for L, which implies
iPN σi Ď I. Since M explanatory learns L and thus does not make inﬁnitely
many mind changes on I, there exists i0 P N such that for all i ě i0 we have
σi “ σi0 . But then for all t ą |σi0| holds
MpI 0
σi0
rtsq “ Mpσi0q “ MpI 1
σi0
rtsq,
thus M does not learn at least one of L “ pospσi0q and L Y t|σi0|u from their
methodical informants. On the other hand both of them lie in L and therefore,
M had not existed in the beginning.
Since allowing inﬁnitely many diﬀerent correct hypotheses in the limit gives
more learning power, the question arises, whether ﬁnitely many hypotheses al-
ready allow to learn more collections of languages. The following proposition
shows that as observed by B¯arzdi¸nˇs and Podnieks [1973] and Case and Smith
[1983] for function learning the hierarchy of vacillatory learning collapses when
learning languages from informants. Note that this contrasts the results in lan-
guage learning from texts by Case [1999], observing for every a P N Y t˚u a
hierarchy
rTxtLimas Ĺ . . . Ĺ rTxtLima
rTxtLima
bs Ĺ rTxtLima
bs Ĺ rTxtLima˚s Ď rTxtLima8s.
b`1s Ĺ . . .
bPNą0
Proposition 28. Let a P N Y t˚u. Then rInf Limas “ rInf Lima˚s.
Proof. Clearly, rInf Limas Ď rInf Lima˚s. For the other inclusion let L be in
rInf Lima˚s and M a learner witnessing this. By Lemma 14 we assume that M
is total. In the construction of the Lima-learner M1, we employ the recursive
function Ξ : pNˆt0, 1uqăø ˆ N Ñ N, which given σ P pNˆt0, 1uqăø and p P N
|σ|
Ξpσ,pq X negpσq “ ∅ and moreover, if σ Ď τ are such that
alters p such that W
|τ|
|σ|
p X negpτq, then Ξpσ, pq “ Ξpτ, pq. One way to do this is
p X negpσq “ W
by letting Ξpσ, pq denote the unique program, which given x successively checks,
whether x “ yi, where pyiqiă|negpσq| is the increasing enumeration of negpσq.
As soon as the answer is positive, the program goes into a loop. Otherwise it
executes the program encoded in p on x, which yields
ϕΞpσ,pqpxq “
if x P negpσq;
Ò,
ϕppxq, otherwise.
Now, M1 works as follows:
I. Compute pi :“ Mpσrisq for all i ď |σ|.
II. Withdraw all pi with the property |negpσq X W
|σ|
pi | ą a.
28
Martin Aschenbach, Timo K¨otzing, and Karen Seidel
III. Deﬁne M1pσq to be a code for the program coresponding to the union vote
of all Ξpσ, piq, for which pi was not withdrawn in the previous step:
Given input x, for n from 0 till 8 do the following: If i :“ π1pnq ď |σ|,
|σ|
|negpσqXW
pi | ď a and ΦΞpσ,piqpxq ď π2pnq, then return 0; otherwise
increment n.
This guarantees
ϕM1pσqpxq “
0,
Ò,
if D i ď |σ| p|negpσq X W
otherwise.
|σ|
pi | ď a ^ ϕΞpσ,piqpxqÓq;
Intuitively, M1pσq eliminates all commission errors in guesses of M on initial
segments of σ, not immediately violating the allowed number of anomalies, and
then asks whether one of them converges on the input, which implies
WM1pσq “
iď|σ|,|negpσqXW
|σ|
pi |ďa
WΞpσ,Mpσrisqq.
In order to show L P Inf LimapMq, let L P L and I P InfpLq. As L P Lima˚pMq,
there is t0 such that all of M ’s hypotheses are in ths | s ď t0u and additionally
| W t0
hs X NzL| ą a for all s ď t0 with | Whs X NzL| ą a. Moreover, we can assume
that for all s ď t0 with | Whs X NzL| ď a we have observed all commission errors
in at most t0 steps, which formally reads as Whs X NzL “ W t0
Then for all t ě t0 we obtain the same set of indices
hs X NzL.
A :“ t ΞpIrts, piq | i ď t ^ |negpIrtsq X W t
pi| ď au
X NzL “ ∅. Further, since ϕh1
and therefore M1 will return syntactically the same hypothesis, namely, h1
“a L. By construction and the choice of t0 there
It remains to argue for Wh1
pxq exists
t0
are no commission errors, i.e., Wh1
in case there is at least one p P A such that ϕppxq exists, there are at most a
t0
arguments, on which ϕh1
t0
Similar to results for language learning from texts for b P t1,8u by Case and
Lynes [1982], we gain a strict hierarchy when bounding the number of inﬁnitely
often occuring correct hypotheses for the target by a ﬁxed number a P N Y t˚u
as already observed in Proposition 8.
is undeﬁned.
t0
t0
7 Conclusion and Future Work
This paper investigates learning formal languages in the limit from informants,
i.e., inferring from a steadily growing sample set of positive and negative data
one or more (almost) correct enumeration procedure(s) for the target. Whereas
Section 2 provides necessary deﬁnitions and the back-bone of delayable learning
restrictions in this setting, Section 3 establishes the normal form of being to-
tal and additionally that, for every delayable learning success criterion, we only
Learning Families of Formal Languages from Informants
29
need to consider learning from canonical informants. Thereafter, in Section 4,
the complete picture for (set-driven) Lim-learning from informants with a de-
layable learning restriction, as depicted in the diagram in Figure 2, is derived.
It is also valid in case exclusively indexable families are considered. Important
examples are all levels in the Chomsky hierarchy but type-0, i.e. recursively
enumerable, formal languages. The proofs combine diﬀerent techniques and em-
ploy the connections provided beforehand. In contrast to the observed hierarchies
when learning success allows for deducing approximations to the target language
of diﬀerent quality in Section 2, Section 6 provides that requiring the learner to
eventually output exactly one correct enumeration procedure is as powerful as
allowing any ﬁnite number of correct descriptions in the limit. Thereafter we
show that, in learning from informants, even when facing all semantic learning
restrictions at hand, we gain more learning power, in case learning is considered
successful also for an inﬁnite number of correct descriptions in the limit.
Inf Ex
SdInf Ex
NU
SNU
WMon Dec
SDec
Conv
Mon
Caut
SMon
Fig. 2. Diagram depicting the relations between delayable learning restrictions in (ex-
planatory) Lim-learning of languages from informants.
Further research could investigate the relationships between the diﬀerent de-
layable learning restrictions for other convergence criteria, where the general
results in Section 3 may be helpful. Further, whether requiring the learner to
be total restricts consistent learning from informants, seems like an appropri-
ate indicator concerning the conjecture of delayability being the right structural
property to gain deeper insights into the connections and diﬀerences between all
the available deﬁnitions of learning success. To this end, results by Akama and
Zeugmann [2008] and Case and K¨otzing [2008] may be helpful.
Another open question regards the relation between learning recursive functions
from texts for their graphs and learning languages from either informants or
texts. It seems like delayability plays a crucial role in order to obtain normal
forms and investigate how learning restrictions relate in each setting. It is yet
not clear, whether delayability is the right assumption to generalize Lemma 7.
Consult the survey by Zeugmann and Zilles [2008] and the standard textbook
30
Martin Aschenbach, Timo K¨otzing, and Karen Seidel
by Jain, Osherson, Royer, and Sharma [1999] for more results in the setting of
function learning which may transfer to learning collections of languages from
informants with such a generalization.
According to Osherson, Stob, and Weinstein [1986] requiring the learner to base
its conjecture only on the previous one and the current datum, makes Lim-
learning harder. While the relations between the delayable learning restrictions
for these so called iterative learners in the presentation mode of solely positive
information has been investigated by Jain, K¨otzing, Ma, and Stephan [2016], so
far this has not been done when learning from informants. For indexable families,
this was already of interest to Lange and Grieser [2003] and may oﬀer surprising
observations.
For automatic structures as alternative approach to model a learner, there have
already been investigations on how diﬀerent types of text aﬀect the explanatory
learnability, see Jain, Luo, and Stephan [2010] and H¨olzl, Jain, Schlicht, Seidel,
and Stephan [2017]. The latter started investigating how learning from canonical
informants and learning from text relate to one another in the automatic setting.
A natural question seems to be what eﬀect other kinds of informants and learning
success criteria have.
Last but not least, rating the model’s value for other research aiming at under-
standing the capability of human and machine learning seems the most challeng-
ing task to tackle.
This work was supported by the German Research Foundation (DFG) under
Grant KO 4635/1-1 (SCL).
Bibliography
Y. Akama and T. Zeugmann. Consistent and coherent learning with δ-delay.
Information and Computation, 206(11):1362–1374, 2008.
D. Angluin. Inductive inference of formal languages from positive data. Infor-
mation and control, 45(2):117–135, 1980.
G. Baliga, J. Case, W. Merkle, F. Stephan, and W. Wiehagen. When unlearning
helps. Information and Computation, 206:694–709, 2008.
J. B¯arzdi¸nˇs. Two theorems on the limiting synthesis of functions. In Theory of
Algorithms and Programs, Latvian State University, Riga, 210:82–88, 1974.
J. B¯arzdi¸nˇs. Inductive inference of automata, functions and programs. In Amer.
Math. Soc. Transl., pages 107–122, 1977.
J. B¯arzdi¸nˇs and K. Podnieks. The theory of inductive inference. In Mathematical
Foundations of Computer Science, 1973.
L. Blum and M. Blum. Toward a mathematical theory of inductive inference.
Information and Control, 28:125–155, 1975.
L. Carlucci, J. Case, S. Jain, and F. Stephan. Non-U-shaped vacillatory and
team learning. Journal of Computer and System Sciences, 74:409–430, 2008.
J. Case. The power of vacillation in language learning. SIAM Journal on Com-
puting, 28(6):1941–1969, 1999.
J. Case. Gold-style learning theory. In Topics in Grammatical Inference, pages
1–23. 2016.
J. Case and T. K¨otzing. Dynamically delayed postdictive completeness and
consistency in learning. In Proc. of ALT (Algorithmic Learning Theory), pages
389–403, 2008.
J. Case and T. K¨otzing. Diﬃculties in forcing fairness of polynomial time in-
ductive inference. In Proc. of Algorithmic Learning Theory, pages 263–277,
2009.
J. Case and C. Lynes. Machine inductive inference and language identiﬁcation.
In Proc. of ICALP (International Colloquium on Automata, Languages and
Programming), pages 107–115, 1982.
J. Case and S. Moelius. Optimal language learning from positive data. Infor-
mation and Computation, 209:1293–1311, 2011.
J. Case and C. Smith. Comparison of identiﬁcation criteria for machine inductive
inference. Theoretical Computer Science, 25(2):193–220, 1983.
M. Fulk. A Study of Inductive Inference Machines. PhD thesis, SUNY at Buﬀalo,
1985.
M. Fulk, S. Jain, and D. Osherson. Open problems in Systems That Learn.
Journal of Computer and System Sciences, 49(3):589–604, December 1994.
E. Gold. Language identiﬁcation in the limit.
Information and Control, 10:
447–474, 1967.
R. H¨olzl, S. Jain, P. Schlicht, K. Seidel, and F. Stephan. Automatic learning
from repetitive texts. In Proc. of Algorithmic Learning Theory, pages 129–150,
2017.
32
Martin Aschenbach, Timo K¨otzing, and Karen Seidel
S. Jain, D. Osherson, J. Royer, and A. Sharma. Systems that Learn: An Intro-
duction to Learning Theory. MIT Press, Cambridge, Massachusetts, second
edition, 1999.
S. Jain, Q. Luo, and F. Stephan. Learnability of automatic classes. In LATA,
pages 321–332, 2010.
S. Jain, T. K¨otzing, J. Ma, and F. Stephan. On the role of update constraints and
text-types in iterative learning. Information and Computation, 247:152–168,
2016.
K. Jantke and H. Beick. Combining postulates of naturalness in inductive infer-
ence. Humboldt-Universit¨at zu Berlin. Sektion Mathematik, 1980.
K. P. Jantke. Monotonic and nonmonotonic inductive inference of functions and
patterns. In Nonmonotonic and Inductive Logic, 1st International Workshop,
Proc., pages 161–177, 1991.
T. K¨otzing. Abstraction and Complexity in Computational Learning in the Limit.
PhD thesis, University of Delaware, 2009.
T. K¨otzing and R. Palenta. A map of update constraints in inductive inference.
In Algorithmic Learning Theory, pages 40–54, 2014.
T. K¨otzing, M. Schirneck, and K. Seidel. Normal forms in semantic lan-
guage identiﬁcation. In Proc. of Algorithmic Learning Theory, pages 493–516.
PMLR, 2017.
S. Lange and G. Grieser. Variants of iterative learning. Theoretical computer
science, 292(2):359–376, 2003.
S. Lange and T. Zeugmann. Monotonic versus non-monotonic language learning.
In Proc. of Nonmonotonic and Inductive Logic, pages 254–269, 1993.
S. Lange and T. Zeugmann. Characterization of language learning from in-
formant under various monotonicity constraints. Journal of Experimental &
Theoretical Artiﬁcial Intelligence, 6(1):73–94, 1994.
S. Lange, T. Zeugmann, and S. Kapur. Monotonic and dual monotonic language
learning. Theoretical Computer Science, 155(2):365–410, 1996.
S. Lange, T. Zeugmann, and S. Zilles. Learning indexed families of recursive
languages from positive data: A survey. Theoretical Computer Science, 397
(1):194–232, 2008.
P. Odifreddi. Classical Recursion Theory, volume II. Elsivier, Amsterdam, 1999.
D. Osherson and S. Weinstein. Criteria of language learning. Information and
Control, 52:123–138, 1982.
D. Osherson, M. Stob, and S. Weinstein. Learning strategies. Information and
Control, 53:32–51, 1982.
D. Osherson, M. Stob, and S. Weinstein. Systems that Learn: An Introduc-
tion to Learning Theory for Cognitive and Computer Scientists. MIT Press,
Cambridge, Mass., 1986.
L. Pitt. Inductive inference, DFAs, and computational complexity. In Proc. of
AII (Analogical and Inductive Inference), pages 18–44, 1989.
H. Rogers. Theory of Recursive Functions and Eﬀective Computability. McGraw
Hill, New York, 1967. Reprinted, MIT Press, 1987.
J. Royer and J. Case. Subrecursive Programming Systems: Complexity and Suc-
cinctness. Research monograph in Progress in Theoretical Computer Science.
Birkh¨auser Boston, 1994.
Learning Families of Formal Languages from Informants
33
G. Sch¨afer-Richter. ¨Uber Eingabeabh¨angigkeit und Komplexit¨at von Inferenzs-
trategien, 1984. Dissertation, RWTH Aachen.
K. Wexler and P. Culicover. Formal Principles of Language Acquisition. MIT
Press, Cambridge, Massachusetts, 1980.
R. Wiehagen. A thesis in inductive inference. In Nonmonotonic and Inductive
Logic, 1st International Workshop, Proc., pages 184–207, 1991.
T. Zeugmann and S. Zilles. Learning recursive functions: A survey. Theoretical
Computer Science, 397:4–56, 2008.
