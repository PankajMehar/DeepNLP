
Lots of biological experiments and theoretical analysis have
demonstrated that the speed and scale of processing infor-
mation by biological neural networks are much faster and
larger than by manual methods [1] [2]. Inspired by animals’
central nervous systems in particular the brain, many kinds
Jie Yang · Pingping Zhang
School of Mathematical Sciences, Dalian University of Technology,
Dalian 116024, China.
E-mail: yangjiee@dlut.edu.cn; jssxzhpp@dlut.edu.cn
Yan Liu
School of information Science and Engineering, Dalian Polytechnic
University, Dalian 116034, China.
E-mail: liuyan.3001@gmail.com
of artiﬁcial neural networks (ANNs) and training methods
have presented to mimic animals’ behavior characteristics.
ANNs are distributed mathematical models that process in-
formation parallelly [3] [4]. They have been used to solve
a wide variety of tasks that are hard to solve by using or-
dinary rule-based programming, including computer vision
and speech recognition [5] [6] [7] [8]. These networks base
on the characteristics and scales of data and the complex-
ity of systems. By adaptively adjusting the weights which
are connected between different nodes in adjacent layers,
ANNs can achieve the purpose of processing information.
As a special class of ANNs, spiking neural networks (SNNs)
can simulate spikes generated between animal dendrites and
axons of neurons [4] [9]. With temporal information coding
in single spikes to process information, they were proved to
be a type of strong anthropomorphic networks.
However, due to many uncontrollable factors, such as
noising inputs, individual spike decay times, thresholding
weights, the abilities of processing information by spiking
neural networks may be affected [10] [11]. In order to get
an available network architecture, it is important to do some
research on its robustness. Because of encoding input vari-
ables by time differences between pulses, spiking neurons
are particularly sensitive to the input signals. Recently, this
has led to several explorations on the computational abili-
ties and learning performance of neuromorphic networks of
spiking neurons with noise [12] [13]. However, these works
have not considered the type of perturbations and the robust-
ness of classiﬁcation ability of SNNs.
To analyze the robustness of SNNs’ classiﬁcation abil-
ities, in this paper we performed a series of numerical ex-
periments on the classical XOR problem and other three
benchmark datasets (i.e., Iris dataset, Wisconsin breast can-
cer dataset and StatLog landsat dataset) with the SpikeProp
algorithm [14]. Notably, the closer the perturbed inputs to
the desired points, the more perturbed points there are in the
2
Jie Yang et al.
Fig. 1 The structure of a simple spiking neural network.
input space. What’s more, the perturbation may be periodic
in practice. These facts led us to consider the sinusoidal and
Gaussian perturbations in this paper.
To summarize, our main contributions include:
– As far as we know, this is the ﬁrst work to validate the ro-
bustness of classiﬁcation ability of SNNs which receive
perturbed inputs.
– Two kinds of perturbations were considered for robust-
ness of classiﬁcation ability of a SNN. In fact, perturba-
tions could be arbitrary styles performing on the inputs.
We only focus on sinusoidal and Gaussian perturbations.
– On the classical XOR problem and other three bench-
mark datasets, we evaluate the classiﬁcation ability of
SNNs and show its robustness experimentally.
2 Spiking neural networks
Compared with traditional neural networks (such as BP),
SNNs have several differences in network architectures. The
most important one is that there are multiple synaptic termi-
nals and the speciﬁc synaptic delay between spiking neurons
in adjacent layers. In addition, due to the fast temporal en-
coding which is very different from traditional rate-coded
networks, spiking neurons can signiﬁcantly improve com-
plex non-linear classiﬁcation performances [4] [14].
2.1 The architectures of SNNs
A simple feed-forward SNN with multiple input spiking neu-
rons and one output spiking neuron is shown in Fig 1. The
network architecture consists of one input layer, one hidden
layer and one output layer, denoted by I, H and O respec-
tively. Each connection between different layers comprises
several synapses and each neuron receives a set of spikes
from all its previous neurons. Formally, assuming that dur-
ing the simulation interval each neuron generates at most
one spike and ﬁres when the internal state variable reaches
a threshold, the state variable x j of neuron j receives out-
puts from all its previous neurons as a weighted sum of the
pre-synaptic contributions:
x j(t) = Σi∈D jΣm
k=1wk
i jyk
i (t)
(1)
where D j denotes the set of pre-synaptic neurons associated
with neuron j, wi j is the weight associated with synaptic
terminal k, and yk
i (t) represents a delayed pre-synaptic po-
tential (PSP) for each terminal,
yk
i (t) = ε(t − ti − dk)
(2)
with ε(t) a spike-response function. The time ti is the ﬁring
time of pre-synaptic neuron i, and dk is the delay associated
with the synaptic terminal k. The ﬁring time is determined as
the ﬁrst time when the state variable reaches the threshold.
The spike-response function is always described as the form
ε(t) =(cid:26) t
τ exp(1− t
0,
τ ), t > 0
t ≤ 0
(3)
where τ models the membrane potential decay time constant
that determines the rise and decay time of the PSP.
2.2 Learning algorithm
The basic SpikeProp algorithm [14] is performed and we
choose the least mean square as the error-function. Given
desired spike times {t d
j } and actual ﬁring times {t j}, we can
derive the form of the error-function
E =
Σj(t j − t d
j )2.
(4)
For error back-propagation, the weights update rule is fol-
lowed:
wk+
i j
(t j) = wk
i j(t j) + ∆wk
i j(t j)
Deﬁne
δj =
t d
j − t j
∂yl
Σil wl
i j
i (t j )
∂t j
(5)
(6)
In the output layer, the basic weight adaptation function for
neurons is derived as
∆wk
i j(t j) = −η
yk
i (t j)(t d
j − t j)
∂yl
i (t j )
∂t j
Σilwl
i j
= −ηyk
i (t j)δj.
(7)
For the hidden layers, the weight adaptation function for
neurons is given by
∆wk
hi(t j) = −η
∂yk
i (t j )
∂t j
yk
h(t j)Σj{δjΣkwk
∂yl
n(t j )
∂t j
Σnlwl
ni
i j
(8)
where η is the learning rate of the network. For more details,
see [14] [15] [16].
Robustness of classiﬁcation ability of spiking neural networks
3 Perturbations to neural networks
Table 1 Encoded inputs and outputs for XOR problem
Input
patterns Output patterns
16
10
10
16
(1)Sinusoidal perturbations
3.1 Traditional neural network perturbation approaches
Traditional neural networks are led to suitable Lyapunov-
Krasovskii functional and by introducing appropriate ran-
dom variables, such as the free weights techniques, one can
analyze stochastic neural networks associated with the value
of parameter uncertainties and numerical experiments to demon-
strate the robust global exponential stability or asymptotic
stability. In [17], the effect of both variation range and dis-
tribution of the time delay were taken into account for delay-
distribution-dependent state estimation. The stochastic per-
turbations are described in terms of a Brownian motion and
the time-varying delay is characterized by introducing a Bernoulli
stochastic variable. In [18], some parameter matrices were
used to express the relationships among the system variables
which were perturbed to study the asymptotic stability of
delay nonlinear cellular neural network. By perturbing the
time variable interval of Hopﬁeld neural networks, [19] in-
vestigates the existence of the equilibrium points and global
robust exponential stability .
The original input ˜x0 is perturbed by a sinusoidal perturba-
tion term, which can be expressed as
˜x = ˜x0 + A sin(2πy)
(10)
where A is a constant between (0,1] to control the perturba-
tion amplitude, and y is a random vector and its component
values belong to [0,1]. The numbers of components associ-
ated with ˜x0 and y are the same.
(2)Gaussian perturbations
The original input ˜x0 is perturbed by a Gaussian perturba-
tion. This is different from the sinusoidal perturbation, and
can be expressed as
3.2 Our perturbation approach
SNNs use the reacted pulses to transmit information. When
the input signal enter into a spiking neural network, the state
of each neuron will change (see formulas (1),(2),(3)). Once
the state variable exceeds the threshold value, the neuron
arouses a pulse. Hence input signals of SNNs have a great
relevance to their robustness. A large number of practical
examples imply that the signals after random perturbations
will not be very different from the original one, and only
few perturbed signals have large deviations from the origi-
nal data. With these basic facts, we are motivated to perform
different types of perturbations on input signals, and inves-
tigate the the robust classiﬁcation abilities of SNNs.
For the sake of simplicity, we only describe perturba-
tions on XOR problem. For other three benchmarks, the op-
eration is similar. Classical XOR problem requires hidden
units to transform the inputs into the desired outputs and
needs to classify 4 points (described as (0,0),(1,1),(1,0),(0,1)
) into 2 categories. It should be noted: if the four points
are perturbed with a same value, it is equivalent to translat-
ing them simultaneously, leading no difference between the
original inputs in essence. For convenience, we choose the
point (1,1) as a target point to test the robustness of spiking
neural networks. The basic perturbation formula is
˜x = ˜x0 + σ
(9)
with ˜x0 being the original input, and σ the noise term. The
formula means if ˜x0 = (a, b), the perturbed input ˜x will be
(a + σ, b + σ).
˜x = ˜x0 + ˜x0(I − exp(−r2/2) · sgn(l))
(11)
where I is an identity matrix, r is a random vector whose
component values belong to [0,1] and sgn(l) is the signal
function associated with l. Here the numbers of components
associated with ˜x0, r and l are also the same.
4 Experimental results
Since our aim is to assess the robustness of classiﬁcation
ability of SNNs, we don’t need to design complex SNNs.
We train a simple spiking neural network on XOR problem
and other three benchmark datasets.
4.1 XOR problem
Firstly, the input and output signals of spiking neural net-
works are coded as in [14]. If Max and Min are extremal
values of a variable x (e.g.an input signal), we can encode it
as a spike ﬁred in the time
f (x) =
x − Min
Max − Min
· L
(12)
with the length of coding interval L.
For a better representation of spike-time patterns, we can
associate a 0 with a “late” ﬁring time and a 1 with an “early”
ﬁring time. With speciﬁc values 0 and 6 for the respective in-
put times, we lead to the temporally encoded XOR [14] [20]
showing in Table 1. In the table, the input numbers repre-
sent spike times (i.e. late ﬁring times and early ﬁring times)
in milliseconds. The actual input patterns contain a setting
4
Jie Yang et al.
Table 2 The result of the correct classiﬁcation with sinusoidal per-
turbations. ROS: Rates of the correct classiﬁcation without sinusoidal
perturbations, RWS: Rates of the correct classiﬁcation with sinusoidal
perturbations.
Table 3 The result of the correct classiﬁcation with Gaussian pertur-
bations. ROG: Rates of the correct classiﬁcation without Gaussian per-
turbations, RWG: Rates of the correct classiﬁcation with Gaussian per-
turbations.
Epoches A
ROS
RWS
Epoches
50
50
50
50
50
50
0.001
0.01
0.1
0.2
0.5
0.8
90.50
89.50
91.00
88.50
87.50
87.50
91.00
87.80
88.90
87.20
82.24
85.58
threshold by adding a third input neuron. Here we deﬁne the
difference between the times equivalent with “0” and “1” as
the coding interval L = 6 ms. We use the feed-forward net-
work with connections that have a delay interval of 15 ms, so
the available synaptic delays are 1-16 ms for classiﬁcation.
According to the formula (3), we calculated the postsynaptic
states and selected τ = 7 ms.
Based on the above settings, numerical experiments were
performed with a spiking neural network composed of one
input layer, one hidden layer and one output layer. There
were three input layer neurons (two encoding neurons and
one threshold neuron), ﬁve hidden layer neurons (of which
one inhibitory neuron generating only negative signal PSPs)
and one output layer neuron. A weight and corresponding
terminals (m = 16) were conﬁgurated between each pair of
synaptic neurons in adjacent layers.
Using computer simulations, we randomly generated 400
perturbed samples for different r to see the distribution of
perturbed samples as shown in Fig 2. Considering the com-
putational complexity and time, we only used 160 perturbed
samples for testing. We varied A and r∗ (the least upper
bound of all components to the random vector r )to control
perturbation amplitudes. The network reliably learned the
XOR patterns with η = 0.01. With different perturbations,
we got the correct classiﬁcation rate of the spiking neural
network. The average of correct classiﬁcation rates with and
without perturbed data are compared in Table 2 and Table 3.
As results are reported in Table 2 and Table 3, the clas-
siﬁcation accuracies associated with original data are differ-
ent but almost equal (about 90%).This is mainly because the
input data were reordered after each epoch. The larger per-
turbation was performed to the input data, the more greatly
rates of correct classiﬁcation with sinusoidal disturbances
decreased (see Table 2). When we disturbed the input data
with Gaussian perturbations, the network did not got simi-
lar results (see Table 3). The correct classiﬁcation rates of
the network did not fall too much. The reason may be that
most of the perturbed data by Gaussian perturbations were
clustered around the desired value. Therefore, it indicates
r∗
0.1
0.2
0.3
0.4
0.5
0.1
0.2
0.3
0.4
0.5
ROG
RWG
92.00
92.00
89.50
89.50
88.50
88.25
89.75
91.75
91.00
88.25
88.62
88.45
87.80
88.15
85.00
88.66
89.60
90.86
89.76
88.81
50
50
50
50
50
100
100
100
100
100
spiking neural networks have strong anti-interference abili-
ties.
4.2 Other three benchmarks
To further validate robustness of classiﬁcation ability of SNNs
in practice, we consider the following three benchmarks with
realistic signiﬁcance. As XOR problem does, we ﬁrst adopt
the method in [14] to encode continuous input variables in
min,...,In
spike times. Specially, for a variable n with a range[In
we use N neurons with Gaussian receptive ﬁelds to encode
the input variable. For a neuron i, its center was set to In
min +
min}/(N −2) and width σ = 1/β·{In
(2i−3)/2·{In
max −
In
min}/(N − 2). We set β = 1.5 for the remainder experi-
ments. As for output classiﬁcation, we encoded the patterns
according to a winner-take-all paradigm.
max− In
max],
(1) Iris dataset
The Iris ﬂower data set or Fisher’s Iris data set [21] is
a multivariate data set as a typical test case for many clas-
siﬁcation techniques in machine learning. The data set con-
sists of 50 samples from each of three species of Iris (Iris
setosa, Iris virginica and Iris versicolor). Four features are
measured for each sample: the length and the width of the
sepals and petals in centimetres, respectively. Based on the
combination of these four features, we add above perturba-
tions in each samples to build perturbed input sets. In this set
of experiments, we implement a SNN with three layers just
like for XOR problem. But each feature was encoded by 12
neurons with Gaussian receptive ﬁelds(yielding 48 encoding
neurons and one threshold neuron). And the SNN consists of
10 hidden layer neurons (only one neuron of which gener-
ates negative signal PSPs) and 3 output neurons. The num-
ber of corresponding synaptic terminals is same as the last
experiment(i.e.m = 16). We train this SNN with perturbed
input data or clear input data to distinguish the species from
each other. The results are presented in Table 4 and Table 5.
Robustness of classiﬁcation ability of spiking neural networks
r*=3.0,sigma=0.5
r*=3.0,sigma=1.0
r*=3.0,sigma=1.5
1.8
1.6
1.4
1.2
0.8
0.6
0.4
0.2
1.8
1.6
1.4
1.2
0.8
0.6
0.4
0.2
1.8
1.6
1.4
1.2
0.8
0.6
0.4
0.2
1.8
1.6
1.4
1.2
0.8
0.6
0.4
0.2
1.8
1.6
1.4
1.2
0.8
0.6
0.4
0.2
1.8
1.6
1.4
1.2
0.8
0.6
0.4
0.2
0.2
0.4
0.6
0.8
1.2
1.4
1.6
1.8
r*=4.0,sigma=1.0
0.2
0.4
0.6
0.8
1.2
1.4
1.6
1.8
0.2
0.4
0.6
0.8
1.2
1.4
1.6
1.8
r*=2.0,sigma=1.0
0.2
0.4
0.6
0.8
1.2
1.4
1.6
1.8
0.2
0.4
0.6
0.8
1.2
1.4
1.6
1.8
r*=8.0,sigma=1.0
0.2
0.4
0.6
0.8
1.2
1.4
1.6
1.8
Fig. 2 The perturbed samples for different r∗. The desired point is (1,1) and the closer to the desired point, the more perturbed points there are.
Besides, the perturbed points become more scattered when r∗ increases.
Table 4 The result of the correct classiﬁcation with sinusoidal pertur-
bations on Iris dataset. ROS: Rates of the correct classiﬁcation without
sinusoidal perturbations, RWS: Rates of the correct classiﬁcation with
sinusoidal perturbations.
Table 5 The result of the correct classiﬁcation with Gaussian pertur-
bations on Iris dataset. ROG: Rates of the correct classiﬁcation without
Gaussian perturbations, RWG: Rates of the correct classiﬁcation with
Gaussian perturbations.
Epoches A
ROS
RWS
Epoches
and 2 output layer neurons. The results are presented in Ta-
ble 6 and Table 7.
500
500
500
500
500
500
0.001
0.01
0.1
0.2
0.5
0.8
96.50
94.60
91.73
91.50
89.35
88.50
95.71
93.84
88.90
88.40
87.62
87.58
(2) Wisconsin breast cancer (Original) dataset
The breast cancer (Original) dataset [22] is from the Uni-
versity of Wisconsin Hospitals and contains 699 case en-
tries, divided into benign and malignant cases. Each case
has 9 measurements and each measurement is assigned an
integer between 1 and 10, with larger numbers indicating
a greater likelihood of malignancy. In our experiments, we
encoded each measurement with 7 equally spaced neurons
covering the input range. We set 15 hidden layer neurons
r∗
0.1
0.2
0.3
0.4
0.5
0.1
0.2
0.3
0.4
0.5
0.1
0.2
0.3
0.4
0.5
ROG
RWG
96.10
94.80
94.50
91.50
89.56
96.21
95.75
94.25
91.00
89.25
96.25
95.35
91.27
91.08
89.21
96.02
94.43
94.05
90.13
88.00
96.66
94.90
93.76
89.74
88.85
96.01
94.60
90.86
90.67
89.01
750
750
750
750
750
1000
1000
1000
1000
1000
1500
1500
1500
1500
1500
6
Jie Yang et al.
Table 6 The result of the correct classiﬁcation with sinusoidal pertur-
bations on Wisconsin breast cancer (Original) dataset. ROS: Rates of
the correct classiﬁcation without sinusoidal perturbations, RWS: Rates
of the correct classiﬁcation with sinusoidal perturbations.
Table 8 The result of the correct classiﬁcation with sinusoidal pertur-
bations on StatLog landsat dataset. ROS: Rates of the correct classi-
ﬁcation without sinusoidal perturbations, RWS: Rates of the correct
classiﬁcation with sinusoidal perturbations.
Epoches A
ROS
RWS
Epoches A
ROS
RWS
1500
1500
1500
1500
1500
1500
0.001
0.01
0.1
0.2
0.5
0.8
97.50
97.34
95.60
95.50
96.02
93.56
97.60
97.20
95.53
93.80
94.84
91.68
6000
6000
6000
6000
6000
6000
0.001
0.01
0.1
0.2
0.5
0.8
85.50
85.17
85.00
85.21
84.50
83.10
85.61
84.80
84.90
85.20
82.32
82.04
Table 7 The result of the correct classiﬁcation with Gaussian pertur-
bations on Wisconsin breast cancer (Original) dataset. ROG: Rates of
the correct classiﬁcation without Gaussian perturbations, RWG: Rates
of the correct classiﬁcation with Gaussian perturbations.
Table 9 The result of the correct classiﬁcation with Gaussian pertur-
bations on StatLog landsat dataset. ROG: Rates of the correct clas-
siﬁcation without Gaussian perturbations, RWG: Rates of the correct
classiﬁcation with Gaussian perturbations.
Epoches
1000
1000
1000
1000
1000
1500
1500
1500
1500
1500
r∗
0.1
0.2
0.3
0.4
0.5
0.1
0.2
0.3
0.4
0.5
ROG
RWG
95.75
95.85
94.75
92.00
91.57
97.40
97.13
95.57
96.03
93.54
96.06
94.60
94.86
91.96
91.17
97.52
96.59
93.86
95.45
91.60
Epoches
6000
6000
6000
6000
6000
7500
7500
7500
7500
7500
r∗
0.1
0.2
0.3
0.4
0.5
0.1
0.2
0.3
0.4
0.5
ROG
RWG
85.30
85.07
83.50
83.46
81.56
85.60
85.00
84.50
82.58
81.80
85.02
84.45
82.83
83.15
81.00
85.62
84.75
83.80
82.15
80.97
(3) StatLog landsat dataset
To test the robustness of SNNs on a larger dataset, we
investigated the Landsat dataset as described in the StatLog
survey of machine learning algorithms [23]. This dataset
consists of a training set of 4435 cases and a test set of 2000
cases and contains 6 ground cover types (classes). Each sam-
ple contains the values of a 33 pixel patch and each pixel is
described by 4 spectral bands. For classiﬁcation of a single
pixel, each case contains the values of a 33 pixel patch, with
each pixel described by 4 spectral bands, totaling 36 inputs
per case. For each band, we used the average value of cor-
responding bands of 9 pixels as a new band of a pixel. Then
the case was represented with one average pixel and each
separate band of it was encoded with 25 neurons. In this set
of experments The results obtained by the Statlog survey are
summarized in Table 8 and Table 9.
From the above results, we could conclude that the ap-
plication of SNNs with SpikeProp algorithm on temporally
encoded versions of benchmark problems yields a satisfac-
tory robustness for sinusoidal and Gaussian perturbations.
5 Conclusions
In this work, the robustness of the classiﬁcation ability of
SNNs has been investigated by disturbing input signals with
different perturbation methods not only for the classical XOR
problem but also for some other complicated realistic prob-
lems. From the experiments results, it can be concluded that
nevertheless perturbations will affect the classiﬁcation abil-
ity of SNNs, the classiﬁcation ability does not decrease dra-
matically and the networks have a certain anti-interference
capability.
References
1. Rochester, N., J.H. Holland, L.H. Habit, and W.L, Duda,Tests on a
cell assembly theory of the action of the brain, using a large digital
computer. IRE Transactions on Information Theory,2,80-93(1956)
2. Thorpe S, Fize D, Marlot C, Speed of processing in the human vi-
sual system. Nature,381,520-522 (1996)
3. Werbos, P.J,Beyond Regression: New Tools for Prediction and
Analysis in the Behavioral Sciences. PhD thesis, Harvard University,
(1974)
4. Wolfgang Maass,Networks of spiking neurons: The third gen-
eration of neural network models.Neural Networks,10,1659-
1671(1997)
5. M Riesenhuber, T Poggio,Hierarchical models of object recogni-
tion in cortex. Nature neuroscience,2,1019-1025(1999)
6. Fukushima, K.,Neocognitron: A self-organizing neural network
model for a mechanism of pattern recognition unaffected by shift
in position. Biological Cybernetics,36,93-202(1980)
7. A. Graves, M. Liwicki, S. Fernandez, R. Bertolami, H. Bunke,
Improved
J. Schmidhuber,A Novel Connectionist System for
Robustness of classiﬁcation ability of spiking neural networks
Unconstrained Handwriting Recognition. IEEE Transactions on
PAMI,31,855-868(2009)
8. D.V. Buonomano, M. Merzenich. A neural network model of tem-
poral code generation and position-invariant pattern recognition.
Neural Comput,11,103-116(1999)
9. Bienenstock,E, A model of neocortex.Network:Computation in
Neural Systems,6,179-124(1995)
10. Vogl, T.P., Mangis, J.K., Rigler, A.K., Zink, W.T., Alkon,
the back-propagation
D.L., Accelerating the convergence of
method,Biol.Cybern.,59,257-263(1988)
11. Ghosh-Dastidar,
networks
detection,Integr.Comput.-Aid E.,14,187-212(2007)
neural
for EEG classiﬁcation and epilepsy and seizure
S., Adeli, H.,Improved
spiking
12. Maass,W., Noisy spiking neurons with temporal coding have more
computational power than sigmoidal neurons,In: Advances in Neural
Information Processing Systems,MIT Press,Cambridge,USA,9,211-
217 (1997)
13. Wolfgang Maass.Noise as a Resource for Computation and
Learning in Networks of Spiking Neurons.Proceedings of the
IEEE,102,860-880(2014)
14. Sander M.Bothe,
backpropagation in temporally encoded networks of
neurons. Neurocomputing 48,17-37(2002)
Joost N.Kok, Han La Poutre. Error-
spiking
15. Gerstner,W., Kistler,W., Spiking Neuron Models.Cambridge Uni-
versity Press,England (2002)
16. Jie Yang, Wenyu Yang, Wei Wu. A remark on the error-
backpropagation learning algorithm for spiking. Applied Mathemat-
ics Letters,25,1118-1120(2012)
17. Haibo Bao, Jinde Cao. Delay-distribution-dependent state estima-
tion for discrete-time stochastic neural networks with random delay.
Neural Networks,24,19-28(2011)
18. Mo Y Z, Ding M Z,Yu J M. Stability analysis of nonlinear cellular
neural networks with time-varying delay. J Chongqing Univ Nat Sci
Ed,22,817-822(2010)
19. Li Y B, Wang R L. Stability of Reaction-diffusion Hopﬁeld Neural
Networks with S-type Distributed Delays. J Harbin Univ Nat Sci Ed,
15,63-66(2010)
20. Jie Yang, Wenyu Yang, Wei Wu. A novel spiking perceptron that
can solve XOR problem. Neural Network world,21,45-50(2011)
21. Fisher, R.A. The Use of Multiple Measurements in Taxonomic
Problems. Annals of Eugenics,7,179-188(1936)
22. W.H. Wolberg, Cancer dataset obtained from Williams H. Wolberg
from the University of Wisconsin Hospitals, Madison, 1991.
23. D. Michie, D.J. Spiegelhalter, C.C. Taylor (Eds.), Machine Learn-
ing, Neural and Statistical Classiﬁcation, Ellis Horwood, West Sus-
sex, 1994.
