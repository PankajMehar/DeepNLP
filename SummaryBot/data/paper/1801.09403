
In the last decade, deep learning has achieved remarkable
results in computer vision, speech recognition and natural
language processing, obtaining in some tasks human-like [1]
or even super-human [2] performance. The roots of recent
successes of deep learning can be found in: (i) the increase
of the data available for training neural networks, (ii) the
rising of commodity computational power needed to crunch
the data, (iii) the development of new techniques, archi-
tectures, and activation functions that improve convergence
during training of deeper networks, overcoming the obstacle
of vanishing/exploding gradient [3], [4].
For many years, neural networks have usually employed
logistic sigmoid activation functions. Unfortunately, this acti-
vation is affected by saturation issues. This problem reduces
its effectiveness and, nowadays,
its usage in feedforward
networks is discouraged [5]. To overcome such weakness and
improve accuracy results, an active area of research has been
devoted to design novel activation functions.
The training procedure of these architectures usually in-
volve optimization of the weights of their layers only, while
non-linearities are generally pre-speciﬁed and their (possible)
parameters are usually considered as hyper-parameters to be
tuned manually. In this paper, we introduce two approaches
able to automatically learn combinations of base activation
functions (such as: the identity function, ReLU, and tanh)
during training; our aim is to identify a search space for
the activation functions by means of convex combination or
afﬁne combination of the base functions. To the best of our
knowledge, this is one of the ﬁrst attempts to automatically
combine and optimize activation functions during the training
phase.
We tested different well-known networks employing cus-
tomary activation functions on three standard datasets and we
compared their results with those obtained applying our novel
approaches. The techniques proposed in this paper outper-
formed the baselines in all the experiments, even when using
deep architectures: we found a 3.01 percentage points increase
in the top-1 accuracy for AlexNet on ILSVRC-2012.
This paper is organized as follows: in Section II the related
works are summarized; in Section III the proposed methods
are described; in Section IV the experimental results are pre-
sented; ﬁnally, conclusions and future works are summarized
in Section V.
II. RELATED WORK
Designing activation functions that enable fast training of
accurate deep neural networks is an active area of research.
The rectiﬁed linear activation function, introduced in [6] and
argued in [7] to be a better biological model than logistic
sigmoid activation function, has eased the training of deep
neural networks by alleviating the problems related to weight
initialization and vanishing gradient. Slight variations of ReLU
have been proposed over the years, such as leaky ReLU
(LReLU), [8], which addresses dead neuron issues in ReLU
networks, thresholded ReLU [9], which tackles the problem of
large negative biases in autoencoders, and parametric ReLU
(PReLU) [10], which treats the leakage parameter of LReLU
as a per-ﬁlter learnable weight.
A smooth version of ReLU, called softplus, has been
proposed in [11]. Despite some theoretical advantages over
ReLU (it is differentiable everywhere and it has less saturation
issues), this activation function usually does not achieve better
results [7] when compared to its basic version.
More recently, maxout has been introduced in [12] as an
activation function aimed at enhancing dropouts abilities as a
model averaging technique. Among its extensions, it is worth
mentioning the probabilistic maxout [13], and the Lp norm
pooling activation [14] that is able to recover the maxout
activation when p → ∞.
Considering the last developments of activation functions
in neural networks, it is important to mention the exponential
linear unit function (ELU) [15] and the scaled exponential
linear unit function (SELU) [16]. Like ReLU, LReLU, and
PReLU, ELU reduces the vanishing gradient problem. Fur-
thermore, ELU has negative values, allowing to push mean
unit activations closer to zero like batch normalization, and
speeding up the learning. SELU extends this property ensuring
that activations close to zero mean and unit variance that
are propagated through many network layers will converge
towards zero mean and unit variance, even under the presence
of noise and perturbations.
With the exception of PReLU, all previous activations
are pre-speciﬁed (i.e. non-learnable). A ﬁrst attempt to learn
activations in a neural network can be found in [17], where
the authors propose to randomly add or remove logistic or
Gaussian activation functions using an evolutionary program-
ming method. On the other hand, in [18]–[21] the authors
proposed novel approaches to learn the best activation function
per neuron among a pool of allowed activations by means of
genetic and evolutionary algorithms.
A different method has been proposed in [22], which is
able to learn the hardness parameter of a sigmoid function,
similarly to the approach employed by PReLU to learn the
leakage parameter.
However, all the previous learning techniques are limited by
the fact that the family of functions over which the learning
takes place is either ﬁnite or a simple parameterization of
customary activation functions.
Recently, in [23] the authors tackle the problem from a
different angle using piecewise linear activation functions that
are learned independently for each neuron using gradient
descent. However, (i) the number of linear pieces is treated as
a hyper-parameter of the optimization problem; (ii) the number
of learned parameters increases proportionally to the amount
of hidden units; (iii) all the learned piecewise linear activation
functions are ReLU(x) for x large enough (i.e. there exists
u ∈ R such that g(x) = ReLU(x) for x ≥ u), thus reducing
the expressivity of the learned activation functions by design.
It is worth mentioning that, in [24] the authors propose the
network in network approach where they replace activation
functions in convolutional layers with small multi-layer per-
ceptrons.
In this paper we try to overcome some of the limitations
of the aforementioned approaches. Indeed, the two techniques
explained in Section III: (i) increase the expressivity power of
the learned activation functions by enlarging the hypothesis
space explored during training with respect
to [17]–[22];
(ii) restrict the hypothesis space with respect to [23], [24], in
order to allow a faster training without the need of a careful
initialization of network weights (see Proposition 1 and the
following lines).
III. METHODS
A neural network Nd made of d hidden layers can be seen
as the functional composition of d functions Li followed by a
ﬁnal mapping ¯L that depends on the task at hand (e.g. classi-
ﬁcation, regression): Nd = ¯L◦ Ld◦ . . .◦ L1. In particular, each
hidden layer function Li can be written as the composition of
two functions, gi followed by σi, the former being a suitable
remapping of the layer input neurons, the latter being the
activation function of the layer: Li = σi ◦ gi. In the most
general case, both σi and gi are parameterized and belongs to
some hypothesis spaces Hσi and Hgi, respectively. Hence, the
learning procedure of Li amounts to an optimization problem
over the layer hypothesis space Hi = Hσi × Hgi.
Usually, σi is taken as a non-learnable function; therefore, in
the most common scenario Hσi is a singleton: Hi = {σi} ×
Hgi. For example, for a fully-connected layer from Rni to
Rmi with ReLU activation we have that Hgi is the set of all
afﬁne transformations from Rni to Rmi, i.e. Hi = {ReLU}×
Lin(Rni, Rmi) × K(Rmi), where Lin(A, B) and K(B) are
the sets of linear maps between A and B, and the set of
translations of B, respectively.
In this paper, we introduce two techniques to deﬁne learn-
able activation functions that could be plugged in all hidden
layers of a neural network architecture. The two approaches
differ in how they deﬁne the hypothesis space Hσi. Both of
them are based on the following idea: (i) select a ﬁnite set of
activation functions F := {f1, . . . , fN}, whose elements will
be used as base elements; (ii) deﬁne the learnable activation
function σi as a linear combination of the elements of F;
(iii) identify a suitable hypothesis space Hσi; (iv) optimize
the whole network, where the hypothesis space of each hidden
layer is Hi = Hσi × Hgi.
We will now give some basic deﬁnitions used throughout
the paper. Note that, hereinafter, all activation functions from
R to R will naturally extend to functions from Rn to Rn by
means of entrywise application.
Given a vector space V and a ﬁnite subset A ⊆ V , we can
deﬁne the following two subsets of V :
(i) the convex hull of A, namely:
conv(A) := {(cid:80)
aﬀ(A) := {(cid:80)
i ciai |(cid:80)
i ciai |(cid:80)
(ii) the afﬁne hull of A, namely:
i ci = 1, ci ≥ 0, ai ∈ A};
i ci = 1, ai ∈ A}.
We remark that, neither conv(A) nor aﬀ(A) are vector sub-
spaces of V . Indeed, conv(A) is just a generic convex subset
in V reducing to a (|A| − 1)-dimensional simplex whenever
the elements of A are linearly independent. On the other hand,
aﬀ(A) is an afﬁne subspace of V of dimension |A| − 1, i.e.
for an arbitrary ¯a ∈ aﬀ(A) the set {a − ¯a | a ∈ aﬀ(A)}
is a linear subspace of V of dimension |A| − 1. Clearly,
conv(A) ⊂ aﬀ(A).
Let F := {f0, f1, . . . , fN} be a ﬁnite collection of activa-
tion functions fi from R to R. We can deﬁne a vector space
i cifi. Note
that, despite F is (by deﬁnition) a spanning set of F , it is not
generally a basis; indeed |F| ≥ dim F .
Since (almost everywhere) differentiability is a property
preserved by ﬁnite linear combinations, and since conv(F ) ⊂
aﬀ(F ) ⊆ F , assuming that F contains (almost everywhere)
differentiable activation functions, conv(F ) and aﬀ(F ) are
made of (almost everywhere) differentiable functions, i.e. valid
F from F by taking all linear combinations (cid:80)
implies that ¯f =(cid:80)
i cifi with(cid:80)
(⇐) By hypothesis, ¯f = (cid:80)
¯f (0) =(cid:80)
i cifi with (cid:80)
i cifi(0) = 0 and ¯f(cid:48)(0) =(cid:80)
i ci = 1, namely ¯f ∈ aﬀ(F ).
i ci = 1. Hence,
i ci = 1,
i (0) =(cid:80)
i cif(cid:48)
namely ¯f approximates the identity near origin.
Since conv(F ) ⊂ aﬀ(F ), also conv(F ) enjoys the same
property. It is important to underline that, activation functions
approximating the identity near origin, have been argued to
be desirable to train deep networks randomly initialized with
weights near zero [26]. Indeed, for such an activation function
the initial gradients will be relatively large, thus speeding up
the training phase. Note that, such behavior is enjoyed also
by ReLU, which approximates the identity only from one
side: i.e. x → 0+. Furthermore, training deep networks with
activation functions not approximating the identity near origin
requires a more careful initialization, in order to properly scale
the layers input [27].
For the aforementioned reasons: (i) preserved differen-
tiability; (ii) absence of the requirement of monotonicity;
(iii) approximation of the identity near origin; both aﬀ(F )
and conv(F ) are good candidates for Hσi.
Thanks to the previous deﬁnitions and the aforementioned
properties, we can now formalize the two techniques to build
learnable activation functions as follows:
(i) choose a ﬁnite set F = {f1, . . . , fN}, where each fi is
a (almost everywhere) differentiable activation function
approximating the identity near origin (at least from one
side);
of all the fi ∈ F;
conv(F ).
(ii) deﬁne a new activation function ¯f as a linear combination
the sets aﬀ(F ) or
(iii) select as hypothesis space H ¯f
In Section IV, we present results using the following choices
for F:
F := {id, tanh},
Since conv(F ) ⊂ aﬀ(F ),
F := {id, ReLU},
F := {ReLU, tanh}, F := {id, ReLU, tanh},
(1)
where id is the identity function. Clearly, other choices of F
may be available, provided the requirements in (i) are satisﬁed.
the convex hull-based tech-
nique can to be understood as a regularized version of the
afﬁne hull-based one, where the corresponding hypothesis
space has been explicitly constrained to be compact. Such
a regularization, in addition to restrict the complexity of the
hypothesis space, guarantees that the ﬁnal activation function
is monotonic (provided all fi ∈ F are monotonic as well).
Moreover, the convex hull-based technique together with F :=
{id, ReLU,} recovers the learnable LReLU activation func-
tion, i.e. LReLUα(x) = x if x ≥ 0, while LReLUα(x) = αx
otherwise, where 0 ≤ α (cid:28) 1 (usually α = 10-2). Indeed,
conv(F ) = { ¯f := p · id +(1 − p) · ReLU with 0 ≤ p ≤ 1}
and since ReLU = id for x ≥ 0 and ReLU = 0 otherwise,
we have that ¯f = p · id +(1 − p) · id = id for x ≥ 0 and
¯f = p · id +(1 − p) · 0 = p · id otherwise, i.e. LReLUp.
It is worth mentioning that, as shown in Figure 2, layers
using convex hull-based and afﬁne hull-based activations with
Fig. 1. The ﬁgure shows the relationship between afﬁne hull, convex hull,
and convex cone for a set F made of three linearly independent elements:
F := {f1, f2, f3}. Since dim F = |F| = 3, conv(F ) is a 2-simplex
(the gray triangle in the ﬁgure), while aﬀ(F ) is a plane within the three
dimensional vector space F (the plane identiﬁed by the dashed circle in the
ﬁgure). cone(F ) is the three dimensional manifold delimited by the three
incident straight lines li := {x ∈ F | x = αfi, ∀0 ≤ α ∈ R} for i = 1, 2, 3,
i.e. the cone extremal rays. It can easily be seen that conv(F ) corresponds
to the intersection of cone(F ) with aﬀ(F ).
activation functions for a neural network that can be learned
by means of gradient descent.
The activations used in real world scenarios are usually
monotonic increasing functions. Unfortunately, the monotonic-
ity is not ensured under arbitrary linear combination, meaning
that even if all fi ∈ F are non-decreasing functions, an
arbitrary element ¯f ∈ F might be neither a non-decreasing nor
a non-increasing function. As a matter of fact, considering only
i ≥ 0 ∀fi ∈ F),
non-decreasing differentiable functions (f(cid:48)
all non-decreasing differentiable functions in F lie inside the
convex cone cone(F ) ⊂ F , i.e.:
cone(F ) := {(cid:80)
i cifi | ci ≥ 0, fi ∈ F}.
Indeed, ∀g ∈ cone(F ) we have that g(cid:48) ≥ 0. Thanks to
the deﬁnition of aﬀ(F ), cone(F ), and conv(F ), we can
conclude that conv(F ) = cone(F ) ∩ aﬀ(F ), which implies
that monotonicity of the elements of F is preserved by all
the elements of conv(F ) but not by aﬀ(F ) (see Figure 1).
Nevertheless, in [25] it is shown that even non-monotonic
activation functions can approximate arbitrarily complex func-
tions for sufﬁciently large neural networks. Indeed, in [5] the
authors trained a feedforward network using cosine activation
functions on the MNIST dataset obtaining an error rate smaller
than 1%. Therefore, also aﬀ(F ) is a proper candidate for Hσi.
In addition, the afﬁne subspace aﬀ(F ) enjoys the following
property.
Proposition 1. Let all fi ∈ F be linearly independent and
approximate the identity function near origin (i.e. fi(0) = 0
i (0) = 1), then, ¯f ∈ F approximates the identity if and
and f(cid:48)
only if ¯f ∈ aﬀ(F ).
Proof. The proof is immediate. Let us expand F (cid:51) ¯f as ¯f =
i cifi (by hypothesis the fi form a basis) and prove the
two-way implication.
(⇒) By hypothesis, ¯f (0) = 0 and ¯f(cid:48)(0) = 1. The relation
i ci = 1. In turns, this
(cid:80)
on the derivative reads(cid:80)
i (0) =(cid:80)
i cif(cid:48)
TABLE I
KE R A SNE T ARCHITECTURE
Id
conv_1
conv_2
conv_3
conv_4
fc_5
fc_6
Layers
2D convolution
Activation
2D convolution
Activation
Max pooling
Dropout
2D convolution
Activation
2D convolution
Activation
Max pooling
Dropout
Fully connected
Activation
Dropout
Fully connected
Activation
Properties
32 × (3, 3)
32 × (3, 3)
(2, 2)
25%
64 × (3, 3)
64 × (3, 3)
(2, 2)
25%
512
50%
10
softmax
pixel value by 255, and we augmented the resulting dataset
during training phase by means of random horizontal ﬂip and
image shifting;
ILSVRC-2012: a 1k classes classiﬁcation task with 1.2M
training examples and 50k validation examples. The examples
are colour images of various sizes [30]. We resized each
image to 256x256 pixels, we subtracted pixel mean values, and
we augmented the dataset during training phase by randomly
cropping to 227x277 pixels and horizontally ﬂipping the
images. Note that, we did not train our network with the
relighting data-augmentation proposed in [31].
The considered architectures are the following:
LeNet-5: a convolutional network made of two convolu-
tional layers followed by two fully connected layers [32].
The convolutional layers have respectively 20 and 50 ﬁlters
of size 5 × 5, while the hidden fully connected layer is made
of 500 neurons. Max pooling with size 2×2 is used after each
convolutional layer, without dropout. We assessed LeNet-5
on Fashion-MNIST and CIFAR-10 datasets, resulting in
two networks with 431k and 657k parameters, respectively;
KerasNet: a convolutional neural network included in the
Keras framework [33]. It is made of 4 convolutional layers
and 2 fully connected layers, and it employs both max
pooling and dropout. This architecture is presented in Ta-
ble I. We tested KerasNet on both Fashion-MNIST and
CIFAR-10 datasets, resulting in two networks with 890k and
1.2M parameters, respectively;
ResNet-56: a residual network made of 56 total layers,
employing pooling and skip connection [34]. Its performance
has been evaluated on CIFAR-10, corresponding to a network
with 858k parameters;
AlexNet: a convolutional network made of 5 convolutional
and 3 fully connected layers [31]. We tested it against
ILSVRC-2012 dataset, resulting in a network with 61M pa-
rameters. Note that, as shown in [31], ReLU-based activation
functions signiﬁcantly outperform networks based on other
activations. Therefore, in this context ReLU-based networks
Fig. 2. The ﬁgure shows how a neural network layer with convex/afﬁne hull-
based activation can be seen as a two-stage pipeline. Here the k-th layer is
a two neurons layer, with set F composed of two base functions. The ﬁrst
stage of the pipeline is made of two stacked layers, each one featuring only
one activation belonging to F. The weights between the two stacked layers
are shared. The second stage of the pipeline is a 1D-convolution with kernel
of size 1 between the stacked layers, which play the role of n channels for
the convolution. The weights of the convolution are constrained to sum to
one, and also to be positive when using the convex hull-based technique.
n base functions can also be seen as the following two-stage
pipeline: (i) n stacked fully connected layers, each of them
featuring one of the base functions, and all of them sharing
weights; (ii) a 1D-convolution with kernel of size 1 between
the n stacked layers (which are treated by the convolution as
n separate channels), whose weights are constrained to sum
to one (and to be positive in case of the convex hull-based
technique)1.
IV. RESULTS
We tested the convex hull-based and the afﬁne hull-based
approaches by evaluating their effectiveness on three pub-
licly available datasets used for image classiﬁcation, greatly
differing in the number of input features and examples.
Moreover, each dataset was evaluated using different network
architectures. These networks were trained and tested using as
activation functions (for all their hidden layers) those learned
by the convex hull-based and the afﬁne hull-based approaches
combining the base activations reported in Equation (1). In
addition, the base activation functions alone and LReLU were
also employed in order to compare the overall performance.
Speciﬁcally, the datasets we used are:
Fashion-MNIST: it is a dataset of Zalando’s article im-
ages, composed by a training and test sets of 60k and 10k
examples, respectively. Each example is a 28x28 grayscale
image, associated with a label that belongs to 10 classes [28].
We divided each pixel value by 255, and we augmented the
resulting dataset during training phase by means of random
horizontal ﬂip and image shifting;
CIFAR-10: it is a balanced dataset of 60k 32x32 colour
images belonging to 10 classes. There are 50k training images
and 10k test images [29]. Also in this case we divided each
1The 1D-convolution with kernel of size 1 can also be seen as a weighted
average of the stacked fully connected layers (with possibly negative weights
in case of the afﬁne hull-based technique).
Fig. 3. The ﬁgure shows the hidden afﬁne hull-based and convex hull-based activation functions learned during the training of the KerasNet architecture
on CIFAR-10 dataset. Activations learned on aﬀ({id, ReLU, tanh}) are represented with solid lines, while those learned on conv({id, ReLU, tanh}) are
dashed.
only have been built and tested for comparison.
All networks were implemented from scratch using the Keras
framework [33] on top of TensorFlow [35]. Notice that,
our AlexNet implementation is a porting to Keras of the
Caffe architecture2. We trained LeNet-5, KerasNet, and
ResNet-56 using RMSprop with learning rate 10-4 and
learning rate decay over each mini-batch update of 10-6. For
AlexNet we used SGD with starting learning rate 10-2, step-
wise learning rate decay, weight-decay hyper-parameter 5·10-4,
and momentum 0.9.
Table II shows the top-1 accuracy for all the run exper-
iments. The best conﬁgurations (shaded cells in the table)
are always achieved using our techniques, where in 5 out
of 6 experiments the afﬁne hull-based approach outperformed
convex hull-based ones. The uplift in top-1 accuracy using our
approaches compared to customary activations goes from 0.69
percentage points (pp) for LeNet-5 on Fashion-MNIST
up to 4.59 pp for KerasNet on CIFAR-10. It is worth
mentioning that, even in deep neural networks, such as
AlexNet, a substantial increase is observed, i.e. 3.01 pp on
ILSVRC-2012.
Moreover,
the proposed techniques usually achieve bet-
ter results than their corresponding base activation functions
(boldface in the table). Note that, the novel activations work
well for very deep networks and also with various architectures
involving different types of layer (e.g. residual unit, dropout,
pooling, convolutional, and fully connected).
Furthermore, our experiments show how the learning of
the leakage parameter achieved by the activation based on
conv({id, ReLU}) allows to outperform or to achieve the
same results of LReLU.
Figure 3 shows activations learned by KerasNet on
CIFAR-10 when using convex hull-based and afﬁne hull-
based activations with F = {id, ReLU, tanh}. It is possible
to notice that, as already theoretically proved in Section III,
convex combinations preserved monotonicity of the base ac-
tivation functions while afﬁne ones did not.
2See https://github.com/BVLC/caffe/tree/master/models/bvlc alexnet.
V. CONCLUSION
In this paper we introduced two novel techniques able to
learn new activations starting from a ﬁnite collection F of base
functions. Both our ideas are based on building an arbitrary
linear combination of the elements of F and on deﬁning a
suitable hypothesis space where the learning procedure of the
linear combination takes place. The hypothesis spaces for the
two techniques are conv(F ) and aﬀ(F ). We showed that,
provided all the elements of F approximate the identity near
origin, aﬀ(F ) is the only set where it is possible to ﬁnd
combined activations that also approximate id near origin.
Moreover, aﬀ(F ) allows to explore non-monotonic activation
functions, while conv({id, ReLU}) may be seen as a learnable
LReLU activation function.
We tested the two techniques on various architec-
tures (LeNet-5, KerasNet, ResNet-56, AlexNet) and
datasets (Fashion-MNIST, CIFAR-10, ILSVRC-2012),
comparing results with LReLU and single base activation
functions.
In all our experiments, the techniques proposed in this paper
achieved the best performance and the combined activation
functions learned using our approaches usually outperform
the corresponding base components. The effectiveness of the
proposed techniques is further proved by the increase in
performance achieved using networks with different depths
and architectures.
be to analyze other sets (F) of base functions.
In our opinion, an interesting extension of this work would
REFERENCES
[1] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke,
D. Yu, and G. Zweig, “Achieving human parity in conversational speech
recognition,” arXiv preprint arXiv:1610.05256, 2016.
[2] D. C. Ciresan, U. Meier, J. Masci, and J. Schmidhuber, “A committee of
IEEE, 2011,
neural networks for trafﬁc sign classiﬁcation.” in IJCNN.
pp. 1918–1921.
[3] Y. Bengio, P. Simard, and P. Frasconi, “Learning long-term dependencies
with gradient descent is difﬁcult,” IEEE transactions on neural networks,
vol. 5, no. 2, pp. 157–166, 1994.
[4] X. Glorot and Y. Bengio, “Understanding the difﬁculty of training
deep feedforward neural networks,” in Proceedings of the Thirteenth
International Conference on Artiﬁcial Intelligence and Statistics, 2010,
pp. 249–256.
[5] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT Press,
2016.
EXPERIMENT RESULTS. THE TABLE SHOWS THE TOP-1 ACCURACY RESULTS FOR ALL THE ANALYZED NETWORKS ON FA S H I O N-MNIST TEST SET,
CIFAR-10 TEST SET, AND ILSVRC-2012 VALIDATION SET. CONVEX HULL-BASED AND AFFINE HULL-BASED ACTIVATIONS ACHIEVING TOP-1
ACCURACY RESULTS GREATER THAN THEIR CORRESPONDING BASE ACTIVATION FUNCTIONS ARE HIGHLIGHTED IN BOLDFACE. THE BEST RESULT FOR
EACH NETWORK/DATASET IS SHADED.
TABLE II
Activation
id
ReLU
tanh
LReLU
conv({id, ReLU})
conv({id, tanh})
conv({tanh, ReLU})
conv({id, ReLU, tanh})
aﬀ({id, ReLU})
aﬀ({id, tanh})
aﬀ({tanh, ReLU})
aﬀ({id, ReLU, tanh})
Fashion-MNIST
CIFAR-10
ILSVRC-2012
LeNet-5
KerasNet
LeNet-5
KerasNet
ResNet-56
AlexNet
90.50%
91.06%
92.33%
91.03%
91.87%
92.36%
92.56%
92.21%
92.83%
92.65%
93.02%
92.80%
90.51%
90.79%
93.43%
91.13%
92.39%
93.64%
92.04%
92.94%
93.37%
94.41%
93.48%
94.41%
75.27%
80.37%
78.96%
80.94%
80.94%
79.45%
80.21%
80.48%
80.52%
78.97%
81.23%
80.13%
75.23%
79.97%
82.86%
80.07%
84.74%
78.53%
84.80%
85.21%
84.93%
86.05%
84.83%
87.45%
42.60%
89.18%
82.65%
89.38%
90.51%
86.46%
88.31%
89.60%
88.92%
82.97%
89.44%
88.62%
56.75%
57.03%
57.54%
58.13%
56.55%
58.48%
60.04%
57.20%
[6] K. Jarrett, K. Kavukcuoglu, Y. LeCun et al., “What is the best multi-
stage architecture for object recognition?” in Computer Vision, 2009
IEEE 12th International Conference on.
IEEE, 2009, pp. 2146–2153.
[7] X. Glorot, A. Bordes, and Y. Bengio, “Deep sparse rectiﬁer neural
networks,” in Proceedings of the Fourteenth International Conference
on Artiﬁcial Intelligence and Statistics, 2011, pp. 315–323.
[8] A. L. Maas, A. Y. Hannun, and A. Y. Ng, “Rectiﬁer Nonlinearities
Improve Neural Network Acoustic Models,” in ICML Workshop on Deep
Learning for Audio, Speech and Language Processing, 2013.
[9] K. Konda, R. Memisevic, and D. Krueger, “Zero-bias autoencoders and
the beneﬁts of co-adapting features,” arXiv preprint arXiv:1402.3337,
2014.
[10] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectiﬁers:
Surpassing human-level performance on imagenet classiﬁcation,”
in Proceedings of
the 2015 IEEE International Conference on
Computer Vision (ICCV), ser. ICCV ’15. Washington, DC, USA:
IEEE Computer Society, 2015, pp. 1026–1034. [Online]. Available:
http://dx.doi.org/10.1109/ICCV.2015.123
[11] C. Dugas, Y. Bengio, F. B´elisle, C. Nadeau, and R. Garcia, “Incorpo-
rating second-order functional knowledge for better option pricing,” in
Advances in neural information processing systems, 2001, pp. 472–478.
[12] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Ben-
gio, “Maxout networks,” in Proceedings of
the 30th International
Conference on International Conference on Machine Learning-Volume
28.
JMLR. org, 2013, pp. III–1319.
[13] W. Sun, F. Su, and L. Wang, “Improving deep neural networks with
multilayer maxout networks,” 2014 IEEE Visual Communications and
Image Processing Conference, VCIP 2014, pp. 334–337, dec 2015.
[14] C. Gulcehre, K. Cho, R. Pascanu, and Y. Bengio, “Learned-norm
pooling for deep feedforward and recurrent neural networks,” in Joint
European Conference on Machine Learning and Knowledge Discovery
in Databases. Springer, 2014, pp. 530–546.
[15] D.-A. Clevert, T. Unterthiner, and S. Hochreiter, “Fast and Accurate
Deep Network Learning by Exponential Linear Units (ELUs),” Pro-
ceedings of ICLR 2016, nov 2016.
[16] G. Klambauer, T. Unterthiner, A. Mayr, and S. Hochreiter, “Self-
normalizing neural networks,” in Advances in neural information pro-
cessing systems, 2017.
[17] Y. Liu and X. Yao, “Evolutionary design of artiﬁcial neural networks
with different nodes,” in Evolutionary Computation, 1996., Proceedings
of IEEE International Conference on.
IEEE, 1996, pp. 670–675.
[18] R. Poli, “Parallel distributed genetic programming, new ideas in opti-
mization,” 1999.
[19] D. Weingaertner, V. K. Tatai, R. R. Gudwin, and F. J. Von Zuben,
“Hierarchical evolution of heterogeneous neural networks,” in Evolu-
tionary Computation, 2002. CEC’02. Proceedings of the 2002 Congress
on, vol. 2.
IEEE, 2002, pp. 1775–1780.
[20] A. J. Turner and J. F. Miller, “Cartesian genetic programming encoded
artiﬁcial neural networks: a comparison using three benchmarks,” in
Proceedings of the 15th annual conference on Genetic and evolutionary
computation. ACM, 2013, pp. 1005–1012.
[21] M. M. Khan, A. M. Ahmad, G. M. Khan, and J. F. Miller, “Fast learning
neural networks using cartesian genetic programming,” Neurocomputing,
vol. 121, pp. 274–289, 2013.
[22] A. J. Turner and J. F. Miller, “Neuroevolution: evolving heterogeneous
artiﬁcial neural networks,” Evolutionary Intelligence, vol. 7, no. 3, pp.
135–154, 2014.
[23] F. Agostinelli, M. Hoffman, P. Sadowski, and P. Baldi, “Learning acti-
vation functions to improve deep neural networks,” in ICLR Workshop,
2015.
[24] M. Lin, Q. Chen, and S. Yan, “Network in network,” arXiv preprint
arXiv:1312.4400, 2013.
[25] G. Cybenko, “Approximation by superpositions of a sigmoidal function,”
Mathematics of Control, Signals, and Systems (MCSS), vol. 2, no. 4, pp.
303–314, 1989.
[26] H. H. Aghdam and E. J. Heravi, Guide to Convolutional Neural
Networks. Springer, 2017.
[27] D. Sussillo and L. Abbott, “Random walk initialization for training very
deep feedforward networks,” arXiv preprint arXiv:1412.6558, 2014.
[28] H. Xiao, K. Rasul, and R. Vollgraf. (2017) Fashion-MNIST: a novel
image dataset for benchmarking machine learning algorithms.
[29] A. Krizhevsky and G. Hinton, “Learning multiple layers of features from
tiny images,” Technical report, University of Toronto, Tech. Rep., 2009.
[30] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and
L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,”
International Journal of Computer Vision (IJCV), vol. 115, no. 3, pp.
211–252, 2015.
[31] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in Advances in neural infor-
mation processing systems, 2012, pp. 1097–1105.
[32] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” Proceedings of the IEEE, vol. 86,
no. 11, pp. 2278–2324, 1998.
[33] F. Chollet et al., “Keras,” https://github.com/fchollet/keras, 2015.
[34] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.
Irving, M.
[35] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.
Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow,
A. Harp, G.
Isard, Y. Jia, R. Jozefowicz, L. Kaiser,
M. Kudlur, J. Levenberg, D. Man´e, R. Monga, S. Moore, D. Murray,
C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar,
P. Tucker, V. Vanhoucke, V. Vasudevan, F. Vi´egas, O. Vinyals,
P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng,
“TensorFlow: Large-scale machine learning on heterogeneous systems,”
2015, software available from tensorﬂow.org.
[Online]. Available:
https://www.tensorﬂow.org/
