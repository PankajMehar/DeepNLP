
The massive development of connected devices and highly
precise instruments has introduced the world to vast volumes
of high-dimensional data. Traditional data analytics cannot
cope with these massive amounts, which motivates well inves-
tigating dimensionality reduction schemes capable of gleaning
out efﬁciently low-dimensional information from large-scale
datasets. Dimensionality reduction is a vital ﬁrst step to render
tractable critical learning tasks, such as large-scale regression,
classiﬁcation, and clustering, and allows for processing of
datasets that might otherwise not be tractable.
[16],
[10],
Dimensionality reduction methods have been extensively
studied by the signal processing and machine learning
communities [2],
[17]. Principal component
analysis (PCA) [10] is the ‘workhorse’ method yielding
low-dimensional representations that preserve most of the
high-dimensional data variance. Multi-dimensional scaling
(MDS) [12] on the other hand, maintains pairwise distances
between data when going from high- to low-dimensional
spaces, while local linear embedding (LLE) [16] only pre-
serves relationships between neighboring data. Information
from non-neighboring data is lost in LLE’s low-dimensional
representation, which may in turn inﬂuence the performance
of ensuing tasks such as classiﬁcation or clustering [6], [21]. It
is also worth stressing that all aforementioned approaches cap-
ture and preserve linear relationships between data. However,
for data residing on highly nonlinear manifolds using only lin-
ear relations might produce low-dimensional representations
that are not accurate. Generalizing PCA, Kernel PCA [9] can
capture nonlinear relationships between data, for a preselected
Work in this paper was
supported by NSF 1500713, and NIH
1R01GM104975-01.
kernel function. In addition, Laplacian eigenmaps [2] preserve
nonlinear similarities between neighboring data.
While all the aforementioned approaches have been suc-
cessful in reducing the dimensionality of various types of
data, they do not consider additional information during the
dimensionality reduction process. This prior information may
be task speciﬁc, e.g. provided by some “expert” or the physics
of the problem, or it could be inferred from alternate views of
the data, and can provide additional insights for the desired
properties of the low-dimensional representations. In fMRI
signals for instance, in addition to time series collected at
different brain regions, one may also have access to the
connectivity patterns among these regions. As shown in [8],
[9], [18], [19] for PCA, this additional information can be
encoded in a graph, and incorporated into the dimensionality
reduction process through graph-aware regularization.
The present paper puts forth a novel framework for di-
mensionality reduction that can capture nonlinear relations
between data, and also exploit additional information through
graph regularization. This framework encompasses all afore-
mentioned approaches, and markedly broadens their scope.
II. PRELIMINARIES AND PROBLEM STATEMENT
Consider a dataset with N vectors of dimension D col-
lected as columns of the matrix X := [x1, . . . , xN ]. Without
loss of generality, it will be assumed that the sample mean
N −1 PN
n=1 xn has been removed from each xi. Dimensional-
ity reduction looks for a set of d-dimensional vectors {yi}N
i=1,
with d < D, that preserve certain properties of {xi}. MDS
for instance aims to preserve pairwise distances among {xi}
when obtaining the corresponding low-dimensional represen-
tations {yi}, while LLE attempts to preserve neighborhoods.
As will be shown in the ensuing subsections, all these dimen-
sionality reduction schemes are special cases of kernel-based
PCA.
A. Principal component analysis
Given X, PCA ﬁnds a linear subspace of dimension d so
that hopefully all the data lie on or close to it (in the least-
squares sense). Speciﬁcally, PCA solves
min
Ud,{yi}
i=1
kxi − Udyik2
s. to U⊤
d Ud = I
(1)
where Ud ∈ RD×d is an orthonormal matrix. The optimal
solution of (1) is yi = U⊤
d xi, where Ud is formed by the
eigenvectors of XX⊤ = UΣU⊤ corresponding to the d
largest eigenvalues [7]. For future use, consider the singular
value decomposition (SVD) X = UΣV⊤. Given {yi}, the
original vectors can be recovered as xi = Udyi. PCA
thrives when data lie close to a d-dimensional hyperplane. Its
complexity is that of eigendecomposing XX⊤, i.e., O(N D2),
which means PCA is more affordable when D ≪ N . In
contrast, dimensionality reduction of small sets of high-
dimensional vectors (D ≫ N ) becomes more tractable with
dual PCA.
B. Dual PCA and Kernel PCA
The SVD of Y implies that Ud = YVΣ−1, which in
d yi =
turn yields the low-dimensional vectors as ψi = U⊤
Σ−1
d Y⊤yi. Collecting Ψ := [ψ1, . . . , ψN ], we have
d V⊤
Y = U⊤
d X = ΣdV⊤
(2)
where Σd ∈ Rd×d is a diagonal matrix containing the
d leading eigenvalues of X⊤X, and Vd ∈ RN ×d is the
submatrix of V collecting the corresponding eigenvectors of
X⊤X. The complexity of dual PCA is O(DN 2); therefore,
it is preferable when D ≫ N . Moreover, it can be readily
veriﬁed that besides (1), Y in (2) is also the optimal solution
to the following optimization problem
min
kKx − Y⊤Yk2
s. to YY⊤ = Λd
(3)
where Kx := X⊤X, and Λd denotes a d × d diagonal matrix
containing the d largest eigenvalues of Kx. Compared to
PCA, dual PCA needs only the inner products {x⊤
i xj } in
order to obtain the low-dimensional representations, but not
X itself. Hence, dual PCA can yield low-dimensional vectors
{yi} of general (non-metric) objects that are not necessarily
expressed using vectors {xi}, so long as inner products (a.k.a
correlations) of the latter are known. Furthermore, expanding
the cost in (3), we can equivalently express it as
tr(YK
−1
⊤).
(4)
min
−tr(YKxY
⊤) ⇔ min
Y:YY⊤=Λd
Y:YY⊤=Λd
While PCA performs well for data that lie close to a hyper-
plane, this property might not be true for many datasets [9].
In such cases one may resort to kernel PCA. Kernel PCA
“lifts” {xi} using a nonlinear function φ, onto a higher
(possibly inﬁnite) dimensional space, where the data may lie
on or near a linear hyperplane, and then ﬁnds low-dimensional
representations {yi}. Kernel PCA is obtained by solving (3)
with [Kx]i,j = κ(xi, xj) = φ⊤(xi)φ(xj), where κ(xi, xj)
denotes a prescribed kernel function [5].
C. Local linear embedding
Another popular method that deals with data that cannot be
presumed close to a hyperplane is LLE. It assumes that {xi}
lie on a smooth manifold, which can be locally approximated
by tangential hyperplanes. Speciﬁcally, LLE assumes that
each datum can be expressed as a linear combination of its
neighbors; that is, xi = Pj∈Ni wij xj + ei, where Ni is a
set containing the indices of the nearest neighbors of xi, in
the Euclidean distance sense. In order to solve for {wij }, the
following optimization problem is considered
W = arg min
ˇW
kX − X ˇWk2
s. to ˇwij = 0, ∀i /∈ Nj, X
ˇwij = 1
(5)
where ˇwij denotes the (i, j)-th entry of ˇW. Upon obtaining
W as the constrained least-squares solution in (5), LLE
ﬁnds {yi} that best preserve the neighborhood relationships
encoded in W, by solving
min
Y:Y⊤Y=Λd
⇔ min
Y:Y⊤Y=Λd
kY − YWk2
tr[Y(I − W)(I − W)⊤Y⊤]
(6)
where Λd is a diagonal matrix. Conventional LLE adopts
Λd = I, which is subsumed by the constraint in (6). Nonethe-
less, the difference is just a scaling of {yi} when Λd 6= I.
If the diagonal of Λd collects the d smallest eigenvalues of
matrix (I − W)(I − W)⊤, then (6) is a special case of kernel
PCA with [cf. (4)]
Kx = [(I − W)(I − W)⊤]†
(7)
where † denotes pseudo inverse. Similarly, other popular
dimensionality reduction methods such as MDS and Laplacian
eigenmaps can also be viewed as special cases of kernel PCA,
by appropriately selecting Kx [4]. Thus,
(4) can be viewed
as an encompassing framework for nonlinear dimensionality
reduction. This framework is the foundation of the general
graph-aware methods we develop in the ensuing section.
III. GRAPH-AWARE NONLINEAR DIMENSIONALITY REDUCTION
Matrices Kx for dual PCA, kernel PCA and LLE depend
only on X. However, as mentioned in Sec. I, additional
structural information potentially useful for the dimensionality
reduction task may be available. This knowledge can be en-
coded in a graph and embodied in Y via graph regularization.
Speciﬁcally, suppose there exists a graph G over which the
data is smooth; that is, vectors {xi} on nodes of G are also
close to each other in Euclidean distance. With A denoting
the adjacency matrix of G, we have [A]ij = aij 6= 0 if node
i is connected with node j. The Laplacian of G is deﬁned as
LG := D − A, where D is a diagonal matrix with entries
[D]ii = dii = Pj aij . Now consider
tr(YLG Y⊤) =
i=1
j6=i
aij(yi − yj)2
(8)
which is a weighted sum of the distances of adjacent yi’s on
the graph. By minimizing (8) over Y, the low-dimensional
representations corresponding to adjacent nodes with large
edge weights aij will be close to each other.
A. Kernel PCA on graphs
Introducing (8) as a regularization term to the original
kernel PCA problem in (4), we arrive at
min
Y:YY⊤=Λd
tr(YK−1
x Y⊤) + γtr(YLG Y⊤)
(9)
where γ is a positive scalar, and Λd collects the d smallest
x + γLG = ¯VΛ ¯V⊤. Combining the
eigenvalues of K−1
Laplacian regularization with the kernel PCA formulation, (9)
is capable of ﬁnding {yi} that preserve the “lifted” covariance
captured by Kx, while at
the same time, promoting the
Algorithm 1 Local nonlinear embedding over graphs
Input: X, γ
S1. Estimate W from X.
S2. Obtain kernel matrix Kx via (7).
S3. Find low-dimensional representations Y using (10).
smoothness of the low-dimensional representations over the
graph G. Problem (9) admits a closed-form solution as
Y = Λ1/2
¯V⊤
q=1 θqK(q)
(10)
where ¯Vd denotes the sub-matrix of ¯V containing columns
corresponding to eigenvalues in Λd. When the γ is set to 0,
one obtains the solution of kernel PCA [cf. (2)].
Remark 1: When the kernel function needed to form Kx
is not known a priori, one can use Kx = PQ
in
(9), where {K(q)
x } is a known dictionary of kernel matrices,
and {θq} are estimated as variables in (9) [1]. In addition,
instead of directly using LG, a family of graph kernels
r(LG ) := UGr(Λ)U⊤
G can be employed, where r(.) is a scalar
function of the eigenvectors of LG . By appropriately selecting
r(.), different graph properties can be accounted for. As an
example, when r sets eigenvalues above a certain threshold
to 0, it acts as a sort of “low pass” ﬁlter over the graph. A
detailed discussion of how to choose graph kernels can be
found in [14], [15]. Furthermore, instead of prescribing an
r(LG ), a data-driven dictionary-based approach could also be
employed to learn the proper graph kernel for the task at hand
[15].
Remark 2: Even though only a single graph regularizer is
introduced in (9), our method is ﬂexible to include multiple
graph regularizers based on different graphs. Therefore, the
proposed method offers a powerful tool for dimensionality
reduction on so-called multi-layer graphs, which encode the
relationships among data across multiple graphs [11], [22].
B. Local nonlinear embedding on graphs
Broadening the premise of LLE, we pursue here a more
general dimensionality reduction framework that captures
nonlinear correlations between neighboring data, in addition
to the structure induced by the graph G. To this end, suppose
that each datum can be represented by its neighbors as
[xi]m = X
j∈Ni
where {hij(·)}N
admitting a P th-order expansion
hij(cid:0)[xj]m(cid:1) + [ei]m, m = 1, . . . , D (11)
i,j=1 are prescribed scalar nonlinear functions
hij(z) =
p=1
wij [p]zp
(12)
where coefﬁcients {wij[p]} are to be determined. Taylor’s
expansion asserts that for P sufﬁciently large, (12) offers
an accurate approximation for all memoryless differentiable
nonlinear functions. Such a nonlinear model has been used
for gragh topology identiﬁcation [20]. With the estimated W
at hand, the low-dimensional representations can be obtained
via (10), with Kx as in (7); see also Algorithm 1.
(a)
(b)
(c)
(d)
(e)
(f)
Fig. 1: Embedding results of two manifolds: linear hyperplane
and trefoil (a) visualization of {xi}N
i=1 obtained
from (b) PCA; (c) LLE with K = 20; (d) LNEG with K =
20; (e) LLE with K = 40; (f) LNEG with K = 40.
i=1; and {yi}N
IV. NUMERICAL TESTS
The performance of the generalized version of LLE was
assessed using tests on synthetic data. Alg. 1 is tested using
Kx as in (7) for the locally nonlinear embedding (LNE) both
without and with graph regularization (the latter abbreviated
as LNEG), and is compared with LLE and PCA [10]. For all
experiments, the graph G is constructed with adjacecy matrix
As with (i, j)th entry aij = xix⊤
j /kxikkxjk. Two types
of tests are carried out in order to: a) evaluate embedding
performance for a single manifold; and b) assess how infor-
mative the low-dimensional embeddings are for distinguishing
different manifolds.
Embedding experiment. In the ﬁrst experiment, the em-
bedding performance of the proposed method is assessed. A
3-dimensional Swiss roll manifold is generated, and 1, 000
data are randomly sampled from the manifold as shown in
Fig. 1 (a). Fig. 1 (b) showcases the 2-dimensional embeddings
obtained from PCA, while Figs. 1 (c) and (d) illustrate
the resulting embeddings from LLE and LNEG respectively,
where neighborhoods of K = 20 data are considered. Figs.
1 (e) and (f) illustrate embeddings obtained by LLE and
LNEG with K = 40. The regularization parameter of LNEG
is set
to
P = 2. Clearly, by exploiting the nonlinear relationships
to γ = 0.1, and the polynomial order is set
1
-1
-2
-1
-2
-3
(a)
1.00000015
1.0000001
1.00000005
0.99999995
0.9999999
0.99999985
-2.5
-2
-1.5
-1
-0.5
0.5
1.5
2.5
-1
-2
-3
-4
-5
-4
0.1
0.08
0.06
0.04
0.02
-0.02
-0.04
-0.06
-0.08
-3
-2
-1
-0.1
-0.02
0.02
0.04
0.06
0.08
0.1
(b)
(c)
(d)
Fig. 2: Embedding results of two manifolds: a linear hyperplane with hole and a trefoil (a) visualization of two manifolds;
and {yi}N
i=1 obtained from (b) LLE with K = 40; and, (c) LNEG with K = 40 and P = 2; (d) PCA.
Trefoil-knots 
-1
-2
-4
-2
(a)
1.00000004
1.00000003
1.00000002
1.00000001
0.99999999
0.99999998
0.99999997
0.99999996
0.99999995
0.99999994
-2
-1
-0.4
-0.6
-0.8
-1
-1.2
-1.4
-1.6
-1.8
-2
-2.2
-2.4
-1.5
0.12
0.1
0.08
0.06
0.04
0.02
-0.02
-0.04
-0.06
-1
-0.5
0.5
1.5
-0.08
-0.04
-0.02
0.02
0.04
0.06
0.08
(b)
(c)
(d)
Fig. 3: Embedding results of two manifolds: a nonlinear sphere and a trefoil (a) visualization of two manifolds; and {yi}N
obtained from (b) LLE with K = 40; (c) LNEG with K = 40 and P = 3; and (d) PCA.
i=1
Plane-hole-trefoil
Sphere-trefoil
LLE LNE LNEG LLE LNE LNEG
0.17
0.25
0.16
0.44
0.14
0.15
0.21
0.19
0.20
0.36
0.18
0.18
0.17
0.17
0.17
0.18
0.21
0.13
0.20
0.20
0.49
0.29
0.39
0.48
0.46
0.39
0.21
0.27
0.26
0.28
0.21
0.43
10
20
30
40
PCA
TABLE I: Clustering error rate on low-dimensional represen-
tations obtained from: LLE, LNE, LNEG and PCA.
between data, the resulting low-dimensional representations
are capable of better preserving the structure of the manifold,
thus allowing for more accurate visualization.
Clustering experiment. In this experiment, the ability of
Alg. 1 to provide meaningful embeddings for clustering of
different manifolds is assessed. Two 3-dimensional manifolds,
a linear hyperplane with a hole around the origin and a trefoil
are generated on the same ambient space as per [3], and
200 and 400 data are sampled from them, respectively. Here
each manifold corresponds to a different cluster. Fig. 2(a)
illustrates the sampled points from the generated manifolds.
Z1 ∈ R3×200 and Z2 ∈ R3×400 contain the data gener-
ated from the linear hyperplane and the trefoil, respectively.
Both manifolds are then linearly embedded in R100, that is
Xi = PZi +Ei, where P ∈ R100×3 is an orthonormal matrix,
and E is a noise matrix with entries sampled from a zero
mean Gaussian distribution with variance 0.01. Afterwards,
the 100-dimensional data in X := [X1 X2] are embedded
into 2-dimensional representations Y ∈ R2×600 using LLE,
LNEG and PCA. Figures. 2(b), (c), and (d) depict the 2-
dimensional embeddings Y provided by LLE, LNEG, and
PCA, respectively. Similarly, Fig. 3 illustrates the resulting
embeddings when Z2 is sampled from a nonlinear sphere. In
both cases, the nonlinear methods result in embeddings that
separate the two manifolds. To further assess the performance,
K-means is carried out on the resulting Y [13]. Table I
shows the clustering error when running K-means on the
low-dimensional embeddings given by PCA, LLE, LNE and
LNEG, across different values of K. The proposed approaches
provide embeddings that enhance separability of the two man-
ifolds, resulting in lower clustering error compared to LLE
and PCA. In addition, greater performance gain is observed
when both manifolds are nonlinear, as in the case of Fig. 3.
V. CONCLUSIONS
This paper introduced a general framework for nonlinear
dimensionality reduction over graphs. By leveraging nonlinear
relationships between data, low-dimensional representations
were obtained to preserve these nonlinear correlations. Graph
regularization was employed to account for additional prior
knowledge when seeking the low-dimensional representations.
An efﬁcient algorithm that admits closed-form solution was
developed, and several tests were conducted on simulated
data to demonstrate its effectiveness. To broaden the scope of
this study, several intriguing directions open up: a) extensive
numerical tests on real data; b) development of data-dependent
schemes that are capable of selecting appropriate kernels; c)
online implementations that can handle streaming data; and
d) generalizations to cope with large-scale graphs and high-
dimensional datasets.
REFERENCES
[1] F. R. Bach, G. R. Lanckriet, and M. I. Jordan, “Multiple kernel learning,
conic duality, and the smo algorithm,” in Proc.intl. Conf. on Machine
learning, New York, USA, 2004, pp. 6–13.
[2] M. Belkin and P. Niyogi, “Laplacian eigenmaps for dimensionality
reduction and data representation,” Neural Computation, vol. 15, no. 6,
pp. 1373–1396, Mar. 2003.
[3] E. Elhamifar and R. Vidal, “Sparse manifold clustering and embedding,”
in Advances in Neural Information Processing Systems, Granada, Spain,
2011, pp. 55–63.
[4] A. Ghodsi, “Dimensionality reduction -A short tutorial,” Department of
Statistics and Actuarial Science, Univ. of Waterloo, Ontario, Canada,
vol. 37, p. 38, 2006.
[5] J. Ham, D. D. Lee, S. Mika, and B. Sch¨olkopf, “A kernel view of the
dimensionality reduction of manifolds,” in Proc. Intl. Conf. on Machine
Learning. Alberta, Canada: ACM, Jul. 2004, p. 47.
[6] J. A. Hartigan and M. A. Wong, “Algorithm AS 136: A K-means
clustering algorithm,” Journal of the Royal Statistical Society, vol. 28,
no. 1, pp. 100–108, Jan. 1979.
[7] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical
Learning. Springer, 2009.
[8] B. Jiang, C. Ding, and J. Tang, “Graph-Laplacian PCA: Closed-form
solution and robustness,” in Proc. Conf. on Computer Vision and Pattern
Recognition, Portland, Oregon, Jun. 2013, pp. 3492–3498.
[9] T. Jin, J. Yu, J. You, K. Zeng, C. Li, and Z. Yu, “Low-rank matrix
factorization with multiple hypergraph regularizer,” Pattern Recognition,
vol. 48, no. 3, pp. 1011–1022, Mar. 2015.
[10] I. Jolliffe, Principal Component Analysis. Wiley Online Library, 2002.
[11] M. Kivel¨a, A. Arenas, M. Barthelemy, J. P. Gleeson, Y. Moreno, and
M. A. Porter, “Multilayer networks,” Journal of Complex Networks,
vol. 2, no. 3, pp. 203–271, 2014.
[12] J. B. Kruskal and M. Wish, Multidimensional Scaling.
Sage, 1978,
vol. 11.
[13] S. Lloyd, “Least-squares quantization in PCM,” IEEE Trans. Info.
Theory, vol. 28, no. 2, pp. 129–137, 1982.
[14] D. Romero, V. N. Ioannidis, and G. B. Giannakis, “Kernel-based Re-
construction and Kalman Filtering of Space-time Functions on Dynamic
Graphs,” IEEE Journal on Special Topics in Signal Processing, vol. 11,
no. 6, 2017.
[15] D. Romero, M. Ma, and G. B. Giannakis, “Kernel-based reconstruction
of graph signals,” IEEE Transactions on Signal Processing, vol. 65,
no. 3, pp. 764–778, Feb. 2017.
[16] S. T. Roweis and L. K. Saul, “Nonlinear dimensionality reduction by
locally linear embedding,” Science, vol. 290, no. 5500, pp. 2323–2326,
Dec. 2000.
[17] B. Sch¨olkopf, A. Smola, and K.-R. M¨uller, “Kernel principal component
analysis,” in Proc. Intl. Conf. on Artiﬁcial Neural Networks, Lausanne,
Switzerland, Oct. 1997, pp. 583–588.
[18] N. Shahid, N. Perraudin, V. Kalofolias, G. Puy, and P. Vandergheynst,
“Fast robust PCA on graphs,” IEEE Journal of Selected Topics in Signal
Processing, vol. 10, no. 4, pp. 740–756, Feb. 2016.
[19] F. Shang, L. Jiao, and F. Wang, “Graph dual regularization non-negative
matrix factorization for co-clustering,” Pattern Recognition, vol. 45,
no. 6, pp. 2237–2250, Jun. 2012.
[20] Y. Shen, B. Baingana, and G. B. Giannakis, “Kernel-based structural
equation models for topology identiﬁcation of directed networks,” IEEE
Trans. Sig. Proc., vol. 65, no. 10, pp. 2503–2516, May 2017.
[21] J. A. Suykens and J. Vandewalle, “Least squares support vector machine
classiﬁers,” Neural Processing Letters, vol. 9, no. 3, pp. 293–300, Jun.
1999.
[22] P. A. Traganitis, Y. Shen, and G. B. Giannakis, “Topology inference of
multilayer networks,” in Intl. Workshop on Network Science for Comms.,
Atlanta, GA, May 2017.
