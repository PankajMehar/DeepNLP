
This paper studies the Restricted Isometry Property (RIP) of random projections for sub-
spaces. It reveals that the distance between two low-dimensional subspaces remain almost
unchanged after being projected by a Gaussian random matrix with overwhelming proba-
bility, when the ambient dimension after projection is suﬃciently large in comparison with
the dimension of subspaces.
1.1 Motivation
In the era of data deluge, labeling huge amount of large-scale data can be time-consuming,
costly, and even intractable, so unsupervised learning has attracted increasing attention in
∗The authors are with the Department of Electronic Engineering, Tsinghua University, Beijing 100084,
China. The corresponding author of this paper is Yuantao Gu (gyt@tsinghua.edu.cn).
recent years. One of such methods emerging recently, subspace clustering (SC) [1, 2, 3, 4],
which depicts the latent structure of a variety of data as a union of subspaces, has been
shown to be powerful in a wide range of applications, including motion segmentation, face
clustering, and anomaly detection.
It also shows great potential to some previously less
explored datasets, such as network data, gene series, and medical images.
Traditional subspace clustering methods, however, suﬀer from the deﬁciency in simi-
larity representation, so it can be computationally expensive to adapt them to large-scale
datasets.
In order to alleviate the high computational burden, a variety of works have
been done to address the crucial problem of how to eﬃciently handle large-scale datasets.
Compressed Subspace Clustering (CSC) [5] also known as Dimensionality-reduced Subspace
Clustering [6] is a method that performs SC on randomly compressed data points. Because
the random compression reduces the dimension of the ambient space, the computational
cost of ﬁnding the self-representation in SC can be eﬃciently reduced. Based on the con-
cept of subspace aﬃnity, which characterizes the similarity between two subspaces, and
the mathematical tools introduced in [2], the conditions under which several popular al-
gorithms can successfully cluster the compressed data have been theoretically studied and
numerically veriﬁed [7, 8].
Because the data points are randomly projected from a high-dimensional ambient space
RN to a new medium-dimensional ambient space Rn, a worry is that the similarity between
any two low-dimensional subspaces increases and the SC algorithms are less likely to per-
form well. Inspired by the well-known Johnson-Lindenstrauss (JL) Lemma [9, 10] and the
Restricted Isometry Property (RIP) [11, 12, 13], which allows the use of random projection
to reduce the space dimension while keeping the Euclidean distance between any two data
points and leads to the boom of sparse signal processing including Compressed Sensing (CS)
[14, 15, 16, 17, 18, 19, 20], one may speculate whether the similarity (or distance) between
any two given subspaces can remain almost unchanged, if the dimension of the latent sub-
space that the data lie in is small compared with that of the ambient space after projection
n. It should be highlighted that this conjecture is not conﬁned to the SC problem, so we
believe that it may beneﬁt future studies on other subspace related topics.
Motivated by the conjecture about whether the similarity between any two given sub-
spaces can remain almost unchanged after random projection, we study the RIP of Gaussian
random projections for a ﬁnite set of subspaces. In order to give more solid guarantees and
more precise insight into the law of magnitude of the dimensions for CSC and other sub-
space related problems, we derive an optimum probability bound of the RIP of Gaussian
random compressions for subspaces in this paper. Compared with our previous work [21],
the probability bound has been improve from 1 − O(1/n) to 1 − e−O(n), which is optimum
when we consider the state-of-the-art statistical probability theories for Gaussian random
matrix.
1.2 Main Results
The projection Frobenius norm (F-norm for short) distance is adopted in this work to
measure the distance between two subspaces. It should be noted that we slightly generalize
the deﬁnition in [22] to the situation where the dimensions of the two subspaces are diﬀerent.
Deﬁnition 1 ([21]) (Projection Frobenius norm distance between subspaces) The
generalized projection F-norm distance between two subspaces X1 and X2 is deﬁned as
D(X1,X2) :=
√2kU1UT
1 − U2UT
2 kF,
where Ui denotes an arbitrary orthonormal basis matrix for subspace Xi, i = 1, 2.
We will focus on the change of the distance between any two low-dimensional sub-
spaces after being randomly projected from RN to Rn (n < N ). The projection of a
low-dimensional subspace by using a Gaussian random matrix is deﬁned as below.
Deﬁnition 2 (Gaussian random projection for subspace) The Gaussian random pro-
jection of a d-dimensional subspace X ⊂ RN onto Rn (d < n < N ) is deﬁned as below,
−→ Y = {y|y = Φx,∀x ∈ X},
where the projection matrix Φ ∈ Rn×N is composed of entries independently drawn from
Gaussian distribution N (0, 1/n).
One may notice that the dimensions of subspaces remain unchanged after random pro-
jection with probability one.
Based on the deﬁnitions above, the main theoretical result of this work is stated as
follows.
Theorem 1 Suppose X1, . . . ,XL ⊂ RN are L subspaces with dimension less than d. After
−→ Yi ⊂ Rn, i =
random projection by using a Gaussian random matrix Φ ∈ Rn×N , Xi
1,··· , L, n < N . There exist constants c1(ε), c2(ε) > 0 depending only on ε such that for
any two subspaces Xi and Xj, for any n > c1(ε) max{d, ln L},
(1 − ε) D2(Xi,Xj) < D2(Yi,Yj) < (1 + ε) D2(Xi,Xj)
(1)
holds with probability at least 1 − e−c2(ε)n.
Theorem 1 reveals that the distance between two subspaces remains almost unchanged
after random projection with overwhelming probability, when the ambient dimension after
projection n is suﬃciently large.
1.3 Our Contribution
In this paper, we study the RIP of Gaussian random matrices for projecting a ﬁnite set of
subspaces. The problem is challenging as random projections neither preserve orthogonality
nor normalize the vectors deﬁning orthonormal bases of the subspaces. In order to measure
the change in subspace distance induced by random projections, both eﬀects have to be
carefully quantiﬁed. Based on building a metric space of subspaces with the projection
F-norm distance, which is closely connected with subspace aﬃnity, we start from verifying
that the aﬃnity between two subspaces concentrates on its estimate with overwhelming
probability after Gaussian random projection. Then we successfully reach the RIP of two
subspaces and generalize it to the situation of a ﬁnite set of subspaces, as stated in Theorem
1.
The main contribution of this work is to provide a mathematical tool, which can shed
light on many problems including CSC. As a direct result of Theorem 1, when solving the SC
problem at a large scale, one may conduct SC on randomly compressed samples to alleviate
the high computational burden and still have theoretical performance guarantee. Because
the distance between subspaces almost remains unchanged after projection, the clustering
error rate of any SC algorithm may keep as small as that conducting in the original space.
Considering that our theory is independent of SC algorithms, this may beneﬁt future studies
on other subspace related topics.
Except our previous work [21] that will be compared with in Section 6, as far as we know,
there is no relevant work that study the distance preserving property between subspaces
after random projection.
1.3.1 Comparison with JL Lemma and RIP for Sparse Signals
The famous Johnson-Lindenstrauss Lemma illustrates that there exists a map from a higher-
dimensional space into a lower-dimensional space such that the distance between a ﬁnite
set of data points will change little after being mapped.
Lemma 1 (JL Lemma) [9, 10] For any set V of L points in RN , there exists a map
f : RN → Rn, n < N , such that for all x1, x2 ∈ V,
(1 − ε)kx1 − x2k2
2 ≤ kf (x1) − f (x2)k2
2 ≤ (1 + ε)kx1 − x2k2
if n is a positive integer satisfying n ≥ 4lnL/(ε2/2 − ε3/3), where 0 < ε < 1 is a constant.
The RIP of random matrix illustrates that the distance between two sparse vectors will
change little with high probability after random projection.
Deﬁnition 3 [11, 12, 13] The projection matrix Φ ∈ Rn×N , n < N satisﬁes RIP of order
k if there exists a δk ∈ (0, 1) such that
(1−δk)kx1−x2k2
2 ≤ kΦx1−Φx2k2
2 ≤ (1+δk)kx1−x2k2
Table 1: Comparison with other dimension-reduction theories including JL Lemma, RIP
for sparse signals, and our previous results [21]
JL Lemma
RIP for
sparse signals
RIP for
low-dimensional subspaces
[21]
this work
object
any set of L
points in RN
all k-sparse
signals in RN
any set of L d-dimensional
subspaces in RN
metric
compression
method
error bound
condition
success
probability
Euclidean distance
projection F-norm distance
kxi − xjk2
1√2kPi − PjkF
some map f
Gaussian random matrix
(1 − ε, 1 + ε)
n ≥ 4lnL
ε2/2−ε3/3
(1 − δk, 1 + δk)
n ≥ c1kln(cid:0) N
k(cid:1)
1 − e−c2n
(1 − ε, 1 + ε)
n large enough n > c1max{d, ln L}
1− 2dL(L−1)
(ε−d/n)2n
1 − e−c2n
holds for any two k-sparse vectors x1, x2 ∈ RN .
Theorem 2 [13] A Gaussian random matrix Φ ∈ Rn×N , n < N has the RIP of order k for
n ≥ c1kln(cid:0) N
k(cid:1) with probability 1 − e−c2n, where c1, c2 > 0 are constants depending only on
δk, the smallest nonnegative constant satisfying Deﬁnition 3.
In summary, the above works focus on the change of the distance between points after
determinate mapping or random projection. In comparison, our work views a subspace as
a whole and studies the distance between subspaces, which to the best of our knowledge
has never been studied before. Moreover, the above works study the points in Euclidean
space with l2-norm, while our work study the subspaces on the Grassmannian manifold
with F-norm metric, which is highly nonlinear and more complex. A detailed comparison
to explain the diﬀerences between our work and related works is presented in Table 1.
1.3.2 Comparison with RIP for Signals in UoS
There are literatures studying the distance preserving properties of compressed data points,
which may be sparse on speciﬁc basis or lie in a couple of subspaces or surfaces [23, 24, 25,
26].
The authors of [23] extended the RIP to signals that are sparse or compressible with
respect to a certain basis Ψ, i.e., x = Ψα, where Ψ is represented as a unitary N×N matrix
and α is a k-sparse vector. The work of [24] proves that with high probability the random
projection matrix Φ can preserve the distance between two signals belonging to a Union
of Subspaces (UoS). In [25], it is shown that random projection preserves the structure of
surfaces. Given a collection of L surfaces of linearization dimension d, if they are embedded
into a space of O(dδ2 log(Ld/δ)) dimension, the surfaces are preserved in the sense that for
any pair of points on these surfaces the distance between them are preserved. The main
contribution of [26] is stated as follows. If S is an n point subset of RN , 0 < δ < 1
3 and
n = 256d log n(max{d, 1/δ})2, there is a mapping of RN into Rn under which volumes of
sets of size at most d do not change by more than a factor of 1 + δ, and the distance of
points from aﬃne hulls of sets of size at most k − 1 is preserved within a relative error of δ.
According to above survey, those works study embedding of Euclidean distances be-
tween points in subspaces, while we discuss embedding of a ﬁnite set of subspaces in terms
of the projection F-norm distance.
In both the related works and this paper, the same
mathematical tool of concentration inequalities and random matrix theory are adopted to
derive the RIP for two diﬀerent objects, i.e., data points in Euclidean space and subspaces
in Euclidean space (or points on Grassmann manifold), respectively. In comparison, both
Euclidean space and random projection are linear, but Grassmannian is not linear, let along
the projection on it, so the new problem is much more diﬃcult than the existing one, and
a core contribution of this work is dealing with the above challenges with a brand-new
geometric proof, the technique in which has hardly been used previously to derive the RIP
for data points.
1.4 Organization
The rest of this paper is organized as follows. Based on the introduction of principal
angles, aﬃnity, and its connection with the projection F-norm distance, we study the RIP
for subspaces in the top level in Section 2. The main result of Theorem 1 is proved by
using two core propositions of Lemma 4 and Theorem 3.
In Section 3, we focus on the
probability and concentration inequalities of Gaussian random matrix to prepare necessary
mathematical tools that will be used through this work. In Section 4, we prove the ﬁrst
core proposition of Lemma 4, which states that the aﬃnity between a line and a subspace
will concentrate on its estimate with overwhelming probability after random projection. In
Section 5, we prove the second core proposition of Theorem 3, which provides a general
theory that the aﬃnity between two subspaces with arbitrary dimensions demonstrates
concentration after random projection. In Section 6, we compare those theories with our
previous results and highlight the novelty. We conclude this work in Section 7. Most proofs
of lemmas and remarks are included in the Appendix 8.
1.5 Notations
Vectors and matrices are denoted by lower-case and upper-case letter, respectively, both in
boldface. AT denotes matrix transposition. kak and kAkF denote ℓ2 norm of vector a and
Frobenius norm of matrix A. smax(A) and smin(A) denote the largest and smallest singular
value of matrix A, respectively. Subspaces are denoted by X ,Y, and S. C(A) denotes the
column space of matrix A. We use S⊥ to denote the orthonormal complement space of S.
PS (v) denotes the projection of vector v onto subspace S.
2 RIP of Gaussian Random Projection for Subspaces
2.1 Preliminary
Before starting the theoretical analysis, we ﬁrst introduce the deﬁnition of principal angles
and aﬃnity. These two concepts have been widely adopted to describe the relative position
and to measure the similarity between two subspaces. Our theoretical analysis will ﬁrst
focus on the estimation of these quantities before and after random projection. Then using
the connection between aﬃnity and projection F-norm distance derived in [21], we can
readily derive the result in Theorem 1.
The principal angles (or canonical angles) between two subspaces provide a robust way
to characterize the relative subspace positions [27, 28].
Deﬁnition 4 The principal angles θ1,··· , θd1 between two subspaces X1 and X2 of dimen-
sions d1 ≤ d2, are recursively deﬁned as
cos θk = max
x1∈X1
with the orthogonality constraints xT
xT
1kx2k
xT
1 x2
=:
kx1kkx2k
max
kx1kkkx2kk
x2∈X2
i xil = 0, l = 1,··· , k − 1, i = 1, 2.
Beside deﬁnition, an alternative way of computing principal angles is to use the singular
value decomposition [29].
Lemma 2 Let the columns of Ui be orthonormal bases for subspace Xi of dimension di, i =
1, 2 and suppose d1 ≤ d2. Let λ1 ≥ λ2 ≥ ··· ≥ λd1 ≥ 0 be the singular values of UT
1 U2,
then cos θk = λk, k = 1,··· , d1.
Based on principle angles, aﬃnity is deﬁned to measure the similarity between subspaces
[2].
Deﬁnition 5 The aﬃnity between two subspaces X1 and X2 of dimension d1 ≤ d2 is deﬁned
as
aﬀ (X1,X2) :=(cid:18) d1
Xk=1
cos2 θk(cid:19)1/2
= kUT
1 U2kF,
where the columns of Ui are orthonormal bases of Xi, i = 1, 2.
The relationship between distance and aﬃnity is revealed in Lemma 3. Because of the
concise deﬁnition and easy computation of aﬃnity, we will start the theoretical analysis
with aﬃnity, and then present the results with distance by using Lemma 3.
Lemma 3 [21] The distance and aﬃnity between two subspaces X1 and X2 of dimension
d1, d2, are connected by
D2(X1,X2) =
d1 + d2
− aﬀ 2(X1,X2).
2.2 Theoretical Results
In this section, we will present the main theoretical results about the aﬃnity and distance
between subspaces. Before that, let us introduce some basic notations to be used. We
denote the random projection of subspaces of X1 and X2 as Y1 and Y2, respectively. We
denote DX = D(X1,X2) and DY = D(Y1,Y2) as the distances before and after random
projection. Similarly, we use aﬀX = aﬀ(X1,X2) and aﬀY = aﬀ(Y1,Y2) to denote the
aﬃnities before and after projection. Without loss of generality, we always suppose that
d1 ≤ d2. For simplicity, we refer the aﬃnity (distance) after random projection as projected
aﬃnity (projected distance).
To begin with, we focus on a special case that one subspace is degenerated to a line
(one-dimensional subspace). The following lemma provides an estimation of the aﬃnity
between a line and a subspace after Gaussian random projection. When the dimensionality
of the new ambient space is large enough, the real projected aﬃnity will highly concentrate
around this estimation with overwhelming probability.
Lemma 4 Suppose X1,X2 ⊂ RN are a line and a d-dimension subspace, d ≥ 1, respectively.
Let λ = aﬀX denote their aﬃnity. If they are projected onto Rn, n < N, by a Gaussian
random matrix Φ ∈ Rn×N , Xi
−→ Yi, i = 1, 2, then the projected aﬃnity, aﬀY , can be
estimated by
and there exist constants c1(ε), c2(ε) > 0 depending only on ε such that for any n > c1(ε)d,
Y − aﬀ
holds with probability at least 1 − e−c2(ε)n.
aﬀ 2
(cid:12)(cid:12)(cid:12)
Y(cid:12)(cid:12)(cid:12)
aﬀ
Y = λ2 +
n(cid:0)1 − λ2(cid:1) ,
< (1 − λ2)ε
(2)
(3)
Then, we study the general case of projecting two subspaces of arbitrary dimensions.
As mentioned in the last subsection, we will begin with the estimation of aﬃnity and then
restate the result in terms of distance.
The following theorem reveals the concentration of aﬃnity between two arbitrary sub-
spaces after random projection.
Theorem 3 Suppose X1,X2 ⊂ RN are two subspaces with dimension d1 ≤ d2, respectively.
Take
aﬀ
Y = aﬀ 2
X +
d2
(d1 − aﬀ 2
X )
(4)
−→ Yi, i =
as an estimate of the aﬃnity between two subspaces after random projection, Xi
1, 2. Then there exist constants c1(ε), c2(ε) > 0 depending only on ε such that for any
n > c1(ε)d2,
< (d1 − aﬀ 2
X )ε
(5)
Y − aﬀ
holds with probability at least 1 − e−c2(ε)n.
aﬀ 2
(cid:12)(cid:12)(cid:12)
Y(cid:12)(cid:12)(cid:12)
Because of its concision when evaluating the relative position in Deﬁnition 5, we present
the concentration by using aﬃnity in Lemma 4 and Theorem 3, which play essential role
in RIP for subspaces. Their proofs, which unfurl main text of this work, are postponed to
Section 4 and Section 5, respectively.
Using Lemma 3 and Theorem 3, we derive an estimation of the projected distance.
Similarly we prove that the true projected distance will highly concentrate around this
estimate with overwhelming probability.
Corollary 1 Suppose X1,X2 ⊂ RN are two subspaces with dimension d1 ≤ d2, respectively.
We use
Y = D2
X −
d2
n (cid:18)D2
X −
2 (cid:19)
d2 − d1
−→
as an estimation of the distance between two subspaces after random projection, Xi
Yi, i = 1, 2. Then there exist constants c1(ε), c2(ε) > 0 depending only on ε such that for
any n > c1(ε)d2,
D2
Y − D
<(cid:18)D2
X −
2 (cid:19) ε
d2 − d1
(cid:12)(cid:12)(cid:12)
Y(cid:12)(cid:12)(cid:12)
holds with probability at least 1 − e−c2(ε)n.
Proof Combining (4) and (6) by using Lemma 3, we readily get that |D2
aﬀ
to (5).
Y|. Using Lemma 3 again, we have D2
2 = d1 − aﬀ 2
X − d2−d1
Y −
X . Therefore, (7) is identical
Y| = |aﬀ 2
Y − D
(6)
(7)
2.3 Proof of Theorem 1
Now we are ready to prove the RIP of Gaussian random matrix for projecting a ﬁnite set
of subspaces using the results above.
Without loss of generality, we assume that di ≤ dj ≤ d. According to Corollary 1, there
exist constants c1,1, c2,1 > 0 depending only on ε such that for any n > c1,1dj,
D2(Xi,Xi) −(cid:18) dj
2(cid:19)(cid:18)D2(Xi,Xi) −
2 (cid:19) < D2(Yi,Yi)
dj − di
< D2(Xi,Xi) +(cid:18)−
2(cid:19)(cid:18)D2(Xi,Xi) −
dj
2 (cid:19) .
dj − di
holds with probability at least 1 − e−c2,1n. When n > 2d/ε, we have dj/n ≤ d/n < ε/2. In
this case, we have both
D2(Yi,Yi) > D2(Xi,Xi) −(cid:18) dj
2(cid:19) D2(Xi,Xi)
=(cid:18)1 −
dj
n −
2(cid:19) D2(Xi,Xi) > (1 − ε) D2(Xi,Xi),
D2(Yi,Yi) < D2(Xi,Xi) +(cid:18)−
dj
2(cid:19) D2(Xi,Xi)
=(cid:18)1 −
dj
2(cid:19) D2(Xi,Xi) < (1 + ε) D2(Xi,Xi),
(8)
(9)
hold with probability at least 1− e−c2,1n. Note that (8) and (9) hold for any 1 ≤ i < j ≤ L.
Then the probability is at least 1− L(L−1)
, there exists constant
c2 depending only on ε, such that L(L−1)
c2,1}, then
when n > c1 max{d, ln L}, conditions n > c1,1d, n > 2d/ε, and n > 1
that are
required above are all satisﬁed, the probability is at least 1 − e−c2n with c2 > 0. Then we
reach the ﬁnal conclusion.
e−c2,1n. If n > 1
c2,1
e−c2,1n < e−c2n. Take c1 := max{c1,1, 2
ε , 2
ln L(L−1)
ln L(L−1)
c2,1
3 Concentration Inequalities for Gaussian Distribution
Before proving the main results, we ﬁrst introduce some useful concentration inequalities for
Gaussian distribution. Most of them are proved using the following lemma, which provides
a strict estimation of the singular values of Gaussian random matrix.
Lemma 5 [30] Let A be an N × n matrix whose elements aij are independent Gaussian
random variables. Then for every t ≥ 0, one has
and
P(cid:16)smax(A) ≥
P(cid:16)smin(A) ≤
√N + √n + t(cid:17) ≤ e− t
√N − √n − t(cid:17) ≤ e− t
2 ,
2 .
10
(10)
(11)
Based on Lemma 5, we are ready to prove some useful lemmas that will be directly used
to prove our theories on RIP of random projection for subspaces. Before doing that, we
ﬁrst deﬁne standard Gaussian random matrix and verify that the function satisfying certain
condition can be written as a single exponential function.
Deﬁnition 6 A Gaussian random matrix (or vector) has i.i.d. zero-mean Gaussian random
entries. A standard Gaussian random matrix A ∈ Rn×N has i.i.d. zero-mean Gaussian
random entries with variance 1/n. Each column of A is a standard Gaussian random
vector.
Lemma 6 Given
if for all k, it holds that
f (ε, n, τ ) =
Xk=1
ak(ε, n)e−gk(ε,n,τ ),
hk(ε) := lim
τ→0
bk(ε) := lim
n→∞
lim
n→∞
ln ak(ε, n)
gk(ε, n, τ )
> 0,
< hk(ε),
(12)
(13)
(14)
then there exist universal constants n0, c1 > 0, and c2 > 0 depending only on ε, such that
when n > n0, τ < c1, it satisﬁes that f (ε, n, τ ) < e−c2n.
Proof The proof is postponed to Appendix 8.1.
Remark 1 Lemma 6 illustrates that the summation of ﬁnite multiple exponential decay
functions can always be bounded by a single exponential function.
The following lemma illustrates that the norm of standard Gaussian random vector
concentrates around 1 with high probability, especially when the dimensionality is high.
Lemma 7 Assume that a ∈ Rn is a standard Gaussian random vector. For any ε > 0, we
have
hold for n > n0, where n0 and c are constants dependent on ε.
P(cid:0)(cid:12)(cid:12)kak2 − 1(cid:12)(cid:12) > ε(cid:1) < e−c(ε)n
(15)
Proof The proof is postponed to Appendix 8.2.
Furthermore, Corollary 2 generalizes Lemma 7 to case when we project standard Gaus-
sian random vector by orthonormal matrix.
11
Corollary 2 Let a ∈ Rn be a standard Gaussian random vector. For any given orthonormal
matrix V = [v1,··· , vd] ∈ Rn×d and ε > 0, we have
hold for n > c1d, where c1, c2 are constants dependent on ε.
kVTak2 −
> ε(cid:19) < e−c2(ε)n
(16)
P(cid:18)(cid:12)(cid:12)(cid:12)(cid:12)
n(cid:12)(cid:12)(cid:12)(cid:12)
Proof The proof is postponed to Appendix 8.3.
Corollary 3 extends Lemma 5 to column-normalized standard Gaussian random matrix.
It reveals that a column-normalized Gaussian random matrix is a high-quality approxima-
tion to an orthonormal matrix, because all its singular values are very close to 1.
Corollary 3 Let A = [a1,··· , ak] ∈ Rn×k be a standard Gaussian random matrix. Each
column of ¯A is normalized from the corresponding column of A, that is
¯A =(cid:20) a1
ka1k
,··· ,
ak
kakk(cid:21) .
Then we can get the bound of the minimum and maximum of the singular value of ¯A as
below
P(cid:0)s2
P(cid:0)s2
min(cid:0) ¯A(cid:1) < 1 − ε(cid:1) < e−c2,1(ε)n,
max(cid:0) ¯A(cid:1) > 1 + ε(cid:1) < e−c2,2(ε)n,
∀n > c1,1k,
∀n > c1,2k,
(17)
(18)
where c1,1 and c2,1, c1,2 and c2,2 are constants dependent on ε in (17) and (18), respectively.
Proof The proof is postponed to Appendix 8.4.
Remark 2 For A ∈ R(n−d0)×k be a standard Gaussian random matrix, we have (18) hold
for n > c1,2 max{k, d0}, where c1,2 and c2,2 are constants dependent on ε.
The following lemma studies a property of Gaussian random projection. Intuitively, it
illustrates that if a line and a subspace are perpendicular to each other, they will still be
almost perpendicular after Gaussian random projection.
Lemma 8 Assume u1 ∈ RN is a unit vector, U2 ∈ RN×d is an orthonormal matrix, and
u1 is perpendicular to U2. Let Φ ∈ Rn×N be a standard Gaussian random matrix. We use
a1 = Φu1 and A2 = ΦU2 to denote the projection of u1 and U2 by using Φ. If V2 is an
arbitrary orthonormal basis of C(A2), then for ε > 0, we have
> ε(cid:17) ≤ e−c2(ε)n
(19)
hold for n > c1(ε)d, where c1, c2 are constants determined by ε.
P(cid:16)(cid:13)(cid:13)VT
2 a1(cid:13)(cid:13)
Proof The proof is postponed to Appendix 8.5.
12
Corollary 4 In Lemma 8, if we further deﬁne ¯a1 = a1/ka1k as the normalized projection
of u1, then for ε > 0, we have
hold for n > c1(ε)d, where c1, c2 are constants determined by ε.
P(cid:16)(cid:13)(cid:13)VT
2 ¯a1(cid:13)(cid:13)
> ε(cid:17) ≤ e−c2(ε)n
(20)
Proof The proof is postponed to Appendix 8.6.
Remark 3 Using the same notations in Corollary 4, if we take Φ ∈ R(n−d0)×N , ¯a1 ∈ Rn−d0
and V2 ∈ R(n−d0)×d, then we have still have
P(cid:16)(cid:13)(cid:13)VT
2 ¯a1(cid:13)(cid:13)
for n > c1(ε) max{d, d0}, where c1 and c2 are constants dependent on ε. This can by readily
veriﬁed by replacing n with n − d0 in (20) and then applying Lemma 6. The detailed proof
is postponed to Appendix 8.7.
> ε(cid:17) ≤ e−c2(ε)n
Finally we state the independence of random vectors, matrices, and their column-
spanned subspaces.
Deﬁnition 7 Two random vectors are independent, if and only if the distribution of any
one of them does not have inﬂuence on that of the other. Two random matrices are indepen-
dent, if and only if any two columns of them are independent. Furthermore, we introduce
the independence between a random matrix and a subspace, which holds true if and only if
the subspace is spanned by the columns of another random matrix that is independent of
the ﬁrst one. Finally, two subspaces are independent, if and only if they are spanned by the
columns of two independent random matrices, respectively.
Lemma 9 Assume U and V are two matrices satisfying UTV = 0, and Φ is a Gaussian
random matrix. Then ΦU and ΦV are independent. This can be readily veriﬁed by calcu-
lating the correlation between any two entries in ΦU and ΦV, respectively. They are all
zero.
4 Proof of Lemma 4
The proof of Lemma 4 is made up of two steps. At ﬁrst, we derive an accurate expression
of the error about estimating aﬀY . Then, the estimate error is bounded by utilizing the
concentration inequalities for Gaussian distribution that we derived in the previous section.
Step 1) Let us begin from choosing the bases for the line X1 and the subspace X2 and
then calculating the aﬃnity after projection.
According to the deﬁnition of aﬃnity, λ = cos θ, where θ is the only principal angle
between X1 and X2. We use u and u1 to denote, respectively, the basis of X1 and a unit
13
vector in X2, which constructs the principal angle with u. Therefore, u can be rewritten
into the following form
u = λu1 +p1 − λ2u0,
where u0 denotes some unit vector orthogonal to X2. Based on the above deﬁnition, we can
choose U = [u1, ..., ud] as the basis of X2. Notice that {u2,··· , ud} could be freely chosen
as long as the orthonormality is satisﬁed.
After projecting X1 by random Gaussian matrix, we get subspace Y1, whose basis vector
is
(21)
(22)
a = Φu = λΦu1 +p1 − λ2Φu0
= λa1 +p1 − λ2a0,
where a1 := Φu1 and a0 := Φu0 are not orthogonal to each other. As for Y2, considering
that ΦU is not a set of orthonormal basis, we do orthogonalization by using Gram-Schmidt
process. Denote the orthonormalized matrix as V = [v1,··· , vd]. By the deﬁnition of
Gram-Schmidt process, the ﬁrst column of V should be
which does not change its direction after the orthogonalization.
v1 =
a1
ka1k
(23)
Remark 4 Consider the aﬃnity between two subspaces S1 and S2, with dimension 1 and
d ≥ 1. Let v1 and V2 be the orthonormal basis for S1 and S2, respectively. Then the aﬃnity
equals the norm of the projection of v1 onto S2, i.e., λ = kPS2(v1)k =(cid:13)(cid:13)VT
By the deﬁnition of aﬃnity and (22), we can calculate the aﬃnity between Y1 and Y2
2 v1(cid:13)(cid:13).
as
aﬀ 2
VT a
kak(cid:13)(cid:13)(cid:13)(cid:13)
Y =(cid:13)(cid:13)(cid:13)(cid:13)
λVTa1 +p1 − λ2VTa0(cid:13)(cid:13)(cid:13)
kak2 (cid:13)(cid:13)(cid:13)
kak2 (cid:16)λ2kVTa1k2 + (1 − λ2)kVTa0k2 + λp1 − λ2kaT
1 a0k(cid:17) .
Because a1 lies in Y2, we have
kVTa1k = ka1k.
By taking the norm on both sides of (22), we write
kak2 = λ2ka1k2 + (1 − λ2)ka0k2 + 2λp1 − λ2ka0kka1k.
14
(24)
(25)
(26)
Recalling (2) and inserting the estimation into (27), the estimate error is deduced as
(cid:12)(cid:12)(cid:12)
aﬀ 2
aﬀ 2
Y(cid:12)(cid:12)(cid:12)
aﬀ 2
Y − aﬀ
(cid:18)1 −
the RHS of (28) into three parts using triangle inequality.
kak2 (cid:19)−(cid:18)1−(1−λ2)(cid:18)1−
kak2 − kVTa0k2
kak2 (cid:19)(cid:12)(cid:12)(cid:12)(cid:12)
Eliminating kVTa1k and ka1k by inserting (25) and (26) into (24), we get
kak2(cid:0)kak2−(1−λ2)ka0k2 +(1−λ2)kVTa0k2(cid:1)
Y =
kak2 − kVTa0k2
= 1 − (1 − λ2)(cid:18)ka0k2
kak2 (cid:19) .
=(cid:12)(cid:12)(cid:12)(cid:12)
(1 − λ2)(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)
Y −(cid:18)λ2 +
1−(1−λ2)(cid:18)ka0k2
kak2 − kVTa0k2
=(cid:12)(cid:12)(cid:12)(cid:12)
n(cid:19) −(cid:18)ka0k2
=(1 − λ2)(cid:12)(cid:12)(cid:12)(cid:12)
n(cid:19) −(cid:18)ka0k2
kak2 − kVTa0k2
kak2 (cid:19)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:18)1 −
kVTa0k2
ka0k2
n(cid:12)(cid:12)(cid:12)(cid:12)
kak2 − 1(cid:12)(cid:12)(cid:12)(cid:12)
+(cid:12)(cid:12)(cid:12)(cid:12)
kak2 −
kak2 (cid:12)(cid:12)(cid:12)(cid:12)
n(cid:12)(cid:12)(cid:12)(cid:12)
kak2 − 1(cid:12)(cid:12)(cid:12)(cid:12)
with probability at least 1 − 2e−c2,1(ε1)n, we have
(cid:12)(cid:12)kak2 − 1(cid:12)(cid:12) < ε1
(cid:12)(cid:12)ka0k2 − 1(cid:12)(cid:12) < ε1.
n(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:13)(cid:13)VTa0(cid:13)(cid:13)
kak2 − 1(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)
≤(cid:12)(cid:12)(cid:12)(cid:12)
≤(cid:12)(cid:12)(cid:12)(cid:12)
kVTa0k2 −
n(cid:12)(cid:12)(cid:12)(cid:12)
ka0k2
< ε2.
and
Using (30), (31), and (32) in (29), for n > max{n0, c1,2}d, with probability at least
1 − 2e−c2,1(ε1)n − e−c2,2(ε2)n,
we have
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:18)1 −
n(cid:19) − ka0k2
kak2  1 −
Xi=1
cos2 θi!(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
15
2ε1
1 − ε1
≤(cid:18) 3d
ε2
1 − ε1
+ 3(cid:19) ε1 +
2n
ε1
1 − ε1
3ε2
Since u0 is orthogonal to ui, i = 1,··· , d, by Corollary 2, for n > c1,2(ε2)d, with probability
at least 1 − e−c2,2(ε2)n, we have
Step 2) Before bounding the estimate error by concentration inequalities, we ﬁrst split
n(cid:19)(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)
(28)
Since both a and a0 are standard Gaussian random vectors, by Lemma 7, for n > n0,
(29)
(27)
(30)
(31)
(32)
(33)
(34)
where the last inequality holds for ε1 < 1/3.
To complete the proof, we need to formulate (34) and (33) into the shape of (3) and
a single exponential function, respectively. Letting ε1 = ε2 =: ε/6 and inserting (34) into
(28), we have
(cid:12)(cid:12)(cid:12)
aﬀ 2
Y − aﬀ
Y(cid:12)(cid:12)(cid:12) ≤(cid:0)1 − λ2(cid:1)(cid:18)(cid:18) 3d
≤(cid:0)1 − λ2(cid:1)(cid:18)(cid:18)3
=(cid:0)1 − λ2(cid:1) ε.
By Remark 1, we know that there exist constants c1, c2, such that (33) is greater than
1 − e−c2(ε)n for any n > c1(ε)d and close the proof.
+ 3(cid:19) ε1 +
+ 3(cid:19) ε1 +
3ε2
2 (cid:19)
2 (cid:19)
3ε2
2n
5 Proof of Theorem 3
The proof of Theorem 3 is divided into two parts. In subsection 5.1, we will complete the
main body of the proof by using an important lemma, which will be proved in subsection
5.2. Before we start, let us introduce some auxiliary variables.
Remark 5 Assume there are two subspaces S1 and S2, with dimension d1 ≤ d2. Let
˜Ui = [˜ui,1,··· , ˜ui,di] denote any orthonormal matrix for subspace Si, i = 1, 2. One may
do singular value decomposition as ˜UT
1 , where the singular values λk =
cos θk, 1 ≤ k ≤ d1 are located on the diagonal of Λ, and θ1 ≤ θ2 ≤ ··· ≤ θd1 denote the
principal angles between S1 and S2. After reshaping, we have
˜U1 = Q2ΛQT
UT
2 U1 :=(cid:16) ˜U2Q2(cid:17)T
˜U1Q1 = Λ =
λ1

. . .
λd1

where
Ui := ˜UiQi = [ui,1,··· , ui,di] ,
i = 1, 2
are the orthonormal basis, which have the closest connection with the aﬃnity between these
two subspaces.
Deﬁnition 8 (principal orthonormal bases) We refer U1 and U2 as principal orthonor-
mal bases for S1 and S2, if they are derived by using the method in Remark 5.
According to Remark 5, for subspaces X1 and X2, we can get their principal orthonormal
bases U1 and U2, respectively. After projection by multiplying a standard Gaussian random
16
matrix Φ, the original basis matrix changes to Ai = ΦUi = [ai,1,··· , ai,di] , whose columns
are no longer unitary and orthogonal to each other. Then we normalize each columns as
kai,diki , whose columns are now unitary but still not
¯Ai = [¯ai,1,··· , ¯ai,di] = h ai,1
orthogonal to each other. However, by Corollary 3, we know that ¯Ai can be used as a good
approximation for the orthonormal basis of Yi. We will see that ¯Ai plays an important role
in estimating the aﬃnity after projection.
,··· ,
kai,1k
ai,di
We also need to deﬁne an accurate orthonormal basis for the projected subspace. One
eﬃcient way is to process ¯Ai by using Gram-Schmidt orthogonalization, whose result is
deﬁned as Vi = [vi,1,··· , vi,di] , i = 1, 2.
5.1 Main Body
In order to prove Theorem 3, we need to calculate aﬀ 2
Because ¯Ai is very close to an orthonormal matrix, we may use kVT
aﬃnity after projection. By using triangle inequality, we have
Y and estimate its bias from aﬀ
Y .
¯A1k to estimate the
(35)
aﬀ 2
Y − aﬀ
(cid:12)(cid:12)(cid:12)
Y(cid:12)(cid:12)(cid:12) ≤(cid:12)(cid:12)(cid:12)
aﬀ 2
Y −(cid:13)(cid:13)VT
¯A1(cid:13)(cid:13)
F(cid:12)(cid:12)(cid:12)
+(cid:12)(cid:12)(cid:12)(cid:13)(cid:13)VT
¯A1(cid:13)(cid:13)
F − aﬀ
Y(cid:12)(cid:12)(cid:12)
Therefore, the following proof can be divided into three steps. The ﬁrst step is to bound
the error caused by using ¯A1 as an approximation of V1 to compute the aﬃnity. To do
that, we will introduce an important lemma, which is the essence of the proof. The second
step is to bound the diﬀerence between the approximated aﬃnity and our estimate, which
can be derived by using Lemma 4. Finally, we combine these two bounds and complete the
proof.
Step 1) For the ﬁrst item in the RHS of (35), according to the deﬁnition of aﬃnity, we
have
(cid:12)(cid:12)(cid:12)
aﬀ 2
Y −(cid:13)(cid:13)VT
¯A1(cid:13)(cid:13)
F(cid:12)(cid:12)(cid:12)
d1
=(cid:12)(cid:12)(cid:12)kVT
2 V1k2
=(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
Xk=1(cid:16)kVT
Xk=1(cid:12)(cid:12)(cid:12)kVT
F(cid:12)(cid:12)(cid:12)
¯A1(cid:13)(cid:13)
F −(cid:13)(cid:13)VT
2(cid:17)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
2 v1,kk2 −(cid:13)(cid:13)VT
2 ¯a1,k(cid:13)(cid:13)
2(cid:12)(cid:12)(cid:12)
2 v1,kk2 −(cid:13)(cid:13)VT
2 ¯a1,k(cid:13)(cid:13)
d1
(36)
Lemma 10 There exist constants c1,1(ε1) and c2,1(ε1) > 0 depending only on ε1, such that
for any n > c1,1(ε1)d2, we have
(cid:12)(cid:12)(cid:12)kVT
2 v1,kk2 −(cid:13)(cid:13)VT
2 ¯a1,k(cid:13)(cid:13)
hold with probability at least 1 − e−c2,1(ε1)n.
Proof The proof is postponed to Section 5.2.
2(cid:12)(cid:12)(cid:12) ≤(cid:0)1 − λ2
k(cid:1) ε1,
∀k = 1,··· , d1
(37)
17
Plugging (37) into (36), we have
(cid:12)(cid:12)(cid:12)
aﬀ 2
Y −(cid:13)(cid:13)VT
¯A1(cid:13)(cid:13)
F(cid:12)(cid:12)(cid:12) ≤ ε1
d1
Xk=1(cid:0)1 − λ2
k(cid:1)
(38)
hold with probability at least 1 − d1e−c2,1(ε1)n for any n > c1,1(ε1)d2.
Step 2) For the second estimation error in the RHS of (35), we can convert this problem
about one subspace Y1 with dimension d1 into d1 subproblems, each of which is about 1-
dimensional subspace, and then use Lemma 4 to estimate the error. Denote X1,k := C{u1,k},
Y1,k := C{a1,k}, 1 ≤ k ≤ d1. According to the deﬁnition of aﬃnity and Remark 5, we have
that the aﬃnity between X1,k and X2 is kUT
2 u1,kk = λk, and the aﬃnity between Y1,k and
Y2 is equal to (cid:13)(cid:13)VT
and Y1,k and Y2 are the projected subspaces, we have
Yk(cid:12)(cid:12)(cid:12) ≤(cid:0)1 − λ2
k(cid:1) ε2,
2 ¯a1,k(cid:13)(cid:13). Using Lemma 4, where X1,k and X2 are the original subspaces
(cid:12)(cid:12)(cid:12)(cid:13)(cid:13)VT
2 ¯a1,k(cid:13)(cid:13)
hold with probability at least 1 − d1e−c2,2(ε2)n for any n > c1,2(ε2)d2, where
k = 1,··· , d1
− aﬀ
(39)
Plugging the deﬁnition of aﬀ
we have
F − aﬀ
(cid:12)(cid:12)(cid:12)(cid:13)(cid:13)VT
¯A1(cid:13)(cid:13)
Y(cid:12)(cid:12)(cid:12)
aﬀ
Yk := λ2
k +
d2
n (cid:0)1 − λ2
k(cid:1) .
Y in (4), (39), and (40) into the second estimation error,
d2
k +
(d1 − aﬀ 2
d2
(1 − λ2
X )(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)
k)(cid:19)(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
d1
=(cid:12)(cid:12)(cid:12)(cid:12)(cid:13)(cid:13)VT
F −(cid:18)aﬀ 2
¯A1(cid:13)(cid:13)
X +
=(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
−(cid:18)λ2
Xk=1(cid:18)(cid:13)(cid:13)VT
2 ¯a1,k(cid:13)(cid:13)
Xk=1(cid:12)(cid:12)(cid:12)(cid:13)(cid:13)VT
Yk(cid:12)(cid:12)(cid:12)
2 ¯a1,k(cid:13)(cid:13)
− aﬀ
Xk=1
(1 − λ2
k).
≤ ε2
d1
d1
(41)
(40)
(42)
Step 3) Combining (38) and (41) together into (35), we have that for any n > max{c1,1(ε1), c1,2(ε2)}d2,
aﬀ 2
Y − aﬀ
(cid:12)(cid:12)(cid:12)
d1
Y(cid:12)(cid:12)(cid:12) ≤ (ε1 + ε2)
Xk=1(cid:0)1 − λ2
k(cid:1)
= (ε1 + ε2)(cid:0)d1 − aﬀ 2
X(cid:1)
hold with probability at least 1 − d1e−c2,1(ε1)n − d1e−c2,2(ε2)n. Let ε1 = ε2 =: ε/2, according
to Remark 1, one can easily verify that there exist constants c1, c2 depending only on ε,
when n > c1(ε)d2,
holds with probability at least 1 − e−c2(ε)n. Then we complete the proof.
aﬀ 2
Y − aﬀ
(cid:12)(cid:12)(cid:12)
Y(cid:12)(cid:12)(cid:12) ≤(cid:0)d1 − aﬀ 2
X(cid:1) ε
18
5.2 Proof of Lemma 10
In order to improve the readability of the proof, we deﬁne intensively all the variables
required in advance. Not that some variables deﬁned before are also summarized here to
make this part self-contained.
We use X1 and X2 to denote the subspaces before projection, with dimensions d1 ≤ d2.
The principal orthonormal bases for X1 and X2 are denoted as U1 and U2, respectively.
The kth column of Ui is denoted as ui,k, which spans a 1-dimensional subspace denoted
as Xi,k, k = 1,··· , di, i = 1, 2. In addition, we deﬁne Ui,1:k as the matrix composed of the
ﬁrst k columns of Ui. That is Ui,1:k = [ui,1,··· , ui,k] , 1 ≤ k ≤ di, i = 1, 2. The subspace
spanned by the columns of Ui,1:k is denoted as Xi,1:k = C(Ui,1:k).
We use Y1 and Y2 to denote the subspaces after projection, respectively, from X1 and
X2 by using a standard Gaussian random matrix Φ. The dimensions of Y1 and Y2 stay
to be d1 and d2 with probability 1. Ai = ΦUi is a basis for Yi and its kth column is
denoted as ai,k, which spans a 1-dimensional subspace denoted as Yi,k = C(ai,k). We deﬁne
Ai,1:k = [ai,1,··· , ai,k] as the composition of the ﬁrst k columns of Ai. The subspace
spanned by the columns in Ai,1:k is denoted as Yi,1:k = C(Ai,1:k).
We use ¯A1 and ¯A2 to denote the column-normalized result of A1 and A2, respectively.
V1 and V2 are deﬁned as the orthonormalized result of ¯A1 and ¯A2, respectively, by using
Gram-Schmidt orthogonalization. As a consequence, Vi provides an orthonormal basis for
Yi. Similarly, Vi,1:k denotes the matrix composed of the ﬁrst k columns of Vi.
Let’s start the proof of Lemma 10 from the LHS of (37). According to the deﬁnition of
V2, v1,k, ¯a1,k, and Remark 4, we have
kVT
kVT
2 v1,kk2 = kPY2(v1,k)k2 = 1 −(cid:13)(cid:13)(cid:13)
2 ¯a1,kk2 = kPY2(¯a1,k)k2 = 1 −(cid:13)(cid:13)(cid:13)
PY ⊥
PY ⊥
(v1,k)(cid:13)(cid:13)(cid:13)
(¯a1,k)(cid:13)(cid:13)(cid:13)
(43)
(44)
As a consequence, the LHS of (37) is derived as the diﬀerence of squared norm of the
projection of v1,k and ¯a1,k onto to the orthogonal complement of Y2, i.e.
(45)
2 v1,kk2 − kVT
(cid:12)(cid:12)kVT
2 ¯a1,kk2(cid:12)(cid:12) =(cid:12)(cid:12)(cid:12)(cid:12)(cid:13)(cid:13)(cid:13)
PY ⊥
(v1,k)(cid:13)(cid:13)(cid:13)
−(cid:13)(cid:13)(cid:13)
PY ⊥
(¯a1,k)(cid:13)(cid:13)(cid:13)
2(cid:12)(cid:12)(cid:12)(cid:12)
In order to analyze v1,k, we take a close look at the Gram-Schmidt orthogonalization
process. We introduce
αk := kPY1,1:k−1(¯a1,k)k = kVT
1,1:k−1¯a1,kk
as the cosine of the only principal angle between Y1,k and Y1,1:k−1, and
bk :=
αk
PY1,1:k−1(¯a1,k)
19
(46)
(47)
as a unit vector along the direction of the projection of a1,k onto Y1,1:k−1. As a consequence,
the Gram-Schmidt orthogonalization process is represented by
Then we introduce
¯a1,k = PY1,1:k−1(¯a1,k) + PY ⊥
kv1,k.
= αkbk +q1 − α2
1,1:k−1
(¯a1,k)
ˆλk := kPY2(¯a1,k)k = kVT
βk := kPY2(bk)k = kVT
2 ¯a1,kk,
2 bkk
(48)
(49)
(50)
to denote, respectively, the cosine of the only principal angle between Y1,k and Y2 and that
between C{bk} and Y2. Now projecting both side of (48) on the orthogonal complement of
Y2, we have
(51)
(bk).
q1 − ˆλ2
k ¯a⊥1,k = αkq1 − β2
kb⊥k +q1 − α2
kPY ⊥
(v1,k),
where ¯a⊥1,k and b⊥k denotes, respectively, the unit vectors along PY ⊥
(¯a1,k) and PY ⊥
Moving the ﬁrst item in the RHS of (51) to the LHS and then taking norm on both
sides, we get
(1 − α2
k)kPY ⊥
(v1,k)k2 = 1 − ˆλ2
k + α2
k(cid:0)1 − β2
k(cid:1) − 2αkq1 − ˆλ2
kq1 − β2
kh¯a⊥1,k, b⊥k i.
(52)
In addition, the norm of the projection of ¯a1,k onto to Y⊥2 could be directly represented by
using ˆλk as
Inserting both (52) and (53) into (45), we write
kPY ⊥
(¯a1,k)k2 = 1 − ˆλ2
k.
(53)
k + α2
2 v1,kk2 − kVT
(cid:12)(cid:12)kVT
=(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
1 − ˆλ2
k(cid:0)1 − β2
k + 1 − β2
k(cid:16)1 − ˆλ2
α2
2 ¯a1,kk2(cid:12)(cid:12)
k(cid:1) − 2αkq1 − ˆλ2
1 − α2
k(cid:17) + 2(cid:12)(cid:12)(cid:12)(cid:12)
αkq1 − ˆλ2
1 − α2
kq1 − β2
kh¯a⊥1,k, b⊥k i
− (1 − ˆλ2
kq1 − β2
kh¯a⊥1,k, b⊥k i(cid:12)(cid:12)(cid:12)(cid:12)
k)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(54)
Using the fact that geometric mean is no more than arithmetic mean and α2
will be veriﬁed soon that α2
k is a small quantity, we further reshape (54) as
k < 1/3, which
k +(cid:12)(cid:12)(cid:12)
k(cid:17)(cid:16)α2
In the following, we will estimate the four quantities of 1− ˆλ2
(cid:12)(cid:12)(cid:12)kVT
2 v1,kk2 −(cid:13)(cid:13)VT
2(cid:12)(cid:12)(cid:12) ≤
2 ¯a1,k(cid:13)(cid:13)
2(cid:16)1 − ˆλ2
k + 1 − β2
αkh¯a⊥1,k, b⊥k i(cid:12)(cid:12)(cid:12)(cid:17) .
k, 1− β2
k, α2
k, and h¯a⊥1,k, b⊥k i
(55)
separately.
20
Using basic algebra, (56) is reshaped to
λ2
k +
d2
n (cid:0)1 − λ2
1 − ˆλ2
k ≤1 − λ2
k −
=(cid:0)1 − λ2
<(cid:0)1 − λ2
d2
k(cid:1) − ˆλ2
n (cid:0)1 − λ2
k(cid:1)(cid:18)1 −
k(cid:1) (1 + ε1) ,
k ≤(cid:0)1 − λ2
k(cid:1) ε1,
k(cid:1) +(cid:0)1 − λ2
+ ε1(cid:19)
k = 1,··· , d1.
k(cid:1) ε1
d2
(56)
(57)
Let us ﬁrst consider 1 − ˆλ2
k. Recalling its deﬁnition in (49), we have already estimate
ˆλ2
k in (39). Inserting (40) into (39), with probability at least 1− e−c2,1(ε1)n, we have for any
n > c1,1(ε1)d2
The bound of (57) looks direct because ˆλk denotes the aﬃnity compressed from λk.
According to Lemma 4, the former can be estimated by the latter.
Second let us check αk = kVT
1,1:k−1¯a1,kk. Intuitively, because X1,1:k−1 is orthogonal to
X1,k, the new subspaces Y1,1:k−1 (projected from X1,1:k−1) and Y1,k (projected from X1,k) are
approximately orthogonal to each other. Actually, V1,1:k−1 and ¯a1,k satisfy all conditions
in Corollary 4. As a consequence, there exist constants c1,2(ε2), c2,2(ε2), such that for any
ε2 < 1
k < ε2 hold with probability at least
1 − e−c2,2(ε2)n.
3 and n > c1,2(ε2)d1 > c1,2(ε2)(k − 1), we have α2
Next we consider 1 − β2
k, which is also bounded by 1 − λ2
k, Notice that bk lies in
Y1,1:k−1 ⊂ Y1,1:k and βk is the norm of the projection of bk onto Y2. Because the minimum
of the norm of the projection of a unit vector in Y1,1:k onto Y2 approximates λk, 1 − β2
should be very close to 1−λ2
k. The diﬀerence between them can be bounded by the following
lemma.
Lemma 11 For any n > c1,3(ε3)d2, we have
1 − β2
k ≤(cid:0)1 − λ2
k(cid:1) (1 + ε3)
holds with probability at least 1 − e−c2,3(ε3)n.
Proof The proof is postponed to Appendix 8.8.
(58)
Finally, as for the last term to be estimated, h¯a⊥1,k, b⊥k i is proved to be a small quantity
in Lemma 12. Intuitively, ¯a⊥1,k and b⊥k is unit projections of ¯a1,k ∈ Y1,k and bk ∈ Y1,1:k−1,
respectively, onto Y⊥2 . Consequently, the inner product between ¯a⊥1,k and b⊥k should be very
small if Y1,k and Y1,1:k−1, which are independent with each other, are both independent
with Y⊥2 .
Lemma 12 There exist constants c1,4(ε4), c2,4(ε4), such that for any n > c1,4(ε4)d2, we
have (cid:12)(cid:12)(cid:12)h¯a⊥1,k, b⊥k i(cid:12)(cid:12)(cid:12)
Proof The proof is postponed to Appendix 8.9.
< ε4 holds with probability at least 1 − e−c2,4(ε4)n.
21
Now, we are ready to complete the proof by using the concentration properties derived
< ε4 into (55), we have for any
above. Plugging (57), (58), α2
n > max{c1,l(εl)}d2, l = 1, 2, 3, 4,
k < ε2, and (cid:12)(cid:12)(cid:12)h¯a⊥1,k, b⊥k i(cid:12)(cid:12)(cid:12)
2(cid:12)(cid:12)(cid:12) ≤
2 ¯a1,k(cid:13)(cid:13)
hold with probability at least 1 −P4
(cid:12)(cid:12)(cid:12)kVT
2 v1,kk2 −(cid:13)(cid:13)VT
2(cid:0)1 − λ2
then we have
k(cid:1) (2 + ε1 + ε3) (ε2 + √ε2ε4)
l=1 e−c2,l(εl)n. Let ε1 < 1 and ε3 < 1, ε2 = ε4 =: ε/12,
(cid:12)(cid:12)(cid:12)kVT
2 v1,kk2 −(cid:13)(cid:13)VT
2 ¯a1,k(cid:13)(cid:13)
2(cid:12)(cid:12)(cid:12) ≤(cid:0)1 − λ2
k(cid:1) ε.
According to Remark 1, we claim that there exist constants c1, c2, such that for any
n > c1(ε)d2, (59) holds with probability at least 1 − e−c2(ε)n.
(59)
6 Related Works
Our earlier results on the RIP of subspaces in [21] are cited below.
Theorem 4 Suppose X1,X2 ⊂ RN are two subspaces with dimension d1 ≤ d2, respectively.
If X1 and X2 are projected into Rn by a Gaussian random matrix Φ ∈ Rn×N , Xk
−→
Yk, k = 1, 2, then we have
(1 − ε)D2
X ≤ D2
Y ≤ (1 + ε)D2
X ,
with probability at least
when n is large enough.
1 −
4d1
(ε − d2/n)2n
Theorem 5 For any set composed by L subspaces X1,··· ,XL ∈ RN of dimension no more
than d, if they are projected into Rn by a Gaussian random matrix Φ ∈ Rn×N , Xk
−→
Yk, k = 1,··· , L, and d ≪ n < N , then we have
(1 − ε)D2(Xi,Xj) ≤ D2(Yi,Yj) ≤ (1 + ε)D2(Xi,Xj),
∀i, j
with probability at least
when n is large enough.
1 −
2dL(L − 1)
(ε − d/n)2n
Compared with the our previous results, this paper has the following two main improve-
ments. Firstly, because we use more advanced random matrix theories and deal with the
error more skillfully, the probability bound 1− e−O(n) derived in this paper is much tighter
than the 1 − O(1/n) in the previous work, where we used Chebyshev inequality. Such
22
improvement provides a more accurate law of magnitude of the dimensions in this random
projection problem, and the improved probability bound is optimum, if one compares it
with the analogical conclusions in the theory of Compressed Sensing. Secondly, Theorem 5
requires n ≫ d, but it does not specify how large n should be or the connection between ε,
d, L, and the lower bound of n. In comparison, Theorem 1 in this paper rigorously clariﬁes
that the conclusion will hold as long as n is larger than c1(ε) max{d, ln L}.
7 Conclusion
In this paper, we utilize the random matrix theory to rigorously prove the RIP of Gaussian
random compressions for low-dimensional subspaces. Mathematically, we demonstrate that
as long as the dimension after compression n is larger than c1(ε) max{d, ln L}, with proba-
bility no less than 1 − e−c2(ε)n, the distance between any two subspaces after compression
remains almost unchanged. The probability bound 1− e−O(n) is optimum in the asymptotic
sense, in comparison with the analogical optimum theoretical result of RIP in Compressed
Sensing. Our work can provide a solid theoretical foundation for Compressed Subspace
Clustering and other low-dimensional subspace related problems.
8 Appendix
8.1 Proof of Lemma 6
We ﬁrst prove the special case that K = 1, i.e., f = ae−g for short. According to (13),
(14), and the deﬁnition of limitation, there exist constants n0 and c1 > 0 depending only
on ε. When n > n0, τ < c1, we have g
3 > 0
depending only on ε, we can have
3 . Let c2 := h−b
n > h − h−b
3 , and ln a
n < b + h−b
f = ae−g = e−(g−ln a) = exp(cid:18)−n(cid:18) g
n −
≤ exp(cid:18)−n(cid:18)h −
3 (cid:19)(cid:19)
h − b
= exp(cid:18)−
h − b
h − b
3 − b −
n(cid:19) = e−c2n.
ln a
n (cid:19)(cid:19)
Now we consider the general case of arbitrary K. According to the above analysis, we
have that, for each term of f , there exist constants n0,k, c1,k > 0, and c2,k > 0 depending only
on ε. When n > n0,k, τ < c1,k, it satisﬁes that ake−gk < e−c2,kn. Let n0 := maxk n0,k,
c1 :=
mink c1,k > 0,
c2 := mink c2,k > 0. Then when n > n0, τ < c1, we have that
f =
Xk=1
ake−gk <
Xk=1
e−nc2,k ≤ e−c2n,
and complete the proof.
23
8.2 Proof of Lemma 7
Regarding √na as a matrix belonging to Rn×1, and using Lemma 5, we have that with
probability at least 1 − 2e− t
2 ,
√n − 1 − t ≤ smin(√na) = √nkak = smax(√na) ≤ √n + 1 + t.
Taking square and subtracting n from both sides, we have
−(cid:16)2√n(1 + t) + (1 + t)2(cid:17) ≤ −(cid:16)2√n(1 + t) − (1 + t)2(cid:17) ≤ nkak2−n ≤ 2√n(1+t)+(1 + t)2 ,
with probability at least 1 − 2e− t
By choosing ε satisfying nε = 2√n(1 + t) + (1 + t)2, we can get
2 .
t = √n(cid:0)√1 + ε − 1 − 1/√n(cid:1) .
=: n0,1, we have t > 0. Substituting this equation into the expression
(60)
(61)
When n >(cid:16)
of probability, we have
1√ε+1−1(cid:17)2
P(cid:16)(cid:12)(cid:12)(cid:12)kak2 − 1(cid:12)(cid:12)(cid:12)
> ε(cid:17) < 2 exp(cid:16)−n(cid:0)√1 + ε − 1 − 1/√n(cid:1)2
/2(cid:17) .
According to Lemma 6, there exist constants n0,2 and c dependent on ε, such that the RHS
of (61) is smaller than e−cn. Taking n0 = max{n0,1, n0,2}, we complete the proof.
8.3 Proof of Corollary 2
nε
d , we have
according to (61) in the proof of Lemma 7, by replacing ε with nε
d VTa ∈ Rd is a standard Gaussian random vector. As a consequence,
Proof Notice thatp n
P (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
/2! ,
r n
VTa(cid:13)(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)(cid:13)
d ε+1−1(cid:19)2
d ε+1−1(cid:19)2
where d >(cid:18)
1√ n
1√ n
1 ≤ d, we need n > 3d
ε =: c1,1d. According to Lemma 6, there exist constants c1,2, c2 de-
pendent on ε, such that the RHS of (62) is smaller than e−c2n. Taking c1 := max{c1,1, c1,2}
and dividing both sides of the expression in P(·) in (62) by n/d, we complete the proof.
ε − 1 − 1/√d(cid:19)2
− 1(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
is required. In order to satisfy this requirement, i.e.,(cid:18)
d ! < 2 exp −d(cid:18)r1 +
(62)
8.4 Proof of Corollary 3
In order to bound s2
min(cid:0) ¯A(cid:1), noticing that
min(cid:0) ¯A(cid:1) ≥
s2
s2
min (A)
max
i kaik2 ,
(63)
we may turn to estimate s2
min (A) and maxi kaik2 separately.
24
We begin from estimating s2
min (A). According to Lemma 5, with probability at least
1 − e−t2/2, we have
s2
min (A) ≥
Let 1 − ε1 be the RHS of (64) then we have
=(cid:16)1 −pk/n − t/√n(cid:17)2
√k − t(cid:17)2
n(cid:16)√n −
t = √n(cid:16)1 − √1 − ε1 −pk/n(cid:17) .
(64)
(65)
When n >
(1−√1−ε1)2 =: ˆc0,1k, we have t > 0. Plugging (65) into e−t2/2, we can get
/2(cid:19) . According to
the probability that (65) violates as exp(cid:18)−n(cid:16)1 −pk/n − √1 − ε1(cid:17)2
Lemma 6, the above probability can be bounded by e−ˆc2,1(ε1)n for n > ˆc1,1k. Then we have
s2
min (A) ≥ 1 − ε1
(66)
hold for n > max{ˆc1,1, ˆc0,1}k =: ˆc3k with probability at least 1 − e−ˆc2,1(ε1)n.
Next we estimate maxi kaik2. According to Lemma 7, with probability at least 1 −
ke−ˆc2,2(ε2)n, for n > n0,1, we have
(67)
(68)
Plugging (66) and (67) into (63), we have
i kaik2 ≤ 1 + ε2.
max
s2
min(cid:0) ¯A(cid:1) ≥
1 − ε1
1 + ε2
= 1 −
ε1 + ε2
1 + ε2
Let ε1 = ε2 =: ε/2, with probability at least 1 − e−ˆc2,1(ε/2)n − ke−ˆc2,2(ε/2)n, we have
s2
min(cid:0) ¯A(cid:1) ≥ 1 − (ε/2 + ε/2) = 1 − ε.
Take c1,1 := max{ˆc3, n0,1} and we prove the ﬁrst part of this corollary.
In order to bound s2
the counterparts of (63), (66), and (67), respectively, as
max(cid:0) ¯A(cid:1), following the same approach, we could derive step by step
s2
s2
max (A)
min
i kaik2 ,
max(cid:0) ¯A(cid:1) ≤
s2
max (A) ≤ 1 + ε1
for n > ˆc4k with probability at least
1 − exp(cid:18)−n(cid:16)1 +pk/n − √1 + ε1(cid:17)2
/2(cid:19) > 1 − e−ˆc2,3n,
and
i kaik2 ≥ 1 − ε2
min
25
(69)
(70)
(71)
for n > n0,2 with probability at least 1 − ke−ˆc2,2(ε2)n − e−ˆc2,3(ε1)n. Then we have
s2
max(cid:0) ¯A(cid:1) ≤
1 + ε1
1 − ε2
= 1 +
ε1 + ε2
1 − ε2
(72)
Similarly reshaping (72) and letting ε2 ≤ 1/2, ε1 = ε2 =: ε/4, taking c1,2 := max{ˆc4, n0,2},
we prove the second part of the corollary.
8.5 Proof of Lemma 8
Using the orthogonality between u1 and U2, C(A2) is independent with a1. As an or-
thonormal basis of such subspace, V2 is also independent with a1. Then, according to
Deﬁnition 6, a1 conditioned on V2 is still a standard Gaussian random vector. As a conse-
quence, √nVT
2 a1 ∈ Rd×1, the entries of which are independent standard Gaussian random
variables, satisﬁes the condition in Lemma 5. With probability no more than e− t
2 , we have
(73)
(cid:13)(cid:13)
Let ε :=(cid:16)√d + 1 + t(cid:17)2
√nVT
= s2
2 a1(cid:13)(cid:13)
/n, we can get
max(cid:0)√nVT
t = √nε −
2 a1(cid:1) ≥(cid:16)√d + 1 + t(cid:17)2
√d − 1.
When n > 4d
ε =: c1,1d, we have t > √d − 1 ≥ 0. Plugging (74) into e− t
(73) holding is at least 2 exp(cid:18)−(cid:16)√nε − √d − 1(cid:17)2
/2(cid:19) . According to Lemma 6, there exist
constants c1,2, c2, such that when n > c1,2d, this probability is smaller than e−c2n. Taking
c1 := max{c1,1, c1,2} and dividing both sides of (73) by n, we conclude the lemma.
(74)
2 , the probability of
8.6 Proof of Corollary 4
According to the deﬁnition of ¯a1 and basic probability, we have
P(cid:16)(cid:13)(cid:13)VT
2 ¯a1(cid:13)(cid:13)
> ε(cid:17) =P (cid:13)(cid:13)VT
ka1k2 > ε!
2 a1(cid:13)(cid:13)
ka1k2 < ε!
=1 − P (cid:13)(cid:13)VT
2 a1(cid:13)(cid:13)
≤1 − P(cid:16)ka1k2 > 1 − ε and (cid:13)(cid:13)VT
< ε (1 − ε)(cid:17)
2 a1(cid:13)(cid:13)
=P(cid:16)ka1k2 < 1 − ε or (cid:13)(cid:13)VT
> ε (1 − ε)(cid:17)
2 a1(cid:13)(cid:13)
≤P(cid:0)ka1k2 < 1 − ε(cid:1) + P(cid:16)(cid:13)(cid:13)VT
> ε (1 − ε)(cid:17) .
2 a1(cid:13)(cid:13)
(75)
Now we may estimate the two items in the RHS of (75), separately. By using Lemma 7,
for n > n0, we have
P(ka1k2 < 1 − ε) < e−c2,1(ε)n.
(76)
26
By using Lemma 8, for ε < 1
3 and n > c1,2d, we have
Plugging (76) and (77) into (75), we readily get
P(cid:16)(cid:13)(cid:13)VT
2 a1(cid:13)(cid:13)
P(cid:16)(cid:13)(cid:13)VT
2 ¯a1(cid:13)(cid:13)
n > c1(ε)d, we have (cid:13)(cid:13)VT
2 ¯a1(cid:13)(cid:13)
8.7 Proof of Remark 3
> ε(1 − ε)(cid:17) < e−c2,2(2ε/3)n.
> ε(cid:17) < e−c2,1(ε)n + e−c2,2(2ε/3)n.
According to Remark 1, we claim that there exist constants c1, c2, such that for any
> ε with probability no more than e−c2(ε)n.
(77)
(78)
Replacing n with n − d0 in (20), we can get
P(cid:16)(cid:13)(cid:13)VT
2 ¯a1(cid:13)(cid:13)
> ε(cid:17) ≤ e−ˆc2(ε)(n−d0).
We only need to prove that there exist constants c1, c2, when n > c1 max{d, d0}, we have
n , according to Lemma 6, we have
P(cid:16)(cid:13)(cid:13)VT
2 ¯a1(cid:13)(cid:13)
> ε(cid:17) ≤ e−ˆc2(ε)(n−d0) ≤ e−c2n. Let τ := d0
ˆc2(ε) (n − d0)
= lim
τ→0
h : = lim
τ→0
b : = lim
n→∞
Then when n > n0, τ = d0
By choosing c1 := max{n0, 1
condition of (78) holding, that is n > ˆc1d, is also satisﬁed.
lim
n→∞
ln 1
n ≤ τ0, there exists constant c2, such that e−ˆc2(ε)(n−d0) ≤ e−c2n.
n ≤ τ0. The
, ˆc1}, when n > c1d0, we have n > n0, τ = d0
ˆc2 (1 − τ ) = ˆc2 > 0,
lim
n→∞
= 0 < h.
τ0
8.8 Proof of Lemma 11
Using the deﬁnition of βk in (50) and the fact that Y1,1:k−1 ⊂ Y1,1:k, we have
β2
k =(cid:13)(cid:13)VT
2 bk(cid:13)(cid:13)
= min
kbk=1
b∈Y1,1:k−1
kVT
2 bk2 ≥ min
kbk=1
b∈Y1,1:k
kVT
2 bk2.
Removing both side of (79) from one, we write
1 − β2
k ≤ 1 − min
kbk=1
b∈Y1,1:k
kVT
2 bk2 = 1 − min
kbk=1
b∈Y1,1:k
kPY2(b)k2 = max
kbk=1
b∈Y1,1:k
kPY ⊥
(b)k2.
(79)
(80)
Then we will loose the condition and rewrite the expression of this maximization problem
step by step, and ﬁnally convert it to a problem about the extreme singular value of random
matrix.
For any vector b in Y1,1:k, it can be spanned by the columns of ¯A1,1:k as
b = ¯A1,1:kx =
xj ¯a1,j,
Xj=1
27
(81)
where x = [x1,··· , xk]T denotes the weight vector. Consequently, the condition of kbk = 1
can be loosen to the condition on x, i.e.,
kxk2 ≤ (cid:13)(cid:13)
s2
¯A1,1:kx(cid:13)(cid:13)
min(cid:0) ¯A1,1:k(cid:1)
Inserting (81) and (82) in (80), we have
1 − β2
k ≤ max
s2
s2
kbk2
min(cid:0) ¯A1,1:k(cid:1)
kxk2≤xu(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
2 
Xj=1
PY ⊥
kxk2≤xu(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
Xj=1
xjPY ⊥
kxk2≤xu(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
Xj=1
min(cid:0) ¯A1,1:k(cid:1)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
xj ¯a1,j
(¯a1,j)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
j ¯a⊥1,j(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
xjq1 − ˆλ2
= max
= max
=: xu.
(82)
(83)
where ˆλj is deﬁned in (49).
According to (57) and the decreasing order of λ1 ≥ ··· ≥ λd1, we have
1 − ˆλ2
j ≤ (1 − λ2
j )(1 + ε) ≤ (1 − λ2
k)(1 + ε),
∀j = 1,··· , k < d1,
(84)
hold with probability at least 1 − e−c2,1(ε1)n for any n > c1,1(ε1)d2. Inserting (84) in (83),
we have
1 − β2
k ≤ (1 − λ2
k)(1 + ε1) max
≤ (1 − λ2
k)(1 + ε1)s2
= (1 − λ2
k)(1 + ε1)
Xj=1
xj ¯a⊥1,j(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
kxk2≤xu(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
max(cid:16) ¯A⊥1,1:k(cid:17) xu
max(cid:16) ¯A⊥1,1:k(cid:17)
min(cid:0) ¯A1,1:k(cid:1)
s2
s2
(85)
where ¯A⊥1,1:k =h¯a⊥1,1,··· , ¯a⊥1,ki .
Now we need to bound the denominator and numerator in the RHS of (85), separately.
As to the denominator, according to Corollary 3, we have for n > c1,2(ε2)d1,
P(cid:0)s2
min(cid:0) ¯A1,1:k(cid:1) > 1 − ε2(cid:1) > 1 − e−c2,2(ε2)n.
(86)
As for estimating the numerator, since that A1,1:k is correlated with Y2, we can not
directly apply the available lemmas about the concentration inequalities of independent
Gaussian random matrix. However, by using the following techniques, we could manage to
convert the problem of estimating s2
normalized random matrix satisfying the independence condition.
max(cid:16) ¯A⊥1,1:k(cid:17) to a problem about the singular value of a
28
Remark 6 Recalling Remark 5 about the characteristics of principal orthonormal bases
U1, U2 for subspaces X1,X2, and following the decomposition way in the proof of Lemma
4, we can decompose each column of U1 as the projections onto X2 and its orthogonal
complement and get
U1 = U2Λ + U0Λ⊥,
(87)
where
Λ⊥ =
p1 − λ2
. . .


d1
q1 − λ2
2 U0 = 0.
and C(U0) ∈ RN×d1 is a subspace of X ⊥2 , that is UT
After random projection, the decomposition in Remark 6 changes to
A1 = A2Λ + A0Λ⊥,
(88)
where A0 = ΦU0. Projecting both side of (88) onto the orthogonal complement of Y2, we
have
(A1) = PY ⊥
which means that the normalized column of PY ⊥
identical to the normalized column of PY ⊥
That is
PY ⊥
(A0)Λ⊥,
(A1), i.e., ¯a⊥1,k deﬁned in (51) are exactly
(A0), which is denoted as ¯a⊥0,k, k = 1,··· , d1.
¯A⊥1:1:k =h¯a⊥0,1,··· , ¯a⊥0,ki =: ¯A⊥0,1:k.
(89)
Considering its property of isotropy, a Gaussian random vector remains Gaussian distri-
bution when it is projected to an independent subspace. This is demonstrated in Remark
7.
Remark 7 Let A1 ∈ Rn×d1 and A2 ∈ Rn×d2, d1 ≤ d2 be two Gaussian random matrices.
We denote V2 as an orthonormal basis of C(A2). The projection of A1 = [a1,1,··· , a1,d1]
onto C(A2) is denoted by B1 = [b1,1,··· , b1,d1], i.e., b1,k = PC(A2)(a1,k). If A1 and A2 are
independent, we have B1 = V2Ω, where Ω ∈ Rd2×d1 is a Gaussian random matrix.
This can be readily veriﬁed by using the fact that B1 = V2VT
2 A1 =: V2Ω, where Ω =
(ωi,j) := VT
2 A1. Because A1 is independent with C(A2), as well as its orthonormal basis
V2, the distribution of A1 is not inﬂuenced, if we ﬁrst condition V2 and regard it as a given
matrix. Consequently, we can readily check that ωi,j are i.i.d. zero mean Gaussian random
variables.
29
Recalling that UT
0 U2 = 0, which means uT
0,iu2,j = 0, 1 ≤ i ≤ d1, 1 ≤ j ≤ d2. According
to Lemma 9, we have that a0,i and a2,j are independent. Moreover, a0,i is independent with
Y2 and thus independent with its orthogonal complement, Y⊥2 . Then according to Remark
7, the projection of A0,1:k ∈ Rn×k onto Y⊥2 can be written as
A⊥0,1:k :=ha⊥0,1,··· , a⊥0,ki = V⊥2 Ω1:k,
(a0,j), V⊥2 ∈ Rn×(n−d2) is an arbitrary orthonormal basis of Y⊥2 , and
where a⊥0,j := PY ⊥
Ω1:k ∈ R(n−d2)×k is a Gaussian random matrix. According to the orthonormality of V⊥2 ,
we normalize both sides of (90) as
(90)
¯A⊥0,1:k = V⊥2
¯Ω1:k,
(91)
where ¯Ω1:k denotes the column-normalized Ω1:k. Because left multiplying an orthonormal
matrix does not change its singular value, we have
Combining (89) and (92), and using Remark 2, we have
smax(cid:16) ¯A⊥0,1:k(cid:17) = smax(cid:0) ¯Ω1:k(cid:1) .
P(cid:16)s2
max(cid:16) ¯A⊥1,1:k(cid:17) < 1 + ε3(cid:17) = P(cid:0)s2
max(cid:0) ¯Ω1:k(cid:1) < 1 + ε3(cid:1)
> 1 − e−c2,3(ε3)n
(92)
(93)
hold when n > c1,3(ε3)d2.
Plugging both bounds of denominator and numerator, i.e., (86) and (93), into (85), we
can get
1 − β2
k(cid:1) (1 + ε1)
k(cid:1)(cid:18)1 +
with probability at least 1 −P3
ε3 ≤ 1/2, ε1 = ε2 = ε3 =: ε/7, we have ε2+ε3
1−ε2
Remark 1 to reshape the probability, we readily complete the proof.
k ≤(cid:0)1 − λ2
=(cid:0)1 − λ2
l=1 e−c2,l(εl)n for any n > max{c1,l(εl)}d2. Let ε2 ≤ 1/2,
1−ε2 ≤ 2 (ε2 + ε3) + 3ε1 = ε. By using
1 − ε2(cid:19) ,
1 + ε3
1 − ε2
ε2 + ε3
1 − ε2
1 + ε3
+ ε1
+ ε1
(94)
1+ε3
8.9 Proof of Lemma 12
We will calculate the inner product between ¯a⊥1,k and b⊥k . Recalling that bk is the projection
of a1,k onto C(A1,1:k−1), it is not obvious whether ¯a⊥1,k and b⊥k are independent, and this
aggravate the problem to estimate their inner product directly.
In order to solve this,
therefore, we have to ﬁnd the relationship between product and projection and then convert
the problem to the situation described in Remark 3.
30
Recalling the previous result that ¯a⊥1,k = ¯a⊥0,k in (89) and using the fact of b⊥k ∈
C( ¯A⊥1,1:k−1), we write
(cid:12)(cid:12)(cid:12)h¯a⊥1,k, b⊥k i(cid:12)(cid:12)(cid:12)
=(cid:12)(cid:12)(cid:12)h¯a⊥0,k, b⊥k i(cid:12)(cid:12)(cid:12)
≤(cid:13)(cid:13)(cid:13)
=(cid:13)(cid:13)(cid:13)
PC( ¯A⊥
PC( ¯A⊥
1,1:k−1)(¯a⊥0,k)(cid:13)(cid:13)(cid:13)
0,1:k−1)(¯a⊥0,k)(cid:13)(cid:13)(cid:13)
(95)
Now we need to construct an orthonormal basis for C( ¯A⊥0,1:k−1) and build its connection
with ¯a⊥0,k. Recalling Remark 7 and the deduction in the proof of Lemma 11, we reshape
(91) as
h ¯A⊥0,1:k−1, ¯a0,ki = V⊥2 (cid:2) ¯Ω1:k−1, ¯ωk(cid:3) ,
where ¯ωk denotes the last column of ¯Ω1:k ∈ R(n−d2)×k. We next apply Gram-Schimidt
orthogonalization to Ω1:k−1 and get W1:k−1, which is an orthonormal basis for C( ¯Ω1:k−1).
Because of the orthonormality of V⊥2 , V⊥2 W1:k−1 is an orthonormal basis for C( ¯A⊥0,1:k−1).
As a consequence, we are able to calculate the RHS of (95) as
(cid:13)(cid:13)(cid:13)
PC( ¯A⊥
0,1:k−1)(¯a⊥0,k)(cid:13)(cid:13)(cid:13)
=(cid:13)(cid:13)(cid:13)(cid:13)(cid:16)V⊥2 W1:k−1(cid:17)T
=(cid:13)(cid:13)(cid:13)(cid:13)(cid:16)V⊥2 W1:k−1(cid:17)T
=(cid:13)(cid:13)WT
1:k−1 ¯ωk(cid:13)(cid:13) .
¯a⊥0,k(cid:13)(cid:13)(cid:13)(cid:13)
V⊥2 ¯ωk(cid:13)(cid:13)(cid:13)(cid:13)
(96)
Recalling that ¯Ω1:k is a column-normalized Gaussian random matrix, ¯ωk should be inde-
pendent with each column of ¯Ω1:k−1, and thus independent with C( ¯Ω1:k−1) = C(W1:k−1).
Combining (95) and (96), and using Remark 3, we have
P(cid:18)(cid:12)(cid:12)(cid:12)h¯a⊥1,k, b⊥k i(cid:12)(cid:12)(cid:12)
> ε4(cid:19) ≤ P(cid:16)(cid:13)(cid:13)WT
1:k−1 ¯ωk(cid:13)(cid:13)
for all n ≥ c1,4d2. The proof is completed.
> ε4(cid:17) < e−c2,4(ε4)n
(97)
References
[1] E. Elhamifar and R. Vidal, “Sparse subspace clustering,” in IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2009, pp. 2790–2797.
[2] M. Soltanolkotabi and E. J. Candes, “A geometric analysis of subspace clustering with
outliers,” The Annals of Statistics, vol. 40, no. 4, pp. 2195–2238, 2012.
[3] E. Elhamifar and R. Vidal, “Sparse subspace clustering: Algorithm, theory, and ap-
plications,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35,
no. 11, pp. 2765–2781, 2013.
31
[4] R. Heckel and H. B¨olcskei, “Robust subspace clustering via thresholding,” IEEE Trans-
actions on Information Theory, vol. 61, no. 11, pp. 6320–6342, 2015.
[5] X. Mao and Y. Gu, “Compressed subspace clustering: A case study,” in IEEE Global
Conference on Signal and Information Processing, 2014, pp. 453 – 457.
[6] R. Heckel, M. Tschannen, and H. Bolcskei, “Subspace clustering of dimensionality-
reduced data,” in IEEE International Symposium on Information Theory, 2014, pp.
2997–3001.
[7] R. Heckel, M. Tschannen, and H. B¨olcskei, “Dimensionality-reduced subspace cluster-
ing,” arXiv preprint arXiv:1507.07105, 2015.
[8] Y. Wang, Y.-X. Wang, and A. Singh, “A theoretical analysis of noisy sparse subspace
clustering on dimensionality-reduced data,” arXiv preprint arXiv:1610.07650, 2016.
[9] W. B. Johnson and J. Lindenstrauss, “Extensions of lipschitz maps into a hilbert
space,” Contemporary mathematics, vol. 26, pp. 189–206, 1984.
[10] S. Dasgupta and A. Gupta, “An elementary proof of the johnson-lindenstrauss lemma,”
International Computer Science Institute, Technical Report, pp. 99–006, 1999.
[11] E. J. Candes and T. Tao, “Decoding by linear programming,” IEEE Transactions on
Information Theory, vol. 51, no. 12, pp. 4203–4215, 2005.
[12] E. J. Cand`es, “The restricted isometry property and its implications for compressed
sensing,” Comptes Rendus Mathematique, vol. 346, no. 9–10, pp. 589–592, 2008.
[13] R. Baraniuk, M. Davenport, R. Devore, and M. Wakin, “A simple proof of the restricted
isometry property for random matrices,” Constructive Approximation, vol. 28, no. 28,
pp. 253–263, 2015.
[14] D. L. Donoho, “Compressed sensing,” IEEE Transactions on Information Theory,
vol. 52, no. 4, pp. 1289–1306, 2006.
[15] E. J. Candes, J. Romberg, and T. Tao, “Robust uncertainty principles: exact signal
reconstruction from highly incomplete frequency information,” IEEE Transactions on
Information Theory, vol. 52, no. 2, pp. 489–509, 2006.
[16] S. Aeron, V. Saligrama, and M. Zhao, “Information theoretic bounds for compressed
sensing,” IEEE Transactions on Information Theory, vol. 56, no. 10, pp. 5111–5130,
2010.
[17] E. Candes and J. Romberg, “Sparsity and incoherence in compressive sampling,” In-
verse problems, vol. 23, no. 3, p. 969, 2007.
32
[18] Y. C. Eldar and G. Kutyniok, Compressed sensing: theory and applications. Cam-
bridge University Press, 2012.
[19] A. Eftekhari and M. B. Wakin, “New analysis of manifold embeddings and signal recov-
ery from compressive measurements,” Applied and Computational Harmonic Analysis,
vol. 39, no. 1, pp. 67–109, 2015.
[20] G. Kutyniok, A. Pezeshki, R. Calderbank, and T. Liu, “Robust dimension reduction, fu-
sion frames, and grassmannian packings,” Applied and Computational Harmonic Anal-
ysis, vol. 26, no. 1, pp. 64–76, 2009.
[21] G. Li and Y. Gu, “Restricted isometry property of gaussian random projection for
ﬁnite set of subspaces,” IEEE Transactions on Signal Processing (to appear), arXiv
preprint arXiv:1704.02109, 2017.
[22] A. Edelman, T. A. Arias, and S. T. Smith, “The geometry of algorithms with orthogo-
nality constraints,” SIAM Journal on Matrix Analysis and Applications, vol. 20, no. 2,
pp. 303–353, 1998.
[23] M. A. Davenport, P. T. Boufounos, M. B. Wakin, and R. G. Baraniuk, “Signal pro-
cessing with compressive measurements,” IEEE Journal of Selected Topics in Signal
Processing, vol. 4, no. 2, pp. 445–460, 2010.
[24] T. Blumensath and M. E. Davies, “Sampling theorems for signals from the union
of ﬁnite-dimensional linear subspaces,” IEEE Transactions on Information Theory,
vol. 55, no. 4, pp. 1872–1882, 2009.
[25] P. K. Agarwal, S. Har-Peled, and H. Yu, “Embeddings of surfaces, curves, and moving
points in euclidean space,” in Proceedings of the twenty-third annual symposium on
Computational geometry. ACM, 2007, pp. 381–389.
[26] A. Magen, “Dimensionality reductions that preserve volumes and distance to aﬃne
spaces, and their algorithmic applications,” Randomization and approximation tech-
niques in computer science, pp. 953–953, 2002.
[27] C. Jordan, “Essai sur la g´eom´etrie `a n dimensions,” Bulletin de la Soci´et´e math´ematique
de France, vol. 3, pp. 103–174, 1875.
[28] A. Gal´antai and H. C. J., “Jordan’s principal angles in complex vector spaces,” Nuner-
ical Linear Algebra with Applications, vol. 13, pp. 589–598, 2006.
[29] A. Bj¨orck and G. H. Golub, “Numerical methods for computing the angles between
linear subspaces,” Mathematics of Computation, vol. 27, pp. 579–594, 1973.
[30] K. R. Davidson and S. J. Szarek, “Local operator theory, random matrices and banach
spaces,” Handbook in Banach Spaces, pp. 317–366, 2001.
33
