Byexploringandinteractingwithanenvironment,rein-forcementlearningcansuccessfullydeterminetheoptimalfeedbackcontrollerwithrespecttothelong-termrewardsreceivedbyanagent[1],[2].Whereastheideaofdeterminingtheoptimalfeedbackcontrollerintermsofacostoversometimehorizonisstandardinthecontrolsliterature[3],reinforce-mentlearningisaimedatlearningthelong-termrewardsbyexploringthestatesandactions.Assuch,theagentdynamicsisnolongerexplicitlytakenintoaccount,butratherissubsumedbythedata.Moreover,eventherewardsneednotnecessarilybeknownapriori,butcanbeobtainedthroughexploration,aswell.Ifnoinformationabouttheagentdynamicsisavailable,however,anagentmightendupincertainregionsofthestatespacewhichmustbeavoidedwhileexploring.Avoidingsuchregionsofthestatespaceisreferredtoassafety.Safetyin-cludescollisionavoidance,boundary-transgressionavoidance,connectivitymaintenanceinteamsofmobilerobots,andothermandatoryconstraints,andthistensionbetweenexplorationThisworkofM.OhnishiwassupportedbyScandinavia-JapanSasakawaFoundation.M.OhnishiiswiththeSchoolofElectricalEngineering,KTHRoyalInstituteofTechnology,Stockholm,Sweden(e-mail:motoya@kth.se).L.WangandM.EgerstedtarewiththeSchoolofElectricalandCom-puterEngineering,GeorgiaInstituteofTechnology,Atlanta,GA(e-mail:liwang@gatech.edu;magnus@gatech.edu).G.NotomistaiswiththeSchoolofMechanicalEngineering,GeorgiaInstituteofTechnology,Atlanta,GA(e-mail:g.notomista@gatech.edu).andsafetybecomesparticularlypronouncedinrobotics,wheresafetyiscrucial.Inthispaper,weaddressthissafetyissue,byemployingmodellearningincombinationwithbarriercertiﬁcates.Inparticular,wefocusonlearningforasystemwithdiscrete-timenonstationaryagentdynamics.Nonstationaritycomes,forexample,fromfailuresofactuators,batterydegradations,orsuddenenvironmentaldisturbances.Theresultisamethodthatadaptstoatime-varyingagentdynamicsandsimultaneouslyextractsthedynamicstructurewithouthavingtoknowhowtheagentdynamicschangesovertime.Theresultingmodelwillbeusedforbarriercertiﬁcates.Moreover,certainconditionsarealsopresentedunderwhichthemonotonicimprovementofabarrier-certiﬁedfeedbackcontrollerisensured.Overthelastdecade,thesafetyissuehasbeenaddressedunderthenameofsafelearning,andplentyofsolutionshavebeenproposed[4]–[13].Toensuresafetywhileexploring,aninitialknowledgeoftheagentdynamics,somesafemaneuverortheirlong-termrewards,orateacheradvisingtheagentisnecessary[4],[14].Toobtainamodeloftheagentdy-namics,humanoperatorsmaymaneuvertheagentandrecorditstrajectories[12],[15],or,startingfromaninitialsafemaneuver,thesetofsafefeedbackcontrollerscanbeexpandedbyexploringthestates[4],[5].Itisalsopossiblethatanagentcontinuesexploringwithoutenteringthestateswithlowlong-termrewardsassociatedwithsomesafemaneuver(e.g.[16]).Duetotheinherentuncertainty,theworstcasescenario(e.g.possiblelowestrewards)istypicallytakenintoaccountwhenexpandingthesetofsafefeedbackcontrollers[13],[17].Toaddresstheissueofthisuncertaintyfornonlinear-modelestimationtasks,Gaussianprocessregression[18]isastrongtool,andmanysafelearningstudieshavetakenadvantageofitsproperty(e.g.[4],[6],[7],[10],[13]).Nevertheless,whentheagentdynamicsisnonstationary,theassumptionsoftenmadeinthesafelearningliteraturecannotholdanymore.Insuchcases,weneedtoadaptivelylearntheagentdynamicstomitigatetheeffectofanunexpectedviolationofsafety.Moreover,whentheagentdynamicsvaries,thelong-termrewardsassociatedwithstatesandcontrolsalsovary.Assuch,thelong-termrewardsmustalsobelearnedinanadaptivemanner.Thesearethecoremotivationsofthispaper.Toconstrainthestateswithinadesiredsaferegion,weemploycontrolbarrierfunctions(cf.[19]–[24]).Whentheaccuratemodeloftheagentdynamicsisavailable,controlbarriercertiﬁcatesensurethatanagentremainsinthesafestatesforalltimebyconstrainingtheinstantaneouscontrol2inputateachtime,andthatanagentoutsideofthesafestatesisforcedbacktosafety.Ausefulpropertyofcontrolbarriercertiﬁcatesisitsnon-conservativeness,i.e.,itmodiﬁesfeed-backcontrollerswhenviolationsofsafetyaretrulyimminent.Ontheotherhand,theglobaloptimalityofsolutionstotheconstrainedfeedback-controlleroptimizationisnecessarytoensurethemonotonicimprovementofafeedbackcontroller.Ourﬁrstcontributionofthispaperistoproposeadiscrete-timecontrolbarriercertiﬁcatewhichensurestheglobaloptimalityundersomemildconditions.Thisisanimprovementofthepreviouslyproposeddiscrete-timecontrolbarriercertiﬁcate[24],andhencehaswiderapplicability.Toadaptivelylearnanonlinearmodeloftheagentdy-namics,weemploykerneladaptiveﬁltering[25],whichisanadaptiveextensionofthekernelridgeregression[26],[27]orGaussianprocesses.Multikerneladaptiveﬁltering(cf.[28]–[32])isastate-of-the-artkerneladaptiveﬁltering,whichadaptivelyachievesacompactrepresentationofanonlinearfunctioncontainingmulti-component/partially-linearfunctions,andhasamonotoneapproximationpropertyforapossiblyvaryingtargetfunction.Oursecondcontributionofthispaperistoregardamodeloftheagentdynamicsasacombinationofmultiplestructuralcomponents,andapplymultikerneladaptiveﬁlteringtosimultaneouslylearnamodelandthedynamicstructure.Thekeyideaistheuseofanadaptivesparseoptimizationtoextracttrulyactivestructuralcomponents.Lastly,the(action-)valuefunction,whichapproximatesthelong-termrewards,needstobeadaptivelyestimatedundernonstationarity.Therefore,wewishtofullyexploitthenon-linearadaptiveﬁlteringtechniques.Actually,manyattemptshavebeenmadetoapplytheonlinelearningtechniquestoreinforcementlearning(see[33]–[38]).Asaresult,so-calledoff-policyapproaches,whichareconvergentevenwhensamplesarenotgeneratedbythetargetfeedbackcontroller(see[34]),havebeenproposed.However,whatdifferentiatesthe(action-)valuefunctionapproximationfromanordinarysupervisedlearning,whereinput-outputpairsaregiven,isthattheoutputofthetrue(action-)valuefunctionisnotexplicitlyobserved.Ourﬁnalcontributionofthispaperis,byassumingdeterministicagentdynamics,toappropriatelyreformulatethe(action-)valuefunctionapproximationproblemsothatanykernel-basedlearning,whichiswidely-studiednonparametrictechnique,becomesstraightforwardlyapplicable.Totesttheefﬁcacy,theproposedlearningframeworkisimplementedonabrushbot,whosedynamicsisunknownandhighlycomplex,andweconductanexperimentintherealworld.Thisischallengingduetomanyuncertaintiesandlackofsimulatorsoftenusedinapplicationsofreinforcementlearningtorobotics(see[39]forreinforcementlearninginrobotics).II.BACKGROUNDMATERIALWeintroducenotation,relatedworksonsafelearning,andbackgroundmaterial,includingcontrolbarrierfunctions,andkerneladaptiveﬁltering.A.NotationThroughout,R,N,andN∗arethesetsofrealnumbers,nonnegativeintegers,andpositiveintegers,respectively.Givenanypairofintegersm,n∈Nsuchthatm≤n,wedenotetheset{m,m+1,···,n}bym,n.Thestateandthecontrolinputattimeinstancen∈Narerepresentedbyxn∈X⊂Rnx,andun∈U⊂Rnu,wherenx,nu∈N∗,respectively.ThefunctionsR:X×U→R,Vφ:X→R,andQφ:X×U→Rrepresenttheinstantaneous-rewardfunction,thevaluefunction,andtheaction-valuefunctionassociatedwithafeedbackcontrollerφ:X→U,respectively.Theestimateofafunctionϕattimeinstancenisdenotedbyϕ(n).Wedenotethediscountfactorforthe(action-)valuefunctionapproximationbyγ∈(0,1).Themetricprojectionofapointx∈RLontoagivenclosedconvexsetC⊂RLisdeﬁnedbyPC(x):=argminy∈C(cid:5)x−y(cid:5)RL.Let(cid:5)·(cid:5)Hbethenorminducedbytheinnerproduct(cid:6)·,·(cid:7)Hinaninner-productspaceH.Inparticular,deﬁne(cid:6)x,y(cid:7)RL:=xTyforL-dimensionalrealvectorsx,y∈RL,and(cid:5)x(cid:5)RL:=(cid:1)(cid:6)x,x(cid:7)RL,where(·)Tstandsfortransposition.Thevectoreidenotestheunitvectorwithoneattheithpositionandzeroselsewhere.Wedenotethenull(zero)functionby0,andtheemptysetby∅.B.RelatedWorkonSafeLearningTheprimaryfocusofthispaperisthesafetyissuewhileexploring.Typically,someinitialknowledges,suchasaninitialsafecontrollerandamodeloftheagentdynamics,arerequiredtoaddressthesafetyissuewhileexploring,andmodellearningisoftenemployedtogether.Inthissection,weintroducesomerelatedworkonsafelearningwithmodellearning,andpresentabriefsummaryofexisting(action-)valuefunctionapproximationtechniques.1)ModelLearningforSafeManeuver:Therecentworkin[13],[7],and[4]assumeaninitialconservativesetofsafecontrollers,whichisgraduallyexpandedasmoredatabecomeavailable.Theseapproachesaredesignedforstationaryagentdynamics,andGaussianprocessesareemployedtoobtaintheconﬁdenceintervalofthemodel.Toensuresafety,controlbarrierfunctionsandcontrolLyapunovfunctionsareemployedin[13]and[4],respectively.Ontheotherhand,theworkin[10]usesatrajectoryoptimizationbasedontherecedinghorizoncontrolandmodellearningbyGaussianprocesses,whichiscomputationallyexpensivewhenthemodelishighlynonlinear.Incontrasttotheseapproaches,ourapproachconsidersapossiblynonstationaryagentdynamicsandemploysadaptivemodellearningtoadaptivelyestimatethetime-varyingsetofsafecontrollers.2)(Action-)valueFunctionApproximation:Weintroduce,brieﬂy,ideasofexisting(action-)valuefunctionapproxima-tiontechniques.TheBellmanequationdeﬁnesthetemporaldifferenceerror:Vφ(n)(xn)−Tφ(Vφ(n)(xn)),(1)whereTφ(Vφ(n)(xn)):=γVφ(n)(xn+1)+R(xn,φ(xn)),attimeinstancen+1.Sincetheoutputsofthevaluefunction3isnotdirectlyobserved,supervisedlearningmethodscannotbedirectlyapplied.Theupdateruleofthetemporaldifferencelearning[40],[41]isgivenbyVφ(n+1)(xn)=λ(cid:2)Tφ(Vφ(n)(xn))−Vφ(n)(xn)(cid:3),whereλisthestepsize.Notethattheoutputdependsonthecurrentestimatorofthevaluefunctioninthisupdaterule.Ontheotherhand,so-calledoff-policymethods(e.g.theresiduallearning[33],theleastsquarestemporaldifferencealgorithm[42],andthegradienttemporaldifferencelearning[34],[43])havebeenalsoproposed.Theseapproachesareprovedtoconvergeundercertainconditionsaslikemostsupervisedlearningmethodsevenwhensamplesarenotgeneratedbythecontrollerφ.Theleastsquarestemporaldifferencealgorithmhasbeenextendedtokernel-basedmethods[37],includingGaussianprocesses(e.g.GaussianprocesstemporaldifferenceandGaussianprocessSARSA[35]).Unlikethesekernel-basedmethods,ourapproachexplicitlydeﬁnesaso-calledreproducingkernelHilbertspacesothatthe(action-)valuefunctionapproximationbecomesanordinarykernel-basedsupervisedlearning,andthatanykernel-basedmethodcanbestraightforwardlyappliedwithoutmodifying.Wereferthereadersto[44]forasummaryofparametricvaluefunctionapproximationtechniques.C.Discrete-timeControlBarrierFunctionToavoidcertainregionsofthestatespace,weemploycon-trolbarrierfunctions,whichonlymodifyfeedbackcontrollerswhensafetyisabouttobeviolated.DeﬁnethesetofsafestatesC⊂XasC:={x∈X|B(x)≥0},(2)whereB:X→Riscalledthediscrete-timeexponentialbarrierfunction.Deﬁnition1([24,Deﬁnition4]).AmapB:X→Risadiscrete-timeexponentialcontrolbarrierfunctionifthereexistsacontrolinputun∈UsuchthatB(xn+1)−B(xn)≥−ηB(xn),∀n∈N,0<η≤1.(3)NotethatweintentionallyremovedtheconditionB(x0)≥0originallypresentedin[24,Deﬁnition4].Then,theforwardinvarianceandasymptoticstabilityofthesetofsafestatesareensuredbythefollowingproposition.Proposition1.ThesetCdeﬁnedin(2)forsomediscrete-timeexponentialcontrolbarrierfunctionB:X→RisforwardinvariantwhenB(x0)≥0,andisasymptoticallystablewhenB(x0)<0.Proof.See[24,Proposition4]fortheproofofforwardinvari-ance.ThesetC⊂Xisasymptoticallystableaslimn→∞B(xn)≥limn→∞(1−η)nB(x0)=0,wheretheinequalityholdsfrom[24,Proposition1].Proposition1impliesthatanagentremainsinthesetofsafestatesdeﬁnedin(2)foralltimeifB(x0)≥0and(3)aresatisﬁed,andtheagentoutsideofthesetofsafestatesisforcedbacktosafety.D.MultikernelAdaptiveFiltering–UseofSparseOptimizationAswewillseelaterinSectionIII-A,barriercertiﬁedcontrollersareefﬁcientlycomputedwhentheagentdynamicsisafﬁnetocontrolinputs.Here,weintroducetheideaofmultikerneladaptiveﬁltering,whichwillbeusedtosimultane-ouslylearnamodelandthedynamicstructure.Weemphasizethatoneadvantageofkernel-basedmethodsistheirconvex-analyticformulations.Kerneladaptiveﬁlteringisatoolofnonlinearfunctionestimation.Duetoitscelebratedpropertyofreproducingkernels,theframeworkoflinearadaptiveﬁlteringisdirectlyappliedtononlinearfunctionestimationtasksinapossiblyinﬁnite-dimensionalfunctionalspace,namelyareproducingkernelHilbertspace.Deﬁnition2([45,page343]).GivenanonemptysetZandHwhichisaHilbertspacedeﬁnedinZ,thefunctionκ(z,w)ofz,w∈ZiscalledareproducingkernelofHif1)foreveryw∈Z,κ(z,w)asafunctionofz∈ZbelongstoH,and2)ithasthereproducingproperty,i.e.,thefollowingholdsforeveryw∈Zandeveryϕ∈Hthatϕ(w)=(cid:6)ϕ,κ(·,w)(cid:7)H.IfHhasareproducingkernel,HiscalledaReproducingKernelHilbertSpace(RKHS).OneofthecelebratedexamplesofkernelsistheGaussiankernelκ(z,w):=1(2πσ2)L/2exp(cid:4)−(cid:5)z−w(cid:5)2RL2σ2(cid:5),z,w∈RL,σ>0.Itiswell-knownthattheGaussianreproducingkernelHilbertspacehasuniversality[46],i.e,anycontinuousfunctiononeverycompactsubsetofRLcanbeapproximatedwithanarbitraryaccuracy.Anotherwidelyusedkernelisthepolynomialkernelκ(z,w):=(zTw+c)d,c≥0,d∈N∗.Multikerneladaptiveﬁltering[28]exploitsmultiplekernelstoconductlearninginthesumspaceofRKHSsassociatedwitheachkernel.LetM∈N∗bethenumberofkernelsemployed.Denote,byDm,n:={κm(·,˜zm,j)}j∈1,rm,n,m∈1,M,rm,n∈N∗,thetime-dependentsetoffunctions,referredtoasadictionary,attimeinstancenforthemthkernelκm(·,·).Thecurrentestimatorψ(n)nisevaluatedatthecurrentinputzn,inalinearform,asψ(n)n(zn):=hTnkn(zn)=M(cid:6)m=1hTm,nkm,n(zn),(4)wherehn:=[hT1,n,hT2,n,···,hTM,n]T:=[h1,h2,···,hrn]∈Rrn,rn:=(cid:7)m∈1,Mrm,n,isthecoefﬁcentvector,andkn(zn):=(cid:2)k1,n(zn)Tk2,n(zn)T···kM,n(zn)T(cid:3)T∈Rrn,km,n(zn):=(cid:8)κm(zn,˜zm,1),κm(zn,˜zm,2),···,κm(cid:9)zn,˜zm,rm,n(cid:10)(cid:11)T∈Rrm,n.AnillustrativeexampleofanestimatorisgiveninFigure1,whereψnistheunknownfunctiontobe4Fig.1.Anillustrationofmultikerneladaptiveﬁltering.Byemployingmultiplekernelfunctions,acompactrepresentationoftheunknownfunctionψntobeestimatedisachieved.Gaussiankernelsκm,m∈1,3,withdifferentscaleparameters(i.ewidth)areemployedinthisexample.estimatedattimeinstancen.Tocurvethegrowthofdictionarysizesandobtainacompactrepresentationoftheunknownfunctiontobeestimated,asparseoptimizationcanbeapplied.Givenaninput-outputpair(zn,δn),zn∈RL,δn∈R,attimeinstancen,deﬁneaclosedconvexsetCℓ⊂Rrn,ℓ∈n−sn+1,n⊂N,sn∈N∗asCℓ:={h∈Rrn||hTkn(zℓ)−δℓ|≤ρ},ρ≥0,(5)whichisasetofcoefﬁcientvectorhsatisfyinginstantaneous-error-zerowithaprecisionparameterρ.ThecostforasparseoptimizationattimeinstancenisdeﬁnedbyΘn(h):=12n(cid:6)ℓ=n−sn+11sndist2(h,Cℓ)+µ(cid:5)h(cid:5)1,(6)wheredist(h,Cℓ):=mina∈Cℓ(cid:5)h−a(cid:5)Rrn,andtheℓ1-normregularization(cid:5)h(cid:5)1:=(cid:7)rni=1|hi|withaparameterµ≥0promotessparsityofh.Theupdateruleoftheadaptiveproximalforward-backwardsplitting[47],whichisanadaptiveﬁlteringdesignedforsparseoptimizations,forthecost(6)isgivenbyhn+1=proxλµ(cid:12)(1−λ)I+λn(cid:6)ℓ=n−sn+11snPCℓ(cid:13)(hn),(7)whereλ∈(0,2)isthestepsize,Iistheidentityoperator,andproxλµ(h)=rn(cid:6)isgn(hi)max{|hi|−λµ,0}ei,(8)wheresgn(·)isthesignfunction.Then,thestrictlymonotoneapproximationproperty[47]:(cid:5)hn+1−h∗n(cid:5)Rrn<(cid:5)hn−h∗n(cid:5)Rrn,∀h∗n∈Ωn:=argminh∈RrnΘn(h),holdsifhn/∈Ωn(cid:11)=∅.Undernonstationarity,thismonotoneapproximationpropertytellsthat,nomatterhowthetargetvector(function)changes,wecanatleastguaranteethatthecurrentestimatorhngetsclosertothecurrenttargetvector.Thispropertyplaysakeyroleinthesafety-awareadaptivereinforcementlearningpresentedinSectionIII.Assumethatthedictionaryisﬁxedforn≥NforsomeN∈N,i.e.,Dm,n=Dm,N,∀n≥N,m∈1,M,andthatΩ:=(cid:14)n≥NΩnisnonempty.Then,(cid:5)hn+1−h∗(cid:5)RrN<Fig.2.Anillustrationofmonotoneapproximationproperty.TheestimatehnmonotonicallyapproachestothesetΩofoptimalvectorsh∗bysequentiallyminimizingthedistancebetweenhnandΩn.(cid:5)hn−h∗(cid:5)RrN,∀h∗∈Ω,n≥N,holdsifhn/∈Ωn(see[48]fordetail).ThisisillustratedinFigure2.DictionaryConstruction:Byusingsparseoptimizations,nonactivestructuralcomponentsrepresentedbysomekernelfunctionsarepruned,andthedictionaryisreﬁnedastimegoesby.Ontheotherhand,weemploytwonoveltyconditionswhenaddingthekernelfunctions{κm(·,zn)}m∈1,Mtothedictionary:(i)themaximum-dictionary-sizeconditionrn≤rmax,rmax∈N∗,(9)i.e.,themaximumdictionarysizeisrmax+M,and(ii)thelarge-normalized-errorcondition|δn−ψ(n)n(zn)|2>ǫ|ψ(n)n(zn)|2,ǫ≥0.(10)InSectionIII-B,weapplymultikerneladaptiveﬁlteringtoextractthedynamicstructureoftheagentdynamics.III.SAFETY-AWAREADAPTIVEREINFORCEMENTLEARNINGInthissection,wepresentasafety-awareadaptiverein-forcementlearningframework.Throughout,weconsiderthefollowingdiscrete-timedeterministicnonlinearmodelofthenonstationaryagentdynamics,xn+1−xn=pn(xn,un)+fn(xn)+gn(xn)un,(11)wherepn:X×U→Rnx,fn:X→Rnx,gn:X→Rnx×nu.Hereafter,weregardX×UasthesameasZ⊂Rnx+nuundertheone-to-onecorrespondencebetweenz:=[xT,uT]T∈Zand(x,u)∈X×U,ifthereisnoconfusion.A.SafeManeuver:Discrete-timeControlBarrierFunctionGivenadiscrete-timeexponentialcontrolbarrierfunctionBand0<η≤1,thebarriercertiﬁedsafecontrolspaceattimeinstancenisdeﬁneasSn(x):={un∈U|B(xn+1)−B(xn)≥−ηB(xn)}.(12)FromProposition1,thesetCdeﬁnedin(2)isforwardinvariantandasymptoticallystableifun∈Sn(xn)foralln∈N∗.Aspointedoutin[24],Sn(xn)⊂Uisnotaconvexsetin5general.Toensureglobaloptimalityofthesolutionstotheconstrainedfeedback-controlleroptimizationwhenupdatingafeedbackcontroller(seeSectionIII-D),wemakethefollowingmoderateassumptionsAssumption1.1)Control-afﬁneagentdynamics:Theagentdynamicsiscontrolafﬁne,i.e.,pn=0.2)ExistenceofLipschitzcontinuousgradientofthebarrierfunction:GivenR:={(1−t)xn+t(fn(xn)+gn(xn)u)|t∈[0,1],u∈U},thereexistsaconstantν≥0suchthatthegradientofthediscrete-timeexponentialcontrolbarrierfunctionB,denotedby∂B(x)∂x,satisﬁesthat(cid:15)(cid:15)(cid:15)(cid:15)∂B(a)∂x−∂B(b)∂x(cid:15)(cid:15)(cid:15)(cid:15)Rnx≤ν(cid:5)a−b(cid:5)Rnx,∀a,b∈R.Then,thefollowingtheoremholds.Theorem1.UnderAssumptions1.1and1.2,(3)issatisﬁedifunsatisﬁesthefollowing:∂B(xn)∂x(fn(xn)+gn(xn)un)≥−ηB(xn)+ν2(cid:5)fn(xn)+gn(xn)un(cid:5)2Rnx.(13)Moreover,(13)deﬁnesaconvexconstraintforun.Proof.SeeAppendixA.Theorem1essentiallyimpliesthat,evenwhenthegradientofBalongtheshiftofxndecreasessteeply,(3)followsif(13)issatisﬁed.FromTheorem1,thesetˆSn,deﬁnedasˆSn:={un∈U|∂B(xn)∂x(fn(xn)+gn(xn)un)≥−ηB(xn)+ν2(cid:5)fn(xn)+gn(xn)un(cid:5)2Rnx}⊂Sn,(14)isconvex.Aswitnessedintheliteratures(e.g.[22]),anagentmightencounterdeadlocksituations,wheretheconstrainedcontrolkeepstheagentremaininthesamestate,whencontrolbarriercertiﬁcatesareemployed.Itisevenpossiblethatthereisnosafecontroldrivingtheagentfromthosestates.However,anelaborativedesignofcontrolbarrierfunctionsremediesthisissue,asshowninthefollowingexample.Example1.Iftheagentisnonholonomic,turninginwardsaferegionswhenapproachingtheirboundarymightbeinfeasible.Toreducetheriskofsuchdeadlocksituations,controlbarrierfunctionsmaybedesignedasB(x)=˜B(x)−υΓ(cid:4)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)θ−atan2(cid:17)∂˜B(x)∂y,∂˜B(x)∂x(cid:18)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:5),υ>0,(15)wherethestatex=[x,y,θ]consistsoftheXpositionx,theYpositiony,andtheorientationθofanagentfromtheworldframe,{x∈X|˜B(x)≥0}istheoriginalsaferegion,andΓisastrictlyincreasingfunction.ThiscontrolbarrierfunctionFig.3.Anillustrationofhowanonholonomicagentavoidsdeadlocks.Whentheorientationoftheagentisnotconsidered(i.e,˜B(x)isthebarrierfunction),theremightbenosafecontroldrivingtheagentfromthosestatesastheleftﬁgureshows.Bytakingintoaccounttheorientation(i.e,B(x)isthebarrierfunction),theagentturnsinwardthesaferegionbeforereachingitsboundariesastherightﬁgureshows.forcestheagentturninwardtheoriginalsaferegionbeforereachingitsboundaries.AnillustrationofExample1isgiveninFigure3.B.AdaptiveModelLearning–CaptureMeaningfulStructureWehaveseen,intheprevioussubsection,thatcontrol-afﬁnemodelsaredesirabletoobtainbarriercertiﬁedcontrollersefﬁciently.Here,weproposeamodellearningtechniquethatalsolearnsthedynamicstructure.Weassumethatnx=1forsimplicity(wecanemploynxestimatorsifnx>1).Deﬁneψn(xn,un):=pn(xn,un)+fn(xn)+gn(xn)un,whereψn:Z→R.Wesupposethatpn∈Hp,fn∈Hf,andgn∈Hg,whereHp,Hf,andHgareRKHSs,therebybeingabletoemploykernel-basedalgorithmstolearnamodel.However,becausethedomainsofpn,fn,andgnaredifferent,learningofthefunctionψnisinfeasibleatthisstage.LetHubetheRKHSassociatedwiththereproducingkernelκ(u,v):=uTv,u,v∈U,i.eapolynomialkernelwithc=0andd=1,andHc:={ϕ:U→R|∃α∈R,ϕ(u)=α}=span{1},where1:U→{1},thesetofconstantfunctions.Proposition2.ThespaceHcisanRKHSassociatedwiththereproducingkernelκ(u,v)=1,∀u,v∈U,withtheinnerproductdeﬁnedas(cid:6)α1,β1(cid:7)Hc:=αβ,α,β∈R.Proof.SeeAppendixB.Then,thefollowingpropositionimpliesthatψncanbeapproximatedinthesumspaceofRKHSsdeﬁnedlater.Proposition3([49,Theorem13]).LetH1andH2betwoRKHSsassociatedwiththereproducingkernelsκ1andκ2.ThenthecompletionofthetensorproductofH1andH2,denotedbyH1⊗H2,isanRKHSassociatedwiththereproducingkernelκ1⊗κ2.FromPropositions2and3,wecannowassumethatˆfn∈Hf⊗Hcandˆgn∈Hg⊗Hu,whereˆfn:Z→R,ˆfn(x,u):=fn(x),andˆgn:Z→R,ˆgn(x,u):=gn(x)u,respectively.Assuch,ψncanbeapproximatedintheRKHSHψ:=Hp+Hf⊗Hc+Hg⊗Hu.ByviewingtheRKHSHψasasumspaceofM∈N∗RKHSseachofwhichisassociatedwiththereproducingkernelκm,m∈1,M,thecurrentestimatorψ(n)ncanbeevaluatedatthecurrentinputzn:=[xTn,uTn]T∈Zasin(4).6Algorithm1AdaptiveModelLearningRequirement:λ∈(0,2),ρ≥0(precisionparameter),rmax>0(maximum-dictionary-sizecondition)ǫ≥0(large-normalized-errorcondition),µ≥0(sparsityparame-ter),sn∈N∗(datasize),x0∈X,andu0∈UInitialization:D−m,0=∅,hm,0=[],m∈1,MOutput:p(n)n(zn),f(n)n(xn),andg(n)n(xn)⊲(16),(17),and(18)forn∈Ndo-Receivexn,xn+1∈Xandun∈U(δn:=xn+1−xn)-Checkifthenoveltyconditionsaresatisﬁed⊲(9),(10)ifNoveltyconditionsaresatisﬁedthenDictionaryincrementform∈1,M:Dm,n=D−m,n∪{κm(·,[xTn,uTn]T)}Redeﬁnehm,nas[hTm,n,0]TendifUpdatehn⊲(5),(7),and(8)Eliminatetheobsoletedictionaryelementsform∈1,M:D−m,n+1:={κm(·,˜zm,j)∈Dm,n|hTm,n+1ej(cid:11)=0}Discardthezeroentriesofhm,n+1endforSupposethattheRKHSsHp,Hf,andHgcanbeexpressedasthesumspacesofMp,Mf,Mg∈N∗RKHSs,whereMp+Mf+Mg=M.Theevaluationsofthecurrentestimatorsp(n)nandf(n)natthecurrentinputszn:=[xTn,uTn]Tandxnarethengivenbyp(n)n(zn)=Mp(cid:6)m=1hTm,nkm,n(zn),(16)f(n)n(xn)=ˆf(n)n(zn)=Mp+Mf(cid:6)m=Mp+1hTm,nkm,n(zn).(17)Inordertousethelearnedmodelincombinationwithcontrolbarrierfunctions,eachentryofthevectorg(n)n(xn)(thecurrentestimateofgn(xn))isrequired.Assume,withoutlossofgenerality,that{ei}i∈1,nu⊂U(thisisalwayspossibleforU(cid:11)=∅bytransformingthecoordinateofcontrolinputsandreducethedimensionnuifnecessary).Then,theithentryofthevectorg(n)n(xn)isgivenbyg(n)n(xn)ei=ˆg(n)n(xn,ei)=M(cid:6)m=Mp+Mf+1hm,nkm,n([xTn,eTi]T).(18)TheproposedmodellearningalgorithmissummarizedinAlgorithm1.Weconcludethissubsectionbygivingthefollowingremarks.Remark1.Thefollowingtheoremensuresthatψncanbeuniquelydecomposedintopn,ˆfn,andˆgn.Theorem2.AssumethatXandUhavenonemptyinteriors.AssumealsothatHpisaGaussianRKHS.Then,HψisthedirectsumofHp,Hf⊗Hc,andHg⊗Hu,i.e,theintersectionofanytwooftheRKHSsHp,Hf⊗Hc,andHg⊗Huis{0}.Proof.SeeAppendixC.Inmostofthekernel-basedmethodssuchasGaussianpro-cessesandthekernelrecursiveleastmeansquaresalgorithm[50],tractabilityoftheinnerproductbetweenkernelfunctionsisanessentialproperty(e.g.forcomputingtheGrammatrix).InasumspaceofRKHSs,theinnerproducthasnoclosedformingeneral.Nevertheless,fromTheorem2,theinnerproductiscomputedas(cid:6)ψ1,ψ2(cid:7)Hψ=(cid:6)p1,p2(cid:7)Hp+(cid:19)ˆf1,ˆf2(cid:20)Hf⊗Hc+(cid:6)ˆg1,ˆg2(cid:7)Hg⊗Hu,ψi:=pi+ˆfi+ˆgi∈Hψ,i∈1,2.Remark2.Byusingasparseoptimizationforthecoefﬁcientvectorhn∈Rrn,wewishtoextractastructureofthemodel.Inthepresentstudy,thetermp(n)nisdesiredtobedroppedoff,whenthetrueagentdynamicsiscontrolafﬁne.Toeffectivelyachieveacompactrepresentationofthemodel,itmightberequiredtoappropriatelyweighthekernelfunctionstoincludesomepreferencesonastructureofthemodel.Thefollowingpropositionimpliesthattheresultingkernelsarestillreproducingkernels.Proposition4([29,Theorem2]).Letκ:Z×Z→RbethereproducingkernelofanRKHS(H,(cid:6)·,·(cid:7)H).Then,τκ(z,w),z,w∈Zforarbitraryτ>0isthereproducingkerneloftheRKHS(Hτ,(cid:6)·,·(cid:7)Hτ)withtheinnerproduct(cid:6)z,w(cid:7)Hτ:=τ−1(cid:6)z,w(cid:7)H,z,w∈Z.C.AdaptiveReinforcementLearningWereformulatethe(action-)valuefunctionapproximationproblemtoenabletodirectlyapplyanykernel-basedmethod,includingmultikerneladaptiveﬁltering.TheBellmanequationofafeedbackcontrollerφ:X→Uforthevaluefunctionisgivenin(1).Assumethetargetfeedbackcontrollerφisdeterministic,andletHVbeanRKHScontainingthevaluefunctionVφ.Byslightlymodifying(1),weobtainVφn(xn)−γVφn(xn+1)=Rn(xn,φ(xn)),(19)wherethevaluefunctionandtheinstantaneous-rewardfunc-tionarenowtime-dependent.Deﬁneafunctionψn:X2→R,whereR2nx⊃X2=X×X,asψn([xT,yT]T):=Vφn(x)−γVφn(y),∀x,y∈X.(20)Then,(19)isreformulatedasψn([xTn,xTn+1]T)=Rn(xn,φ(xn)).Assuch,solvingtheBellmanequationcomesdowntotheiterativenonlinearfunctionestimationwiththeinput-outputpairs{([xTn,xTn+1]T,Rn(xn,φ(xn)))}n∈N.Inthecaseoftheaction-valuefunction,theBellmanequationofafeedbackcontrollerφ:X→UisgivenbyQφn(xn,un)=γQφn(xn+1,φ(xn+1))+Rn(xn,un),(21)whereQφn∈HQdenotesthetrueaction-valuefunctionwithrespecttoφ,containedinanRKHSHQ,attimeinstancen.7Bydeﬁningafunctionψn:Z2→R,whereR2(nx+nu)⊃Z2=Z×Z,asψn([xT,uT,yT,vT]T):=Qφn(x,u)−γQφn(y,v),x,y∈X,u,v∈U,theBellmanequationin(21)issolvedviaiterativenonlinearfunctionestimationwiththeinput-outputpairs{([xTn,uTn,xTn+1,φ(xn+1)T]T,Rn(xn,un))}n∈N.Theorem3.SupposethatHQisanRKHSassociatedwiththereproducingkernelκQ(·,·):Z×Z→R.Then,Hψ:={ϕ|ϕ([zT,wT]T)=ϕQ(z)−γϕQ(w),γ∈(0,1)ϕQ∈HQ,z,w∈Z},isalsoanRKHSwiththeinnerproductdeﬁnedby(cid:6)ϕ1,ϕ2(cid:7)Hψ:=(cid:19)ϕQ1,ϕQ2(cid:20)HQ,(22)ϕi([zT,wT]T):=ϕQi(z)−γϕQi(w),∀z,w∈Z,i∈1,2.ThereproducingkerneloftheRKHSHψisgivenbyκ([zT,wT]T,[˜zT,˜wT]T):=(cid:9)κQ(z,˜z)−γκQ(z,˜w)(cid:10)−γ(cid:9)κQ(w,˜z)−γκQ(w,˜w)(cid:10),z,w,˜z,˜w∈Z.Proof.SeeAppendixD.FromTheorem3,anykernel-basedmethodcanbeap-pliedbyassumingthatψn∈Hψ.Supposethatmultikerneladaptiveﬁlteringisemployed.GivenasumspaceofMRKHSseachofwhichisassociatedwiththereproducingkernelκQm,m∈1,M,wedeﬁnethereproducingker-nelsκm([zT,wT]T,[˜zT,˜wT]T):=(cid:9)κQm(z,˜z)−γκQm(z,˜w)(cid:10)−γ(cid:9)κQm(w,˜z)−γκQm(w,˜w)(cid:10),m∈1,M,andtheircorre-spondingRKHSsasinTheorem3.ProvidedthedictionariesDm,n:={κm(·,[˜zTm,j,˜wTm,j]T)}j∈1,rm,n,m∈1,M,theestimatorψ(n)nisthenexpressedasψ(n)n([zT,wT]T):=hTnkn([zT,wT]T)=M(cid:6)m=1hTm,nkm,n([zT,wT]T),z,w∈Z,(23)wherekm,n([zT,wT]T)Tej:=κm(cid:9)[zT,wT]T,[˜zTm,j,˜wTm,j]T(cid:10),j∈1,rm,n,fromwhichweobtaintheestimatoroftheaction-valuefunctionQφn(n)∈HQasQφn(n)(z):=hTnkQn(z)=M(cid:6)m=1hTm,nkQm,n(z),(24)wherekQn(z):=(cid:2)kQ1,n(z)TkQ2,n(z)T···kQM,n(z)T(cid:3)T∈Rrn,andkQm,n(z)Tej:=κQm(cid:9)z,˜zTm,j(cid:10)−γκQm(z,˜wm,j),j∈1,rm,n.Resultingaction-valuefunctionapproximationalgorithmissummarizedinAlgorithm2.Remark3.Employingtheaction-valuefunctionenablestouserandomcontrolinputsinsteadofthetargetfeedbackcontrollerφforexploration.Algorithm2AdaptiveAction-valueFunctionApproximationAlgorithmRequirement:Assumption1,κQmdeﬁnedin(26),λ∈(0,2),ρ≥0(precisionparameter),rmax>0(maximum-dictionary-sizecondition)ǫ≥0(large-normalized-errorcondition),µ≥0(sparsityparameter),sn∈N∗(datasize),x0∈Xandu0∈UInitialization:D−m,0=∅,hm,0=[],m∈1,MOutput:Qφn(n)(zn)⊲(24)forn∈Ndo-Receivexn,xn+1∈X,andδn:=Rn(xn,un)∈R-Obtainφ(xn+1)∈ˆSn(xn+1)⊲Algorithm3ifRandomExplorationthenGeneraterandomcontrolinputun+1∈ˆSn(xn+1)⊲Algorithm1,Theorem1elseun+1=φ(xn+1)⊲Algorithm3endif-Checkifthenoveltyconditionsaresatisﬁed⊲(9),(10)ifNoveltyconditionsaresatisﬁedthenDictionaryincrementform∈1,M:Dm,n=D−m,n∪{κm(·,[xTn,uTn,xTn+1,φ(xn+1)T]T)}Redeﬁnehm,nas[hTm,n,0]TendifUpdatehn⊲(5),(7),and(8)Eliminatetheobsoletedictionaryelementsform∈1,M:D−m,n+1:={κm(·,[˜zTm,j,˜wTm,j]T)∈Dm,n|hTm,n+1ej(cid:11)=0}Discardthezeroentriesofhm,n+1endforRemark4.Whenthecoefﬁcientvectorhnfortheestimatorψ(n)nin(23)ismonotonicallyapproachingtoaoptimalpointh∗intheEuclideannormsense,soisthecoefﬁcientvectorforthe(action-)valuefunctionbecausethesamecoefﬁcientvectorisusedtoestimateψnandQφn(see(23)and(24)).Supposeweemployamethodinwhichψ(n)nismonotonicallyapproachingtoaoptimalfunctionψ∗nintheHilbertiannormsense.Then,thefollowingcorollaryimpliesthatanestimatoroftheaction-valuefunctionalsosatisﬁesthemonotonicity.Corollary1.LetHψ∋ψ(n)n(z,w):=Qφn(n)(z)−γQφn(n)(w)andHψ∋ψ∗n(z,w):=Qφn∗(z)−γQφn∗(w),z,w∈Z,whereQφn(n),Qφn∗∈HQ.Then,ifψ(n)nisapproachingtoψ∗nintheHilbertiannormsense,i.e.,(cid:15)(cid:15)(cid:15)ψ(n+1)n−ψ∗n(cid:15)(cid:15)(cid:15)Hψ≤(cid:15)(cid:15)(cid:15)ψ(n)n−ψ∗n(cid:15)(cid:15)(cid:15)Hψ,itholdsthat(cid:15)(cid:15)(cid:15)Qφn(n+1)−Qφn∗(cid:15)(cid:15)(cid:15)HQ≤(cid:15)(cid:15)(cid:15)Qφn(n)−Qφn∗(cid:15)(cid:15)(cid:15)HQ.Proof.SeeAppendixE.D.Safety-awareFeedback-controllerUpdateGivencurrentfeedbackcontrollerφ:X→U,assumethattheaction-valuefunctionQφnwithrespecttoφattimeinstance8Algorithm3Feedback-controllerUpdateRequirement:Assumption1,Nf∈N∗(updatefrequency)Initialization:φ=0forn∈NdoifnmodNf=0thenUpdateφ⊲(27),Algorithm1,andTheorem1φ=φ+endifendfornisavailable.Then,thefeedbackcontrollerφ+givenbyφ+(x):=argmaxu∈ˆSn(x)(cid:8)Qφn(x,u)(cid:11),(25)whereˆSn(x)⊂Uisdeﬁnedin(14),iswell-known(e.g.[51])tosatisfythatQφn(x,φ(x))≤Qφ+n(x,φ+(x)),whereQφ+nistheaction-valuefunctionwithrespecttoφ+attimeinstancen.Inpractice,weusetheestimatorofQφnbecausetheexactfunctionQφnisunavailable.Forexample,theaction-valuefunctionisestimatedoverNf∈N∗iterations,andthefeedbackcontrollerisupdatedeveryNfiterations.Toobtainanalyticalsolutionsfor(25),wefollowtheargumentsin[35].SupposethatQφn(n)isgivenby(24).WedeﬁnethereproducingkernelκQm,m∈1,MasthetensorkernelgivenbyκQm([xT,uT]T,[yT,vT]T):=κxm(x,y)κum(u,v),(26)whereκum(u,v)is,forexample,deﬁnedbyκum(u,v):=1+14(uTv).Then,(25)becomesφ+(x):=argmaxu∈ˆSn(x)(cid:8)hTnkQn([xT,uT]T)(cid:11),(27)wherethetargetvaluebeingmaximizedislineartouatx.Therefore,convexityofthesetˆSn(x)⊂Uimpliesthatanoptimalsolutionto(27)isguaranteedtobegloballyoptimal,ensuringthemonotonicimprovementofthefeedbackcontroller.Theproposedfeedback-controllerupdateissummarizedinAlgorithm3,wheremodstandsforthemodulooperation.IV.EXPERIMENTALRESULTSTheproposedlearningframeworkisimplementedonabrushbot,whichhashighlynonlinear,nonholonomic,andcomplexdynamics(seeFigure4).Theobjectiveofthisex-perimentistoﬁndafeedbackcontrollerdrivingthebrushbottotheorigin,whilerestrictingtheregionofexploration.TheexperimentisconductedattheRobotarium,aremotelyaccessiblerobottestbedatGeorgiainstituteoftechnology[52].A.ExperimentalConditionTheexperimentalconditionsformodellearning,reinforce-mentlearning,controlbarrierfunctions,andtheirparametersettingsarepresentedbelow.BrushMotorBrushFig.4.Apictureofthebrushbotusedintheexperiment.Vibrationsofthetwomotorspropagatetothetwobrushes,drivingthebrushbot.Controlinputsareoftwodimensioneachofwhichcorrespondstotherotationalspeedofamotor.1)Modellearning:Thestatex=[x,y,θ]TconsistsoftheXpositionx,Ypositiony,andtheorientationθ∈[−π,π]ofthebrushbotfromtheworldframe.Theexactpositionsandtheorientationarerecordedbyamotioncapturesystemevery0.3second.Acontrolinputuisoftwodimensioneachofwhichcorrespondstotherotationalspeedofamotor.Toimprovethelearningefﬁciencyandreducethetotallearningtimerequired,weidentifythemostsigniﬁcantdimensionandreducethedimensionstolearn.Thesoleinputvariableofpn,fn,andgn,fortheshiftsofxandy,isassumedtobeθ,andtheshiftofθisassumedtobeconstantoverthestate,andhencedependsonnothingbutcontrolinputs(seeSectionIV-A.4).Thebrushbotusedinthepresentstudyisnonholonomic,i.e.,itcanonlygoforward,andpositivecontrolinputsbasicallydrivethebrushbotinthesamewayasnegativecontrolinputs.Assuch,weusetherotationalspeedsofthemotorsasthecontrolinputsMoreover,toeliminatetheeffectofstaticfrictionsonthemodel,weassumethatthezerocontrolinputgiventothealgorithmactuallygeneratesomeminimumcontrolinputsuδtothemotors,i.e.,theactualmaximumcontrolinputstothemotorsaregivenbyumax+uδ,whereumaxisthemaximumcontrolinputfedtothealgorithm.2)Reinforcementlearning:Thestatefortheaction-valuefunctionapproximationconsistsofthedistance(cid:15)(cid:15)[x,y]T(cid:15)(cid:15)R2fromtheoriginandtheorientationθ.TheinstantaneousrewardisgivenbyRn(x,u)=−(cid:15)(cid:15)[x,y]T(cid:15)(cid:15)2R2+2,∀n∈N(28)wheretheconstantisaddedtopreventtheresultingvalueofexploredstatesfrombecomingnegative,i.elowerthanthe9valueoutsideoftheregionofexploration.3)Discrete-timecontrolbarriercertiﬁcate:Controlbarriercertiﬁcatesareusedtolimittheregionofexplorationtoarecutangulararea:x∈[−xmax,xmax],y∈[−ymax,ymax],wherexmax>0andymax>0.Becausethebrushbotcanonlygoforward,weemploythefollowingfourbarrierfunctions:B1(x)=xmax−x−υ|θ+π|,B2(x)=x+xmax−υ|θ|,B3(x)=ymax−y−υ(cid:16)(cid:16)(cid:16)θ+π2(cid:16)(cid:16)(cid:16),B4(x)=y+ymax−υ(cid:16)(cid:16)(cid:16)θ−π2(cid:16)(cid:16)(cid:16),(seeExample1).NotethatthosefunctionssatisfyAssumption1.2andtheLipschitzconstantνiszeroexceptataroundθ=−π2,0,π2,π.4)Parametersettings:Theparametersettingissumma-rizedinTableI.FiveGaussiankernelswithdifferentscaleparametersσareemployedintheaction-valuefunctionap-proximation(i.e,M=5),andsixGaussiankernelsareemployedinmodellearningforxandy(i.e,Mp=Mf=Mg=6).Inmodellearningforθ,wedeﬁneHp:={ϕ:Z→R|∃α∈R,ϕ(z)=α}andHf=Hg:={ϕ:X→R|∃α∈R,ϕ(x)=α}(i.e,Mp=Mf=Mg=1).ThekernelsofHpandHfareweighedbyτ=0.1inmodellearning(seeRemark2).5)Procedure:Thetimeinterval(durationofoneiteration)forlearningis0.3seconds,andrandomexplorationsareconductedfortheﬁrst300secondscorrespondingto1000iterations.Whileexploring,themodellearningalgorithmadaptivelylearnsamodelwhosecontrol-afﬁneterms,i.e.fn(x)+gn(x)u,isusedincombinationwithbarriercertiﬁ-cates.Althoughbarrierfunctionsemployedintheexperimentreducedeadlocksituations,thebrushbotisforcedtoturninwardtheregionofexplorationwhenadeadlockisdetected.Notethatthebarriercertiﬁcatesareintentionallyviolatedinsuchacase.Thefeedbackcontrollerisupdatedevery50seconds.After300seconds,westoplearningamodelandtheaction-valuefunction,andthefeedbackcontrollerreplacesrandomexplorations.Thebrushbotisforcedtostopwhenitentersintothecircleofradius0.2centeredattheorigin.Whenthebrushbotisdrivenclosetotheoriginandentersthiscircle,itispushedawayfromtheorigintoseeifitreturnstotheoriginagain(seeFigure10).B.ResultsFigure5plotsp(n)n([xT,[0,0]]T),f(n)n(x),andg(n)n(x)ei,i∈1,2,forxandyatn=1000.Recallthatthesefunctionsonlydependonθinthisexperimenttoimprovethelearningefﬁciency.Fortheshiftofθ,theestimatorsareconstantoverthestate(seeSectionIV-A.1),andtheresultisg(n)n(x)e1=1.38,g(n)n(x)e2=−0.77,andp(n)n([xT,[0,0]]T)=f(n)n(x)=0atn=1000.AscanbeseeninFigure5,p(n)n([xT,[0,0]T]T)isalmostzeroandsoisf(n)n(x),implyingthattheproposedalgorithmsuccessfullydropsoffirrelevantstructuralcomponentsofamodel.Figure6plotsthetrajectoryofthebrushbotwhileexploring(i.e.X,Ypositionsfromn=0ton=1000).ItisobservedTABLEISUMMARYOFTHEPARAMETERSETTINGSGeneralSetting(Parameter)(Description)(Value)xmaxmaximumXposition1.2ymaxmaximumYposition1.2ηBarrier-functionparameter0.1υcoefﬁcientinbarrierfunctions0.1uδactualminimumcontrol0.4umaxmaximumcontrolinput0.623ModelLearningforxandy(Parameter)(Description)(Value)λstepsize0.3sndatasize5µregularizationparameter0.0001ρprecisionparameter0.001ǫlarge-normalized-error0.1rmaxmaximum-dictionary-size500σscaleparameters{10,5,2,1,0.5,0.2}ModelLearningforθ(Parameter)(Description)(Value)λstepsize0.03sndatasize10µregularizationparameter0ρprecisionparameter0.01ǫlarge-normalized-error0.1rmaxmaximum-dictionary-size3Action-valueFunctionApproximation(Parameter)(Description)(Value)λstepsize0.3sndatasize10µregularizationparameter0.0001ρprecisionparameter0.05ǫlarge-normalized-error0.1rmaxmaximum-dictionary-size2000γdiscountfactor0.95σscaleparameters{10,5,2,1,0.5}thatthebrushbotremainsintheregionofexploration(x∈[−1.2,1.2]andy∈[−1.2,1.2])mostofthetime.Moreover,thevaluesofbarrierfunctionsBi,i∈1,4,forthewholetrajectoryareplottedinFigure7.Eventhoughsomeviolationsofsafetyareseenintheﬁgure,thebrushbotreturnstothesaferegionbeforelargeviolationsoccur.Despiteunknown,highlycomplexandpossiblynonstationarysystem,theproposedsafety-awarelearningframeworkisshowntoworkefﬁciently.Figure8plotsthetrajectoriesoftheoptimalfeedbackcontrollerlearnedbythebrushbot.Oncetheoptimalfeed-backcontrollerreplacesrandomexplorations,thebrushbotreturnstotheoriginuntiln=1016astheﬁrstﬁgureshows.Thebrushbotispushedbyasweeperattimein-stancen=1031,1075,1101,1128,1181,andn=1230,andthetrajectoriesofthebrushbotafterbeingpushedatn=1031,1075,1101arealsoshowninFigure8.Dashedlinesinthelastﬁgureindicatesthetimewhenthebrushbotispushedaway.Givenrelativelyshortlearningtimeandthatnosimulatorisused,thebrushbotlearnsthedesirablebehaviorsufﬁcientlywell.Figure9plotstheshapeofQφn(n)(cid:21)(cid:8)(cid:15)(cid:15)[x,y]T(cid:15)(cid:15)R2,θ(cid:11)T,[0,0]T(cid:22)overX,Ypositionsatn=1000.Itisobservedthatwhenthecontrolinputiszero(i.e.,whenthebrushbotbasicallydoesnotmove),thevicinity10-3-2-10123 (rad)-0.2-0.15-0.1-0.0500.050.10.150.2Outputg(n)n(x)e1forxg(n)n(x)e2forxg(n)n(x)e1foryg(n)n(x)e2foryp(n)n([xT,[0,0]]T)forxp(n)n([xT,[0,0]]T)foryf(n)n(x)forxandyFig.5.Estimatedoutputofthemodelestimatoratu=[0,0]Tandn=1000overtheorientationθ.Irrelevantstructuressuchasp(n)nandf(n)ndroppedoffsuccessfully.-1.5-1-0.500.511.5-1.5-1-0.500.511.5n=0n=1000xy02004006008001000-1.5-1-0.500.511.5IterationPositionXpositionYpositionFig.6.Theleftﬁgureshowsthetrajectoryofthebrushbotwhileexploring,andtherightﬁgureshowsX,Ypositionsoveriterations.Theregionofexplorationislimitedtox∈[−1.2,1.2]andy∈[−1.2,1.2].Thebrushbotremainsintheregionmostofthetime.020040060080010001200-0.500.511.522.5IterationValuesofbarrierfunctionsB1(x)B2(x)B3(x)B4(x)Fig.7.Thevaluesoffourcontrolbarrierfunctionsemployedintheexperimentforthewholetrajectory.Eventhoughsomeviolationsofsafetyareseen,thebrushbotreturnstothesaferegionbeforelargeviolationsoccur.Thenonholonomicbrushbotadaptivelylearnsamodelandhowtoturninwardtheregionofexplorationbeforereachingtheboundariesoftheregionofexploration.oftheoriginhasthehighestvalue,whichisreasonable.Finally,Figure10showstwotrajectoriesofthebrushbotreturningtotheoriginbyusingtheaction-valuefunctionsavedatn=1000.Afterbeingpushedawayfromtheorigin,thebrushbotsuccessfullyreturnstotheoriginagain.11-1.5-1-0.500.511.5-1.5-1-0.500.511.5n=1000n=1016xy-1.5-1-0.500.511.5-1.5-1-0.500.511.5n=1035n=1054xy-1.5-1-0.500.511.5-1.5-1-0.500.511.5n=1078n=1085xy-1.5-1-0.500.511.5-1.5-1-0.500.511.5n=1103n=1111xy100010501100115012001250-1.5-1-0.500.511.5IterationPositionXpositionYpositionFig.8.Trajectoriesoftheoptimalfeedbackcontrollerlearnedbythebrushbot.Theoptimalfeedbackcontrollerreplacesrandomexplorationsatn=1000,andthebrushbotreturnstotheoriginuntiln=1016(ﬁrstﬁgure).Thebrushbotispushedbyasweeperattimeinstancen=1031,1075,1101,1128,1181,andn=1230.Dashedlinesinthelastﬁgureindicatesthetimewhenthebrushbotispushedaway.Thebrushbotlearnsthedesirablebehaviorsufﬁcientlywell.51011.51510.50200-0.5-1-1-1.5xyFig.9.Theshapeoftheaction-valuefunctionoverX,Ypositionsatthecontrolinputu=0andn=1000.Thevicinityoftheoriginhasthehighestvaluewhenthecontrolinputiszero.C.DiscussionOneofthechallengesoftheexperimentisthatnoinitialdataorsimulatorsareavailable.Despitethefactthatthebrushbotwithhighlycomplexsystemhastolearnanoptimalcontrollerwhiledealingwithsafetybyemployingadaptivemodellearning,theproposedlearningframeworkworkswellintherealworld.Ifsomeinitialdataorknowledgesaboutamodelareavailable,itisalsopossibletocombinetheproposedframeworkwithothermethodsdesignedforsta-tionarydynamics.Formodellearningandtheaction-valuefunctionapproximation,multikerneladaptiveﬁlteringmightbereplacedbyotherkernel-basedmethodsaswell.V.CONCLUSIONThelearningframeworkpresentedinthispapersuccess-fullytiedmodellearning,reinforcementlearning,andbar-riercertiﬁcates,enablingsafety-awarereinforcementlearningforunknown,highlynonlinear,nonholonomic,andpossiblynonstationaryagentdynamics.Theproposedmodellearningalgorithmisabletocaptureastructureoftheagentdynamicsbyemployingasparseoptimization.Theresultingmodelhaspreferablestructureforpreservingefﬁcientcomputationsofbarriercertiﬁcates.Asetofmildconditionsensuringconvexityofthebarriercertiﬁedsafecontrolspaceispresented.Convex-ityofthesafecontrolspaceandanappropriatedesignofkernelfunctionsfortheaction-valuefunctionguaranteetheglobalop-timalityofsolutionstothefeedback-controllerupdate,whichensuresthemonotonicimprovementofafeedbackcontroller.Inaddition,the(action-)valuefunctionapproximationproblemisappropriatelyreformulatedsothatkernel-basedmethodsincludingmultikerneladaptiveﬁlteringcanbedirectlyapplied.Theexperimentalresultshowstheefﬁcacyoftheproposedlearningframeworkintherealworld.12Fig.10.Twotrajectoriesofthebrushbotreturningtotheoriginbyusingtheaction-valuefunctionsavedatn=1000.Redarrowsshowthetrajectories.Afterbeingpushedawayfromtheorigin,thebrushbotsuccessfullyreturnstotheoriginagain.APPENDIXAPROOFOFTHEOREM1Thelineintegralof∂B(x)∂xispathindependentbecauseitisthegradientofthescalerﬁeldB[53].Letx(t):=(1−t)xn+txn+1=xn+t(fn(xn)+gn(xn)un),wheret∈[0,1]parameterizesthelinepathbetweenxnandxn+1,thendB(x(t))dt=∂B(x(t))∂x(fn(xn)+gn(xn)un).Therefore,foranypathAfromxntoxn+1,itholdsthatB(xn+1)−B(xn)=(cid:23)A∂B(x)∂x·dx=(cid:23)10dB(x(t))dtdt≥(cid:23)10(cid:24)∂B(xn)∂x−νt(fn(xn)+gn(xn)un)T(cid:25)(fn(xn)+gn(xn)un)dt=∂B(xn)∂x(fn(xn)+gn(xn)un)−ν2(cid:5)fn(xn)+gn(xn)un(cid:5)2Rnx.(A.1)TheinequalityimpliesthatB(xn+1)−B(xn)isgreaterthanorequaltothatinthecasewhen∂B(x)∂xdecreasealongthelinepathatthemaximumrate.When(13)issatisﬁed,itholdsfrom(A.1)thatB(xn+1)−B(xn)≥−ηB(xn),whichisthediscrete-timeexponentialcontrolbarriercertiﬁ-catedeﬁnedin(3).Equation(13)canberewrittenas∂B(xn)∂x(fn(xn)+gn(xn)un)−ν2(cid:5)fn(xn)+gn(xn)un(cid:5)2Rnx≥−ηB(xn).(A.2)Theﬁrstterminthelefthandsideof(A.2)isafﬁnetoun,thesecondtermisthecombinationofaconcavefunction−ν2(cid:5)·(cid:5)2Rnxandanafﬁnefunctionofun,whichisconcave.Therefore,thelefthandsideof(A.2)isaconcavefunction,andtheinequality(A.2)deﬁnesaconvexconstraint.APPENDIXBPROOFOFPROPOSITION2Sinceκ(u,v)=1(u)=1,∀u,v∈Uisapositivedeﬁnitekernel,itdeﬁnestheuniqueRKHSgivenbyspan{1},whichiscompletebecauseitisaﬁnite-dimensionalspace.Foranyϕ:=α1∈Hc,(cid:6)ϕ,ϕ(cid:7)Hc=α2≥0andtheequalityholdsifandonlyifα=0,orequivalently,ϕ=0.Thesymmetryandthelinearityalsohold,andhence(cid:6)·,·(cid:7)Hcdeﬁnestheinnerproduct.Foranyu∈U,itholdsthat(cid:6)ϕ,κ(·,u)(cid:7)Hu=(cid:6)α1,1(cid:7)Hu=α=ϕ(u).Therefore,thereproducingpropertyissatisﬁed.APPENDIXCPROOFOFTHEOREM2Thefollowinglemmasareusedtoprovethetheorem.Lemma1([54,Theorem2]).LetX⊂Rnxbeanysetwithnonemptyinterior.Then,theRKHSassociatedwiththeGaussiankernelforanarbitraryscaleparameterσ>0doesnotcontainanypolynomialonX,includingthenonzeroconstantfunction.Lemma2.AssumethatX⊂RnxandU⊂Rnuhavenonemptyinteriors.Then,theintersectionoftheRKHSHuassociatedwiththekernelκ(u,v):=uTv,u,v∈U,andtheRKHSHc:={ϕ:U→R|∃α∈R,ϕ(u)=α}is{0},i.e.Hc∩Hu={0}.Proof.Itisobviousthatthefunctionϕ(u)=0,∀u∈UisanelementofbothoftheRKHSs(vectorspaces)HuandHc.Therefore,itisenoughtoshowthatthereexistsu∈Usatisfyingthatϕ(u)(cid:11)=ϕ(uint),uint∈int(U),whereint(U)13denotestheinteriorofU,foranyϕ∈Hu\{0}.Assumethatϕ(v)(cid:11)=0forsomev∈U.From[49,Theorem3],theRKHSHuisexpressedasHu=span{κ(·,u)}u∈U,whichisﬁnitedimension,implyingthatanyfunctioninHuislinear.Sincethereexistsu=uint+̺v∈Uforsome̺>0,itisprovedthatϕ(u)=ϕ(uint+̺v)=ϕ(uint)+̺ϕ(v)(cid:11)=ϕ(uint).Lemma3([55,Proposition1.3]).GivenvectorspacesH1andH2.IfH2=H21⊕H22,thenH1⊗H21∩H1⊗H22={0},i.e,H1⊗H2=(H1⊗H21)⊕(H1⊗H22).Lemma4.GivenX⊂RnxandU⊂Rnu,letH1,H2,andHbeassociatedwiththeGaussiankernelsκ1(x,y):=1(√2πσ)nxexp(cid:21)−(cid:8)x−y(cid:8)2Rnx2σ2(cid:22),x,y∈X,κ2(u,v):=1(√2πσ)nuexp(cid:21)−(cid:8)u−v(cid:8)2Rnu2σ2(cid:22),u,v∈U,andκ([xT,uT]T,[yT,vT]T):=1(√2πσ)nx+nuexp(cid:24)−(cid:5)[xT,uT]T−[yT,vT]T(cid:5)2Rnx+nu2σ2(cid:25),x,y∈X,u,v∈U,respectively,foranarbitraryσ>0.Then,byregardingafunctioninH1⊗H2asafunctionovertheinputspaceX×U⊂Rnx+nu,itholdsthatH=H1⊗H2.Proof.H1⊗H2hasthereproducingkerneldeﬁnedbyκ⊗([xT,uT]T,[yT,vT]T):=κ1(x,y)κ2(u,v)=1(√2πσ)nx(√2πσ)nuexp(cid:4)−(cid:5)x−y(cid:5)2Rnx2σ2(cid:5)exp(cid:4)−(cid:5)u−v(cid:5)2Rnu2σ2(cid:5)=1(√2πσ)nx+nuexp(cid:4)−(cid:5)x−y(cid:5)2Rnx+(cid:5)u−v(cid:5)2Rnu2σ2(cid:5)=κ([xT,uT]T,[yT,vT]T).Thisveriﬁestheclaim.WearenowreadytoproveTheorem2.ProofofTheorem2.ByLemmas2and3,itisderivedthatHf⊗Hc∩Hg⊗Hu={0}.ByLemmas1,3,and4,itholdsthatHp∩Hf⊗Hc={0}andHp∩Hg⊗Hu={0}.APPENDIXDPROOFOFTHEOREM3WeshowthattheoperatorU:HQ→Hψ,whichmapsϕQ∈HQtoafunctionϕ∈Hψ,ϕ([zT,wT]T)=ϕQ(z)−γϕQ(w)whereγ∈(0,1),z,w∈Z,isbijective.First,foranyϕQ1,ϕQ2∈HQ,U(ϕQ1+ϕQ2)([zT,wT]T)=(ϕQ1+ϕQ2)(z)−γ(ϕQ1+ϕQ2)(w)=(ϕQ1(z)−γϕQ1(w))+(ϕQ2(z)−γϕQ2(w))=U(ϕQ1)([zT,wT]T)+U(ϕQ2)([zT,wT]T),∀z,w∈Z,andU(αϕQ1)([zT,wT]T)=αϕQ1(z)−γαϕQ1(w)=α(ϕQ1(z)−γϕQ1(w))=αU(ϕQ1)([zT,wT]T),∀α∈R,∀z,w∈Z,fromwhichthelinearityholds.Second,themappingUissurjectivebydeﬁnition.Therefore,itisenoughtoshowthatker(U)=0[56].Thisisshowninthefollowing.U(ϕQ)([zT,zT]T)=(1−γ)ϕQ(z)=0,∀z∈Z,∀ϕQ∈ker(U)=⇒ϕQ=0.Therefore,thespaceHψwiththeinnerproductdeﬁnedin(22)isisometrictotheRKHSHQ,andhenceisaHilbertspace.Next,weshowthatHψisanRKHS.BecauseκQ(·,z)−γκQ(·,w)∈HQ,itistruethatκ(·,[zT,wT]T)∈Hψ.Moreover,itholdsthat(cid:26)κ(·,[zT,wT]T),κ(·,[˜zT,˜wT]T)(cid:27)Hψ=(cid:26)κQ(·,z)−γκQ(·,w),κQ(·,˜z)−γκQ(·,˜w)(cid:27)HQ=(cid:9)κQ(z,˜z)−γκQ(z,˜w)(cid:10)−γ(cid:9)κQ(w,˜z)−γκQ(w,˜w)(cid:10)=κ([zT,wT]T,[˜zT,˜wT]T),andthat(cid:26)ϕ,κ(·,[zT,wT]T)(cid:27)Hψ=(cid:26)ϕQ,κQ(·,z)−γκQ(·,w)(cid:27)HQ=ϕQ(z)−γϕQ(w)=ϕ([zT,wT]T),∀ϕ∈Hψ.Therefore,κ(·,·):Z2×Z2→RisthereproducingkernelwithwhichtheRKHSHψisassociated.APPENDIXEPROOFOFCOROLLARY1FromthedeﬁnitionoftheinnerproductintheRKHSHψ,itfollowsthat(cid:15)(cid:15)(cid:15)Qφn(n+1)−Qφn∗(cid:15)(cid:15)(cid:15)HQ=(cid:15)(cid:15)(cid:15)ψ(n+1)n−ψ∗n(cid:15)(cid:15)(cid:15)Hψ≤(cid:15)(cid:15)(cid:15)ψ(n)n−ψ∗n(cid:15)(cid:15)(cid:15)Hψ=(cid:15)(cid:15)(cid:15)Qφn(n)−Qφn∗(cid:15)(cid:15)(cid:15)HQ.REFERENCES[1]R.S.SuttonandA.G.Barto,Reinforcementlearning:Anintroduction.MITPress,1998.[2]F.L.LewisandD.Vrabie,“Reinforcementlearningandadaptivedynamicprogrammingforfeedbackcontrol,”IEEECircuitsandSystemsMagazine,vol.9,no.3,2009.[3]D.Liberzon,Calculusofvariationsandoptimalcontroltheory:aconciseintroduction.PrincetonUniversityPress,2011.14[4]F.Berkenkamp,M.Turchetta,A.P.Schoellig,andA.Krause,“Safemodel-basedreinforcementlearningwithstabilityguarantees,”inProc.NIPS,2017.[5]F.Berkenkamp,R.Moriconi,A.P.Schoellig,andA.Krause,“Safelearningofregionsofattractionforuncertain,nonlinearsystemswithGaussianprocesses,”inProc.CDC,2016,pp.4661–4666.[6]J.Schreiter,D.Nguyen-Tuong,M.Eberts,B.Bischoff,H.Markert,andM.Toussaint,“SafeexplorationforactivelearningwithGaussianprocesses,”inProc.ECMLPKDD,2015,pp.133–149.[7]A.K.Akametalu,J.F.Fisac,J.H.Gillula,S.Kaynama,M.N.Zeilinger,andC.J.Tomlin,“Reachability-basedsafelearningwithGaussianprocesses,”inProc.CDC,2014,pp.1424–1431.[8]S.Shalev-Shwartz,S.Shammah,andA.Shashua,“Safe,multi-agent,reinforcementlearningforautonomousdriving,”arXivpreprintarXiv:1610.03295,2016.[9]H.B.Ammar,R.Tutunov,andE.Eaton,“Safepolicysearchforlifelongreinforcementlearningwithsublinearregret,”inProc.ICML,2015,pp.2361–2369.[10]D.A.Niekerk,B.V.andB.Rosman,“Onlineconstrainedmodel-basedreinforcementlearning,”inProc.AUAI,2017.[11]J.Achiam,D.Held,A.Tamar,andP.Abbeel,“Constrainedpolicyoptimization,”inProc.ICML,2017.[12]P.AbbeelandA.Y.Ng,“Explorationandapprenticeshiplearninginreinforcementlearning,”inProc.ICML,2005,pp.1–8.[13]L.Wang,E.A.Theodorou,andM.Egerstedt,“Safelearningofquadrotordynamicsusingbarriercertiﬁcates,”arXivpreprintarXiv:1710.05472,2017.[14]J.GarcıaandF.Fern´andez,“Acomprehensivesurveyonsafereinforce-mentlearning,”J.Mach.Learn.Res.,vol.16,no.1,pp.1437–1480,2015.[15]B.D.Argall,S.Chernova,M.Veloso,andB.Browning,“Asurveyofrobotlearningfromdemonstration,”RoboticsandAutonomousSystems,vol.57,no.5,pp.469–483,2009.[16]P.Geibel,“ReinforcementlearningforMDPswithconstraints,”inProc.ECML,vol.4212,2006,pp.646–653.[17]S.P.CoraluppiandS.I.Marcus,“Risk-sensitiveandminimaxcontrolofdiscrete-time,ﬁnite-stateMarkovdecisionprocesses,”Automatica,vol.35,no.2,pp.301–309,1999.[18]C.E.RasmussenandC.K.Williams,Gaussianprocessesformachinelearning.MITpressCambridge,2006,vol.1.[19]X.Xu,P.Tabuada,J.W.Grizzle,andA.D.Ames,“Robustnessofcontrolbarrierfunctionsforsafetycriticalcontrol,”Proc.IFAC,vol.48,no.27,pp.54–61,2015.[20]P.WielandandF.Allg¨ower,“Constructivesafetyusingcontrolbarrierfunctions,”Proc.IFAC,vol.40,no.12,pp.462–467,2007.[21]P.Glotfelter,J.Cort´es,andM.Egerstedt,“Nonsmoothbarrierfunctionswithapplicationstomulti-robotsystems,”IEEEControlSystemsLetters,vol.1,no.2,pp.310–315,2017.[22]L.Wang,A.D.Ames,andM.Egerstedt,“Safetybarriercertiﬁcatesforcollisions-freemultirobotsystems,”IEEETrans.Robotics,2017.[23]A.D.Ames,X.Xu,J.W.Grizzle,andP.Tabuada,“Controlbarrierfunctionbasedquadraticprogramswithapplicationtoautomotivesafetysystems,”arXivpreprintarXiv:1609.06408,2016.[24]A.AgrawalandK.Sreenath,“Discretecontrolbarrierfunctionsforsafety-criticalcontrolofdiscretesystemswithapplicationtobipedalrobotnavigation,”inProc.RSS,2017.[25]W.Liu,J.Pr´ıncipe,andS.Haykin,KernelAdaptiveFiltering.NewJersey:Wiley,2010.[26]K.R.M¨uller,S.Mika,G.Ratsch,K.Tsuda,andB.Scholkopf,“Anintroductiontokernel-basedlearningalgorithms,”IEEETrans.NeuralNetworks,vol.12,no.2,pp.181–201,2001.[27]B.Sch¨oelkopfandA.Smola,Learningwithkernels.MITPress,Cambridge,2002.[28]M.Yukawa,“Multikerneladaptiveﬁltering,”IEEETrans.SignalPro-cessing,vol.60,no.9,pp.4672–4682,Sept.2012.[29]——,“AdaptivelearninginCartesianproductofreproducingkernelHilbertspaces,”IEEETrans.SignalProcessing,vol.63,no.22,pp.6037–6048,Nov.2015.[30]O.TodaandM.Yukawa,“Onlinemodel-selectionandlearningfornonlinearestimationbasedonmultikerneladaptiveﬁltering,”IEICETrans.FundamentalsofElectronics,CommunicationsandComputerSciences,vol.100,no.1,pp.236–250,2017.[31]M.OhnishiandM.Yukawa,“OnlinelearninginL2spacewithmultipleGaussiankernels,”inProc.EUSIPCO,2017,pp.1594–1598.[32]——,“OnlinenonlinearestimationviaiterativeL2-spaceprojections:Reproducingkernelofsubspace,”arXivpreprintarXiv:1712.04573,2017.[33]L.Baird,“Residualalgorithms:Reinforcementlearningwithfunctionapproximation,”inProc.ICML,1995,pp.30–37.[34]S.Mahadevan,B.Liu,P.Thomas,W.Dabney,S.Giguere,N.Jacek,I.Gemp,andJ.Liu,“Proximalreinforcementlearning:Anewtheoryofsequentialdecisionmakinginprimal-dualspaces,”arXivpreprintarXiv:1405.6757,2014.[35]Y.Engel,S.Mannor,andR.Meir,“ReinforcementlearningwithGaussianprocesses,”inProc.ICML,2005,pp.201–208.[36]D.Ormoneitand´S.Sen,“Kernel-basedreinforcementlearning,”Ma-chinelearning,vol.49,no.2,pp.161–178,2002.[37]X.Xu,D.Hu,andX.Lu,“Kernel-basedleastsquarespolicyiterationforreinforcementlearning,”IEEETrans.NeuralNetworks,vol.18,no.4,pp.973–992,2007.[38]B.Bethke,J.P.How,andA.Ozdaglar,“Kernel-basedreinforcementlearningusingBellmanresidualelimination,”inMITWorkingPaper,2008.[39]J.Kober,J.A.Bagnell,andJ.Peters,“Reinforcementlearninginrobotics:Asurvey,”TheInternationalJournalofRoboticsResearch,vol.32,no.11,pp.1238–1274,2013.[40]G.Tesauro,“TemporaldifferencelearningandTD-Gammon,”Commu-nicationsoftheACM,vol.38,no.3,pp.58–68,1995.[41]J.N.TsitsiklisandB.VanR.,“Analysisoftemporal-diffferencelearn-ingwithfunctionapproximation,”inAdvancesinNeuralInformationProcessingSystems,1997,pp.1075–1081.[42]J.A.Boyan,“Least-squarestemporaldifferencelearning,”inProc.ICML,1999,pp.49–56.[43]R.S.Sutton,H.R.Maei,andC.Szepesv´ari,“AconvergentO(n)temporal-differencealgorithmforoff-policylearningwithlinearfunctionapproximation,”inAdvancesinNeuralInformationProcessingSystems,2009,pp.1609–1616.[44]M.GeistandO.Pietquin,“Abriefsurveyofparametricvaluefunctionapproximation,”RapportInterne,Sup´elec,2010.[45]N.Aronszajn,“Theoryofreproducingkernels,”Trans.Amer.Math.Soc.,vol.68,no.3,pp.337–404,May1950.[46]I.Steinwart,“Ontheinﬂuenceofthekernelontheconsistencyofsupportvectormachines,”J.Mach.Learn.Res.,vol.2,pp.67–93,2001.[47]Y.Murakami,M.Yamagishi,M.Yukawa,andI.Yamada,“Asparseadaptiveﬁlteringusingtime-varyingsoft-thresholdingtechniques,”inProc.IEEEICASSP,2010,pp.3734–3737.[48]I.YamadaandN.Ogura,“Adaptiveprojectedsubgradientmethodforasymptoticminimizationofsequenceofnonnegativeconvexfunctions,”NumericalFunctionalAnalysisandOptimization,vol.25,no.7&8,pp.593–617,2004.[49]A.BerlinetandA.C.Thomas,ReproducingkernelHilbertspacesinprobabilityandstatistics.Kluwer,2004.[50]Y.Engel,S.Mannor,andR.Meir,“Thekernelrecursiveleast-squaresalgorithm,”IEEETrans.SignalProcess.,vol.52,no.8,pp.2275–2285,Aug.2004.[51]M.L.PutermanandS.L.Brumelle,“Ontheconvergenceofpolicyiter-ationinstationarydynamicprogramming,”MathematicsofOperationsResearch,vol.4,no.1,pp.60–69,1979.[52]D.Pickem,L.Wang,P.Glotfelter,Y.Diaz-Mercado,M.Mote,A.Ames,E.Feron,andM.Egerstedt,“Safe,remote-accessswarmroboticsre-searchontherobotarium,”arXivpreprintarXiv:1604.00640,2016.[53]L.V.Ahlfors,“Complexanalysis:anintroductiontothetheoryofanalyticfunctionsofonecomplexvariable,”NewYork,London,p.177,1953.[54]H.Q.Minh,“SomepropertiesofGaussianreproducingkernelHilbertspacesandtheirimplicationsforfunctionapproximationandlearningtheory,”ConstructiveApproximation,vol.32,no.2,pp.307–338,2010.[55]R.A.Ryan,IntroductiontotensorproductsofBanachspaces.SpringerScience&BusinessMedia,2013.[56]G.Strang,Introductiontolinearalgebra.Wellesley-CambridgePressWellesley,MA,1993,vol.3.
