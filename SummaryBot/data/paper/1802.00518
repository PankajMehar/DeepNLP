
Various models of signals and images have been studied in
recent years including dictionary models, tensor and manifold
models. Dictionaries that sparsely represent signals are used
in applications such as compression, denoising, and medical
image reconstruction. Dictionaries learned from training data
sets may outperform analytical models since they are adapted
to signals (or signal classes). The goal of dictionary learning
is to ﬁnd a matrix D such that an input matrix P, representing
the data set, can be written as P ≈ DZ, with Z denoting the
(unknown) sparse representation matrix.
The learning of synthesis dictionary and sparsifying trans-
form models has been studied in several works [1]–[11]. The
convergence of speciﬁc learning algorithms has been studied
in recent works [6], [7], [12]–[16]. The learning problems are
typically non-convex and some of these works prove conver-
gence to critical points in the problems [6], [7], [17]. Others
(e.g., [15], [18]) prove recovery of generative models for
speciﬁc (often computationally expensive) algorithms, but rely
on many restrictive assumptions. A very recent work considers
a structured dictionary learning objective showing with high
probability that there are no spurious local minimizers, and
also provides a speciﬁc convergent algorithm [19], [20].
In this work, we analyze the convergence properties of a
structured (unitary) dictionary or transform learning algorithm
that involves computationally cheap updates and works well in
applications such as image denoising and magnetic resonance
image reconstruction [10], [16], [21]. Our goal is to simulta-
neously ﬁnd an n × n sparsifying transformation matrix W
and an n× N sparse coefﬁcients (representation) matrix Z for
training data represented as columns of a given n× N matrix
P, by solving the following constrained optimization problem:
F s.t. WT W = Id,(cid:13)(cid:13)Z(.,j)
(cid:13)(cid:13)0 ≤ s∀j.
(cid:107)WP − Z(cid:107)2
arg min
W,Z
(1)
The columns Z(.,j) of Z have at most s non-zeros (corre-
sponding to the (cid:96)0 “norm”), where s is a given parameter.
Alternatives to Problem (1) would replace the column-wise
sparsity constraint with a constraint on the sparsity of the
entire matrix Z (i.e., aggregate sparsity), or use a sparsity
penalty (e.g., (cid:96)p penalties with 0 ≤ p ≤ 1). Problem (1)
also corresponds to learning a (synthesis) dictionary WT for
representing the data P as WT Z.
Optimizing Problem (1) by alternating between updating
W (operator update step) and Z (sparse coding step) would
generate the following updates. The tth Z update is given as
(.,j) = Hs(Wt−1P(.,j)) ∀ j, where the thresholding operator
Zt
Hs(·) zeros out all but the s largest magnitude elements of
a vector (leaving the s entries unchanged). The subsequent
W update is based on the full singular value decomposition
(SVD) of ZtPT (cid:44) VΣUT with Wt = VUT . The method is
shown in Algorithm 1.
Recent works have shown convergence of Algorithm 1 or
its variants to critical points in the equivalent unconstrained
problems [16], [22], [23]. Here, we further prove local linear
convergence of the method to the underlying generative (data)
model under mild assumptions that depend on properties of
the underlying/generating sparse coefﬁcients. Our experiments
show that the method is also robust or insensitive to initial-
ization in practice.
II. CONVERGENCE ANALYSIS
The main contribution of this work is the local convergence
analysis of Algorithm 1. In particular, we show that under mild
assumptions, the iterates in Algorithm 1 converge linearly to
the underlying (generating) data model.
Algorithm 1: Alternating Optimization for (1)
Input: Training data matrix P, maximum iteration
count L, sparsity s
Output: WL, ZL
Initialize: W0 and t = 1
for t ≤ L do
Zt
(.,j) = Hs
PZtT =UtΣtVtT
Wt = VtUtT
t = t + 1
(cid:0)Wt−1P(.,j)
(cid:1) ∀ j
end
A. Notation
In the remainder of this work, we adopt the following nota-
tion. The unitary transformation matrix, sparse representation
matrix, and training data matrix are denoted as W ∈ Rn×n,
Z ∈ Rn×N , and P ∈ Rn×N , respectively. For a matrix X,
we denote its jth column, ith row, and entry (i, j) by X(·,j),
X(i,·), and X(i,j) respectively. The tth iterate or approximation
in the algorithm is denoted using (·)t, with a lower-case t,
and (·)T denotes the transpose. The function S(y) returns
the support (i.e., set of indexes) of a vector y ∈ Rn, or the
locations of the nonzero entries in y. Matrix Dk denotes an
n × n diagonal matrix of ones and a zero at location (k, k).
Additionally, ˜Dk denotes an N × N diagonal matrix that has
ones at entries (i, i) for i ∈ S(Z∗
(k,·)) and zeros elsewhere,
and matrix Z∗ is deﬁned in Section II-B (see assumption
(A1)). The Frobenious norm, denoted (cid:107)X(cid:107)2
F , is the the sum of
squared elements of X, and (cid:107)X(cid:107)2 denotes the spectral norm.
Lastly, Id denotes the appropriately sized identity matrix.
B. Assumptions
Before presenting our main results, we will brieﬂy discuss
our assumptions and explain their implications:
(A1) Generative model: There exists a Z∗ and unitary W∗
such that W∗P = Z∗, and (cid:107)P(cid:107)2 = 1 (normalized).
(A2) Sparsity: The columns of Z∗ are s−sparse,
i.e.,
(cid:107)Z∗
(A3) Spectral property: The underlying Z∗ satisﬁes the bound
κ4(cid:0)Z∗(cid:1) max1≤k≤n (cid:107)DkZ∗Z∗T Z∗ ˜Dk(cid:107)2 < 1, where κ(·)
(·,j)(cid:107)0 ≤ s ∀j.
denotes the condition number (ratio of largest to smallest
singular value).
(A4) Initialization: (cid:107)W0 − W∗(cid:107)F ≤  for an appropriate
sufﬁciently small  > 0. 1
The generative model assumption simply states that there
exists an underlying transform and representation matrix for
the data set P. So we would like to investigate if Algo-
rithm 1 can ﬁnd such underlying models (minimizers in (1)).
(cid:18) Z∗
(cid:19)
1Although we do not specify the best (largest permissible)  explicitly,  <
with β(·) denoting the smallest nonzero magnitude
2 minj β
in a vector, will arise in one of our proof steps. The actual permissible  is
also dictated as per (convergence of) Taylor series expansions discussed in
the proof.
(·,j)
(·,j)
(cid:107)Z∗
(cid:107)2
Assumption (A2) states that the columns of Z∗ have at most
s nonzeros. We assume that the coefﬁcients are “structured”
in assumption (A3), satisfying a spectral property, which
will be used to establish our theorems. Later we discuss a
conjecture that states that this property holds for a speciﬁc
probabilistic model. Assumption (A4) states that the initial
sparsifying transform is sufﬁciently close to the solution W∗.
This assumption also simpliﬁes our proof and has been made
in other works such as [15], [18], which address the issue
of speciﬁc (good) initialization separately from the main
algorithm. In this work, we empirically show the effect of
general initializations in Section III.
C. Main Results and Proofs
We state the convergence results in Theorems II.1 and II.2.
Theorem II.1 establishes that Algorithm 1 converges to the
underlying generative model under the assumptions discussed
in Section II-B. It also makes an additional assumption on Z∗
that Z∗Z∗T = Id, which simpliﬁes assumption (A3). Theo-
rem II.2 presents the more general result based on Assumption
(A3). We only include the (simpler) proof of Theorem II.1,
while the full details of the general Theorem II.2’s proof can be
found in [24]. Following Theorem II.2, Conjecture 1 states that
Assumption (A3) holds under a commonly used probabilistic
assumption on the sparse representation matrix Z∗.
Theorem II.1. Under Assumptions (A1)−(A4) and assuming
Z∗Z∗T = Id,
the Frobenius error between the iterates
generated by Algorithm 1 and the underlying generative model
in Assumption (A1) is bounded as follows:
(cid:107)Zt − Z∗(cid:107)F ≤ qt−1, (cid:107)Wt − W∗(cid:107)F ≤ qt,
(2)
where q (cid:44) max1≤k≤n (cid:107)DkZ∗ ˜Dk(cid:107)2 and  is ﬁxed based on
the initialization.
Since Z∗Z∗T = Id, by Assumption (A3) it follows that
q < 1 above, and thus the Theorem establishes that the iterates
converge at a linear rate to the underlying generative model.
In the following, we prove Theorem II.1 using induction
on the approximation error of iterates with respect to Z∗ and
W∗. Let the series {Et} and {∆t} be deﬁned as
Et := Wt − W∗,
∆t := Zt − Z∗.
(3)
(4)
By Assumption (A4), (cid:107)E0(cid:107)F ≤ . We ﬁrst provide a proof
for the base case of t = 1. This proof involves two main parts.
First, we show that the error between Z1 and Z∗ is bounded
(in norm) by . Then, we show that (cid:107)W1 − W∗(cid:107)F ≤ q,
where q is iteration-independent.
For the ﬁrst part, we use Assumptions (A1), (A2), and (A4).
Assumption (A4) ensures that a superset of the support of Z∗
Z1
is recovered in the ﬁrst iteration. In particular, each column of
the sparse coefﬁcients matrix Z1 in Algorithm 1 satisﬁes
= Hs(W∗P(·,j) + E0P(·,j))
(·,j) + E0P(·,j))
j E0P(·,j),
(·,j) = Hs(W0P(·,j))
= Hs(Z∗
= Z∗
(·,j) + Γ1
(Eq.3)
(5)
(A1)
(A4)
j is a diagonal matrix with a one in the (i, i)th entry
where Γ1
if i ∈ S(Z1
(·,j)) and zero otherwise and E0 is as deﬁned in (3).
The last equality in (5) follows from the fact that the support
(·,j) for small . In particular, since
of Z1
(·,j) includes that of Z∗
(cid:13)(cid:13)P(·,j)
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)Z∗
(cid:13)(cid:13)2 =
(cid:13)(cid:13)∞ ≤(cid:13)(cid:13)E0P(·,j)
(cid:13)(cid:13)E0P(·,j)
, we have
(·,j)
(cid:13)(cid:13)2 ≤(cid:13)(cid:13)E0(cid:13)(cid:13)F
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)Z∗
(cid:16) Z∗
(·,j)
(cid:17)
Therefore, whenever (cid:107)E0(cid:107)F ≤  < 1
with
β(·) denoting the smallest nonzero magnitude in a vector, the
support of Z1
(·,j) (i.e., the entries of the
perturbation term E0P(·,j) are not large enough to change the
support). The following results then hold:
(·,j) includes that of Z∗
2 minj β
(·,j)
(·,j)(cid:107)2
(cid:107)Z∗
(cid:107)Z1 − Z∗(cid:107)2
(Eq.5)
= (cid:107)[Γ1
(i)≤ (cid:107)E0P(cid:107)2
(ii)≤ (cid:107)E0(cid:107)2
N E0P(·,N )](cid:107)2
= (cid:107)E0(cid:107)2
F .
1E0P(·,1), ..., Γ1
F(cid:107)P(cid:107)2
j; step (ii) holds for the
2 = 1,
(A1)
Step (i) follows by deﬁnition of Γ1
Frobenius norm of a product of matrices; and since (cid:107)P(cid:107)2
the last equality holds. Therefore, we can conclude that
(cid:107)Z1 − Z∗(cid:107)F ≤ (cid:107)E0(cid:107)F
(A4)≤ .
(6)
Next, we analyze the quality of the updated transform W1.
To bound (cid:107)W1 − W∗(cid:107)F by q, we rely on Taylor Series
expansions of the matrix inverse and positive-deﬁnite square
root functions. Denote the full SVD of Z∗Z1T as U1
T .
zΣ1
Then, from Assumption (A1) and Algorithm 1, we have,
zV1
W∗T Z∗Z1T (A1)
, W1 = V1U1T
Then, W1 is expressed in terms of the SVD of Z∗Z1T as
= U1Σ1V1T
= PZ1T
W1 = V1
zU1
W∗.
Using (7), the error between W1 and W∗ satisﬁes
(cid:107)W1 − W∗(cid:107)F = (cid:107)V1
= (cid:107)(V1
zU1
zU1
W∗ − W∗(cid:107)F
zU1
T − Id(cid:107)F ,
(7)
(8)
where the matrix V1
T can be further rewritten as follows:
T − Id)W∗(cid:107)F = (cid:107)V1
zU1
(cid:0)Σ1
(cid:123)(cid:122)
(a)
(cid:1)−1
(cid:125)
)−1
= V1
= (Z∗Z1T
(cid:124)
(cid:124)
(cid:123)(cid:122)
(b)
U1
U1
(Z∗Z1T
zΣ1
zU1
Z1Z∗T )
(cid:125)
(9)
V1
zU1
It is easy to show that since Z∗Z∗T = Id, Z∗Z1T is invertible
for all  < 1 (sufﬁcient condition).
Using (4) and the assumption Z∗Z∗T = Id, the Taylor
Series expansions for the matrix inverse and positive-deﬁnite
square root in (9), can be written as
(a) = (Z∗Z1T
)−1 = (Id + Z∗∆1T
)−1
= Id − Z∗∆1T
+ O((∆1)2)
(b) = (Z∗Z1T
= Id +
Z1Z∗T )
(Z∗∆1T
+ ∆1Z∗T ) + O((∆1)2).
Therefore we can rewrite (9) as
V1
zU1
= (a)(b)
= Id +
(∆1Z∗T − Z∗∆1T
) + O((∆1)2),
terms, and is bounded in norm by C(cid:13)(cid:13)∆1(cid:13)(cid:13)2 for some constant
where O((∆1)2) denotes corresponding higher order series
C (independent of the iterates).
the ﬁrst transform iterate W1 and W∗ is bounded as
Substituting the above expressions in (8), the error between
(cid:107)W1 − W∗(cid:107)F
T − Id(cid:107)F
zU1
(Eq.8)
= (cid:107)V1
≈ 1
(cid:107)∆1Z∗T − Z∗∆1T(cid:107)F .
(10)
The approximation error in (10) is bounded in norm by
C2, which is negligible for small . So we only bound the
(dominant) term 0.5(cid:107)∆1Z∗T − Z∗∆1T(cid:107)F . Since the matrix
∆1Z∗T − Z∗∆1T clearly has a zero diagonal, we have the
following inequalities:
(cid:107)W1 − W∗(cid:107)F ≈ 1
(cid:107)∆1Z∗T − Z∗∆1T(cid:107)F
(cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)
(cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)
(cid:107)DkZ∗ ˜Dk∆1
T(cid:107)2
2 ≤
(cid:107)DkZ∗ ˜Dk(cid:107)2
2(cid:107)∆1
(k,·)(cid:107)2
k=1
≤ max
(cid:107)DkZ∗ ˜Dk(cid:107)2
k=1
(cid:107)∆1
(k,·)(cid:107)2
2 = q(cid:107)Z1 − Z∗(cid:107)F
(6)≤ q,
(k,·)
(cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)
k=1
where q (cid:44) maxk (cid:107)DkZ∗ ˜Dk(cid:107)2.
Thus, we have shown the results for the t = 1 case. We
complete the proof of Theorem II.1 by observing that for each
subsequent iteration t = τ + 1, the same steps as above can
be repeated along with the induction hypothesis (IH) to show
that
(cid:107)Zτ +1 − Z∗(cid:107)F = (cid:107)∆τ +1(cid:107)F ≤ (cid:107)Eτ(cid:107)F
= (cid:107)Wτ − W∗(cid:107)F
(IH)≤ qτ 
(cid:107)Wτ +1 − W∗(cid:107)F ≤ q(cid:107)Zτ +1 − Z∗(cid:107)F ≤ q(qτ ).
The next result generalizes Theorem II.1 by removing the
assumption Z∗Z∗T = Id.
Theorem II.2. Under Assumptions (A1) − (A4), the iterates
in Algorithm 1 converge linearly to the underlying generative
(cid:4)
Fig. 1: The performance of Algorithm 1 for recovering W∗
for s = 5 and s = 10.
model in Assumption (A1), i.e., the Frobenius error between
the iterates and the generative model satisﬁes
(cid:107)Zt − Z∗(cid:107)F ≤ qt−1, (cid:107)Wt − W∗(cid:107)F ≤ qt,
where q (cid:44) κ4(cid:0)Z∗(cid:1) max1≤k≤n (cid:107)DkZ∗Z∗T Z∗ ˜Dk(cid:107)2 < 1 and
(11)
 is ﬁxed based on the initialization.
Note that dropping the unit spectral norm (normalization)
condition on P in Assumption (A1) does not affect the (cid:107)Wt−
W∗(cid:107)F bound in Theorem II.2 and only creates a scaling in
the (cid:107)Zt − Z∗(cid:107)F bound, where the  gets replaced by (cid:107)P(cid:107)2.
Conjecture 1. Suppose the locations of the s nonzeros in each
column of Z∗ is chosen uniformly at random, and the non-
zero entries are i.i.d. as Z∗
sN ). Then, for ﬁxed,
small
for large enough N.
n , q (cid:44) κ4(cid:0)Z∗(cid:1) max1≤k≤n (cid:107)DkZ∗Z∗T Z∗ ˜Dk(cid:107)2 < 1
(i,j) ∼ N (0, n
s√
Conjecture 1 thus states that under the assumed probabilistic
model for Z∗, when N is large enough or there is sufﬁcient
training data (or columns of P), then Algorithm 1 is assured
to have rapid local linear iterate convergence to the underlying
generative model. This conjecture can be empirically veriﬁed
through simulations and the numerical results supporting it can
be found in [24]. The experiments presented in this paper will
focus on illustrating the local convergence of Algorithm 1 and
its robustness to initialization.
III. EXPERIMENTS
In this section, we show numerical experiments in support
of our analytical conclusions. We also provide results further
illustrating the robustness of the algorithm to initializations.
In our experiments, we generated the training data using
randomly generated W∗ and Z∗, with n = 50, N = 10000,
and s = {5, 10}. The transform W∗ is generated in each
case by applying Matlab’s orth() function on a standard
Gaussian matrix. The representation matrix Z∗ is generated
for each s as described in Conjecture 1, i.e., the support of
Fig. 2: The performance of Algorithm 1 with various initial-
izations for s = 5 (top) and s = 10 (bottom).
(cid:17)
(·,j)
(·,j)(cid:107)2
(cid:16) Z∗
In the ﬁrst experiment,
each column of Z∗ is chosen uniformly at random and the
nonzero entries are drawn i.i.d. from a Gaussian distribution
with mean zero and variance n/sN.
the initial W0 in Algorithm 1
is chosen to satisfy (cid:107)W0 − W∗(cid:107)F ≤  with  =
(see (5)). Fig. 1 shows the behavior
0.49 minj β
of the Frobenious error between Wt and W∗ in the algo-
rithm. The observed (linear) convergence of the iterates to the
generative operator W∗ is in accordance with Theorem II.2
and our Conjecture 1.
Next, we study the performance of Algorithm 1 with differ-
ent initializations for n = 50, N = 10000, and s = {5, 10}.
Fig. 2 shows the objective function in Problem (1) over the
algorithm iterations. Since the training data satisfy Assump-
tions (A1) and (A2), the minimum objective value in (1) is
0. Six different types of initializations are considered. The
ﬁrst, labeled ‘eps’, denotes an initialization as in Fig. 1 with
. The other initializations are as
 = 0.49 minj β
follows: entries of W0 drawn i.i.d. from a standard Gaussian
distribution (labeled ‘rand’); an n × n identity matrix W0
labeled ‘id’; a discrete cosine transform (DCT) initialization
labeled ‘dct’; entries of W0 drawn i.i.d. from a uniform distri-
bution ranging from 0 to 1 (labeled ‘unif’); and W0 = 0n×n
labeled ‘zero’. For more general initializations (other than
‘eps’), we see that the behavior of Algorithm 1 is split into
(cid:16) Z∗
(cid:107)Z∗
(cid:17)
(cid:107)Z∗
(·,j)
(·,j)(cid:107)2
[9] S. Ravishankar and Y. Bresler, “Learning sparsifying transforms,” IEEE
Trans. Signal Process., vol. 61, no. 5, pp. 1072–1086, 2013.
[10] S. Ravishankar and Y. Bresler, “Closed-form solutions within sparsifying
transform learning,” in IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), 2013, pp. 5378–5382.
[11] B. Wen, S. Ravishankar, and Y. Bresler, “Structured overcomplete sparsi-
fying transform learning with convergence guarantees and applications,”
International Journal of Computer Vision, vol. 114, no. 2-3, pp. 137–
167, 2015.
[12] D. A. Spielman, H. Wang, and J. Wright, “Exact recovery of sparsely-
used dictionaries,” in Proceedings of the 25th Annual Conference on
Learning Theory, 2012, pp. 37.1–37.18.
[13] S. Arora, R. Ge, and A. Moitra, “New algorithms for learning incoherent
and overcomplete dictionaries,” in Proceedings of The 27th Conference
on Learning Theory, 2014, pp. 779–806.
[14] Y. Xu and W. Yin, “A fast patch-dictionary method for whole image
recovery,” Inverse Problems and Imaging, vol. 10, no. 2, pp. 563–583,
2016.
[15] A. Agarwal, A. Anandkumar, P. Jain, P. Netrapalli, and R. Tandon,
“Learning sparsely used overcomplete dictionaries,” Journal of Machine
Learning Research, vol. 35, pp. 1–15, 2014.
[16] S. Ravishankar and Y. Bresler, “(cid:96)0 sparsifying transform learning with
IEEE Trans.
efﬁcient optimal updates and convergence guarantees,”
Signal Process., vol. 63, no. 9, pp. 2389–2404, May 2015.
[17] C. Bao, H. Ji, Y. Quan, and Z. Shen, “Dictionary learning for sparse
coding: Algorithms and convergence analysis,” IEEE Transactions on
Pattern Analysis and Machine Intelligence, vol. 38, no. 7, pp. 1356–
1369, July 2016.
[18] A. Agarwal, A. Anandkumar, P. Jain, and P. Netrapalli,
“Learning
sparsely used overcomplete dictionaries via alternating minimization,”
SIAM Journal on Optimization, vol. 26, no. 4, pp. 2775–2799, 2016.
[19] J. Sun, Q. Qu, and J. Wright, “Complete dictionary recovery over the
sphere I: Overview and the geometric picture,” IEEE Transactions on
Information Theory, vol. 63, no. 2, pp. 853–884, Feb 2017.
[20] J. Sun, Q. Qu, and J. Wright,
“Complete dictionary recovery over
IEEE
the sphere II: Recovery by riemannian trust-region method,”
Transactions on Information Theory, vol. 63, no. 2, pp. 885–914, Feb
2017.
[21] S. Ravishankar and Y. Bresler, “Data-driven learning of a union of
IEEE
sparsifying transforms model for blind compressed sensing,”
Transactions on Computational Imaging, vol. 2, no. 3, pp. 294–309,
2016.
[22] S. Ravishankar and Y. Bresler, “Efﬁcient blind compressed sensing using
sparsifying transforms with convergence guarantees and application to
magnetic resonance imaging,” SIAM Journal on Imaging Sciences, vol.
8, no. 4, pp. 2519–2557, 2015.
[23] C. Bao, H. Ji, and Z. Shen, “Convergence analysis for iterative data-
driven tight frame construction scheme,” Applied and Computational
Harmonic Analysis, vol. 38, no. 3, pp. 510–523, 2015.
[24] A. Ma S. Ravishankar and D. Needell, “Analysis of fast structured
dictionary learning,” Information and Inference, 2018, in preparation.
two phases. In the ﬁrst phase, the iterates slowly decrease the
objective. When the iterates are close enough to a solution,
the second phase occurs and during this phase, Algorithm 1
enjoys rapid convergence (towards 0). Note that the objective’s
convergence rate in the second phase is comparable to that of
the ‘eps’ case. The behavior of Algorithm 1 is similar for
s = 5 and s = 10, with the latter case taking more iterations
to enter the second phase of convergence. This makes sense
since there are more variables to learn for larger s.
IV. CONCLUSION
This work presented an analysis of a fast alternating min-
imization algorithm for unitary sparsifying operator learning.
We proved local linear convergence of the algorithm to the
underlying generative model under mild assumptions. Numer-
ical experiments illustrated this local convergence behavior,
and demonstrated that the algorithm is robust to initialization
in practice. The full version of this work, including the proof of
Theorem II.2 and numerical results that support Conjecture 1
can be found in [24]. A theoretical analysis of the algorithm’s
robustness observed in Fig. 2 is left for future work.
ACKNOWLEDGMENTS
Saiprasad Ravishankar was supported in part by the follow-
ing grants: NSF grant CCF-1320953, ONR grant N00014-15-
1-2141, DARPA Young Faculty Award D14AP00086, ARO
MURI grants W911NF-11-1-0391 and 2015-05174-05, NIH
grants R01 EB023618 and U01 EB018753, and a UM-SJTU
seed grant. Anna Ma and Deanna Needell were supported
by the NSF DMS #1440140 (while they were in residence
at the Mathematical Science Research Institute in Berkeley,
California, during the Fall 2017 semester), NSF CAREER
DMS #1348721, and the NSF BIGDATA DMS #1740325.
REFERENCES
[1] M. Aharon, M. Elad, and A. Bruckstein, “K-SVD: An algorithm for
designing overcomplete dictionaries for sparse representation,” IEEE
Transactions on Signal Processing, vol. 54, no. 11, pp. 4311–4322, 2006.
[2] M. Yaghoobi, T. Blumensath, and M. Davies, “Dictionary learning for
sparse approximations with the majorization method,” IEEE Transac-
tions on Signal Processing, vol. 57, no. 6, pp. 2178–2191, 2009.
[3] J. Mairal, F. Bach, J. Ponce, and G. Sapiro, “Online learning for matrix
factorization and sparse coding,” J. Mach. Learn. Res., vol. 11, pp.
19–60, 2010.
[4] D. Barchiesi and M. D. Plumbley, “Learning incoherent dictionaries for
sparse approximation using iterative projections and rotations,” IEEE
Transactions on Signal Processing, vol. 61, no. 8, pp. 2055–2065, 2013.
[5] L. N. Smith and M. Elad, “Improving dictionary learning: Multiple
IEEE Signal Processing
dictionary updates and coefﬁcient reuse,”
Letters, vol. 20, no. 1, pp. 79–82, Jan 2013.
[6] C. Bao, H. Ji, Y. Quan, and Z. Shen, “L0 norm based dictionary learning
by proximal methods with global convergence,” in IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2014, pp. 3858–
3865.
[7] S. Ravishankar, R. R. Nadakuditi, and J. A. Fessler, “Efﬁcient sum of
outer products dictionary learning (SOUP-DIL) and its application to
inverse problems,” IEEE Transactions on Computational Imaging, vol.
3, no. 4, pp. 694–709, Dec 2017.
[8] S. Ravishankar, B. E. Moore, R. R. Nadakuditi, and J. A. Fessler,
“Efﬁcient learning of dictionaries with low-rank atoms,” in 2016 IEEE
Global Conference on Signal and Information Processing (GlobalSIP),
Dec 2016, pp. 222–226.
