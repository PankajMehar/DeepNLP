
Compressive Sensing (CS) is a method in signal processing
which aims to reconstruct signals from a small number of
measurements. By exploiting the fact that the unknown signal
is sparse, its reconstruction is achieved with sampling rates far
less than the Nyquist rate [1], [2].
In this paper, we focus on Binary Compressive Sensing
(BCS) which conﬁnes the signals of interest only to binary
{0, 1}-valued binary signals. This is motivated by the wide use
of binary signals in engineering, for example, fault detection
[3], black-and-white image reconstruction [4], and digital com-
munications [5], to name a few. Related works are as follows.
Nakarmi et al. [6] designed a novel sensing matrix tailored for
binary signal reconstruction. Wang et al. [7] combined ℓ1 norm
with ℓ∞ norm to reconstruct sparse binary signals. Nagahara
[8] exploited the sum of weighted ℓ1 norms to effectively
reconstruct signals whose entries are integer-valued and, in
particular, binary signals and bitonal images. Keiper et al. [9]
analyzed the phase transition of binary Basis Pursuit.
To the best of our knowledge, most of BCS algorithms
developed so far are based on convex optimization methods.
Although this family of methods enjoys appealing theoretical
properties such as reconstruction guarantees (e.g. [9], [10]),
they were found to be slow in large-scale applications [11].
The purpose of this work is to ﬁll in the gap by propos-
ing a fast BCS method that attains accurate reconstruction
results. The proposed method leverages the gradient descent
approach for smoothed ℓ0 norm [12] by introducing an ad-
ditional probability distribution prior for binary signals in
the penalty function. We empirically compare the proposed
method with state-of-the-art convex-optimization-based BCS
algorithms and demonstrate its advantage of in reconstruction
speed, and more impressively, in reconstruction accuracy.
The paper is organized as follows. First we present a short
review on CS and BCS algorithms in Section II. We then
present the proposed algorithm in Section III. This is the main
contribution of the paper. Emperical experiments results are
presented in Section IV to demonstrate the competitiveness of
the proposed algorithm. We given conclusion in Section V.
Notations:
For a vector v = [v1, · · · , vN ]⊤, we use kvkp to denote the
ℓp norm of v, where 1 ≤ p ≤ ∞. We use kvk0 to denote the
the number of non-zero entries of the vector v. P(E) denotes
the probability of an event E. For an integer n, we use [n] as
a short-hand notation for the set {1, · · · , n}. We write 1N as
the N -dimensional column vector with all entries equal to 1.
II. BINARY COMPRESSIVE SENSING (BCS)
In the standard CS scheme, one aims to recover a sparse
signal from its linear measurements. The constraints posed by
the measurements can be formulated as
Φz = y,
(1)
where Φ ∈ Rm×N , m ≪ N , is a measurement matrix, z ∈
RN , and y = Φx is the measurement vector of the sparse
signal x ∈ RN . CS algorithms exploit the fact that x is sparse
and seek a sparse solution z of Equation 1.
In the BCS scheme, signals of interest are conﬁned to binary
signals. Notice that a binary signal x is sparse if and only if its
As the measurement matrix Φ is known to the observer, (1)
recover dense binary signals as well as the sparse ones.
complementary signalex := 1N − x is almost fully supported.
can be converted to Φez = ey, where ez := 1N − z and ey :=
Φex = Φ1N − y, and vice versa. This observation allows us to
Two types of models for binary signals have been consid-
ered in the literature (e.g., [13], [7], [8]). (i) Sparseness prior:
x is a deterministic vector which is binary and sparse, i.e.,
most of its entries are 0 and only few are 1; (ii) Probability
distribution prior: x is a random vector whose entries are
independent and identically distributed (i.i.d.) with probability
distribution P(xj = 1) = p for some ﬁxed 0 ≤ p ≤ 1. If p
is small, a realization of x is likely a sparse binary signal. In
this work, we shall consider the second model which covers
dense binary signals with large support as well as sparse binary
signals.
Below we give a short review on CS/BCS methods that are
related to our work.
A. ℓ0 minimization (L0)
A naive approach to ﬁnding sparse solutions is the ℓ0
minimization,
min
z∈RN
kzk0
subject to Φz = y.
(P0)
This method works generally for continuously-valued signals
i.e., signals x ∈ RN whose entries are
that are sparse,
mostly zero. However, solving ℓ0 minimization requires a
combinatorial search and is therefore NP-hard [14].
B. Smoothed ℓ0 minimization (SL0)
z2
z2
The Smoothed ℓ0 minimization (SL0) [12] replaces the ℓ0
norm in (P0) with a non-convex relaxation:
Φz = y
Φz = y
min
z∈RN
This is motivated by the observation that
which implies for any z = (z1, . . . , zN )⊤ ∈ RN ,
lim
σ→0
NXi=1(cid:18)1 − exp(cid:18) −z2
2σ2(cid:19)(cid:19) subject to Φz = y.
2σ2(cid:21) =(1
exp(cid:20) −t2
NXi=1(cid:18)1 − exp(cid:20) −z2
i=1(cid:16)1 − exph −z2
2σ2(cid:21)(cid:19) = kzk0.
2σ2i(cid:17) is a smooth func-
if t = 0
if t 6= 0,
lim
σ→0
(2)
Noticing that z 7→PN
tion for any ﬁxed σ > 0, Mohimani et al. [12] proposed an
algorithm based on the gradient descent method. The algorithm
iteratively obtains an approximate solution by decreasing σ.
Mohammadi et al. [15] adapted the SL0 algorithm par-
ticularly to non-negative signals. Their algorithm, called the
Constrained Smoothed ℓ0 method (CSL0), incorporates the
non-negativity constraints by introducing appropriate weight
functions into their cost function. Empirically, they showed
that CSL0 achieves better performance than SL0 for recon-
struction of non-negative signals.
C. Basis Pursuit (BP)
A well-known and by now standard relaxation of (P0) is the
ℓ1-minimization, also known as the Basis Pursuit (BP) [16]:
min
z∈RN
kzk1
subject to Φz = y.
(P1)
Similarly as
continuously-valued signals x ∈ RN that are sparse.
this method works generally for
(P0),
D. Boxed Basis Pursuit (Boxed BP)
Donoho et al. [13] proposed the Boxed Basis Pursuit (Boxed
BP) for the reconstruction of k-simple bounded signals:
min
z∈[0,1]N
kzk1
subject to Φz = y.
The intuition behind Boxed BP is straightforward: the ℓ1
norm minimization promotes sparsity of the solution while
the restriction z ∈ [0, 1]N reduces the set of feasible solutions.
Recently, Keiper et al. [9] analyzed the performance of Boxed
BP for reconstruction of binary signals.
E. Sum of Norms (SN)
Wang et al. [7] introduced the following optimization prob-
lem which combines the ℓ1 and the ℓ∞ norm:
min
z∈RN
kzk1 + λkz −
· 1N k∞ subject to Φz = y.
As before, minimizing the term kzk1 promotes sparsity of z.
On the other hand, minimizing the term kz − 1
2 · 1N k∞ forces
the entries |zi − 1
2 | to be small but of equal magnitude. These
facts are illustrated in Fig. 1. The two norms are balanced by
a tuning parameter λ > 0.
( 1
2 , 1
2 )
z1
(1, 0)
z1
(1, 0)
Fig. 1. Left: the minimization of kzk1 ﬁnds sparse solutions, Right: the
minimization of kz − 1
2 | to be small but
of equal magnitude.
2 · 1N k∞ forces the entries |zi − 1
(0, p)
(1, 1 − p)
zi
Fig. 2. The contribution of zi to the function (1 − p)kzk1 + pkz − 1N k1.
F. Sum of Absolute Values (SAV)
Nagahara [8] proposed the following method for reconstruc-
tion of discrete signals whose entries are chosen independently
from a set of ﬁnite alphabets α = {α1, α2, . . . , αL} with
a priori known probability distribution. In the special case
α = {0, 1} of binary signals, SAV is formulated as,
min
z∈RN
(1 − p)kzk1 + pkz − 1N k1
subject to Φz = y,
where p = P(xj = 1), j ∈ [N ], is the probability distribution
of the entries of x. Here, the contribution of each entry zi to
the function (1 − p)kzk1 + pkz − 1N k1 is given by (Fig. 2)
if zi < 0,
−zi + p
(1 − 2p)zi + p if 0 ≤ zi < 1,
zi − p
if zi ≥ 1.

If p ≈ 0, i.e., x is sparse, then (1−p)kzk1+pkz−1N k1 ≈ kzk1
which corresponds to BP.
III. BOX-CONSTRAINED SUM OF SMOOTHED ℓ0
Note that L0 and SL0 utilize the ℓ0 norm and its smoothed
version respectively, however, they do not exploit the fact that
x is binary. On the other hand, BP, Boxed BP, SN, and SAV
utilize the ℓ1 norm in one way or another and all methods
except BP are particularly adjusted to the binary setting. A
natural question arises whether one can also adjust L0 and SL0
to the binary setting and achieve a better recovery performance
for binary signals.
We note that Boxed BP takes into account the binary prior of
x by imposing the restriction x ∈ [0, 1]N . It is straightforward
to apply the same trick to L0 and SL0, which are then called
Boxed L0 and Boxed SL0 respectively. Note however that
Boxed L0 is again NP-hard and is therefore not applicable.
Fig. 3 shows that Boxed SL0 achieves an improvement in
recovery performance over SL0 while requiring a similar run
time. However, its error rate appears to be far worse than
Boxed BP and SN.
In this paper, we aim to adapt the formulation of SAV
and the restriction x ∈ [0, 1]N to SL0, in order to achieve
a better performance than that of existing algorithms. A
straightforward adaptation yields the following problem. For
σ > 0 small,
min
z∈[0,1]N
Fσ(z)
subject to Φz = y,
(3)
where
Fσ(z) , (1 − p)
NXi=1(cid:16)1 − e−z2
i /(2σ2)(cid:17)
NXi=1(cid:16)1 − e−(zi−1)2/(2σ2)(cid:17)
+ p
NXi=1(cid:16)1 − (1 − p) e−z2
i /(2σ2) − p e−(zi−1)2/(2σ2)(cid:17)
and p = P(xj = 1), ∀j ∈ [N ]. Note that by (2), we have
lim
σ→0
Fσ(z) = (1 − p)kzk0 + pkz − 1N k0
so that F0(z) can be approximated by Fσ(z) with small σ > 0.
Next, we will use a weight function to incorporate the
restriction z ∈ [0, 1]N into the function Fσ(z). For integers
k ≥ 1, let
wk(t) ,(1
if 0 ≤ t ≤ 1
k otherwise.
(5)
For σ > 0 and integers k ≥ 1, we deﬁne
F boxed
σ,k (z)
NXi=1
wk(zi)(cid:16)1 − (1 − p) e−z2
i /(2σ2) − p e−(zi−1)2/(2σ2)(cid:17) .
Note that since 1 − (1 − p)e−t2/(2σ2) − p e−(t−1)2/(2σ2) > 0
for all t ∈ R, minimizing F boxed
σ,k (z) forces wk(zi) to be small
so that all zi’s lie within [0, 1]. In this way, the restriction z ∈
[0, 1]N is merged into the penalty function. Our optimization
problem now reads as follows. For σ > 0 small and k ∈ N
large,
min
z∈RN
F boxed
σ,k (z)
subject to Φz = y.
with
The algorithm proposed to solve the problem is based
on the gradient descent method and its implementation is
similar to that of algorithms in [12], [15]. A major differ-
ence of our algorithm from theirs is that the cost function
σ,k (z)
which is designed particularly for binary valued signal, by
adapting the formulation of SAV [8].
2σ2(cid:17)(cid:17) of SL0 is replaced with F boxed
i=1(cid:16)1 − exp(cid:16) −z2
PN
The proposed algorithm is comprised of two nested loops. In
the outer loop, we slowly decrease σ and iteratively search for
an optimal solution from a coarse to a ﬁne scale by decreasing
σ by a factor of 0 < d < 1. As σ decreases, we also gradually
increase k so that a larger penalty is put on solutions that
Algorithm 1 Box-Constrained Sum of Smoothed ℓ0 (BSSL0)
1: Data: Measurement matrix Φ ∈ Rm×N , observation y ∈
Rm, probability distribution prior p = P(xj = 1);
2: Parameters: The minimal σ: σmin, the inner-loop iteration
the gradient descent factor: µ, and the σ
number: L,
decreasing factor: d;
3: Initialization: ˆx = Φ⊤(ΦΦ⊤)−1
4:
5: for 1 : Iters do
6:
Iters = logd(cid:0) σmin
σ (cid:1), k = 1 + N p
for 1 : L, do
Iters ;
y, σ = 2 max |ˆx|,
ˆx ← ˆx − σ2µ∇F boxed
σ,k (z) % Gradient descent
ˆx ← ˆx − Φ⊤(ΦΦ⊤)−1(Φˆx − y) % Projection
7:
8:
9:
10:
end for
σ = σ × d
k = k + N p
Iters
(4)
11:
12: end for
13: ˆx ← round(ˆx) % Round to a binary vector.
have entries outside the range [0, 1]. The inner loop performs
a gradient descent of L iterations for the function F boxed
σ,k (z),
where σ and k are given from the outer loop. In each iteration,
the obtained solution is projected back into the set of feasible
solutions {z : Φz = y}.
The numerical experiments in Section IV indicate that for
binary signals the proposed algorithm outperforms SL0, as
well as achieving better recovery performance than other
CS/BCS algorithms (BP, Boxed BP, SN, and SAV).
As already mentioned, our algorithm is implemented anal-
ogous to SL0 [12], [15] except for the penalty function. The
parameters used in our algorithm are exactly the same as
in [12] except k and p. As justiﬁed in [12, Section IV-B],
we use the minimum ℓ2 norm solution of Φz = y as an
initial estimate, that is, we initialize ˆx = Φ⊤(ΦΦ⊤)−1
y. The
initialization value for σ is discussed in [12, Remark 5 in
Section III]. Also, the choice of the step-size σ2µ for the
gradient descent is justiﬁed in [12, Remark 2 in Section III],
and the choice of k is explained in [15, Lemma 1].
The gradient of F boxed
σ,k (z), appearing in Algorithm 1, is
obtained by
σ,k (z) =  ∂F boxed
∇F boxed
, . . . ,
∂F boxed
σ,k (z)
σ,k (z)
∂z1
∂zN !⊤
σ2 (cid:18)(1 − p)zi exp(cid:20) −z2
2σ2(cid:21)
(cid:21)(cid:19) a.e.,
+p (zi − 1) exp(cid:20) −(zi − 1)2
wk(zi)
2σ2
∂F boxed
σ,k (z)
∂zi
where we used the fact that w′
k(t) = 0 for all t except t = 0, 1.
IV. NUMERICAL EXPERIMENTS
In this section, we empirically investigate the proposed
algorithm BSSL0. In particular, the performance of BSSL0
is compared with the state-of-the-art CS and BCS algorithms
described in Section II. Our experiment have been carried
reconstruction accuracy. At the same time, BSSL0 is several
orders of faster compared with the baseline methods.
B. Experiment 2: Bitonal Image Reconstruction
We repeat
the bitonal
image reconstruction experiment
designed in [8]. Consider the 37 × 37-pixel bitonal image
shown in Fig. 4 (left). Random Gaussian noise with a mean of
0 and a standard deviation of 0.1 is added to each pixel, and
the resulted blurred image is shown in Fig. 4 (right). Denote
the disturbed image as X.
Fig. 4. Original image (left) and noise-added image (right).
We apply the discrete Fourier transform (DFT) to the real
valued matrix X. This could be written as a linear equation:
(W ⊗ W )vec(X) = vec( ˆX),
where W is the DFT matrix.
Let y ∈ C685 be the half-size randomly downsampled
vector of vec( ˆX). Let the sensing matrix Φ ∈ C685×1369
be the matrix generated by corresponding randomly down-
sampling row vectors from W ⊗W . We present y and Φ to the
optimization methods and reconstruct the image. For SN, the
tuning parameter λ is empirically tested from 50 to 1000 in the
stepsize of 50, and the λ = 800 is chosen as it performed best.
For SAV and BSSL0, we have chosen p = P(xj = 0) = 0.5
as a rough estimate for the sparsity of the bitonal image (this
estimate p = 0.5 was also used in [8]). We set σmin = 0.01,
d = 0.9, µ = 2, and L = 3 for the parameters of BSSL0.
out
in MATLAB R2015a environment on a 2GHz Intel
Core i7 Macintosh notebook computer with 8 GB RAM.
The codes for reproducing the experiments are available at
https://github.com/liutianlin0121/BSSL0.
A. Experiment 1: Binary Sparse Signal Reconstruction
In this experiment, we reconstruct binary signal using
BSSL0 and other baseline methods. Let the measurement ma-
trix Φ ∈ R40×100 be random Gaussian, that is, each component
of Φ is drawn from the standard normal distribution. For each
p, varied from 0 to 1 with step-size 0.05, the binary signal
x ∈ R100 is generated with all its entries drawn independently
by P(xi = 1) = p and P(xi = 0) = 1 − p. With Φ and x
generated, we compute the measurement vector y = Φx and
apply the respective algorithms (BP, Boxed BP, SN, SAV, SL0,
Boxed SL0, BSSL0) to get the reconstructed signal z, which
is an estimate of x. Note that Boxed SL0 is a special case
of CSL0 that constrains all signals to the boxed range [0,1].
The following measures are considered for the evaluation of
reconstruction result: (i) Failure of Perfect Reconstruction
(FPR): 1 if z 6= x (failed to perfectly reconstruct) and 0
otherwise (succeed to perfectly reconstruct); (ii) Noise Signal
Ratio (NSR): NSR = kx−zk2
(as deﬁned in [8]). (iii) Run time
kxk2
measured by the tic/toc command in MATLAB as a coarse
estimation of the complexities of the algorithms. For a ﬁxed
p, the process up to this point is repeated 10000 times and
evaluation results for the measurements are averaged. For SN,
we set the parameter λ to be 100 as ﬁne-tuned in [7]. For
BSSL0, we let σmin = 0.1, d = 0.5, µ = 2, and L = 1000.
0.5
0.5
0.5
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
1 − p
BP
Boxed BP
SN
SAV
SL0
Boxed SL0
BSSL0
Fig. 3. Upper ﬁgure: Failure of Perfect Recovery; middle ﬁgure: NSR; lower
ﬁgure: Run time
From Fig. 3 we see that, compared with the baseline meth-
ods, BSSL0 yield better reconstruction results in all metrics of
Fig. 5. Reconstructed images by the BP (upper left), by SN (upper right), by
SAV (lower left), and by proposed BSSL0 (lower right).
TABLE I
THE RUN TIME COMPARISON
Optimization Method
Basis Pursuit
SN
SAV
BSSL0 (proposed)
Run Time
185.2044 seconds
406.1007 seconds
191.5366 seconds
0.92577 seconds
[14] B. K. Natarajan, “Sparse approximate solutions to linear systems,” SIAM
Journal on Computing, vol. 24, no. 2, pp. 227–234, 1995.
[15] M. Mohammadi, E. Fatemizadeh, and M. H. Mahoor, “Non-negative
sparse decomposition based on constrained smoothed ℓ0 norm,” Signal
Processing, vol. 100, pp. 42–50, 2014.
[16] S. S. Chen, D. L. Donoho, and M. A. Saunders, “Atomic decomposition
by basis pursuit,” SIAM Review, vol. 43, no. 1, pp. 129–159, 2001.
The reconstructed ﬁgures from BP, SN, SAV, and BSSL0
is shown in Fig. 5. The time consumption is shown in
Tab. I. The ﬁgures and the table show that the proposed
BSSL0 signiﬁcantly substantially enhances the reconstruction
accuracy as well as the speed.
V. CONCLUSION
In this paper, we proposed a novel algorithm BSSL0 for
reconstructing binary signals from reduced measurements. The
algorithm is derived based on smooth relaxation techniques
and gradient descent methods. By experiments, we show that
the proposed algorithm outperforms the state-of-the-art convex
optimization methods in its reconstruction accuracy as well as
in its speed.
ACKNOWLEDGMENT
T. Liu and D. G. Lee acknowledge the support by the DFG
Grant PF 450/6-1. The authors are grateful to Robert Fischer
and G¨otz E. Pfander for helpful suggestions.
REFERENCES
[1] D. L. Donoho, “Compressed sensing,” IEEE Transactions on Informa-
tion Theory, vol. 52, no. 4, pp. 1289–1306, 2006.
[2] S. Foucart and H. Rauhut, A Mathematical Introduction to Compressive
Sensing. Basel: Birkh¨auser, 2013.
[3] D. Bickson, D. Baron, A. Ihler, H. Avissar, and D. Dolev, “Fault
identiﬁcation via nonparametric belief propagation,” IEEE Transactions
on Signal Processing, vol. 59, no. 6, pp. 2602–2613, 2011.
[4] M. F. Duarte, M. A. Davenport, D. Takbar, J. N. Laska, T. Sun,
K. F. Kelly, and R. G. Baraniuk, “Single-pixel imaging via compressive
sampling,” IEEE signal Processing Magazine, vol. 25, no. 2, pp. 83–91,
2008.
[5] K. Wu and X. Guo, “Compressive sensing of digital sparse signals,” in
Wireless Communications and Networking Conference (WCNC), 2011
IEEE.
IEEE, 2011, pp. 1488–1492.
[6] U. Nakarmi and N. Rahnavard, “Bcs: Compressive sensing for binary
IEEE,
sparse signals,” in Military Communications Conference 2012.
2012, pp. 1–5.
[7] S. Wang and N. Rahnavard, “Binary compressive sensing via sum
of ℓ1-norm and ℓ∞-norm regularization,” in Military Communications
Conference 2013 IEEE.
IEEE, 2013, pp. 1616–1621.
[8] M. Nagahara, “Discrete signal reconstruction by sum of absolute values,”
IEEE Signal Processing Letters, vol. 22, no. 10, pp. 1575–1579, 2015.
[9] S. Keiper, G. Kutyniok, D. G. Lee, and G. E. Pfander, “Compressed
sensing for ﬁnite-valued signals,” Linear Algebra and its Applications,
2017.
[10] J.-H. Lange, M. E. Pfetsch, B. M. Seib, and A. M. Tillmann, “Sparse
recovery with integrality constraints,” arXiv preprint arXiv:1608.08678,
2016.
[11] D. L. Donoho and Y. Tsaig, “Fast solution of ell1-norm minimization
problems when the solution may be sparse,” IEEE Transactions on
Information Theory, vol. 54, no. 11, pp. 4789–4812, 2008.
[12] H. Mohimani, M. Babaie-Zadeh, and C. Jutten, “A fast approach for
overcomplete sparse decomposition based on smoothed ℓ0 norm,” IEEE
Transactions on Signal Processing, vol. 57, no. 1, pp. 289–301, 2009.
[13] D. L. Donoho and J. Tanner, “Precise undersampling theorems,” Pro-
ceedings of the IEEE, vol. 98, no. 6, pp. 913–924, 2010.
