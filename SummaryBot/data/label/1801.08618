
Deep neural networks are among the most influential architectures
of deep learning algorithms, being deployed in many mobile intelli-
gent applications. End-side services, such as intelligent personal
assistants (IPAs), autonomous cars, and smart home services often
employ either simple local models or complex remote models on the
cloud. Mobile-only and cloud-only computations are currently the
status-quo approaches. In this paper, we propose an efficient, adap-
tive, and practical engine, JointDNN, for collaborative computation
between a mobile device and cloud for DNNs in both inference
and training phase. JointDNN not only provides an energy and
performance efficient method of querying DNNs for the mobile
side, but also benefits the cloud server by reducing the amount of
its workload and communications compared to the cloud-only ap-
proach. Given the DNN architecture, we investigate the efficiency
of processing some layers on the mobile device and some layers
on the cloud server. We provide optimization formulations at layer
granularity for forward and backward propagation in DNNs, which
can adapt to mobile battery limitations and cloud server load con-
straints and quality of service. JointDNN achieves up to 18× and
32× reductions on the latency and mobile energy consumption of
querying DNNs compared to the status-quo approaches, respec-
tively.
KEYWORDS
mobile cloud computing, deep learning, deep neural networks, in-
telligent services
1 