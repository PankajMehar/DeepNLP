
JRMONIZ@ANDREW.CMU.EDU
DAVID.KRUEGER@UMONTREAL.CA
We propose Nested LSTMs (NLSTM), a novel RNN architecture with multiple levels of memory.
Nested LSTMs add depth to LSTMs via nesting as opposed to stacking. The value of a memory cell
in an NLSTM is computed by an LSTM cell, which has its own inner memory cell. Speciﬁcally,
= ft (cid:12) ct−1 + it (cid:12) gt, NLSTM
instead of computing the value of the (outer) memory cell as couter
memory cells use the concatenation (ft (cid:12) ct−1, it (cid:12) gt) as input to an inner LSTM (or NLSTM)
memory cell, and set couter
. Nested LSTMs outperform both stacked and single-layer
LSTMs with similar numbers of parameters in our experiments on various character-level language
modeling tasks, and the inner memories of an LSTM learn longer term dependencies compared with
the higher-level units of a stacked LSTM.
Keywords: Nested LSTMs, LSTMs, Character-Level Language Modeling
= hinner
1. 