
It is desirable to train convolutional networks (CNNs) to
run more efﬁciently during inference. In many cases how-
ever, the computational budget that the system has for in-
ference cannot be known beforehand during training, or
the inference budget is dependent on the changing real-time
resource availability. Thus, it is inadequate to train just
inference-efﬁcient CNNs, whose inference costs are not ad-
justable and cannot adapt to varied inference budgets. We
propose a novel approach for cost-adjustable inference in
CNNs - Stochastic Downsampling Point (SDPoint). During
training, SDPoint applies feature map downsampling to a
random point in the layer hierarchy, with a random down-
sampling ratio. The different stochastic downsampling con-
ﬁgurations known as SDPoint instances (of the same model)
have computational costs different from each other, while
being trained to minimize the same prediction loss. Shar-
ing network parameters across different instances provides
signiﬁcant regularization boost. During inference, one may
handpick a SDPoint instance that best ﬁts the inference bud-
get. The effectiveness of SDPoint, as both a cost-adjustable
inference approach and a regularizer, is validated through
extensive experiments on image classiﬁcation.
1. 