
In model-based reinforcement learning it is typi-
cal to treat the problems of learning the dynamics
model and learning the reward function separately.
However, when the dynamics model is ﬂawed, it
may generate erroneous states that would never
occur in the true environment. A reward func-
tion trained only to map environment states to
rewards (as is typical) would have little guidance
in such states. This paper presents a novel error
bound that accounts for the reward model’s be-
havior in states sampled from the model. This
bound is used to extend the existing Hallucinated
DAgger-MC algorithm, which offers theoretical
performance guarantees in deterministic MDPs
that do not assume a perfect model can be learned.
Empirically, this approach to reward learning can
yield dramatic improvements in control perfor-
mance when the dynamics model is ﬂawed.
1. 