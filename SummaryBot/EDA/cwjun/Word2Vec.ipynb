{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=pd.read_pickle(\"sourceDF.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fusarium Damaged Kernels Detection Using Trans...</td>\n",
       "      <td>. The present work shows the application of tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VR Goggles for Robots: Real-to-sim Domain Adap...</td>\n",
       "      <td>This paper deals with the reality gap from a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Matrix Completion for Structured Observations</td>\n",
       "      <td>— The need to predict or ﬁll-in missing data, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stochastic Downsampling for Cost-Adjustable In...</td>\n",
       "      <td>It is desirable to train convolutional networ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Learning Combinations of Activation Functions</td>\n",
       "      <td>—In the last decade, an active area of researc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ALTERNATING MULTI-BIT QUANTIZATION FOR RECURRE...</td>\n",
       "      <td>Recurrent neural networks have achieved excel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AUGMENTED SPACE LINEAR MODEL</td>\n",
       "      <td>The linear model uses the space deﬁned by the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>JointDNN: An Efficient Training and Inference ...</td>\n",
       "      <td>Deep neural networks are among the most influ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Low-rank Bandit Methods for High-dimensional D...</td>\n",
       "      <td>We consider high dimensional dynamic multi-pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Less is more: sampling chemical space with act...</td>\n",
       "      <td>The  development  of  accurate  and  transfe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Distributed Newton Methods for Deep Neural</td>\n",
       "      <td>Deep learning involves a difﬁcult non-convex ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>DISCRETE AUTOENCODERS FOR SEQUENCE MODELS</td>\n",
       "      <td>Recurrent models for sequences have been rece...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Kernel Distillation for Gaussian Processes</td>\n",
       "      <td>Gaussian processes (GPs) are ﬂexible models t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Short-term Memory of Deep RNN</td>\n",
       "      <td>. The extension of deep learning towards tempo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Training Neural Networks by Using Power Linear...</td>\n",
       "      <td>In this paper, we introduce ”Power Linear Uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Elements of Effective Deep Reinforcement Learn...</td>\n",
       "      <td>Tactical driving decision making is crucial f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Rigorous Restricted Isometry Property of Low-D...</td>\n",
       "      <td>Dimensionality reduction is in demand to redu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ReNN: Rule-embedded Neural Networks</td>\n",
       "      <td>—The artificial neural network shows powerful ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Interpretable Deep Convolutional Neural Networ...</td>\n",
       "      <td>—Model interpretability is a requirement in ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>On Polynomial time Constructions of Minimum He...</td>\n",
       "      <td>. A decision tree T in Bm := {0, 1}m is a bina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Improving Active Learning in Systematic Reviews</td>\n",
       "      <td>Systematic reviews are essential to summarizi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>One-class Collective Anomaly Detection based o...</td>\n",
       "      <td>.  Intrusion  detection for computer network s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Fast Binary Compressive Sensing via \\ell_0 Gra...</td>\n",
       "      <td>—We present a fast Compressive Sensing algorit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3D Object Dense Reconstruction from a Single D...</td>\n",
       "      <td>—In this paper, we propose a novel approach, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Pretraining Deep Actor-Critic Reinforcement Le...</td>\n",
       "      <td>Pretraining with expert demonstrations have b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>COBRA: A Fast and Simple Method for Active Clu...</td>\n",
       "      <td>Clustering is inherently ill-posed: there oft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Sensitivity Sampling Over Dynamic Geometric Da...</td>\n",
       "      <td>Sensitivity based sampling is crucial for con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>An Incremental Path-Following Splitting Method...</td>\n",
       "      <td>The linearly constrained nonconvex nonsmooth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>The Intriguing Properties of Model Explanations</td>\n",
       "      <td>Linear approximations to the decision boundar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Learning to Emulate an Expert Projective Cone ...</td>\n",
       "      <td>Projective cone scheduling deﬁnes a large cla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Deep Neural Networks with Data Dependent Impli...</td>\n",
       "      <td>Though deep neural networks (DNNs) achieve re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Nonlinear Dimensionality Reduction on Graphs</td>\n",
       "      <td>—In this era of data deluge, many signal proce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>DeepDTA: Deep Drug-Target Binding Affinity Pre...</td>\n",
       "      <td>The identification of novel drug-target (DT) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Deep Learning of Constrained Autoencoders for ...</td>\n",
       "      <td>—Unsupervised feature extractors are known to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>DxNAT - Deep Neural Networks for Explaining No...</td>\n",
       "      <td>—Non-recurring trafﬁc congestion is caused by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>A Modiﬁed Sigma-Pi-Sigma Neural Network with A...</td>\n",
       "      <td>Sigma-Pi-Sigma neural networks (SPSNNs) as a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Certified Defenses against Adversarial Examples</td>\n",
       "      <td>While neural networks have achieved high accu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Obfuscated Gradients Give a False Sense of Sec...</td>\n",
       "      <td>We identify obfuscated gradients as a phe- no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Deep Multi-view Learning to Rank</td>\n",
       "      <td>—We study the problem of learning to rank from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Matrix completion with deterministic pattern a...</td>\n",
       "      <td>We consider the matrix completion problem wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Cardiac Arrhythmia Detection from ECG Combinin...</td>\n",
       "      <td>The most common type of arrhythmia is atrial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>FASTGCN: FAST LEARNING WITH GRAPH CONVOLU-TION...</td>\n",
       "      <td>The graph convolutional networks (GCN) recent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Learning Families of Formal Languages from Pos...</td>\n",
       "      <td>. For 50 years, research in the area of induct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Scalable L´evy Process Priors for Spectral Ker...</td>\n",
       "      <td>Gaussian processes are rich distributions ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Evaluating approaches for supervised semantic ...</td>\n",
       "      <td>Relational data sources are still one of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Analysis of Fast Alternating Minimization for ...</td>\n",
       "      <td>—Methods exploiting sparsity have been popular...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Nested LSTMs</td>\n",
       "      <td>JRMONIZ@ANDREW.CMU.EDU DAVID.KRUEGER@UMONTREA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Optimizing Non-decomposable Measures with Deep...</td>\n",
       "      <td>We present a class of algorithms capable of d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>On the Inter-relationships among Drift rate, F...</td>\n",
       "      <td>We propose two general and falsiﬁable hypothe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Fast Power system security analysis with Guide...</td>\n",
       "      <td>. We propose a new method to eﬃciently compute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Causal Learning and Explanation of Deep Neural...</td>\n",
       "      <td>Deep neural networks are complex and opaque. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Clustering and Unsupervised Anomaly Detection ...</td>\n",
       "      <td>—Clustering is essential to many tasks in patt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Spherical CNNs</td>\n",
       "      <td>Convolutional Neural Networks (CNNs) have bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Incremental kernel PCA and the Nystr¨om method</td>\n",
       "      <td>Incremental versions of batch algorithms are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Algorithmic Linearly Constrained Gaussian Proc...</td>\n",
       "      <td>We algorithmically construct multi-output Gau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>A Survey of Recent Advances in Texture Represe...</td>\n",
       "      <td>Texture is a fundamental characteristic of ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>EVALUATING THE ROBUSTNESS OF NEURAL NETWORKS: ...</td>\n",
       "      <td>The robustness of neural networks to adversar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>ACCELERATING RECURRENT NEURAL NETWORK LANGUAGE...</td>\n",
       "      <td>This paper presents methods to accelerate rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Personalized Survival Prediction with Contextu...</td>\n",
       "      <td>Accurate and transparent prediction of cancer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>LINKS: A HIGH-DIMENSIONAL ONLINE CLUSTERING ME...</td>\n",
       "      <td>We present a novel algorithm, called Links, d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   Fusarium Damaged Kernels Detection Using Trans...   \n",
       "1   VR Goggles for Robots: Real-to-sim Domain Adap...   \n",
       "2       Matrix Completion for Structured Observations   \n",
       "3   Stochastic Downsampling for Cost-Adjustable In...   \n",
       "4       Learning Combinations of Activation Functions   \n",
       "5   ALTERNATING MULTI-BIT QUANTIZATION FOR RECURRE...   \n",
       "6                        AUGMENTED SPACE LINEAR MODEL   \n",
       "7   JointDNN: An Efficient Training and Inference ...   \n",
       "8   Low-rank Bandit Methods for High-dimensional D...   \n",
       "9   Less is more: sampling chemical space with act...   \n",
       "10         Distributed Newton Methods for Deep Neural   \n",
       "11          DISCRETE AUTOENCODERS FOR SEQUENCE MODELS   \n",
       "12         Kernel Distillation for Gaussian Processes   \n",
       "13                      Short-term Memory of Deep RNN   \n",
       "14  Training Neural Networks by Using Power Linear...   \n",
       "15  Elements of Effective Deep Reinforcement Learn...   \n",
       "16  Rigorous Restricted Isometry Property of Low-D...   \n",
       "17               ReNN: Rule-embedded Neural Networks    \n",
       "18  Interpretable Deep Convolutional Neural Networ...   \n",
       "19  On Polynomial time Constructions of Minimum He...   \n",
       "20    Improving Active Learning in Systematic Reviews   \n",
       "21  One-class Collective Anomaly Detection based o...   \n",
       "22  Fast Binary Compressive Sensing via \\ell_0 Gra...   \n",
       "23  3D Object Dense Reconstruction from a Single D...   \n",
       "24  Pretraining Deep Actor-Critic Reinforcement Le...   \n",
       "25  COBRA: A Fast and Simple Method for Active Clu...   \n",
       "26  Sensitivity Sampling Over Dynamic Geometric Da...   \n",
       "27  An Incremental Path-Following Splitting Method...   \n",
       "28    The Intriguing Properties of Model Explanations   \n",
       "29  Learning to Emulate an Expert Projective Cone ...   \n",
       "..                                                ...   \n",
       "41  Deep Neural Networks with Data Dependent Impli...   \n",
       "42       Nonlinear Dimensionality Reduction on Graphs   \n",
       "43  DeepDTA: Deep Drug-Target Binding Affinity Pre...   \n",
       "44  Deep Learning of Constrained Autoencoders for ...   \n",
       "45  DxNAT - Deep Neural Networks for Explaining No...   \n",
       "46  A Modiﬁed Sigma-Pi-Sigma Neural Network with A...   \n",
       "47    Certified Defenses against Adversarial Examples   \n",
       "48  Obfuscated Gradients Give a False Sense of Sec...   \n",
       "49                   Deep Multi-view Learning to Rank   \n",
       "50  Matrix completion with deterministic pattern a...   \n",
       "51  Cardiac Arrhythmia Detection from ECG Combinin...   \n",
       "52  FASTGCN: FAST LEARNING WITH GRAPH CONVOLU-TION...   \n",
       "53  Learning Families of Formal Languages from Pos...   \n",
       "54  Scalable L´evy Process Priors for Spectral Ker...   \n",
       "55  Evaluating approaches for supervised semantic ...   \n",
       "56  Analysis of Fast Alternating Minimization for ...   \n",
       "57                                       Nested LSTMs   \n",
       "58  Optimizing Non-decomposable Measures with Deep...   \n",
       "59  On the Inter-relationships among Drift rate, F...   \n",
       "60  Fast Power system security analysis with Guide...   \n",
       "61  Causal Learning and Explanation of Deep Neural...   \n",
       "62  Clustering and Unsupervised Anomaly Detection ...   \n",
       "63                                     Spherical CNNs   \n",
       "64     Incremental kernel PCA and the Nystr¨om method   \n",
       "65  Algorithmic Linearly Constrained Gaussian Proc...   \n",
       "66  A Survey of Recent Advances in Texture Represe...   \n",
       "67  EVALUATING THE ROBUSTNESS OF NEURAL NETWORKS: ...   \n",
       "68  ACCELERATING RECURRENT NEURAL NETWORK LANGUAGE...   \n",
       "69  Personalized Survival Prediction with Contextu...   \n",
       "70  LINKS: A HIGH-DIMENSIONAL ONLINE CLUSTERING ME...   \n",
       "\n",
       "                                             abstract  \n",
       "0   . The present work shows the application of tr...  \n",
       "1    This paper deals with the reality gap from a ...  \n",
       "2   — The need to predict or ﬁll-in missing data, ...  \n",
       "3    It is desirable to train convolutional networ...  \n",
       "4   —In the last decade, an active area of researc...  \n",
       "5    Recurrent neural networks have achieved excel...  \n",
       "6    The linear model uses the space deﬁned by the...  \n",
       "7    Deep neural networks are among the most influ...  \n",
       "8    We consider high dimensional dynamic multi-pr...  \n",
       "9     The  development  of  accurate  and  transfe...  \n",
       "10   Deep learning involves a difﬁcult non-convex ...  \n",
       "11   Recurrent models for sequences have been rece...  \n",
       "12   Gaussian processes (GPs) are ﬂexible models t...  \n",
       "13  . The extension of deep learning towards tempo...  \n",
       "14   In this paper, we introduce ”Power Linear Uni...  \n",
       "15   Tactical driving decision making is crucial f...  \n",
       "16   Dimensionality reduction is in demand to redu...  \n",
       "17  —The artificial neural network shows powerful ...  \n",
       "18  —Model interpretability is a requirement in ma...  \n",
       "19  . A decision tree T in Bm := {0, 1}m is a bina...  \n",
       "20   Systematic reviews are essential to summarizi...  \n",
       "21  .  Intrusion  detection for computer network s...  \n",
       "22  —We present a fast Compressive Sensing algorit...  \n",
       "23  —In this paper, we propose a novel approach, 3...  \n",
       "24   Pretraining with expert demonstrations have b...  \n",
       "25   Clustering is inherently ill-posed: there oft...  \n",
       "26   Sensitivity based sampling is crucial for con...  \n",
       "27   The linearly constrained nonconvex nonsmooth ...  \n",
       "28   Linear approximations to the decision boundar...  \n",
       "29   Projective cone scheduling deﬁnes a large cla...  \n",
       "..                                                ...  \n",
       "41   Though deep neural networks (DNNs) achieve re...  \n",
       "42  —In this era of data deluge, many signal proce...  \n",
       "43   The identification of novel drug-target (DT) ...  \n",
       "44  —Unsupervised feature extractors are known to ...  \n",
       "45  —Non-recurring trafﬁc congestion is caused by ...  \n",
       "46   Sigma-Pi-Sigma neural networks (SPSNNs) as a ...  \n",
       "47   While neural networks have achieved high accu...  \n",
       "48   We identify obfuscated gradients as a phe- no...  \n",
       "49  —We study the problem of learning to rank from...  \n",
       "50   We consider the matrix completion problem wit...  \n",
       "51    The most common type of arrhythmia is atrial...  \n",
       "52   The graph convolutional networks (GCN) recent...  \n",
       "53  . For 50 years, research in the area of induct...  \n",
       "54   Gaussian processes are rich distributions ove...  \n",
       "55   Relational data sources are still one of the ...  \n",
       "56  —Methods exploiting sparsity have been popular...  \n",
       "57   JRMONIZ@ANDREW.CMU.EDU DAVID.KRUEGER@UMONTREA...  \n",
       "58   We present a class of algorithms capable of d...  \n",
       "59   We propose two general and falsiﬁable hypothe...  \n",
       "60  . We propose a new method to eﬃciently compute...  \n",
       "61   Deep neural networks are complex and opaque. ...  \n",
       "62  —Clustering is essential to many tasks in patt...  \n",
       "63   Convolutional Neural Networks (CNNs) have bec...  \n",
       "64   Incremental versions of batch algorithms are ...  \n",
       "65   We algorithmically construct multi-output Gau...  \n",
       "66   Texture is a fundamental characteristic of ma...  \n",
       "67   The robustness of neural networks to adversar...  \n",
       "68   This paper presents methods to accelerate rec...  \n",
       "69   Accurate and transparent prediction of cancer...  \n",
       "70   We present a novel algorithm, called Links, d...  \n",
       "\n",
       "[71 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re_comp=re.compile(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences=re_comp.split(df.abstract[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " 'The present work shows the application of transfer learning for a pre-trained deep neural network (DNN), using a small image dataset (\\xe2\\x89\\x88 12,000) on a single workstation with enabled NVIDIA GPU card that takes up to 1 hour to complete the training task and archive an overall average accuracy of 94.7%.',\n",
       " 'The DNN presents a 20% score of misclassi\\xef\\xac\\x81cation for an external test dataset.',\n",
       " 'The accuracy of the proposed methodology is equivalent to ones using HSI methodology (81%-91%) used for the same task, but with the advantage of being independent on special equipment to classify wheat kernel for FHB symptoms.',\n",
       " '']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xef in position 0: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-39c276268926>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0monepaper\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstract\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msrc_sentences\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monepaper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#     templ=re_comp.split(onepaper)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#     if len(templ) > 10:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/JunChangWook/anaconda2/lib/python2.7/site-packages/nltk/tokenize/__init__.pyc\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \"\"\"\n\u001b[1;32m     93\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/JunChangWook/anaconda2/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1235\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \"\"\"\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/JunChangWook/anaconda2/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \"\"\"\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/JunChangWook/anaconda2/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/JunChangWook/anaconda2/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \"\"\"\n\u001b[1;32m   1315\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/JunChangWook/anaconda2/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/JunChangWook/anaconda2/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_break\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'next_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/JunChangWook/anaconda2/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36mtext_contains_sentbreak\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1335\u001b[0m         \"\"\"\n\u001b[1;32m   1336\u001b[0m         \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m \u001b[0;31m# used to ignore last token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1337\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_annotate_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1338\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/JunChangWook/anaconda2/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_annotate_second_pass\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         \u001b[0mheuristic\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4.1\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfrequent\u001b[0m \u001b[0msentence\u001b[0m \u001b[0mstarter\u001b[0m \u001b[0mheuristic\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4.1\u001b[0m\u001b[0;36m.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m         \"\"\"\n\u001b[0;32m-> 1472\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1473\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_second_pass_annotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/JunChangWook/anaconda2/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \"\"\"\n\u001b[1;32m    309\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/JunChangWook/anaconda2/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_annotate_first_pass\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    575\u001b[0m           \u001b[0;34m-\u001b[0m \u001b[0mellipsis_toks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mindices\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mellipsis\u001b[0m \u001b[0mmarks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \"\"\"\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0maug_tok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_pass_annotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_tok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0maug_tok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/JunChangWook/anaconda2/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_tokenize_words\u001b[0;34m(self, plaintext)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0mparastart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mplaintext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m                 \u001b[0mline_toks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xef in position 0: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "src_sentences=[]\n",
    "len_sent=0\n",
    "\n",
    "for onepaper in df.abstract:\n",
    "    src_sentences += nltk.sent_tokenize(onepaper)\n",
    "#     templ=re_comp.split(onepaper)\n",
    "#     if len(templ) > 10:\n",
    "#         len_sent=len_sent+len(templ)\n",
    "#         src_sentences=src_sentences+templ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124\n",
      "— The need to predict or ﬁll-in missing data, often referred to as matrix completion, is a common challenge in to- day’s data-driven world.\n",
      "Previous strategies typically assume that no structural difference between observed and missing entries exists.\n",
      "Unfortunately, this assumption is woefully unrealistic in many applications.\n",
      "For example, in the classic Netﬂix challenge, in which one hopes to predict user-movie ratings for unseen ﬁlms, the fact that the viewer has not watched a given movie may indicate a lack of interest in that movie, thus suggesting a lower rating than otherwise expected.\n",
      "We propose adjusting the standard nuclear norm minimization strategy for matrix completion to account for such structural differences between observed and unobserved entries by regularizing the values of the unobserved entries.\n",
      "We show that the proposed method outperforms nuclear norm minimization in certain settings.\n",
      "thus violating the assumption of uniform sampling of observed entries across movies.\n",
      "On the ﬂip side, a missing entry may indicate a user’s lack of interest in that particular movie.\n",
      "Similarly, in sensor networks, entries may be missing because of geographic limitations or missing connections; in survey data, incomplete sections may be irrelevant or unimportant to the user.\n",
      "In these settings, it is then reasonable to expect that missing entries have lower values1 than observed entries.\n",
      "In this work, we propose a modiﬁcation to the traditional NNM for matrix completion that still results in a semi-deﬁnite optimization problem, but also encourages lower values among the unobserved entries.\n",
      "We show that this method works better than NNM alone under certain sampling conditions.\n",
      "\n",
      " Recurrent neural networks have achieved excellent performance in many applica- tions.\n",
      "However, on portable devices with limited resources, the models are often too large to deploy.\n",
      "For applications on the server with large scale concurrent requests, the latency during inference can also be very critical for costly computing resources.\n",
      "In this work, we address these problems by quantizing the network, both weights and activations, into multiple binary codes {−1, +1}.\n",
      "We formulate the quantization as an optimization problem.\n",
      "Under the key observation that once the quantization coefﬁcients are ﬁxed the binary codes can be derived efﬁciently by binary search tree, alternating minimization is then applied.\n",
      "We test the quantiza- tion for two well-known RNNs, i.e., long short term memory (LSTM) and gated recurrent unit (GRU), on the language models.\n",
      "Compared with the full-precision counter part, by 2-bit quantization we can achieve ∼16× memory saving and ∼6× real inference acceleration on CPUs, with only a reasonable loss in the accuracy.\n",
      "By 3-bit quantization, we can achieve almost no loss in the accuracy or even surpass the original model, with ∼10.5× memory saving and ∼3× real inference acceleration.\n",
      "Both results beat the exiting quantization works with large margins.\n",
      "We extend our alternating quantization to image classiﬁcation tasks.\n",
      "In both RNNs and feedforward neural networks, the method also achieves excellent performance.\n",
      "\n",
      "  The  development  of  accurate  and  transferable  machine learning  (ML)  potentials  for predicting  molecular  energetics  is  a  challenging  task.\n",
      " The  process  of  data  generation  to  train  such  ML  potentials is a task neither well understood nor researched in detail.\n",
      "In this work, we present a fully  automated  approach  for  the  generation  of  datasets  with  the  intent  of  training  universal  ML  potentials.\n",
      " It  is based on the concept  of active learning  (AL) via Query by Committee (QBC),  which uses the disagreement between an ensemble of ML potentials to infer the reliability of the  ensemble’s prediction.\n",
      "QBC allows our AL algorithm to automatically sample regions of chemical  space  where  the  machine  learned  potential  fails  to  accurately  predict  the  potential  energy.\n",
      " AL  improves the overall fitness of ANAKIN-ME (ANI) deep learning potentials in rigorous test cases  by mitigating human biases in deciding what new training data to use.\n",
      " AL also reduces the training  set size to a fraction of the data required when using naive random sampling techniques.\n",
      "To provide  validation of our AL approach we develop the COMP6 benchmark (publicly available on GitHub),  which contains a diverse set of organic molecules.\n",
      "We show the use of our proposed AL technique  develops  a  universal  ANI  potential  (ANI-1x),  which  provides  very  accurate  energy  and  force  predictions on the entire COMP6 benchmark.\n",
      "This universal potential achieves a level of accuracy  on par with the best ML potentials for single molecule or materials, while remaining applicable to  the general class of organic molecules comprised of the elements CHNO.\n",
      " \n",
      " Deep learning involves a difﬁcult non-convex optimization problem with a large number of weights between any two adjacent layers of a deep structure.\n",
      "To handle large data sets or complicated networks, distributed training is needed, but the calculation of function, gradient, and Hessian is expensive.\n",
      "In particular, the communication and the synchronization cost may become a bottleneck.\n",
      "In this paper, we focus on situations where the model is distributedly stored, and propose a novel distributed Newton method for training deep neural networks.\n",
      "By variable and feature-wise data partitions, and some careful designs, we are able to explicitly use the Jacobian matrix for matrix-vector products in the Newton method.\n",
      "Some techniques are incorporated to reduce the running time as well as the memory con- sumption.\n",
      "First, to reduce the communication cost, we propose a diagonalization method such that an approximate Newton direction can be obtained without com- munication between machines.\n",
      "Second, we consider subsampled Gauss-Newton \f",
      "matrices for reducing the running time as well as the communication cost.\n",
      "Third, to reduce the synchronization cost, we terminate the process of ﬁnding an ap- proximate Newton direction even though some nodes have not ﬁnished their tasks.\n",
      "Details of some implementation issues in distributed environments are thoroughly investigated.\n",
      "Experiments demonstrate that the proposed method is effective for the distributed training of deep neural networks.\n",
      "In compared with stochastic gra- dient methods, it is more robust and may give better test accuracy.\n",
      "\n",
      " In this paper, we introduce ”Power Linear Unit” (PoLU) which increases the nonlinearity capacity of a neural net- work and thus helps improving its performance.\n",
      "PoLU adopts several advantages of previously proposed activa- tion functions.\n",
      "First, the output of PoLU for positive inputs is designed to be identity to avoid the gradient vanishing problem.\n",
      "Second, PoLU has a non-zero output for negative inputs such that the output mean of the units is close to zero, hence reducing the bias shift effect.\n",
      "Thirdly, there is a satu- ration on the negative part of PoLU, which makes it more noise-robust for negative inputs.\n",
      "Furthermore, we prove that PoLU is able to map more portions of every layer’s in- put to the same space by using the power function and thus increases the number of response regions of the neural net- work.\n",
      "We use image classiﬁcation for comparing our pro- posed activation function with others.\n",
      "In the experiments, MNIST, CIFAR-10, CIFAR-100, Street View House Numbers (SVHN) and ImageNet are used as benchmark datasets.\n",
      "The neural networks we implemented include widely-used ELU- Network, ResNet-50, and VGG16, plus a couple of shallow networks.\n",
      "Experimental results show that our proposed ac- tivation function outperforms other state-of-the-art models with most networks.\n",
      "\n",
      ".\n",
      "A decision tree T in Bm := {0, 1}m is a binary tree where each of its internal nodes is labeled with an integer in [m] = {1, 2, .\n",
      ".\n",
      ".\n",
      ", m}, each leaf is labeled with an assignment a ∈ Bm and each internal node has two outgoing edges that are labeled with 0 and 1, respectively.\n",
      "Let A ⊂ {0, 1}m.\n",
      "We say that T is a decision tree for A if (1) For every a ∈ A there is one leaf of T that is labeled with a.\n",
      "(2) For every path from the root to a leaf with internal nodes labeled with i1, i2, .\n",
      ".\n",
      ".\n",
      ", ik ∈ [m], a leaf labeled with a ∈ A and edges labeled with ξi1 , .\n",
      ".\n",
      ".\n",
      ", ξik ∈ {0, 1}, a is the only element in A that satisﬁes aij = ξij for all j = 1, .\n",
      ".\n",
      ".\n",
      ", k.\n",
      "Our goal is to write a polynomial time (in n := |A| and m) algorithm that for an input A ⊆ Bm outputs a decision tree for A of minimum depth.\n",
      "This problem has many applications that include, to name a few, computer vision, group testing, exact learning from membership queries and game theory.\n",
      "Arkin et al.\n",
      "and Moshkov [4,14] gave a polynomial time (ln|A|)- approx- imation algorithm (for the depth).\n",
      "The result of Dinur and Steurer [6] for set cover implies that this problem cannot be approximated with ratio (1− o(1))· ln|A|, unless P=NP.\n",
      "Moskov studied in [14] the combinatorial measure of extended teaching dimension of A, ETD(A).\n",
      "He showed that ETD(A) is a lower bound for the depth of the decision tree for A and then gave an exponential time ETD(A)/ log(ETD(A))-approximation al- gorithm.\n",
      "In this paper we further study the ETD(A) measure and a new com- binatorial measure, DEN(A), that we call the density of the set A.\n",
      "We show that DEN(A) ≤ ETD(A) + 1.\n",
      "We then give two results.\n",
      "The ﬁrst result is that the lower bound ETD(A) of Moshkov for the depth of the decision tree for A is greater than the bounds that are obtained by the classical technique used in the literature.\n",
      "The second result is a poly- nomial time (ln 2)DEN(A)-approximation (and therefore (ln 2)ETD(A)- approximation) algorithm for the depth of the decision tree of A.\n",
      "We also show that a better approximation ratio implies P=NP.\n",
      "We then apply the above results to learning the class of disjunctions of predicates from membership queries [5].\n",
      "We show that the ETD of this class is bounded from above by the degree d of its Hasse diagram.\n",
      "We then show that Moshkov algorithm can be run in polynomial time and is (d/ log d)-approximation algorithm.\n",
      "This gives optimal algorithms when the degree is constant.\n",
      "For example, learning axis parallel rays over constant dimension space.\n",
      "\f",
      " \n",
      ".\n",
      " Intrusion  detection for computer network systems has  been  becoming  one of the most critical tasks for network administrators today.\n",
      " It  has an important role for organizations, governments and our society  due to the valuable resources hosted on computer networks.\n",
      "Traditional  misuse detection strategies are unable to detect new and unknown intru-  sion  types.\n",
      " In  contrast,  anomaly detection in  network security  aims  to  distinguish  between illegal  or  malicious events and  normal behavior of  network systems.\n",
      "Anomaly detection can be considered as a classification  problem where  it  builds  models of normal network behavior, of which it  uses  to  detect  new patterns  that  significantly deviate  from the  model.\n",
      " Most  of the  current  approaches on  anomaly  detection is  based  on the  learning of normal behavior and anomalous actions.\n",
      "They  do not include  memory that is they do not take into account previous events classify new  ones.\n",
      "In  this  paper, we propose a one-class collective anomaly detection  model based  on  neural  network learning.\n",
      " Normally a  Long  Short-Term  Memory Recurrent Neural Network (LSTM  RNN) is trained only on nor-  mal  data,  and  it  is  capable of predicting several time-steps ahead of an  input.\n",
      " In  our approach, a  LSTM  RNN  is  trained on normal time series  data  before performing a  prediction for each time-step.\n",
      "Instead  of con-  sidering  each  time-step separately,  the  observation of prediction  errors  from a  certain number of time-steps is  now proposed as  a  new idea  for  detecting collective anomalies.\n",
      "The  prediction  errors of a  certain  num-  ber  of the latest time-steps above a  threshold will  indicate  a  collective  anomaly.\n",
      "The  model is  evaluated on a  time series version of the KDD  1999 dataset.\n",
      "The  experiments demonstrate that the proposed model is  capable to detect collective anomaly efficiently.\n",
      " \n",
      "—Non-recurring trafﬁc congestion is caused by tem- porary disruptions, such as accidents, sports games, adverse weather, etc.\n",
      "We use data related to real-time trafﬁc speed, jam factors (a trafﬁc congestion indicator), and events collected over a year from Nashville, TN to train a multi-layered deep neural network.\n",
      "The trafﬁc dataset contains over 900 million data records.\n",
      "The network is thereafter used to classify the real- time data and identify anomalous operations.\n",
      "Compared with traditional approaches of using statistical or machine learning techniques, our model reaches an accuracy of 98.73 percent when identifying trafﬁc congestion caused by football games.\n",
      "Our approach ﬁrst encodes the trafﬁc across a region as a scaled image.\n",
      "After that the image data from different timestamps is fused with event- and time-related data.\n",
      "Then a crossover operator is used as a data augmentation method to generate training datasets with more balanced classes.\n",
      "Finally, we use the receiver operating characteristic (ROC) analysis to tune the sensitivity of the classiﬁer.\n",
      "We present the analysis of the training time and the inference time separately.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(len_sent)\n",
    "\n",
    "# for src in \n",
    "\n",
    "# for src in src_sentences:\n",
    "    \n",
    "#     print(src)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(src_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_sentences_path=\"src_sentences.txt\"\n",
    "with open(src_sentences_path, 'w') as f:\n",
    "    for one_sent in src_sentences:\n",
    "        f.write(one_sent.strip() + os.linesep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_sentences_path=\"src_sentences.txt\"\n",
    "lines=LineSentence(src_sentences_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec2file_path=\"paper2vec.w2v\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.8 ms, sys: 4.61 ms, total: 21.4 ms\n",
      "Wall time: 20.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "paper2vec=Word2Vec(lines, size=100, window=5, min_count=5, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.1 ms, sys: 2.3 ms, total: 13.4 ms\n",
      "Wall time: 10.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1414"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "paper2vec.train(lines, total_examples=paper2vec.corpus_count, epochs=paper2vec.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.47 ms, sys: 1.4 ms, total: 2.87 ms\n",
      "Wall time: 3.47 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "paper2vec.save(vec2file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x1a14c9d9d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paper2vec.init_sims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-32-06ad176c7d0d>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-06ad176c7d0d>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    (/paper2vec.most_similar(positive=['horizon'],, topn=7))\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#paper2vec.most_similar(positive=['horizon'], topn=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_words = 10000\n",
    "word_embeddings = pd.DataFrame(paper2vec.wv.syn0norm[:num_words, :], index=paper2vec.wv.index2word[:num_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.180018</td>\n",
       "      <td>0.024955</td>\n",
       "      <td>-0.175808</td>\n",
       "      <td>-0.041288</td>\n",
       "      <td>0.034148</td>\n",
       "      <td>-0.104546</td>\n",
       "      <td>-0.074016</td>\n",
       "      <td>-0.054612</td>\n",
       "      <td>0.008149</td>\n",
       "      <td>0.136747</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196491</td>\n",
       "      <td>-0.089203</td>\n",
       "      <td>-0.150222</td>\n",
       "      <td>0.014647</td>\n",
       "      <td>-0.077477</td>\n",
       "      <td>0.053206</td>\n",
       "      <td>0.128186</td>\n",
       "      <td>0.210214</td>\n",
       "      <td>-0.091556</td>\n",
       "      <td>-0.047315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.179634</td>\n",
       "      <td>0.021618</td>\n",
       "      <td>-0.174601</td>\n",
       "      <td>-0.043487</td>\n",
       "      <td>0.031058</td>\n",
       "      <td>-0.104199</td>\n",
       "      <td>-0.064623</td>\n",
       "      <td>-0.049727</td>\n",
       "      <td>0.009654</td>\n",
       "      <td>0.136975</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.197109</td>\n",
       "      <td>-0.088264</td>\n",
       "      <td>-0.147492</td>\n",
       "      <td>0.021542</td>\n",
       "      <td>-0.083328</td>\n",
       "      <td>0.054570</td>\n",
       "      <td>0.133878</td>\n",
       "      <td>0.208645</td>\n",
       "      <td>-0.083176</td>\n",
       "      <td>-0.041643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.175609</td>\n",
       "      <td>0.024692</td>\n",
       "      <td>-0.173475</td>\n",
       "      <td>-0.039687</td>\n",
       "      <td>0.029153</td>\n",
       "      <td>-0.106686</td>\n",
       "      <td>-0.072349</td>\n",
       "      <td>-0.052387</td>\n",
       "      <td>0.012807</td>\n",
       "      <td>0.142512</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.195941</td>\n",
       "      <td>-0.083529</td>\n",
       "      <td>-0.145962</td>\n",
       "      <td>0.019129</td>\n",
       "      <td>-0.081930</td>\n",
       "      <td>0.050742</td>\n",
       "      <td>0.131874</td>\n",
       "      <td>0.210362</td>\n",
       "      <td>-0.089700</td>\n",
       "      <td>-0.048325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.176822</td>\n",
       "      <td>0.026531</td>\n",
       "      <td>-0.171857</td>\n",
       "      <td>-0.039579</td>\n",
       "      <td>0.031407</td>\n",
       "      <td>-0.106231</td>\n",
       "      <td>-0.074509</td>\n",
       "      <td>-0.047489</td>\n",
       "      <td>0.011113</td>\n",
       "      <td>0.139541</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.194121</td>\n",
       "      <td>-0.090075</td>\n",
       "      <td>-0.150133</td>\n",
       "      <td>0.014832</td>\n",
       "      <td>-0.079351</td>\n",
       "      <td>0.055607</td>\n",
       "      <td>0.131894</td>\n",
       "      <td>0.209654</td>\n",
       "      <td>-0.089881</td>\n",
       "      <td>-0.043408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.175560</td>\n",
       "      <td>0.031502</td>\n",
       "      <td>-0.178270</td>\n",
       "      <td>-0.046266</td>\n",
       "      <td>0.026421</td>\n",
       "      <td>-0.097513</td>\n",
       "      <td>-0.066691</td>\n",
       "      <td>-0.055177</td>\n",
       "      <td>0.009657</td>\n",
       "      <td>0.139551</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.199578</td>\n",
       "      <td>-0.086241</td>\n",
       "      <td>-0.144474</td>\n",
       "      <td>0.018253</td>\n",
       "      <td>-0.080001</td>\n",
       "      <td>0.051767</td>\n",
       "      <td>0.126629</td>\n",
       "      <td>0.211034</td>\n",
       "      <td>-0.088220</td>\n",
       "      <td>-0.045446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "the  0.180018  0.024955 -0.175808 -0.041288  0.034148 -0.104546 -0.074016   \n",
       "of   0.179634  0.021618 -0.174601 -0.043487  0.031058 -0.104199 -0.064623   \n",
       "a    0.175609  0.024692 -0.173475 -0.039687  0.029153 -0.106686 -0.072349   \n",
       "is   0.176822  0.026531 -0.171857 -0.039579  0.031407 -0.106231 -0.074509   \n",
       "to   0.175560  0.031502 -0.178270 -0.046266  0.026421 -0.097513 -0.066691   \n",
       "\n",
       "           7         8         9     ...           90        91        92  \\\n",
       "the -0.054612  0.008149  0.136747    ...    -0.196491 -0.089203 -0.150222   \n",
       "of  -0.049727  0.009654  0.136975    ...    -0.197109 -0.088264 -0.147492   \n",
       "a   -0.052387  0.012807  0.142512    ...    -0.195941 -0.083529 -0.145962   \n",
       "is  -0.047489  0.011113  0.139541    ...    -0.194121 -0.090075 -0.150133   \n",
       "to  -0.055177  0.009657  0.139551    ...    -0.199578 -0.086241 -0.144474   \n",
       "\n",
       "           93        94        95        96        97        98        99  \n",
       "the  0.014647 -0.077477  0.053206  0.128186  0.210214 -0.091556 -0.047315  \n",
       "of   0.021542 -0.083328  0.054570  0.133878  0.208645 -0.083176 -0.041643  \n",
       "a    0.019129 -0.081930  0.050742  0.131874  0.210362 -0.089700 -0.048325  \n",
       "is   0.014832 -0.079351  0.055607  0.131894  0.209654 -0.089881 -0.043408  \n",
       "to   0.018253 -0.080001  0.051767  0.126629  0.211034 -0.088220 -0.045446  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_PREMADE_TSNE=False\n",
    "tsne_filepath='tsne.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 782 ms, sys: 109 ms, total: 891 ms\n",
      "Wall time: 895 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not USE_PREMADE_TSNE:\n",
    "    tsne=TSNE(random_state=0)\n",
    "    tsne_points=tsne.fit_transform(word_embeddings.values)\n",
    "    with open(tsne_filepath, 'wb') as f:\n",
    "        pickle.dump(tsne_points, f)\n",
    "else:\n",
    "    with open(tsne_filepath, 'rb') as f:\n",
    "        tsne_points=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne_df=pd.DataFrame(tsne_points, index=word_embeddings.index, columns=['x_coord', 'y_coord'])\n",
    "tsne_df['word']=tsne_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_coord</th>\n",
       "      <th>y_coord</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>10.849365</td>\n",
       "      <td>15.018153</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>-25.819042</td>\n",
       "      <td>4.652394</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>4.380425</td>\n",
       "      <td>-0.105996</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>-3.731924</td>\n",
       "      <td>26.455095</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>15.701976</td>\n",
       "      <td>-12.356300</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>-17.899464</td>\n",
       "      <td>17.251976</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>-3.423039</td>\n",
       "      <td>12.941175</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>-30.364056</td>\n",
       "      <td>-23.101585</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>-51.148087</td>\n",
       "      <td>7.567214</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>-11.350349</td>\n",
       "      <td>1.634736</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>37.526531</td>\n",
       "      <td>7.624701</td>\n",
       "      <td>we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>We</th>\n",
       "      <td>20.353172</td>\n",
       "      <td>2.610003</td>\n",
       "      <td>We</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>26.559399</td>\n",
       "      <td>-27.973005</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>In</th>\n",
       "      <td>30.254128</td>\n",
       "      <td>-77.957573</td>\n",
       "      <td>In</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>53.627975</td>\n",
       "      <td>87.510574</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The</th>\n",
       "      <td>-19.236605</td>\n",
       "      <td>53.182766</td>\n",
       "      <td>The</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as</th>\n",
       "      <td>-72.826378</td>\n",
       "      <td>-58.227768</td>\n",
       "      <td>as</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>-1.925701</td>\n",
       "      <td>43.169994</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>-20.427063</td>\n",
       "      <td>-45.019062</td>\n",
       "      <td>this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>8.111273</td>\n",
       "      <td>-26.463268</td>\n",
       "      <td>data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by</th>\n",
       "      <td>22.547598</td>\n",
       "      <td>48.642567</td>\n",
       "      <td>by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>an</th>\n",
       "      <td>26.335648</td>\n",
       "      <td>20.648045</td>\n",
       "      <td>an</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>-20.656080</td>\n",
       "      <td>32.176144</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>our</th>\n",
       "      <td>-96.066841</td>\n",
       "      <td>39.970467</td>\n",
       "      <td>our</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>are</th>\n",
       "      <td>45.531219</td>\n",
       "      <td>60.262360</td>\n",
       "      <td>are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>-21.786581</td>\n",
       "      <td>78.477974</td>\n",
       "      <td>learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neural</th>\n",
       "      <td>45.623901</td>\n",
       "      <td>-49.485420</td>\n",
       "      <td>neural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>-39.186707</td>\n",
       "      <td>-7.042574</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>41.882015</td>\n",
       "      <td>34.798340</td>\n",
       "      <td>from</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>training</th>\n",
       "      <td>19.426435</td>\n",
       "      <td>-51.013531</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>network</th>\n",
       "      <td>-25.562037</td>\n",
       "      <td>-88.260498</td>\n",
       "      <td>network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>65.535080</td>\n",
       "      <td>-72.376625</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision</th>\n",
       "      <td>-21.010496</td>\n",
       "      <td>-11.383981</td>\n",
       "      <td>decision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>may</th>\n",
       "      <td>-83.984642</td>\n",
       "      <td>76.047623</td>\n",
       "      <td>may</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>then</th>\n",
       "      <td>-72.732079</td>\n",
       "      <td>15.301725</td>\n",
       "      <td>then</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>detection</th>\n",
       "      <td>75.123016</td>\n",
       "      <td>-39.334259</td>\n",
       "      <td>detection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>between</th>\n",
       "      <td>-3.935689</td>\n",
       "      <td>-65.777428</td>\n",
       "      <td>between</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing</th>\n",
       "      <td>-43.819550</td>\n",
       "      <td>99.038422</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>94.347946</td>\n",
       "      <td>34.666660</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has</th>\n",
       "      <td>25.258423</td>\n",
       "      <td>107.007744</td>\n",
       "      <td>has</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trafﬁc</th>\n",
       "      <td>-39.385635</td>\n",
       "      <td>-61.098969</td>\n",
       "      <td>trafﬁc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entries</th>\n",
       "      <td>52.599640</td>\n",
       "      <td>-26.830927</td>\n",
       "      <td>entries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>large</th>\n",
       "      <td>-87.133377</td>\n",
       "      <td>-8.858412</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use</th>\n",
       "      <td>-4.921171</td>\n",
       "      <td>101.982224</td>\n",
       "      <td>use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>memory</th>\n",
       "      <td>-71.062195</td>\n",
       "      <td>-29.918470</td>\n",
       "      <td>memory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>such</th>\n",
       "      <td>-110.158943</td>\n",
       "      <td>11.056382</td>\n",
       "      <td>such</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>-11.466496</td>\n",
       "      <td>-25.899935</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>propose</th>\n",
       "      <td>67.537071</td>\n",
       "      <td>45.439842</td>\n",
       "      <td>propose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>each</th>\n",
       "      <td>-1.151576</td>\n",
       "      <td>-42.051682</td>\n",
       "      <td>each</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>-44.718746</td>\n",
       "      <td>-37.025097</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>networks.</th>\n",
       "      <td>9.112943</td>\n",
       "      <td>-99.589462</td>\n",
       "      <td>networks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>also</th>\n",
       "      <td>-59.446594</td>\n",
       "      <td>-86.739502</td>\n",
       "      <td>also</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>For</th>\n",
       "      <td>56.604149</td>\n",
       "      <td>-4.289849</td>\n",
       "      <td>For</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quantization</th>\n",
       "      <td>-103.469353</td>\n",
       "      <td>-37.870159</td>\n",
       "      <td>quantization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>potentials</th>\n",
       "      <td>-67.123199</td>\n",
       "      <td>47.680538</td>\n",
       "      <td>potentials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lower</th>\n",
       "      <td>25.457790</td>\n",
       "      <td>75.806648</td>\n",
       "      <td>lower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>∈</th>\n",
       "      <td>-3.780224</td>\n",
       "      <td>-12.775221</td>\n",
       "      <td>∈</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deep</th>\n",
       "      <td>1.813133</td>\n",
       "      <td>66.074196</td>\n",
       "      <td>deep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>57.329929</td>\n",
       "      <td>20.008276</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>algorithm</th>\n",
       "      <td>-52.781303</td>\n",
       "      <td>28.466627</td>\n",
       "      <td>algorithm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 x_coord     y_coord          word\n",
       "the            10.849365   15.018153           the\n",
       "of            -25.819042    4.652394            of\n",
       "a               4.380425   -0.105996             a\n",
       "is             -3.731924   26.455095            is\n",
       "to             15.701976  -12.356300            to\n",
       "and           -17.899464   17.251976           and\n",
       "that           -3.423039   12.941175          that\n",
       "for           -30.364056  -23.101585           for\n",
       "in            -51.148087    7.567214            in\n",
       "with          -11.350349    1.634736          with\n",
       "we             37.526531    7.624701            we\n",
       "We             20.353172    2.610003            We\n",
       "on             26.559399  -27.973005            on\n",
       "In             30.254128  -77.957573            In\n",
       ".              53.627975   87.510574             .\n",
       "The           -19.236605   53.182766           The\n",
       "as            -72.826378  -58.227768            as\n",
       "time           -1.925701   43.169994          time\n",
       "this          -20.427063  -45.019062          this\n",
       "data            8.111273  -26.463268          data\n",
       "by             22.547598   48.642567            by\n",
       "an             26.335648   20.648045            an\n",
       "A             -20.656080   32.176144             A\n",
       "our           -96.066841   39.970467           our\n",
       "are            45.531219   60.262360           are\n",
       "learning      -21.786581   78.477974      learning\n",
       "neural         45.623901  -49.485420        neural\n",
       "be            -39.186707   -7.042574            be\n",
       "from           41.882015   34.798340          from\n",
       "training       19.426435  -51.013531      training\n",
       "...                  ...         ...           ...\n",
       "network       -25.562037  -88.260498       network\n",
       "new            65.535080  -72.376625           new\n",
       "decision      -21.010496  -11.383981      decision\n",
       "may           -83.984642   76.047623           may\n",
       "then          -72.732079   15.301725          then\n",
       "detection      75.123016  -39.334259     detection\n",
       "between        -3.935689  -65.777428       between\n",
       "missing       -43.819550   99.038422       missing\n",
       "it             94.347946   34.666660            it\n",
       "has            25.258423  107.007744           has\n",
       "trafﬁc        -39.385635  -61.098969        trafﬁc\n",
       "entries        52.599640  -26.830927       entries\n",
       "large         -87.133377   -8.858412         large\n",
       "use            -4.921171  101.982224           use\n",
       "memory        -71.062195  -29.918470        memory\n",
       "such         -110.158943   11.056382          such\n",
       ",             -11.466496  -25.899935             ,\n",
       "propose        67.537071   45.439842       propose\n",
       "each           -1.151576  -42.051682          each\n",
       "model         -44.718746  -37.025097         model\n",
       "networks.       9.112943  -99.589462     networks.\n",
       "also          -59.446594  -86.739502          also\n",
       "For            56.604149   -4.289849           For\n",
       "quantization -103.469353  -37.870159  quantization\n",
       "potentials    -67.123199   47.680538    potentials\n",
       "lower          25.457790   75.806648         lower\n",
       "∈              -3.780224  -12.775221             ∈\n",
       "deep            1.813133   66.074196          deep\n",
       "AL             57.329929   20.008276            AL\n",
       "algorithm     -52.781303   28.466627     algorithm\n",
       "\n",
       "[68 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsne_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"ecc3d316-e11d-4b45-9541-079fb0193772\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\") {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  var js_urls = ['https://cdn.pydata.org/bokeh/release/bokeh-0.12.0.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.0.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-compiler-0.12.0.min.js'];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      Bokeh.$(\"#ecc3d316-e11d-4b45-9541-079fb0193772\").text(\"BokehJS successfully loaded\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.0.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.0.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.0.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.0.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i](window.Bokeh);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <div class=\"plotdiv\" id=\"fc087622-5de7-413f-b3d0-4f5487ac19ca\"></div>\n",
       "    </div>\n",
       "<script type=\"text/javascript\">\n",
       "  \n",
       "  (function(global) {\n",
       "    function now() {\n",
       "      return new Date();\n",
       "    }\n",
       "  \n",
       "    if (typeof (window._bokeh_onload_callbacks) === \"undefined\") {\n",
       "      window._bokeh_onload_callbacks = [];\n",
       "    }\n",
       "  \n",
       "    function run_callbacks() {\n",
       "      window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "      delete window._bokeh_onload_callbacks\n",
       "      console.info(\"Bokeh: all callbacks have finished\");\n",
       "    }\n",
       "  \n",
       "    function load_libs(js_urls, callback) {\n",
       "      window._bokeh_onload_callbacks.push(callback);\n",
       "      if (window._bokeh_is_loading > 0) {\n",
       "        console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "        return null;\n",
       "      }\n",
       "      if (js_urls == null || js_urls.length === 0) {\n",
       "        run_callbacks();\n",
       "        return null;\n",
       "      }\n",
       "      console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "      window._bokeh_is_loading = js_urls.length;\n",
       "      for (var i = 0; i < js_urls.length; i++) {\n",
       "        var url = js_urls[i];\n",
       "        var s = document.createElement('script');\n",
       "        s.src = url;\n",
       "        s.async = false;\n",
       "        s.onreadystatechange = s.onload = function() {\n",
       "          window._bokeh_is_loading--;\n",
       "          if (window._bokeh_is_loading === 0) {\n",
       "            console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "            run_callbacks()\n",
       "          }\n",
       "        };\n",
       "        s.onerror = function() {\n",
       "          console.warn(\"failed to load library \" + url);\n",
       "        };\n",
       "        console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      }\n",
       "    };var element = document.getElementById(\"fc087622-5de7-413f-b3d0-4f5487ac19ca\");\n",
       "    if (element == null) {\n",
       "      console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'fc087622-5de7-413f-b3d0-4f5487ac19ca' but no matching script tag was found. \")\n",
       "      return false;\n",
       "    }\n",
       "  \n",
       "    var js_urls = [];\n",
       "  \n",
       "    var inline_js = [\n",
       "      function(Bokeh) {\n",
       "        Bokeh.$(function() {\n",
       "            var docs_json = {\"5ff3f346-8d20-44bd-bf3b-55c6a0daff44\":{\"roots\":{\"references\":[{\"attributes\":{\"plot\":{\"id\":\"430c5b08-908e-4431-bb69-39b0714c74cc\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"f4d99cf9-929f-4227-85c4-f0f846b2cbc2\",\"type\":\"HelpTool\"},{\"attributes\":{\"overlay\":{\"id\":\"760f57a1-2a36-41b7-83d1-d9a3a6214796\",\"type\":\"BoxAnnotation\"},\"plot\":{\"id\":\"430c5b08-908e-4431-bb69-39b0714c74cc\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"c09ded4f-8bca-4fd5-b565-31b8733e3783\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"plot\":null,\"text\":\"t-SNE Word Embeddings\",\"text_font_size\":{\"value\":\"16pt\"}},\"id\":\"8001958c-8267-4c44-b620-c79e011cea97\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"41f1880b-c326-4fd9-bdae-bc93083aa4b8\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"data_source\":{\"id\":\"48aa1c47-fda0-405d-80ee-c283c37e1da2\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"0aeb308d-b6e8-4c2b-ac46-1ddf25e00baa\",\"type\":\"Circle\"},\"hover_glyph\":{\"id\":\"20922894-8fa8-4e0b-a00f-e09ec5032a2c\",\"type\":\"Circle\"},\"nonselection_glyph\":{\"id\":\"3f61c25a-2573-48d0-a628-f6d4a37b41a3\",\"type\":\"Circle\"},\"selection_glyph\":null},\"id\":\"591be22d-4b12-4a5a-8d4e-a3f86e79233c\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"orange\"},\"size\":{\"units\":\"screen\",\"value\":10},\"x\":{\"field\":\"x_coord\"},\"y\":{\"field\":\"y_coord\"}},\"id\":\"20922894-8fa8-4e0b-a00f-e09ec5032a2c\",\"type\":\"Circle\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"red\"},\"line_alpha\":{\"value\":0.2},\"line_color\":{\"value\":\"red\"},\"size\":{\"units\":\"screen\",\"value\":10},\"x\":{\"field\":\"x_coord\"},\"y\":{\"field\":\"y_coord\"}},\"id\":\"0aeb308d-b6e8-4c2b-ac46-1ddf25e00baa\",\"type\":\"Circle\"},{\"attributes\":{\"formatter\":{\"id\":\"41f1880b-c326-4fd9-bdae-bc93083aa4b8\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"430c5b08-908e-4431-bb69-39b0714c74cc\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"bd76bf7a-7f48-46a2-90c8-92f9b73fca50\",\"type\":\"BasicTicker\"},\"visible\":false},\"id\":\"65941c42-b7d1-4442-aa68-a3b7947d67f5\",\"type\":\"LinearAxis\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"760f57a1-2a36-41b7-83d1-d9a3a6214796\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"size\":{\"units\":\"screen\",\"value\":10},\"x\":{\"field\":\"x_coord\"},\"y\":{\"field\":\"y_coord\"}},\"id\":\"3f61c25a-2573-48d0-a628-f6d4a37b41a3\",\"type\":\"Circle\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"y_coord\",\"index\",\"word\",\"x_coord\"],\"data\":{\"index\":[\"the\",\"of\",\"a\",\"is\",\"to\",\"and\",\"that\",\"for\",\"in\",\"with\",\"we\",\"We\",\"on\",\"In\",\".\",\"The\",\"as\",\"time\",\"this\",\"data\",\"by\",\"an\",\"A\",\"our\",\"are\",\"learning\",\"neural\",\"be\",\"from\",\"training\",\"show\",\"or\",\"labeled\",\"method\",\"can\",\"tree\",\"which\",\"proposed\",\"network\",\"new\",\"decision\",\"may\",\"then\",\"detection\",\"between\",\"missing\",\"it\",\"has\",\"traf\\ufb01c\",\"entries\",\"large\",\"use\",\"memory\",\"such\",\",\",\"propose\",\"each\",\"model\",\"networks.\",\"also\",\"For\",\"quantization\",\"potentials\",\"lower\",\"\\u2208\",\"deep\",\"AL\",\"algorithm\"],\"word\":[\"the\",\"of\",\"a\",\"is\",\"to\",\"and\",\"that\",\"for\",\"in\",\"with\",\"we\",\"We\",\"on\",\"In\",\".\",\"The\",\"as\",\"time\",\"this\",\"data\",\"by\",\"an\",\"A\",\"our\",\"are\",\"learning\",\"neural\",\"be\",\"from\",\"training\",\"show\",\"or\",\"labeled\",\"method\",\"can\",\"tree\",\"which\",\"proposed\",\"network\",\"new\",\"decision\",\"may\",\"then\",\"detection\",\"between\",\"missing\",\"it\",\"has\",\"traf\\ufb01c\",\"entries\",\"large\",\"use\",\"memory\",\"such\",\",\",\"propose\",\"each\",\"model\",\"networks.\",\"also\",\"For\",\"quantization\",\"potentials\",\"lower\",\"\\u2208\",\"deep\",\"AL\",\"algorithm\"],\"x_coord\":[10.849365234375,-25.819042205810547,4.380425453186035,-3.7319235801696777,15.70197582244873,-17.899463653564453,-3.423038959503174,-30.364055633544922,-51.14808654785156,-11.350349426269531,37.52653121948242,20.353172302246094,26.559398651123047,30.254127502441406,53.62797546386719,-19.236604690551758,-72.82637786865234,-1.9257007837295532,-20.42706298828125,8.111272811889648,22.547597885131836,26.335647583007812,-20.65608024597168,-96.06684112548828,45.531219482421875,-21.78658103942871,45.6239013671875,-39.18670654296875,41.882015228271484,19.426435470581055,100.80094909667969,-38.2813606262207,-33.850067138671875,81.4390869140625,-54.887054443359375,13.473445892333984,35.75285339355469,-49.00901412963867,-25.562036514282227,65.53507995605469,-21.010496139526367,-83.9846420288086,-72.7320785522461,75.12301635742188,-3.9356892108917236,-43.819549560546875,94.34794616699219,25.2584228515625,-39.38563537597656,52.599639892578125,-87.13337707519531,-4.921171188354492,-71.06219482421875,-110.15894317626953,-11.466495513916016,67.53707122802734,-1.1515755653381348,-44.718746185302734,9.112942695617676,-59.44659423828125,56.604148864746094,-103.46935272216797,-67.12319946289062,25.45779037475586,-3.7802236080169678,1.8131330013275146,57.32992935180664,-52.78130340576172],\"y_coord\":[15.018153190612793,4.6523942947387695,-0.10599611699581146,26.455095291137695,-12.356300354003906,17.251976013183594,12.941174507141113,-23.101585388183594,7.567214012145996,1.63473641872406,7.624701499938965,2.6100034713745117,-27.973005294799805,-77.95757293701172,87.51057434082031,53.18276596069336,-58.22776794433594,43.16999435424805,-45.01906204223633,-26.463268280029297,48.6425666809082,20.64804458618164,32.176143646240234,39.97046661376953,60.262359619140625,78.47797393798828,-49.48542022705078,-7.042573928833008,34.79833984375,-51.01353073120117,-17.61310577392578,45.4421272277832,19.533069610595703,6.410828590393066,-15.454357147216797,32.24382400512695,-11.442055702209473,69.29147338867188,-88.260498046875,-72.37662506103516,-11.383980751037598,76.04762268066406,15.301725387573242,-39.334259033203125,-65.77742767333984,99.03842163085938,34.66666030883789,107.00774383544922,-61.098968505859375,-26.8309268951416,-8.85841178894043,101.98222351074219,-29.91847038269043,11.056382179260254,-25.899934768676758,45.439842224121094,-42.05168151855469,-37.02509689331055,-99.58946228027344,-86.739501953125,-4.289849281311035,-37.87015914916992,47.680538177490234,75.80664825439453,-12.77522087097168,66.0741958618164,20.008275985717773,28.46662712097168]}},\"id\":\"48aa1c47-fda0-405d-80ee-c283c37e1da2\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"plot\":{\"id\":\"430c5b08-908e-4431-bb69-39b0714c74cc\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"dc8b0068-064b-46cc-b1de-d45a1b02f1cc\",\"type\":\"PanTool\"},{\"attributes\":{\"formatter\":{\"id\":\"866c7f8b-3481-4d6a-bce7-9bf512062e37\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"430c5b08-908e-4431-bb69-39b0714c74cc\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"6e00868e-300d-4394-b527-1a7bf5a8a1e0\",\"type\":\"BasicTicker\"},\"visible\":false},\"id\":\"b4180781-2db8-4ee6-b66a-441323644a14\",\"type\":\"LinearAxis\"},{\"attributes\":{\"plot\":{\"id\":\"430c5b08-908e-4431-bb69-39b0714c74cc\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"b59633ef-f1ec-4bcf-ae93-13e160e3cbbd\",\"type\":\"ResetTool\"},{\"attributes\":{\"below\":[{\"id\":\"b4180781-2db8-4ee6-b66a-441323644a14\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"65941c42-b7d1-4442-aa68-a3b7947d67f5\",\"type\":\"LinearAxis\"}],\"outline_line_color\":{\"value\":null},\"plot_height\":800,\"plot_width\":800,\"renderers\":[{\"id\":\"b4180781-2db8-4ee6-b66a-441323644a14\",\"type\":\"LinearAxis\"},{\"id\":\"3d32876f-a702-4214-b3fb-3a8a4e25b5ca\",\"type\":\"Grid\"},{\"id\":\"65941c42-b7d1-4442-aa68-a3b7947d67f5\",\"type\":\"LinearAxis\"},{\"id\":\"465a45aa-0e30-434d-b3b6-614a1456ba56\",\"type\":\"Grid\"},{\"id\":\"760f57a1-2a36-41b7-83d1-d9a3a6214796\",\"type\":\"BoxAnnotation\"},{\"id\":\"591be22d-4b12-4a5a-8d4e-a3f86e79233c\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"8001958c-8267-4c44-b620-c79e011cea97\",\"type\":\"Title\"},\"tool_events\":{\"id\":\"71d5732e-e92d-4513-a177-ff750c33884d\",\"type\":\"ToolEvents\"},\"toolbar\":{\"id\":\"10052553-ff49-44a0-9fcd-dd13f83b2078\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"3b651dc7-a7a1-4e3e-b948-ad7a2c60d977\",\"type\":\"DataRange1d\"},\"y_range\":{\"id\":\"cb8d7420-0790-4ef0-8c29-957ba37d188b\",\"type\":\"DataRange1d\"}},\"id\":\"430c5b08-908e-4431-bb69-39b0714c74cc\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"plot\":{\"id\":\"430c5b08-908e-4431-bb69-39b0714c74cc\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"ed141cbd-6b6f-4e8f-b272-b7071b73c2f7\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"bd76bf7a-7f48-46a2-90c8-92f9b73fca50\",\"type\":\"BasicTicker\"},{\"attributes\":{\"grid_line_color\":{\"value\":null},\"plot\":{\"id\":\"430c5b08-908e-4431-bb69-39b0714c74cc\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"6e00868e-300d-4394-b527-1a7bf5a8a1e0\",\"type\":\"BasicTicker\"}},\"id\":\"3d32876f-a702-4214-b3fb-3a8a4e25b5ca\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"866c7f8b-3481-4d6a-bce7-9bf512062e37\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_scroll\":{\"id\":\"ed141cbd-6b6f-4e8f-b272-b7071b73c2f7\",\"type\":\"WheelZoomTool\"},\"active_tap\":\"auto\",\"tools\":[{\"id\":\"dc8b0068-064b-46cc-b1de-d45a1b02f1cc\",\"type\":\"PanTool\"},{\"id\":\"ed141cbd-6b6f-4e8f-b272-b7071b73c2f7\",\"type\":\"WheelZoomTool\"},{\"id\":\"c09ded4f-8bca-4fd5-b565-31b8733e3783\",\"type\":\"BoxZoomTool\"},{\"id\":\"eb5ee856-a824-4e4d-82d9-aab8ab508934\",\"type\":\"SaveTool\"},{\"id\":\"b59633ef-f1ec-4bcf-ae93-13e160e3cbbd\",\"type\":\"ResetTool\"},{\"id\":\"f4d99cf9-929f-4227-85c4-f0f846b2cbc2\",\"type\":\"HelpTool\"},{\"id\":\"c9b9db79-9a31-4e7c-99d1-08bcf4ff5dea\",\"type\":\"HoverTool\"}]},\"id\":\"10052553-ff49-44a0-9fcd-dd13f83b2078\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"71d5732e-e92d-4513-a177-ff750c33884d\",\"type\":\"ToolEvents\"},{\"attributes\":{},\"id\":\"6e00868e-300d-4394-b527-1a7bf5a8a1e0\",\"type\":\"BasicTicker\"},{\"attributes\":{\"plot\":{\"id\":\"430c5b08-908e-4431-bb69-39b0714c74cc\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"eb5ee856-a824-4e4d-82d9-aab8ab508934\",\"type\":\"SaveTool\"},{\"attributes\":{\"callback\":null,\"plot\":{\"id\":\"430c5b08-908e-4431-bb69-39b0714c74cc\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"tooltips\":\"@word\"},\"id\":\"c9b9db79-9a31-4e7c-99d1-08bcf4ff5dea\",\"type\":\"HoverTool\"},{\"attributes\":{\"callback\":null},\"id\":\"3b651dc7-a7a1-4e3e-b948-ad7a2c60d977\",\"type\":\"DataRange1d\"},{\"attributes\":{\"callback\":null},\"id\":\"cb8d7420-0790-4ef0-8c29-957ba37d188b\",\"type\":\"DataRange1d\"},{\"attributes\":{\"dimension\":1,\"grid_line_color\":{\"value\":null},\"plot\":{\"id\":\"430c5b08-908e-4431-bb69-39b0714c74cc\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"bd76bf7a-7f48-46a2-90c8-92f9b73fca50\",\"type\":\"BasicTicker\"}},\"id\":\"465a45aa-0e30-434d-b3b6-614a1456ba56\",\"type\":\"Grid\"}],\"root_ids\":[\"430c5b08-908e-4431-bb69-39b0714c74cc\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.0\"}};\n",
       "            var render_items = [{\"docid\":\"5ff3f346-8d20-44bd-bf3b-55c6a0daff44\",\"elementid\":\"fc087622-5de7-413f-b3d0-4f5487ac19ca\",\"modelid\":\"430c5b08-908e-4431-bb69-39b0714c74cc\",\"notebook_comms_target\":\"bae339fc-e260-4a06-8313-5c26533545c0\"}];\n",
       "            \n",
       "            Bokeh.embed.embed_items(docs_json, render_items);\n",
       "        });\n",
       "      },\n",
       "      function(Bokeh) {\n",
       "      }\n",
       "    ];\n",
       "  \n",
       "    function run_inline_js() {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }\n",
       "    }\n",
       "  \n",
       "    if (window._bokeh_is_loading === 0) {\n",
       "      console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "      run_inline_js();\n",
       "    } else {\n",
       "      load_libs(js_urls, function() {\n",
       "        console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "        run_inline_js();\n",
       "      });\n",
       "    }\n",
       "  }(this));\n",
       "</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare the data in a form suitable for bokeh.\n",
    "plot_data=ColumnDataSource(tsne_df)\n",
    "\n",
    "# create the plot and configure it\n",
    "tsne_plot = figure(title='t-SNE Word Embeddings',\n",
    "                    plot_width = 800,\n",
    "                    plot_height = 800,\n",
    "                    active_scroll='wheel_zoom')\n",
    "\n",
    "# add a hover tool to display words on roll-over\n",
    "tsne_plot.add_tools(HoverTool(tooltips='@word'))\n",
    "\n",
    "tsne_plot.circle('x_coord', 'y_coord', source=plot_data,\n",
    "                color='red', line_alpha=0.2, fill_alpha=0.1,\n",
    "                size=10, hover_line_color='orange')\n",
    "\n",
    "# adjust visual elements of the plot\n",
    "tsne_plot.title.text_font_size=value('16pt')\n",
    "tsne_plot.xaxis.visible=False\n",
    "tsne_plot.yaxis.visible=False\n",
    "tsne_plot.grid.grid_line_color=None\n",
    "tsne_plot.outline_line_color=None\n",
    "\n",
    "# show time!\n",
    "show(tsne_plot);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
