Network anomaly detection refers to the problem  of detecting  illegal or malicious  activities or events from  normal connections  or expected behavior of network    systems [3, 4].
It  has become  one of the most popular subjects in the network  security domain due to the fact that many organizations  and governments  are  now seeking  good solutions  to protect valuable resources on computer networks  from unauthorized  and illegal accesses, network attacks or malware.
Over the last  three decades, machine learning techniques are known as a common approach  for  developing  network  anomaly  detection  models  [2, 3].
Network anomaly detection  is usually posed as a type of classification  problem: given a dataset representing  normal and anomalous examples, the goal is to build a learning classifier which  is capable of signaling when a new anomalous  data sample is encountered   [4].
Most  of the existing approaches consider an anomaly as a discrete single data  point: cases when  they occur “individually”  and “separately”  [5, 6, 15].
In  such  approaches,  anomaly detection  models do not have the ability to represent  the  information  from previous data points or events  for evaluating  a current point.
In  network  security domain however,  some  kinds of attacks (e.g. Denial of Ser-  vice - DoS ) usually occur for a long  period  of time (several   minutes)   [9], and are  often represented  by  a  sequence  of single data  points.
Thus  these attacks will  be indicated only if a  sequence  of single data points are considered  as attacks.
In  this context, network  data can be considered  as a time series data that are  sequences  of events obtained over repeated measurements  of time.
Many ap-  proaches  ranging from statistical techniques  to machine  learning techniques  are  employed  for analyzing time series data, and their efficiency has been proven  in  time series forecasting  problems.
These  approaches are based on the informa-  tion of previous events to forecast  the incoming step.
In  order to detect these  kinds of attack mentioned above, anomaly  detection models should be capable of  memorizing the information  from a number  of previous  events, and representing  the relationship between  them and with the current event.
The  model should  not only have the ability to estimate prediction  errors anomalous scores for each  individual  time-step but  also be  able to  observe sequences  of time-steps that  are potential to be collective anomaly.
To  avoid important mistakes, one must  always consider  every outcome: in this sense a highly anomalous value may still  be linked to a perfectly  normal condition, and conversely.
In  this work, we aim  to build an anomaly detection model for this kind of attacks (known as collective  anomaly  detection in [4]).
Collective anomaly is the term to refer to a collection of related anomalous  data  instances with respect to the whole dataset [4].
A  single data  point in  a  collective  anomaly  may not be considered as anomalies  by itself, but the occur-  rence of a sequence  of single points together  may indicate a collective  anomaly.
Hidden Markov model, Probabilistic Suffix Trees, etc.
are popular techniques for  collective anomaly detection [4].
Recently, Long Short-Term Memory Recurrent  Neural  Network  [7] has been recognized  as a powerful technique  to represent the  relationship  between a current event and previous events, and handles time se-  ries problems [11, 13].
However, these approaches  are proposed only for predicting  time series anomalies at individual level (predicting prediction error anomalous  score for each time-step), not at the collective  level (observing prediction  errors  for a sequence  of time-steps).
Moreover,  both normal and anomalous  data are    employed  for training stage: either for training process (constructing  classified  models) or  validation process (estimating model parameters).
Thus,  such the  models  are limited to detect new  kinds of network  attack.
Collecting and label-  ing anomalous data are also expensive and time-consuming tasks.
Therefore, we  will propose a collective anomaly  detection model by using the predictive power  of LSTM  RNN.
The  ability to detect collective  anomaly  of the proposed model  will be demonstrated on DoS attack group in the KDD  Cup  1999 dataset.
The  rest of the paper is  organized  as follows.
We briefly review some work  related to anomaly detection and  LSTM   RNN  in  Section 2.
In  Section 3,  we  give a short introduction to LSTM  RNN.
This  is followed by a section proposing  the collective anomaly  detection model using LSTM  RNN.
Experiments, Results  and Discussion are presented  in Section 5 and Section 6 respectively.
The  paper  concludes with highlights and future directions  in Section 7.
2     Related  Work  When considering a time series dataset, point anomalies are often directly linked  to the value of the considered  sample.
However, attempting  real time collective  anomaly detection implies always being aware of previous samples, and  more  precisely their  behavior.
This  means that  every time-step should include  an  evaluation of the current value combined with the evaluation of preceding infor-  mation.
In this section, we briefly describe previous work applying LSTM  RNN  for time series and collective anomaly  detection problems  [11, 13, 14, 16].
Olsson et al.
[14] proposed  an unsupervised approach for detecting  collective  anomalies.
In order to detect a group  of the anomalous examples, the anomalous  score of the group  of data points was probabilistically  aggregated from the contri-  bution  of each individual example.
Obtaining the collective  anomalous score was  carried out in an unsupervised  manner, thus it is suitable for both unsupervised  and supervised  approaches to scoring individual anomalies.
The model was eval-  uated on an artificial dataset and two industrial datasets, detecting  anomalies  in moving cranes and anomalies in fuel consumption.
In  [11],  Malhotra et  al.
applied a  LSTM   network for addressing anomaly  detection  problem in  time series fashion.
A  stacked LSTM   network  trained on  only normal data was used to predict values of a number of time-steps (L  steps)  ahead.
Thus,  the prediction  model produced L  prediction values in the period of  L  time-steps for a single data point.
It then resulted in a prediction error vector  with L  elements  for each data point.
The  prediction error of a single point was  then computed by modeling its prediction  error vector to fit a multivariate Gaus-  sian distribution, which was used to assess the likelihood  of anomaly behavior.
Their  model was demonstrated to perform well on four datasets.
Marchi et al.
[13, 12]  presented a  novel approach by  combining non-linear  predictive denoising autoencoders  (DA)  with LSTM   for identifying abnormal  acoustic signals.
Firstly,  LSTM  Recurrent DA  was employed to predict auditory  spectral features of the next  short-term frame from its  previous frames.
The  network  trained on normal acoustic recorders tends to behave well on normal    data,  and yields small reconstruction  errors whereas  the reconstruction  errors  from abnormal acoustic signals are high.
The  reconstruction  errors of the au-  toencoder  was used as an “anomaly score”, and a reconstruction  error above a  predetermined threshold indicates a novel acoustic  event.
The model was trained  on a public dataset  containing in-home sound events, and evaluated on a dataset  including new anomaly events.
The  results demonstrated  that their model per-  formed significantly better than  existing methods.
The  idea  is  also used in  a  practical acoustic example [13, 12], where LSTM  RNNs are used to predict short-  term frames.
In  [16]  Ralf  C  et at.
employed  LSTM-RNN   for intrusion detection  problem  in  supervised manner.
The  processed version of the KDD   Cup  1999 dataset,  which is  represented  in  time-series,  were fed to the LSTM-RNN.
The  network  has five outputs  representing  the four groups  of attacks and normal connections  in the data.
Both labeled normal connections and labeled attacks were  used for  training the model and estimating the best LSTM-RNN  architecture and its pa-  rameters.
The selected model was then evaluated  on 10% of the corrected  dataset  under measurements  of confusion  matrix and accuracy.
The  results shown  that  their model performed well in terms of accuracy, especially it achieved very high  performance  on two groups of attacks, Probe and DoS.
To  the best of our knowledge,  non of previous work using LSTM   RNN  ad-  dresses the problem of collective anomaly detection.
Thus,  we aim to develop  collective anomaly detection using LSTM   RNN.
Our  solution consists of two  stages:  (1)  LSTM  RNN will be employed  to represent  the relationship  between  previous time-steps and current  one in order to estimate anomalous score (known  as prediction error) for each time-step.
This  stage is  considered  as developing  a  time series anomaly detection, and  similar to the previous work [11]; (2) A  method will be proposed for observing  sequences of single data points based on  their anomalous scores to detect collective anomaly.
The second stage makes our  work original and different from previous  work that applied LSRM  RNN for time  series anomaly  detection.
This  will prove very efficient  in our example: First,  we  will train an LSTM  RNN on only normal data in order to learn the normal be-  havior.
The  trained model will be validated on normal validation set in order to  estimate the model parameters.
The result classifier will then employ to rate the  anomalous score for data at each time-step.
The anomalous score of a sequence  of time steps will be aggregated from the contribution  of each individual one.
By  imposing  a predetermine  threshold  a sequence  of single time-steps  will indicate  as a collective anomaly if its anomalous  score is higher than the threshold.
More  details on our approach can be found in Section 4.
3     Preliminaries  In  this section we briefly describe the structure of Long Short Term  Memory  nodes, and  the architecture of a  LSTM   RNN  using LSTM  hidden layer.
The  LSTM  was proposed by Hochreiter  et al.
[7]  in 1997, and has already proven to  be a powerful technique  for addressing the problem of time series prediction.
The  difference  initiated by  LSTM   regarding other types of RNN  resides in  its “smart”  nodes presented  in Hidden layer block in Fig.
1.
Each of these cells  contains three gates, input gate, forget  gate and output gate, which decide how  to react to an input.
Depending on the strength of the information  each node  receives,  it will decide to block it or pass it on.
The  information is also filtered  with the set of weights associated  with the cells when  it  is transferred  through  these cells.
Fig.
1.
LSTM RNN  Architecture  The  LSTM   node structure enables a  phenomenon  called  backpropagation  through time.
By  calculating for each hidden layer  the  partial  derivatives of  the output, weight  and input values, the system can move backwards to trace  the evolving error between  real output and predicted output.
Afterwards, the    network  uses the derivative of this evolution  to adapt its  weights  and decrease  prediction error.
This  learning method is named Gradient descent.
A  simple LSTM   RNN  as in Fig  1 consists three layers: Input  layer, LSTM  hidden layer  and  output layer.
The  input  and  output layers are  the same as  those in multi-layered  perceptrons (MLPs).
The input nodes are the input data,  and the output nodes can be sigmoid, tanh or other transform  functions.
The  LSTM  hidden layer is formed from a number  of the “smart”  nodes that are fully  connected  to the input and  output nodes.
Two  common  techniques,  Gradient  descent and Back-propagation can be used for optimizing  its  loss function  and  updating its parameters.
As mentioned before, Long Short-Term Memory has the power to incorporate  a behaviour into a network by training it with normal data.
The system becomes  representative of the variations of the data.
In other words,  a prediction is made  focusing on two features: the value  of a sample and its position at a specific time.
This  means that two same  input value at different times probably results in two  different  outputs.
It  is  because a  LSTM  RNN  is  stateful, i.e. has a “memory”,  which changes in response  to inputs.
4     Proposed  Approach  As  mentioned  in the Related work section, one of the most recent research us-  ing LSTM   RNNs  for building anomaly detection model in  time series data  is  from [11].
The  model was demonstrated  to be efficient  for detect anomalies  of  time series data.
In  their model, the prediction errors of a  data  point is  com-  puted by fitting its prediction  errors vector  to a multivariate Gaussian distribu-  tion which is then used to assess the likelihood of anomalous  behavior.
Hence,  it is only suitable for detecting  abnormal events  that happen instantly i.e. in a  very short time such as in the electrocardiogram  or power  demand applications  since the model do not have the ability  of estimating  likelihood anomaly for a  long period of time (a sequence  of data points).
Consequently,  the model is not  suitable for the collective  anomaly detection  in the context of network  security  where some kinds of network  attack last for a significant period time.
Therefore, in this paper we propose  a  new approach of using LSTM   RNNs  for cyber-security  attacks at collective level.
We will use a  simple LSTM  RNN  architecture, in contrast to a stacked LSTM  in [11].
This does not change the core  principle of the method:  when given  sufficient training, a LSTM  RNN adapts its  weights,  which become  characteristic of the training data.
In the network,  each  output nodes represents  each time-step that the model predicts.
For  example,  if the model is  fed a  data point at time-step t-th for predicting values at  next  three steps ahead, the first, second and third  output nodes will represent  the  values at  time-steps (t +  1)-th,  (t +  2)-th and  (t +  3)-th  respectively.
Instead  of fitting a mixture Gaussian model, we simple compute  the prediction error of  a individual data point by mean over the prediction errors at three time-steps.
We show the LSTM  RNNs  ability  to learn the behavior of a  training set, and  in this stage it acts like a time series anomaly detection  model.
Following this    stage, we proposed  terms (defined  below)  to monitor  the prediction errors of a  certain number of successive  data points and locate collective  anomaly  in data.
The second stage allows our model to detect collective anomaly, and make model  original and different  from  previous ones.
Thus,  the performance  of our model  is  not compared to that of previous models using LSTM   RNN  on time series  anomaly  detection, e.g. [11].
Terms for measuring prediction  errors of data point, and monitoring anoma-  lous behavior in a period of time-steps are defined as below:  –    Relative   Error   (RE):   the relative prediction  error between  a real value x  and its prediction value xˆ from LSTM  RNN at every time-step is defined as  in eq.1. Note that a single data can not be considered as a collective anomaly.
However, the larger value of RE  a data point has, the higher probability the  single data point belongs to a collective anomaly.
RE  (x, xˆ) = |x −	xˆ|		 (1)  –   Prediction   Error    Threshold  (PET):   It  is  employed to  determiner an  individual query point (a time-step)  can be classified as a normal time-step  or  considered as  an  element in  a  potential collective anomaly.
Its  predic-  tion error RE  above threshold PET  may indicate a  element  in  a  collective  anomaly.
–   Collective  Range   (CR):  a minimum  number  of anomalies  appearing  suc-  cessively in a network  flow are considered as a collective anomaly.
Both P ET  and C R  are estimated based on the best classification  performance  of the  model on normal validation set.
5     Experiments  5.1      Datasets  In  order to  demonstrate  the  efficient  performance   of the  proposed model, we  choose a dataset related to the network security domain, the KDD  1999 dataset [1,  8], for our experiments.
The dataset in tcpdump format was collected  from a sim-  ulated military-like environment  over a period of 5 weeks (from  1st  March 1999  to 9th  April  1999).
The dataset is composed of a two-weeks for training, weeks 1  and week  3  (free attack), one week for validation, week  2 (labeled attacks), and  other two weeks for testing, weeks 4 and 5 (both normal and anomalous data).
There  are four main groups of attacks in  the dataset, but  we restrict our  experiments  on a  specific attack, Neptune,  in  the Denial-of-Service group.
The  dataset is  also  converted into a  time  series version before feeding into  these  models.
More details about how to obtain a time series version from the original  tcpdump  dataset, and  how to  choose training, validation and  testing sets are  presented  in the following paragraphs.
The  first crucial  step is  to build  a  conveniently  usable time series dataset  out of the tcpdump data, and to select the interested features.
We use terminal    commands  and a python program to convert the original tcpdump data in the  KDD  1999 dataset into a time dependent function.
This  method is a development  of the  proposed transformation  in  [10] that  acts  directly  on  the  tcpdump to  obtain real  time statistics  of the  data.
Our  scheme follows this  step  by  step  transition as described below:  tcpdump ⇒	pcap ⇒	csv  Each  day  of records can  be  time-filtered  and  input  into a  new .pcap file.
This  also has the advantage of giving a  first approach on visualizing the data  by  using Wireshark functionalities.
Once this  is  done, the tshark command is  adapted to select and transfer  the relevant information  from the records into a  csv format.
We may note that doing this is a first step towards faster computation  and better system efficiency,  since all  irrelevant pcap  columns can be ignored.
Although the method  for converting  data do not suit for the model performing  real-time, it is sufficient  enough  for evaluating  the proposed  model in detecting  collective anomaly as  the main objective of this work.
However,  if  the attack  data is recorded  in real-time under time series format, then our method  can be  applied in real-time  detection.
In our experiments, we use 6-days normal traffic in the first and third weeks  for training, ntrain , and one-day  normal traffic (Thursday) in week 3 for valida-  tion, nvalid .
Testing sets include 1-day normal  traffic from week 3 (Friday), ntest ,  and 1-day  data containing  attacks in week 2  (Wednesday),  atest .
The  protocol  will be the following: training the network with ntrain , using nvalid  for choosing  Prediction  Error  Threshold (PET)  and Collective Range (CR),  and evaluating  the proposed  models on ntest   and atest .
Table  1.
Parameter Settings  LSTM  RNN Parameters  Input  Size  Hidden LSTM Layer   Output  Sigmoid Layer   Learning Rate   Number of Epochs  Momentum  Batch  Size  Collective  Thresholds  Prediction  Error  Threshold  (PET)  Collective  Range (CR)   1  10  1,  2 or 3  10−4  100  0.5  1  0.3  4  5.2   Experimental  Settings  Our experiments are aim to demonstrate the ability of detecting collective  anoma-  lies of the proposed model.
However,  there is no anomalous instances available    for training and validation stages, it  is much harder than binary classification  problem in  estimating  hyper-parameters,  optimizing  network architectures  and  setting thresholds.
Thus,  we will briefly discus this issues in next paragraph, and  design two experiments, a preliminary experiment  for choosing  these thresholds  and a main experiment for evaluating the proposed model.
We investigate  three network architectures.
The  difference amongst  these ar-  chitectures is  only the size of output layer,  with one, two and  three outputs  for predicting 1-step, 2-step and 3-step ahead respectively.
This  means that the  three-output  network can predict three values for three steps ahead with a cur-  rent input.
The number  of hidden nodes and the learning rate can strongly influ-  ence the performance  of a LSTM  RNN.
Each synapse of a network  is weighted  differently,  and can be considered  as a unique interpretation  of the input data.
Each  node of the hidden layer is  storage  space for these interpretations.
Theo-  retically, the higher number of hidden nodes, the more information  the network  can contain.
This  also means more computation,  and may lead to over-fitting.
Therefore, it  should be trade off between   the detection  rate and the computa-  tional expenses on constructing  models and querying new data.
In  this paper,  we choose  the size of LSTM  hidden layer equal to 10.
The  learning rate is an-  other factor directly linked to the speed at  which a  LSTM   RNN  can improve  its predictions.
For a time step t during training process, the synapse  weights of  the neural network  are updated.
The  learning rate defines  how much we wish  a  weight to be modified  at  each instant.
We choose  a  common  used value for  learning rate, 10−4 .
The  preliminary experiment  aim to tune Prediction  Error  Threshold (PET)  and  Collective Range  (CR)   by  using only normal validation set,  nvalid .
This  means that these parameters  are choose so as to the model can correctly classify  most of instances in nvalid  (say 95%, 97%  or 100%)  belonging  to normal class.
In  this paper, we set these threshold so as to keep 100%  of examples in nvalid  as  normal data.
The  choice of CR   depends on  how long a  single data  point  represents  for.
In  our data, each single point represents  a period of 10 minutes,  so  CR.
Thus  we choose CR   equal to  4 which is  equivalent to  a  period of 40  minutes.
One CR  chosen,  we will compute  detection  rate of the model with 20  different  values of PET ranging from 0.05  to 1.0.  on nvalid .
The  smallest value  of PET  that  enables the model to  correctly classify 100% examples in  nvalid  is  chosen.
More details  about  network architectures, parameters settings are  presented  in Table 1.
The  main experiment  is to shown the ability of LSTM-RNNs  in detecting  a  disproportionate durable change in a time series anomaly.
Once the preliminary  experiment is  complete, these trained models with collective thresholds, PET  and CR  is employed  to detect anomalous  region in data, nvalid , ntest   and atest .
This  experimental results include the prediction error of each single data point,  specific  anomalous  regions on normal validation set and test sets from the three  models.
The  training  error is  plotted in  Fig  2.
Fig  3,  4 and  5  illustrate the  prediction errors on validation set (vn ),  testing sets (ntest   and atest ).
Specific    regions  predicted as collective anomaly and the proportion  of these anomalous  regions are presented  in Table 2.
Fig.
2.
The  training errors of the proposed model  Fig.
3.
The  prediction error from 3-step ahead model on validation set.
6     Results  and  Discussion  The  Table 2 shows  the collective  anomaly prediction of the proposed  model on  three datasets,  nvalid , ntest   and atest .
The collective anomaly prediction includes  specific  regions in data and the percentage of data instances (time-steps) within  these regions.
There is no anomalous  region found in normal validation  set, nvalid  because the thresholds, P ET  and C R  have been tuned to classify 100% of nvalid    Fig.
4.
The  prediction error from 3-step ahead model on normal test.
Fig.
5.
The  prediction error from 3-step ahead model on anomaly test.
Table  2.
The  prediction collective anomalies in validation and test sets  1-Step  Ahead   Anomaly  2-Step Ahead  Anomaly  3-Step  Ahead  Dataset  nvalid   ntest   atest  Anomaly  region  107  - 111  116  - 124   125  - 134  -  -  -  -  -  ratio  0.0%  0.0%  8.79%  Anomaly  region  -  -  -  107  - 111  116  - 124   125  - 134  135  - 139   140 - 144  ratio  0.0%  0.0%  12.13%  Anomaly  region  -  215  - 219   107  - 111  116  - 124  125  - 134  135 - 139  140 - 145  150  - 154  Anomaly  ratio  0.0%  1.67%  14.23%    belonging  to normal class.
Only one regions  in normal test, ntest   is found by the  third  classifier (3-step ahead), which is  False  Negative.
In  anomaly test, many  regions are detected as collective anomalies by both of three classifiers.
It seems  to be that the more steps ahead the model can predict, the more regions can  be found.
They  are three regions  (8.79%), five regions (12.13%) and six regions  (14.23%) found by the first, second and third classifiers respectively.
The  fig 2 illustrates the training errors from three classifiers  with 1-step,  2-  step and 3-step prediction ahead.
These errors tend to converge after 100 epochs,  and the error curve of the 3-step classifier levels off earlier than two others.
The  fig 3,  4 and 5 also present  the prediction errors of the third classifiers on nvalid ,  ntest   and atest .
In  fig 3  and 4, the prediction errors are just fluctuated  around  0.2, and few individual  errors have large values.
Thus,  these datasets are not  considered  as collective  anomaly.
However,  the error patterns in fig 5 are quite  different.
The  errors within time steps 110  to 155  is  quite high, much higher  than the rest of error regions.
This  regions are detected as collective  anomalies  as presented  in Table 2.
In training stage, the more prediction time-steps a model has, the less training  error the model produces (see fig.
2).
This  suggests  that the models with more  prediction  time-steps tend to learn the normal behaviors of network traffic more  efficiently  than  ones with less  prediction time-steps.
However,  the number of  prediction  time-steps can enrich the model’s ability to detect anomaly  regions in  the anomaly testing set atest .
This  may imply that the three-steps model is more  robust to learn the normal behaviors  and identify collective  anomalies than the  two others.
7     Conclusion  and  Further  work  In this paper, we have proposed a model  for collective  anomaly  detection based  on Long  Short-Term Memory Recurrent Neural Network.
We have motivated  this method through investigating LSTM   RNN  in  the problem of time series,  and adapted it to detect collective  anomalies by proposing the measurements  in  Section 4.
We investigated  the hyper-parameters,  the suitable number  of inputs  and some thresholds  by using the validation set.
The proposed model is evaluated by using the time series version  of the KDD  1999 dataset.
The  results suggest  that proposed  model is  efficiently  capable of  detecting  collective  anomalies  in the dataset.
However,  they must be used with  caution.
The  training data fed into a network  must be organized  in a coherent  manner to guarantee  the stability of the system.
In  future work, we will focus  on how to improve the classification  accuracy of the model.
We also observed  that implementing  variations in a LSTM  RNNs number  of inputs might trigger  different  output reactions.
References  1.
DARPA  intrusion detection evaluation.
(n.d.).
(Retrieved  June  30,  2016), http:  //www.ll.mit.edu/ideval/data/1999data.html    2.
Ahmed, M., Mahmood, A.N.,  Hu, J.:  A  survey of network anomaly detection tech-  niques.
Journal  of Network and Computer Applications  60, 19–31  (2016)  3.
Bhattacharyya, D.K.,  Kalita,  J.K.: Network anomaly detection: A machine learning  perspective.
CRC Press  (2013)  4.
Chandola,  V.,  Banerjee, A.,  Kumar,  V.:  Anomaly detection: A  survey.
ACM  com-  puting surveys (CSUR)   41(3),   15  (2009)  5.
Chmielewski,  A.,   Wierzchon,  S.T.:   V-detector  algorithm  with  tree-based  struc-  tures.
In:  Proc.
of  the  International  Multiconference on  Computer  Science  and  Information Technology, Wis/la (Poland).
pp.
9–14.
Citeseer (2006)  6.
Hawkins,  S.,  He, H.,  Williams,  G.,  Baxter,  R.:  Outlier  detection using replicator  neural  networks.
In:  International  Conference on Data  Warehousing and  Knowl-  edge Discovery.
pp.
170–180.
Springer (2002)  7.
Hochreiter, S.,  Schmidhuber,  J.:  Long  short-term memory.
Neural  computation  9(8), 1735–1780  (1997)  8.
KDD  Cup  Dataset (1999), available at the following  website  http://kdd.ics.uci.
edu/databases/kddcup99/kddcup99.html  9.
Lee,  W.,  Stolfo,  S.J.:   A  framework for  constructing  features and  models for  in-  trusion detection systems.
ACM   transactions on Information and  system security  (TiSSEC) 3(4), 227–261  (2000)  10.
Lu,  W.,  Ghorbani,  A.A.:   Network anomaly detection based on wavelet analysis.
EURASIP Journal  on Advances  in Signal Processing 2009,  4 (2009)  11.
Malhotra, P.,  Vig,  L.,  Shroff, G.,  Agarwal,  P.:  Long short term memory networks  for anomaly detection in  time series.
In: Proceedings.
p.
89.
Presses  universitaires  de Louvain (2015)  12.
Marchi, E.,  Vesperini,  F.,  Eyben,  F.,  Squartini,  S.,  Schuller,  B.:  A  novel approach  for automatic acoustic novelty detection using a  denoising autoencoder  with bidi-  rectional lstm neural networks.
In: 2015  IEEE International Conference on Acous-  tics,  Speech and Signal  Processing (ICASSP).
pp.
1996–2000.
IEEE (2015)  13.
Marchi,  E.,   Vesperini,  F.,   Weninger, F.,   Eyben,  F.,   Squartini,  S.,   Schuller,  B.:  Non-linear  prediction  with  lstm  recurrent  neural  networks for  acoustic  novelty  detection.
In:  2015  International  Joint  Conference on Neural  Networks (IJCNN).
pp.
1–7.
IEEE (2015)  14.
Olsson, T.,  Holst, A.:  A  probabilistic  approach to aggregating anomalies for unsu-  pervised anomaly detection with industrial  applications.
In:  FLAIRS Conference.
pp.
434–439  (2015)  15.
Salama,  M.A.,  Eid,  H.F.,  Ramadan,  R.A.,   Darwish,  A.,  Hassanien, A.E.:   Hybrid  intelligent intrusion detection scheme.
In: Soft computing in industrial applications,  pp.
293–303.
Springer (2011)  16.
Staudemeyer, R.C., Omlin, C.W.:  Evaluating performance  of long short-term  mem-  ory recurrent neural networks on intrusion detection data.
In:  Proceedings of the  South  African  Institute  for  Computer  Scientists  and  Information Technologists  Conference.
pp.
218–224.
ACM  (2013)
Lots of biological experiments and theoretical analysis have demonstrated that the speed and scale of processing infor- mation by biological neural networks are much faster and larger than by manual methods [1] [2].
Inspired by animals’ central nervous systems in particular the brain, many kinds Jie Yang · Pingping Zhang School of Mathematical Sciences, Dalian University of Technology, Dalian 116024, China.
E-mail: yangjiee@dlut.edu.cn; jssxzhpp@dlut.edu.cn Yan Liu School of information Science and Engineering, Dalian Polytechnic University, Dalian 116034, China.
E-mail: liuyan.3001@gmail.com of artiﬁcial neural networks (ANNs) and training methods have presented to mimic animals’ behavior characteristics.
ANNs are distributed mathematical models that process in- formation parallelly [3] [4].
They have been used to solve a wide variety of tasks that are hard to solve by using or- dinary rule-based programming, including computer vision and speech recognition [5] [6] [7] [8].
These networks base on the characteristics and scales of data and the complex- ity of systems.
By adaptively adjusting the weights which are connected between different nodes in adjacent layers, ANNs can achieve the purpose of processing information.
As a special class of ANNs, spiking neural networks (SNNs) can simulate spikes generated between animal dendrites and axons of neurons [4] [9].
With temporal information coding in single spikes to process information, they were proved to be a type of strong anthropomorphic networks.
However, due to many uncontrollable factors, such as noising inputs, individual spike decay times, thresholding weights, the abilities of processing information by spiking neural networks may be affected [10] [11].
In order to get an available network architecture, it is important to do some research on its robustness.
Because of encoding input vari- ables by time differences between pulses, spiking neurons are particularly sensitive to the input signals.
Recently, this has led to several explorations on the computational abili- ties and learning performance of neuromorphic networks of spiking neurons with noise [12] [13].
However, these works have not considered the type of perturbations and the robust- ness of classiﬁcation ability of SNNs. To analyze the robustness of SNNs’ classiﬁcation abil- ities, in this paper we performed a series of numerical ex- periments on the classical XOR problem and other three benchmark datasets (i.e., Iris dataset, Wisconsin breast can- cer dataset and StatLog landsat dataset) with the SpikeProp algorithm [14].
Notably, the closer the perturbed inputs to the desired points, the more perturbed points there are in the 2 Jie Yang et al.
Fig.
1 The structure of a simple spiking neural network.
input space.
What’s more, the perturbation may be periodic in practice.
These facts led us to consider the sinusoidal and Gaussian perturbations in this paper.
To summarize, our main contributions include: – As far as we know, this is the ﬁrst work to validate the ro- bustness of classiﬁcation ability of SNNs which receive perturbed inputs.
– Two kinds of perturbations were considered for robust- ness of classiﬁcation ability of a SNN.
In fact, perturba- tions could be arbitrary styles performing on the inputs.
We only focus on sinusoidal and Gaussian perturbations.
– On the classical XOR problem and other three bench- mark datasets, we evaluate the classiﬁcation ability of SNNs and show its robustness experimentally.
2 Spiking neural networks Compared with traditional neural networks (such as BP), SNNs have several differences in network architectures.
The most important one is that there are multiple synaptic termi- nals and the speciﬁc synaptic delay between spiking neurons in adjacent layers.
In addition, due to the fast temporal en- coding which is very different from traditional rate-coded networks, spiking neurons can signiﬁcantly improve com- plex non-linear classiﬁcation performances [4] [14].
2.1 The architectures of SNNs A simple feed-forward SNN with multiple input spiking neu- rons and one output spiking neuron is shown in Fig 1.
The network architecture consists of one input layer, one hidden layer and one output layer, denoted by I, H and O respec- tively.
Each connection between different layers comprises several synapses and each neuron receives a set of spikes from all its previous neurons.
Formally, assuming that dur- ing the simulation interval each neuron generates at most one spike and ﬁres when the internal state variable reaches a threshold, the state variable x j of neuron j receives out- puts from all its previous neurons as a weighted sum of the pre-synaptic contributions: x j(t) = Σi∈D jΣm k=1wk i jyk i (t) (1) where D j denotes the set of pre-synaptic neurons associated with neuron j, wi j is the weight associated with synaptic terminal k, and yk i (t) represents a delayed pre-synaptic po- tential (PSP) for each terminal, yk i (t) = ε(t − ti − dk) (2) with ε(t) a spike-response function.
The time ti is the ﬁring time of pre-synaptic neuron i, and dk is the delay associated with the synaptic terminal k.
The ﬁring time is determined as the ﬁrst time when the state variable reaches the threshold.
The spike-response function is always described as the form ε(t) =(cid:26) t τ exp(1− t 0, τ ), t > 0 t ≤ 0 (3) where τ models the membrane potential decay time constant that determines the rise and decay time of the PSP.
2.2 Learning algorithm The basic SpikeProp algorithm [14] is performed and we choose the least mean square as the error-function.
Given desired spike times {t d j } and actual ﬁring times {t j}, we can derive the form of the error-function E = Σj(t j − t d j )2.
(4) For error back-propagation, the weights update rule is fol- lowed: wk+ i j (t j) = wk i j(t j) + ∆wk i j(t j) Deﬁne δj = t d j − t j ∂yl Σil wl i j i (t j ) ∂t j (5) (6) In the output layer, the basic weight adaptation function for neurons is derived as ∆wk i j(t j) = −η yk i (t j)(t d j − t j) ∂yl i (t j ) ∂t j Σilwl i j = −ηyk i (t j)δj.
(7) For the hidden layers, the weight adaptation function for neurons is given by ∆wk hi(t j) = −η ∂yk i (t j ) ∂t j yk h(t j)Σj{δjΣkwk ∂yl n(t j ) ∂t j Σnlwl ni i j (8) where η is the learning rate of the network.
For more details, see [14] [15] [16].
Robustness of classiﬁcation ability of spiking neural networks 3 Perturbations to neural networks Table 1 Encoded inputs and outputs for XOR problem Input patterns Output patterns 16 10 10 16 (1)Sinusoidal perturbations 3.1 Traditional neural network perturbation approaches Traditional neural networks are led to suitable Lyapunov- Krasovskii functional and by introducing appropriate ran- dom variables, such as the free weights techniques, one can analyze stochastic neural networks associated with the value of parameter uncertainties and numerical experiments to demon- strate the robust global exponential stability or asymptotic stability.
In [17], the effect of both variation range and dis- tribution of the time delay were taken into account for delay- distribution-dependent state estimation.
The stochastic per- turbations are described in terms of a Brownian motion and the time-varying delay is characterized by introducing a Bernoulli stochastic variable.
In [18], some parameter matrices were used to express the relationships among the system variables which were perturbed to study the asymptotic stability of delay nonlinear cellular neural network.
By perturbing the time variable interval of Hopﬁeld neural networks, [19] in- vestigates the existence of the equilibrium points and global robust exponential stability .
The original input ˜x0 is perturbed by a sinusoidal perturba- tion term, which can be expressed as ˜x = ˜x0 + A sin(2πy) (10) where A is a constant between (0,1] to control the perturba- tion amplitude, and y is a random vector and its component values belong to [0,1].
The numbers of components associ- ated with ˜x0 and y are the same.
(2)Gaussian perturbations The original input ˜x0 is perturbed by a Gaussian perturba- tion.
This is different from the sinusoidal perturbation, and can be expressed as 3.2 Our perturbation approach SNNs use the reacted pulses to transmit information.
When the input signal enter into a spiking neural network, the state of each neuron will change (see formulas (1),(2),(3)).
Once the state variable exceeds the threshold value, the neuron arouses a pulse.
Hence input signals of SNNs have a great relevance to their robustness.
A large number of practical examples imply that the signals after random perturbations will not be very different from the original one, and only few perturbed signals have large deviations from the origi- nal data.
With these basic facts, we are motivated to perform different types of perturbations on input signals, and inves- tigate the the robust classiﬁcation abilities of SNNs. For the sake of simplicity, we only describe perturba- tions on XOR problem.
For other three benchmarks, the op- eration is similar.
Classical XOR problem requires hidden units to transform the inputs into the desired outputs and needs to classify 4 points (described as (0,0),(1,1),(1,0),(0,1) ) into 2 categories.
It should be noted: if the four points are perturbed with a same value, it is equivalent to translat- ing them simultaneously, leading no difference between the original inputs in essence.
For convenience, we choose the point (1,1) as a target point to test the robustness of spiking neural networks.
The basic perturbation formula is ˜x = ˜x0 + σ (9) with ˜x0 being the original input, and σ the noise term.
The formula means if ˜x0 = (a, b), the perturbed input ˜x will be (a + σ, b + σ).
˜x = ˜x0 + ˜x0(I − exp(−r2/2) · sgn(l)) (11) where I is an identity matrix, r is a random vector whose component values belong to [0,1] and sgn(l) is the signal function associated with l.
Here the numbers of components associated with ˜x0, r and l are also the same.
4 Experimental results Since our aim is to assess the robustness of classiﬁcation ability of SNNs, we don’t need to design complex SNNs. We train a simple spiking neural network on XOR problem and other three benchmark datasets.
4.1 XOR problem Firstly, the input and output signals of spiking neural net- works are coded as in [14].
If Max and Min are extremal values of a variable x (e.g.an input signal), we can encode it as a spike ﬁred in the time f (x) = x − Min Max − Min · L (12) with the length of coding interval L.
For a better representation of spike-time patterns, we can associate a 0 with a “late” ﬁring time and a 1 with an “early” ﬁring time.
With speciﬁc values 0 and 6 for the respective in- put times, we lead to the temporally encoded XOR [14] [20] showing in Table 1.
In the table, the input numbers repre- sent spike times (i.e. late ﬁring times and early ﬁring times) in milliseconds.
The actual input patterns contain a setting 4 Jie Yang et al.
Table 2 The result of the correct classiﬁcation with sinusoidal per- turbations.
ROS: Rates of the correct classiﬁcation without sinusoidal perturbations, RWS: Rates of the correct classiﬁcation with sinusoidal perturbations.
Table 3 The result of the correct classiﬁcation with Gaussian pertur- bations.
ROG: Rates of the correct classiﬁcation without Gaussian per- turbations, RWG: Rates of the correct classiﬁcation with Gaussian per- turbations.
Epoches A ROS RWS Epoches 50 50 50 50 50 50 0.001 0.01 0.1 0.2 0.5 0.8 90.50 89.50 91.00 88.50 87.50 87.50 91.00 87.80 88.90 87.20 82.24 85.58 threshold by adding a third input neuron.
Here we deﬁne the difference between the times equivalent with “0” and “1” as the coding interval L = 6 ms.
We use the feed-forward net- work with connections that have a delay interval of 15 ms, so the available synaptic delays are 1-16 ms for classiﬁcation.
According to the formula (3), we calculated the postsynaptic states and selected τ = 7 ms.
Based on the above settings, numerical experiments were performed with a spiking neural network composed of one input layer, one hidden layer and one output layer.
There were three input layer neurons (two encoding neurons and one threshold neuron), ﬁve hidden layer neurons (of which one inhibitory neuron generating only negative signal PSPs) and one output layer neuron.
A weight and corresponding terminals (m = 16) were conﬁgurated between each pair of synaptic neurons in adjacent layers.
Using computer simulations, we randomly generated 400 perturbed samples for different r to see the distribution of perturbed samples as shown in Fig 2.
Considering the com- putational complexity and time, we only used 160 perturbed samples for testing.
We varied A and r∗ (the least upper bound of all components to the random vector r )to control perturbation amplitudes.
The network reliably learned the XOR patterns with η = 0.01.
With different perturbations, we got the correct classiﬁcation rate of the spiking neural network.
The average of correct classiﬁcation rates with and without perturbed data are compared in Table 2 and Table 3.
As results are reported in Table 2 and Table 3, the clas- siﬁcation accuracies associated with original data are differ- ent but almost equal (about 90%).This is mainly because the input data were reordered after each epoch.
The larger per- turbation was performed to the input data, the more greatly rates of correct classiﬁcation with sinusoidal disturbances decreased (see Table 2).
When we disturbed the input data with Gaussian perturbations, the network did not got simi- lar results (see Table 3).
The correct classiﬁcation rates of the network did not fall too much.
The reason may be that most of the perturbed data by Gaussian perturbations were clustered around the desired value.
Therefore, it indicates r∗ 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 ROG RWG 92.00 92.00 89.50 89.50 88.50 88.25 89.75 91.75 91.00 88.25 88.62 88.45 87.80 88.15 85.00 88.66 89.60 90.86 89.76 88.81 50 50 50 50 50 100 100 100 100 100 spiking neural networks have strong anti-interference abili- ties.
4.2 Other three benchmarks To further validate robustness of classiﬁcation ability of SNNs in practice, we consider the following three benchmarks with realistic signiﬁcance.
As XOR problem does, we ﬁrst adopt the method in [14] to encode continuous input variables in min,...,In spike times.
Specially, for a variable n with a range[In we use N neurons with Gaussian receptive ﬁelds to encode the input variable.
For a neuron i, its center was set to In min + min}/(N −2) and width σ = 1/β·{In (2i−3)/2·{In max − In min}/(N − 2).
We set β = 1.5 for the remainder experi- ments.
As for output classiﬁcation, we encoded the patterns according to a winner-take-all paradigm.
max− In max], (1) Iris dataset The Iris ﬂower data set or Fisher’s Iris data set [21] is a multivariate data set as a typical test case for many clas- siﬁcation techniques in machine learning.
The data set con- sists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor).
Four features are measured for each sample: the length and the width of the sepals and petals in centimetres, respectively.
Based on the combination of these four features, we add above perturba- tions in each samples to build perturbed input sets.
In this set of experiments, we implement a SNN with three layers just like for XOR problem.
But each feature was encoded by 12 neurons with Gaussian receptive ﬁelds(yielding 48 encoding neurons and one threshold neuron).
And the SNN consists of 10 hidden layer neurons (only one neuron of which gener- ates negative signal PSPs) and 3 output neurons.
The num- ber of corresponding synaptic terminals is same as the last experiment(i.e.m = 16).
We train this SNN with perturbed input data or clear input data to distinguish the species from each other.
The results are presented in Table 4 and Table 5.
Robustness of classiﬁcation ability of spiking neural networks r*=3.0,sigma=0.5 r*=3.0,sigma=1.0 r*=3.0,sigma=1.5 1.8 1.6 1.4 1.2 0.8 0.6 0.4 0.2 1.8 1.6 1.4 1.2 0.8 0.6 0.4 0.2 1.8 1.6 1.4 1.2 0.8 0.6 0.4 0.2 1.8 1.6 1.4 1.2 0.8 0.6 0.4 0.2 1.8 1.6 1.4 1.2 0.8 0.6 0.4 0.2 1.8 1.6 1.4 1.2 0.8 0.6 0.4 0.2 0.2 0.4 0.6 0.8 1.2 1.4 1.6 1.8 r*=4.0,sigma=1.0 0.2 0.4 0.6 0.8 1.2 1.4 1.6 1.8 0.2 0.4 0.6 0.8 1.2 1.4 1.6 1.8 r*=2.0,sigma=1.0 0.2 0.4 0.6 0.8 1.2 1.4 1.6 1.8 0.2 0.4 0.6 0.8 1.2 1.4 1.6 1.8 r*=8.0,sigma=1.0 0.2 0.4 0.6 0.8 1.2 1.4 1.6 1.8 Fig.
2 The perturbed samples for different r∗.
The desired point is (1,1) and the closer to the desired point, the more perturbed points there are.
Besides, the perturbed points become more scattered when r∗ increases.
Table 4 The result of the correct classiﬁcation with sinusoidal pertur- bations on Iris dataset.
ROS: Rates of the correct classiﬁcation without sinusoidal perturbations, RWS: Rates of the correct classiﬁcation with sinusoidal perturbations.
Table 5 The result of the correct classiﬁcation with Gaussian pertur- bations on Iris dataset.
ROG: Rates of the correct classiﬁcation without Gaussian perturbations, RWG: Rates of the correct classiﬁcation with Gaussian perturbations.
Epoches A ROS RWS Epoches and 2 output layer neurons.
The results are presented in Ta- ble 6 and Table 7.
500 500 500 500 500 500 0.001 0.01 0.1 0.2 0.5 0.8 96.50 94.60 91.73 91.50 89.35 88.50 95.71 93.84 88.90 88.40 87.62 87.58 (2) Wisconsin breast cancer (Original) dataset The breast cancer (Original) dataset [22] is from the Uni- versity of Wisconsin Hospitals and contains 699 case en- tries, divided into benign and malignant cases.
Each case has 9 measurements and each measurement is assigned an integer between 1 and 10, with larger numbers indicating a greater likelihood of malignancy.
In our experiments, we encoded each measurement with 7 equally spaced neurons covering the input range.
We set 15 hidden layer neurons r∗ 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 ROG RWG 96.10 94.80 94.50 91.50 89.56 96.21 95.75 94.25 91.00 89.25 96.25 95.35 91.27 91.08 89.21 96.02 94.43 94.05 90.13 88.00 96.66 94.90 93.76 89.74 88.85 96.01 94.60 90.86 90.67 89.01 750 750 750 750 750 1000 1000 1000 1000 1000 1500 1500 1500 1500 1500 6 Jie Yang et al.
Table 6 The result of the correct classiﬁcation with sinusoidal pertur- bations on Wisconsin breast cancer (Original) dataset.
ROS: Rates of the correct classiﬁcation without sinusoidal perturbations, RWS: Rates of the correct classiﬁcation with sinusoidal perturbations.
Table 8 The result of the correct classiﬁcation with sinusoidal pertur- bations on StatLog landsat dataset.
ROS: Rates of the correct classi- ﬁcation without sinusoidal perturbations, RWS: Rates of the correct classiﬁcation with sinusoidal perturbations.
Epoches A ROS RWS Epoches A ROS RWS 1500 1500 1500 1500 1500 1500 0.001 0.01 0.1 0.2 0.5 0.8 97.50 97.34 95.60 95.50 96.02 93.56 97.60 97.20 95.53 93.80 94.84 91.68 6000 6000 6000 6000 6000 6000 0.001 0.01 0.1 0.2 0.5 0.8 85.50 85.17 85.00 85.21 84.50 83.10 85.61 84.80 84.90 85.20 82.32 82.04 Table 7 The result of the correct classiﬁcation with Gaussian pertur- bations on Wisconsin breast cancer (Original) dataset.
ROG: Rates of the correct classiﬁcation without Gaussian perturbations, RWG: Rates of the correct classiﬁcation with Gaussian perturbations.
Table 9 The result of the correct classiﬁcation with Gaussian pertur- bations on StatLog landsat dataset.
ROG: Rates of the correct clas- siﬁcation without Gaussian perturbations, RWG: Rates of the correct classiﬁcation with Gaussian perturbations.
Epoches 1000 1000 1000 1000 1000 1500 1500 1500 1500 1500 r∗ 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 ROG RWG 95.75 95.85 94.75 92.00 91.57 97.40 97.13 95.57 96.03 93.54 96.06 94.60 94.86 91.96 91.17 97.52 96.59 93.86 95.45 91.60 Epoches 6000 6000 6000 6000 6000 7500 7500 7500 7500 7500 r∗ 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 ROG RWG 85.30 85.07 83.50 83.46 81.56 85.60 85.00 84.50 82.58 81.80 85.02 84.45 82.83 83.15 81.00 85.62 84.75 83.80 82.15 80.97 (3) StatLog landsat dataset To test the robustness of SNNs on a larger dataset, we investigated the Landsat dataset as described in the StatLog survey of machine learning algorithms [23].
This dataset consists of a training set of 4435 cases and a test set of 2000 cases and contains 6 ground cover types (classes).
Each sam- ple contains the values of a 33 pixel patch and each pixel is described by 4 spectral bands.
For classiﬁcation of a single pixel, each case contains the values of a 33 pixel patch, with each pixel described by 4 spectral bands, totaling 36 inputs per case.
For each band, we used the average value of cor- responding bands of 9 pixels as a new band of a pixel.
Then the case was represented with one average pixel and each separate band of it was encoded with 25 neurons.
In this set of experments The results obtained by the Statlog survey are summarized in Table 8 and Table 9.
From the above results, we could conclude that the ap- plication of SNNs with SpikeProp algorithm on temporally encoded versions of benchmark problems yields a satisfac- tory robustness for sinusoidal and Gaussian perturbations.
5 Conclusions In this work, the robustness of the classiﬁcation ability of SNNs has been investigated by disturbing input signals with different perturbation methods not only for the classical XOR problem but also for some other complicated realistic prob- lems.
From the experiments results, it can be concluded that nevertheless perturbations will affect the classiﬁcation abil- ity of SNNs, the classiﬁcation ability does not decrease dra- matically and the networks have a certain anti-interference capability.
References 1.
Rochester, N., J.H. Holland, L.H. Habit, and W.L, Duda,Tests on a cell assembly theory of the action of the brain, using a large digital computer.
IRE Transactions on Information Theory,2,80-93(1956) 2.
Thorpe S, Fize D, Marlot C, Speed of processing in the human vi- sual system.
Nature,381,520-522 (1996) 3.
Werbos, P.J,Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences.
PhD thesis, Harvard University, (1974) 4.
Wolfgang Maass,Networks of spiking neurons: The third gen- eration of neural network models.Neural Networks,10,1659- 1671(1997) 5.
M Riesenhuber, T Poggio,Hierarchical models of object recogni- tion in cortex.
Nature neuroscience,2,1019-1025(1999) 6.
Fukushima, K.,Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position.
Biological Cybernetics,36,93-202(1980) 7.
A.
Graves, M.
Liwicki, S.
Fernandez, R.
Bertolami, H.
Bunke, Improved J.
Schmidhuber,A Novel Connectionist System for Robustness of classiﬁcation ability of spiking neural networks Unconstrained Handwriting Recognition.
IEEE Transactions on PAMI,31,855-868(2009) 8.
D.V. Buonomano, M.
Merzenich.
A neural network model of tem- poral code generation and position-invariant pattern recognition.
Neural Comput,11,103-116(1999) 9.
Bienenstock,E, A model of neocortex.Network:Computation in Neural Systems,6,179-124(1995) 10.
Vogl, T.P., Mangis, J.K., Rigler, A.K., Zink, W.T., Alkon, the back-propagation D.L., Accelerating the convergence of method,Biol.Cybern.,59,257-263(1988) 11.
Ghosh-Dastidar, networks detection,Integr.Comput.-Aid E.,14,187-212(2007) neural for EEG classiﬁcation and epilepsy and seizure S., Adeli, H.,Improved spiking 12.
Maass,W., Noisy spiking neurons with temporal coding have more computational power than sigmoidal neurons,In: Advances in Neural Information Processing Systems,MIT Press,Cambridge,USA,9,211- 217 (1997) 13.
Wolfgang Maass.Noise as a Resource for Computation and Learning in Networks of Spiking Neurons.Proceedings of the IEEE,102,860-880(2014) 14.
Sander M.Bothe, backpropagation in temporally encoded networks of neurons.
Neurocomputing 48,17-37(2002) Joost N.Kok, Han La Poutre.
Error- spiking 15.
Gerstner,W., Kistler,W., Spiking Neuron Models.Cambridge Uni- versity Press,England (2002) 16.
Jie Yang, Wenyu Yang, Wei Wu. A remark on the error- backpropagation learning algorithm for spiking.
Applied Mathemat- ics Letters,25,1118-1120(2012) 17.
Haibo Bao, Jinde Cao.
Delay-distribution-dependent state estima- tion for discrete-time stochastic neural networks with random delay.
Neural Networks,24,19-28(2011) 18.
Mo Y Z, Ding M Z,Yu J M.
Stability analysis of nonlinear cellular neural networks with time-varying delay.
J Chongqing Univ Nat Sci Ed,22,817-822(2010) 19.
Li Y B, Wang R L.
Stability of Reaction-diffusion Hopﬁeld Neural Networks with S-type Distributed Delays.
J Harbin Univ Nat Sci Ed, 15,63-66(2010) 20.
Jie Yang, Wenyu Yang, Wei Wu. A novel spiking perceptron that can solve XOR problem.
Neural Network world,21,45-50(2011) 21.
Fisher, R.A. The Use of Multiple Measurements in Taxonomic Problems.
Annals of Eugenics,7,179-188(1936) 22.
W.H. Wolberg, Cancer dataset obtained from Williams H.
Wolberg from the University of Wisconsin Hospitals, Madison, 1991.
23.
D.
Michie, D.J. Spiegelhalter, C.C. Taylor (Eds.), Machine Learn- ing, Neural and Statistical Classiﬁcation, Ellis Horwood, West Sus- sex, 1994.

The wheat is the main source of nutrients to the world population, most of its produc- tion is converted into ﬂour for human consumption.
In Southern Brazil, where 90% of the domestic wheat is produced, Fusarium head blight (FHB), a fungal disease, is a ma- jor concern.
Apart from yield loss, the causal agent Fusarium graminearum may cause mycotoxin contamination of wheat products, creating health problems.
Therefore, to avoid potential health risks, Fusarium affected grains must be iden- tiﬁed and segregated, before their processing, to avoid its incorporation into food for humans and animal feed.
Usually, the detection of Fusarium head blight (FHB) is carried out manually by human experts using a process that may be both lengthy and tiresome.
Moreover, the effectiveness of this kind of detection may drop with factors such as fatigue, external distractions and optical illusions [2].
Thus, improving the detection of Fusarium Head Blight (FHB) in wheat kernels has been a major goal, due to the health risks associated with the ingestion of the mycotoxin, mainly deoxynivalenol (DON).
Methods capable of performing this disease detection in an automatic way are highly demanded in the productive wheat chain, to segregate lots.
Most automatic meth- ods proposed to date rely on image processing to perform their tasks [2, 3, 8].
Barbedo et al.
[2] used Hyperspectral imaging (HSI) for detecting Fusarium head blight (FHB) in wheat kernels using an algorithm.
The outcome was a Fusarium index (FI), ranging from 0 to 1, that can be interpreted as the likelihood of the kernel to be damaged by FHB.
According to the authors, hyperspectral imagery is currently not sensi- tive enough to estimate DON content directly.
However, an indirect estimation from the Fusarium damaged kernels was successfully achieved, with an accuracy of approximately 91% [2].
Other study investigated the use of hyperspectral imaging (HSI) for deoxyni- valenol (DON) screening in wheat kernels.
The developed algorithm achieved accuracies of 72% and 81% for the three- and two-class classiﬁcation schemes, respectively.
The results, although not accurate enough to provide conclusive screening, indicating that the algorithm could be used for initial screening to detect wheat batches that warrant further analysis regarding their DON content [3].
Min and Cho [9] presented a review about nondestructive detection of fungi and mycotoxins in grains, focusing on spectroscopic techniques and chemometrics.
The spec- troscopy has advantages over conventional methods including the rapidness and nonde- structive nature of this approach.
However, some limitations as expensive and complex setup equipment’s and low accuracy due to external interferents exist, which must be overcome before widespread adoption of these techniques.
The application of computer vision on digital images offers a high-throughput and non-invasive alternative to analytical and immunological methods.
This paper presents an automated method to detect Fusarium Damaged Kernels, which uses the application of computer vision to digital images.
The main goal of this work is the use of machine learning algorithms and computer vision techniques to detect Fusarium Damaged Kernels in wheat, based on digital images.
2.
Material and Methods Digital images of Fusarium Head Blight symptomatic and non-symptomatic wheat ker- nels were available at the National Research Centre for Wheat (Embrapa Wheat), located in Passo Fundo, Rio Grande do Sul State, Brazil.
The images were obtained by recording a video of 06:25 minutes using an Olym- pus SP-810UZ digital camera with 36x optical zoom, 24mm wide-angle view, and 14- megapixel resolution, 720p HD video and Olympus Lens 36x Wide Optical Zoom ED 4.3-154.8mm 1:2.9-5.
All tasks run on an iMac workstation conﬁgured with 32GB of RAM DDR3 1600MHz, a 3.5GHz quad-core Intel Core i7 processor, and a NVIDIA GeForce GTX 780M GPU with 4GB of GDDR5 memory.
The TensorFlow 1.0.1 built from source with CUDA Toolkit 8.0 and cuDNN v5.1 to enable GPU support.
All scripts were developed using Python 2.7. 2.1. Methodology To classify digital images in predeﬁned classes, we could use one of the several methods developed in recent two decades [10, 6].
Methodologies to solve this kind of problem was developed both in Artiﬁcial Intelligence (AI), a research area in Computer Science, and in Statistics.
The main differences between Statistics and AI approach are the size of the task, for statistics point of view, algorithms and techniques are limited when then input size of image dataset are greater than tens of thousand pictures and for a large number of classiﬁcation sets.
During the training stage of a system to classify images and objects based only on information content embedded in a single digital ﬁle, it is necessary that this system would detect all possible contexts where the object could occur.
Small differences in color, luminosity, angle and other could be misinterpreted by the proposed system and results in the wrong or low-level prediction classiﬁcation.
The key advantage of using AI strategy in the image classiﬁcation task are re- lated to knowing how to combine layers and manage the relationship between levels of information, without needing any strong assumption related to the type of dependency or relational structure among input information.
In some cases, the improvements in the accuracy and precision gain are archive using these ﬁne tune setting, but the most of the effort is made only with the input infor- mation, in other words, the processes are designed to take the most of the self-learning way.
2.2. Deep Convolution Neural Network (DNN) The most used architecture of the neural network for image classiﬁcation task is called convolutional neural network (CNN).
The convolution operation or, sometimes called convolution layer is related to the operation to process or respond to “stimuli” in a limited region known as the receptive ﬁeld.
The receptive ﬁeld from each neuron contains a partial overlap of information from input layer (raw image) and, in this way, the preprocessing or further operations occur with a minimal amount of effort.
Other advantages of this technique are related to the possibility of using distributed algorithms (even GPU versions) to calculate, ﬁlters and processing small pieces of information, one each time and aggregated the results when necessary [7].
The deep portion of CNN come from the stacking or combination of several layers where the output of preceding layer is used as an input for the next one.
The most common layers employed in DNN are convolution, ReLU (Rectiﬁed Linear Units), tanh (Hyper- bolic Tangent Function), max pooling, average pooling, fully connected, concat, dropout and softmax.
To better understand this relationship see a CNN example in Figure 1.
2.3. Transfer Learning Transfer learning is a machine learning method which utilizes a pre-trained neural net- work, this technique allows the detachment of the lasts outer layer (classiﬁcation layer) Figure 1.
An example of relationship between layers in a CNN with convolution, max pooling and dense neuron connections annotated on the illustration.
Source: [7, Figure 2] and uses the remains structure to retraining and get new weights corresponding the classes of interest – damaged kernel in our case (Figure 2).
Figure 2.
An illustration of a representation of a Deep Convolutional Neural Net- work, trained on top of ImageNet with detailed information about Feature Extrac- tion part and Classiﬁcation part.
Each small box represents one of the layer types in the Inception Network: Convolution AvgPool, MaxPool, Concat, Dropout, Fully Connected and Softmax.
Source: Adapted from [4, Figure 5] In this work, we use as a pre-trained neural network the output from [11].
Szegedy et al.
[11] developed a 22 layer deep convolution neural network on top of ImageNet for classifying 1000 leaf-node categories, using 1.2 million images for training, 50,000 for validation and 100,000 images for testing.
In brief, transfer learning makes it possible to classify new classes based on a new set of images, reusing the feature extraction part and re-train the classiﬁcation part of the new picture set.
Since feature extraction part of the network it was already trained (which is the most complex part), the new neural network could be trained with less computational resources and time.
2.4. Machine Learning Framework Nowadays we have some options to build and analyze deep neural networks using ma- chine learning algorithms.
The ﬁnal choice would base on the computational infrastruc- ture available to run this task, either the number of classes, the intended purpose and where the ﬁnal Net will be deployed to handle.
For this work, it is necessary a framework for machine learning that could run on a distributed system with both CPU and GPU, with the possibility to deploy the ﬁnal network to servers, desktops, mobile applications and embedded systems in an easy way.
Alongside these needs, it is necessary that the chosen framework could easily im- plement the Transfer Learning techniques, described before.
Based on these requirements, the natural choice is TensorFlow [1].
Abadi et al.
[1] presented TensorFlow as an interface for expressing and executing machine learning algorithms that can be performed with little or no change in a broad range of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computa- tional devices such as GPU cards.
2.5. Image Pre-processing To generate the normal kernel images was used the FFMPEG library1 to split the 6:25 minutes movie into 11,555 individual ﬁles (1280x720).
Both normal and damaged kernel images were arranged in the separate folder for later use.
Before using the images for generating the neural network, they as randomly al- located in two distinct image sets: 80% and 20% for training and validation set, respec- tively.
In the process of training the Net, other parts of wheat plant structure like spikes and leaves were used too in the composition of the DNN intending to classify better the wheat damaged kernels.
3.
Results and Rationale Comparing the effort described in [11] to training a whole DNN from scratch with the number of pictures necessary to get reasonable results.
In our case, the number of pictures available at the moment (≈ 12,000) probably will not archive this scores and deﬁnitely, the time and computational infrastructure necessary to training and evaluate the resultant DNN must be larger than installed capacity.
A broad range of applications are using transfer learning, Devikar [4] describe the use in image classiﬁcation of various dog breeds with an overall accuracy of 96% from 11 dog breeds.
Wang et al.
[13] describes the application for remote scene classiﬁcation and attempt to form a baseline for transferring pre-trained DNN to remote sensing images with various spatial and spectral information.
Esteva et al.
[5] describes the use of DNN for dermatologist-level classiﬁcation of skin cancer, trained end-to-end from images directly, using only pixels and disease labels as inputs for a dataset of 129,450 clinical images.
1FFMPEG library, see more information on http://www.ffmpeg.org.
Tk´aˇc and Verner [12] presents many business applications, using artiﬁcial neural networks, related to ﬁnancial distress and bankruptcy problems, stock price forecasting, and decision support, with particular attention to classiﬁcation tasks.
For other uses of CNN and DNN, see [10, 6].
The training procedure was carried out on the described workstation and took up to 1 hour to ﬁnish, and the output retrained neural network archive an overall average accuracy of 94.7%.
The main structure of the ﬁnal neural network is presented in Figure 3.
For validation purposes and to check the correctness in classifying new images of wheat kernels (with and without Fusarium damage) we choose a new dataset to validate against DNN.
These images were from National Research Centre for Wheat articles pub- lished over last decade about this content, alongside with other open source images of Fusarium damaged kernel found over the Internet.
At this point, we could share our positive experience in using the transfer learning techniques in relation to time to training a DNN with a new set of images and classes and the overall accuracy achieved with this initial image dataset.
The results from validation dataset present a total of 20% of misclassiﬁcation, and in half (10%) the new images were classiﬁed as damaged leaves class.
This results could be related to two main reasons: (a) the small number of pictures with Fusarium Damaged Kernels and (b) the prevalence of the normal wheat kernels present in the initial dataset (≈ 80%).
4.
Conclusions The associated use of Transfer Learning, TensorFlow, and Inception-v3 cut the time nec- essary to training and the necessity to have a large image dataset (≈ 120,000) to start the classiﬁcation procedure with a good accuracy level, compared to training a DNN from scratch.
The misclassiﬁcation for damaged leaves class could be associated with the char- acteristics of damage both in kernel and leaves (in the most cases) where the region color of lesions was more blight that the standard wheat kernel.
Unfortunately, the symptoms on leaves present in this initial dataset was not sep- arated by diseases.Thus, it was not possible to claim that some visual characteristics of FHB in leaves could be transferred to kernel evaluations in our context.
This hypothesys needs to be investigated by adding new images to the present dataset related to Fusarium Damaged Kernel and wheat leaves with FHB symptoms.
Beside this misclassiﬁcation for a new dataset, the overall average accuracy archived (94.7%) for this Fusarium Damaged Kernels Deep Neural Network (FDK- DNN).
Therefore, there is a potential of using this methodology for classifying Fusarium Damaged Kernel by means of smartphone camera.
The accuracy of the proposed methodology is equivalent to ones using HSI methodology presented by [3].
An interesting future work could be related to using a mixed of RGB pictures, and layers from HSI operational spectra for Fusarium Damaged Kernel proposed by [2].
Figure 3.
An illustration of Deep Convolutional Neu- ral Network, trained using TensorFlow [1] and Transfer Learning techniques from a pre-trained Inception-v3 [11] for an image dataset contain- ing Fursarim Damaged Ker- nel, normal kernel, spikes and leaves from wheat.
The output deep neural network archive an overall average ac- curacy of 94.7%.
kernels using hyperspectral imaging.
Biosystems Engineering, 155:24–32, 2017.
[4] P.
Devikar.
Transfer learning for image classiﬁcation of various dog breeds.
In- ternational Journal of Advanced Research in Computer Engineering & Technology (IJARCET), 5(12):2707–2715, 2016.
[5] A.
Esteva, B.
Kuprel, R.
A.
Novoa, J.
Ko, S.
M.
Swetter, H.
M.
Blau, and S.
Thrun.
Dermatologist-level classiﬁcation of skin cancer with deep neural networks.
Nature, 542(7639):115–118, 2017.
[6] Y.
Guo, Y.
Liu, A.
Oerlemans, S.
Lao, S.
Wu, and M.
S.
Lew.
Deep learning for visual understanding: A review.
Neurocomputing, 187:27 – 48, 2016.
ISSN 0925- 2312.
doi: https://doi.org/10.1016/j.neucom.2015.09.116.
Recent Developments on Deep Big Vision.
[7] A.
Krizhevsky, I.
Sutskever, and G.
E.
Hinton.
Imagenet classiﬁcation with deep convolutional neural networks.
In Advances in neural information processing sys- tems, pages 1097–1105, 2012.
[8] P.
V.
Maloney, S.
Petersen, R.
A.
Navarro, D.
Marshall, A.
L.
McKendry, J.
M.
Costa, and J.
P.
Murphy.
Digital image analysis method for estimation of fusarium- damaged kernels in wheat.
Crop Science, 54(5):2077–2083, 2014.
[9] H.
Min and B.-K.
Cho.
Spectroscopic techniques for nondestructive detection of fungi and mycotoxins in agricultural materials: A review.
Journal of Biosystems Engineering, 40(1):67–77, 2015.
References [1] M.
Abadi, A.
Agarwal, P.
Barham, E.
Brevdo, Z.
Chen, C.
Citro, G.
S.
Corrado, A.
Davis, J.
Dean, M.
Devin, et al.
Tensorﬂow: Large-scale machine learning on heterogeneous distributed systems.
arXiv preprint arXiv:1603.04467, 2016.
[2] J.
G.
Barbedo, C.
S.
Tibola, and J.
M.
Fernandes.
Detecting fusarium head blight in wheat kernels using hyperspectral imaging.
Biosystems Engineering, 131:65–76, 2015.
[3] J.
G.
A.
Barbedo, C.
S.
Tibola, and M.
I.
P.
Lima.
Deoxynivalenol screening in wheat [10] J.
Schmidhuber.
Deep learning in neural networks: An overview.
Neural Networks, 61:85 – 117, 2015.
ISSN 0893-6080.
doi: https://doi.org/10.1016/j.neunet.2014.09.
003.
[11] C.
Szegedy, W.
Liu, Y.
Jia, P.
Sermanet, S.
Reed, D.
Anguelov, D.
Erhan, V.
Van- houcke, and A.
Rabinovich.
Going deeper with convolutions.
In Computer Vision and Pattern Recognition (CVPR), 2015.
[12] M.
Tk´aˇc and R.
Verner.
Artiﬁcial neural networks in business: Two decades of ISSN 1568-4946.
doi: research.
Applied Soft Computing, 38:788 – 804, 2016.
https://doi.org/10.1016/j.asoc.2015.09.040.
[13] J.
Wang, C.
Luo, H.
Huang, H.
Zhao, and S.
Wang.
Transferring pre-trained deep cnns for remote scene classiﬁcation with general features learned from linear pca network.
Remote Sensing, 9(3):225, 2017.

In this paper, we are aiming at developing an eﬀective optimization algorithm for solving the linearly constrained nonconvex nonsmooth program: min x∈X f (x) + r(x), s.t. Ax ≤ b, (1) where x = [x1, .
.
.
, xn] ∈ Rd, xi ∈ Rdi and Pn smooth but possibly nonconvex and r(x) =Pn A ∈ Rp×d, b ∈ Rp and X ⊂ Rd is a closed and convex set.
Problem (1) abstracts a plethora of mathematical models arising from deep learning [1], distributed optimization and coordination [2], network utility maximization [29], resource allocation [33], statistical learning [11] and so on.
Two typical problems widely used in practice are: 1) the constrained concave penalized linear regression (CCPLR) [19] and 2) the nonconvex support vector machine (NCSVM) [8].
i=1 di = d.
The objective function f : Rd → R is a i=1 ri(xi), where ri is nonsmooth and possibly nonconvex.
Despite its eﬀectiveness, it is very hard to ﬁnd even a stationary solution of problem (1).
The diﬃculty comes from two aspects.
From the perspective of problem structure, the nonsmoothness of the objective function prohibits the use of the gradient while the projection onto the set {x : Ax ≤ b} is intractable in general.
In another perspective of computational cost, the per-iteration cost is proportional to the full dimension and hence extremely expensive for high-dimensional data-driven applications.
Therefore, an eﬃcient algorithm with a theoretical guarantee is in demand.
However, none of the existing algorithms meet these requirements.
In this paper, we propose a novel incremental path-following splitting algorithm, denoted as IPFS, to resolve problem (1).
The key idea behind our approach is to construct a sequence of δ-smoothed problems with log-barrier functions and approximately solve each problem via a splitting method.
In speciﬁc, we introduce a slack variable s ≥ 0 to transform the inequality constraint into an equality constraint, and eliminate the non-negative constraint by using a log-barrier function.
This leads to several following δ-smoothed problem, which are much easier than problem (1), f (x) + r(x) − δ min x∈X s.t. Ax + s = b.
Pi=1 log(si), (2) In fact, we are able to show that the stationary solution to a sequence of δ-smoothed problems constitute a path which converges to one stationary solution of problem (1).
In this light, the proposed IPFS method approximately follow this path, and return an ǫ-approximate stationary solution of problem (1).
Furthermore, either cyclic or randomized variable selection rule, which are both amenable to high- dimensional optimization, is assigned to alleviate the issue of computational cost.
Finally, we present the detailed convergence and iteration complexity analysis in the appendix.
The contributions of our work are summarized as follows: • We propose to construct a sequence of the δ-smoothed problems, where a path is constituted and converges to one stationary solution of problem (1) as δ → 0.
• We develop a novel incremental path-following splitting algorithm, denoted as IPFS, together with either cyclic or randomized variable selection rule, which are both amenable to high-dimensional optimization.
• We evaluate the eﬃcacy of the proposed algorithm on the constrained concave penalized linear regression and the nonconvex support vector machine.
Experimental results demonstrate that our method consistently outperforms other competing methods.
Related works: To the best of our knowledge, our approach is the ﬁrst incremental algorithm developed for solving problem (1) with a theoretical guarantee and very low per-iteration cost.
This is achieved through optimizing a sequence of a smoothed problem with decreasing parameter via an incremental splitting method.
The following brieﬂy discusses related work in literature.
The log-barrier function has been widely used in the path-following method for linear programming [14], and then generalized to the self-concordant function [26, 31] for convex programming.
Recent years have witnessed the renewed interests of the path-following method in solving Lagrangian decomposition in separable convex optimization [6] and constrained convex minimization [5], where the idea behind is also strongly relevant to the continuation method, a standard technique in training neural network [17].
Very recently, a new continuation method proposed by Hazan et al.
[15] has been proven globally convergent for a special class of unconstrained non-convex smooth programs.
However, it remains unclear if the path-following method can be extended to solve problem (1) with a theoretical guarantee.
The splitting method [21, 7] is the standard tool for solving the linearly constrained convex programs.
Much eﬀort has been devoted to establishing the theoretical guarantee of the nonconvex splitting meth- ods.
However, the existing analysis is limited to a fraction of problems, or requires some strong as- sumptions.
The iteration complexity analysis has been established for the nonconvex smooth consensus problem and sharing problem [18], the nonconvex problems with Kurdyka-Lojasiewicz (KL) condition [32] and the symmetric nonnegative matrix factorization [24, 23].
Very recently, Jiang et al.
[20] presented a uniﬁed framework to deﬁne the ǫ-stationary solution of nonconvex nonsmooth problems, and presented the iteration complexity of the splitting method in terms of variational inequality.
Melo and Monteiro [25] analyzed the nonconvex Jacobi-type non-Euclidean splitting method with an elegant iteration complexity analysis, while Gon¸calves et al.
[12] obtained the similar complexity result for the nonconvex proximal splitting method with over-relaxation step-size.
On the other hand, Combettes and Pesquet [4] proposed block-coordinate ﬁxed-point algorithms, which achieved very low per-iteration cost by incorporating random sweeping.
However, problem (1) does not fall into the class of problems dis- cussed before.
Therefore, it still remains unclear whether problem (1) can be solved by the randomized splitting method with a theoretical guarantee.
2 Algorithm We make the following assumptions throughout the paper: Assumption 1.
The set of the stationary solutions of problem (1) is nonempty.
Assumption 2.
The set X =Qn i=1 Xi is bounded, and each Ai has full column rank.
Assumption 3.
The objective f is diﬀerentiable and each partial derivative ∇if is Lipschitz continuous.
In speciﬁc, there exists a constant Li > 0 such that, for i = 1, .
.
.
, n, k∇if (x) − ∇if (y)k2 ≤ Li kx − yk2 , ∀x, y ∈ Rd. Assumption 3 is standard and satisﬁed by many loss functions in machine learning.
For example, the least square or logistic loss, i.e., f (x) = 1 i=1 l(x, ξi) where ξi = (ai, bi) is a single data sample, and l(x, ξi) is deﬁned as: N PN or log(cid:16)1 + exp(cid:16)−bi · a⊤ i x(cid:17)(cid:17) .
2(cid:13)(cid:13)(cid:13) a⊤ i x − bi(cid:13)(cid:13)(cid:13) We proceed to the optimality of problem (1).
Deﬁnition 1.
Let h : Rd → R ∪ {+∞} be a proper lower semi-continuous function.
Suppose h(¯x) is ﬁnite for a given ¯x.
For v ∈ Rd, we say that • v is a regular sub-gradient of h at ¯x, written v ∈ ∂ˆh(¯x), if lim x6=¯x inf x→¯x h(x) − h(¯x) − hv, x − ¯xi kx − ¯xk2 ≥ 0.
• v is a general sub-gradient of h at ¯x, written v ∈ ∂h(¯x), if there exist sequences {xk} and {vk} such that xk → ¯x with h(xk) → h(¯x), and vk ∈ ∂ˆh(xk) with vk → v when k → +∞.
The following proposition lists some facts about the semi-continuous functions.
Proposition 4.
Let h : Rd → R ∪ {+∞} and g : Rd → R ∪ {+∞} be proper lower semi-continuous functions.
Then it holds that: 1.
Fermats rule remains true: if ¯x is a local minimum of h, then 0 ∈ ∂h(¯x).
2.
If h is continuously diﬀerentiable at x, then ∂(h + g)(x) = ∂h(x) + ∂g(x).
3.
If h is locally Lipschitz continuous at x, then ∂(h + g)(x) ⊂ ∂h(x) + ∂g(x).
4.
Suppose h(x) is locally Lipschitz continuous, X is a closed and convex set, and ¯x is a local minimum of h on X .
Then there exists v ∈ ∂h(¯x) such that (x − ¯x)⊤v ≥ 0,∀x ∈ X .
Assumption 5.
The set of generalized gradient of ri, denoted as ∂ri, is assumed to be bounded.
In addition, the proximal mapping of each ri, deﬁned as: proxαri(x) = argmin (cid:20)ri(y) + 2(cid:21) 2α ky − xk2 is easily obtained.
Remark 6.
Assumption 5 is standard and satisﬁed by ℓ1-norm and the smoothly clipped absolute devia- tion (SCAD) [9], and also reasonable since the solution to ri(x)+ 1 2 is unique for α > 0 suﬃciently small.
2α kxk2 We are ready to introduce the notion of an ǫ-stationary solution of problem (1).
After introducing the slack variable s ≥ 0, the Lagrangian function is deﬁned as: L (x, s, λ) = f (x) + r(x) + hλ, Ax + s − bi .
Based on the ﬁrst-order optimality condition, we deﬁne an ǫ-stationary solution of problem (1) as follows: Deﬁnition 2.
We call x∗ ∈ Rd to be an ǫ-stationary solution of problem (1) if there exists s∗ ≥ 0, and λ∗ ∈ Rp such that the following holds true, dist(cid:16)−∇if (x∗) − A⊤ i λ∗, ∂ri(x∗ i )(cid:17) ≤ ǫ, i = 1, .
.
.
, n, j(cid:1) λ∗ j ≥ −ǫ, j = 1, .
.
.
, p, (cid:0)sj − s∗ kAx∗ + s∗ − bk2 ≤ ǫ, where Ai is the i-th column of A, s ≥ 0 and dist(x,H) is the standard Euclidean distance between x and a closed convex set H.
The solution x∗ is a stationary solution of problem (1) if ǫ = 0 holds true.
Remark 7.
Given a stationary solution of problem (1), it has been recognized as a signiﬁcant reference for several ﬁrst-order methods to obtain the iterates, which converge to this solution.
Furthermore, the stationary solution attained by the proposed algorithm could be a local minimizer under some conditions, such as second-order suﬃcient condition [27].
We propose an incremental path-following splitting method, denoted as IPFS, and analyze its iteration complexity with cyclic or randomized variable selection rule.
The proposed algorithm is presented in Algorithm 1.
The algorithm is double-looped.
In each iteration of the outer loop, we consider a δ-smoothed problem, i.e., problem (2), where δ > 0 is a smoothing parameter.
We optimize this problem via a splitting method with a variable selection rule, and obtain an ǫδ-stationary solution to problem (2).
Then we move to the next iteration of the outer loop, and decrease δ to γδ, where 0 < γ < 1.
Each inner loop is devoted to optimize problem (2) via the splitting method.
In each iteration, we select a set of index I ⊆ {1, 2, .
.
.
, n}, and update {xi}i∈I based on the last iterate, i.e., xk.
In speciﬁc, we introduce β > 0 and deﬁne a function as: Lk(cid:16){xi}i∈I , xk, s, λ(cid:17) = Xi∈I (cid:20)D∇if (xk), xi − xk (cid:13)(cid:13)(cid:13) iE + i + s − b+ + Aixi +Xi /∈I + ri(xi)(cid:21) − δ Aixi +Xi /∈I +*λ,Xi∈I Li + 1 Aixk (3) Given s = sk and λ = λk, this function is strongly convex for {xi}i∈I according to the boundedness of ∂ri.
To this end, we obtain {xk+1 {xk+1 }i∈I by following: }i∈I = argmin (4) Aixk xi − xk i(cid:13)(cid:13)(cid:13) 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) Xi∈I {xi∈Xi}i∈I Lk(cid:16){xi}i∈I , xk, sk, λk(cid:17) .
log(si) Xi=1 i + s − b(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) Algorithm 1 Incremental Path-Following Splitting Method (IPFS) Initialize: the primal variable ¯x ∈ Rd; the slack variable ¯s ≥ 0; the dual variable ¯λ ∈ Rp; the smoothing parameter δ > 0.
Set: the ratio γ ∈ (0, 1); the ﬁnal tolerance ǫ > 0.
while δ > ǫ do Set k ← 0.
Set β > 0 according to δ.
Set (cid:0)x0, s0, λ0(cid:1) ←(cid:0)¯x, ¯s, ¯λ(cid:1).
while the stopping criterion is not satisﬁed do 1.
Pick up a set of index, i.e., I, according to a variable selection rule.
Update {xk+1 }i∈I via Eq. (4).
Update {xk+1 }i /∈I ← {xk i }i /∈I.
Update sk+1 via Eq. (5).
2.
3.
4.
5.
Update λk+1 via Eq. (6).
Update k ← k + 1.
6.
end while δ ← γδ.
(cid:0)¯x, ¯s, ¯λ(cid:1) ←(cid:0)xk+1, sk+1, λk+1(cid:1).
end while }i /∈I = {xk s∈Rp "−δ Furthermore, we set {xk+1 sk+1 = argmin and i }i /∈I .
Finally, we obtain sk+1 and λk+1 by following: log(si) + Xi=1 2 (cid:13)(cid:13)(cid:13)(cid:13) s + Axk+1 − b + λk+1 = λk + β(cid:16)Axk+1 + sk+1 − b(cid:17) .
2# , λk(cid:13)(cid:13)(cid:13)(cid:13) (5) (6) Remark 8.
We assume that problem (4) can be solved exactly in theoretical analysis for convenience since it has been shown in [13] that this subproblem admits a closed-form solution for a few special nonconvex regularization function ri, e.g., SCAD [9] if Ai is an identity matrix and Xi = Rdi.
However, the closed-form solution is generally intractable.
As an alternative, some iterative methods can be used to approximately solve it.
We refer the interested readers to [13] for more details.
In what follows, we discuss the variable selection rules: cyclic and randomized.
1.
cyclic rule.
Let I be the set of index selected at the k-th iteration of one inner loop, we have where k mod n is the remainder for k divided by n.
I = {ik = k mod n}, 2.
randomized rule.
At each k-th iteration, the index i ∈ I is selected at random with probability pi > 0, i.e., Prob (i ∈ I) = pi ≥ pmin > 0.
Finally, we design a reasonable procedure to identify the tolerance ǫδ > 0.
This procedure, also known as stopping criterion, is crucial for the performance of the proposed algorithm.
In speciﬁc, it is unnecessary to obtain very accurate local solution when δ is large, which might take a lot of iterations.
In what follows, we clarify the connection between ǫδ used in the stopping criterion and δ.
Stopping Criterion: We repeat the inner loop of the splitting method until the following statements hold true, where C > 0 is set as a constant which is independent of δ.
(cid:13)(cid:13)(cid:13) (cid:13)(cid:13)(cid:13) xk+1 − xk(cid:13)(cid:13)(cid:13)2 ≤ Cδ, Axk+1 + sk+1 − b(cid:13)(cid:13)(cid:13)2 ≤ Cδ, 2.1 Discussion Firstly, we compare our method to some existing methods.
1.
Our method is greatly diﬀerent from stochastic alternating direction method (SADM) [28].
SADM randomly draw a subset of data samples at each iteration while our method randomly selects a subset of decision variables.
In addition, the theoretical guarantee of SADM is established only when the objective is convex.
2.
Our method is related to the algorithm presented in [18].
However, the theoretical guarantee of that algorithm is only established only when applied to solve the nonconvex smooth consensus and sharing problems.
3.
Our method is related to the two variants of ADMM analyzed in [20], i.e., proximal ADMM-m and proximal ADMM-g.
However, neither of these methods has a theoretical guarantee when applied to solve problem (1).
In speciﬁc, problem (1) can be reformulated as min x∈X f (x) + r(x), s.t. Ax + s = b, s ≥ 0.
The objective is nonsmooth with respect to x and s, which violates the conditions in [20].
In addition, the computational cost of proximal ADMM-m and proximal ADMM-g is very expensive on high-dimensional problems.
Secondly, we remark that the parameter setting varies from practical usage to theoretical analysis.
For example, we prove in the next section that the iterates converge to an ǫ-stationary solution if β remains as a suﬃciently large constant.
However, β should be adapted to accelerate the method and improve the practical performance.
3 Main Result In this section, we present the convergence and iteration complexity analysis of the proposed algorithm with cyclic or randomized variable selection rule.
3.1 Convergence We deﬁne a set of stationary solutions of problem (2), and prove that (¯x, ¯s) converges to one of the stationary solutions.
Deﬁnition 3.
We call (cid:0)xδ,∗, sδ,∗(cid:1) ∈ Rd× Rp to be an ǫδ-stationary solution of problem (2) if there exists λδ,∗ ∈ Rp such that the following statement holds true, sδ,∗ j λδ,∗ i λδ,∗, ∂ri(xδ,∗ dist(cid:16)−∇if (xδ,∗) − A⊤ (cid:13)(cid:13)(cid:13) (cid:13)(cid:13)(cid:13) Axδ,∗ + sδ,∗ − b(cid:13)(cid:13)(cid:13)2 ≤ ǫδ.
i )(cid:17) ≤ ǫδ, i = 1, .
.
.
, n, j − δ(cid:13)(cid:13)(cid:13)2 ≤ ǫδ, j = 1, .
.
.
, p, The solution (cid:0)xδ,∗, sδ,∗(cid:1) is a stationary solution of problem (2) if ǫδ = 0 holds true.
At a high-level, for any given δ > 0, (¯x, ¯s) can approximate(cid:0)xδ,∗, sδ,∗(cid:1) well as the inner loop goes.
Since (cid:0)xδ,∗(cid:1) converges to x∗ as δ → 0, we can characterize the limiting behavior of our algorithm.
Theorem 9.
As δ → 0, we show that ¯x → x∗ deterministically or in terms of expectation for cyclic and randomized variable selection rules, where x∗ is a stationary solution of problem (1).
3.2 Iteration Complexity In this subsection, we analyze the iteration complexity of the proposed algorithm.
Speciﬁcally, we use the measure of optimality in terms of variational inequality, instead of the closeness to the optimal solution, which is intractable in nonconvex optimization.
The ǫ-stationary solution of problem (1) is regarded reached if the following statements both hold true, 1.
The smoothing parameter is smaller than the ﬁnal tolerance, i.e., δ < ǫ.
2.
The stopping criterion in each inner loop is satisﬁed deterministically or in terms of expectation for cyclic and randomized variable selection rules, respectively.
Now we are ready to state our main result on the iteration complexity.
Speciﬁcally, we show that the iteration number in inner loop, and O(log( 1 proposed algorithm returns ǫ-stationary solution of problem (1) within O(cid:0) 1 ǫ )) iterations in term of the iteration number in outer loop.
Theorem 10.
Suppose either cyclic or randomized variable selection rule is employed, the proposed IPFS algorithm returns an ǫ-stationary solution of problem (1) deterministically or in terms of expectation ǫ(cid:1) iterations in term of the for cyclic and randomized variable selection rules, respectively, within O(cid:0) 1 iteration number in inner loop, and O(log( 1 ǫ(cid:1) iterations in term of the ǫ )) iterations in term of the iteration number in outer loop.
Table 1: Statistics of datasets.
Dataset Number of Samples Dimension ohscal classic 20news mnist a9a w8a SUSY 11,162 7,094 16,242 16,242 32,561 64,700 5,000,000 11,465 41,681 100 100 123 300 19 Remark 11.
We highlight the iteration complexity in order of O(cid:0) 1 methods when applied to solve several non-convex problems.
[3, 22].
In practice, a variety of large-scale artiﬁcial intelligence and machine learning applications requires the solution with low accuracy, i.e., ǫ ≈ 10−3.
ǫ(cid:1) is theoretical optimal for ﬁrst-order 4 Experiments In this section, we evaluate the eﬃcacy of our algorithm on the linearly constrained nonconvex problem arising from machine learning.
We compare our method with two well-known heuristic algorithms since the existing exact convergence method for problem (1) is unknown.
More speciﬁcally, we consider the inexact augmented Lagrangian method (InexactALM) [16] and the proximal alternating direction method of multipliers (proximal ADMM-m) [20], which are referred to “heuristic” since the theoretical guarantees of these two methods are only valid on convex minimization and a class of nonconvex minimization.
We conduct our experiments on the synthetic data in the ﬁrst task, named Constrained Concave Penal- ized Linear Regression [10], and on the real data in the second task, named Nonconvex Support Vector Machine [30].
The objective value is used as the metric in our experiments.
4.1 Constrained Concave Penalized Linear Regression Problem: The problem of Constrained Concave Penalized Linear Regression (CCPLR) has been rec- ognized as one linearly constrained nonconvex nonsmooth program, which covers a few interesting applications in statistical learning and image processing.
In speciﬁc, it is aiming at recovering a sparse signal x∗ ∈ Rd with s ≪ d non-zero components from the observation y ∈ Rn and b ∈ Rm, which are deﬁned as y = Ax∗ + ǫ1 and b = Cx∗ + ǫ2.
Here A ∈ Rn×d and C ∈ Rm×d are measurement matrices, and ǫ1 ∈ Rm and ǫ2 ∈ Rn are white noises.
Mathematically, it is deﬁned as: min x∈Rd 2kAx − yk2 2 + Pλ(x), s.t. Cx − b ≤ 0, (7) where Pλ(w) is deﬁned as: Pλ(w) = λkwk1 − Xi=1(cid:20) w2 i − 2wi + λ2 2(θ − 1) I(λ < wi ≤ θλ) + (λwi − (θ + 1)λ2 )I(wi > θλ)(cid:21) .
Settings: We generate A ∈ Rn×d with independent standard Gaussian entries and normalized it in column; We generate C ∈ Rm×d with independent standard Gaussian entries; ǫ1 ∼ N (0, σ2Id), and ǫ2 contains independent random entries uniformly distributed in [0, σ], where σ = 0.1 or σ = 0.3. We select diﬀerent regularization parameters in {1.4, 1.6, 1.8, 2.0} to show that our algorithms are robust.
We implement with the random initialization, and the terminate that the relative change of the consecutive objective function values is less than 10−7.
Experimental results: Figure 1 shows the objective value as a function of time cost (in seconds) where σ = 0.1 and σ = 0.3. We observe that IPFS-Cyclic and IPFS-Randomized consistently outperform InexactALM and proximal ADMM-m on all datasets, especially when the dimension is high.
This con- ﬁrms the advantage of our algorithms over InexactALM and proximal ADMM-m is the solid theoretical guarantee.
Furthermore, IPFS-Randomized performs the best mainly because of its low per-iteration cost, strongly supporting the usage of randomized algorithms.
4.2 Nonconvex Support Vector Machine Problem: The problem of NonConvex Support Vector Machine (NCSVM) is a very powerful binary classiﬁcation tool with high accuracy and great ﬂexibility.
Mathematically, it is deﬁned as: min x∈Rd α1⊤ξ + Pλ(x), s.t. 1 − ξ − b · A⊤x ≤ 0, ξ ≤ 0, (8) where 1 ∈ Rn, and Pλ(w) is deﬁned as before.
Settings: We set σ = 1.4, and vary the regularization parameter λ in {1.4, 1.6, 1.8, 2.0}.
We use seven datasets123 to evaluate the proposed algorithm, where the statistics is presented in Table 1.
The remaining setting is the same as that used in the CCPLR problem.
Experimental results: Figure 2 shows the objective value as a funtion of time cost (in seconds).
In- deed, our algorithms outperform InexactALM and proximal ADMM-m consistently, while IPFS-Randomized performs the best by a signiﬁcantly large margin on high-dimensional real data.
5 Conclusions In this paper, we proposed a novel incremental path-following splitting algorithm, denoted as IPFS, to solve the linearly constrained nonconvex nonsmooth program, which abstracts quite a few machine learning applications.
To the best of our knowledge, this is the ﬁrst incremental method developed for 1http://www.csie.ntu.edu.tw/cjlin/libsvmtools/datasets/ 2https://www.shi-zhong.com/software/docdata.zip 3www.cs.nyu.edu/roweis/data.html 10 Table 2: Performance comparison of the referred algorithms.
Methods IPFS-R IPFS-C IALM proximal ADMM-m IPFS-R IPFS-C IALM proximal ADMM-m IPFS-R IPFS-C IALM proximal ADMM-m IPFS-R IPFS-C IALM proximal ADMM-m λ = 1.4 (objective,time) (6265.58, 23.82) (6323.42, 40.09) (6345.11, 13.18) (6381.79, 4.68) λ = 1.6 (objective,time) (7164.32, 23.26) (7226.43, 41.31) (7251.31, 13.30) (7282.70, 4.64) λ = 1.8 (objective,time) (8066.46, 23.32) (8129.12, 41.15) (8157.26, 13.26) (8176.47, 4.31) λ = 2.0 (objective,time) (8932.08, 23.16) (9031.59, 40.81) (9063.15, 12.65) (9053.72, 4.63) (6296.56, 20.28) (7167.99, 20.20) (8051.38, 20.49) (8961.63, 20.30) (6367.52, 43.83) (6388.92, 12.23) (6394.51, 4.73) (487.81, 24.29) (331.24, 44.99) (1791.67, 95.13) (965.95, 214.00) (527.80, 23.94) (550.85, 47.30) (907.07, 276.96) (1183.02, 152.83) (7276.89, 42.36) (7301.49, 12.38) (7288.24, 4.49) (10.54, 24.22) (8.13, 44.53) (1685.02, 320.12) (1100.06, 207.95) (32.32, 24.85) (9.20, 46.87) (11.24, 958.18) (8184.76, 43.74) (8213.84, 12.13) (8163.68, 4.80) (9.18, 23.97) (10.13, 44.85) (12.13, 233.88) (1230.22, 244.28) (10.131, 23.90) (10.17, 45.72) (15.21, 230.16) (1350.04, 184.68) (1513.66, 218.79) (9093.21, 43.51) (9125.88, 13.32) (9017.17, 4.67) (10.10, 23.99) (12.13, 44.76) (10.13, 106.33) (1359.44, 246.96) (11.37, 22.47) (12.19, 45.99) (18.21, 126.17) (1676.46, 264.80) (n, d) λ = 1.4 λ = 1.6 λ = 1.8 λ = 2.0 7000 6900 6800 6700 6600 6500 6400 6300 6200 7000 6900 6800 6700 6600 6500 6400 6300 6200 1600 1400 1200 1000 800 600 1600 1400 1200 1000 800 600 400 IPFS-C IPFS-R InexactALM ADMM IPFS-C IPFS-R InexactALM ADMM 8000 7800 7600 7400 7200 7000 IPFS-C IPFS-R InexactALM ADMM 9000 8800 8600 8400 8200 8000 IPFS-C IPFS-R InexactALM ADMM 10000 9800 9600 9400 9200 9000 8800 10 12 14 16 10 12 14 16 10 12 14 10 12 14 Time Cost Time Cost Time Cost Time Cost IPFS-C IPFS-R InexactALM ADMM IPFS-C IPFS-R InexactALM ADMM 8000 7800 7600 7400 7200 7000 IPFS-C IPFS-R InexactALM ADMM 9000 8800 8600 8400 8200 8000 IPFS-C IPFS-R InexactALM ADMM 10000 9800 9600 9400 9200 9000 8800 10 12 14 16 10 12 14 16 10 12 14 10 12 14 16 Time Cost Time Cost Time Cost Time Cost IPFS-C IPFS-R InexactALM ADMM 10 15 Time Cost IPFS-C IPFS-R InexactALM ADMM IPFS-C IPFS-R InexactALM ADMM 10 15 Time Cost IPFS-C IPFS-R InexactALM ADMM 2000 1800 1600 1400 1200 1000 800 2000 1800 1600 1400 1200 1000 800 600 400 200 2200 2000 1800 1600 1400 1200 1000 800 2200 2000 1800 1600 1400 1200 1000 800 600 400 IPFS-C IPFS-R InexactALM ADMM IPFS-C IPFS-R InexactALM ADMM 2400 2200 2000 1800 1600 1400 1200 1000 800 10 15 20 10 12 14 Time Cost Time Cost IPFS-C IPFS-R InexactALM ADMM IPFS-C IPFS-R InexactALM ADMM 2400 2200 2000 1800 1600 1400 1200 1000 800 600 10 15 20 Time Cost 10 15 20 Time Cost 10 15 Time Cost 10 12 14 16 Time Cost Figure 1: Comparison of IPFS-C (Cyclic variable selection rule) and IPFS-R (Randomized variable selection rule) with InexactALM and ADMM (proximal ADMM-m) on Constrained Concave Penalized Linear Regression.
11 Datasets λ = 1.4 λ = 1.6 λ = 1.8 λ = 2.0 ×105 ×105 IPFS-C IPFS-R ADMM InexactALM 10 15 Time Cost ×105 IPFS-C IPFS-R ADMM InexactALM 2.5 1.5 0.5 3.5 2.5 1.5 0.5 2.5 1.5 0.5 4.5 3.5 2.5 1.5 0.5 ×105 IPFS-C IPFS-R ADMM InexactALM IPFS-C IPFS-R ADMM InexactALM ×105 3.5 2.5 1.5 0.5 10 15 Time Cost 10 12 14 16 Time Cost IPFS-C IPFS-R ADMM InexactALM IPFS-C IPFS-R ADMM InexactALM ×105 IPFS-C IPFS-R ADMM InexactALM 10 15 Time Cost IPFS-C IPFS-R ADMM InexactALM ×105 3.5 2.5 1.5 0.5 ×105 10 20 30 40 50 10 20 30 40 50 10 20 30 40 50 60 10 20 30 40 50 60 Time Cost Time Cost Time Cost 105 12 10 IPFS-C IPFS-R ADMM InexactALM 105 12 10 IPFS-C IPFS-R ADMM InexactALM 200 400 600 800 Time Cost ×104 IPFS-C IPFS-R ADMM InexactALM 100 200 300 400 500 600 700 800 Time Cost ×104 IPFS-C IPFS-R ADMM InexactALM 105 14 12 10 IPFS-C IPFS-R ADMM InexactALM 100 200 300 400 500 600 700 Time Cost ×104 IPFS-C IPFS-R ADMM InexactALM Time Cost 105 IPFS-C IPFS-R ADMM InexactALM 100 200 300 400 500 600 Time Cost 18 16 14 12 10 IPFS-C IPFS-R ADMM InexactALM ×104 10 10 15 20 25 30 35 10 15 20 25 30 10 15 20 25 30 35 40 10 15 20 25 30 Time Cost Time Cost Time Cost Time Cost IPFS-C IPFS-R ADMM InexactALM IPFS-C IPFS-R ADMM InexactALM ×104 10 IPFS-C IPFS-R ADMM InexactALM ×104 10 10 15 20 25 10 15 20 25 10 15 20 25 Time Cost Time Cost Time Cost ×104 10 ×104 10 IPFS-C IPFS-R ADMM InexactALM 10 15 20 25 Time Cost ×106 IPFS-C IPFS-R ADMM InexactALM 100 200 300 400 500 600 Time Cost 700 800 900 ×106 IPFS-C IPFS-R ADMM InexactALM 100 200 300 400 500 600 Time Cost 700 800 900 ×104 2.5 1.5 0.5 IPFS-C IPFS-R ADMM InexactALM 10 15 Time Cost ×104 2.5 1.5 0.5 IPFS-C IPFS-R ADMM InexactALM 10 15 Time Cost ×106 ×104 2.5 1.5 0.5 IPFS-C IPFS-R ADMM InexactALM 200 400 600 Time Cost 800 IPFS-C IPFS-R ADMM InexactALM 10 15 Time Cost ×106 8.5 7.5 6.5 5.5 ×104 2.5 1.5 0.5 IPFS-C IPFS-R ADMM InexactALM 200 400 600 800 Time Cost IPFS-C IPFS-R ADMM InexactALM 10 15 Time Cost Figure 2: Comparison of IPFS-C (Cyclic variable selection rule) and IPFS-R (Randomized variable selection rule) with InexactALM and ADMM (proximal ADMM-m) on Nonconvex Support Vector Ma- chine.
12 solving problem (1) with a theoretical guarantee.
Furthermore, the cyclic and randomized block variable selection rules signiﬁcantly improve the eﬃciency of the proposed algorithm on high-dimensional data, as conﬁrmed by our experiments on nonconvex penalized linear regression and support vector machine tasks.
References [1] Y.
Bengio.
Learning deep architectures for ai.
Foundations and trends R(cid:13) in Machine Learning, 2(1):1–127, 2009.
[2] D.
P.
Bertsekas and J.
N.
Tsitsiklis.
Parallel and Distributed Computation: Numerical Methods, volume 23.
Prentice hall Englewood Cliﬀs, NJ, 1989.
[3] C.
Cartis, N.
I.
M.
Gould, and P.
L.
Toint.
On the complexity of steepest descent, newton’s and regularized newton’s methods for nonconvex unconstrained optimization problems.
SIAM journal on optimization, 20(6):2833–2852, 2010.
[4] P.
L.
Combettes and J-C.
Pesquet.
Stochastic quasi-fej´er block-coordinate ﬁxed point iterations with random sweeping.
SIAM Journal on Optimization, 25(2):1221–1248, 2015.
[5] Q.
T.
Dinh, A.
Kyrillidis, and V.
Cevher.
An inexact proximal path-following algorithm for con- strained convex minimization.
SIAM Journal on Optimization, 24(4):1718–1745, 2014.
[6] Q.
T.
Dinh, I.
Necoara, C.
Savorgnan, and M.
Diehl.
An inexact perturbed path-following method for lagrangian decomposition in large-scale separable convex optimization.
SIAM Journal on Op- timization, 23(1):95–125, 2013.
[7] J.
Eckstein and D.
P.
Bertsekas.
On the douglas-rachford splitting method and the proximal point algorithm for maximal monotone operators.
Mathematical Programming, 55(1):293–318, 1992.
[8] S.
Ertekin, L.
Bottou, and C.
L.
Giles.
Nonconvex online support vector machines.
IEEE Trans- actions on Pattern Analysis and Machine Intelligence, 33(2):368–381, 2011.
[9] J.
Fan and R.
Li. Variable selection via nonconcave penalized likelihood and its oracle properties.
Journal of the American statistical Association, 96(456):1348–1360, 2001.
[10] E.
X.
Fang, B.
He, H.
Liu, and X.
Yuan.
Generalized alternating direction method of multipliers: New theoretical insights and applications.
Mathematical Programming Computation, 7(2):149–187, 2015.
[11] J.
Friedman, T.
Hastie, and R.
Tibshirani.
The Elements of Statistical Learning, volume 1.
Springer series in statistics Springer, Berlin, 2001.
[12] M.
L.
N.
Gon¸calves, J.
G.
Melo, and R.
D.
C.
Monteiro.
Convergence rate bounds for a proximal admm with over-relaxation stepsize parameter for solving nonconvex linearly constrained problems.
ArXiv Preprint: 1702.01850, 2017.
[13] P.
Gong, C.
Zhang, Z.
Lu, J.
Huang, and J.
Ye. A general iterative shrinkage and thresholding algorithm for non-convex regularized optimization problems.
In ICML, pages 37–45, 2013.
13 [14] C.
C.
Gonzaga.
Path-following methods for linear programming.
SIAM review, 34(2):167–224, 1992.
[15] E.
Hazan, K.
Y.
Levy, and S.
Shalev-Shwartz.
On graduated optimization for stochastic non-convex problems.
In ICML, pages 1833–1841, 2016.
[16] B.
He, H.-K.
Xu, and X.
Yuan.
On the proximal jacobian decomposition of alm for multiple- block separable convex minimization problems and its relationship to admm.
Journal of Scientiﬁc Computing, 66(3):1204–1217, 2016.
[17] G.
E.
Hinton, S.
Osindero, and Y-W.
Teh.
A fast learning algorithm for deep belief nets.
Neural Computation, 18(7):1527–1554, 2006.
[18] M.
Hong, Z-Q.
Luo, and M.
Razaviyayn.
Convergence analysis of alternating direction method of multipliers for a family of nonconvex problems.
SIAM Journal on Optimization, 26(1):337–364, 2016.
[19] G.
M.
James, C.
Paulson, and P.
Rusmevichientong.
The constrained lasso.
Technical report, University of Southern California, 2012.
[20] B.
Jiang, T.
Lin, S.
Ma, and S.
Zhang.
Structured nonconvex and nonsmooth optimization: Algo- rithms and iteration complexity analysis.
ArXiv Preprint 1605.02408, 2016.
[21] P-L.
Lions and B.
Mercier.
Splitting algorithms for the sum of two nonlinear operators.
SIAM Journal on Numerical Analysis, 16(6):964–979, 1979.
[22] D.
C.
Liu and J.
Nocedal.
On the limited memory bfgs method for large scale optimization.
Mathematical programming, 45(1-3):503–528, 1989.
[23] S.
Lu, M.
Hong, and Z.
Wang.
A nonconvex splitting method for symmetric nonnegative matrix factorization: Convergence analysis and optimality.
IEEE Transactions on Signal Processing, 2017.
[24] S.
Lu, M.
Hong, and Z.
Wang.
A stochastic nonconvex splitting method for symmetric nonnegative matrix factorization.
In AISTATS, pages 812–821, 2017.
[25] J.
G.
Melo and R.
D.
C.
Monteiro.
Iteration-complexity of a jacobi-type non-euclidean admm for multi-block linearly constrained nonconvex programs.
ArXiv Preprint: 1705.07229, 2017.
[26] Y.
Nesterov and A.
Nemirovskii.
Interior-point polynomial algorithms in convex programming.
SIAM, 1994.
[27] J.
Nocedal and S.
Wright.
Numerical Optimization.
Springer Science & Business Media, 2006.
[28] H.
Ouyang, N.
He, L.
Tran, and A.
Gray.
Stochastic alternating direction method of multipliers.
In ICML, pages 80–88, 2013.
[29] D.
P.
Palomar and M.
Chiang.
A tutorial on decomposition methods for network utility maximiza- tion.
Selected Areas in Communications, IEEE Journal on, 24(8):1439–1451, 2006.
[30] B.
Peng.
Methodologies and Algorithms on Some Non-convex Penalized Models for Ultra High Dimensional Data.
Thesis, University of Minnesota, 2016.
14 [31] J.
Renegar.
A mathematical view of interior-point methods in convex optimization.
SIAM, 2001.
[32] Y.
Wang, W.
Yin, and J.
Zeng.
Global convergence of admm in nonconvex nonsmooth optimization.
ArXiv Preprint 1511.06324, 2015.
[33] L.
Xiao, M.
Johansson, and S.
P.
Boyd.
Simultaneous routing and resource allocation via dual decomposition.
Communications, IEEE Transactions on, 52(7):1136–1144, 2004.
A Proof of Theorem 9 We ﬁrstly construct a potential function Φ(x, s, λ) and present a key technical lemma which guarantees k=0 is non- increasing and lower bounded under some conditions of the penalty parameter β > 0.
The potential function Φ(x, s, λ) is deﬁned as that, when optimizing δ-smoothed version, i.e., problem (2), the sequence of (cid:8)Φ(xk, sk, λk)(cid:9)+∞ Φ(x, s, λ) = f (x) + r(x) − δ Xj=1 log(sj) + hλ, Ax + s − bi + 2 kAx + s − bk2 2 , where δ > 0 is a given smoothing parameter, and β > 0 is a penalty parameter chosen according to δ.
A.1 Proof of Technical Lemmas Lemma 12.
When optimizing the δ-smoothed version, there exists s > 0.
If the following statement holds true, √2δ s2 , β > k=0 is non-increasing and lower bounded.
Therefore, we conclude that then a sequence of (cid:8)Φ(xk, sk, λk)(cid:9)+∞ Φ(xk, sk, λk) → Φ∗, and (cid:13)(cid:13)Axk+1 + sk+1 − b(cid:13)(cid:13)2 → 0, (cid:13)(cid:13)xk+1 − xk(cid:13)(cid:13)2 → 0 and (cid:13)(cid:13)sk+1 − sk(cid:13)(cid:13)2 → 0 as k → ∞, deterministically and almost surely for cyclic and randomized variable selection rules, respectively.
Furthermore, a sequence of (cid:8)(cid:0)xk, sk, λk(cid:1)(cid:9)+∞ Proof.
It follows from the update of {xk+1 k=0 remains bounded.
}i∈I , {xk+1 }i /∈I that (9) (10) Φ(xk, sk, λk) − Φ(xk+1, sk, λk) ≥ Furthermore, Φ(xk+1, s, λk) is strongly convex with respect to s, and hence by using the update of sk+1, we have Φ(xk+1, sk, λk) − Φ(xk+1, sk+1, λk) ≥ Finally, by using the update of λk+1, we have Φ(xk+1, sk+1, λk) − Φ(xk+1, sk+1, λk+1) = − (11) xk i − xk+1 2Xi∈I (cid:13)(cid:13)(cid:13) (cid:13)(cid:13)(cid:13) 2 (cid:13)(cid:13)(cid:13) sk − sk+1(cid:13)(cid:13)(cid:13) β (cid:13)(cid:13)(cid:13) λk − λk+1(cid:13)(cid:13)(cid:13) 15 In what follows, we try to show that there exists s > 0 such that sk that {sk}+∞ sk+1 j λk+1 exists ¯λ > 0 such that λk j ≥ s for i = 1, 2, .
.
.
, p.
It is clear j=1 log(sj) is strictly convex.
Firstly, we obtain that = δ through combining the update of sk+1 and λk+1.
Then we suﬃce to show that there k=1 is a bounded sequence since −Pp j ≤ ¯λ.
In speciﬁc, we have ∂ri(xk+1 1, .
.
.
, xk N ) − A⊤ = −∇if (xk ) ∋ −∇if (xk i  i(cid:17) − βA⊤ Xj≤i i (cid:16)sk+1 − sk(cid:17) i(cid:17) − βA⊤ N ) − A⊤ For i = 1, 2, .
.
.
, n, we obtain that A⊤ k=1 is a bounded sequence, Xi and the set of generalized gradient of ri are both bounded sets.
Without loss of generality, A = [A1 A2 .
.
.
AN ] j ≤ ¯λ for some ¯λ > 0.
Therefore, we have is assumed to be full row rank, which leads to the fact that λk (12) i λk − (Li + 1)(cid:16)xk+1 i − xk i λk+1 − (Li + 1)(cid:16)xk+1 i − xk i λk is bounded since {sk}+∞ j +Xj>i 1, .
.
.
, xk Ajxk+1 Ajxk j + sk − b Finally, we combine (10), (11) and (12) to obtain that (cid:13)(cid:13)(cid:13) λk − λk+1(cid:13)(cid:13)(cid:13)2 =(cid:13)(cid:13)(cid:13)(cid:13) sk − sk+1(cid:13)(cid:13)(cid:13)(cid:13)2 ≤ 2Xi∈I (cid:13)(cid:13)(cid:13) s2 (cid:13)(cid:13)(cid:13) sk − sk+1(cid:13)(cid:13)(cid:13)2 +(cid:18) β (cid:13)(cid:13)(cid:13) 2 − Φ(xk, sk, λk) − Φ(xk+1, sk+1, λk+1) ≥ i − xk+1 xk µ2 βs4(cid:19)(cid:13)(cid:13)(cid:13) sk − sk+1(cid:13)(cid:13)(cid:13) (13) Furthermore, the function Φ(cid:0)xk, sk, λk(cid:1) has been shown lower bounded in [20], i.e., there exists Φ∗ ∈ R such that Φ(xk, sk, λk) ≥ Φ∗.
For cyclic variable selection rule, it holds deterministically that, (cid:13)(cid:13)(cid:13) xk − xk+1(cid:13)(cid:13)(cid:13)2 =sXi∈I (cid:13)(cid:13)(cid:13) (cid:13)(cid:13)(cid:13) i − xk+1 xk 2 → 0, as k → +∞.
For randomized variable selection rule, we take the conditional expectation over both sides of (13) and obtain that E"Xi∈I (cid:13)(cid:13)(cid:13) Xi=1(cid:20)(cid:13)(cid:13)(cid:13) i − xk+1 xk 2# .
(cid:13)(cid:13)(cid:13) 2(cid:21) ≥ 0, (cid:13)(cid:13)(cid:13) Φ(xk, sk, λk) − EhΦ(xk+1, sk+1, λk+1) | (xk, sk, λk)i ≥ Since pi ≥ pmin for i = 1, 2, .
.
.
, N , we have Φ(xk, sk, λk) − EhΦ(xk+1, sk+1, λk+1) | (xk, sk, λk)i ≥ pmin i − ˜xk+1 xk that, k=0 is a super-martingale with respect to the natural history; and by the super-martingale convergence k=0 converges almost surely.
Therefore, it also surely holds true where ˜xk+1 is a “virtual” iterate assuming that all variables are updated once.
Thus(cid:8)Φ(xk, sk, λk)(cid:9)+∞ theorem, the sequence (cid:8)Φ(xk, sk, λk)(cid:9)+∞ Similarly, we obtain that (cid:13)(cid:13)sk+1 − sk(cid:13)(cid:13)2 → 0 and (cid:13)(cid:13)Axk+1 + sk+1 − b(cid:13)(cid:13)2 = 1 +∞ deterministically and almost surely for cyclic and randomized variable selection rules, respectively.
This completes the proof.
(cid:13)(cid:13)(cid:13) xk − xk+1(cid:13)(cid:13)(cid:13)2 → 0, as k → +∞.
β (cid:13)(cid:13)λk − λk+1(cid:13)(cid:13)2 → 0 as k → 16 From Lemma 12, it is easy to see that the stopping criterion must be satisﬁed.
Then we present a lemma which guarantees that, ¯x ∈ Rd approaches one local solution to problem (2), where the Lagrangian function is deﬁned as Lδ (x, s, λ) = f (x) + r(x) − δ Xi=1 log(si) + hλ, Ax + s − bi .
Lemma 13.
Let (¯x, ¯s) satisﬁes the stopping criterion deterministically and almost surely for cyclic and randomized variable selection rules, respectively.
Then (¯x, ¯s) is an δ-stationary solution of problem (2).
That is to say, there exists ¯λ ∈ Rp such that the following statement holds true, dist(cid:16)−∇if (¯x) − A⊤ ¯λ, ∂ri(¯xi)(cid:17) ≤ δ, i = 1, .
.
.
, n, (cid:13)(cid:13)¯sj ¯λj − δ(cid:13)(cid:13)2 ≤ δ, j = 1, .
.
.
, p, kA¯x + ¯s − bk2 ≤ δ, deterministically for cyclic variable selection rule and Ehdist(cid:16)−∇if (¯x) − A⊤ ¯λ, ∂ri(¯xi)(cid:17)i ≤ δ, i = 1, .
.
.
, n, E(cid:2)(cid:13)(cid:13)¯sj ¯λj − δ(cid:13)(cid:13)2(cid:3) ≤ δ, j = 1, .
.
.
, p, E [kA¯x + ¯s − bk2] ≤ δ, for randomized variable selection rule.
Proof.
When optimizing a δ-smoothed version, the ﬁrst-order optimality condition for (xk+1, sk+1, λk+1) is i − xk+1 xk (cid:13)(cid:13)(cid:13)2  , j λk+1 = δ for 1 ≤ j ≤ p.
It j − xk+1 xk i λk+1, ∂ri(xk+1 Xj=1 1≤i≤n{diam(Xi)}.
)(cid:17) ≤ D dist(cid:16)−∇if (xk+1) − A⊤ where i ∈ I and D = max (cid:13)(cid:13)(cid:13)2 kAjk2(cid:13)(cid:13)(cid:13)  + (2Li + 1)(cid:13)(cid:13)(cid:13) β (cid:13)(cid:13)λk − λk+1(cid:13)(cid:13)2 and sk+1 Furthermore, we have (cid:13)(cid:13)Axk+1 + sk+1 − b(cid:13)(cid:13)2 = 1 follows from Lemma 12 that there exists suﬃciently large ¯K > 0 such that, for k ≥ ¯K, we have (cid:13)(cid:13)(cid:13)2 (cid:13)(cid:13)(cid:13)2  + (Li + 1)(cid:13)(cid:13)(cid:13) xk j − xk+1 xk i − xk+1  ≤ δ, β (cid:13)(cid:13)(cid:13) λk − λk+1(cid:13)(cid:13)(cid:13)2 ≤ δ, (cid:13)(cid:13)(cid:13)2  + (Li + 1)(cid:13)(cid:13)(cid:13)  ≤ δ, E(cid:13)(cid:13)(cid:13) λk − λk+1(cid:13)(cid:13)(cid:13)2 ≤ δ, for cyclic variable selection rule, and E D kAjk2(cid:13)(cid:13)(cid:13) kAjk2(cid:13)(cid:13)(cid:13) Xj=1 Xj=1 (cid:13)(cid:13)(cid:13)2 j − xk+1 xk i − xk+1 xk D 17 for randomized variable selection rule.
In this case, the stopping criterion is satisﬁed, and hence the In conclusion, we obtain that above inequality holds for (cid:0)¯x, ¯s, ¯λ(cid:1) when optimizing δ-smoothed version.
¯λ, ∂ri(¯xi)(cid:17) ≤ δ, i = 1, .
.
.
, n, ¯sj ¯λj − δ = 0, j = 1, .
.
.
, p, kA¯x + ¯s − bk2 ≤ δ, dist(cid:16)−∇if (¯x) − A⊤ for cyclic variable selection rule, and Ehdist(cid:16)−∇if (¯x) − A⊤ ¯λ, ∂ri(¯xi)(cid:17)i ≤ δ, i = 1, .
.
.
, n, E(cid:2)¯sj ¯λj − δ(cid:3) = 0, j = 1, .
.
.
, p, E [kA¯x + ¯s − bk2] ≤ δ, for randomized variable selection rule.
A.2 Proof of Theorem Since a sequence of (¯x, ¯s) remains bounded as δ decreases, the set of the limiting points is non-empty.
We consider a sub-sequence of (¯x, ¯s) indexed by {kl}+∞ l=1 which converges to (x∗, s∗).
By using Lemma 13, we conclude that, there exists ¯λ ∈ Rp such that (¯x, ¯s) indexed by kl satisﬁes that dist(cid:16)−∇if (¯x) − A⊤ ¯λ, ∂ri(¯xi)(cid:17) ≤ (γ)klδ0, i = 1, .
.
.
, n, ¯sj ¯λj − (γ)klδ0(cid:13)(cid:13)(cid:13)2 ≤ (γ)klδ0, j = 1, .
.
.
, p, kA¯x + ¯s − bk2 ≤ (γ)klδ0, (cid:13)(cid:13)(cid:13) deterministically for cyclic variable selection rule and Ehdist(cid:16)−∇if (¯x) − A⊤ ¯λ, ∂ri(¯xi)(cid:17)i ≤ (γ)klδ0, i = 1, .
.
.
, n, ¯sj ¯λj − (γ)klδ0(cid:13)(cid:13)(cid:13)2i ≤ (γ)klδ0, j = 1, .
.
.
, p, E [kA¯x + ¯s − bk2] ≤ (γ)klδ0, Eh(cid:13)(cid:13)(cid:13) for randomized variable selection rule.
Here δ0 is the initial smoothing parameter.
We know that (γ)kl → 0 as l → +∞ since γ ∈ (0, 1).
Therefore, we conclude that x∗ is a stationary solution of problem (1) deterministically for cyclic variable selection rule and in terms of expectation for randomized variable selection rule.
This completes the proof.
18 B Proof of Theorem 10 B.1 Proof of Technical Lemma We present a technical lemma to show the number of iterations required to reach the stopping criterion when optimizing δ-smoothed version of problem (1), i.e., problem (2).
Speciﬁcally, this number is disproportionate to the value of δ, which makes a lot of sense since problem (2) becomes harder as δ → 0.
Lemma 14.
Suppose cyclic or randomized variable selection rule is employed for optimizing δ-smoothed version, i.e., problem (2), the stopping criterion is satisﬁed deterministically or in terms of expectation, respectively, within O( 1 δ ) iterations.
Proof.
From Lemma 13, it suﬃces to show that, there exists C > 0 such that, if k ≥ C statement holds true, δ , the following D Xj=1 kAjk2(cid:13)(cid:13)(cid:13) j − xk+1 xk for cyclic variable selection rule, and E D Xj=1 kAjk2(cid:13)(cid:13)(cid:13) j − xk+1 xk i − xk+1 xk (cid:13)(cid:13)(cid:13)2 (cid:13)(cid:13)(cid:13)2  + (Li + 1)(cid:13)(cid:13)(cid:13)  ≤ δ, β (cid:13)(cid:13)(cid:13) λk − λk+1(cid:13)(cid:13)(cid:13)2 ≤ δ, (cid:13)(cid:13)(cid:13)2  + (Li + 1)(cid:13)(cid:13)(cid:13) E(cid:13)(cid:13)(cid:13)  ≤ δ, λk − λk+1(cid:13)(cid:13)(cid:13)2 ≤ δ, (cid:13)(cid:13)(cid:13)2 i − xk+1 xk for randomized variable selection rule.
It follows from Lemma 12 that, for ∀K ≥ 1, we have Φ(cid:0)x0, s0, λ0(cid:1) − Φ∗ ≥ Xk=0 2 Xi∈Ik+1(cid:13)(cid:13)(cid:13) xk i − xk+1 +(cid:18) βs2 2µ − (cid:13)(cid:13)(cid:13) βs2(cid:19)(cid:13)(cid:13)(cid:13) λk − λk+1(cid:13)(cid:13)(cid:13) 2  , (14) where Ik is denoted as the active set chosen at the k-th iteration of the inner loop when optimizing δ-smoothed version.
Combining the fact that yields that the iteration complexity is O( 1 δ ).
By using similar technique, we can obtain the same (cid:13)(cid:13)(cid:13) xk − xk+1(cid:13)(cid:13)(cid:13) = Xi∈Ik+1(cid:13)(cid:13)(cid:13) i − xk+1 xk (cid:13)(cid:13)(cid:13) 19 complexity for randomized variable selection rule, where it holds true that j − xk+1 xk kAjk2(cid:13)(cid:13)(cid:13) D E Xj=1 This completes the proof.
B.2 Proof of Theorem i − xk+1 xk (cid:13)(cid:13)(cid:13)2  + (Li + 1)(cid:13)(cid:13)(cid:13) E(cid:13)(cid:13)(cid:13) (cid:13)(cid:13)(cid:13)2  ≤ δ, λk − λk+1(cid:13)(cid:13)(cid:13)2 ≤ δ.
On one hand, it is clear to derive from δ ← γδ that δ ≤ ǫ is satisﬁed within O(log( 1 ǫ )) iterations in term of the iteration number in outer loop.
On the other hand, by using Lemma 14, we obtain the iterations required in term of the iteration number in inner loop is T = log( 1 ǫ ) Xk=0 γkδ0 δ0 ǫ − 1 γ − 1 ≤ Cγ δ0 − γδ0 which implies that the iterations required in term of the iteration number in inner loop is O( 1 completes the proof.
ǫ ).
This 20
Pioneered by the Deep Q-network [Mnih et al., 2015] and followed up by various extensions and advancements [Mnih et al., 2016; Lillicrap et al., 2015; Schulman et al., 2015; Schulman et al., 2017], Deep Reinforcement Learning (DRL) algorithms show great potential in solving high-dimensional real-world robotics sensory control tasks.
However, DRL methods typically require several millions of training sam- ples, making them infeasible to train directly on real robotic systems.
As a result, DRL algorithms are generally trained in simulated environments, then transferred to and deployed in real scenes.
However, the reality gap, also referred to as the ∗indicates equal contribution.
domain shift, namely the noise pattern, texture, lighting con- dition discrepancies, etc., between synthetic renderings and real sensory readings, imposes major challenges for gener- alizing the sensory control policies trained in simulation to reality.
In this paper, we focus on visual control tasks, where au- tonomous agents perceive the environment with their onboard cameras, and execute commands based on color image read- ing streams.
A natural way and also the typical choice in the recent literature of dealing with the reality gap for visual con- trol, is by increasing the visual ﬁdelity of the simulated im- ages [Bousmalis et al., 2017], by matching the distribution of synthetic images to that of the real ones [Tobin et al., 2017], and by gradually adapting the learned features and represen- tations from the simulated domain to the real-world domain [Rusu et al., 2017].
These sim-to-real methods, however, inevitably have to add preprocessing steps for each individ- ual training frame to the already expensive learning pipeline of DRL control policies; also, the complete policy training phase has to be conducted again for each visually different real-world scene.
Attempts have also been made in computer graphics to directly increase the quality of the simulators, to make the synthetically rendered images more visually realis- tic; however, the rendering for detailed and realistic texture and modality often adds to the computational burden.
This paper attempts to tackle the reality gap in the visual control domain from a novel perspective, with the aim of adding minimal extra computational burden to the learning pipeline.
We cope with the reality gap only during the ac- tual deployment phase of agents in real-world scenarios, by adapting the real camera streams to the synthetic modality, so as to translate the unfamiliar or unseen features of real images back into the simulated style, which the agents have already learned how to deal with during training in the simulators.
Compared to other sim-to-real methods bridging the reality gap, our proposed real-to-sim approach, which we refer to as VR Goggles, has several appealing properties: • First of all, our approach is highly lightweight.
It does not add any extra processing burden to the training phase of DRL policies.
• Secondly, our proposed method is highly ﬂexible and efﬁcient.
Since we decouple the policy training and the adaptation operations, the preparations for transferring the polices from simulation to the real world can be con- ducted in parallel with the training of DRL control poli- cies.
From each visually different real-world environ- ment that we expect to deploy the agent in, we just need to collect several (typically on the order of 2000) im- ages, and train a model of VR Goggles for each of them.
More importantly, we do not need to retrain or ﬁnetune the visual control policy for new environments.
As an additional contribution, we propose a new shift loss, which enables us to generate consistent synthetic image streams without information to impose temporal constraints, or even sequential training data.
We show that shift loss is a promising and much cheaper alternative to the constraints im- posed by optical ﬂow, and we demonstrate its effectiveness in artistic style transfer for videos and domain adaptation.
2 Related Works 2.1 Domain Adaptation Domain adaptation, also referred to as image-to-image trans- lation, targets translating images from a source domain into a target domain.
We here focus on the most general unsu- pervised methods that require the least manual effort and are applicable to robotics control tasks.
CycleGAN [Zhu et al., 2017a] introduced a cycle- consistent loss to enforce an inverse mapping from the tar- get domain to the source domain on top of the source to target mapping.
It does not require paired data from the two domains of interest, and shows convincing results for relatively simple data distributions containing few semantic types.
However, in terms of translating between more com- plex data distributions containing many more semantic types, its results are not as satisfactory, in that permutations of se- mantics often occur.
CyCADA [Hoffman et al., 2017] added a semantic constraint on top, to enforce a match between the semantic map of the translated image and that of the input.
However, the semantic loss was not added in its experiments on large datasets due to memory limitations.
Following the observation that several most recent and ad- vanced robotics simulators do provide semantic ground truth, and the semantic segmentation literature is quite mature (e.g., [Chen et al., 2017]), we adopt the semantic constraint from CyCADA into our method.
We are able to include the seman- tic loss calculation with special conﬁgurations (Sec.
4.2).
2.2 Domain Adaptation for DRL DRL approaches have been adopted into robotics control tasks such as manipulation and navigation.
Below we review the recent literature with an emphasis on works taking the re- ality gap into consideration.
For manipulation, [Bousmalis et al., 2017] bridged the re- ality gap by adapting synthetic images to the realistic do- main during the training phase.
However, this addition of an adaptation step before every training iteration can greatly slow down the whole learning pipeline.
[Tobin et al., 2017] proposed to ramdomise the texture of objects, lighting condi- tions, and camera positions during training, in the hope that the learned model will generalize naturally to real-world sce- narios.
However, such randomization unfortunately cannot be satisﬁed at a low cost by most of the popular robotic simu- lators.
Moreover, there is no guarantee that these randomized simulations can cover the visual modality of a random real- world scene.
[Rusu et al., 2017] deals with the reality gap by progressively adapting the learned features and represen- tations of a model trained in simulation to that of the realistic domain.
This method, however, still needs to go through an expensive control policy training phase for each visually dif- ferent real-world scenario.
Artistic style transfer for videos works on video sequences instead of individual frames.
It targets generating temporally consistent stylizations for sequential frames.
[Ruder et al., 2017] provides a key observation that: a trained stylization network with a total downsampling factor of s (e.g., s = 4 for a network with 2 convolutional layers of stride 2), is shift invariant to shifts equal to the multiples of s pixels, but can output signiﬁcantly different stylizations otherwise.
This un- For navigation, where autonomous agents are expected to encounter sensor readings of environments at a much larger scale than manipulation, the reality gap has not been directly dealt with in the literature of learning-based visual control to the best of our knowledge.
Some works, however, chose spe- cial setups to circumvent the reality gap.
For example, 2D Lidar [Tai et al., 2017; Zhang et al., 2017b] and depth images [Zhang et al., 2017a; Tai et al., 2018] are sometimes chosen as the sensor modality for transferring the navigation policies to the real world, since the discrepancies between the simu- lated domain and the real-world domain for them are smaller than those for color images.
[Zhu et al., 2017b] conducted real-world experiments with visual inputs.
But in their se- tups, the real-world scene is highly visually similar to their simulated environment, which is a condition that can rarely be met in practice.
In this paper, we mainly consider domain adaptation for visual navigation tasks using DRL, which has not yet been considered in the literature.
We believe the adaptation for navigation is much more challenging than for manipula- tion, since navigation agents usually work in environments at much larger scales with more complexities than the con- ﬁned workspace for manipulators.
We believe our proposed real-to-sim method can be naturally adopted in manipulation.
An important aspect of domain adaptation, within the con- text of dealing with the reality gap for DRL, is the consis- tency between subsequent frames, which has not yet been considered in any of the aforementioned adaptation meth- ods.
As a method for solving sequential decision making, the consistency between the subsequent input frames for DRL agents can be critical for the successful fulﬁllment of their ﬁ- nal goals.
Apart from the solutions for solving the reality gap for DRL, the general domain adaptation literature also lacks works considering sequential frames instead of single frames.
Therefore, we look to borrow techniques from other re- search ﬁelds that successfully extend single-frame algorithms to the video domain, among which the most applicable meth- ods are those from the artistic style transfer literature.
2.3 Artistic Style Transfer for Videos Artistic style transfer is a technique for transfering the artistic style of artworks to photographs [Johnson et al., 2016].
Figure 1: The VR Goggles pipeline.
We depict the computation of the losses LGANX, LcycY , LsemY and LshiftX.
We show both outdoor and indoor scenarios for demonstration, where the adaptation for the outdoor scene is trained with the semantic loss Lsem (since its simulated domain CARLA has ground truth semantic labels to train a segmentation network fX), and the indoor one without (since its simulated domain Gazebo does not provide semantic ground truth).
The components marked in red are those involved in the ﬁnal deployment phase: a real sensor reading is captured (y ∼ preal), then passed through the Goggles module (generator GX) to be translated into the simulated domain X, where the DRL agents were originally trained; the translated image ˆx is then fed to DRL agents, to output control commands for autonomous vehicles.
For clarity, we skip the counterpart losses LGANY , LcycX, LsemX and LshiftY .
desired property (of not being shift invariant) causes the out- put of the trained network to change signiﬁcantly for even very small changes in the input, which leads to temporal in- consistency (under the assumption that only relatively lim- ited changes would appear in subsequent frames of the in- coming sequential data).
However, their solution of adding temporal constraints between generated subsequent frames, is rather expensive, as it requires optical ﬂow as input during deployment.
[Huang et al., 2017] incorporated the temporal constraint into the single-frame artistic style transfer pipeline and is a relatively cheap solution.
However, we believe that constraining optical ﬂow on single input images is not well- deﬁned.
We suspect that the improved temporal consistency in [Huang et al., 2017] is actually due to the inexplicitly im- posed consistency constraints for regional shifts by optical ﬂow.
We validate this suspicion in our experiments (Sec.
4.1).
We believe that the fundamental problem causing the in- consistency (the shift variance) can be solved by an additional constraint of shift loss, which we will introduce in Sec.
3.4. We show that the shift loss enables us to constrain the consis- tency between generated subsequent frames, without the need for the relatively expensive optical ﬂow constraint.
We argue that for a network that has been properly trained to learn a smooth function approximation, small changes in the input should also result in small changes in the output.
3 Methods 3.1 Problem formulation We consider visual data sources from two domains: X, con- taining sequential frames {x0, x1, x2,···} (e.g., synthetic images output from a simulator; x ∼ psim, where psim de- notes the simulated data distribution), and Y, containing se- quential frames {y0, y1, y2,···} (e.g., real camera readings from the onboard camera of a mobile robot; y ∼ preal, where preal denotes the distrbution of the real sensory readings).
We emphasize that, although we require our method to generate consistent outputs for sequential inputs, we do not need the training data to be sequential; we formalize it in this way only because some baseline methods have this requirement.
As we have discussed, DRL agents are typically trained in the simulated domain X, while they are expected to perform tasks in the real-world domain Y.
And as we have discussed before, we choose to tackle this problem by translating the images from real domain images to the synthetic domain dur- ing deployment.
In the following we introduce the details of our approach for performing domain adaptation.
Also to cope with the sequential nature of the incoming data streams, we introduce a technique for constraining the consistency of the translated subsequent frames.
3.2 CycleGAN Loss To achieve this, we ﬁrst build on top of CycleGAN [Zhu et al., 2017a], which learns two generative models to map between domains: GY : X → Y, with its discriminator DY, and GX : Y → X, with its discriminator DX, via training two GANs simultaneously: LGANY (GY, DY; X, Y) = Ey [log DY(y)] LGANX (GX, DX; Y, X) = Ex [log DX(x)] (1) + Ex [log(1 − DY(GY(x)))] , (2) + Ey [log(1 − DX(GX(y)))] , in which GY learns to generate images GY(x) matching those from domain Y, while GX tries translating y to im- ages from domain X.
Following CycleGAN, we also add the cycle consistency loss to constrain those mappings: LcycY (GX, GY; Y) = Ey [||GY(GX(y)) − y||1] , LcycX (GY, GX; X) = Ex [||GX(GY(x)) − x||1] .
(3) (4) 3.3 Semantic Loss Since our translation domains of interest are between syn- thetic images and real-world sensor images, we take advan- tage of the fact that many recent robotic simulators provide ground truth semantic labels and add the semantic constraint inspired by CyCADA [Hoffman et al., 2017].
X∼psimGXGYˆYLcycYLsemYLshiftXfXfXˆX[x→·,y→·]GXY[x→·,y→·]LGANXDXtrainedinXY∼prealindoorˆXoutdooroutdoorindoorindooroutdoorAssuming that for images from domain X, the ground truth semantic information SX is available, a semantic segmenta- tion network fX can be easily obtained by minimizing the cross-entropy loss, denoted CrossEnt(SX, fX(X)).
We further assume that the ground truth semantic for do- main Y is lacking (which is the case for most real scenarios), meaning that fY is not easily accessible.
In this case, we provide ”semi” semantic supervision to the training agents.
After fX for semantic segmentaion of domain X is ob- tained, ”semi” semantic supervision for the generators can be incorporated, by imposing consistency between the seman- tic map of the input and that of the generated output.
This semantically consistent image translation can be achieved by minimizing the following losses (we use fX to also generate ”semi” semantic labels for domain Y): LsemY (GY; X, fX) = CrossEnt(fX(X), fX(GY(X))) (5) LsemX (GX; Y, fX) = CrossEnt(fX(Y), fX(GX(Y))) (6) 3.4 Shift Loss for Consistent Generation literature for image-to-image Different from the current translation or domain adaptation, our model is additionally expected to output consistent images for sequential input data.
Although by adding Lsem, the semantics of the con- secutive outputs are constrained, inconsistences and artifacts still occur quite often.
Moreover, in cases where ground truth semantics are unavailable from either domain, the sequential outputs are even less constrained, which could potentially lead to inconsistent DRL policy outputs.
To constrain the consistency even in these situations, following the discussion from Sec.
2.3, we introduce shift loss below.
For an input image x, we use x[x→i,y→j] to denote the re- sult of a shift operation: shifting x along the X axis by i pixels, and j pixels along the Y axis.
We sometimes omit y → 0 or x → 0 in the subscript if the image is only shifted along the X or Y axis.
According to [Ruder et al., 2017], a trained stylization net- work is shift invariant to shifts of multiples of s pixels (s represents the total downsampling factor of the network), but can output signiﬁcantly different stylizations otherwise.
This causes the output of trained network to change greatly for even very small changes in the input.
We thus propose to add a conceptually simple yet direct and effective shift loss: LshiftY (GY;X) = E (cid:104)(cid:12)(cid:12)(cid:12)(cid:12)GY(x)[x→i,y→j] − GY(x[x→i,y→j]) (cid:12)(cid:12)(cid:12)(cid:12)2 (cid:104)(cid:12)(cid:12)(cid:12)(cid:12)GX(y)[x→i,y→j] − GX(y[x→i,y→j])(cid:12)(cid:12)(cid:12)(cid:12)2 LshiftX (GX;Y) = E y, i,j∼u(1,s−1) x, i,j∼u(1,s−1) (cid:105) (cid:105) (7) (8) where u denotes the uniform distribution.
Shift loss constrains the shifted output to match the output of the shifted input, regarding the shifts as image-scale move- ments.
under the assumption that only limited regional move- ment would appear in subsequent input frames, shift loss ef- fectively smoothes the mapping function for small regional movements, restricting the changes in its outputs for subse- quent inputs frames.
It can be regarded as a cheap alternative for imposing consistency constraints on small movements, eliminating the need for the relatively expensive optical ﬂow information, which is crucial for meeting the requirement of real-time control in robotics.
3.5 Full Objective Our full objective for learning VR Goggles (Fig.
1) is: L(GY, GX, DY, DX; X, Y, fX) = (9) LGANY (GY, DY; X, Y) + LGANX(GX, DX; Y, X) LcycY (GX, GY; Y) + LcycX (GY, GX; X)(cid:1) + λcyc + λsem (LsemY (GY; X, fX) + LsemX (GX; Y, fX)) + λshift (LshiftY (GY; X) + LshiftX (GX; Y)) , where λcyc, λsem and λshift controls the weighting for each loss.
This corresponds to solving the following optimization: DY,DX L(GY, GX, DY, DX).
(10) G∗Y, G∗X = arg min max GY,GX (cid:0) 4 Experiments 4.1 Artistic Style Transfer for Videos To evaluate our method, we ﬁrstly conduct experiments for artistic style transfer for video sequences, to validate the ef- fectiveness of shift loss on constraining consistency for se- quential frames.
We collect a training dataset of 98 HD video footage sequences (from VIDEVO1, containing 2450 frames in total); the Sintel2 sequences are used for testing, as their ground-truth optical ﬂow is available.
We compare the per- formance of the models trained under the following setups: • FF [Johnson et al., 2016]: Conanical feed forward style transfer trained on single frames; • FF+ﬂow [Huang et al., 2017]: FF trained on sequential images, with optical ﬂow added for imposing temporal constraints on subsequent frames.
shift loss, as discussed in Sec.
3.4. • Ours: FF trained on single frames, with an additional We do not compare our method with that of [Ruder et al., 2017], as they require optical ﬂow as input during deploy- ment.
This is relatively expensive for our target application of real-time control.
Implementation-wise, we use the pretrained VGG-19 as the loss network, relu2 2 as the content layer, relu1 2, relu2 2, relu3 2 and relu4 2 as the style layers.
We set the weight for each loss as follows: 1e5 for content, 2 for style, 1e-7 for spa- tial regularization, 10 for optical ﬂow, and 100 for shift.
The downsampling factor s for our transformer network is 4; we use the same transformer network architecture and style im- ages as in [Johnson et al., 2016].
Shifts are uniformly sam- pled from [1, s − 1] for every training frame.
As a proof of concept, we begin our evaluation by compar- ing the setups on their ability to generate shift invariant styl- izations.
In particular, for each image x in the testing dataset, we generate 4 more test images by shifting the original image along the X axis by 1, 2, 3, 4 pixels respectively, and pass all 5 frames (x, x[x→1], x[x→2], x[x→3], x[x→4]) through the trained network to examine the consistency of the generated images (Fig.
2).
1https://www.videvo.net/ 2http://sintel.is.tue.mpg.de/ Figure 3: Temporal error maps between generated stylizations for subsequence input frames.
1st row: input frames; 2nd ∼ 4th row: temporal error maps (with the corresponding stylizations shown on top) of outputs from FF, FF+ﬂow, and Ours.
We here choose a very challenging style (mosaic) for temporal consistency, as it contains many ﬁne details, with tiny tiles laid over the original image in the ﬁnal stylizations.
Yet, Ours achieves very high consistency.
lower temporal loss with the shift loss constraint.
4.2 Domain Adaptation for Outdoor Scenarios Next we validate the shift loss in the ﬁeld of domain adapta- tion, ﬁrstly in outdoor urban street scenarios (where we col- lect synthetic domain images X ∼ psim from the CARLA sim- ulator [Dosovitskiy et al., 2017], and realistic domain images Y ∼ preal from the RobotCar dataset [Maddern et al., 2017]).
We compare the following three setups: • CyCADA [Hoffman et al., 2017]: CycleGAN with se- mantic constraints, trained on single frames; • CyCADA+ﬂow: CyCADA with the temporal constraint as in [Huang et al., 2017], trained on sequential frames; • Ours: CyCADA with shift loss, trained on single frames; we refer to this as VR Goggles.
Table 1: Temporal loss comparison between FF, FF+ﬂow and Ours.
This metric is part of the optimization objective of FF+ﬂow, while optical ﬂow is never provided to Ours; yet our method is able to achieve lower temporal loss on the evaluated Sintel sequences.
Figure 2: Shift-invariance evaluation, comparing between FF, FF+ﬂow and Ours.
We shift an input image x along the X axis by 1, 2, 3, 4 pixels respectively and feed all 5 frames through the net- works trained via FF, FF+ﬂow and Ours, and show the generated stylizations.
We mark the most visible differences in small circles and dim the rest of the generated images.
As is discussed in [Ruder et al., 2017], FF generates almost identical stylizations for x and x[x→4] (because 4 is a multiple of the total downsampling factor of the trained network), but those for x[x→1],x[x→2],x[x→3] differ sig- niﬁcantly.
FF+ﬂow improves the shift-invariance, but we suspect the improvement is due to the inexplicit consistency constraint on regional shifts imposed by optical ﬂow.
Ours, is able to generate shift-invariant stylizations with the proposed shift loss.
The results shown in Fig.
2 validate the discussion from [Ruder et al., 2017], since the stylizations for x and x[x→4] from FF are almost identical (s = 4 for the trained network), but differ signiﬁcantly otherwise.
FF-ﬂow improves the in- variance by a limited amount; Ours method is capable of gen- erating consistent stylizations for shifted input frames, with the shift loss directly reducing the shift variance.
We continue by evaluating the consistency of the stylized In Fig.
3, we show the temporal sequential input frames.
error maps, the same metric as in [Huang et al., 2017], of two stylized consecutive frames for each method.
Ours (bottom row) achieves the highest temporal consistency.
Furthermore, we evaluate the temporal loss computed us- ing the ground truth optical ﬂow for the Sintel sequences (Ta- ble 1).
Although the temporal loss is part of the optimization objective of FF-ﬂow, and our method does not have access to any optical ﬂow information, Ours is still able to achieve x[x→1]xx[x→2]x[x→3]x[x→4]FFABOursABFF+ﬂowABmosaiclamuse0.1190.0930.0860.1320.1080.1100.1270.1040.0980.1150.0890.0830.1240.0950.087FFOursFF+ﬂow0.1220.0960.0900.1130.0910.0890.1130.0850.0780.1520.1300.1270.1250.0990.0920.1540.1310.1300.1230.0970.0900.1350.1120.1120.1380.1080.1070.1210.1000.0920.1320.1060.0940.1290.1040.0940.1140.0900.0910.1270.0960.0830.1320.1020.101FFOursFF+ﬂowambush5bamboo1market6temple2sleeping2shaman3alley2bamboo2alley1sleeping1Figure 5: Real-world visual control experiment.
A DRL agent is trained in a simulated ofﬁce environment, that is able to navigate to chairs based on visual input.
Without retraining or ﬁnetuning the DRL policy, our proposed VR Goggles enables the mobile robot to directly deploy this policy in real ofﬁce environments, achieving 100% success rate in a set of real-world experiments.
We refer to the attached video for details of the real-world experiments.
Speciﬁcally, we begin by training a DRL agent in a simu- lated ofﬁce environment (A3C [Mnih et al., 2016], 20k roll- outs, 8 parallel CPU threads), to accomplish the task of navi- gating to chairs based purely on its front-facing color camera readings; the agent obtained a reward of −0.005 for a step cost, −0.05 for collision, and 1 for reaching the target.
Then, we deploy the trained DRL policy onto a real robot in a real- world ofﬁce, and compare the following adaptations: • NoGoggles: Feed the sensor readings directly to the trained DRL policy; • CycleGAN [Zhu et al., 2017a]: Use CycleGAN to trans- late the real sensory inputs to the synthetic domain be- fore feeding to the DRL policy; since the synthetic do- main here (Gazebo) does not provide ground truth se- mantics, we drop the semantic constraint Lsem; the VR Goggles to translate the input images.
• Ours: Use models trained by CycleGAN + shift loss as We use the same network conﬁguration as in Sec.
4.2, except that here the input images are of size 360 × 640.
We show in the attached video that, without domain adap- tation, directly deploying the DRL policy fails completely in the real-world tasks; our proposed method achieves the high- est success rate (0%, 60% and 100% for NoGoggles, Cycle- GAN and Ours respectively) due to the quality and consis- tency of the translated streams.
The control cycle runs in real- time at 13Hz on a Nvidia Jetson TX2.
In the video, we also show that VR Goggles can easily train a new model for a new type of chair without any adjustment to the previously trained control policy.
We limited the velocity of the robot due to the camera exposure time, since motion blur can greatly in- ﬂuence the adaptation quality.
We leave it as future work to evaluate on more compatible platforms.
5 Conclusions To conclude, we tackle the reality gap when deploying DRL visual control policies trained in simulation to the real world, by translating the real image streams back to the synthetic domain during the deployment phase.
Due to the sequential nature of the incoming sensor streams for control tasks, we propose shift loss to increase the consistency of the translated subsequent frames.
We validate the shift loss in both artistic Figure 4: Comparison of the translated images for sequential input frames for the different approaches.
1st row: two subsequent input frames from the realistic domain, with several representative images from the simulated domain shown in between; 2nd ∼ 4th row: out- puts from CyCADA, CyCADA+ﬂow and Ours.
Our method is able to output consistent subsequent frames and eliminate artifacts.
We adjust the brightness of some zoom-ins for visualization purposes.
We pretrain the segmentation network fX using Deeplab [Chen et al., 2017].
It is worth mentioning that the original CyCADA paper did not use the semantic constraint in their experiments due to memory issues.
We are able to incorpo- rate semantic loss calculation, by cropping the input images.
Actually, a naive random crop would highly likely lead to se- mantic permutations; so we crop inputs of the two domains in the same training iteration from the same random position, and our empirical results show that this greatly stabilizes the adaptation.
The input images are of size 450 × 800, we train the network with 256 × 256 crops.
We use the same network architecture as in CycleGAN, and train for 50 epochs with a learning rate of 2e − 4, as we observe no performance gain training for longer iterations.
4, we show a comparison of the subsequent frames generated by the three approaches.
Our method again achieves the highest consistency and eliminates more artifacts due to the smoothness of the learned model.
In Fig.
4.3 Domain Adaptation for Indoor Scenarios with Real-world Robotic Experiments Finally, we conduct domain adaptation for indoor ofﬁce en- vironments (X ∼ psim rendered from a self-built Gazebo [Koenig et al., 2004] world and Y ∼ preal captured from a real ofﬁce, using a RealSense R200 camera mounted on a Turtlebot3 Wafﬂe).
We validate our proposed method of us- ing the VR Goggles to facilitate the transfer of polices trained in the simulated domain to the realistic domain, with a set of real-world robotic experiments.
CycleGANNoGogglesVRGogglesDRLpolicytrainedinGazebo0%60%100%Successratestyle transfer for videos, and domain adaptation.
In the end, we successfully verify our domain adaptation method for vi- sual control through a set of real-world robot experiments.
Several future works can be conducted based on our method.
For example, training DRL agents on more com- plicated tasks, and in more complicated simulated environ- ments that provide ground truth semantic labels, such as the newly released MINOS [Savva et al., 2017].
Also, since in this paper we have mainly focused on learning based visual navigation, applying our method to manipulation tasks would be an interesting direction.
References [Bousmalis et al., 2017] Konstantinos Bousmalis, Alex Ir- pan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey, Mri- nal Kalakrishnan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt Konolige, et al.
Using simulation and domain adapta- tion to improve efﬁciency of deep robotic grasping.
arXiv preprint arXiv:1709.07857, 2017.
[Chen et al., 2017] LC Chen, G Papandreou, I Kokkinos, K Murphy, and AL Yuille.
Deeplab: Semantic image seg- mentation with deep convolutional nets, atrous convolu- tion, and fully connected crfs.
IEEE transactions on pat- tern analysis and machine intelligence, 2017.
[Dosovitskiy et al., 2017] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun.
In Conference Carla: An open urban driving simulator.
on Robot Learning, pages 1–16, 2017.
[Hoffman et al., 2017] Judy Hoffman, Eric Tzeng, Tae- sung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei A Efros, and Trevor Darrell.
Cycada: Cycle- consistent adversarial domain adaptation.
arXiv preprint arXiv:1711.03213, 2017.
[Huang et al., 2017] Haozhi Huang, Hao Wang, Wenhan Luo, Lin Ma, Wenhao Jiang, Xiaolong Zhu, Zhifeng Li, and Wei Liu.
Real-time neural style transfer for videos.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 783–791, 2017.
[Johnson et al., 2016] Justin Johnson, Alexandre Alahi, and Li Fei-Fei.
Perceptual losses for real-time style transfer In European Conference on Com- and super-resolution.
puter Vision, pages 694–711.
Springer, 2016.
[Koenig et al., 2004] Nathan Koenig, B A, and Andrew Howard.
Design and use paradigms for gazebo, an open- In Intelligent Robots and source multi-robot simulator.
Systems, 2004.(IROS 2004).
Proceedings.
2004 IEEE/RSJ International Conference on, volume 3, pages 2149–2154.
IEEE, 2004.
[Lillicrap et al., 2015] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, Continuous con- David Silver, and Daan Wierstra.
arXiv preprint trol with deep reinforcement learning.
arXiv:1509.02971, 2015.
[Maddern et al., 2017] Will Maddern, Geoff Pascoe, Chris Linegar, and Paul Newman.
1 Year, 1000km: The Oxford RobotCar Dataset.
The International Journal of Robotics Research (IJRR), 36(1):3–15, 2017.
[Mnih et al., 2015] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle- mare, Alex Graves, Martin Riedmiller, Andreas K Fidje- land, Georg Ostrovski, et al.
Human-level control through deep reinforcement learning.
Nature, 518(7540):529, 2015.
[Mnih et al., 2016] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
Asyn- chronous methods for deep reinforcement learning.
In International Conference on Machine Learning, pages 1928–1937, 2016.
[Ruder et al., 2017] Manuel Ruder, Alexey Dosovitskiy, and Thomas Brox.
Artistic style transfer for videos and spher- ical images.
arXiv preprint arXiv:1708.04538, 2017.
[Rusu et al., 2017] Andrei A Rusu, Matej Veˇcer´ık, Thomas Roth¨orl, Nicolas Heess, Razvan Pascanu, and Raia Had- sell.
Sim-to-real robot learning from pixels with progres- sive nets.
In Conference on Robot Learning, pages 262– 270, 2017.
[Savva et al., 2017] Manolis Savva, Angel X Chang, Alexey Dosovitskiy, Thomas Funkhouser, and Vladlen Koltun.
Minos: Multimodal indoor simulator for navigation in complex environments.
arXiv preprint arXiv:1712.03931, 2017.
[Schulman et al., 2015] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz.
Trust In International Conference region policy optimization.
on Machine Learning, pages 1889–1897, 2015.
[Schulman et al., 2017] John Schulman, Filip Wolski, Pra- fulla Dhariwal, Alec Radford, and Oleg Klimov.
Prox- arXiv preprint imal policy optimization algorithms.
arXiv:1707.06347, 2017.
[Tai et al., 2017] Lei Tai, Giuseppe Paolo, and Ming Liu.
Virtual-to-real deep reinforcement learning: Continuous control of mobile robots for mapless navigation.
In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 31–36, Sept 2017.
[Tai et al., 2018] Lei Tai, Jingwei Zhang, Ming Liu, and Wolfram Burgard.
Socially-compliant navigation through raw depth inputs with generative adversarial imitation learning.
In Robotics and Automation (ICRA), 2018 IEEE International Conference on, May 2018.
[Tobin et al., 2017] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel.
Domain randomization for transferring deep neural net- In Intelligent works from simulation to the real world.
Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on, pages 23–30.
IEEE, 2017.
[Zhang et al., 2017a] Jingwei Zhang, Jost Tobias Springen- berg, Joschka Boedecker, and Wolfram Burgard.
Deep reinforcement learning with successor features for navi- gation across similar environments.
In 2017 IEEE/RSJ In- ternational Conference on Intelligent Robots and Systems (IROS), pages 2371–2378, Sept 2017.
[Zhang et al., 2017b] Jingwei Zhang, Lei Tai, Joschka Boedecker, Wolfram Burgard, and Ming Liu.
Neural slam.
arxiv preprint.
arXiv preprint arXiv:1706.09520, 3, 2017.
[Zhu et al., 2017a] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros.
Unpaired image-to-image translation using cycle-consistent adversarial networks.
In Proceed- ings of the IEEE Conference on Computer Vision and Pat- tern Recognition, pages 2223–2232, 2017.
[Zhu et al., 2017b] Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi.
Target-driven visual navigation in indoor scenes In Robotics and Au- using deep reinforcement learning.
tomation (ICRA), 2017 IEEE International Conference on, pages 3357–3364.
IEEE, 2017.

Gaussian Processes (GPs) [Rasmussen, 2004] are powerful tools for regression and classiﬁcation problems as these mod- els are able to learn complex representation of data through expressive covariance kernels.
However, the application of GP in real-world is limited due to their poor scalability.
For a training data of size n, GPs requires O(n3) computation and O(n2) storage for training and O(n2) computation and storage for inference a single test point.
One of the most popular approach to scale GPs to larger [Seeger et al., 2003; dataset is inducing points methods Titsias, 2009; Lawrence et al., 2003].
These methods, though enables GPs to be applied on larger datasets, can result in degradation of performance due to the requirement of choos- ing m (cid:28) n inducing points [Wilson et al., 2014].
Another way of scaling GPs is through structure exploita- tion [Saatc¸i, 2012; Wilson et al., 2014].
These methods uti- lizes existing algebraic structure in the covariance matrix to ∗Equal contribution achieve fast exact learning and inference.
However, all these methods require the input data to have a grid structure which generally do not hold for real-world data .
Recently, the structured kernel interpolation (SKI) frame- work and KISS-GP [Wilson and Nickisch, 2015] further im- prove the scalability of GPs by unifying inducing points methods and structure exploitation.
SKI extends all structure exploiting approaches to arbitrarily located data by interpo- lating covariance matrix on a multidimensional grid (the in- ducing points).
With SKI framework and KISS-GP, the train- ing cost reduce to O(n) for computation and storage, and al- most constant time prediction [Wilson et al., 2015].
Although KISS-GP can be massively scalable, the number of inducing points on grid grows exponentially with dimensions and it also requires kernel to be separable or stationary in order to exploit structure.
On the other hand, we have seen a trend of deploying machine learning systems from servers to various resource- limited devices, such as mobile phone, robotics etc [Howard et al., 2017].
Modern machine learning models such as deep neural networks consist of millions of parameters and cannot be naively deployed on these devices due to com- putational and storage limitations.
Many works have stud- ied how to compress a large neural networks to a smaller one so that deployment of these complex models becomes possible on resource-limited devices [Buciluˇa et al., 2006; Hinton et al., 2015; Chen et al., 2015; Han et al., 2015].
GPs also have shown great applicability to tasks on resource-limited devices, such as control in robotics [Deisen- roth et al., 2015], for their modeling ﬂexibility and uncer- tainty measurements.
All the previous solutions for scal- ing GPs focus on training GPs from scratch.
Though some of these methods might be applied directly to deployment of GPs on resource-limited devices, the potential beneﬁts of transferring the knowledge from trained model were not dis- cussed.
We wish to investigate the possibility of compressing a trained exact GP model to a smaller approximate GP model while preserve the predictive power of the exact model.
In this paper, we propose, kernel distillation, a framework to approximate a trained GP model.
Kernel distillation ex- tends inducing point methods with insights from SKI frame- work and utilizes the knowledges from the trained model.
In particular, our contributions are: • We approximate the exact kernel matrix with a sparse and low-rank structured matrix.
We formulate the kernel matrix approximation problem as a constrained F -norm minimization problem, leading to more accurate kernel approximation compared to previous approximation ap- proaches.
• Our method is a general purpose kernel approximation method.
We do not require kernel function to be sepa- rable or stationary and do not assume input data to have any special structure.
• KISS-GP suffers from curse of dimensionality due to the grid structure of inducing points.
Instead, we select in- ducing points using clustering algorithm on input data, which allow us to handle high-dimensional case without worrying about explosion of inducing points.
• We show one application of kernel distillation is fast and accurate prediction for GP.
We evaluate our approach on various real-world datasets, and the empirical results ev- idence that kernel distillation can better preserving the predictive power of a fully trained GP model and im- proving the speed simultaneously compared to other al- ternatives.
2 Background 2.1 Gaussian Processes We provide a brief introduction for Gaussian Processes for regression problem in this paper.
We denote the dataset as D and it consists of input feature vectors X = {x1, .
.
.
, xn} and real-value targets y = {y1, .
.
.
, yn}.
A Gaussian Process (GP) is a collection of random vari- ables such that any ﬁnite subset of such random variables have a joint Gaussian distribution.
Using GP, We can model a distribution over functions f (x) ∼ GP(µ, kγ), where any set of function values forms a joint Gaussian distribution charac- terized by mean function µ(·) and covariance mapping func- tion kγ(·,·) where γ is the set of hyper-parameters to be trained.
The most common kernel function is RBF kernel deﬁned as: kRBF(x, z) = exp(−0.5||x − z||2/l2) where l is the hyper-parameter.
With GP, we can model the function values evaluated on training data so that: [f (x1), .
.
.
, f (xn)](cid:62) ∼ N (µµµ, KXX ) where µµµi = µ(xi) and the i, j entry of the covariance (KXX )i,j = kγ(xi, xj).
We use kγ(X, X) to denote the calculation of KXX for simplicity through out the paper.
Based on Gaussian Identity, we can arrive at posterior pre- dictive distribution for inference [Rasmussen and Williams, 2006]: f(cid:63)|X, X(cid:63), y ∼ N (KX(cid:63)X (KXX + σ2I)−1y, KX(cid:63)X(cid:63) − KX(cid:63)X (KXX + σ2I)−1KXX(cid:63) ).
The matrix KX(cid:63)X = kγ(X(cid:63), X) is the covariance measured between X(cid:63) and X.
The prediction for mean and variance cost O(n) in time and O(n2) in storage per test point.
Inducing Point Methods Training GPs involves marginalizing the log likelihood of data so that it is only conditioned on the hyper-parameters γ, which costs O(n3) in time and O(n2) in storage .
For the purpose of this paper, we do not focus on training procedure the GPs and we refer the readers to previous literature [Ras- mussen and Nickisch, 2015] for details on training.
2.2 The most common approach to approximate GP is inducing point methods.
Examples of inducing points methods are Subset of Regressors (SoR) [Silverman, 1985], Deterministic Training Conditional (DTC) and the Fully Independent Train- ing Conditional (FITC) Approximation [Snelson and Ghahra- mani, 2005].
There methods could be viewed to use ˜k(x, z) to approximate true kernel function k(x, z): ˜kSoR(x, z) = KxU K−1 ˜kFITC(x, z) = ˜kSoR(x, z) + δx,z U U KU z (cid:16) k(x, z) − ˜kSoR (cid:17) for a set of m inducing points U = [ui], i = 1 .
.
.
m.
For SoR, the approximated kernel matrix KXU K−1 U U KU X has rank at most m.
FITC achieves full rank approximation with diagonal correction, which improves the performance signif- icantly in practice.
These low rank approximations reduce mean and variance prediction time from O(n) and O(n2) to O(m) and O(m2).
The storage cost for mean and variance prediction is reduced from O(n) and O(n2) to O(n) and O(nm).
These meth- ods, however, suffer from a severe deterioration in predictive performance when n is very large as it requires m (cid:28) n to achieve efﬁciency gain [Wilson et al., 2014].
2.3 Structure Exploiting Methods Another family of approaches for scalable GP learning and inference is to exploit existing algebraic structure in KXX.
However, these structures are only exploitable for certain kinds of kernel function.
For example, Toeplitz method [Cun- ningham et al., 2008] requires a stationary kernel and Kro- necker methods [Saatc¸i, 2012; Wilson et al., 2014] requires a product and stationary kernel.
In addition, the exact inference of these approaches requires x to be on rectilinear grid which constraints its application to only limited kinds of data.
Structured Kernel Interpolation (SKI) [Wilson and Nick- isch, 2015] uniﬁes inducing points methods and structure ex- ploiting approaches.
SKI starts with extending SoR method by approximating KXU ≈ W KU U .
SKI further scales GPs by placing U on grid structure so that structure exploiting is possible.
The overall approximation for KXX is: KXX ≈ KXU K−1 = W KU U W (cid:62) = ˜KSKI U U KU X ≈ W KU U K−1 U U KU U W (cid:62) where values of W are the interpolation weights of X to U and W can be extremely sparse as each row only has 4 non- zero values.
The special structure of SKI approximation en- ables.
The inference cost of GPs with SKI reduce to O(n) for both time and storage.
It is worth noticing that in SKI framework, m could be much larger than n due to structure exploitation.
However, enforcing inducing points to be grid structured limits applica- tion of SKI to low-dimensional data (d ≤ 4) as the number of grid point increases exponentially as the dimension increases.
2.4 Knowledge Distillation in Deep Learning Knowledge distillation [Hinton et al., 2015] is used for trans- ferring the generalization ability from an ensemble of large neural networks to a light-weight model which is more suit- able for deployment.
The core idea behind knowledge distil- lation is that we train a neural network model with large ca- pacity, or an ensemble of neural networks, as a teacher model.
Once the teacher model has been trained, we then use the out- put produced by the teacher model as “soft targets” to train a small model as the student model.
Instead of training a student model from scratch, one can also compress a large neural network directly into a smaller one [Chen et al., 2015; Han et al., 2015].
These works ap- ply techniques such as hashing, pruning and quantization to the weight parameters of the neural networks.
As a result, the compressed neural network has much fewer parameters without much drop in predictive performance.
As we will see in the next section, our kernel distillation framework inherits the ideas of both knowledge distillation and model compression.
We compress the exact teacher ker- nel matrix with a scalable structure while the compression process involving optimization from teacher model, which can be viewed as a form of knowledge distillation.
3 Kernel Distillation In this section, we detail our method for kernel distillation.
We assume that we have access to a trained exact GP with full kernel matrix KXX as teacher model and all the training data {X, y}.
We also assume that we perform kernel distilla- tion on a machine with enough computational power to store KXX.
After kernel distillation, we can apply the distilled stu- dent model for on a resource-limited device for inference and other applications.
3.1 Sparse Low-rank Kernel Approximation Algorithm 1 outlines our distillation approach.
Formulation We propose to use a student kernel matrix with a sparse and low-rank structure, ˜KXX = W KU U W (cid:62) to ap- proximate a fully trained kernel matrix KXX.
W is a sparse matrix and KU U is the covariance evaluated at a set of induc- ing points U.
Similar to KISS-GP [Wilson and Nickisch, 2015], we ap- proximate KXU with ˜KXU = W KU U .
In KISS-GP, W is calculated using cubic interpolation on grid-structured induc- ing points.
The number of inducing points grows exponen- tially as the dimension of input data grows, limiting KISS-GP applicable to low-dimensional data.
Instead of enforcing inducing points U to be on grid, which causes curse of dimensionality, we choose m centroids as in- ducing points U using K-means clustering on X.
In addi- tion, we store U in KD-tree T for fast nearest neighbor search which will be used in later optimization.
Algorithm 1 Sparse Low-rank Kernel Approximation 1: Input: A well trained kernel function kγ, training feature vectors X and targets y, step size η, number of iterations T and sparsity b.
J ← indices for b nearest neighbors of xi in U Wi(J) ← argminβ||βKU U (J) − (KXU )i||2 2: Output: Approximated kernel matrix W KU,U W (cid:62) 3: U ← K-MEANS(X) 4: KXX ← kγ(X, X) 5: KU U ← kγ(U, U ) 6: KXU ← kγ(X, U ) 7: Step 1: Initialization 8: W ← 0 ∈ Rn×m 9: for each xi in X do 10: 11: 12: end for 13: Step 2: Gradient Descent 14: for t = 1 to T do 15: 16: 17: 18: 19: 20: end for E ← W KU U W (cid:62) − KXX Ei,i ← 2Ei,i for 1 ≤ i ≤ n ∇W ← E(cid:62)W KU U Project each row of ∇W to b-sparse space Update W ← W − η∇W In kernel distillation, we ﬁnd optimal W through a con- strained optimization problem.
We constrain each row of W to have at most b non-zero entries.
We set the objective func- tion to be the F -norm error between teacher kernel and stu- dent kernel: ||KXX − W KU U W (cid:62)||F min subject to ||Wi||0 ≤ b ∀i where ||Wi||0 denotes the number of non-zero entries at row i of W .
Initialization The initial values of W are crucial for the later optimization.
We initialize W with optimal solution to ||KXU − W KU U||F with the sparsity constraint.
More speciﬁcally, for each xi in X, we ﬁnd its b nearest points in U by querying T .
We denote the indices of these b neighbors as J.
We then initialize each row Wi of W by solving the following linear least square problem: minWi(J) ||Wi(J)KU U (J) − (KXU )i||2 where Wi(J) denotes the entries in row Wi indexed by J and KU U (J) denotes the rows of KU U indexed by J.
The entries in Wi with index not in J are set to zero.
Optimization After W is initialized, we solve the F -norm minimization problem using standard gradient descent.
De- tails of the gradient calculation is given in Algorithm 1.
To satisfy the sparsity constraint, in each iteration, we project each row of the gradient ∇W to b-sparse space according the indices J, and then update W accordingly.
3.2 Fast Prediction One direct application of kernel distillation is for fast predic- tion with approximated kernel matrix.
Given a test point x(cid:63), Methods FITC [Qui˜nonero-Candela and Rasmussen, 2005] KISS-GP [Wilson et al., 2015] Kernel distillation Storage Mean Prediction Variance Prediction O(nm) O(m2) O(1) O(n + 4d) O(m2) O(b log m + b3) O(m) O(1) O(b log m + b3) Table 1: Time and storage complexity for prediction for FITC, KISS and Distillation.
m is number of reducing points, d is the dimension of data and b is the non zero elements we choose in Algorithm 1.
(a) |KXX − Kdistill| (b) |KXX − KKISS| (c) |KXX − KSoR| (d) Error v.s. b Figure 1: Kernel Reconstruction Experiments.
(a) - (c) Absolute error matrix for reconstructing KXX with kernel distillation, KISS-GP and SoR respectively.
(d) F -norm error for reconstructing KXX with distillation under different setting of b (sparsity constraint) for W .
we follow similar approximation scheme in the distillation at test time where we try to approximate Kx(cid:63)X: Kx(cid:63)X ≈ ˜Kx(cid:63)X = W(cid:63) ˜KU X = W(cid:63)KU U W (cid:62) where W(cid:63) is forced to be sparse for efﬁciency.
Then the mean and variance prediction can be approximated by: E[f(cid:63)] ≈ ˜Kx(cid:63)X ( ˜KXX + σ2I)−1y ≈ W(cid:63)KU U W (cid:62)( ˜KXX + σ2I)−1y = W(cid:63) ˜ααα V ar[f(cid:63)] ≈ Kx(cid:63)x(cid:63) − ˜Kx(cid:63)X [ ˜KXX + σ2I]−1 ˜KXx(cid:63) ≈ Kx(cid:63)x(cid:63) − W(cid:63)KU U W (cid:62)[ ˜KXX + σ2I]−1W KU U W (cid:62) = Kx(cid:63)x(cid:63) − W(cid:63)V W (cid:62) (cid:63) (cid:63) where both ˜ααα =KU U W (cid:62)( ˜KXX + σ2I)−1y V =KU U W (cid:62)[ ˜KXX + σ2I]−1W KU U can be precomputed during distillation.
Now the key question is how to estimate W(cid:63) efﬁciently.
We use the same procedure as in the initialization of W to compute W(cid:63).
We start by ﬁnds b nearest neighbors of x(cid:63) in U and mark the indices as J(cid:63) and set elements of W(cid:63) whose indices are not in J(cid:63) to 0.
For entries with indices in J(cid:63), we solve the following least square problem to get the optimal values for W(cid:63)(J(cid:63)): ||W(cid:63)(J(cid:63))KU U (J(cid:63)) − Kx(cid:63)U (J(cid:63))||2.
min W(cid:63)(J(cid:63)) We can see that it takes O(b log m) to query the nearest neighbors, O(b3) to get W(cid:63) and O(b) and O(b2) for mean and variance prediction respectively.
Therefore, in total, predic- tion time complexity is O(b log m + b3).
As for storage com- plexity, we need to store precomputed vector for mean pre- diction and diagonal of matrix for variance prediction which cost O(m2).
Table 1 provides comparison of time and storage complex- ity for different GP approximation approaches.
The stor- age complexity for KISS-GP grows exponentially with di- mension of input data.
In practice, to avoid the exponential growth, KISS-GP also learns a mapping P that project input data to low dimension during training [Wilson et al., 2015].
The assumption that the input data are embedded in a low- dimensional space can sometimes hurt the predictive perfor- mance as we will demonstrate the experiment section.
Kernel distillation, on the other hand, makes no assumption about the dimensionality of input data while can still achieve rea- sonably fast and accurate prediction and reduce the storage cost.
3.3 Other Applications Apart from fast prediction for GPs, kernel distillation could also be used for other applications, such as online update for GPs [McIntire et al., 2016].
When the distilled model is de- ployed on mobile devices or robotics, we might want to adjust the model after seeing new data points.
Online update could potentially beneﬁt from the sparse and low-rank structure of the distilled kernel.
We leave it as a future work to explore how to integrate kernel distillation with online update of GPs. 4 Experiments We evaluate kernel distillation on the ability to approximate the exact kernel, the predictive power and the speed at infer- ence time.
In particular, we compare our approach to FITC and KISS-GP as they are the most popular approaches and are closely related to kernel distillation.
4.1 Kernel Reconstruction We ﬁrst study how well can kernel distillation reconstruct the full teacher kernel matrix.
We generate a 1000 × 1000 kernel matrix KXX from RBF kernel evaluated at (sorted) inputs X Dataset Boston Housing Abalone PUMADYM32N KIN40K Dim # train 455 3,133 7,168 10,000 13 32 # test Exact 0.076 0.434 0.044 0.013 51 1,044 1,024 30,000 FITC KISS-GP Distill 0.091 0.103 0.438 0.439 0.069 0.044 0.030 0.173 0.095 0.446 1.001 0.386 Table 2: SMSE Results Comparison.
Dim is the dimension of the input data.
Number of inducing points (on 2D grid) for KISS-GP are 4,900, 10K, 90K, 250K, and number of inducing points for FITC and kernel distillation are 70, 200, 1K, 1K for the for datasets respectively.
The sparsity b is set to 20 for Boston Housing and 30 for all other datasets.
predictions of kernel distillation are indistinguishable from exact GP and KISS-GP.
As for variance, kernel distillation’s predictions are much closer to the variance outputs from exact GP, while the variance outputs predicted by KISS-GP are far away from the exact solution.
This experiment shows a potential problem in KISS-GP, where it sacriﬁces its ability to provide uncertainty measure- ments, which is a crucial property of Bayesian modeling, for exchanging massive scalability.
On the other hand, kernel distillation can honestly provide uncertainty prediction close to the exact GP model.
4.3 Empirical Study We further evaluate the performance of kernel distillation on several benchmark regression data sets.
A summary of the datasets is given in Table 2.
Setup We compare kernel distillation with teacher kernel (exact GP), FITC as well as KISS-GP.
We use the same in- ducing points selected by K-Means for both FITC and ker- nel distillation.
For KISS-GP, as all the datasets do not lie in lower dimension, we project the input to 2D and con- struct 2D grid data as the inducing points.
Number of in- ducing points (on 2D grid) for KISS-GP are set to 4,900 (70 per grid dimension) for Boston Housing, 10K for Abalone, 90K for PUMADYM32N, 250K for KIN40K.
The number of inducing points for FITC and kernel distillation are 70 for Boston Housing, 200 for Abalone, 1k for PUMADYM32N and KINK40K.
The sparsity b in kernel distillation is set to 20 for Boston Housing and 30 for other datasets.
For all meth- ods, we choose ARD kernel as the kernel function, which is deﬁned as: kARD(x, z) = exp(−0.5 (xi − zi)2/σ2 i ) d(cid:88) i=1 where d is the dimension of the input data and σi’s are the hyper-parameters to learn.
All the experiments were conducted on a PC laptop with Intel Core(TM) i7-6700HQ CPU @ 2.6GHZ and 16.0 GB RAM.
Predictive performance comparison We start by evaluating how well kernel distillation can preserve the predictive per- formance of the teacher kernel.
The metrics we use for eval- uation is the standardized mean square error (SMSE) deﬁned as: SMSE(y, ˆy) = (yi − ˆyi)2/V ar(y) n(cid:88) i=1 (a) Mean (b) Variance Figure 2: Mean (a) and variance (b) prediction comparison for KISS-GP and Kernel Distillation on 1D example.
randomly sampled from N (0, 25).
We compare kernel dis- tillation against KISS-GP and SoR (FITC is essentially SoR with diagonal correction as mentioned in Section 2).
We set number of grid points for KISS-GP as 400 and number of in- ducing points for SoR is set to 200 and kernel distillation to 100.
We set the sparsity b to 6 for kernel distillation.
The F -norm for errors for are 1.22 × 10−5, 8.17 × 10−6, 2.39× 10−7 for KISS-GP, SoR and kernel distillation respec- tively.
Kernel distillation achieves lowest F -norm error com- pared to FITC and KISS-GP even the number of inducing points is much fewer for kernel distillation.
Moreover, from the absolute error matrices (Figure 1 a-c), we can see errors are more evenly distributed for kernel distillation, while there seems to exist a strong error pattern for the other two.
We also show how the sparsity parameter b affect the ap- proximation quality.
We evaluate the error with different choices for b as shown in Figure 1 (d).
We observe that the er- ror converges when the sparsity b is above 5 in this example.
This shows our structured student kernel can approximate the full teacher kernel reasonably well even when W is extremely sparse.
4.2 Toy 1D Example To evaluate our distilled model’s predictive ability, we set up the following experiment.
We sample n = 1000 data points X uniformly from [-10, 10].
We set our response y(x) = sin(x) exp(− x2 2×52 ) +  with  ∼ N (0, 1).
We train an exact GP with RBF kernel as teacher ﬁrst then apply ker- nel distillation with number of inducing points set to 100 and sparsity set to 10.
We compare mean and variance predic- tion of kernel distillation with KISS-GP trained with 400 grid inducing points.
The results are showed in Figure 2.
As we can see, mean (a) Boston Mean (b) Boston Variance (c) Abalone Mean (d) Abalone Variance Figure 3: Test error and variance comparison on Boston Housing (a-b) and Abalone (c-d) under different choices of sparsity constraint b on W .
For variance prediction comparison, we calculate the root square mean error between variance of exact GPs and approximate GPs (KISS-GP and kernel distillation).
for true labels y and model predictions ˆy.
Table 2 summarizes the results.
We can see that exact GPs achieve lowest errors on all of the datasets.
FITC gets second lowest error on almost all datasets except for Boston Hous- ing.
Errors with kernel distillation are very close to FITC while KISS-GP has the largest errors on every dataset.
The poor performance of KISS-GP might be resulted from the loss of information through the projection of input data to low di- mension.
Effects of sparsity We further study the effects of sparsity b on predictive performance.
We choose b to be range from [5, 10, .
.
.
, 40] and compare the test error and variance predic- tion for KISS-GP and kernel distillation on Boston Housing and Abalone datasets.
The results are shown in Figure 3.
As expected, the error for kernel distillation decreases as the sparsity increases and we only need b to be 15 or 20 to outperform KISS-GP.
As for variance prediction, we plot the error between outputs from exact GPs and approximate GPs. We can see that kernel dis- tillation always provides more reliable variance output than KISS-GP on every level of sparsity.
Speed comparison We now evaluate the speed of prediction with kernel distillation.
Again, we compare the speed with FITC and KISS-GP.
The setup for the approximate models is the same as the predictive performance comparison exper- iment.
For each dataset, we run the test prediction on 1000 points and report the average prediction time in seconds.
Table 3 summarizes the results on speed.
It shows that both KISS-GP and kernel distillation are much faster in predic- tion time compared to FITC for all datasets.
Figure 4 shows the detailed comparison of prediction time between KISS-GP and kernel distillation.
Though kernel distillation is slightly slower than KISS-GP, considering the improvement in accu- racy and more reliable uncertainty measurements, the cost in prediction time is acceptable.
Also, though KISS-GP claims to have constant prediction time complexity in theory [Wilson et al., 2015], the actual implementation still is data-dependent and the speed varies on different datasets.
In general, ker- nel distillation provides a better trade-off between predictive power and scalability than its alternatives.
FITC KISS-GP Dataset 0.00061 0.0081 Boston Housing 0.00018 Abalone 0.0631 0.0011 PUMADYM32N 1.3414 KIN40K 1.7606 0.0029 Distill 0.0017 0.0020 0.0035 0.0034 Table 3: Average prediction time in seconds for 1000 test data point of each dataset.
Setup for the models is the same as in Table 2.
Figure 4: Prediction time comparison for kernel distillation and KISS-GP.
The vertical black line shows the standard deviation over 5 rounds of experiments.
5 Conclusion We have proposed a general framework, kernel distillation, for compressing a trained exact GPs kernel into a student ker- nel with low-rank and sparse structure.
Our framework does not assume any special structure on input data or kernel func- tion, and thus can be applied ”out-of-box” on any datasets.
Kernel distillation framework formulates the approximation as a constrained F -norm minimization between exact teacher kernel and approximate student kernel.
The distilled kernel matrix reduces the storage cost to O(m2) compared to O(mn) for other inducing point meth- ods.
Moreover, we show one application of kernel distilla- tion is for fast and accurate GP prediction.
Kernel distillation can produce more accurate results than KISS-GP and the pre- diction time is much faster than FITC.
Overall, our method provide a better balance between speed and predictive perfor- mance than other approximate GP approaches.
[Rasmussen and Williams, 2006] Carl Edward Rasmussen and Christopher KI Williams.
Gaussian processes for ma- chine learning.
2006.
[Rasmussen, 2004] Carl Edward Rasmussen.
Gaussian pro- cesses in machine learning.
In Advanced lectures on ma- chine learning, pages 63–71.
Springer, 2004.
[Saatc¸i, 2012] Yunus Saatc¸i.
Scalable inference for struc- tured Gaussian process models.
PhD thesis, University of Cambridge, 2012.
[Seeger et al., 2003] Matthias Seeger, Christopher Williams, and Neil Lawrence.
Fast forward selection to speed up In Artiﬁcial Intel- sparse gaussian process regression.
ligence and Statistics 9, number EPFL-CONF-161318, 2003.
[Silverman, 1985] Bernhard W Silverman.
Some aspects of the spline smoothing approach to non-parametric regres- sion curve ﬁtting.
Journal of the Royal Statistical Society.
Series B (Methodological), pages 1–52, 1985.
[Snelson and Ghahramani, 2005] Edward and Zoubin Ghahramani.
Sparse gaussian processes using In Advances in neural information pseudo-inputs.
processing systems, pages 1257–1264, 2005.
Snelson [Titsias, 2009] Michalis K Titsias.
Variational learning of in- ducing variables in sparse gaussian processes.
In AISTATS, volume 12, pages 567–574, 2009.
[Wilson and Nickisch, 2015] Andrew Wilson and Hannes Nickisch.
Kernel interpolation for scalable structured gaussian processes (kiss-gp).
In Proceedings of The 32nd International Conference on Machine Learning, pages 1775–1784, 2015.
[Wilson et al., 2014] Andrew Wilson, Elad Gilboa, John P Cunningham, and Arye Nehorai.
Fast kernel learning In Advances for multidimensional pattern extrapolation.
in Neural Information Processing Systems, pages 3626– 3634, 2014.
[Wilson et al., 2015] Andrew Gordon Wilson, Christoph Dann, and Hannes Nickisch.
Thoughts on massively scal- able gaussian processes.
arXiv preprint arXiv:1511.01870, 2015.
References [Buciluˇa et al., 2006] Cristian Buciluˇa, Rich Caruana, and Alexandru Niculescu-Mizil.
Model compression.
In Pro- ceedings of the 12th ACM SIGKDD international con- ference on Knowledge discovery and data mining, pages 535–541.
ACM, 2006.
[Chen et al., 2015] Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, and Yixin Chen.
Compress- In Inter- ing neural networks with the hashing trick.
national Conference on Machine Learning, pages 2285– 2294, 2015.
[Cunningham et al., 2008] John P Cunningham, Krishna V Shenoy, and Maneesh Sahani.
Fast gaussian process meth- ods for point process intensity estimation.
In Proceedings of the 25th international conference on Machine learning, pages 192–199.
ACM, 2008.
[Deisenroth et al., 2015] Marc Peter Deisenroth, Dieter Fox, and Carl Edward Rasmussen.
Gaussian processes for data- IEEE Trans- efﬁcient learning in robotics and control.
actions on Pattern Analysis and Machine Intelligence, 37(2):408–423, 2015.
[Han et al., 2015] Song Han, Huizi Mao, and William J Dally.
Deep compression: Compressing deep neural net- works with pruning, trained quantization and huffman cod- ing.
arXiv preprint arXiv:1510.00149, 2015.
[Hinton et al., 2015] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
Distilling the knowledge in a neural network.
arXiv preprint arXiv:1503.02531, 2015.
[Howard et al., 2017] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam.
Mo- bilenets: Efﬁcient convolutional neural networks for mo- bile vision applications.
arXiv preprint arXiv:1704.04861, 2017.
[Lawrence et al., 2003] Neil Lawrence, Matthias Seeger, and Ralf Herbrich.
Fast sparse gaussian process methods: In Proceedings of the The informative vector machine.
16th Annual Conference on Neural Information Process- ing Systems, number EPFL-CONF-161319, pages 609– 616, 2003.
[McIntire et al., 2016] Mitchell McIntire, Daniel Ratner, and Stefano Ermon.
Sparse gaussian processes for bayesian In Proceedings of the Thirty-Second Con- optimization.
ference on Uncertainty in Artiﬁcial Intelligence, UAI’16, pages 517–526, Arlington, Virginia, United States, 2016.
AUAI Press.
[Qui˜nonero-Candela and Rasmussen, 2005] Joaquin Qui˜nonero-Candela and Carl Edward Rasmussen.
A unifying view of sparse approximate gaussian process Journal of Machine Learning Research, regression.
6(Dec):1939–1959, 2005.
[Rasmussen and Nickisch, 2015] Carl Edward Rasmussen and Hannes Nickisch.
The gpml toolbox version 4.0. 2015.

Compressive Sensing (CS) is a method in signal processing which aims to reconstruct signals from a small number of measurements.
By exploiting the fact that the unknown signal is sparse, its reconstruction is achieved with sampling rates far less than the Nyquist rate [1], [2].
In this paper, we focus on Binary Compressive Sensing (BCS) which conﬁnes the signals of interest only to binary {0, 1}-valued binary signals.
This is motivated by the wide use of binary signals in engineering, for example, fault detection [3], black-and-white image reconstruction [4], and digital com- munications [5], to name a few.
Related works are as follows.
Nakarmi et al.
[6] designed a novel sensing matrix tailored for binary signal reconstruction.
Wang et al.
[7] combined ℓ1 norm with ℓ∞ norm to reconstruct sparse binary signals.
Nagahara [8] exploited the sum of weighted ℓ1 norms to effectively reconstruct signals whose entries are integer-valued and, in particular, binary signals and bitonal images.
Keiper et al.
[9] analyzed the phase transition of binary Basis Pursuit.
To the best of our knowledge, most of BCS algorithms developed so far are based on convex optimization methods.
Although this family of methods enjoys appealing theoretical properties such as reconstruction guarantees (e.g. [9], [10]), they were found to be slow in large-scale applications [11].
The purpose of this work is to ﬁll in the gap by propos- ing a fast BCS method that attains accurate reconstruction results.
The proposed method leverages the gradient descent approach for smoothed ℓ0 norm [12] by introducing an ad- ditional probability distribution prior for binary signals in the penalty function.
We empirically compare the proposed method with state-of-the-art convex-optimization-based BCS algorithms and demonstrate its advantage of in reconstruction speed, and more impressively, in reconstruction accuracy.
The paper is organized as follows.
First we present a short review on CS and BCS algorithms in Section II.
We then present the proposed algorithm in Section III.
This is the main contribution of the paper.
Emperical experiments results are presented in Section IV to demonstrate the competitiveness of the proposed algorithm.
We given conclusion in Section V.
Notations: For a vector v = [v1, · · · , vN ]⊤, we use kvkp to denote the ℓp norm of v, where 1 ≤ p ≤ ∞.
We use kvk0 to denote the the number of non-zero entries of the vector v.
P(E) denotes the probability of an event E.
For an integer n, we use [n] as a short-hand notation for the set {1, · · · , n}.
We write 1N as the N -dimensional column vector with all entries equal to 1.
II.
BINARY COMPRESSIVE SENSING (BCS) In the standard CS scheme, one aims to recover a sparse signal from its linear measurements.
The constraints posed by the measurements can be formulated as Φz = y, (1) where Φ ∈ Rm×N , m ≪ N , is a measurement matrix, z ∈ RN , and y = Φx is the measurement vector of the sparse signal x ∈ RN .
CS algorithms exploit the fact that x is sparse and seek a sparse solution z of Equation 1.
In the BCS scheme, signals of interest are conﬁned to binary signals.
Notice that a binary signal x is sparse if and only if its As the measurement matrix Φ is known to the observer, (1) recover dense binary signals as well as the sparse ones.
complementary signalex := 1N − x is almost fully supported.
can be converted to Φez = ey, where ez := 1N − z and ey := Φex = Φ1N − y, and vice versa.
This observation allows us to Two types of models for binary signals have been consid- ered in the literature (e.g., [13], [7], [8]).
(i) Sparseness prior: x is a deterministic vector which is binary and sparse, i.e., most of its entries are 0 and only few are 1; (ii) Probability distribution prior: x is a random vector whose entries are independent and identically distributed (i.i.d.) with probability distribution P(xj = 1) = p for some ﬁxed 0 ≤ p ≤ 1.
If p is small, a realization of x is likely a sparse binary signal.
In this work, we shall consider the second model which covers dense binary signals with large support as well as sparse binary signals.
Below we give a short review on CS/BCS methods that are related to our work.
A.
ℓ0 minimization (L0) A naive approach to ﬁnding sparse solutions is the ℓ0 minimization, min z∈RN kzk0 subject to Φz = y.
(P0) This method works generally for continuously-valued signals i.e., signals x ∈ RN whose entries are that are sparse, mostly zero.
However, solving ℓ0 minimization requires a combinatorial search and is therefore NP-hard [14].
B.
Smoothed ℓ0 minimization (SL0) z2 z2 The Smoothed ℓ0 minimization (SL0) [12] replaces the ℓ0 norm in (P0) with a non-convex relaxation: Φz = y Φz = y min z∈RN This is motivated by the observation that which implies for any z = (z1, .
.
.
, zN )⊤ ∈ RN , lim σ→0 NXi=1(cid:18)1 − exp(cid:18) −z2 2σ2(cid:19)(cid:19) subject to Φz = y.
2σ2(cid:21) =(1 exp(cid:20) −t2 NXi=1(cid:18)1 − exp(cid:20) −z2 i=1(cid:16)1 − exph −z2 2σ2(cid:21)(cid:19) = kzk0.
2σ2i(cid:17) is a smooth func- if t = 0 if t 6= 0, lim σ→0 (2) Noticing that z 7→PN tion for any ﬁxed σ > 0, Mohimani et al.
[12] proposed an algorithm based on the gradient descent method.
The algorithm iteratively obtains an approximate solution by decreasing σ.
Mohammadi et al.
[15] adapted the SL0 algorithm par- ticularly to non-negative signals.
Their algorithm, called the Constrained Smoothed ℓ0 method (CSL0), incorporates the non-negativity constraints by introducing appropriate weight functions into their cost function.
Empirically, they showed that CSL0 achieves better performance than SL0 for recon- struction of non-negative signals.
C.
Basis Pursuit (BP) A well-known and by now standard relaxation of (P0) is the ℓ1-minimization, also known as the Basis Pursuit (BP) [16]: min z∈RN kzk1 subject to Φz = y.
(P1) Similarly as continuously-valued signals x ∈ RN that are sparse.
this method works generally for (P0), D.
Boxed Basis Pursuit (Boxed BP) Donoho et al.
[13] proposed the Boxed Basis Pursuit (Boxed BP) for the reconstruction of k-simple bounded signals: min z∈[0,1]N kzk1 subject to Φz = y.
The intuition behind Boxed BP is straightforward: the ℓ1 norm minimization promotes sparsity of the solution while the restriction z ∈ [0, 1]N reduces the set of feasible solutions.
Recently, Keiper et al.
[9] analyzed the performance of Boxed BP for reconstruction of binary signals.
E.
Sum of Norms (SN) Wang et al.
[7] introduced the following optimization prob- lem which combines the ℓ1 and the ℓ∞ norm: min z∈RN kzk1 + λkz − · 1N k∞ subject to Φz = y.
As before, minimizing the term kzk1 promotes sparsity of z.
On the other hand, minimizing the term kz − 1 2 · 1N k∞ forces the entries |zi − 1 2 | to be small but of equal magnitude.
These facts are illustrated in Fig.
1.
The two norms are balanced by a tuning parameter λ > 0.
( 1 2 , 1 2 ) z1 (1, 0) z1 (1, 0) Fig.
1.
Left: the minimization of kzk1 ﬁnds sparse solutions, Right: the minimization of kz − 1 2 | to be small but of equal magnitude.
2 · 1N k∞ forces the entries |zi − 1 (0, p) (1, 1 − p) zi Fig.
2.
The contribution of zi to the function (1 − p)kzk1 + pkz − 1N k1.
F.
Sum of Absolute Values (SAV) Nagahara [8] proposed the following method for reconstruc- tion of discrete signals whose entries are chosen independently from a set of ﬁnite alphabets α = {α1, α2, .
.
.
, αL} with a priori known probability distribution.
In the special case α = {0, 1} of binary signals, SAV is formulated as, min z∈RN (1 − p)kzk1 + pkz − 1N k1 subject to Φz = y, where p = P(xj = 1), j ∈ [N ], is the probability distribution of the entries of x.
Here, the contribution of each entry zi to the function (1 − p)kzk1 + pkz − 1N k1 is given by (Fig.
2) if zi < 0, −zi + p (1 − 2p)zi + p if 0 ≤ zi < 1, zi − p if zi ≥ 1.
 If p ≈ 0, i.e., x is sparse, then (1−p)kzk1+pkz−1N k1 ≈ kzk1 which corresponds to BP.
III.
BOX-CONSTRAINED SUM OF SMOOTHED ℓ0 Note that L0 and SL0 utilize the ℓ0 norm and its smoothed version respectively, however, they do not exploit the fact that x is binary.
On the other hand, BP, Boxed BP, SN, and SAV utilize the ℓ1 norm in one way or another and all methods except BP are particularly adjusted to the binary setting.
A natural question arises whether one can also adjust L0 and SL0 to the binary setting and achieve a better recovery performance for binary signals.
We note that Boxed BP takes into account the binary prior of x by imposing the restriction x ∈ [0, 1]N .
It is straightforward to apply the same trick to L0 and SL0, which are then called Boxed L0 and Boxed SL0 respectively.
Note however that Boxed L0 is again NP-hard and is therefore not applicable.
Fig.
3 shows that Boxed SL0 achieves an improvement in recovery performance over SL0 while requiring a similar run time.
However, its error rate appears to be far worse than Boxed BP and SN.
In this paper, we aim to adapt the formulation of SAV and the restriction x ∈ [0, 1]N to SL0, in order to achieve a better performance than that of existing algorithms.
A straightforward adaptation yields the following problem.
For σ > 0 small, min z∈[0,1]N Fσ(z) subject to Φz = y, (3) where Fσ(z) , (1 − p) NXi=1(cid:16)1 − e−z2 i /(2σ2)(cid:17) NXi=1(cid:16)1 − e−(zi−1)2/(2σ2)(cid:17) + p NXi=1(cid:16)1 − (1 − p) e−z2 i /(2σ2) − p e−(zi−1)2/(2σ2)(cid:17) and p = P(xj = 1), ∀j ∈ [N ].
Note that by (2), we have lim σ→0 Fσ(z) = (1 − p)kzk0 + pkz − 1N k0 so that F0(z) can be approximated by Fσ(z) with small σ > 0.
Next, we will use a weight function to incorporate the restriction z ∈ [0, 1]N into the function Fσ(z).
For integers k ≥ 1, let wk(t) ,(1 if 0 ≤ t ≤ 1 k otherwise.
(5) For σ > 0 and integers k ≥ 1, we deﬁne F boxed σ,k (z) NXi=1 wk(zi)(cid:16)1 − (1 − p) e−z2 i /(2σ2) − p e−(zi−1)2/(2σ2)(cid:17) .
Note that since 1 − (1 − p)e−t2/(2σ2) − p e−(t−1)2/(2σ2) > 0 for all t ∈ R, minimizing F boxed σ,k (z) forces wk(zi) to be small so that all zi’s lie within [0, 1].
In this way, the restriction z ∈ [0, 1]N is merged into the penalty function.
Our optimization problem now reads as follows.
For σ > 0 small and k ∈ N large, min z∈RN F boxed σ,k (z) subject to Φz = y.
with The algorithm proposed to solve the problem is based on the gradient descent method and its implementation is similar to that of algorithms in [12], [15].
A major differ- ence of our algorithm from theirs is that the cost function σ,k (z) which is designed particularly for binary valued signal, by adapting the formulation of SAV [8].
2σ2(cid:17)(cid:17) of SL0 is replaced with F boxed i=1(cid:16)1 − exp(cid:16) −z2 PN The proposed algorithm is comprised of two nested loops.
In the outer loop, we slowly decrease σ and iteratively search for an optimal solution from a coarse to a ﬁne scale by decreasing σ by a factor of 0 < d < 1.
As σ decreases, we also gradually increase k so that a larger penalty is put on solutions that Algorithm 1 Box-Constrained Sum of Smoothed ℓ0 (BSSL0) 1: Data: Measurement matrix Φ ∈ Rm×N , observation y ∈ Rm, probability distribution prior p = P(xj = 1); 2: Parameters: The minimal σ: σmin, the inner-loop iteration the gradient descent factor: µ, and the σ number: L, decreasing factor: d; 3: Initialization: ˆx = Φ⊤(ΦΦ⊤)−1 4: 5: for 1 : Iters do 6: Iters = logd(cid:0) σmin σ (cid:1), k = 1 + N p for 1 : L, do Iters ; y, σ = 2 max |ˆx|, ˆx ← ˆx − σ2µ∇F boxed σ,k (z) % Gradient descent ˆx ← ˆx − Φ⊤(ΦΦ⊤)−1(Φˆx − y) % Projection 7: 8: 9: 10: end for σ = σ × d k = k + N p Iters (4) 11: 12: end for 13: ˆx ← round(ˆx) % Round to a binary vector.
have entries outside the range [0, 1].
The inner loop performs a gradient descent of L iterations for the function F boxed σ,k (z), where σ and k are given from the outer loop.
In each iteration, the obtained solution is projected back into the set of feasible solutions {z : Φz = y}.
The numerical experiments in Section IV indicate that for binary signals the proposed algorithm outperforms SL0, as well as achieving better recovery performance than other CS/BCS algorithms (BP, Boxed BP, SN, and SAV).
As already mentioned, our algorithm is implemented anal- ogous to SL0 [12], [15] except for the penalty function.
The parameters used in our algorithm are exactly the same as in [12] except k and p.
As justiﬁed in [12, Section IV-B], we use the minimum ℓ2 norm solution of Φz = y as an initial estimate, that is, we initialize ˆx = Φ⊤(ΦΦ⊤)−1 y.
The initialization value for σ is discussed in [12, Remark 5 in Section III].
Also, the choice of the step-size σ2µ for the gradient descent is justiﬁed in [12, Remark 2 in Section III], and the choice of k is explained in [15, Lemma 1].
The gradient of F boxed σ,k (z), appearing in Algorithm 1, is obtained by σ,k (z) =  ∂F boxed ∇F boxed , .
.
.
, ∂F boxed σ,k (z) σ,k (z) ∂z1 ∂zN !⊤ σ2 (cid:18)(1 − p)zi exp(cid:20) −z2 2σ2(cid:21) (cid:21)(cid:19) a.e., +p (zi − 1) exp(cid:20) −(zi − 1)2 wk(zi) 2σ2 ∂F boxed σ,k (z) ∂zi where we used the fact that w′ k(t) = 0 for all t except t = 0, 1.
IV.
NUMERICAL EXPERIMENTS In this section, we empirically investigate the proposed algorithm BSSL0.
In particular, the performance of BSSL0 is compared with the state-of-the-art CS and BCS algorithms described in Section II.
Our experiment have been carried reconstruction accuracy.
At the same time, BSSL0 is several orders of faster compared with the baseline methods.
B.
Experiment 2: Bitonal Image Reconstruction We repeat the bitonal image reconstruction experiment designed in [8].
Consider the 37 × 37-pixel bitonal image shown in Fig.
4 (left).
Random Gaussian noise with a mean of 0 and a standard deviation of 0.1 is added to each pixel, and the resulted blurred image is shown in Fig.
4 (right).
Denote the disturbed image as X.
Fig.
4.
Original image (left) and noise-added image (right).
We apply the discrete Fourier transform (DFT) to the real valued matrix X.
This could be written as a linear equation: (W ⊗ W )vec(X) = vec( ˆX), where W is the DFT matrix.
Let y ∈ C685 be the half-size randomly downsampled vector of vec( ˆX).
Let the sensing matrix Φ ∈ C685×1369 be the matrix generated by corresponding randomly down- sampling row vectors from W ⊗W .
We present y and Φ to the optimization methods and reconstruct the image.
For SN, the tuning parameter λ is empirically tested from 50 to 1000 in the stepsize of 50, and the λ = 800 is chosen as it performed best.
For SAV and BSSL0, we have chosen p = P(xj = 0) = 0.5 as a rough estimate for the sparsity of the bitonal image (this estimate p = 0.5 was also used in [8]).
We set σmin = 0.01, d = 0.9, µ = 2, and L = 3 for the parameters of BSSL0.
out in MATLAB R2015a environment on a 2GHz Intel Core i7 Macintosh notebook computer with 8 GB RAM.
The codes for reproducing the experiments are available at https://github.com/liutianlin0121/BSSL0.
A.
Experiment 1: Binary Sparse Signal Reconstruction In this experiment, we reconstruct binary signal using BSSL0 and other baseline methods.
Let the measurement ma- trix Φ ∈ R40×100 be random Gaussian, that is, each component of Φ is drawn from the standard normal distribution.
For each p, varied from 0 to 1 with step-size 0.05, the binary signal x ∈ R100 is generated with all its entries drawn independently by P(xi = 1) = p and P(xi = 0) = 1 − p.
With Φ and x generated, we compute the measurement vector y = Φx and apply the respective algorithms (BP, Boxed BP, SN, SAV, SL0, Boxed SL0, BSSL0) to get the reconstructed signal z, which is an estimate of x.
Note that Boxed SL0 is a special case of CSL0 that constrains all signals to the boxed range [0,1].
The following measures are considered for the evaluation of reconstruction result: (i) Failure of Perfect Reconstruction (FPR): 1 if z 6= x (failed to perfectly reconstruct) and 0 otherwise (succeed to perfectly reconstruct); (ii) Noise Signal Ratio (NSR): NSR = kx−zk2 (as deﬁned in [8]).
(iii) Run time kxk2 measured by the tic/toc command in MATLAB as a coarse estimation of the complexities of the algorithms.
For a ﬁxed p, the process up to this point is repeated 10000 times and evaluation results for the measurements are averaged.
For SN, we set the parameter λ to be 100 as ﬁne-tuned in [7].
For BSSL0, we let σmin = 0.1, d = 0.5, µ = 2, and L = 1000.
0.5 0.5 0.5 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8 1 − p BP Boxed BP SN SAV SL0 Boxed SL0 BSSL0 Fig.
3.
Upper ﬁgure: Failure of Perfect Recovery; middle ﬁgure: NSR; lower ﬁgure: Run time From Fig.
3 we see that, compared with the baseline meth- ods, BSSL0 yield better reconstruction results in all metrics of Fig.
5.
Reconstructed images by the BP (upper left), by SN (upper right), by SAV (lower left), and by proposed BSSL0 (lower right).
TABLE I THE RUN TIME COMPARISON Optimization Method Basis Pursuit SN SAV BSSL0 (proposed) Run Time 185.2044 seconds 406.1007 seconds 191.5366 seconds 0.92577 seconds [14] B.
K.
Natarajan, “Sparse approximate solutions to linear systems,” SIAM Journal on Computing, vol.
24, no.
2, pp.
227–234, 1995.
[15] M.
Mohammadi, E.
Fatemizadeh, and M.
H.
Mahoor, “Non-negative sparse decomposition based on constrained smoothed ℓ0 norm,” Signal Processing, vol.
100, pp.
42–50, 2014.
[16] S.
S.
Chen, D.
L.
Donoho, and M.
A.
Saunders, “Atomic decomposition by basis pursuit,” SIAM Review, vol.
43, no.
1, pp.
129–159, 2001.
The reconstructed ﬁgures from BP, SN, SAV, and BSSL0 is shown in Fig.
5.
The time consumption is shown in Tab.
I.
The ﬁgures and the table show that the proposed BSSL0 signiﬁcantly substantially enhances the reconstruction accuracy as well as the speed.
V.
CONCLUSION In this paper, we proposed a novel algorithm BSSL0 for reconstructing binary signals from reduced measurements.
The algorithm is derived based on smooth relaxation techniques and gradient descent methods.
By experiments, we show that the proposed algorithm outperforms the state-of-the-art convex optimization methods in its reconstruction accuracy as well as in its speed.
ACKNOWLEDGMENT T.
Liu and D.
G.
Lee acknowledge the support by the DFG Grant PF 450/6-1.
The authors are grateful to Robert Fischer and G¨otz E.
Pfander for helpful suggestions.
REFERENCES [1] D.
L.
Donoho, “Compressed sensing,” IEEE Transactions on Informa- tion Theory, vol.
52, no.
4, pp.
1289–1306, 2006.
[2] S.
Foucart and H.
Rauhut, A Mathematical Introduction to Compressive Sensing.
Basel: Birkh¨auser, 2013.
[3] D.
Bickson, D.
Baron, A.
Ihler, H.
Avissar, and D.
Dolev, “Fault identiﬁcation via nonparametric belief propagation,” IEEE Transactions on Signal Processing, vol.
59, no.
6, pp.
2602–2613, 2011.
[4] M.
F.
Duarte, M.
A.
Davenport, D.
Takbar, J.
N.
Laska, T.
Sun, K.
F.
Kelly, and R.
G.
Baraniuk, “Single-pixel imaging via compressive sampling,” IEEE signal Processing Magazine, vol.
25, no.
2, pp.
83–91, 2008.
[5] K.
Wu and X.
Guo, “Compressive sensing of digital sparse signals,” in Wireless Communications and Networking Conference (WCNC), 2011 IEEE.
IEEE, 2011, pp.
1488–1492.
[6] U.
Nakarmi and N.
Rahnavard, “Bcs: Compressive sensing for binary IEEE, sparse signals,” in Military Communications Conference 2012.
2012, pp.
1–5.
[7] S.
Wang and N.
Rahnavard, “Binary compressive sensing via sum of ℓ1-norm and ℓ∞-norm regularization,” in Military Communications Conference 2013 IEEE.
IEEE, 2013, pp.
1616–1621.
[8] M.
Nagahara, “Discrete signal reconstruction by sum of absolute values,” IEEE Signal Processing Letters, vol.
22, no.
10, pp.
1575–1579, 2015.
[9] S.
Keiper, G.
Kutyniok, D.
G.
Lee, and G.
E.
Pfander, “Compressed sensing for ﬁnite-valued signals,” Linear Algebra and its Applications, 2017.
[10] J.-H.
Lange, M.
E.
Pfetsch, B.
M.
Seib, and A.
M.
Tillmann, “Sparse recovery with integrality constraints,” arXiv preprint arXiv:1608.08678, 2016.
[11] D.
L.
Donoho and Y.
Tsaig, “Fast solution of ell1-norm minimization problems when the solution may be sparse,” IEEE Transactions on Information Theory, vol.
54, no.
11, pp.
4789–4812, 2008.
[12] H.
Mohimani, M.
Babaie-Zadeh, and C.
Jutten, “A fast approach for overcomplete sparse decomposition based on smoothed ℓ0 norm,” IEEE Transactions on Signal Processing, vol.
57, no.
1, pp.
289–301, 2009.
[13] D.
L.
Donoho and J.
Tanner, “Precise undersampling theorems,” Pro- ceedings of the IEEE, vol.
98, no.
6, pp.
913–924, 2010.

In response to the susceptibility of neural networks to adver- sarial examples (Szegedy et al., 2013), there has been signif- icant interest recently in constructing defenses to increase the robustness of neural networks.
While progress has been made in understanding and defending against adversarial ex- amples, a complete solution has not yet been found.
To the best of our knowledge, all defenses against adversarial ex- amples published in peer-reviewed venues to date (Papernot et al., 2016; Hendrik Metzen et al., 2017; Hendrycks & Gim- pel, 2017; Meng & Chen, 2017; Zantedeschi et al., 2017) are vulnerable to powerful optimization-based attacks (Carlini & Wagner, 2017c;b;a).
As benchmarking against iterative optimization-based at- tacks such as BIM (Kurakin et al., 2016a), PGD (Madry et al., 2018), and Carlini and Wagner’s attack (Carlini & Wagner, 2017c) has become standard practice in evaluating potential defenses, new defenses have arisen that appear to withstand the most powerful optimization-based attacks.
*Equal contribution 1Massachusetts Institute of Technol- ogy 2University of California, Berkeley.
Correspondence to: Anish Athalye <aathalye@mit.edu>, Nicholas Carlini <npc@berkeley.edu>.
We identify one common reason why many defenses provide apparent robustness against iterative attacks: obfuscated gra- dients.
Without a good gradient signal, optimization-based methods cannot succeed.
We identify three types of obfus- cated gradients.
Some defenses cause shattered gradients, either intentionally through non-differentiable operations or unintentionally through numerical instability, resulting in a nonexistent or incorrect gradient signal.
Some defenses are randomized, causing stochastic gradients that depend on test-time entropy unavailable to the attacker.
Other defenses cause vanishing/exploding gradients (Bengio et al., 1994), resulting in an unusable gradient signal.
We propose new techniques to overcome obfuscated gradi- ents caused by these three phenomenon.
We address gra- dient shattering due to non-differentiable operations with a new attack technique we call Backward Pass Differentiable Approximation.
We compute gradients of randomized de- fenses by applying Expectation Over Transformation (Atha- lye et al., 2017).
We solve vanishing/exploding gradients through reparameterization and optimize over a space where vanishing/exploding gradients are not an issue.
To investigate the prevalence of obfuscated gradients and understand the applicability of these attack techniques, we use the ICLR 2018 defenses as a case study.
We ﬁnd that obfuscated gradients are a common occurrence, with 7 of 8 accepted defenses relying on this phenomenon.
Applying the new attack techniques we develop, we overcome obfus- cated gradients and successfully circumvent all 7 of them.
Along with this, we offer an analysis of the evaluations performed in the papers.
Additionally, we hope to provide researchers with a common baseline of knowledge, description of attack techniques, and common evaluation pitfalls, so that future defenses can avoid falling vulnerable to these same attacks.
To promote reproducible research, we release our re- implementation of each of these defenses, along with imple- mentations of our attacks for each.
1 https://github.com/anishathalye/obfuscated-gradients Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples 2.
Preliminaries 2.1. Notation We consider a neural network f (·) used for classiﬁcation where f (x)i represents the probability image x corresponds to label i.
We classify images, represented as x ∈ [0, 1]w·h·3 for a 3-color image of width w and height h.
We use f j(·) to refer to layer j of the neural network, and f 1..j(·) the application of layers 1 through j.
We denote the classiﬁca- tion of the network as c(x) = arg maxif (x)i where the true label of image x is written c∗(x).
2.2. Adversarial Examples Given an image x and classiﬁer f (·), an adversarial example (Szegedy et al., 2013) x(cid:48) satisﬁes two properties: D(x, x(cid:48)) is small for some distance metric D, and c(x(cid:48)) (cid:54)= c∗(x).
That is, for images, x and x(cid:48) appear visually similar but x(cid:48) is classiﬁed incorrectly.
For this paper we use the (cid:96)∞ and (cid:96)2 distortion metrics to measure similarity.
Two images which have a small dis- tortion under either of these metrics will appear visually identical.
We report (cid:96)∞ distance in the normalized [0, 1] space, so that a distortion of 0.031 corresponds to 8/256, and (cid:96)2 distance as the total root-mean-square distortion nor- malized by the total number of pixels.
2.3. Datasets & Models We evaluate these defenses on the same datasets on which they claim robustness: MNIST (LeCun, 1998) for Saman- gouei et al.
(2018), CIFAR-10 (Krizhevsky & Hinton, 2009) for Madry et al.
(2018); Song et al.
(2018); Ma et al.
(2018); Buckman et al.
(2018); Dhillon et al.
(2018), and ImageNet (Krizhevsky et al., 2012) for Guo et al.
(2018); Xie et al.
(2018).
If a defense argues security on MNIST and any other dataset, we only circumvent the defense on the larger dataset.
On MNIST and CIFAR-10, we evaluate defenses over the entire test set and generate untargeted adversarial examples.
On ImageNet, we evaluate over 1000 randomly selected images in the test set, and construct targeted ad- versarial examples with randomly selected target classes.
2 Generating targeted adversarial examples is a strictly harder problem which we believe is also a more meaningful metric, especially for this dataset.
We use standard models for each dataset.
For MNIST we use a standard convolutional neural network which reaches 99.3% accuracy.
On CIFAR-10 we train a wide resnet (Zagoruyko & Komodakis, 2016; He et al., 2016) to 95% accuracy.
For ImageNet we use the InceptionV3 (Szegedy 2Misclassiﬁcation is a less meaningful metric on this dataset, where a misclassiﬁcation of closely related classes, e.g. a German shepherd classiﬁed as a Doberman, may not be meaningful.
et al., 2016) network which reaches 78.0% top-1 and 93.9% top-5 accuracy.
2.4. Attack Methods construct adversarial examples with iterative We optimization-based methods.
At a high level, for a given instance x, optimization attacks attempt to search for a δ such that c(x + δ) (cid:54)= c∗(x) either minimizing (cid:107)δ(cid:107), or maximizing the loss on f (x + δ).
To generate (cid:96)∞ bounded adversarial examples we use Projected Gradient Descent (PGD); for (cid:96)2, we use a Lagrangian relaxation: Carlini and Wagner’s formulation (Carlini & Wagner, 2017c).
The speciﬁc choice of optimizer (e.g., gradient descent or Adam) and regularization (e.g., (cid:96)∞-regularized or (cid:96)2-regularized) is far less important than using optimization-based methods (Madry et al., 2018).
3.
Obfuscated Gradients At a high level, a defense obfuscates gradients when travel- ing in the direction suggested by the gradient is not a useful direction to travel in to construct an adversarial example.
We discover three ways in which defenses cause obfuscated gradients.
We brieﬂy deﬁne and discuss each of them.
Shattered Gradients are caused when a defense is non- differentiable, introduces numeric instability, or otherwise causes the true gradient signal to be incorrect.
Defenses that cause gradient shattering often do so unintentionally, by introducing operations that are differentiable but where the gradient does not point in the direction that actually maximizes classiﬁcation loss.
Stochastic Gradients are caused by randomized defenses, where either the network itself is randomized or the input is randomized before being fed to the classiﬁer.
Evaluat- ing the gradient multiple times gives different results each time.
This can cause single-step methods as well as opti- mization methods using a single sample of the randomness to incorrectly estimate the true gradient direction and fail to converge to a minima of the randomized classiﬁer.
Exploding & Vanishing Gradients are often caused by defenses that consist of multiple iterations of neural network evaluation.
Because this can be viewed as an extremely deep neural network evaluation, it is easy to see why gradients can either vanish or explode.
3.1. Identifying Obfuscated Gradients In some cases it may be obvious that a defense contains a non-differentiable operation.
However, in other cases, it may not be immediately clear.
We discuss below several symptoms and diagnostic tests to help detect obfuscated gradients.
While we do not claim these tests will perfectly Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples cover all cases, we ﬁnd that every defense we are aware of that obfuscates gradients fails at least one of these tests.
Check that iterative attacks are better than single step.
Iterative optimization-based attacks applied in a white-box setting are strictly stronger than single-step attacks and should give strictly superior performance.
If single-step methods give performance superior to multi-step methods, it is likely that the multi-step attack is becoming stuck in its optimization search at a local minimum.
Verify white-box attacks work better than black-box.
The black-box threat model is a strict subset of the white- box threat model, and so attacks in the white-box setting should perform better; however, if a defense is obfuscating gradients, then often times black-box attacks which do not require gradient signals will perform better than white-box attacks.
Ensure unbounded attacks reach 100% success.
With unbounded distortion, any classiﬁer should have 0% robust- ness to adversarial examples (as long as the classiﬁer is not a constant function).
If attacks do not reach 100% success, this indicates the defense is defeating the attack in a subtle manner and may not actually be increasing robustness.
Perform brute-force random sampling.
A ﬁnal simple method to identify obfuscated gradients is to perform brute- force search for adversarial examples (105 or more) within some -ball of each clean image.
If random search ﬁnds adversarial examples not found with optimization methods, the defense is likely to be obfuscating gradients.
4.
Attack Techniques Generating adversarial examples through optimization- based methods requires useful gradients obtained through backpropagation (Rumelhart et al., 1986).
Many defenses therefore either intentionally or unintentionally cause gradi- ent descent to fail because of obfuscated gradients caused by gradient shattering, stochastic gradients, or vanish- ing/exploding gradients.
We discuss a number of techniques that we develop to overcome obfuscated gradients.
4.1. Backward Pass Differentiable Approximation Shattered gradients can be caused either unintentionally, e.g. by numerical instability, or intentionally, e.g. by us- ing non-differentiable operations.
To attack defenses where gradients are not readily available, we introduce a tech- nique we call Backward Pass Differentiable Approximation (BPDA).
3 3The BPDA approach can be used on an arbitrary network, even if it is already differentable, to obtain a more useful gradient.
4.1.1. SPECIAL CASE Many non-differentiable defenses can be expressed as fol- lows: given a pre-trained classiﬁer f (·), construct a prepro- cessor g(·) and let the secured classiﬁer ˆf (x) = f (g(x)) where the preprocessor g(·) satisﬁes g(x) ≈ x (e.g., such a g(·) may perform image denoising to remove the adversar- ial perturbation).
If g(·) is smooth and differentiable, then computing gradients through the combined network ˆf is often sufﬁcient to circumvent the defense (Carlini & Wag- ner, 2017b).
However, recent work has constructed various functions g(·) which are neither smooth nor differentiable, and therefore can not be backpropigated through to generate adversarial examples with a white-box attack.
We introduce a new attack that we call Backward Pass Differ- entiable Approximation.
Because g is constructed with the property that g(x) ≈ x, we can approximate its derivative as the derivative of the identity function: ∇xg(x) ≈ ∇xx = 1.
Therefore, we can approximate the derivative of f (g(x)) at the point ˆx as: ∇xf (g(x))|x=ˆx ≈ ∇xf (x)|x=g(ˆx) This allows us to compute gradients and therefore mount a white-box optimization attack.
Conceptually, this attack is simple.
We perform forward propagation through the neural network as usual, but on the backward pass, we replace g(·) with the identity function.
In practice, the implementation can be expressed in an even simpler way: we approximate ∇xf (g(x)) by evaluating ∇xf (x) at the point g(x).
This gives us an approximation of the true gradient, and while not perfect, is sufﬁciently useful that when averaged over many iterations of gradient descent still generates an adversarial example.
4.1.2. GENERALIZED ATTACK While the above attack is effective for a simple class of networks expressable as f (g(x)) when g(x) ≈ x, it is not fully general.
We now generalize the above approach.
Let h(·) be a smooth, differentable function, so that f (x) ≈ h(x).
To approximate ∇xf (x) perform the forward pass y = f (x) as is done typically.
However, to perform the backward pass, backpropagate the value y through the function h(x).
As long as the two functions are similar, we ﬁnd that the slightly inaccurate gradients still prove useful in constructing an adversarial example.
In practice, we do not use the completely general construc- tion above.
Instead, for functions of the form f (g(x)), where g(x) ≈ h(x) and h(x) is differentiable, we approx- imate ∇xf (g(x)) by replacing g(·) with h(·) on the back- ward pass.
We have found applying BPDA is often necessary: replacing Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples g(·) with h(·) on both passes either is completely ineffective (e.g., with Song et al.
(2018)) or many times less effective (e.g. with Buckman et al.
(2018)).
4.2. Differentiating over Randomness Stochastic gradients arise when using randomized transfor- mations to the input before feeding it to the classiﬁer or when using a stochastic classiﬁer.
When using optimization- based attacks on defenses that employ these techniques, it is necessary to estimate the gradient of the stochastic function.
Expectation over Transformation.
For defenses that em- ploy randomized transformations to the input, we apply Ex- pectation over Transformation (EOT) (Athalye et al., 2017) to correctly compute the gradient over the expected trans- formation to the input.
When attacking a classiﬁer f (·) that ﬁrst randomly trans- forms its input according to a function t(·) sampled from a distribution of transformations T , EOT proposes optimizing the expectation over the transformation Et∼T f (t(x)).
The optimization problem can be solved by gradient descent, not- ing that ∇Et∼T f (t(x)) = Et∼T∇f (t(x)), differentiating through the classiﬁer and transformation, and approximat- ing the expectation with samples at each gradient descent step.
Stochastic classiﬁers.
For defenses that use stochastic classiﬁers, we correctly compute the gradient by computing gradients over the expectation of random parameters.
4.3. Reparameterization We solve vanishing/exploding gradients by reparameteriza- tion.
Assume we are given a classiﬁer f (g(x)) where g(·) performs some optimization loop to transform the input x to a new input ˆx.
Often times, this optimization loop means that differentiating through g(·), while possible, yields ex- ploding or vanishing gradients.
To resolve this, we make a change-of-variable x = h(z) for some function h(·) such that g(h(z)) = h(z) for all z, but h(·) is differentable.
For example, if g(·) projects samples to some manifold in a speciﬁc manner, we might construct h(z) to return points exclusively on the manifold.
This allows us to compute gradients through f (h(z)) and thereby circumvent the defense.
5.
Case Study: ICLR 2018 Defenses As a case study for evaluating the prevalence of obfuscated gradients, we study the ICLR 2018 defenses that argue robustness in a white-box threat model.
We ﬁnd that all but one of these defenses relies on this phenomenon to argue security, and we demonstrate that our techniques can Defense Buckman et al.
(2018) Ma et al.
(2018) Guo et al.
(2018) Dhillon et al.
(2018) Xie et al.
(2018) Song et al.
(2018) Samangouei et (2018) Madry et al.
(2018) al.
Dataset CIFAR CIFAR ImageNet CIFAR ImageNet CIFAR MNIST Distance 0.031 ((cid:96)∞) 0.031 ((cid:96)∞) 0.007 ((cid:96)2) 0.031 ((cid:96)∞) 0.031 ((cid:96)∞) 0.031 ((cid:96)∞) 0.005 ((cid:96)2) Accuracy 0%∗ 5% 1%∗ 0% 0%∗ 9%∗ 0% CIFAR 0.031 ((cid:96)∞) 47% Table 1.
Summary of Results: Seven of eight defense techniques accepted to ICLR 2018 cause obfuscated gradients and are vul- nerable to our attacks.
(Defenses denoted with ∗ also propose combining adversarial training; we report here the defense alone, see §5 for full numbers.) Figure 1.
Illustration of different distortion levels.
Row 1: Clean images.
Row 2: Adversarial examples, distortion  = 0.015.
Row 3: Adversarial examples, distortion  = 0.031.
circumvent each of those that rely on obfuscated gradients.
We omit two defenses with provable security claims and one that only argues black-box security.
We include one paper, Ma et al.
(2018), that was not proposed as a defense per se, but suggests a method to detect adversarial exampls.
There is an asymmetry in attacking defenses versus con- structing robust defenses: to show a defense can be by- passed, it is only necessary to demonstrate one way to do so; in contrast, a defender must show no attack succeeds.
We therefore only give one way to evade a defense when it can be done.
In Table 1 we summarize our results.
Of the eight accepted papers, can be bypassed with our techniques because they rely on obfuscated gradients.
Two of these defenses argue robustness on ImageNet, a much harder task than CIFAR- 10; and one argues robustnes on MNIST, a much easier task than CIFAR-10.
As such, comparing defenses across datasets is difﬁcult.
Throughout this section we use a (cid:96)∞ distortion of 0.015 and 0.031 on CIFAR-10 and ImageNet; we show in Figure 1 what such a distortion looks like for comparison.
Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples 5.1. A Secured Classiﬁer 5.1.1. ADVERSARIAL TRAINING where τ (xi,j,c)k = The ﬁrst paper we consider (Madry et al., 2018) trains a high- capacity neural network to classify adversarial examples generated with optimization methods.
We ﬁnd that this approach does not cause gradient descent to fail in artiﬁcial ways.
Defense Details.
Originally proposed by Szegedy et al.
(2013), adversarial training is a conceptually simple pro- cess.
Given training data X and loss function (cid:96)(·), standard training chooses network weights θ as θ∗ = arg min Ex∈X (cid:96)(x; Fθ).
Adversarial training instead chooses an -ball and solves the min-max formulation θ∗ = arg min (cid:96)(x + δ; Fθ) Ex∈X (cid:20) (cid:21) max δ∈[−,]N To approximately solve this formulation, Madry et al.
(2018) solve the inner maximization problem by generating adver- sarial examples the training data.
Discussion.
The evaluation the authors perform for this defense pass all of our sanity checks.
We ﬁnd this approach does not cause obfuscated gradients and we are unable to substantially invalidate any of the claims made.
However, we make two important observations about this defense: (1) we note that adversarial retraining has been shown to be difﬁcult at ImageNet scale (Kurakin et al., 2016b); and (2) training exclusively on l∞ adversarial examples provides only limited robustness to adversarial examples under other distortion metrics.
5.2. Gradient Shattering 5.2.1. THERMOMETER ENCODING Thermometer encoding (Buckman et al., 2018) is a encod- ing scheme designed to break the local linearity of neural networks.
We ﬁnd as a consequence it also causes gradient shattering and causes traditional optimization-based attacks to fail.
Defense Details.
In contrast to prior work (Szegedy et al., 2013) which viewed adversarial examples as “blind spots” in neural networks, Goodfellow et al.
(2014b) argue that the reason adversarial examples exist is that neural networks be- have in a largely linear manner.
The purpose of thermometer encoding is to break this linearity.
Given an image x, for each pixel color xi,j,c, the l-level thermometer encoding τ (xi,j,c) is a l-dimensional vector (cid:40) if xi,j,c > k/l otherwise For example, for a 10-level thermometer encoding, we have τ (0.66) = 1111110000.
The training process is identical between normal and thermometer networks.
Due to the discrete nature of thermometer encoded val- ues, it is not possible to directly perform gradient descent on a thermometer encoded neural network.
The authors therefore construct Logit-Space Projected Gradient Ascent (LS-PGA) as an attack over the discrete thermometer en- coded inputs.
Using this attack, the authors perform the adversarial training of Madry et al.
(2018) on thermometer encoded networks.
On CIFAR-10, just performing thermometer encoding was found to give 50% accuracy within  = 0.031 under (cid:96)∞ distortion.
By performing adversarial training with 7 steps of LS-PGA, robustness increased to 80%.
Discussion.
While the intention behind this defense is to break the local linearity of neural networks, we ﬁnd that this defense in fact causes gradient shattering.
This can be observed through their black-box attack evaluation: ad- versarial examples generated on a standard adversarially trained model transfer to a thermometer encoded model re- ducing the accuracy to 67%, well below the 80% robustness to the white-box iterative attack.
Evaluation.
We use the BPDA approach from Sec- tion 4.1.2, where we let g(x) = τ (x).
Observe that if we deﬁne ˆτ (xi,j,c)k = min(max(xi,j,c − k/l, 0), 1) then τ (xi,j,c)k = ﬂoor(ˆτ (xi,j,c)k) trained without adversarial so we can let h(x) = ˆτ (x) and replace the backwards pass with the function h(·).
LS-PGA reduces model accuracy to 50% on a thermometer- encoded model training (bounded by  = 0.031).
In constrast, we achieve 1% model accuracy with the lower  = 0.015 (and 0% with  = 0.031).
This shows no measurable improvement from standard models, trained without thermometer encoding.
When we attack a thermometer-encoded adversarially trained model 4, we are able to reproduce the 80% accu- racy at  = 0.031 claim against LS-PGA.
However, our attack reduces model accuracy to 20%.
This is signiﬁcantly weaker than the 50% rate of success on the original Madry et al.
(2018) model.
Because this model is trained against 4That is, a thermometer encoded model that is trained using the approach of (Madry et al., 2018).
Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples where k is a defense hyperparameter the controls the num- ber of nearest neighbors to consider.
The authors use the distance function dj(x, y) =(cid:13)(cid:13)f 1..j(x) − f 1..j(y)(cid:13)(cid:13) 2 to measure the distance between the jth activation layers.
The authors compute a vector of LID values for each sample: −−→ LID(x) = {LIDdj (x)}n j=1.
−−→ Finally, they compute the LID(x) over the training data and adversarial examples generated on the training data, and train a logistic regression classiﬁer to detect adversarial examples.
We are grateful to the authors for releasing their complete source code.
Discussion.
While LID is not a defense itself, the authors assess the ability of LID to detect different types of attacks.
Through solving the formulation min.
|x − x(cid:48)|2 2 + α ((cid:96)(x(cid:48)) + LID-loss(x(cid:48))) the authors attempt to determine if the LID metric is a good metric for detecting adversarial examples.
Here, LID-loss(·) is a function that can be minimized to reduce the LID score.
However, the authors report that this modiﬁed attack still achieves 0% success.
Because Carlini and Wagner’s (cid:96)2 attack is unbounded, any time the attack does not reach 100% success indicates that the attack became stuck in a local minima.
When this happens, it is often possible to slightly modify the loss function and return to 100% attack success (Carlini & Wagner, 2017b).
In this case, we observe the reason that performing this type of adaptive attack fails is that the LID computation is, while differentiable, not differentiable in a useful maner.
Computing the LID term involves computing the k-nearest neighbors when computing ri(x).
Minimizing the gradient of the distance to the current k-nearest neighbors is not representative of the true direction to travel in for the optimal set of k-nearest neighbors.
As a consequence, we ﬁnd that adversarial examples generated with gradient methods when penalizing for a high LID either (a) are not adversarial; or (b) are detected as adversarial, despite penalizing for the LID loss.
Evaluation.
We now evaluate what would happen if a defense would directly apply LID to detect adversarial ex- amples.
Instead of performing gradient descent over a term that is difﬁcult to differentiate through, we have found that generating high conﬁdence adversarial examples (Carlini & Wagner, 2017a) (completely oblivious to to the detector) is sufﬁcient to fool this detector.
We obtain from the authors their detector trained on both the Carlini and Wagner’s (cid:96)2 Figure 2.
Model accuracy versus distortion (under l∞).
Adversar- ial training increases robustness to 50% at  = 0.031; thermometer encoding by itself provides limited value, and when coupled with adversarial training performs worse than adversarial training alone.
the (comparatively weak) LS-PGA attack, it is unable to adapt to the stronger attack we present above.
5.2.2. LOCAL INTRINSIC DIMENSIONALITY (LID) The next paper studies properties of adversarial exam- ples (Ma et al., 2018).
They examine LID, a metric that measures the distance from an input compared to its neigh- bors, and suggest that LID might be useful for detecting adversarial examples.
They present evidence that the LID is signiﬁcantly larger for adversarial examples generated by existing attacks than for normal images, and they construct a classiﬁer that can distinguish these adversarial images from normal images.
The authors emphasize that this classiﬁer is not intended as a defense against adversarial examples 5.
However, it would be natural to wonder whether it would be effective as a defense, so we study its robustness; our results conﬁrm that it is not adequate as a defense.
The method used to compute the LID relies on ﬁnding the k nearest neighbors, a non-differentiable operation, rendering gradient descent based methods ineffective.
Defense Details.
The Local Intrinsic Dimensionality (Amsaleg et al., 2015) “assesses the space-ﬁlling capability of the region surrounding a reference example, based on the distance distribution of the example to its neighbors” (Ma et al., 2018).
Let S be a mini-batch of N clean exam- ples.
Let ri(x) denote the distance (under metric d(x, y)) between sample x and its i-th nearest neighbor in S (under metric d).
Then LID can be approximated by (cid:32) LIDd(x) = − (cid:33)−1 k(cid:88) i=1 log ri(x) rk(x) 5Personal communication with authors.
0.000.010.020.03Perturbation Magnitude0.00.20.40.60.81.0Model AccuracyBaselineThermometerAdv.
TrainAdv.
ThermObfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples attack and train our own on the Fast Gradient Sign attack, both of which were found to be effective at detecting adver- sarial examples generated by other methods.
By generating high-conﬁdence adversarial examples minimizing (cid:96)∞ dis- tortion, we are able to reduce model accuracy to 2% success within  = 0.015.
LID reports these adversarial examples are benign at a 97% rate (unmodiﬁed test data is ﬂagged as benign with a 98% rate).
This evaluation demonstrates that the LID metric can be circumvented, and future work should carefully evaluate if building a detector relying on LID is robust to adversarial examples explicitly targeting such a detector.
This work also raises questions whether a large LID is a fundamental characteristic of all adversarial examples, or whether it is a by-product of certain attacks.
5.2.3. INPUT TRANSFORMATIONS Guo et al.
(2018) defend against adversarial examples through input transformations.
They explore a number of functions for modifying the input before it is fed to the classiﬁer.
We ﬁnd each of these transformations is non- differentiable and causes gradient shattering.
Defense Details.
Guo et al.
(2018) propose ﬁve input transformations to counter adversarial examples: aging over multiple runs.
• Perform random image cropping and rescaling, aver- • Quantize images with bit-depth reduction.
• Apply JPEG compression to remove perturbations.
• Randomly drop pixels, and restore them by performing total variance minimization.
• Image quilting: Reconstruct images by replacing all 5 × 5 patches with patches from “clean” images, using minimum graph cuts in overlapping boundary regions to remove edge artifacts.
The authors explore different combinations of input trans- formations along with different underlying ImageNet clas- siﬁers, including adversarially trained models.
They ﬁnd that input transformations provide protection even with a vanilla classiﬁer, providing varying degrees of robustness for varying transformations and normalized (cid:96)2 perturbation budgets.
Discussion.
The authors ﬁnd that a ResNet-50 classiﬁer provides a varying degree of accuracy for each of the ﬁve proposed input transformations 6 under the strongest attack with a normalized (cid:96)2 dissimilarity of 0.01, with the strongest defenses achieving over 60% top-1 accuracy.
We observe similar results when evaluating an InceptionV3 classiﬁer.
The authors do not succeed in white-box attacks, credit- ing lack of access to test-time randomness as “particularly crucial in developing strong defenses” (Guo et al., 2018).
7 Evaluation.
It is possible to bypass each defense inde- pendently.
We circumvent image cropping and rescaling with a direct application of Expectation Over Transforma- tion (Athalye et al., 2017).
To circumvent bit-depth reduc- tion, JPEG compression, total variance minimization, and image quilting, we use BPDA to approximate the backward pass with the identity function.
Using our attack, classi- ﬁcation accuracy drops to 0.5% for the strongest defense under a perturbation budget 3 times smaller than the small- est perturbation budget considered in Guo et al.
(2018), a root-mean-square perturbation of 0.007 (corresponding to a “normalized” (cid:96)2 perturbation as deﬁned in Guo et al.
(2018) of 0.0003).
5.3. Stochastic Gradients 5.3.1. STOCHASTIC ACTIVATION PRUNING (SAP) SAP randomly drops neurons at every layer of the network proportional to their absolute magnitude (Dhillon et al., 2018).
By using sampling to compute gradients of the expec- tation over values of randomness, we generate adversarial examples that successfully attack this defense.
Defense Details.
SAP randomly drops some neurons of each layer f i to 0 according with probability proportional to their absolute value.
For the hidden activation vector hi = f 1..i(x) at layer i, SAP deﬁnes a probability distribution (cid:32) m(cid:88) (cid:33)−1 j = |hi j| · pi hi k=1 j is proportional to the magnitude of hi That is, pi j compared to the magnitude of the other neurons at this layer.
SAP then computes a modiﬁed distribution j = 1 − (1 − pi j)r qi where r is a defense hyperparameter (discussed below).
Then, each hi with probability qi j is updated to a new value ˆhi j and keeping it otherwise j by dropping it  hi qi ˆhi j = with probability qi with probability 1 − qi 6The authors apply image cropping/rescaling, bit-depth reduc- tion, and JPEG compression as baselines (Personal communication with authors).
7This defense may be stronger in a threat model where the adversary does not have complete information about the exact quilting process used (Personal communication with authors).
Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples so that values less likely to be sampled are scaled up accord- ingly.
The value r is chosen to keep a large enough fraction of the neurons that the accuracy remains high, but not so large that all neurons are kept.
We follow the authors advice and choose r so the test accuracy drops by only 5%.
Discussion.
The authors only evaluate SAP by taking a single step in the gradient direction (Dhillon et al., 2018).
While taking a single step in the direction of the gradient is effective on non-randomized neural networks, when ran- domization is used, computing the gradient with respect to one sample of the randomness is ineffective.
direction of(cid:80)k Evaluation.
To resolve this difﬁcult, we estimate the gra- dients by computing the expectation over instantiations of randomness.
At each iteration of gradient descent, instead of taking a step in the direction of ∇xf (x) we move in the i=1 ∇xf (x) where each invocation is ran- domized with SAP.
We have found that choosing k = 10 provides useful gradients.
We additionally had to resolve a numerical instability when computing gradients: this de- fense caused computing a backward pass to cause exploding gradients due to division by numbers very close to 0.
We re- solve this by clipping gradients or through stable numerical techniques.
With these approaches, we are able to reduce SAP model accuracy to 9% at  = .015, and 0% at  = 0.031.
If we consider an attack successful only when an example is classiﬁed incorrectly 10 times out of 10 (and consider it correctly classiﬁed if it is ever classiﬁed as the correct label), model accuracy is below 10% with  = 0.031.
5.3.2. MITIGATING THROUGH RANDOMIZATION Xie et al.
(2018) defend against adversarial examples by randomly rescaling and padding images so that natural im- ages retain their classiﬁcation while adversarial examples are sufﬁciently perturbed to lose their adversarial nature.
We circumvent this defense by using EOT to synthesize robust adversarial examples (Athalye et al., 2017).
Defense Details.
(Xie et al., 2018) propose to defend against adversarial examples by adding a randomization layer before the input to the classiﬁer.
For a classiﬁer that takes a 299 × 299 input, the defense ﬁrst randomly rescales the image to a r × r image, with r ∈ [299, 331), and then randomly zero-pads the image so that the result is 331×331.
The output is then fed to the classiﬁer.
Discussion.
The authors consider three attack scenarios: vanilla attack (an attack on the original classiﬁer), single- pattern attack (an attack assuming some ﬁxed randomization pattern), and ensemble-pattern attack (an attack over a small ensemble of ﬁxed randomization patterns).
The authors strongest attack reduces InceptionV3 model accuracy to 32.8% top-1 accuracy (over images that were originally classiﬁed correctly).
The authors dismiss a stronger attack over larger choices of randomness, stating that it would be “computationally impossible” (emphasis ours) and that such an attack “may not even converge” (Xie et al., 2018).
Evaluation.
We ﬁnd the authors ensemble-pattern attack overﬁts to the ensemble with ﬁxed randomization.
We by- pass this defense by applying Expectation over Transforma- tion (Athalye et al., 2017), optimizing over the (in this case, discrete) distribution of transformations T and minimizing Et∼T f (t(x)).
We approximate the gradient of the above by sampling and differentiating through the transformation.
Using this attack, even if we consider the attack successful only when an example is classiﬁed incorrectly 10 times out of 10, we can reduce the accuracy of the classiﬁer from 32.8% to 0.0% with a maximum (cid:96)∞ perturbation of  = 0.031.
5.4. Vanishing & Exploding Gradients 5.4.1. PIXELDEFEND Song et al.
(2018) propose using a PixelCNN generative model to project a potential adversarial example back onto the data manifold before feeding it into a classiﬁer.
We bypass this defense using BPDA.
Defense Details.
Song et al.
(2018) argue that adversarial examples mainly lie in the low-probability region of the training distribution.
PixelDefend “puriﬁes” adversarially perturbed images by projecting them back onto the data manifold through the use of a PixelCNN generative model, and then it feeds the resulting image through an unmodiﬁed classiﬁer.
PixelDefend uses a greedy decoding procedure to approximate ﬁnding the highest probability example within an -ball of the input image.
Discussion.
The authors evaluate PixelDefend on CIFAR- 10 over a number of underlying classiﬁers, perturbation budgets, and attack algorithms.
With a maximum (cid:96)∞ perturbation of  = 0.031 on CIFAR- 10, PixelDefend claims 46% accuracy (with a vanilla ResNet classiﬁer).
The authors dismiss the possibility of end-to-end attacks on PixelDefend due to the difﬁculty of differentiating through an unrolled version of PixelDefend due to vanishing gradients and computation cost.
Evaluation.
We sidestep the problem of computing gradi- ents through an unrolled version of PixelDefend by approxi- mating gradients with BPDA, approximating the backward Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples pass with the identity function, and we successfully mount an end-to-end attack using this technique 8.
With this attack, we can reduce the accuracy of a naturally trained classiﬁer which achieves 95% accuracy to 9% with a maximum (cid:96)∞ perturbation of  = 0.031.
Using the adversarially trained classiﬁer of Madry et al.
(2018), using PixelDefend provides no additional robustness over just using the adversarially trained classiﬁer.
5.4.2. DEFENSE-GAN Defense-GAN (Samangouei et al., 2018) uses a Generative Adversarial Network (Goodfellow et al., 2014a) to project samples onto the manifold of the generator before classi- fying them.
We use reparameterization to circumvent the multi-step projection process and construct adversarial ex- amples.
Defense Details.
The defender ﬁrst trains a Generative Adversarial Network with a generator G(z) that maps sam- ples from a latent space (typically z ∼ N (0, 1)) to images that look like training data.
Defense-GAN takes a trained classiﬁer f (·), and to classify an input x, instead of return- ing f (x), returns f (arg minz |G(z) − x|).
To perform this projection to the manifold, the authors take many steps of gradient descent starting from different random initializa- tions.
Defense-GAN was not shown to be effective on CIFAR-10.
We therefore evaluate it on MNIST (where it was argued to be secure).
Discussion.
In Samangouei et al.
(2018), the authors con- struct a white-box attack by unrolling the gradient descent used during classiﬁcation.
Despite an unbounded (cid:96)2 pertur- bation size, Carlini and Wagner’s attack only reaches 30% misclassiﬁcation rate on the most vulnerable modele and under 5% on the strongest.
This leads us to believe that unrolling gradient descent breaks gradients.
Evaluation.
Performing the manifold projection is non- trivial as an inner optimization step when generating adver- sarial examples.
To sidestep this difﬁculty, we show that adversarial examples exist directly on the data manifold.
That is, we construct an adversarial example x(cid:48) = G(z∗) so that |x − x(cid:48)| is small and c(x) (cid:54)= c(x(cid:48)).
To do this, we solve the re-parameterized formulation min.
(cid:107)G(z) − x(cid:107)2 2 + c · (cid:96)(G(z)).
8In place of a PixelCNN, due to the availability of a pre-trained model, we use a PixelCNN++ (Salimans et al., 2017) and discretize the mixture of logistics to produce a 256-way softmax over possible pixel values.
Due to compute limitations, we evaluate our attack over a random sample of 500 images from the test set.
Figure 3.
Images on the MNIST test set.
Row 1: Clean images.
Row 2: Adversarial examples on an unsecured classiﬁer.
Row 3: Adversarial examples on Defense-GAN.
We initialize z = arg minz |G(z)− x| (also found via gradi- ent descent).
We train a WGAN using the code the authors provide (Gulrajani et al., 2017), and a MNIST CNN to 99.3% accuracy.
We run for 50k iterations of gradient descent for generating each adversarial example; this takes under one minute per instance.
The unsecured classiﬁer requires a mean (cid:96)2 distor- tion of 0.0019 (per-pixel normalized, 1.45 un-normalized) to fool.
When we mount our attack on Defense-GAN, we require a mean distortion of 0.0027, an increase in distortion of 1.46×; see Figure 3 for examples of adversarial examples.
The reason our attacks succeed with 100% success without suffering from vanishing or exploding gradients is that our gradient computation only needs to differentiate through the generator G(·) once.
Concurrent to our work, Ilyas et al.
(2017) also develop a nearly identical approach to Defense-GAN; they also ﬁnd it is vulnerable to the attack we outline above, but increase the robustness further with adversarial training.
We do not evaluate this extended approach.
6.
Discussion Having demonstrated attacks on these seven papers, we now take a step back and describe what we believe to be a complete method of evaluating a defense to adversarial examples.
Much of the advice we give has been given in prior work (Carlini & Wagner, 2017a; Madry et al., 2018); we repeat it here and offer our own perspective for com- pleteness.
We hope future work can use this as a guide for performing an evaluation.
6.1. Deﬁne a (realistic) threat model When constructing a defense, it is critical to deﬁne a threat model that limits the adversary.
Prior work used words white-box, grey-box, black-box, no-box to describe slightly different threat models, often overloading the same word.
Instead of attempting to, yet again, redeﬁne the vocabulary, we outline the various aspects of a defense that might be Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples revealed to the adversary or held secret to the defender: 6.3. Evaluate against adaptive attacks • Model architecture and Model weights • Training algorithm and Training data • For defenses that involve randomness, whether the adversary knows the exact sequence of random values that will be chosen, or only the distribution.
• If assuming the adversary does not know the model architecture and weights, if query access is allowed.
If so, if the model output is the logits, probability vector, or predicted label (i.e., arg max).
While there are some aspects of a defense that might be held secret, threat models should not contain unrealistic constraints.
We believe any compelling threat model will at the very least grant knowledge of the model architecture, training algorithm, and allow query access.
We do not believe it is meaningful to restrict the the compu- tational power of an adversary.
If two defenses are equally robust but generating adversarial examples on one takes one second and another takes ten seconds, the robustness has not increased.
Only if the adversary’s computational effort can be shown to be increased exponentially faster than pre- diction runtime might it be it acceptable to use runtime as a security parameter.
However, increasing attack time by a few seconds (as occurs in the defenses we attack in this paper) is not meaningful.
6.2. Make speciﬁc, testable claims After deﬁning a clear threat model, a defense should make make speciﬁc, testable claims.
For example, a defense might claim 90% robustness to (cid:96)∞ adversarial examples of distortion at most  = 0.031, or might claim that mean (cid:96)2 distortion to adversarial examples increases by a factor of two from a baseline model to a secured model (in which case, the baseline should also be clearly deﬁned).
Unfortunately, many of the defenses we evaluate simply claim they are robust without giving speciﬁc bounds.
The biggest violation of this advice is that defense should never claim complete robustness against unbounded attacks: with unlimited distortion any image can be converted into any other, yielding 100% “success”.
In order to allow the claims to be testable, the defense must be speciﬁed completely, with all hyperparameters given.
Releasing source code and a pre-trained model along with the paper is perhaps the most useful method of making ex- plicit claims.
Four of the defenses we study made complete source code available (Madry et al., 2018; Ma et al., 2018; Guo et al., 2018; Xie et al., 2018).
Claiming increased robustness to existing attacks, while speciﬁc and testable, is not by itself a useful claim.
It is important to actively evaluate one’s own defense with new defense-aware attacks to justify claims of security.
In particular, once a defense has been completely speciﬁed, it is important to attempt to circumvent that concrete defense, assuming only that the adversary is restricted to the threat model.
If it can be circumvented, then it is important to not give ways to prevent that speciﬁc attack (i.e., by tweaking a hyperparameter).
After an evaluation, it is acceptable to modify the defense, but then a new attack should be built to target the newly modiﬁed defense.
In this way, concluding an evaluation with a ﬁnal adaptive attack can be seen as analogous to evaluating a model on the test data.
7.
Conclusion Constructing defenses to adversarial examples requires de- fending against not only existing attacks but also future attacks that may be developed.
In this paper, we identify obfuscated gradients, a phenomenon exhibited by certain defenses that makes standard gradient-based methods fail to generate adversarial examples.
We develop three attack techniques to bypass three different types of obfuscated gra- dients.
To evaluate the applicability of our techniques, we use the ICLR 2018 defenses as a case study, circumventing seven of eight accepted defenses.
More generally, we hope that future work will be able to avoid relying on obfuscated gradients for perceived robust- ness and use our evaluation approach to detect when this occurs.
Defending against adversarial examples is an impor- tant area of research and we believe performing a thorough evaluation is a critical step that can not be overlooked.
Acknowledgements We are grateful to Aleksander Madry and Andrew Ilyas for helpful comments on an early draft of this paper.
We thank Bo Li, Xingjun Ma, Laurens van der Maaten, Aurko Roy, Yang Song, and Cihang Xie for useful discussion and insights on their defenses.
This work was partially supported by the National Science Foundation through award CNS-1514457, Qualcomm, and the Hewlett Foundation through the Center for Long-Term Cybersecurity.
References Amsaleg, Laurent, Chelly, Oussama, Furon, Teddy, Girard, St´ephane, Houle, Michael E, Kawarabayashi, Ken-ichi, and Nett, Michael.
Estimating local intrinsic dimension- Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples ality.
In Proceedings of the 21th ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining, pp.
29–38.
ACM, 2015.
Athalye, Anish, Engstrom, Logan, Ilyas, Andrew, and Kwok, Kevin.
Synthesizing robust adversarial examples.
arXiv preprint arXiv:1707.07397, 2017.
Bengio, Y., Simard, P., and Frasconi, P.
Learning long-term dependencies with gradient descent is difﬁcult.
IEEE Transactions on Neural Networks, 5(2):157–166, Mar 1994.
ISSN 1045-9227.
doi: 10.1109/72.279181.
Buckman, Jacob, Roy, Aurko, Raffel, Colin, and Good- fellow, Ian.
Thermometer encoding: One hot way to International Conference resist adversarial examples.
on Learning Representations, 2018.
URL https:// openreview.net/forum?id=S18Su--CW.
ac- cepted as poster.
Carlini, Nicholas and Wagner, David.
Adversarial examples are not easily detected: Bypassing ten detection methods.
AISec, 2017a.
Carlini, Nicholas and Wagner, David.
Magnet and “efﬁcient defenses against adversarial attacks” are not robust to adversarial examples.
arXiv preprint arXiv:1711.08478, 2017b.
Carlini, Nicholas and Wagner, David.
Towards evaluating the robustness of neural networks.
In IEEE Symposium on Security & Privacy, 2017c.
Dhillon, Guneet S., Azizzadenesheli, Kamyar, Bernstein, Jeremy D., Kossaiﬁ, Jean, Khanna, Aran, Lipton, Zachary C., and Anandkumar, Animashree.
Stochastic In- activation pruning for robust adversarial defense.
ternational Conference on Learning Representations, 2018.
URL https://openreview.net/forum?
id=H1uR4GZRZ.
accepted as poster.
Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley, David, Ozair, Sherjil, Courville, Aaron, and Bengio, Yoshua.
Generative adversarial nets.
In Advances in neural information processing systems, pp.
2672–2680, 2014a.
Goodfellow, Ian J, Shlens, Jonathon, and Szegedy, Christian.
Explaining and harnessing adversarial examples.
arXiv preprint arXiv:1412.6572, 2014b.
Guo, Chuan, Rana, Mayank, Cisse, Moustapha, and van der Maaten, Laurens.
Countering adversarial images us- International Conference ing input transformations.
on Learning Representations, 2018.
URL https: //openreview.net/forum?id=SyJ7ClWCb. ac- cepted as poster.
He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian.
Deep residual learning for image recognition.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016.
Hendrik Metzen, Jan, Genewein, Tim, Fischer, Volker, and Bischoff, Bastian.
On detecting adversarial perturbations.
In International Conference on Learning Representations, 2017.
Hendrycks, Dan and Gimpel, Kevin.
Early methods for detecting adversarial images.
In International Conference on Learning Representations (Workshop Track), 2017.
Ilyas, Andrew, Jalal, Ajil, Asteri, Eirini, Daskalakis, Con- stantinos, and Dimakis, Alexandros G.
The robust mani- fold defense: Adversarial training using generative mod- els.
arXiv preprint arXiv:1712.09196, 2017.
Krizhevsky, Alex and Hinton, Geoffrey.
Learning multiple layers of features from tiny images.
2009.
Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.
ImageNet classiﬁcation with deep convolutional neural networks.
In Advances in neural information processing systems, pp.
1097–1105, 2012.
Kurakin, Alexey, Goodfellow, Ian, and Bengio, Samy.
Ad- versarial examples in the physical world.
arXiv preprint arXiv:1607.02533, 2016a.
Kurakin, Alexey, Goodfellow, Ian J., and Bengio, Samy.
Adversarial machine learning at scale.
arXiv preprint arXiv:1611.01236, 2016b.
LeCun, Yann.
The mnist database of handwritten digits.
http://yann.lecun.com/exdb/mnist/, 1998.
Ma, Xingjun, Li, Bo, Wang, Yisen, Erfani, Sarah M., Wijewickrema, Sudanthi, Schoenebeck, Grant, Houle, Michael E., Song, Dawn, and Bailey, James.
Characteriz- ing adversarial subspaces using local intrinsic dimension- ality.
International Conference on Learning Represen- tations, 2018.
URL https://openreview.net/ forum?id=B1gJ1L2aW.
accepted as oral presenta- tion.
Gulrajani, Ishaan, Ahmed, Faruk, Arjovsky, Martin, Du- moulin, Vincent, and Courville, Aaron.
Improved training of wasserstein gans.
arXiv preprint arXiv:1704.00028, 2017.
Madry, Aleksander, Makelov, Aleksandar, Schmidt, Lud- wig, Tsipras, Dimitris, and Vladu, Adrian.
Towards deep learning models resistant to adversarial attacks.
International Conference on Learning Representations, Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples Zantedeschi, Valentina, Nicolae, Maria-Irina, and Rawat, Ambrish.
Efﬁcient defenses against adversarial attacks.
arXiv preprint arXiv:1707.06728, 2017.
2018.
URL https://openreview.net/forum?
id=rJzIBfZAb. accepted as poster.
Meng, Dongyu and Chen, Hao.
MagNet: a two-pronged de- fense against adversarial examples.
In ACM Conference on Computer and Communications Security (CCS), 2017.
arXiv preprint arXiv:1705.09064.
Papernot, Nicolas, McDaniel, Patrick, Wu, Xi, Jha, Somesh, and Swami, Ananthram.
Distillation as a defense to ad- versarial perturbations against deep neural networks.
In Security and Privacy (SP), 2016 IEEE Symposium on, pp.
582–597.
IEEE, 2016.
Rumelhart, David E., Hinton, Geoffrey E., and Williams, Ronald J.
Learning representations by back-propagating errors.
Nature, 323:533–536, 1986.
Salimans, Tim, Karpathy, Andrej, Chen, Xi, and Kingma, Diederik P.
Pixelcnn++: A pixelcnn implementation with discretized logistic mixture likelihood and other modiﬁ- cations.
In ICLR, 2017.
Samangouei, Pouya, Kabkab, Maya, and Chellappa, Rama.
Defense-gan: Protecting classiﬁers against adversarial at- tacks using generative models.
International Conference on Learning Representations, 2018.
URL https:// openreview.net/forum?id=BkJ3ibb0-.
ac- cepted as poster.
Song, Yang, Kim, Taesup, Nowozin, Sebastian, Ermon, Stefano, and Kushman, Nate.
Pixeldefend: Leveraging generative models to understand and defend against adver- sarial examples.
International Conference on Learning Representations, 2018.
URL https://openreview.
net/forum?id=rJUYGxbCW.
accepted as poster.
Szegedy, Christian, Zaremba, Wojciech, Sutskever, Ilya, Bruna, Joan, Erhan, Dumitru, Goodfellow, Ian, and Fer- gus, Rob.
Intriguing properties of neural networks.
ICLR, 2013.
Szegedy, Christian, Vanhoucke, Vincent, Ioffe, Sergey, Shlens, Jon, and Wojna, Zbigniew.
Rethinking the in- ception architecture for computer vision.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
2818–2826, 2016.
Xie, Cihang, Wang, Jianyu, Zhang, Zhishuai, Ren, Zhou, and Yuille, Alan.
Mitigating adversarial effects through randomization.
International Conference on Learning Representations, 2018.
URL https://openreview.
net/forum?id=Sk9yuql0Z.
accepted as poster.
Zagoruyko, Sergey and Komodakis, Nikos.
Wide residual networks.
arXiv preprint arXiv:1605.07146, 2016.

D EEP learning (DL) networks take the form of heuristic and rich architectures that develop unique intermediate data representation.
The complexity of architectures is re- ﬂected by both the sizes of layers and, for a large number of data sets reported in the literature, also by the processing.
In fact, the architectural complexity and the excessive number of weights and units are often built in into the DL data representation by design and are deliberate [1–5].
Although deep architectures are capable of learning highly complex mappings, they are difﬁcult to train, and it is usually hard to interpret what each layer has learnt.
Moreover, gradient- based optimization with random initialization used in training is susceptible to converging to local minima [6], [7].
In addition, it is generally believed that humans analyze complex interactions by breaking them into isolated and understandable hierarchical concepts.
The emergence of part- based representation in human cognition can be conceptually tied to the nonnegativity constraints [8].
One way to enable easier human understandability of concepts in neural networks is to constrain the network’s weights to be nonnegative.
Note that such representation through nonnegative weights of a multilayer network perceptron can implement any shattering B.
O.
Ayinde is with the Department of Electrical and Computer En- gineering, University of Louisville, Louisville, KY, 40292 USA (e-mail: babajide.ayinde@louisville.edu).
J.
M.
Zurada is with the Department of Electrical and Computer Engineer- ing, University of Louisville, Louisville, KY, 40292 USA, and also with the Information Technology Institute, University of Social Science,Ł´odz 90-113, Poland (Corresponding author, e-mail: jacek.zurada@louisville.edu).
This work was supported in part by the NSF under grant 1641042.
of points provided suitable negative bias values are used [9].
Drawing inspiration from the idea of Nonnegative Matrix Factorization (NMF) and sparse coding [8], [10], the hidden structure of data can be unfolded by learning features that have capabilities to model the data in parts.
Although NMF enforces the encoding of both the data and features to be nonnegative thereby resulting in additive data representation, however, incorporating sparse coding within NMF for the purpose of encoding data is computationally expensive, while with AEs, this incorporation is learning-based and fast.
In addition, the performance of a deep network can be enhanced using Nonnegativity Constrained Sparse Autoencoder (NCAE) with part-based data representation capability [11], [12].
It is remarked that weight regularization is a concept that has been employed both in the understandability and generaliza- tion context.
It is used to suppress magnitudes of the weights by reducing the sum of their squares.
Enhancement in sparsity can also be achieved by penalizing sum of absolute values of the weights rather than the sum of their squares [13–17].
In this paper, the work proposed in [11] is extended by modifying the cost function to extract more sparse features, encouraging nonnegativity of the network weights, and enhancing the understandability of the data.
Other related model is the Nonnegative Sparse Autoencoder (NNSAE) trained with an online algorithm with tied weights and linear output activation function to mitigate the training hassle [18].
While [18] uses a piecewise linear decay function to enforce nonnegativity and focuses on shallow architecture, the proposed uses a composite norm with focus on deep architectures.
Dropout is another recently introduced and widely used heuristic to sparsify AEs and prevent overﬁtting by randomly dropping units and their connections from the neural network during training [19], [20].
More recently, different paradigm of AEs that constrain the output of encoder to follow a chosen prior distribution have been proposed [21–23].
In variational autoencoding, the decoder is trained to reconstruct the input from samples that follow chosen prior using variational inference [21].
Realistic data points can be reconstructed in the original data space by feeding the decoder with samples from chosen prior distribution.
On the other hand, adversarial AE matches the encoders output distribution to an arbitrary prior distribution using adversarial training with discriminator and the generator [22].
Upon adversarial training, encoder learns to map data distribution to the prior distribution.
The problem addressed here is three-fold: (i) The inter- pretability of AE-based deep layer architecture fostered by en- forcing high degree of weight’s nonnegativity in the network.
This improves on NCAEs that show negative weights despite imposing nonnegativity constraints on the network’s weights [11].
(ii) It is demonstrated how the proposed architecture can be utilized to extract meaningful representations that unearth the hidden structure of a high-dimensional data.
(iii) It is shown that the resulting nonnegative AEs do not deteriorate their classiﬁcation performance.
This paper considerably ex- pands the scope of the AE model ﬁrst introduced in [24] by: (i) introducing smoothing function for L1 regularization for numerical stability, (ii) illustrating the connection between the proposed regularization and weights’ nonnegativity, (iii) drawing more insight into variety of dataset, (iv) comparing the proposed with recent AE architectures, and lastly (v) sup- porting the interpretability claim with new experiments on text categorization data.
The paper is structured as follows: Section II introduces the network conﬁguration and the notation for nonnegative sparse feature extraction.
Section III discusses the experimental designs and Section IV presents the results.
Finally, conclusions are drawn in Section V.
II.
NONNEGATIVE SPARSE FEATURE EXTRACTION USING CONSTRAINED AUTOENCODERS As shown in [8], one way of representing data is by shattering it into various distinct pieces in a manner that additive merging of these pieces can reconstruct the original data.
Mapping this intuition to AEs, the idea is to sparsely disintegrate data into parts in the encoding layer and subse- quently additively process the parts to recombine the original data in the decoding layer.
This disintegration can be achieved by imposing nonnegativity constraint on the network’s weights [11], [25], [26].
A.
L1/L2-Nonnegativity Constrained Sparse Autoencoder (L1/L2-NCSAE) In order to encourage higher degree of nonnegativity in network’s weights, a composite penalty term (1) is added to the objective function resulting in the cost function expression for L1/L2-NCSAE: n(cid:48)(cid:88) (cid:0)W, b(cid:1) = JAE + β sl(cid:88) 2(cid:88) r=1 (cid:13)(cid:13)(cid:13)(cid:13) 1 (cid:18) sl+1(cid:88) fL1/L2 m(cid:88) (cid:0)w(l) k=1 ij (cid:1) DKL hr(x(k)) JL1/L2-NCSAE l=1 i=1 j=1 (1) where W = {W(1), W(2)} and b = {bx, bh} represent the weights and biases of encoding and decoding layers respec- tively; sl is the number of neurons in layer l.
w(l) represents the connection between jth neuron in layer l−1 and ith neuron ij in layer l and for given input x, (cid:13)(cid:13)(cid:13)σ(W(2)σ(W(1)x(k) + bx) + bh) − x(k)(cid:13)(cid:13)(cid:13)2 (2) || (cid:5) ||2 is where m is the number of training examples, the Euclidean norm.
DKL((cid:5)) is the Kullback-Leibler (KL) divergence for sparsity control [27] with p denoting the desired m(cid:88) JAE = k=1 activation and the average activations of hidden units, n(cid:48) is the number of hidden units, hj(x(k)) = σ(W(1) j x(k) + bx,j) denotes the activation of hidden unit j due to input x(k), and σ((cid:5)) is the element-wise application of the logistic sigmoid, σ(x) = 1/(1 + exp(−x)), β controls the sparsity penalty term, and (cid:40) fL1/L2 (wij) = α1Γ(wij, κ) + ||wij||2 α2 wij < 0 wij ≥ 0 (3) where α1 and α2 are L1 and L2 nonnegativity-constraint weight penalty factors, respectively.
p, β, α1, and α2 are experimentally set to 0.05, 3, 0.0003, and 0.003, respectively using 9000 randomly sampled images from the training set as a held-out validation set for hyperparameter tuning and the network is retrained on the entire dataset.
The weights are updated as below using the error backpropagation: w(l) ij = w(l) ij − ξ ∂w(l) ij JL1/L2-NCSAE(W, b) b(l) i = b(l) i − ξ ∂b(l) JL1/L2-NCSAE(W, b) (4) (5) where ξ > 0 is the learning rate and the gradient of L1/L2- NCSAE loss function is computed as in (6).
∂w(l) ij JL1/L2-NCSAE(W, b) = (cid:0)W, b(cid:1) (cid:13)(cid:13)(cid:13)(cid:13) 1 (cid:18) DKL JAE ∂w(l) ij + β + g(cid:0)w(l) ∂w(l) ij (cid:1) ij (cid:19) hj(x(k)) m(cid:88) k=1 (6) where g(wij) is a composite function denoting the derivative of fL1/L2(wij) (3) with respect to wij as in (7).
(cid:26) α1∇w (cid:107)wij(cid:107) + α2wij wij < 0 wij ≥ 0 (7) g(wij) = (cid:19) Although the penalty function in (1) is an extension of NCAE (obtained by setting α1 to zero), a close scrutiny of the weight distribution of both the encoding and decoding layer in NCAE reveals that many weights are still not nonnegative despite imposing nonnegativity constraints.
The reason for this is that the original L2 norm used in NCAE penalizes the negative weights with big magnitudes stronger than those with smaller magnitudes.
This forces a good number of the weights to take on small negative values.
This paper uses additional L1 to even out this occurrence, that is, the L1 penalty forces most of the negative weights to become nonnegative.
B.
Implication of imposing nonnegative parameters with com- posite decay function The graphical illustration of the relation between the weight distribution and the composite decay function is shown in 3 (a) (b) (c) (d) Fig.
1: (a) Symmetric (G3) and skewed (G1 and G2) weight distributions.
Decay function with three values of α1 and α2 for weight distribution (b) G3 (c) G1 and (d) G2.
Fig.
1.
Ideally, addition of Frobenius norm of the weight matrix (α||W||2 F ) to the reconstruction error in (2) imposes a Gaussian prior on the weight distribution as shown in curve G3 in Fig.
1a.
However, using the composite function in (3) results in imposition of positively-skewed deformed Gaussian distribution as in curves G1 and G2.
The degree of nonnegativity can be adjusted using parameters α1 and α2.
Both parameters have to be carefully chosen to enforce nonnegativity while simultaneously ensuring good supervised learning outcomes.
The effect of L1 (α2 = 0), L2 (α1 = 0) and L1/L2 (α1 (cid:54)= 0 and α2 (cid:54)= 0) nonnegativity penalty terms on weight updates for weight distributions G1, G2 and G3 are respectively shown in Fig.
1c,d, and b.
It can be observed for all the three distributions that L1/L2 regularization enforces stronger weight decay than individual L1 and L2 regulariza- tion.
Other observation from Fig.
1 is that the more positively- skewed the weight distribution becomes, the lesser the weight decay function.
The consequences of minimizing (1) are that: (i) the average reconstruction error is reduced (ii) the sparsity of the hidden layer activations is increased because more negative weights are forced to zero thereby leading to sparsity enhancement, and (iii) the number of nonnegative weights is also increased.
The resultant effect of penalizing the weights simultaneously with L1 and L2 norm is that large positive connections are preserved while their magnitudes are shrunk.
However, the L1 norm in (3) is non-differentiable at the origin, and this can lead to numerical instability during simulations.
To circumvent this drawback, one of the well known smoothing function that approximates L1 norm as in (3) is utilized.
Given any ﬁnite dimensional vector z and positive constant κ, the following smoothing function approximates L1 norm: ||z|| > κ ||z|| ≤ κ ||z|| > κ ||z|| ≤ κ (8) (9) (cid:40) ||z|| Γ(z, κ) = ||z||2 2κ with gradient ∇zΓ(z, κ) = (cid:40) z ||z|| For convenience, we adopt (8) to smoothen the L1 penalty function and κ is experimentally set to 0.1. III.
EXPERIMENTS the dataset In the experiments, three data sets are used, namely: MNIST [28], NORB normalized-uniform [29], and Reuters- 21578 text categorization dataset.
The Reuters-21578 text categorization dataset comprises of documents that featured in 1987 Reuters newswire.
The ModApte split was em- to 10 most frequent classes.
ployed to limit The ModApte split was utilized to limit the categories to 10 most frequent categories.
The bag-of-words format that has been stemmed and stop-word removed was used; see http://people.kyb.tuebingen.mpg.de/pgehler/rap/ for further clariﬁcation.
The dataset contains 11, 413 documents with 12, 317 dimensions.
Two techniques were used to reduce the dimensionality of each document in order to preserve the most informative and less correlated words [30].
To reduce the dimensionality of each document to contain the most informa- tive and less correlated words, words were ﬁrst sorted based on their frequency of occurrence in the dataset.
Words with frequency below 4 and above 70 were then eliminated.
The most informative words that do not occur in every topic were selected based on information gain with the class attribute.
The remaining words (or features) in the dataset were sorted using this method, and the less important features were removed based on the desired dimension of documents.
In this paper, the length of the feature vector for each of the documents was reduced to 200.
In the preliminary experiment, the subset 1, 2 and 6 from the MNIST handwritten digits as extracted for the purpose of understanding how the deep network constructed using L1/L2-NCSAE processes and classiﬁes its input.
For easy in- terpretation, a small deep network was constructed and trained by stacking two AEs with 10 hidden neurons each and 3 softmax neurons.
The number of hidden neurons was chosen to obtain reasonably good classiﬁcation accuracy while keeping the network reasonably small.
The network is intentionally kept small because the full MNIST data would require larger hidden layer size and this may limit network interpretability.
An image of digit 2 is then ﬁltered through the network, and it can be observed in Fig.
2 that sparsiﬁcation of the weights in all the layers is one of the aftermath of nonnegativity −1−0.500.5100.20.40.60.81weight wProbability Distribution P(w)  G1G2G3−1−0.500.51−2−1.5−1−0.50wg(w)   α1= 1,α2= 1α1= 1,α2= 0α1= 0,α2= 1−1−0.500.51−2−1.5−1−0.50wg(w)   α1= 1,α2= 1α1= 1,α2= 0α1= 0,α2= 1−0.500.511.5−1.4−1.2−1−0.8−0.6−0.4−0.20wg(w)   α1= 1,α2= 1α1= 1,α2= 0α1= 0,α2= 14 Fig.
2: Filtering the signal through the L1/L2-NCSAE trained using the reduced MNIST data set with class labels 1, 2 and 6.
The test image is a 28×28 pixels image unrolled into a vector of 784 values.
Both the input test sample and the receptive ﬁelds of the ﬁrst autoencoding layer are presented as images.
The weights of the output layer are plotted as a diagram with one row for each output neuron and one column for every hidden neuron in (L − 1)th layer.
The architecture is 784-10-10-3.
The range of weights are scaled to [-1,1] and mapped to the graycolor map.
w = −1 is assigned to black, w = 0 to grey, and w = 1 is assigned to white color.
That is, black pixels indicate negative, grey pixels indicate zero-valued weights and white pixels indicate positive weights.
constraints imposed on the network.
Another observation is that most of the weights in the network have been conﬁned to nonnegative domain, which removes opaqueness of the deep learning process.
It can be seen that the fourth and seventh receptive ﬁelds of the ﬁrst AE layer have dominant activations (with activation values 0.12 and 0.13 respectively) and they capture most information about the test input.
Also, they are able to ﬁlter distinct part of input digit.
The outputs of the ﬁrst layer sigmoid constitute higher level features extracted from test image with emphasis on the fourth and seventh features.
Subsequently in second layer the second, sixth, eight, and tenth neurons have dominant activations (with activation values 0.0914, 0.0691, 0.0607, and 0.0606 respectively) because they have stronger connections with the dominant neurons in ﬁrst layer than the rest.
Lastly in the softmax layer, the second neuron was 99.62% activated because it has strongest connections with the dominant neurons in second layer thereby classifying the test image as ”2”.
The fostering of interpretability is also demonstrated using a subset of NORB normalized-uniform dataset [29] with class labels ”four-legged animals”, ”human ﬁgures”, ”airplanes”.
The 1024-10-5-3 network conﬁguration was trained on the subset of the NORB data using two stacked L1/L2-NCSAEs and a Softmax layer.
Fig.
3b shows the randomly sampled test patterns and the weights and activations of ﬁrst and second AE layer are shown in Fig.
3a.
The bar charts indicate the activations of hidden units for the sample input patterns.
The features learned by units in each layer are localized, sparse and allow easy interpretation of isolated data parts.
The features mostly show nonnegative weights making it easier to visualize to what input object patterns they respond.
It can be seen that units in the network discriminate among objects in the images and react differently to input patterns.
Third, sixth, eight, and ninth hidden units of layer 1 capture features that are common to objects in class ”2” and react mainly to them as shown in the ﬁrst layer activations.
Also, the features captured by the second layer activations reveal that second and ﬁfth hidden units are mainly stimulated by objects in class ”2”.
The outputs of Softmax layer represent the a posteriori class probabilities for a given sample and are denoted as Softmax scores.
An important observation from Fig.
3a,b, and c is that hidden units in both layers did not capture signiﬁcant representative features for class ”1” white color-coded test sample.
This is one of the reasons why it is misclassiﬁed into class ”3” with probability of 0.57.
The argument also goes for class ”1” dark-grey color-coded test sample misclassiﬁed into class ”3” with probability of 0.60.
In contrast, hidden units in both layers capture signiﬁcant representative features for class ”2” test samples of all color codes.
This is why all class ”2” test samples are classiﬁed correctly with high probabilities as shown in Fig.
3d.
Lastly, the network contains a good number of representative features for class ”3” test samples and was able to classify 4 out of 5 correctly as given in Fig.
3e.
IV.
RESULTS AND DISCUSSION A.
Unsupervised Feature Learning of Image Data In the ﬁrst set of experiments, three-layer L1/L2-NCSAE, NCAE [11], DpAE [19], and conventional SAE network with 196 hidden neurons were trained using MNIST dataset of handwritten digits and their ability to discover patterns in Test sample    (Image) Weights and biases of hidden  neurons in Layer 1, each image is  formed from weights of a single  neuron 1.
The dot-products  of  the input and  Neuron weights in  Layer 1 2.
The bias is added,  then the sigmoid is  applied 3.
The bias is added,  then the sigmoid is applied  4.
The dot-product  with classification  layer weights.
Biases  are added 5.
Finally, the softmax  nonlinearity is applied  to get probabilities -5.881 -3.329 -3.169 -2.919 -3.163 -3.173 -3.098 -27.69 -3.567 -3.344 = 0.072 = 0.044 = 0.022 = 0.12 = 0.036 = 0.073 = 0.13 = 0.016 = 0.038 = 0.082 -3.917 -4.142 -3.550 -3.381 -3.699 -3.969 -3.410 -3.987 -3.899 -3.793 = 0.0425 = 0.0914 = 0.0468 = 0.0439 = 0.0393  = 0.0691 = 0.0528 = 0.0607 = 0.0401 = 0.0606 = 53.16 = 61.07 = 55.39 0.0004 for “1” 0.9962 for “2” 0.0034 for “6” Weights and biases of hidden  neurons in Layer 2.
Each  row is  a vector of weights of a single  neuron Matrix of classification weights  where each row represents one  output neuron  5 Fig.
3: The weights were trained using two stacked L1/L2-NCSAEs. RFs learned from the reduced NORB dataset are plotted as images at the bottom part of (a).
The intensity of each pixel is proportional to the magnitude of the weight connected to that pixel in the input image with negative value indicating black, positive values white, and the value 0 corresponding to gray.
The biases are not shown.
The activations of ﬁrst layer hidden units for the NORB objects presented in (b) are depicted on the bar chart on top of the RFs. The weights of the second layer AE are plotted as a diagram at the topmost part of (a).
Each row of the plot corresponds to the weight of each hidden unit of second AE and each column for weight of every hidden unit of the ﬁrst layer AE.
The magnitude of the weight corresponds to the area of each square; white indicates positive, grey indicates zero, and black negative sign.
The activations of second layer hidden units are shown as bar chart in the right-hand side of the second layer weight diagram.
Each column shows the activations of each hidden unit for ﬁve color-coded examples of the same object.
The outputs of Softmax layer for color-coded test objects with class labels (c) ”fourlegged animals” tagged as class 1, (d) ”human ﬁgures” as class 2, and (e) ”airplanes” as class 3.
high dimensional data are compared.
These experiments were run one time and recorded.
The encoding weights W(1), also known as receptive ﬁelds or ﬁlters as in the case of image data, are reshaped, scaled, centered in a 28 × 28 pixel box and visualized.
The ﬁlters learned by L1/L2-NCSAE are compared with that learned by its counterparts, NCAE and SAE.
It can be easily observed from the results in Fig.
4 that L1/L2-NCSAE learned receptive ﬁelds that are more sparse and localized than those of SAE, DpAE, and NCAE.
It is remarked that the black pixels in both SAE and DpAE features are results of the negative weights whose values and numbers are reduced in NCAE with nonnegativity constraints, which are further reduced by imposing an additional L1 penalty term in L1/L2-NCSAE as shown in the histograms located on the right side of the ﬁgure.
In the case of L1/L2-NCSAE, tiny strokes and dots which constitute the basic part of handwritten digits, are unearthed compared to SAE, DpAE, and NCAE.
Most of the features learned by SAE are major parts of the digits or the blurred version of the digits, which are obviously not as sparse as those learned by L1/L2-NCSAE.
Also, the features learned by DpAE are fuzzy compared to those of L1/L2-NCSAE which are sparse and distinct.
Therefore, the achieved sparsity in the encoding can be traced to the ability of L1 and L2 regularization in enforcing high degree of weights’ nonnegativity in the network.
Likewise in Fig.
5a, L1/L2-NCSAE with other AEs are compared in terms of reconstruction error, while varying the number of hidden nodes.
As expected, it can be observed that L1/L2-NCSAE yields a reasonably lower reconstruction error on the MNIST training set compared to SAE, DpAE, and NCAE.
Although, a close scrutiny of the result also reveals that the reconstruction error of L1/L2-NCSAE deteriorates compared to NCAE when the hidden size grows beyond 400.
However on the average, L1/L2-NCSAE reconstructs better than other AEs considered.
It can also be observed that DpAE with 50% dropout has high reconstruction error when the hidden layer size is relatively small (100 or less).
This is because the few neurons left are unable to capture the dy- namics in the data, which subsequently results in underﬁtting the data.
However, the reconstruction error improves as the hidden layer size is increased.
Lower reconstruction error in the case of L1/L2-NCSAE and NCAE is an indication that nonnegativity constraint facilitates the learning of parts of digits that are essential for reconstructing the digits.
In addition, the KL-divergence sparsity measure reveals that L1/L2-NCSAE has more sparse hidden activations than SAE, DpAE and NCAE for different hidden layer size as shown in Fig.
5b.
Again, averaging over all the training examples, L1/L2-NCSAE yields less activated hidden neurons compared to its counterparts.
Also, using t-distributed stochastic neighbor 3 2 1 3 2 1 1 2 3 (a) (b) (e) Softmax  scores (c) (d) 0.84    0.16      0.00 0.59    0.40      0.01 0.32    0.08      0.60 0.27    0.16      0.57 0.09    0.91      0.00 0.01    0.99      0.00 0.06    0.94      0.00 0.15    0.85      0.00 0.03    0.97      0.00 0.24    0.14      0.62 0.22    0.13      0.65 0.55    0.25      0.20 0.31    0.18      0.52 0.24    0.13      0.63 Weights of 5 hidden units in Layer 2 Weights of 10 hidden units in Layer 1 Activations of  Layer 2 hidden  units  Activations of  Layer 1 hidden  units  Class  1 images  Class  2 images  Class  3  images  0.87    0.13      0.00 6 (a) SAE (b) DpAE (c) NCAE (d) L1/L2-NCSAE Fig.
4: 196 receptive ﬁelds (W(1)) with weight histograms learned from MNIST digit data set using (a) SAE, (b) DpAE (c) NCAE, and (d) L1/L2-NCSAE.
Black pixels indicate negative, and white pixels indicate positive weights.
The range of weights are scaled to [-1,1] and mapped to the graycolor map.
w = −1 is assigned to black, w = 0 to grey, and w = 1 is assigned to white color.
(a) (b) Fig.
5: (a) Reconstruction error and (b) Sparsity of hidden units measured by KL-divergence using MNIST train dataset with p = 0.05.
embedding (t-SNE) to project the 196-D representation of MNIST handwritten digits to 2D space, the distribution of features encoded by 196 encoding ﬁlters of DpAE, NCAE, and L1/L2-NCSAE are respectively visualized in Figs.
6a, b, and c.
A careful look at Fig.
6a reveals that digits ”4” and ”9” are overlapping in DpAE, and this will inevitably increase the chance of misclassifying these two digits.
It can also be observed in Fig.
6b corresponding to NCAE that digit ”2” is projected with two different landmarks.
In sum, the manifolds of digits with L1/L2-NCSAE are more separable than its counterpart as shown in Fig.
6c, aiding the classiﬁer to map out the separating boundaries among the digits more easily.
In the second experiment, SAE, NCAE, L1/L2-NCSAE, and DpAE with 200 hidden nodes were trained using the NORB normalized-uniform dataset.
The NORB normalized- uniform dataset, which is the second dataset, contains 24, 300 training images and 24, 300 test images of 50 toys from 5 generic categories: four-legged animals, human ﬁgures, airplanes, trucks, and cars.
The training and testing sets consist of 5 instances of each category.
Each image consists of two channels, each of size 96× 96 pixels.
The inner 64× 64 pixels of one of the channels cropped out and resized using bicubic interpolation to 32 × 32 pixels that form a vector with 1024 entries as the input.
Randomly selected weights of 90 out of 200 neurons are plotted in Fig.
7.
It can be seen that L1/L2- NCSAE learned more sparse features compared to features learned by all the other AEs considered.
The receptive ﬁelds 100200300400500024681012No. of hidden nodesReconstruction error  SAENCAEL1/L2−NCSAEDpAE4955001.922.12.22.3  10020030040050000.020.040.060.080.10.120.140.160.18No. of hidden nodesKL−Divergence  SAENCAEL1/L2−NCSAEDpAE4804905002468x 10−3  7 Fig.
6: t-SNE projection [31] of 196D representations of MNIST handwritten digits using (a) DpAE (b) NCAE (c) L1/L2- NCSAE.
(a) SAE (b) DpAE (c) NCAE Fig.
7: Weights of randomly selected 90 out of 200 receptive ﬁlters of (a) SAE (b) DpAE (c) NCAE, and (d) L1/L2-NCSAE using NORB dataset.
The range of weights are scaled to [-1,1] and mapped to the graycolor map.
w <= −1 is assigned to black, w = 0 to grey, and w >= 1 is assigned to white color.
(d) L1/L2-NCSAE (a) (b) (c) Fig.
8: The distribution of 200 encoding (W(1)) and decoding ﬁlters (W(2)) weights learned from NORB dataset using (a) DpAE (b) NCAE (c) L1/L2-NCSAE.
learned by L1/L2-NCSAE captured the real actual edges of the toys while the edges captured by NCAE are fuzzy, and -0.15-0.1-0.0500.050.10.15010002000numberµ=-0.0027**-0.15-0.1-0.0500.050.10.15010002000numberµ=-0.0024**-0.6-0.4-0.200.20.40.60.801000200030004000numberAvg(W 1(i,j))= -0.0026*-0.6-0.4-0.200.20.40.60.8020004000numberAvg(W 2(i,j))=0.0826**-0.500.510500010000numberAvg(W 1(i,j))= 0.0017-0.500.510500010000numberAvg(W 2(i,j))=0.1573*8 Fig.
9: Visualizing 20D representations of a subset of Reuters Documents data using (a) DpAE, (b) NCAE, and (c) L1/L2- NCSAE.
(a) (b) Fig.
10: Deep network trained on Reuters-21578 data using (a) DpAE, (b) L1/L2-NCSAE.
The area of each square is proportional to the weight’s magnitude.
The range of weights are scaled to [-1,1] and mapped to the graycolor map.
w = −1 is assigned to black, w = 0 to grey, and w = 1 is assigned to white color.
those learned by DpAE and SAE are holistic.
As shown in the weight distribution depicted in Fig.
8, L1/L2-NCSAE has both its encoding and decoding weights centered around zero with most of its weights positive when compared with those of DpAE and NCAE that have weights distributed almost even on both sides of the origin.
B.
Unsupervised Semantic Feature Learning from Text Data In this experiment DpAE, NCAE, and L1/L2-NCSAE are evaluated and compared based on their ability to extract semantic features from text data, and how they are able to discover the underlined structure in text data.
For this purpose, the Reuters-21578 text categorization dataset with 200 features is utilized to train all the three types of AEs with 20 hidden nodes.
A subset of 500 examples belonging to categories ”grain”, ”crude”, and ”money-fx” was extracted from the test set.
The experiments were run three times, averaged and recorded.
In Fig.
9, the 20-dimensional representations of the Reuters data subset using DpAE, NCAE, and L1/L2-NCSAE are visualized.
It can be observed that L1/L2-NCSAE is able to disentangle the documents into three distinct categories with more linear manifolds than NCAE.
In addition, L1/L2- NCSAE is able to group documents that are closer in the semantic space into the same categories than DpAE that ﬁnds it difﬁcult to group the documents into any distinct categories with less overlap.
TABLE I: Classiﬁcation accuracy on MNIST and NORB dataset Dataset MNIST NORB SAE NCAE NNSAE L1/L2-NCSAE DAE (50% input dropout) DpAE (50% hidden dropout) AAE SAE NCAE NNSAE L1/L2-NCSAE DAE (50% input dropout) DpAE (50% hidden dropout) Before ﬁne-tuning Mean (± SD) p-value 0.735 ± 0.015 <0.001 0.844 (±0.0085) 0.0018 0.702 (±0.027) <0.0001 0.847 (±0.0077) 0.551 (±0.011) <0.0001 0.172 (±0.0021) <0.0001 0.562 ± 0.0245 <0.0001 0.696 (±0.021) 0.406 0.208 (±0.025) <0.0001 0.695 (±0.0084) 0.461 (±0.0019) <0.0001 0.491 (±0.0013) <0.0001 AAE After ﬁne-tuning Mean (± SD) p-value 0.977 ± 0.0007 <0.001 0.974 (±0.0012) 0.812 0.970 (±0.001) <0.0001 0.974 (±0.0087) 0.972 (±0.0021) 0.034 0.964 (±0.0017) <0.0001 0.912 (±0.0016) <0.0001 0.814 ± 0.0099 0.041 0.817 (±0.0095) 0.001 0.738 (± 0.012) <0.001 0.812 (±0.0001) 0.807 (±0.0015) 0.0103 0.815 (±0.0038) <0.0001 0.791 (±0.041) <0.0001 # Epochs 400 126 400 84 400 400 1000 400 305 400 196 400 400 1000 C.
Supervised Learning In the last set of experiments, a deep network was con- structed using two stacked L1/L2-NCSAE and a softmax layer for classiﬁcation to test if the enhanced ability of the network to shatter data into parts and lead to improved classiﬁcation.
Eventually, the entire deep network is ﬁne-tuned to improve the accuracy of the classiﬁcation.
In this set of experiments, the performance of pre-training a deep network with L1/L2- NCSAE is compared with those pre-trained with recent AE architectures.
The MNIST and NORB data sets were utilized, and every run of the experiments is repeated ten times and averaged to combat the effect of random initialization.
The classiﬁcation accuracy of the deep network pre-trained with NNSAE [18], DpAE [19], DAE [32], AAE [22], NCAE, and L1/L2-NCSAE using MNIST and NORB data respectively are detailed in Table I.
The network architectures are 784- 196-20-10 and 1024-200-20-5 for MNIST and NORB dataset respectively.
It is remarked that for training of AAE with two layers of 196 hidden units in the encoder, decoder, discriminator, and other hyperparameters tuned as described in [22], the accuracy was 83.67%.
The AAE reported in Table I used encoder, decoder, and discriminator each with two layers of 1000 hidden units and trained for 1000 epochs.
The classiﬁcation accuracy and speed of convergence are the ﬁgures of merit used to benchmark L1/L2-NCSAE with other AEs. It is observed from the result that L1/L2-NCSAE-based deep network gives an improved accuracy before ﬁne-tuning compared to methods such as NNSAE, NCAE, DpAE, and NCAE.
However, the performance in terms of classiﬁcation accuracy after ﬁne-tuning is very competitive.
In fact, it can be inferred from the p-value of the experiments conducted on MNIST and NORB in Table I that there is no signiﬁcant difference in the accuracy after ﬁne-tuning between NCAE and L1/L2-NCSAE even though most of the weights in L1/L2-NCSAE are nonnegativity constrained.
Therefore it is remarked that even though the interpretability of the deep network has been fostered by constraining most of the weights to be nonnegative and sparse, nothing signiﬁcant has been lost in terms of accuracy.
In addition, network trained with L1/L2-NCSAE was also observed to converge faster than its counterparts.
On the other hand, NNSAE also has nonnegative weights but with deterioration in accuracy, which is more con- spicuous especially before the ﬁne-tuning stage.
The improved accuracy before ﬁne-tuning in L1/L2-NCSAE based network can be traced to its ability to decompose data more into distinguishable parts.
Although the performance of L1/L2- NCSAE after ﬁne-tuning is similar to those of DAE and NCAE but better than NNSAE, DpAE, and AAE, L1/L2-NCSAE constrains most of the weights to be nonnegative and sparse to foster transparency than for other AEs. However, DpAE and NCAE performed slightly more accurate than L1/L2-NCSAE on NORB after network ﬁne-tuning.
In light of constructing an interpretable deep network, an L1/L2-NCSAE pre-trained deep network with 10 hidden neurons in the ﬁrst AE layer, 5 hidden neurons in the second AE, and 10 output neurons (one for each category) in the softmax layer was constructed.
It was trained on Reuters data, and compared with that pre-trained using DpAE.
The interpretation of the encoding layer of the ﬁrst AE is provided by listing words associated with 10 strongest weights, and the interpretation of the encoding layer of the second AE is portrayed as images characterized by both the magnitude and sign of the weights.
Compared to the AE with weights of both signs shown in Fig.
10a, Fig.
10b allows for much better insight into the categorization of the topics.
Topic earn in the output weight matrix resonates with the 5th hidden neuron most, lesser with the 3rd, and somewhat with the 4th.
This resonance can happen only when the 5th hidden neuron reacts to input by words of columns 1 and 4, and in addition, to a lesser degree, when the 3rd hidden neuron reacts to input by words of the 3rd column of words.
So, in tandem, the dominant columns 1, 4 and then also 3 are sets of words that trigger the category earn.
Analysis of the term words for the topic acq leads to a similar conclusion.
This topic also resonates with the two dominant hidden neurons 5 and 3 and somewhat also with neuron 2.
These neurons 5 and 3 are driven again by the columns of words 1,4, and 3.
The difference between the categories is now that to a lesser degree, the category acq is inﬂuenced by the 6th column of words.
An interesting point is in contribution of the 3rd column of words.
The column connects only to the 4th hidden neuron but weights from this neuron in the output layer are smaller and hence less signiﬁcant than for any other of the ﬁve neurons (or rows) of the output weight matrix.
Hence this column is of least relevance in the topical categorization.
D.
Experiment Running Times The training time for networks with and without the non- negativity constraints was compared.
The constrained network converges faster and requires lesser number of training epochs.
In addition, the unconstrained network requires more time per epoch than the constrained one.
The running time experiments were performed using full MNIST benchmark dataset on In- tel(r) Core(TM) i7-6700 CPU @ 3.40Ghz and a 64GB of RAM running a 64-bit Windows 10 Enterprise edition.
The software implementation has been with MATLAB 2015b with batch Gradient Descent method, and LBFGS in minFunc ( [33]) is used to minimize the objective function.
The usage times for constrained and unconstrained networks were also compared.
We consider the usage time in milliseconds (ms) as the time elapsed in ms a fully trained deep network requires to classify all the test samples.
The unconstrained network took 48 ms per epoch in the training phase while the constrained counterpart took 46 ms.
Also, the unconstrained network required 59.9 ms usage time, whereas the network with nonnegative weights took 55 ms.
From the above observations, it is remarked that the nonnegativity constraint simpliﬁes the resulting network.
V.
CONCLUSION This paper addresses the concept and properties of special regularization of DL AE that takes advantage of non-negative encodings and at the same time of special regularization.
It has been shown that by using both L1 and L2 to penalize the negative weights, most of them are forced to be nonnegative and sparse, and hence the network interpretability is enhanced.
In fact, it is also observed that most of the weights in the Softmax layer become nonnegative and sparse.
In sum, it has been observed that encouraging nonnegativity in NCAE-based deep architecture forces the layers to learn part-based repre- sentation of their input and leads to a comparable classiﬁcation accuracy before ﬁne-tuning the entire deep network and not- so-signiﬁcant accuracy deterioration after ﬁne-tuning.
It has also been shown on select examples that concurrent L1 and L2 regularization improve the network interpretability.
The performance of the proposed method was compared in terms of sparsity, reconstruction error, and classiﬁcation accuracy with the conventional SAE and NCAE, and we utilized MNIST handwritten digits, Reuters documents, and the NORB dataset to illustrate the proposed concepts.
10 REFERENCES [1] Y.
Bengio and Y.
LeCun, “Scaling learning algorithms towards ai,” Large-Scale Kernel Machines, vol.
34, no.
1, pp.
1–41, 2007.
[2] Y.
Bengio, “Learning deep architectures for ai,” Foundations and trends® in Machine Learning, vol.
2, no.
1, pp.
1–127, 2009.
[3] G.
Hinton and R.
Salakhutdinov, “Reducing the dimensionality of data with neural networks,” Science, vol.
313, no.
5786, pp.
504–507, 2006.
[4] L.
Deng, “A tutorial survey of architectures, algorithms, and applications for deep learning,” APSIPA Transactions on Signal and Information Processing, vol.
3, p.
e2, 2014.
[5] S.
Bengio, L.
Deng, H.
Larochelle, H.
Lee, and R.
Salakhutdinov, “Guest editors introduction: Special section on learning deep architec- tures,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol.
35, no.
8, pp.
1795–1797, 2013.
[6] Y.
Bengio, P.
Lamblin, D.
Popovici, and H.
Larochelle, “Greedy layer- wise training of deep networks,” Advances in Neural Information Processing Systems, vol.
19, p.
153, 2007.
[7] B.
Ayinde and J.
Zurada, “Clustering of receptive ﬁelds in autoencoders,” in Neural Networks (IJCNN), 2016 International Joint Conference on.
IEEE, 2016, pp.
1310–1317.
[8] D.
D.
Lee and H.
S.
Seung, “Learning the parts of objects by non- negative matrix factorization,” Nature, vol.
401, no.
6755, pp.
788–791, 1999.
[9] J.
Chorowski and J.
M.
Zurada, “Learning understandable neural networks with nonnegative weight constraints,” Neural Networks and Learning Systems, IEEE Transactions on, vol.
26, no.
1, pp.
62–69, 2015.
[10] B.
A.
Olshausen and D.
J.
Field, “Emergence of simple-cell receptive ﬁeld properties by learning a sparse code for natural images,” Nature, vol.
381, no.
6583, pp.
607–609, 1996.
[11] E.
Hosseini-Asl, J.
M.
Zurada, and O.
Nasraoui, “Deep learning of part-based representation of data using sparse autoencoders with non- negativity constraints,” Neural Networks and Learning Systems, IEEE Transactions on, vol.
27, no.
12, pp.
2486–2498, 2016.
[12] M.
Ranzato, Y.
Boureau, and Y.
LeCun, “Sparse feature learning for deep belief networks,” Advances in Neural Information Processing Systems, vol.
20, pp.
1185–1192, 2007.
[13] M.
Ishikawa, “Structural learning with forgetting,” Neural Networks, vol.
9, no.
3, pp.
509–521, 1996.
[14] P.
L.
Bartlett, “The sample complexity of pattern classiﬁcation with neural networks: the size of the weights is more important than the size of the network,” Information Theory, IEEE Transactions on, vol.
44, no.
2, pp.
525–536, 1998.
[15] G.
Gnecco and M.
Sanguineti, “Regularization techniques and subopti- mal solutions to optimization problems in learning from data,” Neural Computation, vol.
22, no.
3, pp.
793–829, 2010.
[16] J.
Moody, S.
Hanson, A.
Krogh, and J.
A.
Hertz, “A simple weight decay can improve generalization,” Advances in Neural Information Processing Systems, vol.
4, pp.
950–957, 1995.
[17] O.
E.
Ogundijo, A.
Elmas, and X.
Wang, “Reverse engineering gene regulatory networks from measurement with missing values,” EURASIP Journal on Bioinformatics and Systems Biology, vol.
2017, no.
1, p.
2, 2017.
[18] A.
Lemme, R.
Reinhart, and J.
Steil, “Online learning and generalization of parts-based image representations by non-negative sparse autoen- coders,” Neural Networks, vol.
33, pp.
194–203, 2012.
[19] G.
E.
Hinton, N.
Srivastava, A.
Krizhevsky, I.
Sutskever, and R.
R.
Salakhutdinov, “Improving neural networks by preventing co-adaptation of feature detectors,” arXiv preprint arXiv:1207.0580, 2012.
[20] N.
Srivastava, G.
Hinton, A.
Krizhevsky, I.
Sutskever, and R.
Salakhut- dinov, “Dropout: A simple way to prevent neural networks from over- ﬁtting,” The Journal of Machine Learning Research, vol.
15, no.
1, pp.
1929–1958, 2014.
[21] D.
P.
Kingma and M.
Welling, “Auto-encoding variational bayes,” arXiv preprint arXiv:1312.6114, 2013.
[22] A.
Makhzani, J.
Shlens, N.
Jaitly, and I.
Goodfellow, “Adversarial autoencoders,” arXiv preprint arXiv:1511.05644, 2015.
[23] Y.
Burda, R.
Grosse, and R.
Salakhutdinov, “Importance weighted autoencoders,” arXiv preprint arXiv:1509.00519, 2015.
[24] B.
O.
Ayinde, E.
Hosseini-Asl, and J.
M.
Zurada, “Visualizing and understanding nonnegativity constrained sparse autoencoder in deep learning,” in Rutkowski L., Korytkowski M., Scherer R., Tadeusiewicz R., Zadeh L., Zurada J.
(eds) Artiﬁcial Intelligence and Soft Computing.
ICAISC 2016.
Lecture Notes in Computer Science, vol 9692.
Springer, 2016, pp.
3–14.
[25] S.
J.
Wright and J.
Nocedal, Numerical optimization.
York, 1999, vol.
2.
Springer New 11 [26] T.
D.
Nguyen, T.
Tran, D.
Phung, and S.
Venkatesh, “Learning parts- based representations with nonnegative restricted boltzmann machine,” in Asian Conference on Machine Learning, 2013, pp.
133–148.
[27] A.
Ng, “Sparse autoencoder,” in CS294A Lecture notes.
URL https: //web.stanford.edu/class/cs294a/sparseAutoencoder 2011new.pdf: Stan- ford University, 2011.
[28] Y.
LeCun, L.
Bottou, Y.
Bengio, and P.
Haffner, “Gradient-based learning applied to document recognition,” Proceedings of the IEEE, vol.
86, no.
11, pp.
2278–2324, 1998.
[29] Y.
LeCun, F.
J.
Huang, and L.
Bottou, “Learning methods for generic object recognition with invariance to pose and lighting,” in Computer Vision and Pattern Recognition, 2004.
CVPR 2004.
Proceedings of the 2004 IEEE Computer Society Conference on, vol.
2.
IEEE, 2004, pp.
II–97.
[30] P.-N.
Tan, M.
Steinbach, V.
Kumar et al., Introduction to data mining.
Pearson Addison Wesley Boston, 2006, vol.
1.
[31] L.
V.
der Maaten and G.
Hinton, “Visualizing data using t-sne,” Journal of Machine Learning Research, vol.
9, no.
11, 2008.
[32] P.
Vincent, H.
Larochelle, Y.
Bengio, and P.
Manzagol, “Extracting and composing robust features with denoising autoencoders,” in 25th International Conference on Machine Learning.
ACM, 2008, pp.
1096– 1103.
[33] R.
H.
Byrd, P.
Lu, J.
Nocedal, and C.
Zhu, “A limited memory algo- rithm for bound constrained optimization,” SIAM Journal on Scientiﬁc Computing, vol.
16, no.
5, pp.
1190–1208, 1995.
Babajide Ayinde (S’09) received the M.Sc. de- gree in Engineering Systems and Control from the King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia.
He is currently a Ph.D. student at the University of Louisville, Kentucky, USA and a recipient of University of Louisville fellowship.
His current research interests include unsupervised feature learning and deep learning techniques and applications.
Jacek M.
Zurada (M’82-SM’83-F’96-LF’14) Ph.D., has received his degrees from Gdansk In- stitute of Technology, Poland.
He now serves as a Professor of Electrical and Computer Engineering at the University of Louisville, KY.
He authored or co- authored several books and over 380 papers in com- putational intelligence, neural networks, machine learning, logic rule extraction, and bioinformatics, and delivered over 100 presentations throughout the world.
In 2014 he served as IEEE V-President, Technical Activities (TAB Chair).
He also chaired the IEEE TAB Periodicals Committee, and TAB Periodicals Review and Advisory Committee and was the Editor- in-Chief of the IEEE Transactions on Neural Networks (1997-03), Associate Editor of the IEEE Transactions on Circuits and Systems, Neural Networks and of The Proceedings of the IEEE.
In 2004-05, he was the President of the IEEE Computational Intelligence Society.
Dr. Zurada is an Associate Editor of Neurocomputing, and of several other journals.
He has been awarded numerous distinctions, including the 2013 Joe Desch Innovation Award, 2015 Distinguished Service Award, and ﬁve honorary professorships.
He has been a Board Member of IEEE, IEEE CIS and IJCNN.

An important problem in database research is determining how to combine multiple data sources that are described by diﬀerent (heterogeneous) schemata [6].
The outcome of such a process is expected to be a uniform integrated view across these data sources.
Relational data sources are still one of the most popular ways to store enterprise or Web data [20].
However, the relational schema lacks a well-deﬁned semantic description.
To deﬁne the semantics of data, we can introduce an ontology [20].
Now our goal is to map attributes from relational data sources to classes and properties in an ontology.
We refer to this problem as semantic labeling.
Semantic labeling plays an important role in data integration [6, 14], augmenting ex- isting knowledge bases [9, 17, 18, 23] or mapping relational sources to ontologies [15, 22].
Various approaches to automate semantic labeling have been developed, including DSL [14] and T2K [17].
Typically automated semantic labeling techniques encounter several prob- lems.
Firstly, there can be naming conﬂicts [15], including those cases where users represent ∗ Work accomplished at Data61, CSIRO.
licensed under Creative Commons License CC-BY Leibniz International Proceedings in Informatics Schloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, Germany XX:2 Semantic labeling the same data in diﬀerent ways.
Secondly, semantically diﬀerent attributes might have syn- tactically similar content, for example, birth date versus date of death.
Thirdly, there are a considerable number of attributes which do not have any corresponding property in the ontology, either by accident or on purpose.
The majority of existing systems focus on the ﬁrst two problems, but do not consider the third problem during evaluation [18, 14].
To address the challenges of automated semantic labeling, we formulate this task as a supervised classiﬁcation problem.
A set of semantic labels known to the classiﬁer is speciﬁed at training time, e.g., from the provided domain ontology.
We also introduce a special class of attributes, called unknown.
The purpose of the unknown class is to capture attributes which will not be mapped to the ontology.
The training data for the classiﬁer will thus consist of source attributes (name and content) and their semantic labels provided by the user, including the unknown labels.
Since manually assigning labels to attributes is a costly operation, a lack of training data is a common problem for semantic labeling systems.
Existing systems [14, 17, 23] use knowledge transfer techniques to overcome this issue.
Instead, we introduce a sampling method similar to bagging for ensemble models [3].
The bagging technique allows us to generate multiple training instances from the user- labeled attributes, thus overcoming the lack of labeled training data.
It also allows us to overcome the common issue of class imbalance, when some semantic labels have more support than others among the attributes.
We can achieve this by re-balancing the training data via preferential bagging from minority class attributes.
The main contributions of this paper are: 1.
We introduce a bagging approach to handle class imbalance and the lack of training data by drawing random subsamples from values of an attribute.
This approach can achieve meaningful diversity in the training data and can increase the number of training instances for under-represented semantic labels.
2.
We address the issue of “unwanted” attributes, i.e., attributes which do not get mapped to any element in the ontology.
In cases where we have a suﬃcient amount of training data, our models can achieve over 80% Mean Reciprocal Rank (MRR) on two sets of data sources from our benchmark.
3.
We construct a classiﬁcation model DINT with hand-engineered semantic labeling fea- tures to implement the above.
In addition, we design two deep learning models CNN and MLP which use very simple features, such as normalized character frequencies and padded character sequences extracted from raw values of data attributes.
4.
We construct a benchmark with a common evaluation strategy to compare diﬀerent approaches for supervised semantic labeling.
Our benchmark includes such models as DINT, CNN, MLP and the state-of-the-art DSL [14], and 5 sets of data sources from diﬀerent domains.
We show that each approach has its strengths and shortcomings, and choosing a particular semantic labeling system depends on the use case.
We have released the implementation of the benchmark under an open source license 1.
This benchmark can be easily extended to include other models and datasets, and can be used to choose the most appropriate model for a given use case.
Problem We illustrate the semantic labeling problem using a simple domain ontology shown in Fig.
1.
Assume we have three data sources “personal-info”, “businessInfo” and “Employees” (see 1 http://github.com/NICTA/serene-benchmark N.
Rümmele, Y.
Tyshetskiy, A.
Collins XX:3 Figure 1 Example ontology.
Tab.
1) whose attributes we choose to label according to the example ontology (Fig.
1).
We deﬁne a semantic label as a tuple consisting of a domain class and its property.
For example, attribute name in the source “personal-info” (see Tab.
1a) is labeled with (Person,name).
Note that semantic labels are ﬁxed by the ontology.
The task of semantic labeling is deﬁned as automatically assigning semantic labels to attributes in a data source.
In the case of supervised semantic labeling, we use existing known semantic labels for data sources to improve the performance when assigning semantic labels to new sources.
For example, assume we are given sources “personal-info” and “businessInfo” with the correct semantic labels, the system should then automatically assign labels to attributes in the source “Employees”.
To build such a system, we cannot just rely on the names of the columns.
For example, columns name in (1a), ceo in (1c) and employee in (1b) all refer to the same property (Person,name).
Using just values of the columns is also problematic.
For example, in (1a) acronyms are used for states, while in (1c) state names are fully written.
Furthermore, values can overlap for semantically heterogeneous columns like for founded in (1c) and birthDate in (1a).
We can also have attributes that are not mapped to any property in the ontology.
There might be two reasons for their existence: (1) we are not interested in the content of an attribute and want to discard it from any future analysis; (2) we might have overlooked an attribute by not designing the ontology accurately.
We do not diﬀerentiate between these two cases and mark all such attributes as unknown class, for example, founded in (1c).
The presence of unknown class makes the task of semantic labeling more complicated.
Estab- lishing approaches to eﬃciently handle such attributes is crucial since in many real-world scenarios relational data sources (either HTML tables [17] or domain speciﬁc data [14]) contain a considerable number of such attributes.
Machine learning techniques proved to be eﬃcient in building predictive models on noisy and messy data.
Yet to apply these techniques we need to represent source attributes as feature vectors, with semantic labels (classes) attached to these vectors.
In Table 2 we show such representation for the source Employees.
We have explicitly shown only 4 possible features, for simplicity.
However, the actual size of a feature vector can be arbitrary long, and the process of designing its components is known as feature engineering.
In the next CityPlacesubclassStatestatepostalCodenamesubclassPersonlivesInbornInOrganizationworksForbirthDatenameoperatesInceoemailphonenameXX:4 Semantic labeling Table 1 Example relational data sources with semantic labels.
name Neil Mary Henry (Person, name) city birthDate 21-05-1916 Waterloo Eveleigh 07-12-1990 Redfern 15-03-2000 (Person, (City, name) birthDate) state NSW NSW NSW (State, name) workplace CSIRO CSIRO Data61 (Organization, name) Semantic labels (a) personal-info employer employee DOB CSIRO Data61 NICTA (Organization, name) Neil Mary Henry (Person, name) 05/21/1916 12/07/1990 03/15/2000 (Person, birthDate) (b) Employees company CSIRO Data61 NICTA ceo state Larry Marshall Australian Capital Territory Adrian Turner Hugh Durrant New South Wales New South Wales Semantic labels (Organization, name) (c) businessInfo (Person,name) (State,name) founded 21-05-1916 12-07-2016 15-03-2002 unknown section we will discuss the features used in the semantic labeling system.
Approaches In this section we describe classiﬁers for the semantic labeling problem used for evaluation.
We also discuss approaches to the problem of unknown attributes and lack of training data.
Once we have a set of labeled data sources, we construct feature vectors for all attributes in this set and mark them as representatives of a class corresponding to their semantic labels.
The constructed set of (feature vector, class label) pairs is then used to train a classiﬁer.
We consider several approaches, divided into 3 major groups: DINT, Deep Learning and the state-of-the-art DSL.
Each approach trains a multi-class classiﬁcation model that produces, at the prediction stage, a list of class probabilities for an attribute in a new source.
The class with the highest predicted probability is then assigned to the attribute at the decision stage.
3.0.1 DINT In our ﬁrst approach DINT (Data INTegrator) we hand-engineer 26 features, which in- clude characteristics such as number of whitespaces and other special characters, statistics of values in the column (e.g, mean/ max/ min string length and numeric statistics) and many more.
The complete list of features is available in the open source benchmark repos- N.
Rümmele, Y.
Tyshetskiy, A.
Collins XX:5 Table 2 Feature vectors for data source Employees.
feature vector attribute entropy mean string length employer employee DOB 1.001 1.461 0.69 5.333 13.333 10 ratio alpha chars ratio unique values class 0.875 0.925 (Organization, name) (Person, name) (Person, birthDate) .
.
.
.
.
.
.
.
.
.
.
.
entropy [11]) of a string X is deﬁned as H(X) = −P itory 2.
One of the important features characterising information content of an attribute is Shannon’s entropy of the attribute’s concatenated rows.
Shannon’s entropy (or information i pi log2 pi, where pi is the probability of a character, whose index in character vocabulary is i, to appear in X, and the summation ranges over all characters in the vocabulary.
To evaluate pi in Shannon’s entropy, we evalu- ate normalized character frequency distribution chardist of an attribute, as character counts in concatenated rows of the attribute, normalized by the total length of the concatenated rows.
The vocabulary of all characters consists of 100 printable characters (including \n).
Finally, we also add the 100-dimensional vector of pi to the attribute feature vector.
In addition to the above features, which can be directly calculated from attribute val- ues, we compute mean cosine similarity of attribute character distribution with character distributions of all class instances.
This adds as many additional scalar features to the full attribute feature vector as there are classes in the training data.
One can expect that names of the attributes should also contain useful information to determine their semantic types, in addition to the information provided by attribute values.
To extract features from attribute names, we compute string similarity metrics: minimum edit distance, two WordNet based similarity measures such as JCN [7] and LIN [10], and k-nearest neighbors using Needle-Wunsch distance [13].
The minimum edit distance between two strings s1 and s2 is the minimum number of edit operations, such as insertion, deletion, substitution, which are required to transform one string into another [11].
We compute the similarity between attribute name and all class instances in the training data.
The number of thus extracted features depends on the number of semantic labels in the training data.
We choose to train a Random Forest [4] (RF) on this set of features.
RF is quite robust on noisy data, works well even with correlated features, and easily captures complex nonlinear relationships between features and target.
Additionally, RF classiﬁers require little hyperparameter tuning, and hence they usually work straight “out of the box”, which makes them a convenient yet versatile classiﬁer to use.
3.0.2 Deep Learning Deep learning has gained much popularity due to its tremendous impact in such areas as speech recognition, object recognition, and machine translation [8].
One of the biggest advantages of deep learning is the ability to process data in its raw form and to discover the representation needed for classiﬁcation, assisting with the feature engineering step.
Broadly speaking, deep learning is an overarching term for artiﬁcial neural networks, where the word “deep” refers to the depth of the network.
At the basic level neural networks are composed of perceptrons, or neural nodes.
There can be several layers of interconnected 2 serene-benchmark XX:6 Semantic labeling neural nodes; The ﬁrst layer is the input layer while the last one is the output layer.
The layers in between these two are called hidden.
Neural nodes in each layer take as input the output of the nodes from the previous layer, perform some computation with a nonlinear activation function (e.g., tanh or RELU) and pass the result to the next layer.
There are generally no connections between nodes in the same layer.
Overall, deep learning models improve in their performance the more data they are trained on.
The exact architecture of deep learning models, i.e., number of layers, number of nodes in each layer, activation functions of neurons and interconnectedness between layers, all inﬂuence the performance of the trained models.
We choose two diﬀerent architectures for our deep learning classiﬁers: (i) Multi-Layer Perceptron (MLP) [19] and (ii) Convolutional Neural Network (CNN) [8].
We have experi- mented with diﬀerent designs of the MLP and CNN networks, varying their hyperparameters that control the number of hidden layers, the numbers of nodes/ﬁlters per layer, dropout probability, etc., and found that the designs, described brieﬂy below, work well for all the datasets in the benchmark.
The input layer of the MLP architecture takes the 101-dimensional feature vector of character frequencies pi (chardist) and Shannon entropy.
Following the input layer, MLP has 3 fully connected hidden layers with 100 nodes per layer, with tanh activations.
After the 1st hidden layer, we introduced a stochastic dropout layer with dropout probability of 0.5, to prevent overﬁtting.
Finally, the output layer of MLP (the actual classiﬁer) is a softmax layer with the number of nodes equal to the number of semantic types (including the ‘unknown’ type).
The CNN model takes as input the one-hot representation of an attribute’s concatenated rows in character space, then embeds it to a dense 64-bit embedding, then passes this embedded "image" of the attribute through two consecutive 1d convolution layers with 100 ﬁlters per layers, followed by a 1-d max-pooling layer, a ﬂattening layer, a dropout layer with probability of dropout 0.5, then a fully connected layer with 100 nodes, and ﬁnally a fully connected softmax output layer (the classiﬁer) with the number of nodes equal to the number of semantic types (including the ‘unknown’ type).
Though we cannot be sure that our ﬁnal choice for the architectures is optimal, it seems to be a good trade-oﬀ between complexity of the models, required computational resources for their training, and their overall performance in semantic labeling task.
We have implemented both models using Keras library with GPU-based TensorFlow backend [1].
3.0.3 DSL The Domain-independent Semantic Labeler (DSL) has been proposed by Pham et al [14], where 6 feature groups based on similarity metrics are constructed.
These metrics measure how attribute names and values are similar to the characteristics of other attributes.
This means that given 5 attributes in the training data (i.e., already labeled instances) with distinct semantic labels, a new attribute will be compared to representatives of each semantic label and 30 features will be calculated in total.
The considered similarity metrics are: attribute name similarity, standard Jaccard similarity for textual data and a modiﬁed version for numerical data, TF-IDF cosine similarity, distribution and histogram similarity.
Instead of building one multi-class classiﬁer, the authors train binary classiﬁers separ- ately for each semantic label.
A binary classiﬁer for a particular semantic label is a Logistic Regression model trained on a set of similarity metrics with representatives of this label.
When predicting semantic labels for a new attribute, they combine the predictions of each classiﬁer to produce the ﬁnal vector of probabilities.
One of the distinctive properties of this N.
Rümmele, Y.
Tyshetskiy, A.
Collins XX:7 approach is the ability to transfer the classiﬁcation model trained in one domain to predict- ing semantic labels for attributes in another domain.
We denote this enhanced approach as DSL+.
3.1 Bagging To train a classiﬁer for semantic labeling, we need data sources to have many labeled at- tributes.
However, the costly operation of manually assigning labels to attributes, and the relative small number of columns compared to data set size, implies that lack of training data is a common problem for semantic labeling systems.
Existing systems [14, 17, 23] use knowledge transfer techniques to overcome this issue.
We introduce a method for increasing training sample size based on a machine learning approach known as bagging [3].
Breiman [3] introduced the concept of bootstrap aggregating, also known as bagging, to construct ensembles of models to improve prediction accuracy.
The method consists in training diﬀerent classiﬁers with bootstrapped replicas of the original dataset.
Hence, diversity is obtained with the resampling procedure by the usage of diﬀerent data subsets.
At the prediction stage each individual classiﬁer estimates an unknown instance, and a majority or weighted vote is used to infer the class.
We modify the idea of bagging for our problem.
It is clear that the semantics of columns in the table “Employees” (Table 1b) will not change whether we have 3 or 1000 rows.
So, we can create several training instances for an attribute, where each instance (called a bag) will contain a random sample (with replacement) of its content.
This procedure is governed by two parameters numBags and bagSize: the ﬁrst parameter controls how many bags are generated per each attribute, while the latter indicates how many rows are sampled per each bag.
In such a way we address the issue of noise by increasing diversity of the training data as well as the issue of insuﬃcient training data.
Another common problem encountered in a wide range of data mining and machine learning initiatives is class imbalance.
Class imbalance occurs when the class instances in a dataset are not equally represented.
In such situation, building standard machine learning models will lead to poor results, since they will favor classes with large populations over the classes with small populations.
To address this issue, we have tried several resampling strategies to equalize the number of instances per each class.
3.2 Unknown class As mentioned previously, some attributes are not mapped to any property in the ontology.
To handle this issue, we introduce one more class called unknown.
For example, attributes which get discarded from the integration process can be marked as unknown.
This way we can help the classiﬁer recognize such attributes in new sources.
In addition, there is another advantage of having the unknown class deﬁned explicitly.
Consider a new attribute with an unseen semantic label, that is, a label which is not present in the training data.
Instead of picking the closest match among known semantic labels, the classiﬁer will mark it as unknown.
The user will then need to validate the attributes that are classiﬁed as unknown.
This will ensure that the unknown class consists only of unwanted attributes.
We do not introduce another class to diﬀerentiate between unwanted attributes and unseen labels since we cannot guarantee that there is no overlap between them.
Only our DINT and Deep Learning approaches support an unknown class.
XX:8 Semantic labeling Experiments We have run all our experiments on a Dell server with 252 GiB of memory, 2 CPUs with 4 cores each, 1 Titan GPU and 1 GeForce 1080 Ti GPU.
The deep learning models have been optimized for GPUs using Tensorﬂow.
The benchmark for semantic labeling system is implemented in Python and is available under an open source license 3.
4.1 Datasets We use 5 diﬀerent sets of data sources in our evaluation, labeled as: museum, city, weather, soccer [14] and weapons [21].
Each set corresponds to a domain with a speciﬁc set of semantic labels.
Descriptive statistics of each domain set are shown in Table 3.
As we can see, the museum and soccer domains are the only domains which have unknown attributes.
The city domain has many semantic labels and attributes while the museum domain contains more data sources.
Table 3 Description of data sources.
Domain weather weapons museum soccer city sources # semantic labels attributes # unknown attributes avg # rows per source avg # attributes per source 15 29 12 10 12 28 20 18 52 44 175 443 138 520 159 42 108.5 54.46 6978.89 2120.16 2251 11 11.66 15.27 11.5 52 To estimate class imbalance within each domain, we plot the class distribution in Figure 2.
The museum domain has the highest imbalance among classes, the soccer and weapons domains also have imbalanced classes, whereas the weather and city domains have equally represented classes.
4.2 Experimental setting We establish a common evaluation framework for the approaches as described in Section 3.
As a performance metric we use Mean Reciprocal Rank (MRR) [5].
To derive a comprehens- ive estimate of performance within domains, we implement two cross-validation techniques: leave one out and repeated holdout.
The leave one out strategy is deﬁned as using one source as the testing sample and the rest of sources in the domain as the training samples.
This procedure is repeated as many times as there are sources in the domain.
We calculate MRR on the testing sample and report the average MRR as the ﬁnal performance metric for each iteration.
For example, for the domain museum we obtain 29 models in total where each model is trained on a diﬀerent 28 sources, and MRR is calculated on the prediction outcome for a single source.
This strategy allows us to estimate the performance of the diﬀerent models given that there are enough instances per each semantic label.
3 http://github.com/NICTA/serene-benchmark N.
Rümmele, Y.
Tyshetskiy, A.
Collins XX:9 30 25 20 15 10 12 numeric identiﬁers of semantic labels 28 numeric identiﬁers of semantic labels 12 40 30 20 10 10 19 numeric identiﬁers of semantic labels (a) weather (b) weapons (c) soccer 159 140 120 100 80 60 40 20 10 21 numeric identiﬁers of semantic labels 10 10 52 numeric identiﬁers of semantic labels 20 30 40 (d) museum (e) city Figure 2 Distribution of attributes according to semantic labels, including the unknown class, in diﬀerent domains.
We can see class imbalance in the museum, soccer and weapons domains.
On the x-axis we have semantic labels sorted by the number of attributes in each class.
The y-axis shows the number of attributes.
In repeated holdout strategy, we randomly sample a ratio p of sources to place in the training sample and use the remaining sources for testing sample, and this procedure is repeated n times.
The ﬁnal MRR score is an average of MRR scores in each iteration.
We use this technique to simulate the scenario when there is a shortage of labeled sources.
We set the ratio p = 0.2 and the number of iterations n = 10.
4.3 Results In this section we report the results of our experiments.
In total we evaluate 13 models, and we report run times required to train the considered models.
To train MLP and CNN models, we need many training instances, so we use bagging (presented in Section 3.1) with parameters numBags=150 and bagSize=100 to increase the size of the initial training set.
We can train the semantic labeling system DINT with diﬀerent sampling strategies.
In particular, we report results when we apply no resampling and bagging with parameters bagSize=100 and numBags=100.
We also experiment with various class imbalance resampling strategies, including resampling to the mean or maximum of instance counts per class.
For brevity and without loss of generality we report results only for the resampling to mean strategy denoted as ResampleToMean.
By design DSL and DSL+ use no resampling.
As mentioned in Section 3.0.1, the DINT model is built on a set of elaborately engineered features.
MLP model, on the other hand, uses only chardist and entropy.
To better compare the performance of MLP and DINT, we create a new model DINT base and reduce the XX:10 Semantic labeling Table 4 MRR scores for leave one out strategy when unknown attributes are not considered.
Model DSL city 0.711 0.971 DINT all DINT base 0.925 DINT base+ 0.925 0.873 MLP 0.877 CNN 0.956 DINT all 0.928 DINT base DINT base+ 0.928 0.969 DINT base 0.929 DINT base+ 0.929 museum soccer weapons weather 0.904 0.848 0.964 0.902 0.93 0.86 0.89 0.93 0.965 0.886 0.939 0.893 0.979 0.913 0.894 0.941 0.956 0.911 0.956 0.907 0.911 0.888 0.901 0.926 0.865 0.801 0.794 0.792 0.862 0.823 0.804 0.887 0.79 0.814 0.802 0.835 0.731 0.826 0.810 0.807 0.799 0.813 0.833 0.825 0.813 0.792 0.788 0.765 Resample DINT all To Mean Table 5 Model training times (s) for leave one out.
Sampling None Bagging Sampling None Bagging Model DSL city museum soccer weapons weather 295.6 10.8 DINT all 10.2 DINT base DINT base+ 10.2 184.2 MLP 184.8 CNN 212 DINT all DINT base 165.5 DINT base+ 165.5 10.9 DINT base 10.8 DINT base+ 10.9 164.3 74.8 20.4 20.7 216.5 276.2 310.3 83.1 80.0 58.6 20.5 18.4 269.5 6.2 5.0 4.1 85 71.3 127.9 35.1 44.0 8.3 4.3 4.3 36.6 8.0 3.9 4.0 26.8 29.7 47.3 26.8 27.8 12.2 2.3 2.8 8.2 2.0 3.6 2.0 11 12.5 11.6 8.6 7.1 2.1 2.1 2.1 Resample DINT all To Mean number of features to just chardist and entropy.
In addition, we create another model DINT base+ by using chardist and entropy and add a feature minimum edit distance.
We choose this feature as feature importance scores produced by the random forest algorithm rank edit distance higher than the other features extracted from names.
Table 4 reports the MRR scores for leave one out strategy.
Surprisingly, models built on just normalized character distributions of attribute values perform in many cases very well.
Deep learning models MLP and CNN are often comparable with DINT models, however they come usually at a higher computational cost.
Run times for training each model are shown in Table 5.
As we can see, DINT models that use bagging to sample more training instances achieve the best results in four domains.
Remarkably, these are also the domains with higher class imbalance and variety among data sources in terms of number of rows and number of columns.
Data sources in the city domains have the same number of attributes.
We have also discovered that bagging needs to be performed both at the training and prediction stages to achieve the best performance.
We have observed that this setting makes a noticeable N.
Rümmele, Y.
Tyshetskiy, A.
Collins XX:11 diﬀerence in domains where the number of rows varies substantially among data sources.
For example, in the museum domain number of rows ranges from 6 to 85235, and in the soccer domain the range is from 500 to 9443.
In terms of computation time, the best performing model DINT all for the museum domain requires a lot of time for training.
The most computationally expensive features are four diﬀerent edit distances: minimum edit distance, JCN, LIN and k-nearest neighbors.
This suggests that the DINT model with all possible features does not scale well with the increasing number of attributes in the training set.
Considering similarity metrics used in other approaches like DSL and T2K [17], computing TF-IDF and Jaccard’s scores may help resolve this runtime issue for DINT all.
For class imbalance, although the ResampleToMean strategy improves the performance of DINT models with no sampling in the domains with the highest class imbalance (i.e., museum and soccer), it appears that the ResampleToMean strategy leads to a decreased performance in the domains with a less prominent imbalance (i.e., weapons and weather).
This leads us to the idea that a class resampling strategy needs to be improved.
One potential strategy may be in combining bagging and resampling strategies.
Instead of ﬁxing numBags for all attributes, the parameter could be changed to be either the mean or maximum of instance counts per each class.
In such a way we can perform a resampling strategy which does not produce replicas of the attributes.
Apart from the city and weapons domains, our newly designed models have a similar performance to DSL.
However, the computational complexity of these models varies.
For the museum domain DINT base+ has a higher MRR than DSL, yet DINT base+ needs half the time less for training.
It appears that attributes which contain a mixture of textual and numeric are a bottleneck for DSL since data sources in the city and weapons domains have multiple mixed data columns.
Table 6 MRR scores for repeated holdout strategy when unknown attributes are not considered.
Sampling None Bagging Resample DINT all To Mean city Model 0.719 DSL 0.782 DSL+ 0.949 DINT all DINT base 0.888 DINT base+ 0.888 0.797 MLP 0.723 CNN 0.945 DINT all 0.919 DINT base DINT base+ 0.919 0.949 0.89 0.89 DINT base DINT base+ museum soccer weapons weather 0.889 0.805 0.927 0.798 0.763 0.778 0.77 0.774 0.791 0.788 0.790 0.789 0.749 0.758 0.611 0.872 0.688 0.684 0.686 0.695 0.664 0.682 0.701 0.688 0.588 0.578 0.564 0.614 0.813 0.553 0.516 0.542 0.663 0.606 0.656 0.634 0.628 0.455 0.451 0.445 0.583 0.621 0.621 0.887 0.882 0.854 0.867 0.852 0.557 0.611 0.611 In cases where there are few labeled instances (repeated holdout strategy in Table 6), we observe that DSL performs well, especially DSL+, which leverages labeled instances from other domains.
We should be aware that in this scenario there are many unseen labels, which makes MRR ill-deﬁned.
If we compare DINT models in this scenario, it suggests that bagging is advantageous in situations when there are few labeled attributes.
Overall, enhancing our DINT model, which uses simple features and bagging, with DSL+ know- XX:12 Semantic labeling Table 7 Performance for leave one out strategy when unknown class is considered.
MRR scores Sampling Model None Bagging DSL DINT all DINT base DINT base+ MLP CNN DINT all DINT base DINT base+ Resample DINT all To Mean DINT base DINT base+ Train time (s) museum soccer museum soccer 36.3 6.8 5.9 6.2 37.6 39.6 64.5 26.7 30.6 6.8 4.2 5.2 0.56 0.866 0.838 0.849 0.802 0.831 0.854 0.839 0.867 0.776 0.721 0.759 0.618 0.827 0.809 0.824 0.784 0.785 0.795 0.863 0.793 0.730 0.69 0.753 156.6 100.6 28.4 33.4 417.2 394.5 395.2 112.5 114.4 100.5 26.2 26.7 ledge transfer capability might result in a more stable semantic labeling system.
Another enhancement may be to introduce resampling strategies into the DSL system.
In addition, we perform experiments for the two domains museum and soccer, where unmapped attributes cause skewed class distributions.
Here we want to establish how well diﬀerent approaches can recognize such attributes.
In Tables 7 and 8 we can see that the performance of semantic labeling systems changes considerably.
Both the DSL and DSL+ performance is aﬀected by their inability to diﬀerentiate "unwanted" attributes.
When performing bagging on attributes in the training data, we introduce diversity by drawing many samples of attribute values.
However, we do not apply any perturbation technique to the names of the attributes and instead use their exact replicas.
In Table 8 we observe that DINT base performs better than DINT base+ when bagging is used.
In datasets with scarce labeled instances our DINT models tend to overﬁt the attribute names that are present in the training data.
This suggests that introducing a technique similar to bagging for column headers might lead to a much better performance.
On the other hand, our results are consistent with the observations in the work of Ritze et al.[17].
Their results indicate that comparing attribute values is crucial for this task while attribute names might introduce additional noise.
Clearly, the performance of our approach DINT varies depending on the chosen bagging parameters numBags and bagSize.
To explore this dependence, we evaluate the performance of DINT with only chardist and entropy features by varying one of the bagging parameters and ﬁxing the other one.
We report the results of our evaluation in Figure 3.
Here we do not consider unknown attributes and choose the repeated holdout strategy to analyze the behavior of bagging when there is a shortage of training data.
Interestingly, increasing the values of the bagging parameters does not always lead to an improved performance, though the computational time required for both the training and prediction stages increases.
The city domain is the most sensitive to bagging parameters.
We assume this is because the city domain is the only domain with an equal distribution of semantic labels, equal numbers of columns and rows across data sources.
It appears that in other domains, bagging makes models more robust towards variance in these characteristics.
N.
Rümmele, Y.
Tyshetskiy, A.
Collins XX:13 Table 8 Performance for repeated holdout strategy when unknown class is considered.
MRR scores Sampling Model None Bagging DSL DSL+ DINT all DINT base DINT base+ MLP CNN DINT all DINT base DINT base+ Resample DINT all To Mean DINT base DINT base+ Train time (s) museum soccer museum soccer 4.2 0.544 0.303 241.3 0.769 0.743 0.742 0.675 0.683 0.827 0.76 0.721 0.637 0.607 0.633 0.355 0.43 0.549 0.608 0.613 0.572 0.534 0.551 0.593 0.59 0.428 0.475 0.479 15.5 215.9 16.2 10.2 10.1 94.4 87.7 101.9 55.6 53.1 35.5 12.2 11.5 9.4 10 13.6 13.5 13.6 2.6 0.9 0.8 0.7 0.6 0.5 0 10 50 100 numBags 150 weather weapons soccer museum city 0.9 0.8 0.7 0.6 0.5 0 10 30 50 bagSize 100 (a) ﬁxing bagSize=100 (b) ﬁxing numBags=50 Figure 3 Dependence of MRR scores for DINT base on the bagging parameters using repeated holdout strategy.
Unknown attributes are not considered.
Related Work The problem of semantic labeling, as addressed in this work, can be regarded as the problem of schema matching in the ﬁeld of data integration [2].
In the schema matching problem we match elements between the source and target schemata.
In our case elements of the source schema are attributes, and we want to map these attributes to properties in the ontology.
The semantic labeling problem is also known in literature as attribute-to-property matching [18, 17].
Indicating semantic correspondences manually might be appropriate if only few data sources need to be integrated, however, it becomes tedious with the growing number of heterogeneous schemata.
Hence, automatic or semi-automatic approaches for schema matching are being actively developed.
From machine learning perspective, we can categorize these approaches into unsupervised techniques which compute various similarity metrics and supervised techniques which build a multi-class classiﬁcation model.
Unsupervised approaches are used in SemanticTyper [16], XX:14 Semantic labeling T2K [18] and its extended version [17].
In all these approaches authors design similarity metrics for attribute names and attribute values, yet one substantial diﬀerence is whether additional knowledge is used in the computation.
For example, authors in [18] and [17] leverage contextual information from DBpedia.
Among supervised approaches, there are probabilistic graphical models used in the work of Limaye et al.
[9] to annotate web tables with entities for cell values, types for attributes and relationships for binary combinations of attributes.
Mulwad et al.
[12] extend this approach by leveraging information from Wikitology Knowledge Base (KB).
The problem with probabilistic graphical models is though that they do not scale with the number of semantic labels in the domain.
Also, Mulwad et al.
as well as Venetis et al.
[23], who used the isA database KB, extract additional data from knowledge bases to assign a semantic label to an attribute.
Hence, these approaches are limited to domains well represented in those knowledge bases.
Our approach, on the other hand, is not domain speciﬁc and allows a model to be trained on any data.
However, we cannot apply a model learnt on one domain to another, which is possible with the DSL approach [14].
To the best of our knowledge, DSL introduced by Pham et al.[14] is among the top semantic labeling systems.
Pham et al.
compare DSL to their previous approach Semantic- Typer [16] and T2K system [18], and achieve higher MRR scores on a variety of datasets.
Therefore, we use DSL as the state-of-the art model in our benchmark to evaluate our new approaches.
Ritze et al.
[17] and Pham et al.
[14] mention the problem of the unknown class.
In the ﬁrst work the authors discuss "unwanted" attributes while in the second work the authors reﬂect on how to handle "unseen" attributes.
In our work we do not diﬀerentiate between these two cases and show that we can successfully identify such attributes when suﬃcient training data is available.
Conclusion In this paper we have studied the problem of supervised semantic labeling and have conduc- ted experiments to evaluate how diﬀerent approaches perform at this task.
Our main ﬁnding is that our bagging sampling technique can provide meaningful diversity to our training data to improve performance.
Additionally, this technique can overcome the lack of labeled attrib- utes in the domain and can increase the number of instances for under-represented semantic labels.
We ﬁnd that given scarce training data, bagging leads to a noticeable improvement in performance, though the state-of-the-art system DSL [14] achieves a better precision by leveraging information about labeled instances from other domains.
However, if we are to consider unwanted attributes and unseen semantic labels, our new system DINT demon- strates the best performance.
Among the semantic labeling systems in our benchmark we have observed that the performance results are highly dependent on the use case.
We have also shown that deep learning models, such as CNN and MLP, can also be applied to solve this problem.
Though these models do not excel in performance in the majority of cases, their advantage is the simplicity of features extracted from attributes.
For example, CNN is built on raw sequences of attribute values.
Surprisingly, we have discovered that even random forests constructed just on character distributions of values and entropy of attributes provide remarkable results in many cases.
This supports the observations in literature that attribute values are crucial for semantic labeling task [18, 17].
Future work may involve exploring a combination of bagging and class imbalance res- ampling strategies.
We have observed that where the domain data has high imbalance among N.
Rümmele, Y.
Tyshetskiy, A.
Collins XX:15 representatives of diﬀerent semantic labels, resampling can lead to an improved perform- ance but a more sophisticated approach is required in domains which do not exhibit these characteristics.
Another possible direction for improvement is to introduce an equivalent of bagging for attribute names.
In addition, our experiments indicate that the performance of systems is often aﬀected by the variance in sizes of data sources and how well each semantic label is represented in the training data.
To this end, we consider including T2KMatch [17] into our benchmark as well as domain sets from the RODI benchmark [15].
References 1 Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, et al.
Tensorﬂow: A system for large-scale machine learning.
In Proc.
of OSDI, pages 265–283, 2016.
Zohra Bellahsene, Angela Bonifati, and Erhard Rahm, editors.
Schema Matching and Mapping.
Data-Centric Systems and Applications.
Springer, 2011.
Leo Breiman.
Bagging predictors.
Machine learning, 24(2):123–140, 1996.
Leo Breiman.
Random forests.
Machine learning, 45(1):5–32, 2001.
5 Nick Craswell.
Mean reciprocal rank.
In Encyclopedia of Database Systems, pages 1703– 6 AnHai Doan, Alon Y.
Halevy, and Zachary G.
Ives.
Principles of Data Integration.
Morgan Kaufmann, 2012.
Jay J Jiang and David W Conrath.
Semantic similarity based on corpus statistics and lexical taxonomy.
arXiv preprint cmp-lg/9709008, 1997.
8 Yann LeCun, Yoshua Bengio, and Geoﬀrey Hinton.
Deep learning.
Nature, 521(7553):436– 1703.
2009.
444, 2015.
9 Girija Limaye, Sunita Sarawagi, and Soumen Chakrabarti.
Annotating and searching web tables using entities, types and relationships.
Proc.
of the VLDB Endowment, 3(1-2):1338– 1347, 2010.
10 Dekang Lin et al.
An information-theoretic deﬁnition of similarity.
In Proc.
of ICML, 11 Christopher D Manning, Prabhakar Raghavan, Hinrich Schütze, et al.
Introduction to volume 98, pages 296–304, 1998.
information retrieval, volume 1.
2008.
12 Varish Mulwad, Tim Finin, and Anupam Joshi.
Semantic message passing for generating linked data from tables.
In Proc.
of ISWC, pages 363–378, 2013.
Saul B Needleman and Christian D Wunsch.
A general method applicable to the search for similarities in the amino acid sequence of two proteins.
Journal of molecular biology, 48(3):443–453, 1970.
13 14 Minh Pham, Suresh Alse, Craig A Knoblock, and Pedro Szekely.
Semantic labeling: a domain-independent approach.
In Proc.
of ISWC, pages 446–462.
Springer, 2016.
15 Christoph Pinkel, Carsten Binnig, Ernesto Jiménez-Ruiz, Evgeny Kharlamov, Wolfgang May, et al.
RODI: Benchmarking relational-to-ontology mapping generation quality.
Se- mantic Web, (Preprint):1–28, 2016.
SK Ramnandan, Amol Mittal, Craig A Knoblock, and Pedro Szekely.
Assigning semantic labels to data sources.
In Proc.
of ESWC, pages 403–417, 2015.
16 17 Dominique Ritze and Christian Bizer.
Matching web tables to dbpedia - A feature utility study.
In Proc.
of EDBT, pages 210–221, 2017.
18 Dominique Ritze, Oliver Lehmberg, and Christian Bizer.
Matching HTML tables to DB- pedia.
In Proc.
of WIMS, page 10, 2015.
19 David E Rumelhart, Geoﬀrey E Hinton, and Ronald J Williams.
Learning internal repres- entations by error propagation.
Technical report, DTIC Document, 1985.
XX:16 Semantic labeling 23 20 Dimitrios-Emmanuel Spanos, Periklis Stavrou, and Nikolas Mitrou.
Bringing relational databases into the semantic web: A survey.
Semantic Web, 3(2):169–209, 2012.
21 Mohsen Taheriyan, Craig A Knoblock, Pedro Szekely, and José Luis Ambite.
Leveraging linked data to discover semantic relations within data sources.
In Proc.
of ISWC, pages 549–565.
Springer, 2016.
22 Mohsen Taheriyan, Craig A.
Knoblock, Pedro A.
Szekely, and José Luis Ambite.
Learning the semantics of structured data sources.
J.
Web Sem., 37:152–169, 2016.
Petros Venetis, Alon Halevy, Jayant Madhavan, Marius Paşca, Warren Shen, Fei Wu, Gengxin Miao, and Chung Wu. Recovering semantics of tables on the web.
Proc.
of the VLDB Endowment, 4(9):528–538, 2011.

Recent studies have highlighted the lack of robustness in state-of-the-art neural network models, e.g., a visually imperceptible adversarial image can be easily crafted to mislead a well-trained network (Szegedy et al., 2013; Goodfellow et al., 2015; Chen et al., 2017a).
Even worse, researchers have identiﬁed that these adversarial examples are not only valid in the digital space but also plausible in the physical world (Kurakin et al., 2016a; Evtimov et al., 2017).
The vulnerability to adversarial examples calls into question safety-critical applications and services deployed by neural networks, including autonomous driving systems and malware detection protocols, among others.
In the literature, studying adversarial examples of neural networks has twofold purposes: (i) se- curity implications: devising effective attack algorithms for crafting adversarial examples, and (ii) robustness analysis: evaluating the intrinsic model robustness to adversarial perturbations to normal examples.
Although in principle the means of tackling these two problems are expected to be inde- pendent, that is, the evaluation of a neural network’s intrinsic robustness should be agnostic to attack methods, and vice versa, existing approaches extensively use different attack results as a measure of robustness of a target neural network.
Speciﬁcally, given a set of normal examples, the attack success rate and distortion of the corresponding adversarial examples crafted from a particular at- tack algorithm are treated as robustness metrics.
Consequently, the network robustness is entangled with the attack algorithms used for evaluation and the analysis is limited by the attack capabilities.
More importantly, the dependency between robustness evaluation and attack approaches can cause ∗Tsui-Wei Weng and Huan Zhang contributed equally Published as a conference paper at ICLR 2018 biased analysis.
For example, adversarial training is a commonly used technique for improving the robustness of a neural network, accomplished by generating adversarial examples and retraining the network with corrected labels.
However, while such an adversarially trained network is made robust to attacks used to craft adversarial examples for training, it can still be vulnerable to unseen attacks.
Motivated by the evaluation criterion for assessing the quality of text and image generation that is completely independent of the underlying generative processes, such as the BLEU score for texts (Papineni et al., 2002) and the INCEPTION score for images (Salimans et al., 2016), we aim to propose a comprehensive and attack-agnostic robustness metric for neural networks.
Stemming from a perturbation analysis of an arbitrary neural network classiﬁer, we derive a universal lower bound on the minimal distortion required to craft an adversarial example from an original one, where the lower bound applies to any attack algorithm and any (cid:96)p norm for p ≥ 1.
We show that this lower bound associates with the maximum norm of the local gradients with respect to the original ex- ample, and therefore robustness evaluation becomes a local Lipschitz constant estimation problem.
To efﬁciently and reliably estimate the local Lipschitz constant, we propose to use extreme value theory (De Haan & Ferreira, 2007) for robustness evaluation.
In this context, the extreme value corresponds to the local Lipschitz constant of our interest, which can be inferred by a set of inde- pendently and identically sampled local gradients.With the aid of extreme value theory, we propose a robustness metric called CLEVER, which is short for Cross Lipschitz Extreme Value for nEtwork Robustness.
We note that CLEVER is an attack-independent robustness metric that applies to any neural network classiﬁer.
In contrast, the robustness metric proposed in Hein & Andriushchenko (2017), albeit attack-agnostic, only applies to a neural network classiﬁer with one hidden layer.
We highlight the main contributions of this paper as follows: • We propose a novel robustness metric called CLEVER, which is short for Cross Lipschitz Extreme Value for nEtwork Robustness.
To the best of our knowledge, CLEVER is the ﬁrst robustness metric that is attack-independent and can be applied to any arbitrary neural network classiﬁer and scales to large networks for ImageNet.
• The proposed CLEVER score is well supported by our theoretical analysis on formal ro- bustness guarantees and the use of extreme value theory.
Our robustness analysis extends the results in Hein & Andriushchenko (2017) from continuously differentiable functions to a special class of non-differentiable functions – neural+ networks with ReLU activations.
• We corroborate the effectiveness of CLEVER by conducting experiments on state-of-the- art models for ImageNet, including ResNet (He et al., 2016), Inception-v3 (Szegedy et al., 2016) and MobileNet (Howard et al., 2017).
We also use CLEVER to investigate defended networks against adversarial examples, including the use of defensive distillation (Papernot et al., 2016) and bounded ReLU (Zantedeschi et al., 2017).
Experimental results show that our CLEVER score well aligns with the attack-speciﬁc robustness indicated by the (cid:96)2 and (cid:96)∞ distortions of adversarial examples.
2 BACKGROUND AND RELATED WORK 2.1 ATTACKING NEURAL NETWORKS USING ADVERSARIAL EXAMPLES One of the most popular formulations found in literature for crafting adversarial examples to mis- lead a neural network is to formulate it as a minimization problem, where the variable δ ∈ Rd to be optimized refers to the perturbation to the original example, and the objective function takes into account unsuccessful adversarial perturbations as well as a speciﬁc norm on δ for assuring similar- ity.
For instance, the success of adversarial examples can be evaluated by their cross-entropy loss (Szegedy et al., 2013; Goodfellow et al., 2015) or model prediction (Carlini & Wagner, 2017b).
The norm constraint on δ can be implemented in a clipping manner (Kurakin et al., 2016b) or treated as a i=1 |δi|p)1/p In particular, when p = ∞, for any p ≥ 1, is often used for crafting adversarial examples.
(cid:107)δ(cid:107)∞ = maxi∈{1,...,d} |δi| measures the maximal variation among all dimensions in δ.
When i=1 |δi| measures the total variation of δ.
The state-of-the-art attack methods for (cid:96)∞, (cid:96)2 and (cid:96)1 norms are the iterative fast gradient sign method (I-FGSM) (Goodfellow et al., 2015; Kurakin et al., 2016b), Carlini and Wagner’s attack (CW attack) (Carlini & Wagner, 2017b), and elastic-net attacks to deep neural net- works (EAD) (Chen et al., 2017b), respectively.
These attacks fall into the category of white-box attacks since the network model is assumed to be transparent to an attacker.
Adversarial examples penalty function (Carlini & Wagner, 2017b).
The (cid:96)p norm of δ, deﬁned as (cid:107)δ(cid:107)p = ((cid:80)d p = 2, (cid:107)δ(cid:107)2 becomes the Euclidean norm of δ.
When p = 1, (cid:107)δ(cid:107)1 = (cid:80)p Published as a conference paper at ICLR 2018 can also be crafted from a black-box network model using an ensemble approach (Liu et al., 2016), training a substitute model (Papernot et al., 2017), or employing zeroth-order optimization based attacks (Chen et al., 2017c).
2.2 EXISTING DEFENSE METHODS Since the discovery of vulnerability to adversarial examples (Szegedy et al., 2013), various defense methods have been proposed to improve the robustness of neural networks.
The rationale for defense is to make a neural network more resilient to adversarial perturbations, while ensuring the resulting defended model still attains similar test accuracy as the original undefended network.
Papernot et al.
proposed defensive distillation (Papernot et al., 2016), which uses the distillation technique (Hinton et al., 2015) and a modiﬁed softmax function at the ﬁnal layer to retrain the network parameters with the prediction probabilities (i.e., soft labels) from the original network.
Zantedeschi et al.
(2017) showed that by changing the ReLU function to a bounded ReLU function, a neural network can be made more resilient.
Another popular defense approach is adversarial training, which generates and augments adversarial examples with the original training data during the network training stage.
On MNIST, the adversarially trained model proposed by Madry et al.
(2017) can successfully defend a majority of adversarial examples at the price of increased network capacity.
Model ensemble has also been discussed to increase the robustness to adversarial examples (Tram`er et al., 2017; Liu et al., 2017).
In addition, detection methods such as feature squeezing (Xu et al., 2017) and example reforming (Meng & Chen, 2017) can also be used to identify adversarial examples.
However, the CW attack is shown to be able to bypass 10 different detection methods (Carlini & Wagner, 2017a).
In this paper, we focus on evaluating the intrinsic robustness of a neural network model to adversarial examples.
The effect of detection methods is beyond our scope.
2.3 THEORETICAL ROBUSTNESS GUARANTEES FOR NEURAL NETWORKS Szegedy et al.
(2013) compute global Lipschitz constant for each layer and use their product to explain the robustness issue in neural networks, but the global Lipschitz constant often gives a very loose bound.
Hein & Andriushchenko (2017) gave a robustness lower bound using a local Lipschitz continuous condition and derived a closed-form bound for a multi-layer perceptron (MLP) with a single hidden layer and softplus activation.
Nevertheless, a closed-form bound is hard to derive for a neural network with more than one hidden layer.
Wang et al.
(2016) utilized terminologies from topology to study robustness.
However, no robustness bounds or estimates were provided for neural networks.
On the other hand, works done by Ehlers (2017); Katz et al.
(2017a;b); Huang et al.
(2017) focus on formally verifying the viability of certain properties in neural networks for any possible input, and transform this formal veriﬁcation problem into satisﬁability modulo theory (SMT) and large-scale linear programming (LP) problems.
These SMT or LP based approaches have high computational complexity and are only plausible for very small networks.
Intuitively, we can use the distortion of adversarial examples found by a certain attack algorithm as a robustness metric.
For example, Bastani et al.
(2016) proposed a linear programming (LP) formula- tion to ﬁnd adversarial examples and use the distortions as the robustness metric.
They observe that the LP formulation can ﬁnd adversarial examples with smaller distortions than other gradient-based attacks like L-BFGS (Szegedy et al., 2013).
However, the distortion found by these algorithms is an upper bound of the true minimum distortion and depends on speciﬁc attack algorithms.
These methods differ from our proposed robustness measure CLEVER, because CLEVER is an estimation of the lower bound of the minimum distortion and is independent of attack algorithms.
Additionally, unlike LP-based approaches which are impractical for large networks, CLEVER is computationally feasible for large networks like Inception-v3.
The concept of minimum distortion and upper/lower bound will be formally deﬁned in Section 3.
3 ANALYSIS OF FORMAL ROBUSTNESS GUARANTEES FOR A CLASSIFIER In this section, we provide formal robustness guarantees of a classiﬁer in Theorem 3.2. Our robust- ness guarantees are general since they only require a mild assumption on Lipschitz continuity of the classiﬁcation function.
For differentiable classiﬁcation functions, our results are consistent with the main theorem in (Hein & Andriushchenko, 2017) but are obtained by a much simpler and more Published as a conference paper at ICLR 2018 Table 1: Table of Notation Notation Notation Deﬁnition dimensionality of the input vector ∆p,min number of output classes f : Rd → RK neural network classiﬁer x0 ∈ Rd xa ∈ Rd δ ∈ Rd (cid:107)δ(cid:107)p original input vector adversarial example distortion := xa − x0 (cid:96)p norm of distortion, p ≥ 1 Deﬁnition minimum (cid:96)p distortion of x0 lower bound of minimum distortion upper bound of minimum distortion Lipschitz constant local Lipschitz constant βL βU Lj Lj Bp(x0, R) hyper-ball with center x0 and radius R CDF cumulative distribution function q,x0 intuitive manner1.
Furthermore, our robustness analysis can be easily extended to non-differentiable classiﬁcation functions (e.g. neural networks with ReLU) as in Lemma 3.3, whereas the analysis in Hein & Andriushchenko (2017) is restricted to differentiable functions.
Speciﬁcally, Corollary 3.2.1 shows that the robustness analysis in (Hein & Andriushchenko, 2017) is in fact a special case of our analysis.
We start our analysis by deﬁning the notion of adversarial examples, minimum (cid:96)p distortions, and lower/upper bounds.
All the notations are summarized in Table 1.
Deﬁnition 3.1 (perturbed example and adversarial example).
Let x0 ∈ Rd be an input vector of a K-class classiﬁcation function f : Rd → RK and the prediction is given as c(x0) = argmax1≤i≤K fi(x0).
Given x0, we say xa is a perturbed example of x0 with noise δ ∈ Rd and (cid:96)p-distortion ∆p if xa = x0 + δ and ∆p = (cid:107)δ(cid:107)p.
An adversarial example is a perturbed exam- ple xa that changes c(x0).
A successful untargeted attack is to ﬁnd a xa such that c(xa) (cid:54)= c(x0) while a successful targeted attack is to ﬁnd a xa such that c(xa) = t given a target class t (cid:54)= c(x0).
Deﬁnition 3.2 (minimum adversarial distortion ∆p,min).
Given an input vector x0 of a classiﬁer f, the minimum (cid:96)p adversarial distortion of x0, denoted as ∆p,min, is deﬁned as the smallest ∆p over all adversarial examples of x0.
Deﬁnition 3.3 (lower bound of ∆p,min).
Suppose ∆p,min is the minimum adversarial distortion of x0.
A lower bound of ∆p,min, denoted by βL where βL ≤ ∆p,min, is deﬁned such that any perturbed examples of x0 with (cid:107)δ(cid:107)p ≤ βL are not adversarial examples.
Deﬁnition 3.4 (upper bound of ∆p,min).
Suppose ∆p,min is the minimum adversarial distortion of x0.
An upper bound of ∆p,min, denoted by βU where βU ≥ ∆p,min, is deﬁned such that there exists an adversarial example of x0 with (cid:107)δ(cid:107)p ≥ βU .
The lower and upper bounds are instance-speciﬁc because they depend on the input x0.
While βU can be easily given by ﬁnding an adversarial example of x0 using any attack method, βL is not easy to ﬁnd.
βL guarantees that the classiﬁer is robust to any perturbations with (cid:107)δ(cid:107)p ≤ βL, certifying the robustness of the classiﬁer.
Below we show how to derive a formal robustness guarantee of a classiﬁer with Lipschitz continuity assumption.
Speciﬁcally, our analysis obtains a lower bound of (cid:96)p minimum adversarial distortion βL = minj(cid:54)=c Lemma 3.1 (Lipschitz continuity and its relationship with gradient norm (Paulaviˇcius & ˇZilinskas, 2006)).
Let S ⊂ Rd be a convex bounded closed set and let h(x) : S → R be a continuously differentiable function on an open set containing S.
Then, h(x) is a Lipschitz function with Lipschitz constant Lq if the following inequality holds for any x, y ∈ S: fc(x0)−fj (x0) Lj (1) )(cid:62) is the gradient of h(x), |h(x) − h(y)| ≤ Lq(cid:107)x − y(cid:107)p, where Lq = max{(cid:107)∇h(x)(cid:107)q : x ∈ S},∇h(x) = ( ∂h(x) and 1 ∂x1 p + 1 q = 1, 1 ≤ p, q ≤ ∞.
,··· , ∂h(x) ∂xd Given Lemma 3.1, we then provide a formal guarantee to the lower bound βL.
Theorem 3.2 (Formal guarantee on lower bound βL for untargeted attack).
Let x0 ∈ Rd and f : Rd → RK be a multi-class classiﬁer with continuously differentiable components fi and let c = argmax1≤i≤K fi(x0) be the class which f predicts for x0.
For all δ ∈ Rd with (cid:107)δ(cid:107)p ≤ min j(cid:54)=c fc(x0) − fj(x0) Lj (2) 1 The authors in Hein & Andriushchenko (2017) implicitly assume Lipschitz continuity and use Mean Value Theorem and H¨older’s Inequality to prove their main theorem.
Here we provide a simple and direct proof with Lipschitz continuity assumption and without involving Mean Value Theorem and H¨older’s Inequality.
Published as a conference paper at ICLR 2018 argmax1≤i≤K fi(x0 + δ) = c holds with 1 q = 1, 1 ≤ p, q ≤ ∞ and Lj for the function fc(x) − fj(x) in (cid:96)p norm.
In other words, βL = minj(cid:54)=c bound of minimum distortion.
p + 1 q is the Lipschitz constant fc(x0)−fj (x0) is a lower Lj The intuitions behind Theorem 3.2 is shown in Figure 1 with an one-dimensional example.
The function value g(x) = fc(x) − fj(x) near point x0 is inside a double cone formed by two lines passing (x0, g(x0)) and with slopes equal to ±Lq, where Lq is the (local) Lipschitz constant of g(x) near x0.
In other words, the function value of g(x) around x0, i.e. g(x0 + δ) can be bounded by g(x0), δ and the Lipschitz constant Lq. When g(x0 + δ) is decreased to 0, an adversarial exam- ple is found and the minimal change of δ is g(x0) Lq The complete proof is deferred to Appendix A.
Remark 1.
Lj we also call it cross Lipschitz constant following (Hein & Andriushchenko, 2017).
q is the Lipschitz constant of the function involving cross terms: fc(x) − fj(x), hence Figure 1: Intuitions behind Theorem 3.2. To distinguish our analysis from (Hein & Andriushchenko, 2017), we show in Corollary 3.2.1 that we can obtain the same result in (Hein & Andriushchenko, 2017) by Theorem 3.2. In fact, the analysis in (Hein & Andriushchenko, 2017) is a special case of our analysis because the authors implicitly assume Lipschitz continuity on fi(x) when requiring fi(x) to be continuously differen- tiable.
They use local Lipschitz constant (Lq,x0) instead of global Lipschitz constant (Lq) to obtain a tighter bound in the adversarial perturbation δ.
Corollary 3.2.1 (Formal guarantee on βL for untargeted attack).
2 Let Lj be local Lipschitz constant of function fc(x)−fj(x) at x0 over some ﬁxed ball Bp(x0, R) := {x ∈ Rd | (cid:107)x−x0(cid:107)p ≤ R} and let δ ∈ Bp(0, R).
By Theorem 3.2, we obtain the bound in (Hein & Andriushchenko, 2017): (3) fc(x0) − fj(x0) (cid:26) (cid:27) , R q,x0 (cid:107)δ(cid:107)p ≤ min min j(cid:54)=c Lj q,x0 An important use case of Theorem 3.2 and Corollary 3.2.1 is the bound for targeted attack: Corollary 3.2.2 (Formal guarantee on βL for targeted attack).
Assume the same notation as For a speciﬁed target class j, we have (cid:107)δ(cid:107)p ≤ in Theorem 3.2 and Corollary 3.2.1. min(cid:8) fc(x0)−fj (x0) , R(cid:9).
Lj q,x0 In addition, we further extend Theorem 3.2 to a special case of non-differentiable functions – neural networks with ReLU activations.
In this case the Lipchitz constant used in Lemma 3.1 can be replaced by the maximum norm of directional derivative, and our analysis above will go through.
Lemma 3.3 (Formal guarantee on βL for ReLU networks).
3 Let h(·) be a l-layer ReLU neural network with Wi as the weights for layer i.
We ignore bias terms as they don’t contribute to gradient.
h(x) = σ(Wlσ(Wl−1 .
.
.
σ(W1x))) where σ(u) = max(0, u).
Let S ⊂ Rd be a convex bounded closed set, then equation (1) holds is the with Lq = supx∈S{| sup(cid:107)d(cid:107)p=1 D+h(x; d)|} where D+h(x; d) := limt→0+ one-sided directional direvative, then Theorem 3.2, Corollary 3.2.1 and Corollary 3.2.2 still hold.
h(x+td)−h(x) 4 THE CLEVER ROBUSTNESS METRIC VIA EXTREME VALUE THEORY In this section, we provide an algorithm to compute the robustness metric CLEVER with the aid of extreme value theory, where CLEVER can be viewed as an efﬁcient estimator of the lower bound βL and is the ﬁrst attack-agnostic score that applies to any neural network classiﬁers.
Recall in Section 3 2 proof deferred to Appendix B 3 proof deferred to Appendix C Published as a conference paper at ICLR 2018 we show that the lower bound of network robustness is associated with g(x0) and its cross Lipschitz q,x0, where g(x0) = fc(x0) − fj(x0) is readily available at the output of a classiﬁer and constant Lj q,x0 is deﬁned as maxx∈Bp(x0,R) (cid:107)∇g(x)(cid:107)q.
Although ∇g(x) can be calculated easily via back Lj q,x0 is more involved because it requires to obtain the maximum value of propagation, computing Lj (cid:107)∇g(x)(cid:107)q in a ball.
Exhaustive search on low dimensional x in Bp(x0, R) seems already infeasible, not to mention the image classiﬁers with large feature dimensions of our interest.
For instance, the feature dimension d = 784, 3072, 150528 for MNIST, CIFAR and ImageNet respectively.
q,x0 is through sampling a set of points x(i) in a ball Bp(x0, R) around One approach to compute Lj x0 and taking the maximum value of (cid:107)∇g(x(i))(cid:107)q.
However, a signiﬁcant amount of samples might be needed to obtain a good estimate of max(cid:107)∇g(x)(cid:107)q and it is unknown how good the estimate is compared to the true maximum.
Fortunately, Extreme Value Theory ensures that the maximum value of random variables can only follow one of the three extreme value distributions, which is useful to estimate max(cid:107)∇g(x)(cid:107)q with only a tractable number of samples.
It is worth noting that although Wood & Zhang (1996) also applied extreme value theory to estimate the Lipschitz constant.
However, there are two main differences between their work and this paper.
First of all, the sampling methodology is entirely different.
Wood & Zhang (1996) calculates the slopes between pairs of sample points whereas we directly take samples on the norm of gradient as in Lemma 3.1. Secondly, the functions considered in Wood & Zhang (1996) are only one-dimensional as opposed to the high-dimensional classiﬁcation functions considered in this paper.
For compari- son, we show in our experiment that the approach in Wood & Zhang (1996), denoted as SLOPE in Table 3 and Figure 4, perform poorly for high-dimensional classiﬁers such as deep neural networks.
4.1 ESTIMATE Lj q,x0 VIA EXTREME VALUE THEORY When sampling a point x uniformly in Bp(x0, R), (cid:107)∇g(x)(cid:107)q can be viewed as a random vari- able characterized by a cumulative distribution function (CDF).
For the purpose of illustration, we derived the CDF for a 2-layer neural network in Theorem D.1.4 For any neural networks, suppose we have n samples {(cid:107)∇g(x(i))(cid:107)q}, and denote them as a sequence of independent and identically distributed (iid) random variables Y1, Y2,··· , Yn, each with CDF FY (y).
The CDF of max{Y1,··· , Yn}, denoted as F n Y (y), is called the limit distribution of FY (y).
Fisher-Tippett- Gnedenko theorem says that F n Y (y), if exists, can only be one of the three family of extreme value distributions – the Gumbel class, the Fr´echet class and the reverse Weibull class.
Theorem 4.1 (Fisher-Tippett-Gnedenko Theorem).
If there exists a sequence of pairs of real num- Y (any + bn) = G(y), where G is a non-degenerate bers (an, bn) such that an > 0 and limn→∞ F n distribution function, then G belongs to either the Gumbel class (Type I), the Fr´echet class (Type II) or the Reverse Weibull class (Type III) with their CDFs as follows: y − aW (cid:3)(cid:9), Gumbel class (Type I): G(y) = exp(cid:8) − exp(cid:2) (cid:110) 0, (cid:0) y−aW (cid:0) aW −y (cid:110) exp{− Fr´echet class (Type II): G(y) = exp{− Reverse Weibull class (Type III): G(y) = bW (cid:1)−cW}, (cid:1)cW}, y ∈ R, if y < aW , if y ≥ aW , if y < aW , if y ≥ aW , bW bW 1, where aW ∈ R, bW > 0 and cW > 0 are the location, scale and shape parameters, respectively.
Theorem 4.1 implies that the maximum values of the samples follow one of the three families of distributions.
If g(x) has a bounded Lipschitz constant, (cid:107)∇g(x(i))(cid:107)q is also bounded, thus its limit distribution must have a ﬁnite right end-point.
We are particularly interested in the reverse Weibull class, as its CDF has a ﬁnite right end-point (denoted as aW ).
The right end-point reveals the upper limit of the distribution, known as the extreme value.
The extreme value is exactly the unknown local cross Lipschitz constant Lj q,x0, we ﬁrst generate Ns samples of x(i) over a ﬁxed ball Bp(x0, R) uniformly and independently in each batch with a total of Nb batches.
We then compute (cid:107)∇g(x(i))(cid:107)q and store the maximum values of each batch in set S.
Next, with samples in S, we perform a maximum likelihood estimation of reverse Weibull distribution parameters, and the location estimate ˆaW is used as an estimate of Lj 4 The theorem and proof are deferred to Appendix D.
q,x0 we would like to estimate in this paper.
To estimate Lj q,x0.
Published as a conference paper at ICLR 2018 4.2 COMPUTE CLEVER: A ROBUSTNESS SCORE OF NEURAL NETWORK CLASSIFIERS Given an instance x0, its classiﬁer f (x0) and a target class j, a targeted CLEVER score of the classiﬁer’s robustness can be computed via g(x0) and Lj q,x0.
Similarly, untargeted CLEVER scores can be computed.
With the proposed procedure of estimating Lj q,x0 described in Section 4.1, we summarize the ﬂow of computing CLEVER score for both targeted attacks and un-targeted attacks in Algorithm 1 and 2, respectively.
Nb, number of samples per batch Ns, perturbation norm p, maximum perturbation R Algorithm 1: CLEVER-t, compute CLEVER score for targeted attack Input: a K-class classiﬁer f (x), data example x0 with predicted class c, target class j, batch size Result: CLEVER Score µ ∈ R+ for target class j 1 S ← {∅}, g(x) ← fc(x) − fj(x), q ← p p−1.
2 for i ← 1 to Nb do for k ← 1 to Ns do randomly select a point x(i,k) ∈ Bp(x0, R) compute bik ← (cid:107)∇g(x(i,k))(cid:107)q via back propagation end S ← S ∪ {maxk{bik}} 8 end 9 ˆaW ← MLE of location parameter of reverse Weibull distribution on S 10 µ ← min( g(x0) , R) ˆa Algorithm 2: CLEVER-u, compute CLEVER score for un-targeted attack Input: Same as Algorithm 1, but without a target class j Result: CLEVER score ν ∈ R+ for un-targeted attack 1 for j ← 1 to K, j (cid:54)= c do 3 end 4 ν ← minj{µj} µj ← CLEVER-t(f, x0, c, j, Nb, Ns, p, R) 5 EXPERIMENTAL RESULTS 5.1 NETWORKS AND PARAMETER SETUP We conduct experiments on CIFAR-10 (CIFAR for short), MNIST, and ImageNet data sets.
For the former two smaller datasets CIFAR and MNIST, we evaluate CLEVER scores on four relatively small networks: a single hidden layer MLP with softplus activation (with the same number of hidden units as in (Hein & Andriushchenko, 2017)), a 7-layer AlexNet-like CNN (with the same structure as in (Carlini & Wagner, 2017b)), and the 7-layer CNN with defensive distillation (Papernot et al., 2016) (DD) and bounded ReLU (Zantedeschi et al., 2017) (BReLU) defense techniques employed.
For ImageNet data set, we use three popular deep network architectures: a 50-layer Residual Net- work (He et al., 2016) (ResNet-50), Inception-v3 (Szegedy et al., 2016) and MobileNet (Howard et al., 2017).
They were chosen for the following reasons: (i) they all yield (close to) state-of-the- art performance among equal-sized networks; and (ii) their architectures are signiﬁcantly different with unique building blocks, i.e., residual block in ResNet, inception module in Inception net, and depthwise separable convolution in MobileNet.
Therefore, their diversity in network architectures is appropriate to test our robustness metric.
For MobileNet, we set the width multiplier to 1.0, achiev- ing a 70.6% accuracy on ImageNet.
We used public pretrained weights for all ImageNet models5.
In all our experiments, we set the sampling parameters Nb = 500, Ns = 1024 and R = 5.
For targeted attacks, we use 500 test-set images for CIFAR and MNIST and use 100 test-set images for ImageNet; for each image, we evaluate its targeted CLEVER score for three targets: a random target class, a least likely class (the class with lowest probability when predicting the original example), 5 Pretrained models can be downloaded at https://github.com/tensorﬂow/models/tree/master/research/slim Published as a conference paper at ICLR 2018 and the top-2 class (the class with largest probability except for the true class, which is usually the easiest target to attack).
We also conduct untargeted attacks on MNIST and CIFAR for 100 test-set images, and evaluate their untargeted CLEVER scores.
Our experiment code is publicly available6.
5.2 FITTING GRADIENT NORM SAMPLES WITH REVERSE WEIBULL DISTRIBUTIONS We ﬁt the cross Lipschitz constant samples in S (see Algorithm 1) with reverse Weibull class dis- tribution to obtain the maximum likelihood estimate of the location parameter ˆaW , scale parameter ˆbW and shape parameter ˆcW , as introduced in Theorem 4.1. To validate that reverse Weibull distri- bution is a good ﬁt to the empirical distribution of the cross Lipschitz constant samples, we conduct Kolmogorov-Smirnov goodness-of-ﬁt test (a.k.a. K-S test) to calculate the K-S test statistics D and corresponding p-values.
The null hypothesis is that samples S follow a reverse Weibull distribution.
Figure 2 plots the probability distribution function of the cross Lipschitz constant samples and the ﬁtted Reverse Weibull distribution for images from various data sets and network architectures.
The estimated MLE parameters, p-values, and the K-S test statistics D are also shown.
We also calculate the percentage of examples whose estimation have p-values greater than 0.05, as illustrated in Figure 3.
If the p-value is greater than 0.05, the null hypothesis cannot be rejected, meaning that the underlying data samples ﬁt a reverse Weibull distribution well.
Figure 3 shows that all numbers are close to 100%, validating the use of reverse Weibull distribution as an underlying distribution of gradient norm samples empirically.
Therefore, the ﬁtted location parameter of reverse Weibull distribution (i.e., the extreme value), ˆaW , can be used as a good estimation of local cross Lipschitz constant to calculate the CLEVER score.
The exact numbers are shown in Table 5 in Appendix E.
(a) CIFAR-MLP (b) MNIST-CNN (c) ImageNet-MobileNet Figure 2: The cross Lipschitz constant samples for three images from CIFAR, MNIST and ImageNet datasets, and their ﬁtted Reverse Weibull distributions with the corresponding MLE estimates of location, scale and shape parameters (aW , bW , cW ) shown on the top of each plot.
The D-statistics of K-S test and p-values are denoted as ks and pval.
With small ks and high p-value, the hypothesized reverse Weibull distribution ﬁts the empirical distribution of cross Lipschitz constant samples well.
(a) Least likely target (b) Random target (c) Top 2 target Figure 3: The percentage of examples whose null hypothesis (the samples S follow a reverse Weibull distribution) cannot be rejected by K-S test with a signiﬁcance level of 0.05 for p = 2 and p = ∞.
All numbers for each model are close to 100%, indicating S ﬁts reverse Weibull distributions well.
6 Source code is available at https://github.com/huanzhang12/CLEVER 80859095100percentage(%)MobileNetResnetInceptionCIFAR-BReLUCIFAR-DDCIFAR-CNNCIFAR-MLPMNIST-BReLUMNIST-DDMNIST-CNNMNIST-MLPp=∞p=280859095100percentage(%)MobileNetResnetInceptionCIFAR-BReLUCIFAR-DDCIFAR-CNNCIFAR-MLPMNIST-BReLUMNIST-DDMNIST-CNNMNIST-MLPp=∞p=280859095100percentage(%)MobileNetResnetInceptionCIFAR-BReLUCIFAR-DDCIFAR-CNNCIFAR-MLPMNIST-BReLUMNIST-DDMNIST-CNNMNIST-MLPp=∞p=2Published as a conference paper at ICLR 2018 5.3 COMPARING CLEVER SCORE WITH ATTACK-SPECIFIC NETWORK ROBUSTNESS We apply the state-of-the-art white-box attack methods, iterative fast gradient sign method (I- FGSM) (Goodfellow et al., 2015; Kurakin et al., 2016b) and Carlini and Wagner’s attack (CW) (Carlini & Wagner, 2017b), to ﬁnd adversarial examples for 11 networks, including 4 networks trained on CIFAR, 4 networks trained on MNIST, and 3 networks trained on ImageNet.
For CW attack, we run 1000 iterations for ImageNet and CIFAR, and 2000 iterations for MNIST, as MNIST has shown to be more difﬁcult to attack (Chen et al., 2017b).
Attack learning rate is individually tuned for each model: 0.001 for Inception-v3 and ResNet-50, 0.0005 for Mo- bileNet and 0.01 for all other networks.
For I-FGSM, we run 50 iterations and choose the optimal  ∈ {0.01, 0.025, 0.05, 0.1, 0.3, 0.5, 0.8, 1.0} to achieve the smallest (cid:96)∞ distortion for each individ- ual image.
For defensively distilled (DD) networks, 50 iterations of I-FGSM are not sufﬁcient; we use 250 iterations for CIFAR-DD and 500 iterations for MNIST-DD to achieve a 100% success rate.
For the problem to be non-trivial, images that are classiﬁed incorrectly are skipped.
We report 100% attack success rates for all the networks, and thus the average distortion of adversarial examples can indicate the attack-speciﬁc robustness of each network.
For comparison, we compute the CLEVER scores for the same set of images and attack targets.
To the best of our knowledge, CLEVER is the ﬁrst attack-independent robustness score that is capable of handling the large networks studied in this paper, so we directly compare it with the attack-induced distortion metrics in our study.
We evaluate the effectiveness of our CLEVER score by comparing the upper bound βU (found by attacks) and CLEVER score, where CLEVER serves as an estimated lower bound, βL.
Table 3 compares the average (cid:96)2 and (cid:96)∞ distortions of adversarial examples found by targeted CW and I-FGSM attacks and the corresponding average targeted CLEVER scores for (cid:96)2 and (cid:96)∞ norms, and Figure 4 visualizes the results for (cid:96)∞ norm.
Similarly, Table 2 compares untargeted CW and I-FGSM attacks with untargeted CLEVER scores.
As expected, CLEVER is smaller than the dis- tortions of adversarial images in most cases.
More importantly, since CLEVER is independent of attack algorithms, the reported CLEVER scores can roughly indicate the distortion of the best pos- sible attack in terms of a speciﬁc (cid:96)p distortion.
The average (cid:96)2 distortion found by CW attack is close to the (cid:96)2 CLEVER score, indicating CW is a strong (cid:96)2 attack.
In addition, when a defense mechanism (Defensive Distillation or Bounded ReLU) is used, the corresponding CLEVER scores are consistently increased (except for CIFAR-BReLU), indicating that the network is indeed made more resilient to adversarial perturbations.
For CIFAR-BReLU, both CLEVER scores and (cid:96)p norm of adversarial examples found by CW attack decrease, implying that bound ReLU is an ineffective defense for CIFAR.
CLEVER scores can be seen as a security checkpoint for unseen attacks.
For example, if there is a substantial gap in distortion between the CLEVER score and the considered attack algorithms, it may suggest the existence of a more effective attack that can close the gap.
Since CLEVER score is derived from an estimation of the robustness lower bound, we further verify the viability of CLEVER per each example, i.e., whether it is usually smaller than the upper bound found by attacks.
Table 4 shows the percentage of inaccurate estimations where the CLEVER score is larger than the distortion of adversarial examples found by CW and I-FGSM attacks in three ImageNet networks.
We found that CLEVER score provides an accurate estimation for most of the examples.
For MobileNet and Resnet-50, our CLEVER score is a strict lower bound of these two attacks for more than 96% of tested examples.
For Inception-v3, the condition of strict lower bound Table 2: Comparison between the average untargeted CLEVER score and distortion found by CW and I-FGSM untargeted attacks.
DD and BReLU represent Defensive Distillation and Bounded ReLU defending methods applied to the baseline CNN network.
CW I-FGSM (cid:96)2 (cid:96)2 1.113 MNIST-MLP 1.500 MNIST-CNN MNIST-DD 1.548 MNIST-BReLU 1.337 0.253 CIFAR-MLP 0.195 CIFAR-CNN CIFAR-DD 0.285 CIFAR-BReLU 0.159 (cid:96)∞ 0.215 0.455 0.409 0.433 0.018 0.023 0.032 0.019 3.564 4.439 5.617 3.851 0.885 0.721 1.136 0.519 (cid:96)∞ 0.178 0.288 0.283 0.285 0.016 0.018 0.024 0.013 CLEVER (cid:96)∞ (cid:96)2 0.041 0.057 0.063 0.065 0.005 0.002 0.004 0.001 0.819 0.721 0.865 0.833 0.219 0.072 0.130 0.045 Published as a conference paper at ICLR 2018 Table 3: Comparison of the average targeted CLEVER scores with average (cid:96)∞ and (cid:96)2 distortions found by CW, I-FSGM attacks, and the average scores calculated by using the algorithm in Wood & Zhang (1996) (denoted as SLOPE) to estimate Lipschitz constant.
DD and BReLU denote Defensive Distillation and Bounded ReLU defending methods applied to the CNN network.
We did not include SLOPE in ImageNet networks because it has been shown to be ineffective even for smaller networks.
(a) avergage (cid:96)∞ distortion of CW and I-FGSM targeted attacks, and CLEVER and SLOPE estimation.
Some very large SLOPE estimates (in parentheses) exceeding the maximum possible (cid:96)∞ distortion are reported as 1.
Least Likely Target CW I-FGSM CLEVER 0.475 MNIST-MLP MNIST-CNN 0.601 MNIST-DD 0.578 MNIST-BReLU 0.601 0.086 CIFAR-MLP 0.053 CIFAR-CNN CIFAR-DD 0.091 CIFAR-BReLU 0.045 0.023 Inception-v3 0.031 Resnet-50 MobileNet 0.025 0.071 0.090 0.103 0.257 0.014 0.005 0.011 0.004 0.002 0.002 0.003 0.223 0.313 0.283 0.276 0.039 0.033 0.053 0.030 0.011 0.015 0.010 SLOPE 0.808 0.996 1 (1.090) 1 (5.327) 0.294 0.153 0.278 0.250 Random Target CW I-FGSM CLEVER SLOPE 0.337 0.813 0.982 0.550 0.953 0.531 3.907 0.544 0.284 0.051 0.148 0.042 0.255 0.066 0.034 0.173 0.021 0.025 0.018 0.173 0.264 0.238 0.238 0.024 0.023 0.032 0.022 0.012 0.012 0.010 0.072 0.088 0.091 0.187 0.014 0.005 0.010 0.003 0.002 0.002 0.002 Top-2 Target CW I-FGSM CLEVER 0.218 0.451 0.412 0.442 0.019 0.022 0.033 0.018 0.010 0.010 0.006 0.119 0.211 0.165 0.196 0.013 0.013 0.014 0.012 0.011 0.010 0.010 0.069 0.070 0.091 0.117 0.014 0.004 0.007 0.002 0.001 0.001 0.001 SLOPE 0.786 0.826 0.984 1 (2.470) 0.286 0.129 0.184 0.095 (b) average (cid:96)2 distortion of CW and I-FGSM targeted attacks, and CLEVER and SLOPE estimation.
Some very large SLOPE estimates (in parentheses) exceeding the sampling radius R = 5 are reported as 5.
Least Likely Target CW I-FGSM CLEVER 2.575 MNIST-MLP 2.377 MNIST-CNN MNIST-DD 2.644 MNIST-BReLU 2.349 1.123 CIFAR-MLP 0.836 CIFAR-CNN CIFAR-DD 2.065 CIFAR-BReLU 0.407 0.628 Inception-v3 0.767 Resnet-50 MobileNet 0.837 1.409 1.257 1.532 3.312 0.620 0.156 0.347 0.140 0.524 0.357 0.617 4.273 4.417 4.957 5.170 1.896 1.067 1.540 0.928 2.244 2.410 2.195 SLOPE 5 (8.028) 5 (9.947) 5 (10.628) 5 (52.058) 5 (5.013) 2.630 4.735 4.125 Random Target CW I-FGSM CLEVER 1.833 2.005 2.240 1.923 0.673 0.372 0.624 0.303 0.595 0.647 0.603 3.369 3.902 4.253 4.544 1.214 0.837 1.097 0.732 2.261 2.098 2.066 1.432 1.227 1.340 2.565 0.597 0.146 0.307 0.103 0.466 0.299 0.439 SLOPE 5 (8.102) 5 (9.619) 5 (9.493) 5 (37.531) 4.806 2.497 4.279 2.944 Top-2 Target CW I-FGSM CLEVER 1.128 1.504 1.542 1.404 0.262 0.188 0.296 0.152 0.287 0.212 0.190 2.374 3.242 3.010 3.778 0.689 0.552 0.582 0.494 2.073 1.682 1.771 1.383 0.987 1.330 1.583 0.599 0.123 0.220 0.052 0.234 0.134 0.144 SLOPE 5 (7.853) 5 (7.921) 5 (9.646) 5 (23.548) 4.949 2.195 3.083 1.564 (a) MNIST: Least likely target (b) MNIST: Random target (c) MNIST: Top 2 target (d) CIFAR: Least likely target (e) CIFAR: Random target (f) CIFAR: Top 2 target Figure 4: Comparison of (cid:96)∞ distortion obtained by CW and I-FGSM attacks, CLEVER score and the slope based Lipschitz constant estimation (SLOPE) by Wood & Zhang (1996).
SLOPE signiﬁcantly exceeds the distortions found by attacks, thus it is an inappropriate estimation of lower bound βL.
is worse (still more than 75%), but we found that in these cases the attack distortion only differs from our CLEVER score by a fairly small amount.
In Figure 5 we show the empirical CDF of the gap between CLEVER score and the (cid:96)2 norm of adversarial distortion generated by CW attack for the same set of images in Table 4.
In Figure 6, we plot the (cid:96)2 distortion and CLEVER scores for each 10 Published as a conference paper at ICLR 2018 Table 4: Percentage of images in ImageNet where the CLEVER score for that image is greater than the adversarial distortion found by different attacks.
Least Likely Target CW I-FGSM Random Target CW I-FGSM Top-2 Target CW I-FGSM MobileNet Resnet-50 Inception-v3 (a) MobileNet L∞ L2 L∞ L2 0% 0% 0% 2% 0% 0% 0% 2% L∞ L2 L∞ L2 0% 0% 0% 4% 4% 0% 0% 0% 25% 0% 0% 0% 23% 0% 0% 0% 15% 0% 0% 0% (c) Inception-v3 L∞ L2 L∞ L2 0% 0% 0% 0% 0% 0% 0% 1% (b) ResNet-50 Figure 5: The empirical CDF of the gap between CLEVER score and the (cid:96)2 norm of adversarial distortion generated by CW attack with random targets for 100 images on 3 ImageNet networks.
(a) MobileNet (b) ResNet-50 (c) Inception-v3 Figure 6: Comparison of the CLEVER scores (circle) and the (cid:96)2 norm of adversarial distortion generated by CW attack (triangle) with random targets for 100 images.
The x-axis is image ID and the y-axis is the (cid:96)2 distortion metric.
(a) Least likely target (b) Random target (c) Top-2 target Figure 7: Comparison of the CLEVER score calculated by Nb = {50, 100, 250, 500} and the (cid:96)2 norm of adversarial distortion found by CW attack (CW) on 3 ImageNet models and 3 target types.
individual image.
A positive gap indicates that CLEVER (estimated lower bound) is indeed less than the upper bound found by CW attack.
Most images have a small positive gap, which signiﬁes the near-optimality of CW attack in terms of (cid:96)2 distortion, as CLEVER sufﬁces for an estimated capacity of the best possible attack.
5.4 TIME V.S. ESTIMATION ACCURACY In Figure 7, we vary the number of samples (Nb = 50, 100, 250, 500) and compute the (cid:96)2 CLEVER scores for three large ImageNet models, Inception-v3, ResNet-50 and MobileNet.
We observe that 11 −0.50.00.51.0gap0.00.20.40.60.81.0CDF−0.50.00.51.0gap0.00.20.40.60.81.0CDF−0.50.00.51.0gap0.00.20.40.60.81.0CDFPublished as a conference paper at ICLR 2018 50 or 100 samples are usually sufﬁcient to obtain a reasonably accurate robustness estimation despite using a smaller number of samples.
On a single GTX 1080 Ti GPU, the cost of 1 sample (with Ns = 1024) is measured as 2.9 s for MobileNet, 5.0 s for ResNet-50 and 8.9 s for Inception-v3, thus the computational cost of CLEVER is feasible for state-of-the-art large-scale deep neural networks.
Additional ﬁgures for MNIST and CIFAR datasets are given in Appendix E.
6 CONCLUSION In this paper, we propose the CLEVER score, a novel and generic metric to evaluate the robustness of a target neural network classiﬁer to adversarial examples.
Compared to the existing robustness evaluation approaches, our metric has the following advantages: (i) attack-agnostic; (ii) applicable to any neural network classiﬁer; (iii) comes with strong theoretical guarantees; and (iv) is computa- tionally feasible for large neural networks.
Our extensive experiments show that the CLEVER score well matches the practical robustness indication of a wide range of natural and defended networks.
Acknowledgment.
Luca Daniel and Tsui-Wei Weng are partially supported by MIT-Skoltech pro- gram and MIT-IBM Watson AI Lab.
Cho-Jui Hsieh and Huan Zhang acknowledge the support of NSF via IIS-1719097.
REFERENCES Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis, Aditya Nori, and An- tonio Criminisi.
Measuring neural net robustness with constraints.
In Advances in Neural Infor- mation Processing Systems, pp.
2613–2621, 2016.
Nicholas Carlini and David Wagner.
Adversarial examples are not easily detected: Bypassing ten detection methods.
arXiv preprint arXiv:1705.07263, 2017a.
Nicholas Carlini and David Wagner.
Towards evaluating the robustness of neural networks.
In IEEE Symposium on Security and Privacy (SP), pp.
39–57, 2017b.
Hongge Chen, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, and Cho-Jui Hsieh.
Show-and-fool: Crafting adversarial examples for neural image captioning.
CoRR, abs/1712.02051, 2017a.
Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh.
Ead: Elastic-net attacks to deep neural networks via adversarial examples.
arXiv preprint arXiv:1709.04114, 2017b.
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh.
Zoo: Zeroth order op- timization based black-box attacks to deep neural networks without training substitute models.
arXiv preprint arXiv:1708.03999, 2017c.
Laurens De Haan and Ana Ferreira.
Extreme value theory: an introduction.
Springer Science & Business Media, 2007.
Ruediger Ehlers.
Formal veriﬁcation of piece-wise linear feed-forward neural networks.
arXiv preprint arXiv:1705.01320, 2017.
Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul Prakash, Amir Rahmati, and Dawn Song.
Robust physical-world attacks on machine learning models.
arXiv preprint arXiv:1707.08945, 2017.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.
Explaining and harnessing adversarial examples.
ICLR’15; arXiv preprint arXiv:1412.6572, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recog- nition.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016.
Matthias Hein and Maksym Andriushchenko.
Formal guarantees on the robustness of a classiﬁer against adversarial manipulation.
arXiv preprint arXiv:1705.08475, 2017.
12 Published as a conference paper at ICLR 2018 Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
Distilling the knowledge in a neural network.
arXiv preprint arXiv:1503.02531, 2015.
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam.
Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications.
arXiv preprint arXiv:1704.04861, 2017.
Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. Safety veriﬁcation of deep neural networks.
In International Conference on Computer Aided Veriﬁcation, pp.
3–29.
Springer, 2017.
Guy Katz, Clark Barrett, David Dill, Kyle Julian, and Mykel Kochenderfer.
Reluplex: An efﬁcient smt solver for verifying deep neural networks.
arXiv preprint arXiv:1702.01135, 2017a.
Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer.
Towards proving the adversarial robustness of deep neural networks.
arXiv preprint arXiv:1709.02802, 2017b.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio.
Adversarial examples in the physical world.
arXiv preprint arXiv:1607.02533, 2016a.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio.
Adversarial machine learning at scale.
ICLR’17; arXiv preprint arXiv:1611.01236, 2016b.
Xuanqing Liu, Minhao Cheng, Huan Zhang, and Cho-Jui Hsieh.
Towards robust neural networks via random self-ensemble.
arXiv preprint arXiv:1712.00673, 2017.
Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song.
Delving into transferable adversarial exam- ples and black-box attacks.
arXiv preprint arXiv:1611.02770, 2016.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks.
arXiv preprint arXiv:1706.06083, 2017.
Dongyu Meng and Hao Chen.
Magnet: a two-pronged defense against adversarial examples.
arXiv preprint arXiv:1705.09064, 2017.
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami.
Distillation as In IEEE Symposium on a defense to adversarial perturbations against deep neural networks.
Security and Privacy (SP), pp.
582–597, 2016.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami.
Practical black-box attacks against machine learning.
In ACM Asia Conference on Com- puter and Communications Security, pp.
506–519, 2017.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
Bleu: a method for automatic evaluation of machine translation.
In Proceedings of the 40th annual meeting on association for computational linguistics, pp.
311–318.
Association for Computational Linguistics, 2002.
Remigijus Paulaviˇcius and Julius ˇZilinskas.
Analysis of different norms and corresponding lipschitz constants for global optimization.
Technological and Economic Development of Economy, 12(4): 301–306, 2006.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans.
In Advances in Neural Information Processing Systems, pp.
2234–2242, 2016.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus.
Intriguing properties of neural networks.
arXiv preprint arXiv:1312.6199, 2013.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna.
Rethink- ing the inception architecture for computer vision.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
2818–2826, 2016.
Florian Tram`er, Alexey Kurakin, Nicolas Papernot, Dan Boneh, and Patrick McDaniel.
Ensemble adversarial training: Attacks and defenses.
arXiv preprint arXiv:1705.07204, 2017.
13 Published as a conference paper at ICLR 2018 Beilun Wang, Ji Gao, and Yanjun Qi. A theoretical framework for robustness of (deep) classiﬁers under adversarial noise.
arXiv preprint arXiv:1612.00334, 2016.
GR Wood and BP Zhang.
Estimation of the lipschitz constant of a function.
Journal of Global Optimization, 8(1):91–103, 1996.
Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep neural networks.
arXiv preprint arXiv:1704.01155, 2017.
Valentina Zantedeschi, Maria-Irina Nicolae, and Ambrish Rawat.
Efﬁcient defenses against adver- sarial attacks.
arXiv preprint arXiv:1707.06728, 2017.
14 Published as a conference paper at ICLR 2018 APPENDIX A PROOF OF THEOREM 3.2 Proof.
According to Lemma 3.1, the assumption that g(x) := fc(x)−fj(x) is Lipschitz continuous with Lipschitz constant Lj (4) q gives Let x = x0 + δ and y = x0 in (4), we get which can be rearranged into the following form q(cid:107)x − y(cid:107)p.
|g(x) − g(y)| ≤ Lj |g(x0 + δ) − g(x0)| ≤ Lj q(cid:107)δ(cid:107)p ≤ g(x0 + δ) ≤ g(x0) + Lj q(cid:107)δ(cid:107)p, q(cid:107)δ(cid:107)p.
g(x0) − Lj (5) When g(x0 + δ) = 0, an adversarial example is found.
As indicated by (5), g(x0 + δ) is lower q(cid:107)δ(cid:107)p ≥ 0, no adversarial bounded by g(x0)−Lj examples can be found: q(cid:107)δ(cid:107)p.
If (cid:107)δ(cid:107)p is small enough such that g(x0)−Lj g(x0) − Lj q(cid:107)δ(cid:107)p ≥ 0 ⇒ (cid:107)δ(cid:107)p ≤ g(x0) q ⇒ (cid:107)δ(cid:107)p ≤ Lj fc(x0) − fj(x0) Lj Finally, to achieve argmax1≤i≤K fi(x0 + δ) = c, we take the minimum of the bound on (cid:107)δ(cid:107)p in (A) over j (cid:54)= c.
I.e. if (cid:107)δ(cid:107)p ≤ min j(cid:54)=c fc(x0) − fj(x0) Lj the classiﬁer decision can never be changed and the attack will never succeed.
B PROOF OF COROLLARY 3.2.1 Proof.
By Lemma 3.1 and let g = fc − fj, we get Lj = maxy∈Bp(x0,R) (cid:107)∇g(y)(cid:107)q = maxy∈Bp(x0,R) (cid:107)∇fj(y) − ∇fc(y)(cid:107)q, which then gives the bound in Theorem 2.1 of (Hein & An- driushchenko, 2017).
q,x0 C PROOF OF LEMMA 3.3 Proof.
For any x, y, let d = y−x be the unit vector pointing from x to y and r = (cid:107)y − x(cid:107)p.
(cid:107)y−x(cid:107)p Deﬁne uni-variate function u(z) = h(x + zd), then u(0) = h(x) and u(r) = h(y) and observe that D+h(x + zd; d) and D+h(x + zd;−d) are the right-hand and left-hand derivatives of u(z), we have (cid:26)D+h(x + zd; d) ≤ Lq undeﬁned u(cid:48)(z) = if D+h(x + zd; d) = D+h(x + zd;−d) if D+h(x + zd; d) (cid:54)= D+h(x + zd;−d) For ReLU network, there can be at most ﬁnite number of points in z ∈ (0, r) such that g(cid:48)(z) does not exist.
This can be shown because each discontinuous z is caused by some ReLU activation, and there are only ﬁnite combinations.
Let 0 = z0 < z1 < ··· < zk−1 < zk = 1 be those points.
Then, using the fundamental theorem of calculus on each interval separately, there exists ¯zi ∈ (zi, zi−1) for each i such that u(r) − u(0) ≤ |u(zi) − u(zi−1)| |u(cid:48)(¯zi)(zi − zi−1)| i=1 k(cid:88) k(cid:88) k(cid:88) i=1 (Mean value theorem) (zi are in line (x, y)) Lq|zi − zi−1|p i=1 = Lq(cid:107)x − y(cid:107)p.
15 Theorem 3.2 and its corollaries remain valid after replacing Lemma 3.1 with Lemma 3.3. Published as a conference paper at ICLR 2018 D THEOREM D.1 AND ITS PROOF Theorem D.1 (FY (y) of one-hidden-layer neural network).
Consider a neural network f : Rd → (cid:1) pieces, RK with input x0 ∈ Rd, a hidden layer with U hidden neurons, and rectiﬁed linear unit (ReLU) activation function.
If we sample uniformly in a ball Bp(x0, R), then the cumulative distribution function of (cid:107)∇g(x)(cid:107)q, denoted as FY (y), is piece-wise linear with at most M =(cid:80)d (cid:0)U i=0 where g(x) = fc(x) − fj(x) for some given c and j, and 1 (cid:33) Proof.
The jth output of a one-hidden-layer neural network can be written as q = 1, 1 ≤ p, q ≤ ∞.
U(cid:88) (cid:32) d(cid:88) U(cid:88) p + 1 fj(x) = Vjr · σ r=1 i=1 Wri · xi + br Vjr · σ (wrx + br) , r=1 where σ(z) = max(z, 0) is ReLU activation function, W and V are the weight matrices of the ﬁrst and second layer respectively, and wr is the rth row of W .
Thus, we can compute g(x) and (cid:107)∇g(x)(cid:107)q below: r=1 U(cid:88) U(cid:88) (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) U(cid:88) r=1 r=1 g(x) = fc(x) − fj(x) = (cid:107)∇g(x)(cid:107)q = and where I(z) is an univariate indicator function: Vcr · σ (wrx + br) − Vjr · σ (wrx + br) U(cid:88) r=1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)q (Vcr − Vjr) · σ (wrx + br) I(wrx + br)(Vcr − Vjr)w(cid:62) (cid:110) 1, 0, if z > 0, if z ≤ 0.
I(z) = Figure 8: Illustration of Theorem D.1 with d = 2, q = 2 and U = 3.
The three hyperplanes wix + bi = 0 divide the space into seven regions (with different colors).
The red dash line encloses the ball B2(x0, R1) and the blue dash line encloses a larger ball B2(x0, R2).
If we draw samples uniformly within the balls, the probability of (cid:107)∇g(x)(cid:107)2 = y is propor- tional to the intersected volumes of the ball and the regions with (cid:107)∇g(x)(cid:107)2 = y.
As illustrated in Figure 8, the hyperplanes wrx + br = 0, r ∈ {1, .
.
.
, U} divide the d dimensional spaces Rd into different regions, with the interior of each region satisfying a different set of inequal- ity constraints, e.g. wr+x + br+ > 0 and wr− x + br− < 0.
Given x, we can identify which region it belongs to by checking the sign of wrx + br for each r.
Notice that the gradient norm is the same for all the points in the same region, i.e. for any x1, x2 satisfying I(wrx1 + br) = I(wrx2 + br) ∀r, 16 Published as a conference paper at ICLR 2018 we have (cid:107)∇g(x1)(cid:107)q = (cid:107)∇g(x2)(cid:107)q.
Since there can be at most M =(cid:80)d i=0 (cid:0)U (cid:1) different regions for a d-dimensional space with U hyperplanes, (cid:107)∇g(x)(cid:107)q can take at most M different values.
Therefore, if we perform uniform sampling in a ball Bp(x0, R) centered at x0 with radius R and denote (cid:107)∇g(x)(cid:107)q as a random variable Y , the probability distribution of Y is discrete and its CDF is piece-wise constant with at most M pieces.
Without loss of generality, assume there are M0 ≤ M distinct values for Y and denote them as m(1), m(2), .
.
.
, m(M0) in an increasing order, the CDF of Y , denoted as FY (y), is the following: FY (m(i)) = FY (m(i−1)) + Vd({x | (cid:107)∇g(x)(cid:107)q = m(i)}) ∩ Vd(Bp(x0, R))) Vd(Bp(x0, R)) , i = 1, .
.
.
, M0, where FY (m(0)) = 0 with m(0) < m(1), Vd(E) is the volume of E in a d dimensional space.
E ADDITIONAL EXPERIMENTAL RESULTS E.1 PERCENTAGE OF EXAMPLES HAVING P VALUE > 0.05 Table 5 shows the percentage of examples where the null hypothesis cannot be rejected by K-S test, indicating that the maximum gradient norm samples ﬁt reverse Weibull distribution well.
Table 5: Percentage of estimations where the null hypothesis cannot be rejected by K-S test for a signiﬁcance level of 0.05.
The bar plots of this table are illustrated in Figure 3.
Least Likely L∞ L2 100.0 100.0 MNIST-MLP 99.8 99.6 MNIST-CNN 99.8 100.0 MNIST-DD 95.4 MNIST-BReLU 93.3 100.0 100.0 CIFAR-MLP 100.0 100.0 CIFAR-CNN 99.5 CIFAR-DD 99.7 CIFAR-BReLU 99.5 99.2 100.0 100.0 Inception-v3 100.0 99.0 Resnet-50 MobileNet 100.0 100.0 Random L2 100.0 99.2 99.6 96.8 100.0 100.0 100.0 100.0 100.0 100.0 100.0 L∞ 100.0 100.0 99.8 96.8 100.0 100.0 100.0 100.0 100.0 100.0 100.0 Top-2 L2 100.0 99.4 99.8 97.6 100.0 100.0 99.7 99.7 100.0 100.0 98.0 L∞ 100.0 100.0 99.8 98.2 100.0 100.0 99.7 99.7 100.0 100.0 99.0 E.2 CLEVER V.S. NUMBER OF SAMPLES Figure 9 shows the (cid:96)2 CLEVER score with different number of samples (Nb = 50, 100, 250, 500) for MNIST and CIFAR models.
For most models except MNIST-BReLU, reducing the number of samples only change CLEVER scores very slightly.
For MNIST-BReLU, increasing the number of samples improves the estimated lower bound, suggesting that a larger number of samples is preferred.
In practice, we can start with a relatively small Nb = a, and also try 2a, 4a,··· samples to see if CLEVER scores change signiﬁcantly.
If CLEVER scores stay roughly the same despite increasing Nb, we can conclude that using Nb = a is sufﬁcient.
17 Published as a conference paper at ICLR 2018 (a) MNIST, Least likely target (b) MNIST, Random target (c) MNIST, Top-2 target (d) CIFAR, Least likely target (e) CIFAR, Random target (f) CIFAR, Top2 target Figure 9: Comparison of the CLEVER score calculated by Nb = {50, 100, 250, 500} and the (cid:96)2 norm of adversarial distortion found by CW attack (CW) on MNIST and CIFAR models with 3 target types.
18
With the fast development of sophisticated machine learning algorithms, artiﬁcial intelligence has been gradually penetrat- ing a number of of brand new ﬁelds with unprecedented speed.
One of the outstanding problems hampering further progress is the interpretability challenge.
This challenge arises when the models build by the machine learning algorithms are to be used by humans in their decision making, particularly when such decisions are subject to legal consequences and/or administrative audits.
For human decision makers operating in those circumstances, to accept the professional and legal responsibility ensuing from decisions assisted by machine to comprehend the models.
This is learning, generally true for areas like criminal justice, health care, terrorism detection, education system and ﬁnancial markets.
To trust the model, decision makers need to ﬁrst under- stand the model’s behavior, and then evaluate and reﬁne the model using their domain knowledge.
Even for areas like book or movie recommendations [1] and automated aids [2], is critical it explanations for a recommendation and an error made could increase the trust and reliance on these systems.
Furthermore, the European General Data Protection Regulation, forthcoming in June, 2018, stipulates the explainability of all automatically made decisions concerning individuals, and that includes the decisions made with or assisted by machine learning models.
Hence, there is a growing demand for interpretability of the machine learning algorithms.
it In this paper, we deﬁne interpretability of a model as the ability to provide visual or textual presentation of the connections between input features and the output predictions.
To realize the goal of interpretability, there are usually two approaches.
One is to design an algorithm that is inherently interpretable, while achieving competitive accuracy of a com- plex model.
The examples are Decision Trees [3], Decision Lists [4], and Decision Sets [5], etc.
The disadvantage of this approach is that there is a trade off between interpretability and accuracy: is not easy to learn an interpretable (so presumably simple) model expressing a complex process with very high accuracy.
The other approach which does not sacriﬁce accuracy takes the opposite approach: it ﬁrst builds a highly accurate model without worrying about interpretabilty, and subsequently uses a separate set of re-representation techniques to assist the user in understanding the behavior of the algorithm.
One of the techniques could be to use the aforementioned relatively simple and interpretable algorithms to explain the behavior of a complex model and the reasons why a given classiﬁer, treated as a black box, classiﬁes a given instance in a particular way, e.g. LIME [6], BETA [7], TREPAN [11].
Deep learning methods have been lately very successful in image processing and natural language processing.
It could be categorized as a representation learning approach [12], which learns reﬁned features that could improve a model’s generalization ability.
Deep learning, however, is highly non- interpretable.
In this paper we are reporting a work in progress where we try to interpret the inner mechanisms of deep learning.
Our method: CNN-INTE is inspired by [8].
We design and implement a tool that helps the user understand how the hidden layers in a deep CNN model work to classify examples.
And the results are expressed in graphs which indicates sequential separations of the true class and the hypothesis.
The main contributions of our method is as follows: • Compared to LIME [6] which only provides local in- terpretation in speciﬁc regions of the feature space, our method provides global interpretation for any test instances in the whole feature space.
• Compared to models which apply inherently interpretable algorithms, e.g. [5], our method has the advantage of not compromising the accuracy of the model to be interpreted.
This produces more reliable interpretation.
• In contrary to [6] and [7] which treat the model to be in- terpreted as black box, we interpret the inner mechanisms of deep CNN models.
• The experiments are implemented in the TensorFlow [9] platform, which makes our model scalable to big datasets more easily.
Scalability is an issue pointed out as future work in [6] and [7] but not realized yet.
II.
RELATED WORK To resolve the problems for “trusting a prediction” and “trusting a model”, two methods are proposed in [6] to explain individual predictions and understand a model’s behavior respectively: Local Interpretable Model-agnostic Explanations (LIME) and Submodular Pick LIME (SP-LIME).
The main idea for LIME is to use inherently interpretable models g to interpret complex models f locally.
They designed an objective function to minimize the unfaithfulness (when g is approximating f in a local area) and the complexity of g.
Although it was stated in their paper that in the objective function g could be any interpretable models, they set g as sparse linear models in their paper.
Based on the individual explanations generated by LIME, they design an submodular pick algorithm: SP-LIME to explain the model as a whole by picking a number of representative and non-redundant instances.
It was suggested in [10] that coverage, precision and effort should be used to evaluate the results of the model interpretation.
Although LIME achieves high precision and low effort, the coverage is not clear.
In other words, LIME is able to explain why a speciﬁc prediction is made using the weights of the local model g, but can’t indicate to what local region the explanation is faithful.
To solve this problem, the Anchor Local Interpretable Model-Agnostic Explanations method (aLIME) was introduced in [10].
In aLIME, the if-then rules are used instead of using the weights in a linear model to explain a speciﬁc prediction (as was executed in LIME).
The idea is based on the Decision Sets algorithm from [5].
These if-then rules are easy to comprehend and has good coverage.
It was pointed out that there is a trade off between inter- pretability and accuracy for machine learning algorithms [5].
In terms of inherently interpretable models, rule-based models, e.g. Decision Trees and Decision Lists are often preferred, as they can ﬁnd a balance between these two factors.
Decision lists are usually considered more interpretable than decision trees, as they use the if-then-else statements with a hierar- chy structure.
But this structure reduces to some extent the interpretability, as to interpret an additional rule all previous rules should be reasoned about.
Also new rules down the list are applied to much narrow feature spaces, which makes the multi-class classiﬁcation difﬁcult where the minority classes deserves equally good rules.
This motivates the proposal of the Decision Sets algorithm in [5], which produces the isolated if- then rules, where each rule could be an independent prediction.
To realize this, an objective function takes into account both interpretability (expressed by precision and recall of rules) and accuracy (expressed by size, length, cover and overlap).
They showed that solving the objective function is a NP- hard problem, and ﬁnds near-optimal solutions of it.
However, Decision set’ accuracy only approaches random forest, and its expressive power just catches up with decision tree.
Another model agnostic explanation approach is the Black Box Explanations through Transparent Approximations (BETA), introduced in [7].
Different from LIME which aims for local interpretation, BETA is a framework which attempts to produce global interpretation for any classiﬁer which are treated as black box classiﬁers.
Based on their previous work on Decision Sets, the authors designed a framework with two level decision sets to taking into account ﬁdelity (faithfulness to the black box model), unambiguity (single and deterministic explanations for each instance), interpretability (complexity minimized) and interactivity (user speciﬁed explorations of the feature’s subspace).
In this two level structure, the outer if-then rules are the “neighborhood descriptors” and the inner if-then rules are “decision logic rules” (how the black box model labels an instance under the outer if-then rules).
Similar to [5], an objective function is built and near-optimal solutions are found.
III.
METHODOLOGY Our methodology could be classiﬁed as post-hoc interpre- tation [14], where a trained model is given and the main task is to interpret it.
This method is close to the second approach mentioned in the fourth paragraph of the introduction section, but is also different in many ways.
First, the model to be interpreted here is not treated as a black box as we directly interpret the hidden layers of a deep CNN.
Second, compared to LIME [6] which only has local interpretability, our method achieves global interpretability.
Similar to LIME, we also provide qualitative interpretation with graphs to visualize the results.
As our method interprets deep CNN via Meta-learning, we ﬁrst brieﬂy introduces deep CNN, meta-learning and then discuss our framework in details.
A.
Deep Convolutional Neural Network This section introduces the deep CNN model we are going to interpret.
As we implement our program using TensorFlow, we use its TensorBoard function to draw the structure of the deep CNN we construct in Fig.
1.
Deep CNN is now the most advanced machine learning algorithm for image classiﬁcation.
It takes advantage of the two-dimensional structure of the input images.
It uses a set of ﬁlters to ﬁlter the pixels of the raw input images to generate higher level representations to be learnt by the model in order to improve the performance.
There are three major components of deep CNN: convolu- tional layer, pooling layer and fully connected layer (same as in regular neural networks).
A deep CNN model is usually a stack of these layers.
In the convolutional layer, a ﬁlter is used to compute dot products between the pixels of the input image at speciﬁc position and the values of the ﬁlter, producing one single value in the output feature map.
The convolution operation is completed after the ﬁlter is slided across the width and height of the input image.
Following the convolutional layer, an activation function, often a rectiﬁed linear unit (ReLU) [15], is applied to inject nonlinearities into the model and speed up the training process.
Following ReLU is the pooling layer which is a non-linear down-sampling layer.
A common algorithm for pooling is the max pooling algorithm.
In this algorithm, each sub-region of the previous feature map is turned into a single maximum value in this region.
Max pooling reduces computation and controls overﬁtting.
In order to calculate the predicted class, after performing max pooling, the feature map needs to be ﬂattened and feed into a fully connected layer.
In the last layer: the output layer, a softmax classiﬁer is applied for prediction.
The structure of the deep CNN model we designed is illustrated in Fig.
1, “Placeholder” represents for the interface to input the training data.
“Reshape” is needed ﬁrst to convert the input one-dimensional image data into two dimensional data.
In our experiment, we use the MNIST dataset [16].
The 784 input features are converted into a two-dimensional 28 × 28 image.
Our model has two series of a convolutional layer followed by a pooling layer: “conv1”-“pool1”-“conv2”- “pool2”, which are followed by one fully connected layer “fc1”.
As a fully connected network is susceptible to suffer from overﬁtting, the “dropout” operation [17] applied after “fc1” aims to reduce it.
In this operation, a probability parameter p is set to keep a speciﬁc neuron with probability p (or drop it with probability 1-p).
The “Adam optimizer” [18], rather than a standard Stochastic Gradient Descent optimizer is used to train the model via modifying the variables and reducing the loss.
“fc2” is the output layer with 10 neurons: each represents the class 0-9.
B.
Meta-learning Meta-learning is an ensemble learning method which learns from the results of the base classiﬁers.
It has a two-level structure, where the algorithms used in the ﬁrst level are called base-learners and the algorithm in the second level is called meta-learner.
The base-learners are trained on the original training data.
The meta-learner is trained by the predictions of the base classiﬁers and the true class of the original training data.
When training the meta-learner, the “Class-combiner” strategy [13] is applied here, where the predictions includes just the predicted class (instead of all classes, as in the “Binary-class-combiner”).
Fig.
1.
Structure of a deep CNN model generated by TensorFlow’s Tensor- Board .
To understand the meta-learning algorithm intuitively, Fig.
2 illustrates a simpliﬁed training process for meta-learning [19].
The numbers 1, 2, 3, 4 represents the four steps of training.
In the 1st step, the base learning algorithms 1 to m are trained on the training data.
In the 2nd step, a validation dataset is used to test the trained classiﬁers 1 to m.
In the 3rd step, the predictions generated in step 2 and the true labels of the validation dataset are used to train a meta-learner.
Finally, in the 4th step, a meta-classiﬁer is produced and the whole meta- learning training process is completed.
Once the training process is accomplished, the test process is much easier to execute.
Fig.
3 presents a simpliﬁed test process [19].
In the 1st step, the test data is applied on the base classiﬁers to generate predictions which combined with the true labels of the test data comprises the meta-level test data in 2nd step.
In the 3rd step, the ﬁnal predictions are generated by testing the meta-level classiﬁer with the predictions in the 2nd step and the accuracy could be calculated.
Fig.
2.
Meta-learning training process.
Fig.
4.
CNN-INTE training process.
Fig.
3.
Meta-learning test process.
C.
Framework Our framework is named as CNN-INTE which stands for Convolutional Neural Network Interpretation.
It is similar to meta-learning, but different in a few ways.
In this work, we interpret the ﬁrst fully connected layer “fc1” of the deep CNN model illustrated in Fig.
1.
The training process is shown in Fig.
5.
In the 1st step, the original training data is used to train a CNN model.
In the 2nd step, the parameters generated in the 1st step are used to calculate the values for the activations of the ﬁrst fully connected layer: fc1.
In the 3rd step, a clustering algorithm is used to cluster the data generated in step 2 into a number of groups which we deﬁne as factors henceforth.
In the 4th step, the data belonging to each of the factors are clustered again generating a number of clusters each assigned a unique ID.
In the 5th step, these IDs are grouped together as the training features in the meta-level, using the labels of the original training data as label for the meta-learner.
In the 6th step, the features of the original training data and the IDs (set as labels) in step 4 are used to train a number of random forests [21].
Assume the training data T has N numbers of instances and layer “fc1” has H neurons.
The labels of the training data are Ty = {l1, l2, ..., lN}.
Once the deep CNN model is trained, for each training instance ti, we calculate the activations at each hidden neuron on this layer.
Hence, we obtain a matrix S with size H × N.
To construct the meta-level training data, we use a clustering algorithm to cluster the matrix along the hidden layer axis into several factors F = {f1, f2, ..., fk}.
Then within each of the factors, we cluster the data again, this time along the axis of the instances.
The clustering results are the IDs each instance belongs to.
For instance, if there 10 clusters, after the second level clustering each instance will have an ID between 0-9.
All the IDs combined with the true labels of the training data builds up the meta-level training data.
To present the technical details of the CNN-INTE training process, we provide the pseudo code in Algorithm 1.
Line 1-3 is the initialization of the algorithm.
In line 4, the activations S are clustered into K factors, where K is the number of clusters set in the clustering algorithm C l.
In lines 5-7 the same clustering algorithm C l is applied on all the factors to generate K sets of ID numbers.
Lines 8-9 uses the generated ID numbers and the true labels of the original training data to train the meta-learner: C m.
Till now, the training process is not done yet.
We still need to generate the base models to be used in the test process.
Lines 10-12 uses the features of the original training data and the ID numbers to train K base models.
The output of the training process would be the meta-lever classiﬁer: ˜M and K base models: B = {M1, M2,··· , MK}.
Algorithm 1 CNN-INTE Training Process 1: Input: activations: S; training data: T ; Meta learning algorithm: C m; Clustering algorithm: C l; Base learning algorithms: {C1, C2,··· , CK} 2: E = ∅ 3: Scv = ∅ 4: {f1, f2,··· , fK} = C l(S) 5: for k = 1··· K do IDsk = C l(fk) 6: 7: end for 8: Scv = {IDs1, IDs2,··· , IDsK, Ty} 9: ˜M = C m(Scv) 10: for k = 1··· K do 11: 12: end for 13: E = ({M1, M2,··· , MK} , ˜M ) 14: Output: Ensemble E Mk = Ck(Tx, IDsk) Fig.
5 is a toy example that illustrates the above process.
In this example, there are 5 hidden neurons and 6 training instances.
We set the number of clusters for both the ﬁrst and second level clustering as 3.
Hence, the matrix S with size 6 × 5 is ﬁrst clustered into 3 factors {f1, f2, f3} horizontally.
For each factor, the activations are again clustered into three clusters vertically, e.g. F1 is clustered into {C11, C12, C13}.
If we set the ID numbers for these cluster as {0, 1, 2}, then the corresponding ID numbers for t1 to t6 in factor f1 according to Fig.
5 are {0, 0, 1, 1, 2, 2}.
Hence, the meta-level training features are expressed as0  0 1 1 0 0 1 1 1 1 This data combined with the corresponding training labels of the original training data is used to train the meta-learner.
Here the meta-learner we used is the Decision Tree [3], an inherently interpretable algorithm.
Its tree structure provides an excellent visual explanation of the predictions.
Fig.
5.
Toy example for the generation of Meta-level training data.
The test process of the meta-model is exactly the same as the meta-learning test process, which is shown in Fig.
6.
In the test process, we use the original test data to test the base classiﬁers generated in the meta-level training process to obtain the meta-level test data’s features.
The base-learner we applied is random forest.
The number of base models is equal to the number of factors.
Hence, we have K base models: B = {M1, M2, ..., MK}.
In the toy example, there are three factors which lead to three base models.
The training data for the ﬁrst base model corresponding to F 1 would be (t1 − l1) (t2 − l2) (t3 − l3) (t4 − l4) (t5 − l5) (t6 − l6) Here ti−li represents for the features of each original training instance.
Once we obtain the K base models, we can use the original test data to test them to produce the meta-level test data.
These data are then feed into the trained decision tree model to interpret individual test predictions.
Fig.
6.
CNN-INTE test process.
IV.
EXPERIMENTS The dataset we use is the MNIST database of handwritten digits from 0 to 9 [16].
We extracted 55,000 examples (the original dataset has 60,000 examples for training) as the training data and 10,000 examples as the test data.
Each of the examples represents for the 28 × 28 images with pixels ﬂattened as 784 features.
The experiments are performed on the TensorFlow platform.
A.
Experimental Setup First of all, we need to train a nice deep CNN model.
We ﬁrst reshape the input training data into 55000 images each with size 28 × 28.
Training all the data on every epoch is expensive, which requires lots of resources of the computer and may lead to the termination of the program.
Here we apply stochastic training: on the ﬁrst epoch, we select a mini- batch of the training data and perform optimization on this batch; once we loop through all the batches, we randomize the training data and start a new epoch.
In our experiment, we set the epoch e = 1000, batch size b = 50.
Stochastic training is cheap and achieves similar performance to using the whole training data in every epoch.
For each mini-batch, in the ﬁrst convolutional layer, we apply 32 ﬁlters (or kernels) each with size 5×5, which generates 32 feature maps.
In the ﬁrst pooling layer we apply ﬁlters with size 2 × 2.
The stride size is set as 2.
The second convolutional layer use 64 ﬁlters with the same size as the ﬁrst convolutional layer.
The second pooling layer has the same parameters as the previous one.
Immediately after this pooling layer is the ﬁrst fully connected layer.
We set the number of neurons for this layer as 128.
To reduce overﬁtting we also set the dropout [17] parameter d = 0.5, which means a neuron’s output has 50% probability to be dropped.
The last layer is the second fully connected layer (or the “readout layer”), which has 10 neurons with each neuron outputs the probability of the corresponding digits 0-9.
The test accuracy of this trained CNN model on the test data is 93.9%.
Now comes the key part for setting up interpretation.
we deﬁne interpretability of a model as the ability to provide visual or textual presentation of the connections between input features and the output predictions.
We ﬁrst feed the trained fully connected layer f c1 with the original training data, which would produce a data S with size of 128 × 55000.
We then cluster S into several factors.
The clustering algorithm we applied is the k-means algorithm [20].
The number of factors is equal to the number of clusters which we set as 8 in this level.
Hence, S is now turned into a list F = {f1, f2, ..., f8} with size 8 × 55000 having each row representing the data belonging to each factor.
In the second level clustering, for each factor in F we use the k-means algorithm again to cluster them into a number of clusters.
We set the number as 10 in our experiment as the number of classes for the original training data is 10.
Hence each cluster will be assigned a unique ID number between 0 and 9.
Then we use the IDs belonging to each training instances and the true labels of the original training data to train a decision tree algorithm.
Due to the limitation of the space, we are unable to show the structure of the trained decision tree here.
We set the maximum depth of the decision tree as 5.
Although deeper decision tree would generate better accuracy, it makes it harder to interpret with too many tree levels.
To obtain the test data for decision tree, we ﬁrst use the original training data’s feature as features and the IDs for each factor in F as labels to train the corresponding random forest algorithm [21], generating 8 base models.
For random forest, we set the number of trees as 20 and the maximum nodes as 2000.
Finally we use the original test data to test the 8 trained base models.
The generated predictions become the features of meta-level test data with sizes of 10000 × 8.
Using the meta-level test data on the trained decision tree produces an accuracy of 92.8% with tree depth=5.
This value is comparable to the test accuracy on the trained deep CNN model: 93.9%.
It should be noted that the decision tree’s accuracy could be further improved by increasing the depth of tree and tuning other related parameters.
B.
Experimental Results To interpret the deep CNN model’s behavior on the test data, we intend to use diagrams generated by our tool: CNN-INTE to examine individual predictions on the test data.
Hence, we provide qualitative interpretations visually.
We arbitrarily selected two test instances that were correctly classiﬁed by the decision tree and one test instance that was wrongly classiﬁed.
It should be noted that this tool could be used on any test instances globally and not just limited to the three cases we provide.
The details of the selected test instances are shown in Table I.
Here “f0-f7” represents the features of the meta- level test data, “label” is the test label in the original test data, “pred” is the prediction generated by the decision tree on the meta-level test data.
“True1” and “True2” represents for the two correctly classiﬁed instances and “Wrong1” is the wrongly classiﬁed instance.
INSTANCES SELECTED FROM THE META-LEVEL TEST DATA TABLE I Features and labels True1 True2 Wrong1 f0 f1 f2 f3 f4 f5 f6 f7 label pred In order to examine the classiﬁcation process visually, we check each feature values according to the trained structure of the decision tree and plot the graphs of the activations corresponding to the true label and the hypothesis.
The in- terpretation result for instance “True1” is shown in Fig.
7.
As the true label for this instance is 3, all other classes could be regarded as hypothesis and this is why there are no graphs for “Hypothesis: 3” in Fig.
7.
Each row represents the examination of the feature values corresponding to different factors in different levels of the trained decision tree, e.g. the ﬁrst row represents the root level of the decision tree.
Since we set the depth of the decision tree as 5, there are 5 rows in all.
Each column stands for the query of if the test instance belongs to the corresponding hypothesis over the nodes visited.
Take the column of “Hypothesis:0” as an example, the goal is to ﬁnd if the label of the test instance is 0.
In the 1st row we extract the activations corresponding to “f6” which satisﬁes the condition that f6 (cid:54) 4.5 (this is determined by the trained decision tree) and draw a graph between activations that belongs to label=0 (hypothesis) and label=3 (true).
Then we check the graph to evaluate if the data corresponding to the true class could be separated from the hypothesis.
The answer is no because the hypothesis represented as blue points overlaps with the true class shown as red points.
Hence, we need to query the trained decision tree further.
The values of the factors we need to check is: f6 (cid:54) 2.5 for 2nd row; f6 (cid:54) 0.5 for 3rd row; f7 (cid:54) 0.5 for 4th row; f1 (cid:54) 0.5 for 5th row.
In this process, we noticed that in the 4th row the true class and the hypothesis class are successfully separated as only the red points corresponding to the true label are left.
Therefore, we don’t need to examine further and that’s why the graph for the 5th row is not displayed.
We highlight the graph with green rectangles if the ﬁnal results are separable and red vice versa.
The same idea is applied on other hypothesis.
We also draw the graphs for instances “True2” and “Wrong1” in Fig.
8 and Fig.
9 respectively.
V.
CONCLUSION AND FUTURE WORK In this work, we present an interpretation tool CNN-INTE, which interprets a hidden layer of a deep CNN model: to ﬁnd out how the learned hidden layer classiﬁes new test instances.
Although we just show the results for the ﬁrst fully connected layer before the read-out layer, the approach could be applied on any hidden layers.
The interpretation is realized by ﬁnding the relationships between the original training data and the trained hidden layer “fc1” via meta- learning.
We used two-level k-means clustering algorithm to ﬁnd the meta-level training data and random forests as base models for generating meta-level test data.
The visual results generated by our program clearly indicate why a test instance is truly or wrongly classiﬁed by checking if there are any overlaps of the corresponding activations.
For future work, we plan to initiate quantiﬁcation of the interpreted results.
In our experiments, one of the things we ﬁnd tricky is the setting of the number of clusters for the k-means algorithm.
In the future, we plan to replace the k-means algorithm with DBSCAN [22] Fig.
7.
Example of a correctly classiﬁed test instance: True1.
which doesn’t need specifying the number of clusters.
As stated in [5], “decision sets” seems to be a better option than decision tree as a inherently interpretable algorithm, so we also plan to replace decision tree with decision sets.
Last but not least, it would be quite meaningful to apply this tool on real world applications where interpretations are demanded either between the training data and the hidden layer or between the hidden layer and the predictions.
ACKNOWLEDGMENT The authors acknowledge the support of the Province of Nova Scotia, of Dalhousie University, and of the the Natural Sciences and Engineering Research Council of Canada under the CREATE program grant.
REFERENCES [1] J.
L.
Herlocker, J.
A.
Konstan, and J.
Riedl, “Explaining collaborative ﬁltering recommendations,” In Proceedings of the 2000 ACM conference on Computer supported cooperative work, pp.
241–250,December 2000.
[2] M.
T.
Dzindolet, S.
A.
Peterson, R.
A.
Pomranky, L.
G.
Pierce, and H.
P.
Beck.
“The role of trust in automation reliance,” Int.
J.
Hum.-Comput.
Stud., vol.58, no.6, pp.697–718, 2003.
[3] J.
Ross Quinlan, C4.
5: programs for machine learning, Elsevier, 2014.
[4] R.
L.
Rivest, “Learning decision lists,” Machine learning, vol.2, no.3, pp.229–246, 1987.
[5] H.
Lakkaraju, S.
H.
Bach, and J.
Leskovec, “interpretable decision sets: A joint framework for description and prediction,” In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp.
1675–1684, ACM, August, 2016.
[6] M.
T.Ribeiro, S.Singh and C.Guestrin, “Why should i trust you?
: Explaining the predictions of any classiﬁer,” In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp.
1135–1144, ACM, August, 2016.
[7] H.
Lakkaraju, E.Kamar, R.Caruana and J.Leskovec, “Interpretable & Explorable Approximations of Black Box Models,” KDD’17 workshop, 2017.
[8] J.
J.Thiagarajan, B.Kailkhura, P.
Sattigeri , and K.
N.Ramamurthy, “TreeView: Peeking into Deep Neural Networks Via Feature-Space Par- titioning,” 30th Conference on Neural Information Processing Systems (NIPS), 2016.
[9] M.Abadi,et al.
“TensorFlow: A System for Large-Scale Machine Learn- ing,” In OSDI, Vol.
16, pp.
265–283, 2016.
[10] M.
T.Ribeiro, S.Singh and C.Guestrin, “Nothing Else Matters: Model- Agnostic Explanations By Identifying Prediction Invariance,” 30th Con- ference on Neural Information Processing Systems (NIPS), 2016.
[11] M.
Craven and J.
W.
Shavlik, “Extracting tree-structured representations of trained networks,” In Advances in neural information processing systems, pp.
24–30, 1996.
[12] I.
Goodfellow, Y.
Bengio and A.
Courville, Deep learning, MIT press, 2016.
[13] P.
K.
Chan and S.
J.
Stolfo, “Experiments on multistrategy learning by meta-learning,” In Proceedings of the second international conference on information and knowledge management, pp.
314–323, ACM, De- cember, 1993.
[14] G.
Montavon, W.
Samek, and K.R. M¨uller, “Methods for interpreting and understanding deep neural networks,” Digital Signal Processing, 2017.
[15] V.
Nair and G.
E.
Hinton, “Rectiﬁed linear units improve restricted boltz- mann machines,” In Proceedings of the 27th international conference on machine learning (ICML-10), pp.
807–814, 2010.
[16] Y.
LeCun, L.
Bottou, Y.
Bengio, and P.
Haffner, “Gradient-based learning applied to document recognition,” Proceedings of the IEEE, 86(11), pp.
2278–2324, November, 1998.
[17] N.
Srivastava, G.
Hinton, A.
Krizhevsky, I.
Sutskever, and R.
Salakhut- dinov, “Dropout: A simple way to prevent neural networks from overﬁt- ting,” The Journal of Machine Learning Research, 15(1), pp.
1929–1958, 2014.
[18] D.
P.
Kingma and J.
Ba, “Adam: A method for stochastic optimization,” arXiv preprint arXiv:1412.6980, 2014.
[19] X.
Liu, X.
Wang, S.
Matwin, and N.
Japkowicz, “Meta-learning for large scale machine learning with MapReduce,” In Big Data, 2013 IEEE International Conference on, pp.
105–110, IEEE, October, 2013.
Fig.
8.
Example of a correctly classiﬁed test instance: True2.
[20] J.
A.
Hartigan and M.
A.
Wong, “Algorithm AS 136: A k-means clustering algorithm,” Journal of the Royal Statistical Society, Series C (Applied Statistics), 28(1), pp.
100-108, 1979.
[21] A.
Liaw and M.
Wiener, “Classiﬁcation and regression by randomFor- est,” R news, 2(3), pp.
18–22, 2002.
[22] M.
Ester, H.
P.
Kriegel, J.
Sander, and X.
Xu, “A density-based algorithm for discovering clusters in large spatial databases with noise,” In Kdd, Vol.
96, No. 34, pp.
226-231, August, 1996.
Fig.
9.
Example of a wrongly classiﬁed test instance: Wrong1.

Not all computations in a deep neural network are of equal importance.
In the conventional deep learning pipeline, an expert crafts a neural architecture and trains it against a prepared dataset.
The success of training a deep model often requires trial and error, and such loop usually has little control on prioritizing the computations happening in the neural network.
Recently researchers started to develop model-simpliﬁcation methods for convolutional neural networks (CNNs), bearing in mind that some computations are indeed non-critical or redundant and hence can be safely removed from a trained model without substantially degrading the model’s performance.
Such methods not only accelerate computational efﬁciency but also sometimes alleviate the model’s overﬁtting effects.
Discovering which subsets of the computations of a trained CNN are more reasonable to prune, however, is nontrivial.
Existing methods can be categorized from either the learning perspective or the computational perspective.
From the learning perspective, some methods use a data-independent approach where the training data does not assist in determining which part of a trained CNN should be pruned, e.g. He et al.
(2017) and Zhang et al.
(2016), while others use a data-dependent approach through typically a joint optimization in generating pruning decisions, e.g., Han et al.
(2015) and An- war et al.
(2017).
From the computational perspective, while most approaches focused on setting ∗This work was done when Jianbo Ye interned at Adobe in 2017 summer.
Published as a conference paper at ICLR 2018 the dense weights of convolutions or linear maps to be structured sparse, we propose here a method adopting a new conception to achieve in effect the same goal.
Instead of regarding the computations of a CNN as a collection of separate computations sitting at different layers, we view it as a network ﬂow that delivers information from the input to the output through different channels across different layers.
We believe saving computations of a CNN is not only about reducing what are calculated in an individual layer, but perhaps more importantly also about understanding how each channel is contributing to the entire information ﬂow in the underlying passing graph as well as removing channels that are less responsible to such process.
Inspired by this new conception, we propose to design a “gate” at each channel of a CNN, controlling whether its received information is actually sent out to other channels after processing.
If a channel “gate” closes, its output will always be a constant.
In fact, each designed “gate” will have a prior intention to close, unless it has a “strong” duty in sending some of its received information from the input to follow-up layers.
We ﬁnd that implementing this idea in pruning CNNs is unsophisticated, as will be detailed in Sec 4.
Our method neither introduces any extra parameters to the existing CNN, nor changes its computa- tion graph.
In fact, it only introduces marginal overheads to existing gradient training of CNN.
It also possess an attractive feature that one can successively build multiple compact models with different inference performances in a single round of resource-intensive training (as were done in our exper- iments).
This eases the process to choose a balanced model to deploy in production.
Probably, the only applicability constraint of our method is that all convolutional layers and fully-connected layer (except the last layer) in the CNN should be batch normalized (Ioffe & Szegedy, 2015).
Given batch normalization has becomes a widely adopted ingredient in designing state-of-the-art deep learning models, and many successful CNN models are using it, we believe our approach has a wide scope of potential impacts.1 In this paper, we start from rethinking a basic assumption widely explored in existing channel prun- ing work.
We point out several issues and gaps in realizing this assumption successfully.
Then, we propose our alternative approach, which work around several numerical difﬁculties.
Finally, we experiment our method across different benchmarks and validate its usefulness and strengths.
2 RELATED WORK Reducing the size of neural network for speeding up its computational performance at inference time has been a long studied topic in the community of neural network and deep learning.
Pioneer works include Optimal Brain Damage (LeCun et al., 1990) and Optimal Brain Surgeon (Hassibi & Stork, 1993).
More recent developments focused on either reducing the structural complexity of a provided network or training a compact or simpliﬁed network from scratch.
Our work can be categorized into the former type.
Thus, the literature review below revolves around that reducing the structural complexity.
To reduce the structural complexity of deep learning models, previous work have largely focused on sparsifying the weights of convolutional kernels or the feature maps across multiple layers in a network (Anwar et al., 2017; Han et al., 2015).
More recently, some work proposes to impose structured sparsity on those vector components motivated from the implementation perspective on specialized hardware (Wen et al., 2016; Zhou et al., 2016; Alvarez & Salzmann, 2016; Lebedev & Lempitsky, 2016).
Yet as argued by authors of (Molchanov et al., 2017), regularization-based pruning techniques require per layer sensitivity analysis which adds extra computations.
Molchanov et al.
(2017) relies on global rescaling of criteria for all layers and does not require sensitivity esti- mation, a beneﬁcial feature that our approach also has.
To our knowledge, it is also much unclear how widely useful are those work in deep learning.
In Section 3, we discuss in details the potential issues in regularization-based pruning techniques potentially hurting them being widely applicable, especially for those which regularize high-dimensional tensor parameters or use magnitude-based pruning methods.
Our approach works around the mentioned issues by constraining the anticipated pruning operations only on batch normalized convolutional layers.
Instead of posing structured 1For convolution layer which is not originally trained with batch normalization, one can still convert it into a “near equivalent” convolution layer with batch normalization by removing the bias term b and properly setting σ + , β = b + µ, where σ and µ are estimated from the outputs of the convolution across all training γ = samples.
Published as a conference paper at ICLR 2018 sparsity on kernels or feature maps, we enforce sparsity on the scaling parameter γ in batch normal- ization operator.
This blocks the sample-wise information passing through part of the channels in convolution layer, and in effect implies one can safely remove those channels.
A recent work (Huang & Wang, 2017) used similar technique as ours to remove unimportant residual modules in ResNet by introducing extra scaling factors to the original network, but some optimiza- tion subtleties as pointed out in our paper were not well explained.
Another recent work called Network-Slimming (Liu et al., 2017) also aims to sparsify the scaling parameters of batch normal- ization.
But instead of using off-the-shelf gradient learning like theirs, we propose a new algorithmic approach based on ISTA and rescaling trick, improving robustness and speed of the undergoing opti- mization.
In particular, The work of Liu et al.
(2017) was able to prune VGG-A model on ImageNet.
It is unclear how their work would deal with the γ-W rescaling effect and whether their approach can be adopted to large pre-trained models, such as ResNets and Inceptions.
We experimented with the pre-trained ResNet-101 and compared to most recent work that were shown to work well with large CNNs. We also experimented with an image segmentation model which has an inception-like module (pre-trained on ImageNet) to locate foreground objects.
3 RETHINKING THE SMALLER-NORM-LESS-INFORMATIVE ASSUMPTION In most regularized linear regressions, a large-norm coefﬁcient is often a strong indicator of a highly informative feature.
This has been widely perceived in statistics and machine learning community, and removing features which have a small coefﬁcient does not substantially affect the regression er- rors.
Therefore, it has been an established practice to use tractable norm to regularize the parameters in optimizing a model and pick the important ones by comparing their norms after training.
How- ever, this assumption is not unconditional.
By using Lasso or ridge regression to select important predictors in linear models, one always has to ﬁrst normalize each predictor variable.
Otherwise, the result might not be explanatory.
For example, ridge regression penalizes more the predictors which has low variance, and Lasso regression enforces sparsity of coefﬁcients which are already small in OLS.
Such normalization condition for the right use of regularization is often unsatisﬁed for nonconvex learning.
For example, one has to carefully consider two issues outlined below.
We provides these two cases to exemplify how regularization could fail or be of limited usage.
There deﬁnitely exist ways to avoid the speciﬁc failures.
Model Reparameterization.
In the ﬁrst case, we show that it is not easy to have ﬁne-grained control of the weights’ norms across different layers.
One has to either choose a uniform penalty in all layers or struggle with the reparameterization patterns.
Consider to ﬁnd a deep linear (convolutional) network subject to a least square with Lasso: for λ > 0, n(cid:88) i=1 E(x,y)∼D(cid:107)W2n ∗ .
.
.
∗ W2 ∗ W1 ∗ x − y(cid:107)2 + λ min {Wi}2n i=1 (cid:107)W2i(cid:107)1 .
The above formulation is not a well-deﬁned problem because for any parameter set {Wi}2n can always ﬁnd another parameter set {W (cid:48) keeping the corresponding l0 norm unchanged by actually setting i=1, one i}2n i=1 such that it achieves a smaller total loss while i = αWi, i = 1, 3, .
.
.
, 2n − 1 and W (cid:48) W (cid:48) i = Wi/α, i = 2, 4, .
.
.
, 2n , where α > 1.
In another word, for any  > 0, one can always ﬁnd a parameter set {Wi}2n i=1 (which is usually non-sparse) that minimizes the ﬁrst least square loss while having its second Lasso term less than .
We note that gradient-based learning is highly inefﬁcient in exploring such model reparameteriza- tion patterns.
In fact, there are some recent discussions around this (Dinh et al., 2017).
If one adopts a pre-trained model, and augments its original objective with a new norm-based parameter regu- larization, the new gradient updates may just increase rapidly or it may take a very long time for the variables traveling along the model’s reparameterization trajectory.
This highlights a theoretical gap questioning existing sparsity inducing formulation and actual computational algorithms whether they can achieve widely satisfactory parameter sparsiﬁcation for deep learning models.
Published as a conference paper at ICLR 2018 Transform Invariance.
In the second case, we show that batch normalization is not compatible with weight regularization.
The example is penalizing l1- or l2-norms of ﬁlters in convolution layer which is then followed by a batch normalization: at the l-th layer, we let xl+1 = max{γ · BNµ,σ,(W l ∗ xl) + β, 0}, where γ and β are vectors whose length is the number of channels.
Likewise, one can easily see that any uniform scaling of W l which changes its l1- and l2-norms would have no ef- fects on the output xl+1.
Alternatively speaking, if one is interested in minimizing the weight norms of multiple layers together, it becomes unclear how to choose proper penalty for each layer.
Theoretically, there always exists an optimizer that can change the weight to one with inﬁnitesimal magnitude without hurting any inference performance.
As pointed by one of the reviewers, one can tentatively avoid this issue by projecting the weights to the surface of unit ball.
Then one has to deal with a non-convex feasible set of parameters, causing extra difﬁculties in developing optimization for data-dependent pruning method.
It is also worth noting that some existing work used such strategy in a layer-by-layer greedy way (He et al., 2017; Zhang et al., 2016).
Based on this discussion, many existing works which claim to use Lasso, group Lasso (e.g. Wen et al.
(2016); Anwar et al.
(2017)), or thresholding (e.g. Molchanov et al.
(2017)) to enforce param- eter sparsity have some theoretical gaps to bridge.
In fact, many heuristic algorithms in neural net pruning actually do not naturally generate a sparse parameterized solution.
More often, thresholding is used to directly set certain subset of the parameters in the network to zeros, which can be problem- atic.
The reason is in essence around two questions.
First, by setting parameters less than a threshold to zeros, will the functionality of neural net be preserved approximately with some guarantees?
If yes, then under what conditions?
Second, how should one set those thresholds for weights across different layers?
Not every layer contributes equally in a neural net.
It is expected that some layers act critically for the performance but only use a small computation and memory budget, while some other layers help marginally for the performance but consume a lot resources.
It is naturally more desirable to prune calculations in the latter kind of layers than the former.
In contrast with these existing approaches, we focus on enforcing sparsity of a tiny set of parameters in CNN — scale parameter γs in all batch normalization.
Not only placing sparse constraints on γ is simpler and easier to monitor, but more importantly, we have two strong reasons: 1.
Every γ always multiplies a normalized random variable, thus the channel importance be- comes comparable across different layers by measuring the magnitude values of γ; 2.
The reparameterization effect across different layers is avoided if its follow-up convolution layer is also batch normalized.
In other words, the impacts from the scale changes of γ parameter are independent across different layers.
Nevertheless, our current work still falls short of a strong theoretical guarantee.
But we believe by working with normalized feature inputs and their regularized coefﬁcients together, one is closer to a more robust and meaningful approach.
Sparsity is not the goal, the goal is to ﬁnd less important channels using sparsity inducing formulation.
4 CHANNEL PRUNING OF BATCH-NORMALIZED CNN We describe the basic principle and algorithm of our channel pruning technique.
4.1 PRELIMINARIES Pruning constant channels.
Consider convolution with batch normalization: xl+1 = max(cid:8)γl · BNµl,σl,l (W l ∗ xl) + βl, 0(cid:9) .
For the ease of notation, we let γ = γl.
Note that if some element in γ is set to zero, say, γ[k] = 0, its output image xl+1 :,:,:,k becomes a constant βk, and a convolution of a constant image channel is almost everywhere constant (except for padding regions, an issue to be discussed later).
Therefore, we show those constant image channels can be pruned while the same functionality of network is approximately kept: Published as a conference paper at ICLR 2018 • If the follow-up convolution layer does not have batch normalization, xl+2 = max(cid:8)W l+1 ∗ xl+1 + bl+1, 0(cid:9) , its values (a.k.a. elements in β) is absorbed into the bias term by the following equation such that new := bl+1 + I(γ = 0) · ReLU(β)T sum reduced(W l+1 bl+1 xl+2 ≈ max(cid:8)W l+1 ∗γ xl+1 + bl+1 new, 0(cid:9) , :,:,·,·) , where ∗γ denotes the convolution operator which is only calculated along channels indexed by non-zeros of γ.
Remark that W ∗ = sum reduced(W:,:,·,·) if W ∗ i,j Wi,j,a,b.
a,b =(cid:80) (cid:0)W l+1 ∗ xl+1(cid:1) + βl+1, 0(cid:9) , • If the follow-up convolution layer has batch normalization, xl+2 = max(cid:8)γl+1 · BNµl+1,σl+1,l+1 instead its moving average is updated as new := µl+1 − I(γ = 0) · ReLU(β)T sum reduced(W l+1 µl+1 :,:,·,·) , (cid:110) γl+1 · BNµl+1 new,σl+1,l+1 (cid:0)W l+1 ∗γ xl+1(cid:1) + βl+1, 0 (cid:111) such that xl+2 ≈ max Remark that the approximation (≈) is strictly equivalence (=) if no padding is used in the convolu- tion operator ∗, a feature that the parallel work Liu et al.
(2017) does not possess.
When the original model uses padding in computing convolution layers, the network function is not strictly preserved after pruning.
In our practice, we ﬁne-tune the pruned network to ﬁx such performance degradation at last.
In short, we formulate the network pruning problem as simple as to set more elements in γ to zero.
It is also much easier to deploy the pruned model, because no extra parameters or layers are introduced into the original model.
To better understand how it works in an entire CNN, imagine a channel-to-channel computation graph formed by the connections between layers.
In this graph, each channel is a node, their infer- ence dependencies are represented by directed edges.
The γ parameter serves as a “dam” at each node, deciding whether let the received information “ﬂood” through to other nodes following the graph.
An end-to-end training of channel pruning is essentially like a ﬂood control system.
There suppose to be rich information of the input distribution, and in two ways, much of the original input information is lost along the way of CNN inference, and the useful part — that is supposed to be preserved by the network inference — should be label sensitive.
Conventional CNN has one way to reduce information: transforming feature maps (non-invertible) via forward propagation.
Our approach introduces the other way: block information at each channel by forcing its output being constant using ISTA.
ISTA.
Despite the gap between Lasso and sparsity in the non-convex settings, we found that ISTA (Beck & Teboulle, 2009) is still a useful sparse promoting method.
But we just need to use it more carefully.
Speciﬁcally, we adopt ISTA in the updates of γs.
The basic idea is to project the parameter at every step of gradient descent to a potentially more sparse one subject to a proxy problem: let l denote the training loss of interest, at the (t + 1)-th step, we set γt+1 = min (1) where ∇γlt is the derivative with respect to γ computed at step t, µt is the learning rate, λ is the penalty.
In the stochastic learning, ∇γlt is estimated from a mini-batch at each step.
Eq. (1) has closed form solution as (cid:107)γ − γt + µt∇γlt(cid:107)2 + λ(cid:107)γ(cid:107)1 , µt γt+1 = proxµtλ(γt − µt∇γlt) , where proxη(x) = max{|x|− η, 0}· sgn(x).
The ISTA method essentially serves as a “ﬂood control system” in our end-to-end learning, where the functionality of each γ is like that of a dam.
When γ is zero, the information ﬂood is totally blocked, while γ (cid:54)= 0, the same amount of information is passed through in form of geometric quantities whose magnitudes are proportional to γ.
Published as a conference paper at ICLR 2018 Scaling effect.
One can also see that if γ is scaled by α meanwhile W l+1 is scaled by 1/α, that is, γ := αγ, W l+1 := W l+1 the output xl+2 is unchanged for the same input xl.
Despite not changing the output, scaling of γ and W l+1 also scales the gradients ∇γl and ∇W l+1l by 1/α and α, respectively.
As we observed, the parameter dynamics of gradient learning with ISTA depends on the scaling factor α if one decides to choose it other than 1.0. Intuitively, if α is large, the optimization of W l+1 is progressed much slower than that of γ.
4.2 THE ALGORITHM We describe our algorithm below.
The following method applies to both training from scratch or re-training from a pre-trained model.
Given a training loss l, a convolutional neural net N , and hyper-parameters ρ, α, µ0, our method proceeds as follows: 1.
Computation of sparse penalty for each layer.
Compute the memory cost per channel for each layer denoted by λl and set the ISTA penalty for layer l to ρλl.
Here kl λl = w · I i I i (cid:88) l(cid:48)∈T (l) w · kl h · cl−1 + w · kl(cid:48) kl(cid:48) h · cl(cid:48) + I l w · I l (2)  , w · I i w · kl size of follow-up convolution at layer l(cid:48).
h is the size of input image of the neural network.
h is the kernel size of the convolution at layer l.
Likewise, kl(cid:48) where • I i w · kl(cid:48) • kl • T (l) represents the set of the follow-up convolutional layers of layer l • cl−1 denotes the channel size of the previous layer, which the l-th convolution operates • I l denotes the channel size of one follow-up layer l(cid:48).
h is the image size of the feature map at layer l.
over; and cl(cid:48) w · I l h is the kernel 2.
γ-W rescaling trick.
For layers whose channels are going to get reduced, scale all γls in batch normalizations by α meanwhile scale weights in their follow-up convolutions by 1/α.
3.
End-to-End training with ISTA on γ.
Train N by the regular SGD, with the exception that γls are updated by ISTA, where the initial learning rate is µ0.
Train N until the loss l plateaus, the total sparsity of γls converges, and Lasso ρ(cid:80) γl are zero and output the pruned model (cid:101)N by absorbing all constant channels into follow- 5.
γ-W rescaling trick.
For γls and weights in (cid:101)N which were scaled in Step 2 before training, 6.
Fine-tune (cid:101)N using regular stochastic gradient learning.
4.
Post-process to remove constant channels.
Prune channels in layer l whose elements in scale them by 1/α and α respectively (scaling back).
up layers (as described in the earlier section.).
l λl(cid:107)γl(cid:107)1 converges.
Remark that choosing a proper α as used in Steps 2 and 5 is necessary for using a large µt · ρ in ISTA, which makes the sparsiﬁcation progress of γls faster.
4.3 GUIDELINES FOR TUNING HYPER-PARAMETERS We summarize the sensitivity of hyper-parameters and their impacts for optimization below: • µ (learning rate): larger µ leads to fewer iterations for convergence and faster progress of • ρ (sparse penalty): larger ρ leads to more sparse model at convergence.
If trained with a sparsity.
But if if µ too large, the SGD approach wouldn’t converge.
very large ρ, all channels will be eventually pruned.
Published as a conference paper at ICLR 2018 • α (rescaling): we use α other than 1.
only for pretrained models, we typically choose α from {0.001, 0.01, 0.1, 1} and smaller α warms up the progress of sparsity.
We recommend the following parameter tuning strategy.
First, check the cross-entropy loss and the regularization loss, select ρ such that these two quantities are comparable at the beginning.
Second, choose a reasonable learning rate.
Third, if the model is pretrained, check the average magnitude of γs in the network, choose α such that the magnitude of rescaled γl is around 100µλlρ.
We found as long as one choose those parameters in the right range of magnitudes, the optimization progress is enough robust.
Again one can monitor the mentioned three quantities during the training and terminate the iterations when all three quantities plateaus.
There are several patterns we found during experiments that may suggest the parameter tuning has not been successful.
If during the ﬁrst few epochs the Lasso-based regularization loss keeps decreas- ing linearly while the sparsity of γs stays near zero, one may decrease α and restart.
If during the ﬁrst few epochs the sparsity of γs quickly raise up to 100%, one may decrease ρ and restart.
If during the ﬁrst few epochs the cross-entropy loss keeps at or increases dramatically to a non-informative level, one may decrease µ or ρ and restart.
5 EXPERIMENTS 5.1 CIFAR-10 EXPERIMENT We experiment with the standard image classiﬁcation benchmark CIFAR-10 with two different net- work architectures: ConvNet and ResNet-20 (He et al., 2016).
We resize images to 32 × 32 and zero-pad them to 40 × 40.
We pre-process the padded images by randomly cropping with size 32 × 32, randomly ﬂipping, randomly adjusting brightness and contrast, and standardizing them such that their pixel values have zero mean and one variance.
ConvNet For reducing the channels in ConvNet, we are interested in studying whether one can easily convert a over-parameterized network into a compact one.
We start with a standard 4-layer convolutional neural network whose network attributes are speciﬁed in Table 1.
We use a ﬁxed learning rate µt = 0.01, scaling parameter α = 1.0, and set batch size to 125.
Model A is trained from scratch using the base model with an initial warm-up ρ = 0.0002 for 30k steps, and then is trained by raising up ρ to 0.001.
After the termination criterion are met, we prune the channels of the base model to generate a smaller network called model A.
We evaluate the classiﬁcation performance of model A with the running exponential average of its parameters.
It is found that the test accuracy of model A is even better than the base model.
Next, we start from the pre-trained model A to create model B by raising ρ up to 0.002.
We end up with a smaller network called model B, which is about 1% worse than model A, but saves about one third parameters.
Likewise, we start from the pre-trained model B to create model C.
The detailed statistics and its pruned channel size are reported in Table 1.
We also train a reference ConvNet from scratch whose channel sizes are 32-64-64-128 with totally 224,008 parameters and test accuracy being 86.3%.
The referenced model is not as good as Model B, which has smaller number of parameters and higher accuracy.
We have two major observations from the experiment: (1) When the base network is over- parameterized, our approach not only signiﬁcantly reduce the number of channels of the base model but also improves its generalization performance on the test set.
(2) Performance degradation seems unavoidable when the channels in a network are saturated, our approach gives satisfactory trade-off between test accuracy and model efﬁciency.
ResNet-20 We also want to verify our second observation with the state-of-art models.
We choose the popular ResNet-20 as our base model for the CIFAR-10 benchmark, whose test accuracy is 92%.
We focus on pruning the channels in the residual modules in ResNet-20, which has 9 convolutions in total.
As detailed in Table 2, model A is trained from scratch using ResNet-20’s network structure as its base model.
We use a warm-up ρ = 0.001 for 30k steps and then train with ρ = 0.005.
We are able to remove 37% parameters from ResNet-20 with only about 1 percent accuracy loss.
Likewise, Model B is created from model A with a higher penalty ρ = 0.01.
Published as a conference paper at ICLR 2018 layer conv1 pool1 conv2 pool2 conv3 pool4 fc param.
size test accuracy (%) output 32 × 32 16 × 16 16 × 16 8 × 8 8 × 8 4 × 4 1 × 1 kernel 5 × 5 3 × 3 5 × 5 3 × 3 3 × 3 3 × 3 4 × 4 base channel model A model B model C channel channel channel 96 192 192 384 1,986,760 89.0 53 86 67 41 64 52 31 52 40 128 0.001 309,655 89.5 128 0.002 207,583 87.6 127 0.008 144,935 86.0 Table 1: Comparisons between different pruned networks and the base network.
group - block 1-1 1-2 1-3 2-1 2-2 2-3 3-1 3-2 3-3 ResNet-20 model A model B channels param size.
= 281,304 test accuracy (%) = 92.0 channels param size.
= 176,596 test accuracy (%) = 90.9 channels param size.
= 90,504 test accuracy (%) = 88.8 16 16 16 32 32 32 64 64 64 12 11 32 28 28 47 34 25 27 18 16 25 Table 2: Comparisons between ResNet-20 and its two pruned versions.
The last columns are the number of channels of each residual modules after pruning.
5.2 ILSVRC2012 EXPERIMENT We experiment our approach with the pre-trained ResNet-101 on ILSVRC2012 image classiﬁcation dataset (He et al., 2016).
ResNet-101 is one of the state-of-the-art network architecture in ImageNet Challenge.
We follow the standard pipeline to pre-process images to 224×224 for training ResNets.
We adopt the pre-trained TensorFlow ResNet-101 model whose single crop error rate is 23.6% with about 4.47 × 107 parameters.
2 We set the scaling parameter α = 0.01, the initial learning rate µt = 0.001, the sparsity penalty ρ = 0.1 and the batch size = 128 (across 4 GPUs).
The learning rate is decayed every four epochs with rate 0.86.
We create two pruned models from the different iterations of training ResNet-101: one has 2.36 × 107 parameters and the other has 1.73 × 107 parameters.
We then ﬁne-tune these two models using the standard way for training ResNet-101, and report their error rates.
The Top-5 error rate increases of both models are less than 0.5%.
The Top-1 error rates are summarized in Table 3.
To the best of our knowledge, only a few work has reported their performances on this very large-scale benchmark w.r.t. the Top-1 errors.
We compare our approach with some recent work in terms of models’ parameter size, ﬂops, and error rates.
As shown in Table 3, our model v2 has achieved a compression ratio more than 2.5 while still maintains more than 1% lower error rates than that of other state of the art models at comparable size of parameters.
In the ﬁrst experiment (CIFAR-10), we train the network from scratch and allocate enough steps for both γ and W adjusting their own scales.
Thus, initialization of an improper scale of γ-W is not really an issue given we optimize with enough steps.
But for the pre-trained models which were originally optimized without any constraints of γ, the γs scales are often unanticipated.
It actually takes as many steps as that of training from scratch for γ to warm up.
By adopting the rescaling trick setting α to a smaller value, we are able to skip the warm-up stage and quick start to sparsify γs.
For example, it might take more than a hundred epoch to train ResNet-101, but it only takes about 5-10 epochs to complete the pruning and a few more epochs to ﬁne-tune.
2https://github.com/tensorflow/models/tree/master/slim Published as a conference paper at ICLR 2018 network resnet-101 pruned (v2, ours) resnet-34 pruned (Li et al., 2017) param size.
resnet-50 pruned (Huang & Wang, 2017) ∼ 1.65 × 107 1.73 × 107 1.93 × 107 2.16 × 107 2.36 × 107 2.5 × 107 4.47 × 107 resnet-101 pruned (v1, ours) resnet-50 resnet-101 resnet-34 ﬂops 3.03 × 109 3.69 × 109 2.76 × 109 3.64 × 109 4.47 × 109 4.08 × 109 7.8 × 109 error (%) ∼ 26.8 25.44 27.8 26.8 24.73 24.8 23.6 ratio 66% 39% 89% 53% Table 3: Attributes of different versions of ResNet and their single crop errors on ILSVRC2012 benchmark.
The last column means the parameter size of pruned model vs.
the base model.
5.3 IMAGE FOREGROUND-BACKGROUND SEGMENTATION EXPERIMENT As we discussed about the two major observations in Section 5.1, a more appealing scenario is to apply our approach in pruning channels of over-parameterized model.
It often happens when one adopts a pre-trained network on a large task (such as ImageNet classiﬁcation) and ﬁne-tunes the model to a different and smaller task (Molchanov et al., 2017).
In this case, one might expect that some channels that were useful in the ﬁrst pre-training task are not quite contributing to the outputs of the second task.
We describe an image segmentation experiment whose neural network model is composed from an inception-like network branch and a densenet network branch.
The entire network takes a 224× 224 image and outputs binary mask at the same size.
The inception branch is mainly used for locating the foreground objects while the densenet network branch is used to reﬁne the boundaries around the segmented objects.
This model is originally trained on multiple datasets.
In our experiment, we attempt to prune channels in both the inception branch and densenet branch.
We set α = 0.01, ρ = 0.5, µt = 2× 10−5, and batch size = 24.
We train the pre-trained base model until all termination criterion are met, and build the pruned model for ﬁne-tuning.
The pruned model saves 86% parameters and 81% ﬂops of the base model.
We also compare the ﬁne-tuned pruned model with the pre-trained base model across different test benchmark.
Mean IOU is used as the evaluation metric3.
It shows that pruned model actually improves over the base model on four of the ﬁve test datasets with about 2% ∼ 5%, while performs worse than the base model on the most challenged dataset DUT-Omron, whose foregrounds might contain multiple objects.
test dataset (#images) MSRA10K (Liu et al., 2011) (2,500) DUT-Omron (Yang et al., 2013) (1,292) Adobe Flickr-portrait (Shen et al., 2016) (150) Adobe Flickr-hp (Shen et al., 2016) (300) COCO-person (Lin et al., 2014) (50) param.
size ﬂops mIOU 83.4% 83.2% 88.6% 84.5% 84.1% mIOU 85.5% 79.1% 93.3% 89.5% 87.5% base model pruned model 1.02 × 107 5.68 × 109 1.41 × 106 1.08 × 109 Table 4: mIOU reported on different test datasets for the base model and the pruned model.
6 CONCLUSIONS We proposed a model pruning technique that focuses on simplifying the computation graph of a deep convolutional neural networks.
Our approach adopts ISTA to update the γ parameter in batch normalization operator embedded in each convolution.
To accelerate the progress of model pruning, we use a γ-W rescaling trick before and after stochastic training.
Our method cleverly avoids some possible numerical difﬁculties such as mentioned in other regularization based related work, hence 3https://www.tensorflow.org/api_docs/python/tf/metrics/mean_iou Published as a conference paper at ICLR 2018 is easier to apply for practitioners.
We empirically validate our method through several benchmarks and show its usefulness and competitiveness in building compact CNN models.
REFERENCES Jose M Alvarez and Mathieu Salzmann.
Learning the number of neurons in deep networks.
In Advances in Neural Information Processing Systems, pp.
2270–2278, 2016.
Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung.
Structured pruning of deep convolutional neural networks.
ACM Journal on Emerging Technologies in Computing Systems (JETC), 13(3):32, 2017.
Amir Beck and Marc Teboulle.
A fast iterative shrinkage-thresholding algorithm for linear inverse problems.
SIAM Journal on Imaging Sciences, 2(1):183–202, 2009.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio.
Sharp minima can generalize for deep nets.
In Proceedings of International Conference on Machine Learning, 2017.
Song Han, Jeff Pool, John Tran, and William Dally.
Learning both weights and connections for efﬁcient neural network.
In Advances in Neural Information Processing Systems, pp.
1135–1143, 2015.
Babak Hassibi and David G Stork.
Second order derivatives for network pruning: Optimal brain surgeon.
In Advances in Neural Information Processing Systems, pp.
164–171, 1993.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
770–778, 2016.
Yihui He, Xiangyu Zhang, and Jian Sun.
Channel pruning for accelerating very deep neural networks.
Proceedings of International Conference on Computer Vision, 2017.
In Zehao Huang and Naiyan Wang.
Data-driven sparse structure selection for deep neural networks.
arXiv preprint arXiv:1707.01213, 2017.
Sergey Ioffe and Christian Szegedy.
Batch normalization: Accelerating deep network training by reducing internal covariate shift.
In International Conference on Machine Learning, pp.
448–456, 2015.
Vadim Lebedev and Victor Lempitsky.
Fast convnets using group-wise brain damage.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
2554–2564, 2016.
Yann LeCun, John S Denker, and Sara A Solla.
Optimal brain damage.
In Advances in Neural Information Processing Systems, pp.
598–605, 1990.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf.
Pruning ﬁlters for efﬁcient convnets.
In International Conference on Learning Representations, 2017.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence Zitnick.
Microsoft coco: Common objects in context.
In European conference on computer vision, pp.
740–755.
Springer, 2014.
Tie Liu, Zejian Yuan, Jian Sun, Jingdong Wang, Nanning Zheng, Xiaoou Tang, and Heung-Yeung Shum.
Learning to detect a salient object.
IEEE Transactions on Pattern analysis and machine intelligence, 33(2): 353–367, 2011.
Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang.
Learning efﬁcient convolutional networks through network slimming.
arXiv preprint arXiv:1708.06519, 2017.
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz.
Pruning convolutional neural net- In International Conference on Learning Representations, works for resource efﬁcient transfer learning.
2017.
Xiaoyong Shen, Aaron Hertzmann, Jiaya Jia, Sylvain Paris, Brian Price, Eli Shechtman, and Ian Sachs.
Au- tomatic portrait segmentation for image stylization.
In Computer Graphics Forum, volume 35, pp.
93–102.
Wiley Online Library, 2016.
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks.
In Advances in Neural Information Processing Systems, pp.
2074–2082, 2016.
Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, and Ming-Hsuan Yang.
Saliency detection via graph- based manifold ranking.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
3166–3173, 2013.
10 Published as a conference paper at ICLR 2018 Xiangyu Zhang, Jianhua Zou, Kaiming He, and Jian Sun.
Accelerating very deep convolutional networks for classiﬁcation and detection.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(10): 1943–1955, 2016.
Hao Zhou, Jose M Alvarez, and Fatih Porikli.
Less is more: Towards compact cnns.
In European Conference on Computer Vision, pp.
662–677.
Springer, 2016.
Figure 1: Visualization of the number of pruned channels at each convolution in the inception branch.
Colored regions represents the number of channels kept.
The height of each bar represents the size of feature map, and the width of each bar represents the size of channels.
It is observed that most of channels in the bottom layers are kept while most of channels in the top layers are pruned.
11
Systematic reviews are essential in various domains to summarize evidence from multiple sources.
They are used in the formulation of public policies and also contribute in medicine in the development of clinical guidance [6, 3].
They involve the searching, screening and synthesis of research ev- idence from multiple sources available in the form of textual documents.
It is critical in systematic reviews to identify all studies relevant to the review in order to minimise bias.
It is easy to see that such a process can be extremely time- consuming and demand signiﬁcant human resources [12].
Researchers have exploited active learning text classiﬁca- tion to make the process of systematic review more eﬃcient.
Active learning is an iterative process that starts from a small set of labelled studies and gradually learns to diﬀer- entiate between relevant and irrelevant studies [10, 15, 4].
Feature extraction models are essential to the success of active learning.
[15] proposed a multi-view approach that encoded documents in terms of diﬀerent feature vectors such as, words that appear in the title and the abstract, keywords and MeSH terms.
Each feature space is used to train a dif- ferent classiﬁer, and an ensemble of those classiﬁers predicts the ﬁnal outcome based on majority voting.
Most previous approaches worked well for studies in the clinical domain, but it was shown in [10] that the task of identifying relevant studies in public health domain resulted in poorer performance.
It was argued that the task of iden- tifying relevant studies is more challenging in this domain compared to clinical domain due to the presence of doc- uments from wide ranging disciplines (e.g., social science, occupational health, education, etc.) in public health com- pared to more speciﬁc studies included in clinical systematic reviews [1].
The authors proposed using topic modelling to extract topic based features using Latent Dirichlet Alloca- tion [2].
In addition to LDA, there are a variety of diﬀerent topic models available in machine learning like probabilistic latent semantic indexing (pLSI).
Hashimoto et al.
[7] presented a topic detection model to improve the performance of the active learning classiﬁer.
It uses a neural network model, popularly referred to as para- graph vectors [8] to extract ﬁxed length feature representa- tions from the documents.
It is argued that the paragraph vectors can encode information contained in the sequence of words.
As opposed to topic modelling techniques like LDA that treat documents as bag-of-words [9].
It was shown in [8] that paragraph vectors can compute semantic relatedness between diﬀerent textual contents of varying lengths.
LDA on the other hand treats documents as bag-of-words and does not utilise the information stored in the sequence and context of words.
[7] models topics using clustered represen- tation of paragraph vectors and represents each document in terms of the normalized distance from cluster centroids.
In this work, we perform experiments on a comprehensive dataset of reviews derived from wide ranging and diverse domains.
We perform experiments with diﬀerent feature ex- traction techniques including bag-of-words, paragraph vec- tors and topic modelling using LDA.
We observe that dif- ferent feature extraction methods work well for diﬀerent do- mains, but no isolated feature extraction technique works well for all reviews.
In previous works [15, 7], a naive active learning algorithm is used.
A naive active learning algo- rithm suﬀers from the problem of retrieving studies that are similar to the previously retrieved studies.
It happens be- cause the active learning classiﬁer does not receive enough diverse samples to classify correctly.
In [13], authors propose active learning with rationals, where, rationals are basically groups of words (phrases) that describe the label.
These ra- tionals are asked from labeler while providing the label and can lead to extra manual eﬀort.
In addition, such an ap- proach can only work well with bag of words representation of documents, and not with paragraph vector based repre- sentation.
In many cases, (including our case), we can not get additional data for the labels because of extra manual eﬀort involved.
[14] proposed an active learning strategy that uses a non-linear SVM, but training such classiﬁers is time consuming and not suited for active learning in clinical text classiﬁcation, since the system can not keep the labeler waiting for documents to labelled.
We propose a novel active learning algorithm for clinical text classiﬁcation that reduces the bias by including novelty in addition to the relevance of documents, and derive mean- ingful insights from our results on how to choose a feature extraction model for a particular review.
Our contributions can be summarized as follows: • We perform experiments on the use of diﬀerent feature extraction models over a comprehensive set of reviews, and contrary to the published results in [7] observe on a larger dataset that paragraph vector based topic mod- elling does not work better compared to bag-of-words and/or simple paragraph vectors based approach for active learning in systematic reviews.
• We propose a novel active learning algorithm that re- moves the inevitable bias in naive algorithms based solely on relevance, and instead develop an algorithm that in its early phases uses the novelty of documents, in addition to their relevance.
• We derive insights from our experiments on the per- formance of diﬀerent feature extraction models for dif- ferent domains, and propose an eﬀective way to choose the correct feature extraction model that requires no prior knowledge of the domain and/or review.
2.
METHODS 2.1 Feature Extraction 2.1.1 Bag-of-Words The most basic and oldest of feature extraction models from text documents is referred to as bag-of-words.
In the bag-of-words model, each of the possible terms in the entire corpus of documents is used to construct a vocabulary.
In addition to simple bag-of-words, the terms may be weighted by tf-idf, which measures the relative popularity of diﬀerent words in a given document and reduces the popularity of commonly used words in the corpus.
2.1.2 Latent Dirichlet Analysis Topic modelling techniques, like LDA [2], have been used in the area of systematic reviews to extract topical features from studies [11].
Such topical representations extracted from the documents are then used for training the active learning classiﬁer.
2.1.3 Paragraph Vectors Recently, a neural network model has been proposed that learns word vectors and paragraph vectors (PV) in a joint manner [8].
That is in contrast to the approach of learn- ing them separately.
Originally, word vectors represented only the words, but paragraph vectors are able to represent a sequence of words in the form of phrases, sentences or paragraphs.
Paragraph vectors have been reported to work with success in some previous works [7], and are expected to encode natural language better than other topic mod- els.
They have been used to measure similarities between Wikipedia article and research papers [5].
They have also been used to extract topics from studies used in systematic reviews [7].
2.1.4 Topic Modelling using PV In [7], a technique for topic modelling using paragraph vec- tors was proposed.
It is argued that paragraph vectors lead to better feature extraction by jointly learning the vector representation of words and documents.
Hence, clustering such paragraph vectors can lead to better topic modelling compared to topic models that are based on bag-of-words representation, like LDA [2].
2.1.5 Clustering Bag-of-Words We can cluster the tf-idf based bag-of-words representa- tion of documents to obtain topics.
The documents can then be represented in terms of the cluster-distance matrix.
It is the most basic of the diﬀerent topic extraction models that was used as a baseline in [7].
2.2 Active Learning The active learning process begins with a small number of manually labelled documents.
These documents are then used to train a classiﬁer that can be used to diﬀerentiate relevant and irrelevant studies among the rest of the stud- ies.
These studies are ordered in decreasing probability of relevance and the top-k studies are manually reviewed by an expert reviewer.
These manually reviewed k studies are then used to retrain the active learning classiﬁer along with the previously labelled studies.
There is an obvious problem with such a naive active learning algorithm.
The classiﬁer can easily get biased to- wards studies that are chosen in the beginning of the pro- cess.
Such a classiﬁer would continue to look for similar studies based on its current knowledge.
It would lead to a biased sample of studies in the training set of the classiﬁer.
We hypothesized that including novelty in addition to rele- vance while choosing documents for active learning can lead to an overall improvement in performance.
Some other ap- proaches like [13] require additional information about the labels, and can only work well with bag-of-words represen- tation of documentation.
In comparison, our method can work well with both bag-of-words and distributed represen- tations (like paragraph vectors).
Note, that to the best of our knowledge, naive active learning is the only algorithm used with success in systematic reviews [7, 12].
2.2.1 Proposed Algorithm To solve the above mentioned problems, we extract topics from documents using LDA topic model.
It gives as output topic vectors for each document in the corpus v(d) ∈ Rk×1.
These topic vectors from diﬀerent documents form a matrix V ∈ Rn×k, where n is the number of studies in the cor- pus and k is the number of topics.
We create a separate matrix of topic vectors for documents d that have already been manually labelled.
We refer to the set of already la- belled documents as H, and the matrix of topic vectors for documents d ∈ H is referred to as S ∈ R|H|×k.
We denote the set of currently unlabelled documents as G, and set of all documents as D.
We use principal component analysis to compute the top-t principal eigen vectors of ST S.
We deﬁne the probability of a document being novel as: p(n|d) = 1 − (cid:107)U U T v(d)(cid:107)2 (cid:107)v(d)(cid:107)2 (1) where U ∈ Rk×t contains t principal eigen vectors, and U U T ∈ Rk×k is the subspace formed by top-t principal eigen vectors of ST S.
It basically measures the novelty of a doc- ument by projecting its topic vector on the principal sub- space formed by topic vectors of documents in the training set.
The projection should be small for a document to be considered novel and vice versa, if not novel.
Additionally, we obtain the probability of a given document d being rele- vant p(r|d) from the classiﬁer.
We compute the probability of document being both relevant and novel as: p(r, n|d) = p(r|d) ∗ p(n|d) (2) We order the documents by above mentioned p(r, n|d), and then select the top-k documents in each iterative step of the active learning algorithm for review by an expert re- viewer.
We continue using novelty of a document in the ac- tive learning process till we have discovered a certain ﬁxed number of topics.
We assign a document d to topic i if i = arg max vk(d) where vk(d) is the value in kth index of topic vector v(d).
Afterwards, we stop incorporating the novelty in the active learning process and continue solely based on relevance.
We assume that a given topic has been discovered if any doc- ument in the labelled set H is assigned to that topic.
We provide a more formal description of the above mentioned process in Algorithm 1.
2.3 Evaluation 2.3.1 Evaluation Method Firstly, we evaluate the performance of active learning classiﬁer using diﬀerent feature extraction methods.
We ex- periment with both a linear-SVM (as used in [7, 10]) and a logistic regression classiﬁer and observe that performance of both classiﬁers is very similar.
Therefore, we use a logistic regression classiﬁer that models the posterior probability of a study being eligible p(y|d).
Secondly, we evaluate our proposed active learning algo- rithm and compare against the naive active learning ap- proach.
The active learning process starts with a small set of manually labelled studies.
Features are extracted from this labelled set using diﬀerent feature extraction models men- tioned above.
At each step of the iterative process a ﬁxed set of studies are reviewed manually by an expert reviewer.
We extract a sample of top-k studies at each iterative step for manual labelling from the ordered list of candidate studies [4, 12].
Thirdly, we derive insights from our experiments that would help us choose the correct feature extraction model without prior knowledge of the reviews.
All our datasets are already labelled by expert reviewers.
We simulated a human feedback active learning strategy [7, 10, 15].
Our evaluation strategy is similar to the previously published work [7].
2.3.2 Parameter Tuning We tune the diﬀerent parameters of PV using cross vali- dation.
We keep the number of topics in LDA at 300, as in [7].
We experimented with dimensionality of paragraph vec- tors and found that using 300 dimensional document vectors performed better in general across diﬀerent reviews.
We ex- perimented with higher value of dimension such as 1000 (as clf = Classiﬁer() clf = trainClassifier(clf, H) R = getRelevanceScores(clf, G) (cid:46) p(r|d) N = getNoveltyScores(V , H, G, t) (cid:46) p(n|d) for ∀d ∈ G do scores(d) = R(d) ∗ N (d) # Get initial manually labelled set H = getInitialLabelledSet G = D − H t = 3 max topics = 150 s = 25 n = 0 # Get topical representation of docs using LDA V = LDA(D, n topic=300) while n <max topics do Algorithm 1 Systematic Reviews Learning 1: procedure Active Learning 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: 37: 38: end procedure 39: function getNoveltyScores(V , H, G, t) 40: 41: 42: 43: clf = Classiﬁer() clf = trainClassifier(clf, H) R = getRelevanceScores(clf, G) for ∀d ∈ G do end for # Get top s documents as per scores G(cid:48) = getTopkDocuments(scores, s) H = H ∪ G(cid:48) G = G − G(cid:48) end for # Get top s documents as per scores G(cid:48) = getTopkDocuments(scores, s) H = H ∪ G(cid:48) G = G − G(cid:48) # Topics discovered in labelled set n = getNumberOfTopicsDiscovered(V, H) N = {} # Get top t Eigen Vectors U = getEigenVectors(H, ncomps = t) for ∀d ∈ G do (cid:46) p(r|d) end while while |G| > 0 do scores(d) = R(d) end while N (d) = 1 − (cid:107)U U T v(d)(cid:107)2 (cid:107)v(d)(cid:107)2 44: end for 45: return N 46: 47: end function 48: function getNumberOfTopicsDiscovered(V, H) 49: 50: 51: 52: end for 53: return |T| 54: 55: end function T = {} for ∀d ∈ H do t = arg maxk vk(d) T = T ∪ t in [7]), but the results were not better.
We tuned the reg- ularization parameter for the linear classiﬁer, but observed that the results are extremely stable across diﬀerent values of the regularization parameter and used the value that gave (a) Cooking Skills (b) Youth Development (c) Tobacco Packaging (d) FABC (e) CAFO (f) NPA (g) ASCD (h) DPCAD (i) STCS (j) FVC (k) SPCHD (l) LHVS Figure 1: X-axis represents the number of documents that have been manually annotated and Y-axis represents the number of relevant documents that have been discovered.
We can observe that diﬀerent feature extraction methods work better for diﬀerent reviews.
BoW works better for public health documents and PV performs better on clinical studies.
We can clearly infer that no feature extraction method is obviously superior to others for all the documents/domains.
Dataset Domain Num.
of citations Fraction of relevant studies LHVS ASCD FABC DPCAD STCS FVC SPCHD NPA CAFO Cooking Skills Youth Development Tobacco Packaging Clinical Clinical Clinical Clinical Clinical Clinical Clinical Animal Studies Animal Studies Public Health Public Health Public Health 1430 6381 24469 3087 4415 2157 14841 29659 3434 10957 14834 2792 0.018 0.010 0.005 0.019 0.026 0.050 0.006 0.168 0.023 0.017 0.091 0.034 Table 1: Statistics of the systematic review datasets used in the experiment.
the best results.
The number of principal eigen vectors that we use to compute p(n|d) are 3, i.e. t = 3 (line 42 in Algo- rithm 1).
The value of parameter max topics was set to 150 in line 6 of Algorithm 1.
It denotes the number of topics to be explored before we stop using novelty as a criteria in our active learning algorithm.
We used the value of s = 25 in the algorithm.
A small values for s implies that the clas- siﬁer would have to be learned too often, and larger value would lead to selecting too few relevant documents in the beginning, and as a result decrease in overall performance.
2.3.3 Evaluation Metric We evaluate the performance of our active learning process using a metric called WSS@95.
It stands for Work Saved over Sampling at 95% yield.
It can be expressed as: W SS@95 = (1 − burden) | yield ≥ 95% yield = T P M + T P A T P M + T P A + F N A burden = T P M + T N M + T P A + F P A (3) (4) (5) where N are the # of studies and superscript M and A de- note manual and automatic screening decisions.
TP, FP, TN and FN stand for # of True Positives, False Positives, True Negatives and False Negatives respectively.
These no- tations/deﬁnitions are similar to those mentioned in [7].
2.4 Datasets We used a number of public health, animal study and clinical review datasets from completed systematic reviews.
Some of these datasets have been previously used in Miwa et al.
[7].
We summarize the characteristics of diﬀerent datasets used in our experiments in Table 1.
We give a short description of diﬀerent datasets used: [10] and Hashimoto et al.
• LHVS: Leukodepletion for patients undergoing heart valve surgery • ASCD: Amiodarone versus other pharmacological in- terventions for prevention of sudden cardiac death • FABC: Altering availability and proximity of products for changing selection and consumption of food, alco- hol and tobacco • NPA: Animal studies on neuropathic pain • CAFO: Concentrated animal feeding operations • DPCAD: Psychological and pharmacological interven- tions for depression in patients with coronary artery disease • STCS: Preoperative statin therapy for patients under- going cardiac surgery • FVC: Interventions for increasing fruit and vegetable consumption in children aged 5 years and under • SPCHD: Internet-based interventions for the secondary prevention of coronary heart disease • Youth Development, Cooking Skills and Plain Tobacco Packaging as described in [7, 10] 3.
RESULTS We investigate the performance of diﬀerent feature extrac- tion techniques in terms of relevant studies discovered and amount of manual annotations required.
We plot the num- ber of relevant studies identiﬁed for a given amount of man- ual annotation for diﬀerent reviews that range from public health to clinical science.
Both these quantities converge to their maximum value during the last iteration of the ac- tive learning process.
We plot the performance for diﬀer- ent feature extraction models in Figure 1.
We denote topic modelling based on paragraph vectors with PV-TM, BoW stands for bag-of-words, BoW-TM denotes topic modelling based on BoW model and LDA refers to Latent Dirichlet Allocation.
We can see in Figure 1 that diﬀerent feature extraction methods work well for diﬀerent studies.
We ob- serve that in majority of the cases, paragraph vectors and BoW models perform better than the rest.
In documents pertaining to public health, BoW performs well, whereas for clinical reviews PV performs well.
These results are con- trary to previous results in [7], where the topic modelling based on PVs seemed to outperform other models across all disciplines.
We observe that BoW and PV perform better than topic modelling based on paragraph vectors on most datasets.
We show that it is not possible to ﬁnd a single feature extraction method that performs superior to others across all domains, but we need to identify one per each review.
When we use our active learning algorithm with BoW model, then we refer to it as IG-BOW (Information Gain- BOW), whereas when we use the paragraph vector model, we refer to it as IG-PV (Information Gain-PV).
We can observe the performance of proposed active learning algo- rithm compared to the naive active learning algorithm as the screening progresses in Figure 2.
We can notice that our pro- posed algorithm explores in the initial phases of the screen- ing process, but as the process continues the performance improves.
It outperforms naive active learning algorithm by the end of the screening process.
We plot the WSS@95 (i.e. WSS at 95% yield) in Figure 3 for our proposed active learn- ing algorithm.
In addition to the naive active learning algo- rithm, we compare our method with an additional baseline algorithm that selects the samples about which the classiﬁer is least conﬁdent during the initial 10% (set using cross val- idation on independent dataset) screening i.e. it randomly (a) Cooking Skills (b) Youth Development (c) Tobacco Packaging (d) FABC (e) CAFO (f) NPA (g) ASCD (h) DPCAD (i) STCS (j) FVC (k) SPCHD (l) LHVS Figure 2: X-axis represents the number of documents that have been manually annotated and Y-axis represents the number of relevant documents that have been discovered.
We can observe that our proposed active learning algorithm, that is IG-PV and IG-BOW explore during the initial phases of screening and then their performance improves as the process continues.
We show the results of proposed active learning algorithm compared to the naive active learning algorithm in terms of WSS@95 in Figure 3.
selects k studies that have 0.4 ≤ p(y = 1|d) ≤ 0.6. We re- fer to this baseline as LC, and consequently, LC-BoW uses bag-of-words as features and LC-PV uses paragraph vectors as features.
We can observe that one of the two, i.e. IG- BoW or IG-PV, performs better compared to naive active learning algorithm using just BoW or PV.
It validates our hypothesis that using novelty to explore during the initial phases of active learning can lead to better results overall, especially in terms of WSS@95.
We only present BoW and PV because (as we mentioned in the previous paragraph) we observed that these two feature extraction methods per- form well across most studies.
We should mention at this point that we also used our active learning algorithm with the feature extraction model presented in [7], but it did not perform comparable to PV or BoW.
Therefore, we did not present the results with PV-TM to keep the analysis simple and sequential.
It has been shown in our results that both PV and BoW work well for speciﬁc systematic reviews.
It is not feasible to estimate with certainty in advance which of the two would work better for a review.
Meanwhile, we obtained the recall for both feature extraction methods after screening through initial 10% of the data.
We can see that comparison in Table 2.
We report the recall for the initial 10% of the data and denote in bold the approach that scores higher in terms of WSS@95 by the end of the process.
We observe that the approach that has higher recall on the initial 10% of the data works well in terms of WSS@95 by the end of the screening process.
As a result, we infer that both approaches - IG- BOW or IG-PV - should be used in the beginning and then the one that performs better should be continued.
We should mention that we experimented with diﬀerent ensembles of feature extraction models, but since the results were not impressive compared to the individual models, we omit the results.
Dataset LHVS ASCD FABC NPA CAFO DPCAD STCS FVC SPCHD Cooking Skills Youth Development Tobacco Packaging IG-BOW IG-PV 0.275 0.671 0.899 0.471 0.821 0.634 0.613 0.345 0.950 0.738 0.426 0.686 0.896 0.537 0.957 0.458 0.345 0.403 0.322 0.275 0.693 0.497 0.435 0.757 Table 2: Recall upon screening through 10% of the studies in the review.
We use the bold notation to mark the overall winner in terms of WSS@95 at the end of the screening process.
4.
DISCUSSION We should mention at this point that we also experi- mented with learning paragraph vectors using additional ex- ternal data in the case of public health and animal studies.
We expected that additional documents might lead to im- proved paragraph vectors.
These external data consisted of studies related to the review in question, but most of these were not directly relevant to the review.
To our surprise, we did not see any improvement in the results upon using such external data for learning paragraph vectors.
On the contrary, the performance decreased as we used more ex- ternal data during the learning of paragraph vectors.
We should also mention that we conducted our experiments on more than 30 reviews, and obtained results similar to those presented in this paper.
We observed that active learning can be an eﬀective strat- egy to semi-automate manual annotations and reduce the workload.
But, the current active learning approaches do not accurately estimate the proportion of relevant studies that have been annotated.
In many cases, it is necessary to extract 95% or more relevant studies in order to avoid bias in the systematic review.
If there was an eﬀective way to estimate the recall of the active learner precisely then the screening process could move towards complete automation.
In the future, we will work towards an accurate recall esti- mation while using an eﬃcient active learning strategy.
5.
CONCLUSION We evaluated diﬀerent feature extraction models over a comprehensive dataset of reviews from varied domains.
We observed that both BoW and PV can outperform other ap- proaches over certain reviews and domains.
We recognized that a naive active learning algorithm suf- fers from bias.
It tries to select documents that are similar to documents in the training data.
Initially, the training data are small and this could lead to reduced performance.
We propose a novelty based active learning algorithm that works on exploring diﬀerent topics during the initial phases of the active learning process and then proceeds based on relevance in the later phases.
It leads to more exploration of diﬀerent topics in the beginning and better performance in terms of WSS@95 by the end.
We evaluate our approach against naive active learning algorithm, and observe that the proposed algorithm works equal to, or better than, the naive algorithm in all instances.
We also develop insights regarding the choice of feature extraction methods for diﬀerent reviews.
We observe based on our experiments on a large number of reviews, that both the feature extraction methods should be used during the screening of initial 10% studies.
Afterwards, the better per- forming approach should be continued.
It leads to some extra manual annotations in the beginning, but the overall gain in terms of WSS@95 compensates for that disadvan- tage.
6.
ACKNOWLEDGEMENT We acknowledge with thanks Annette O’Connor (Iowa State University), the Cochrane Heart Group and the EPPI- Centre (University College London) review team who sup- plied data for conducting this study.
This work was sup- ported by a grant awarded by the UK Medical Research Council: Identifying relevant studies for systematic reviews and health technology assessments using text mining [Grant No. MR/J005037/1].
James Thomas was (in part) sup- ported by the National Institute for Health Research (NIHR) Collaboration for Leadership in Applied Health Research and Care (CLAHRC) North Thames at Bart’s Health NHS Trust.
The views expressed are those of the author(s) and Figure 3: The ﬁgure plots the performance of proposed active learning algorithm and the naive active learning algorithm in terms of WSS@95.
We use the two best performing feature extraction models (i.e. BoW and PV) for comparing the proposed active learning algorithm with the naive algorithm and baseline algorithm (LC-Bow/LC-PV).
We can observe that in 9 out of the 12 datasets the proposed approach (IG-PV/IG-BoW) is the best performing in terms of WSS@95.
The winners have a statistically signiﬁcant lead over all the losers using a t-test at p < 0.05.
not necessarily those of the NHS, the NIHR or the Depart- ment of Health.
7.
REFERENCES [1] C.
C.
Beahler, J.
J.
Sundheim, and N.
I.
Trapp.
Information retrieval in systematic reviews: challenges in the public health arena.
American journal of preventive medicine, 18(4):6–10, 2000.
[2] D.
M.
Blei, A.
Y.
Ng, and M.
I.
Jordan.
Latent dirichlet allocation.
Journal of machine Learning research, 3(Jan):993–1022, 2003.
[3] I.
Chalmers, L.
V.
Hedges, and H.
Cooper.
A brief history of research synthesis.
Evaluation & the health professions, 25(1):12–37, 2002.
[4] A.
M.
Cohen, W.
R.
Hersh, K.
Peterson, and P.-Y.
Yen.
Reducing workload in systematic review preparation using automated citation classiﬁcation.
Journal of the American Medical Informatics Association, 13(2):206–219, 2006.
[5] A.
M.
Dai, C.
Olah, and Q.
V.
Le. Document embedding with paragraph vectors.
arXiv preprint arXiv:1507.07998, 2015.
[6] D.
Gough, S.
Oliver, and J.
Thomas.
An introduction to systematic reviews.
Sage, 2012.
[7] K.
Hashimoto, G.
Kontonatsios, M.
Miwa, and S.
Ananiadou.
Topic detection using paragraph vectors to support active learning in systematic reviews.
Journal of Biomedical Informatics, 2016.
[8] Q.
V.
Le and T.
Mikolov.
Distributed representations of sentences and documents.
In ICML, volume 14, pages 1188–1196, 2014.
[9] T.
Mikolov, I.
Sutskever, K.
Chen, G.
S.
Corrado, and J.
Dean.
Distributed representations of words and phrases and their compositionality.
In Advances in neural information processing systems, pages 3111–3119, 2013.
[10] M.
Miwa, J.
Thomas, A.
O’Mara-Eves, and S.
Ananiadou.
Reducing systematic review workload through certainty-based screening.
Journal of biomedical informatics, 51:242–253, 2014.
[11] Y.
Mo, G.
Kontonatsios, and S.
Ananiadou.
Supporting systematic reviews using lda-based document representations.
Systematic reviews, 4(1):1, 2015.
[12] A.
O’Mara-Eves, J.
Thomas, J.
McNaught, M.
Miwa, and S.
Ananiadou.
Using text mining for study identiﬁcation in systematic reviews: a systematic review of current approaches.
Systematic reviews, 4(1):1, 2015.
[13] M.
Sharma, D.
Zhuang, and M.
Bilgic.
Active learning with rationales for text classiﬁcation.
2015.
[14] S.
Tong and D.
Koller.
Support vector machine active learning with applications to text classiﬁcation.
Journal of machine learning research, 2(Nov):45–66, 2001.
[15] B.
C.
Wallace, T.
A.
Trikalinos, J.
Lau, C.
Brodley, and C.
H.
Schmid.
Semi-automated screening of biomedical citations for systematic reviews.
BMC bioinformatics, 11(1):1, 2010.
0.000.250.500.75ASCDCAFOCooking SkillsDPCADFABCFVCLHVSNPASPCHDSTCSTobacco PackagingYouth DevelopmentDatasetWSS@95method:   BOWLC−BoWIG−BOWPVLC−PVIG−PV
In the last decade, deep learning has achieved remarkable results in computer vision, speech recognition and natural language processing, obtaining in some tasks human-like [1] or even super-human [2] performance.
The roots of recent successes of deep learning can be found in: (i) the increase of the data available for training neural networks, (ii) the rising of commodity computational power needed to crunch the data, (iii) the development of new techniques, archi- tectures, and activation functions that improve convergence during training of deeper networks, overcoming the obstacle of vanishing/exploding gradient [3], [4].
For many years, neural networks have usually employed logistic sigmoid activation functions.
Unfortunately, this acti- vation is affected by saturation issues.
This problem reduces its effectiveness and, nowadays, its usage in feedforward networks is discouraged [5].
To overcome such weakness and improve accuracy results, an active area of research has been devoted to design novel activation functions.
The training procedure of these architectures usually in- volve optimization of the weights of their layers only, while non-linearities are generally pre-speciﬁed and their (possible) parameters are usually considered as hyper-parameters to be tuned manually.
In this paper, we introduce two approaches able to automatically learn combinations of base activation functions (such as: the identity function, ReLU, and tanh) during training; our aim is to identify a search space for the activation functions by means of convex combination or afﬁne combination of the base functions.
To the best of our knowledge, this is one of the ﬁrst attempts to automatically combine and optimize activation functions during the training phase.
We tested different well-known networks employing cus- tomary activation functions on three standard datasets and we compared their results with those obtained applying our novel approaches.
The techniques proposed in this paper outper- formed the baselines in all the experiments, even when using deep architectures: we found a 3.01 percentage points increase in the top-1 accuracy for AlexNet on ILSVRC-2012.
This paper is organized as follows: in Section II the related works are summarized; in Section III the proposed methods are described; in Section IV the experimental results are pre- sented; ﬁnally, conclusions and future works are summarized in Section V.
II.
RELATED WORK Designing activation functions that enable fast training of accurate deep neural networks is an active area of research.
The rectiﬁed linear activation function, introduced in [6] and argued in [7] to be a better biological model than logistic sigmoid activation function, has eased the training of deep neural networks by alleviating the problems related to weight initialization and vanishing gradient.
Slight variations of ReLU have been proposed over the years, such as leaky ReLU (LReLU), [8], which addresses dead neuron issues in ReLU networks, thresholded ReLU [9], which tackles the problem of large negative biases in autoencoders, and parametric ReLU (PReLU) [10], which treats the leakage parameter of LReLU as a per-ﬁlter learnable weight.
A smooth version of ReLU, called softplus, has been proposed in [11].
Despite some theoretical advantages over ReLU (it is differentiable everywhere and it has less saturation issues), this activation function usually does not achieve better results [7] when compared to its basic version.
More recently, maxout has been introduced in [12] as an activation function aimed at enhancing dropouts abilities as a model averaging technique.
Among its extensions, it is worth mentioning the probabilistic maxout [13], and the Lp norm pooling activation [14] that is able to recover the maxout activation when p → ∞.
Considering the last developments of activation functions in neural networks, it is important to mention the exponential linear unit function (ELU) [15] and the scaled exponential linear unit function (SELU) [16].
Like ReLU, LReLU, and PReLU, ELU reduces the vanishing gradient problem.
Fur- thermore, ELU has negative values, allowing to push mean unit activations closer to zero like batch normalization, and speeding up the learning.
SELU extends this property ensuring that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance, even under the presence of noise and perturbations.
With the exception of PReLU, all previous activations are pre-speciﬁed (i.e. non-learnable).
A ﬁrst attempt to learn activations in a neural network can be found in [17], where the authors propose to randomly add or remove logistic or Gaussian activation functions using an evolutionary program- ming method.
On the other hand, in [18]–[21] the authors proposed novel approaches to learn the best activation function per neuron among a pool of allowed activations by means of genetic and evolutionary algorithms.
A different method has been proposed in [22], which is able to learn the hardness parameter of a sigmoid function, similarly to the approach employed by PReLU to learn the leakage parameter.
However, all the previous learning techniques are limited by the fact that the family of functions over which the learning takes place is either ﬁnite or a simple parameterization of customary activation functions.
Recently, in [23] the authors tackle the problem from a different angle using piecewise linear activation functions that are learned independently for each neuron using gradient descent.
However, (i) the number of linear pieces is treated as a hyper-parameter of the optimization problem; (ii) the number of learned parameters increases proportionally to the amount of hidden units; (iii) all the learned piecewise linear activation functions are ReLU(x) for x large enough (i.e. there exists u ∈ R such that g(x) = ReLU(x) for x ≥ u), thus reducing the expressivity of the learned activation functions by design.
It is worth mentioning that, in [24] the authors propose the network in network approach where they replace activation functions in convolutional layers with small multi-layer per- ceptrons.
In this paper we try to overcome some of the limitations of the aforementioned approaches.
Indeed, the two techniques explained in Section III: (i) increase the expressivity power of the learned activation functions by enlarging the hypothesis space explored during training with respect to [17]–[22]; (ii) restrict the hypothesis space with respect to [23], [24], in order to allow a faster training without the need of a careful initialization of network weights (see Proposition 1 and the following lines).
III.
METHODS A neural network Nd made of d hidden layers can be seen as the functional composition of d functions Li followed by a ﬁnal mapping ¯L that depends on the task at hand (e.g. classi- ﬁcation, regression): Nd = ¯L◦ Ld◦ .
.
.◦ L1.
In particular, each hidden layer function Li can be written as the composition of two functions, gi followed by σi, the former being a suitable remapping of the layer input neurons, the latter being the activation function of the layer: Li = σi ◦ gi.
In the most general case, both σi and gi are parameterized and belongs to some hypothesis spaces Hσi and Hgi, respectively.
Hence, the learning procedure of Li amounts to an optimization problem over the layer hypothesis space Hi = Hσi × Hgi.
Usually, σi is taken as a non-learnable function; therefore, in the most common scenario Hσi is a singleton: Hi = {σi} × Hgi.
For example, for a fully-connected layer from Rni to Rmi with ReLU activation we have that Hgi is the set of all afﬁne transformations from Rni to Rmi, i.e. Hi = {ReLU}× Lin(Rni, Rmi) × K(Rmi), where Lin(A, B) and K(B) are the sets of linear maps between A and B, and the set of translations of B, respectively.
In this paper, we introduce two techniques to deﬁne learn- able activation functions that could be plugged in all hidden layers of a neural network architecture.
The two approaches differ in how they deﬁne the hypothesis space Hσi.
Both of them are based on the following idea: (i) select a ﬁnite set of activation functions F := {f1, .
.
.
, fN}, whose elements will be used as base elements; (ii) deﬁne the learnable activation function σi as a linear combination of the elements of F; (iii) identify a suitable hypothesis space Hσi; (iv) optimize the whole network, where the hypothesis space of each hidden layer is Hi = Hσi × Hgi.
We will now give some basic deﬁnitions used throughout the paper.
Note that, hereinafter, all activation functions from R to R will naturally extend to functions from Rn to Rn by means of entrywise application.
Given a vector space V and a ﬁnite subset A ⊆ V , we can deﬁne the following two subsets of V : (i) the convex hull of A, namely: conv(A) := {(cid:80) aﬀ(A) := {(cid:80) i ciai |(cid:80) i ciai |(cid:80) (ii) the afﬁne hull of A, namely: i ci = 1, ci ≥ 0, ai ∈ A}; i ci = 1, ai ∈ A}.
We remark that, neither conv(A) nor aﬀ(A) are vector sub- spaces of V .
Indeed, conv(A) is just a generic convex subset in V reducing to a (|A| − 1)-dimensional simplex whenever the elements of A are linearly independent.
On the other hand, aﬀ(A) is an afﬁne subspace of V of dimension |A| − 1, i.e. for an arbitrary ¯a ∈ aﬀ(A) the set {a − ¯a | a ∈ aﬀ(A)} is a linear subspace of V of dimension |A| − 1.
Clearly, conv(A) ⊂ aﬀ(A).
Let F := {f0, f1, .
.
.
, fN} be a ﬁnite collection of activa- tion functions fi from R to R.
We can deﬁne a vector space i cifi.
Note that, despite F is (by deﬁnition) a spanning set of F , it is not generally a basis; indeed |F| ≥ dim F .
Since (almost everywhere) differentiability is a property preserved by ﬁnite linear combinations, and since conv(F ) ⊂ aﬀ(F ) ⊆ F , assuming that F contains (almost everywhere) differentiable activation functions, conv(F ) and aﬀ(F ) are made of (almost everywhere) differentiable functions, i.e. valid F from F by taking all linear combinations (cid:80) implies that ¯f =(cid:80) i cifi with(cid:80) (⇐) By hypothesis, ¯f = (cid:80) ¯f (0) =(cid:80) i cifi with (cid:80) i cifi(0) = 0 and ¯f(cid:48)(0) =(cid:80) i ci = 1, namely ¯f ∈ aﬀ(F ).
i ci = 1.
Hence, i ci = 1, i (0) =(cid:80) i cif(cid:48) namely ¯f approximates the identity near origin.
Since conv(F ) ⊂ aﬀ(F ), also conv(F ) enjoys the same property.
It is important to underline that, activation functions approximating the identity near origin, have been argued to be desirable to train deep networks randomly initialized with weights near zero [26].
Indeed, for such an activation function the initial gradients will be relatively large, thus speeding up the training phase.
Note that, such behavior is enjoyed also by ReLU, which approximates the identity only from one side: i.e. x → 0+.
Furthermore, training deep networks with activation functions not approximating the identity near origin requires a more careful initialization, in order to properly scale the layers input [27].
For the aforementioned reasons: (i) preserved differen- tiability; (ii) absence of the requirement of monotonicity; (iii) approximation of the identity near origin; both aﬀ(F ) and conv(F ) are good candidates for Hσi.
Thanks to the previous deﬁnitions and the aforementioned properties, we can now formalize the two techniques to build learnable activation functions as follows: (i) choose a ﬁnite set F = {f1, .
.
.
, fN}, where each fi is a (almost everywhere) differentiable activation function approximating the identity near origin (at least from one side); of all the fi ∈ F; conv(F ).
(ii) deﬁne a new activation function ¯f as a linear combination the sets aﬀ(F ) or (iii) select as hypothesis space H ¯f In Section IV, we present results using the following choices for F: F := {id, tanh}, Since conv(F ) ⊂ aﬀ(F ), F := {id, ReLU}, F := {ReLU, tanh}, F := {id, ReLU, tanh}, (1) where id is the identity function.
Clearly, other choices of F may be available, provided the requirements in (i) are satisﬁed.
the convex hull-based tech- nique can to be understood as a regularized version of the afﬁne hull-based one, where the corresponding hypothesis space has been explicitly constrained to be compact.
Such a regularization, in addition to restrict the complexity of the hypothesis space, guarantees that the ﬁnal activation function is monotonic (provided all fi ∈ F are monotonic as well).
Moreover, the convex hull-based technique together with F := {id, ReLU,} recovers the learnable LReLU activation func- tion, i.e. LReLUα(x) = x if x ≥ 0, while LReLUα(x) = αx otherwise, where 0 ≤ α (cid:28) 1 (usually α = 10-2).
Indeed, conv(F ) = { ¯f := p · id +(1 − p) · ReLU with 0 ≤ p ≤ 1} and since ReLU = id for x ≥ 0 and ReLU = 0 otherwise, we have that ¯f = p · id +(1 − p) · id = id for x ≥ 0 and ¯f = p · id +(1 − p) · 0 = p · id otherwise, i.e. LReLUp. It is worth mentioning that, as shown in Figure 2, layers using convex hull-based and afﬁne hull-based activations with Fig.
1.
The ﬁgure shows the relationship between afﬁne hull, convex hull, and convex cone for a set F made of three linearly independent elements: F := {f1, f2, f3}.
Since dim F = |F| = 3, conv(F ) is a 2-simplex (the gray triangle in the ﬁgure), while aﬀ(F ) is a plane within the three dimensional vector space F (the plane identiﬁed by the dashed circle in the ﬁgure).
cone(F ) is the three dimensional manifold delimited by the three incident straight lines li := {x ∈ F | x = αfi, ∀0 ≤ α ∈ R} for i = 1, 2, 3, i.e. the cone extremal rays.
It can easily be seen that conv(F ) corresponds to the intersection of cone(F ) with aﬀ(F ).
activation functions for a neural network that can be learned by means of gradient descent.
The activations used in real world scenarios are usually monotonic increasing functions.
Unfortunately, the monotonic- ity is not ensured under arbitrary linear combination, meaning that even if all fi ∈ F are non-decreasing functions, an arbitrary element ¯f ∈ F might be neither a non-decreasing nor a non-increasing function.
As a matter of fact, considering only i ≥ 0 ∀fi ∈ F), non-decreasing differentiable functions (f(cid:48) all non-decreasing differentiable functions in F lie inside the convex cone cone(F ) ⊂ F , i.e.: cone(F ) := {(cid:80) i cifi | ci ≥ 0, fi ∈ F}.
Indeed, ∀g ∈ cone(F ) we have that g(cid:48) ≥ 0.
Thanks to the deﬁnition of aﬀ(F ), cone(F ), and conv(F ), we can conclude that conv(F ) = cone(F ) ∩ aﬀ(F ), which implies that monotonicity of the elements of F is preserved by all the elements of conv(F ) but not by aﬀ(F ) (see Figure 1).
Nevertheless, in [25] it is shown that even non-monotonic activation functions can approximate arbitrarily complex func- tions for sufﬁciently large neural networks.
Indeed, in [5] the authors trained a feedforward network using cosine activation functions on the MNIST dataset obtaining an error rate smaller than 1%.
Therefore, also aﬀ(F ) is a proper candidate for Hσi.
In addition, the afﬁne subspace aﬀ(F ) enjoys the following property.
Proposition 1.
Let all fi ∈ F be linearly independent and approximate the identity function near origin (i.e. fi(0) = 0 i (0) = 1), then, ¯f ∈ F approximates the identity if and and f(cid:48) only if ¯f ∈ aﬀ(F ).
Proof.
The proof is immediate.
Let us expand F (cid:51) ¯f as ¯f = i cifi (by hypothesis the fi form a basis) and prove the two-way implication.
(⇒) By hypothesis, ¯f (0) = 0 and ¯f(cid:48)(0) = 1.
The relation i ci = 1.
In turns, this (cid:80) on the derivative reads(cid:80) i (0) =(cid:80) i cif(cid:48) TABLE I KE R A SNE T ARCHITECTURE Id conv_1 conv_2 conv_3 conv_4 fc_5 fc_6 Layers 2D convolution Activation 2D convolution Activation Max pooling Dropout 2D convolution Activation 2D convolution Activation Max pooling Dropout Fully connected Activation Dropout Fully connected Activation Properties 32 × (3, 3) 32 × (3, 3) (2, 2) 25% 64 × (3, 3) 64 × (3, 3) (2, 2) 25% 512 50% 10 softmax pixel value by 255, and we augmented the resulting dataset during training phase by means of random horizontal ﬂip and image shifting; ILSVRC-2012: a 1k classes classiﬁcation task with 1.2M training examples and 50k validation examples.
The examples are colour images of various sizes [30].
We resized each image to 256x256 pixels, we subtracted pixel mean values, and we augmented the dataset during training phase by randomly cropping to 227x277 pixels and horizontally ﬂipping the images.
Note that, we did not train our network with the relighting data-augmentation proposed in [31].
The considered architectures are the following: LeNet-5: a convolutional network made of two convolu- tional layers followed by two fully connected layers [32].
The convolutional layers have respectively 20 and 50 ﬁlters of size 5 × 5, while the hidden fully connected layer is made of 500 neurons.
Max pooling with size 2×2 is used after each convolutional layer, without dropout.
We assessed LeNet-5 on Fashion-MNIST and CIFAR-10 datasets, resulting in two networks with 431k and 657k parameters, respectively; KerasNet: a convolutional neural network included in the Keras framework [33].
It is made of 4 convolutional layers and 2 fully connected layers, and it employs both max pooling and dropout.
This architecture is presented in Ta- ble I.
We tested KerasNet on both Fashion-MNIST and CIFAR-10 datasets, resulting in two networks with 890k and 1.2M parameters, respectively; ResNet-56: a residual network made of 56 total layers, employing pooling and skip connection [34].
Its performance has been evaluated on CIFAR-10, corresponding to a network with 858k parameters; AlexNet: a convolutional network made of 5 convolutional and 3 fully connected layers [31].
We tested it against ILSVRC-2012 dataset, resulting in a network with 61M pa- rameters.
Note that, as shown in [31], ReLU-based activation functions signiﬁcantly outperform networks based on other activations.
Therefore, in this context ReLU-based networks Fig.
2.
The ﬁgure shows how a neural network layer with convex/afﬁne hull- based activation can be seen as a two-stage pipeline.
Here the k-th layer is a two neurons layer, with set F composed of two base functions.
The ﬁrst stage of the pipeline is made of two stacked layers, each one featuring only one activation belonging to F.
The weights between the two stacked layers are shared.
The second stage of the pipeline is a 1D-convolution with kernel of size 1 between the stacked layers, which play the role of n channels for the convolution.
The weights of the convolution are constrained to sum to one, and also to be positive when using the convex hull-based technique.
n base functions can also be seen as the following two-stage pipeline: (i) n stacked fully connected layers, each of them featuring one of the base functions, and all of them sharing weights; (ii) a 1D-convolution with kernel of size 1 between the n stacked layers (which are treated by the convolution as n separate channels), whose weights are constrained to sum to one (and to be positive in case of the convex hull-based technique)1.
IV.
RESULTS We tested the convex hull-based and the afﬁne hull-based approaches by evaluating their effectiveness on three pub- licly available datasets used for image classiﬁcation, greatly differing in the number of input features and examples.
Moreover, each dataset was evaluated using different network architectures.
These networks were trained and tested using as activation functions (for all their hidden layers) those learned by the convex hull-based and the afﬁne hull-based approaches combining the base activations reported in Equation (1).
In addition, the base activation functions alone and LReLU were also employed in order to compare the overall performance.
Speciﬁcally, the datasets we used are: Fashion-MNIST: it is a dataset of Zalando’s article im- ages, composed by a training and test sets of 60k and 10k examples, respectively.
Each example is a 28x28 grayscale image, associated with a label that belongs to 10 classes [28].
We divided each pixel value by 255, and we augmented the resulting dataset during training phase by means of random horizontal ﬂip and image shifting; CIFAR-10: it is a balanced dataset of 60k 32x32 colour images belonging to 10 classes.
There are 50k training images and 10k test images [29].
Also in this case we divided each 1The 1D-convolution with kernel of size 1 can also be seen as a weighted average of the stacked fully connected layers (with possibly negative weights in case of the afﬁne hull-based technique).
Fig.
3.
The ﬁgure shows the hidden afﬁne hull-based and convex hull-based activation functions learned during the training of the KerasNet architecture on CIFAR-10 dataset.
Activations learned on aﬀ({id, ReLU, tanh}) are represented with solid lines, while those learned on conv({id, ReLU, tanh}) are dashed.
only have been built and tested for comparison.
All networks were implemented from scratch using the Keras framework [33] on top of TensorFlow [35].
Notice that, our AlexNet implementation is a porting to Keras of the Caffe architecture2.
We trained LeNet-5, KerasNet, and ResNet-56 using RMSprop with learning rate 10-4 and learning rate decay over each mini-batch update of 10-6.
For AlexNet we used SGD with starting learning rate 10-2, step- wise learning rate decay, weight-decay hyper-parameter 5·10-4, and momentum 0.9. Table II shows the top-1 accuracy for all the run exper- iments.
The best conﬁgurations (shaded cells in the table) are always achieved using our techniques, where in 5 out of 6 experiments the afﬁne hull-based approach outperformed convex hull-based ones.
The uplift in top-1 accuracy using our approaches compared to customary activations goes from 0.69 percentage points (pp) for LeNet-5 on Fashion-MNIST up to 4.59 pp for KerasNet on CIFAR-10.
It is worth mentioning that, even in deep neural networks, such as AlexNet, a substantial increase is observed, i.e. 3.01 pp on ILSVRC-2012.
Moreover, the proposed techniques usually achieve bet- ter results than their corresponding base activation functions (boldface in the table).
Note that, the novel activations work well for very deep networks and also with various architectures involving different types of layer (e.g. residual unit, dropout, pooling, convolutional, and fully connected).
Furthermore, our experiments show how the learning of the leakage parameter achieved by the activation based on conv({id, ReLU}) allows to outperform or to achieve the same results of LReLU.
Figure 3 shows activations learned by KerasNet on CIFAR-10 when using convex hull-based and afﬁne hull- based activations with F = {id, ReLU, tanh}.
It is possible to notice that, as already theoretically proved in Section III, convex combinations preserved monotonicity of the base ac- tivation functions while afﬁne ones did not.
2See https://github.com/BVLC/caffe/tree/master/models/bvlc alexnet.
V.
CONCLUSION In this paper we introduced two novel techniques able to learn new activations starting from a ﬁnite collection F of base functions.
Both our ideas are based on building an arbitrary linear combination of the elements of F and on deﬁning a suitable hypothesis space where the learning procedure of the linear combination takes place.
The hypothesis spaces for the two techniques are conv(F ) and aﬀ(F ).
We showed that, provided all the elements of F approximate the identity near origin, aﬀ(F ) is the only set where it is possible to ﬁnd combined activations that also approximate id near origin.
Moreover, aﬀ(F ) allows to explore non-monotonic activation functions, while conv({id, ReLU}) may be seen as a learnable LReLU activation function.
We tested the two techniques on various architec- tures (LeNet-5, KerasNet, ResNet-56, AlexNet) and datasets (Fashion-MNIST, CIFAR-10, ILSVRC-2012), comparing results with LReLU and single base activation functions.
In all our experiments, the techniques proposed in this paper achieved the best performance and the combined activation functions learned using our approaches usually outperform the corresponding base components.
The effectiveness of the proposed techniques is further proved by the increase in performance achieved using networks with different depths and architectures.
be to analyze other sets (F) of base functions.
In our opinion, an interesting extension of this work would REFERENCES [1] W.
Xiong, J.
Droppo, X.
Huang, F.
Seide, M.
Seltzer, A.
Stolcke, D.
Yu, and G.
Zweig, “Achieving human parity in conversational speech recognition,” arXiv preprint arXiv:1610.05256, 2016.
[2] D.
C.
Ciresan, U.
Meier, J.
Masci, and J.
Schmidhuber, “A committee of IEEE, 2011, neural networks for trafﬁc sign classiﬁcation.” in IJCNN.
pp.
1918–1921.
[3] Y.
Bengio, P.
Simard, and P.
Frasconi, “Learning long-term dependencies with gradient descent is difﬁcult,” IEEE transactions on neural networks, vol.
5, no.
2, pp.
157–166, 1994.
[4] X.
Glorot and Y.
Bengio, “Understanding the difﬁculty of training deep feedforward neural networks,” in Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics, 2010, pp.
249–256.
[5] I.
Goodfellow, Y.
Bengio, and A.
Courville, Deep Learning.
MIT Press, 2016.
EXPERIMENT RESULTS.
THE TABLE SHOWS THE TOP-1 ACCURACY RESULTS FOR ALL THE ANALYZED NETWORKS ON FA S H I O N-MNIST TEST SET, CIFAR-10 TEST SET, AND ILSVRC-2012 VALIDATION SET.
CONVEX HULL-BASED AND AFFINE HULL-BASED ACTIVATIONS ACHIEVING TOP-1 ACCURACY RESULTS GREATER THAN THEIR CORRESPONDING BASE ACTIVATION FUNCTIONS ARE HIGHLIGHTED IN BOLDFACE.
THE BEST RESULT FOR EACH NETWORK/DATASET IS SHADED.
TABLE II Activation id ReLU tanh LReLU conv({id, ReLU}) conv({id, tanh}) conv({tanh, ReLU}) conv({id, ReLU, tanh}) aﬀ({id, ReLU}) aﬀ({id, tanh}) aﬀ({tanh, ReLU}) aﬀ({id, ReLU, tanh}) Fashion-MNIST CIFAR-10 ILSVRC-2012 LeNet-5 KerasNet LeNet-5 KerasNet ResNet-56 AlexNet 90.50% 91.06% 92.33% 91.03% 91.87% 92.36% 92.56% 92.21% 92.83% 92.65% 93.02% 92.80% 90.51% 90.79% 93.43% 91.13% 92.39% 93.64% 92.04% 92.94% 93.37% 94.41% 93.48% 94.41% 75.27% 80.37% 78.96% 80.94% 80.94% 79.45% 80.21% 80.48% 80.52% 78.97% 81.23% 80.13% 75.23% 79.97% 82.86% 80.07% 84.74% 78.53% 84.80% 85.21% 84.93% 86.05% 84.83% 87.45% 42.60% 89.18% 82.65% 89.38% 90.51% 86.46% 88.31% 89.60% 88.92% 82.97% 89.44% 88.62% 56.75% 57.03% 57.54% 58.13% 56.55% 58.48% 60.04% 57.20% [6] K.
Jarrett, K.
Kavukcuoglu, Y.
LeCun et al., “What is the best multi- stage architecture for object recognition?” in Computer Vision, 2009 IEEE 12th International Conference on.
IEEE, 2009, pp.
2146–2153.
[7] X.
Glorot, A.
Bordes, and Y.
Bengio, “Deep sparse rectiﬁer neural networks,” in Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics, 2011, pp.
315–323.
[8] A.
L.
Maas, A.
Y.
Hannun, and A.
Y.
Ng, “Rectiﬁer Nonlinearities Improve Neural Network Acoustic Models,” in ICML Workshop on Deep Learning for Audio, Speech and Language Processing, 2013.
[9] K.
Konda, R.
Memisevic, and D.
Krueger, “Zero-bias autoencoders and the beneﬁts of co-adapting features,” arXiv preprint arXiv:1402.3337, 2014.
[10] K.
He, X.
Zhang, S.
Ren, and J.
Sun, “Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation,” in Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV), ser.
ICCV ’15.
Washington, DC, USA: IEEE Computer Society, 2015, pp.
1026–1034.
[Online].
Available: http://dx.doi.org/10.1109/ICCV.2015.123 [11] C.
Dugas, Y.
Bengio, F.
B´elisle, C.
Nadeau, and R.
Garcia, “Incorpo- rating second-order functional knowledge for better option pricing,” in Advances in neural information processing systems, 2001, pp.
472–478.
[12] I.
J.
Goodfellow, D.
Warde-Farley, M.
Mirza, A.
Courville, and Y.
Ben- gio, “Maxout networks,” in Proceedings of the 30th International Conference on International Conference on Machine Learning-Volume 28.
JMLR.
org, 2013, pp.
III–1319.
[13] W.
Sun, F.
Su, and L.
Wang, “Improving deep neural networks with multilayer maxout networks,” 2014 IEEE Visual Communications and Image Processing Conference, VCIP 2014, pp.
334–337, dec 2015.
[14] C.
Gulcehre, K.
Cho, R.
Pascanu, and Y.
Bengio, “Learned-norm pooling for deep feedforward and recurrent neural networks,” in Joint European Conference on Machine Learning and Knowledge Discovery in Databases.
Springer, 2014, pp.
530–546.
[15] D.-A.
Clevert, T.
Unterthiner, and S.
Hochreiter, “Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs),” Pro- ceedings of ICLR 2016, nov 2016.
[16] G.
Klambauer, T.
Unterthiner, A.
Mayr, and S.
Hochreiter, “Self- normalizing neural networks,” in Advances in neural information pro- cessing systems, 2017.
[17] Y.
Liu and X.
Yao, “Evolutionary design of artiﬁcial neural networks with different nodes,” in Evolutionary Computation, 1996., Proceedings of IEEE International Conference on.
IEEE, 1996, pp.
670–675.
[18] R.
Poli, “Parallel distributed genetic programming, new ideas in opti- mization,” 1999.
[19] D.
Weingaertner, V.
K.
Tatai, R.
R.
Gudwin, and F.
J.
Von Zuben, “Hierarchical evolution of heterogeneous neural networks,” in Evolu- tionary Computation, 2002.
CEC’02.
Proceedings of the 2002 Congress on, vol.
2.
IEEE, 2002, pp.
1775–1780.
[20] A.
J.
Turner and J.
F.
Miller, “Cartesian genetic programming encoded artiﬁcial neural networks: a comparison using three benchmarks,” in Proceedings of the 15th annual conference on Genetic and evolutionary computation.
ACM, 2013, pp.
1005–1012.
[21] M.
M.
Khan, A.
M.
Ahmad, G.
M.
Khan, and J.
F.
Miller, “Fast learning neural networks using cartesian genetic programming,” Neurocomputing, vol.
121, pp.
274–289, 2013.
[22] A.
J.
Turner and J.
F.
Miller, “Neuroevolution: evolving heterogeneous artiﬁcial neural networks,” Evolutionary Intelligence, vol.
7, no.
3, pp.
135–154, 2014.
[23] F.
Agostinelli, M.
Hoffman, P.
Sadowski, and P.
Baldi, “Learning acti- vation functions to improve deep neural networks,” in ICLR Workshop, 2015.
[24] M.
Lin, Q.
Chen, and S.
Yan, “Network in network,” arXiv preprint arXiv:1312.4400, 2013.
[25] G.
Cybenko, “Approximation by superpositions of a sigmoidal function,” Mathematics of Control, Signals, and Systems (MCSS), vol.
2, no.
4, pp.
303–314, 1989.
[26] H.
H.
Aghdam and E.
J.
Heravi, Guide to Convolutional Neural Networks.
Springer, 2017.
[27] D.
Sussillo and L.
Abbott, “Random walk initialization for training very deep feedforward networks,” arXiv preprint arXiv:1412.6558, 2014.
[28] H.
Xiao, K.
Rasul, and R.
Vollgraf.
(2017) Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms.
[29] A.
Krizhevsky and G.
Hinton, “Learning multiple layers of features from tiny images,” Technical report, University of Toronto, Tech.
Rep., 2009.
[30] O.
Russakovsky, J.
Deng, H.
Su, J.
Krause, S.
Satheesh, S.
Ma, Z.
Huang, A.
Karpathy, A.
Khosla, M.
Bernstein, A.
C.
Berg, and L.
Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,” International Journal of Computer Vision (IJCV), vol.
115, no.
3, pp.
211–252, 2015.
[31] A.
Krizhevsky, I.
Sutskever, and G.
E.
Hinton, “Imagenet classiﬁcation with deep convolutional neural networks,” in Advances in neural infor- mation processing systems, 2012, pp.
1097–1105.
[32] Y.
LeCun, L.
Bottou, Y.
Bengio, and P.
Haffner, “Gradient-based learning applied to document recognition,” Proceedings of the IEEE, vol.
86, no.
11, pp.
2278–2324, 1998.
[33] F.
Chollet et al., “Keras,” https://github.com/fchollet/keras, 2015.
[34] K.
He, X.
Zhang, S.
Ren, and J.
Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp.
770–778.
Irving, M.
[35] M.
Abadi, A.
Agarwal, P.
Barham, E.
Brevdo, Z.
Chen, C.
Citro, G.
S.
Corrado, A.
Davis, J.
Dean, M.
Devin, S.
Ghemawat, I.
Goodfellow, A.
Harp, G.
Isard, Y.
Jia, R.
Jozefowicz, L.
Kaiser, M.
Kudlur, J.
Levenberg, D.
Man´e, R.
Monga, S.
Moore, D.
Murray, C.
Olah, M.
Schuster, J.
Shlens, B.
Steiner, I.
Sutskever, K.
Talwar, P.
Tucker, V.
Vanhoucke, V.
Vasudevan, F.
Vi´egas, O.
Vinyals, P.
Warden, M.
Wattenberg, M.
Wicke, Y.
Yu, and X.
Zheng, “TensorFlow: Large-scale machine learning on heterogeneous systems,” 2015, software available from tensorﬂow.org.
[Online].
Available: https://www.tensorﬂow.org/
Clustering is inherently subjective [Caruana et al., 2006; von Luxburg et al., 2014]: a single dataset can often be clus- tered in multiple ways, and different users may prefer differ- ent clusterings.
This subjectivity is one of the motivations for constraint-based (or semi-supervised) clustering [Wagstaff et al., 2001; Bilenko et al., 2004].
Methods in this setting ex- ploit background knowledge to obtain clusterings that are more aligned with the user’s preferences.
Often, this knowl- edge is given in the form of pairwise constraints that indicate whether two instances should be in the same cluster (a must- link constraint) or not (a cannot-link constraint) [Wagstaff et al., 2001].
In traditional constraint-based clustering systems the set of constraints is assumed to be given a priori, and in practice, the pairs that are queried are often selected ran- domly.
In contrast, in active clustering [Basu et al., 2004a; Mallapragada et al., 2008; Xiong et al., 2014] it is the method itself that decides which pairs to query.
Typically, active methods query pairs that are more informative than random ones, which improves clustering quality.
This work introduces an active constraint-based clustering method named Constraint-Based Repeated Aggregation (CO- BRA).
It differs from existing approaches in several ways.
First, it aims to maximally exploit constraint transitivity and entailment [Wagstaff et al., 2001], two properties that allow deriving additional constraints from a given set of constraints.
By doing this, the actual number of pairwise constraints that COBRA works with is typically much larger than the num- ber of pairwise constraints that are queried from the user.
Secondly, COBRA introduces the assumption that there ex- ist small local regions in the data that are grouped together in all potential clusterings.
To clarify this, consider the exam- ple of clustering images of people taking different poses (e.g. facing left or right).
There are at least two natural cluster- ing targets for this data: one might want to cluster based on identity or pose.
In an appropriate feature space, one expects images that agree on both criteria (i.e. of a single person, tak- ing a single pose) to be close.
There is no need to consider all of these instances individually, as they will end up in the same cluster for each of the two targets that the user might be interested in.
COBRA aims to group such instances into a super-instance, such that they can be treated jointly in the clustering process.
Doing so substantially reduces the num- ber of pairwise queries.
Thirdly, COBRA is an inherently ac- tive method: the constraints are selected during the execution of the algorithm itself, as constraint selection and algorithm execution are intertwined.
In contrast, existing approaches consist of a component that selects constraints and another one that uses them during clustering.
Our experiments show that COBRA outperforms state-of- the-art active clustering methods in terms of both clustering quality and runtime.
Furthermore, it has the distinct advan- tage that it does not require knowing the number of clusters beforehand, as the competitors do.
In many realistic clus- tering scenarios this number is not known, and running an algorithm with the wrong number of clusters often results in a signiﬁcant decrease in clustering quality.
We discuss related work on (active) constraint-based clus- tering in section 2.
In section 3 we elaborate the key ideas in COBRA and describe the method in more detail.
We present our experimental evaluation in section 4, and conclude in sec- tion 5.
2 Background and Related Work Most existing constraint-based methods are extensions of well-known unsupervised clustering algorithms.
They use the constraints either in an adapted clustering procedure [Wagstaff et al., 2001; Rangapuram and Hein, 2012; Wang et al., 2014], to learn a similarity metric [Xing et al., 2003; Davis et al., 2007], or both [Bilenko et al., 2004; Basu et al., 2004b].
Constraint-based extensions have been developed for most clustering algorithms, including K-means [Wagstaff et al., 2001; Bilenko et al., 2004], spectral clustering [Rangapu- ram and Hein, 2012; Wang et al., 2014], DBSCAN [Lelis and Sander, 2009; Campello et al., 2013] and EM [Shental et al., 2004].
Basu et al.
[2004a] introduce a strategy to select the most informative constraints, prior to performing a single run of a constraint-based clustering algorithm.
They show that ac- tive constraint selection can improve clustering performance.
Several selection strategies have been proposed since [Mal- lapragada et al., 2008; Xu et al., 2005; Xiong et al., 2014], most of which are based on the classic approach of uncer- tainty sampling.
As COBRA also chooses which pairs to query, we consider it to be an active method, and in our ex- periments we compare to other methods in this setting.
Note, however, that COBRA is quite different from existing meth- ods in active constrained clustering and active learning in general.
For COBRA, selecting which pairs to query is inher- ent to the clustering procedure, whereas for most other meth- ods the selection strategy is optional and considered to be a separate component.
In its core, COBRA is related to hierarchical clustering as it follows the same procedure of sequentially trying to merge the two closest clusters.
Constraints have been used in hier- archical clustering before but in different ways.
Davidson et al.
[2009], for example, present an algorithm to ﬁnd a cluster- ing hierarchy that is consistent with a given set of constraints.
Nogueira et al.
[2012] propose an active semi-supervised hi- erarchical clustering algorithm that is based on merge con- ﬁdence.
Also related to ours is the work of Campello et al.
[2013], who have developed a framework to extract from a given hierarchy a ﬂat clustering that is consistent with a given set of constraints.
The key difference is that COBRA starts from super-instances, i.e. small clusters produced by K- means, and that each merging decision is settled by a pairwise constraint.
The idea of working with a small number of representatives (in our case the super-instance medoids, as will be discussed in section 3) instead of all individual instances has been used before, but for very different purposes.
For example, Yan et al.
[2009] use it to speed up unsupervised spectral clustering, whereas we use it to reduce the number of pairwise queries.
3 Constraint-Based Repeated Aggregation Constraint-based clustering algorithms aim to produce a clus- tering of a dataset that resembles an unknown target cluster- ing Y as close as possible.
The algorithm cannot query the cluster labels in Y directly, but can query the relation between pairs of instances.
A must-link constraint is obtained if the instances have the same cluster label in Y , a cannot-link con- straint otherwise.
The aim is to produce a clustering that is close to the target clustering Y , using as few pairwise queries as possible.
Several strategies can be used to exploit constraints in clus- tering.
Figure 1 illustrates some of them.
The most naive strategy is to query all pairwise relations, and construct clus- ters as sets of instances that are connected by a must-link con- straint (Figure 1a).
Though this is clearly not a good strategy in any scenario, it allows us to formulate a baseline for further improvements.
It always results in a perfect clustering, but at a very high cost: for a dataset of N instances,(cid:0)N (cid:1) questions are asked.
The previous strategy can be improved by exploiting con- straint transitivity and entailment, two well known prop- erties in constraint-based clustering [Wagstaff et al., 2001; Bilenko et al., 2004].
Must-link constraints are known to be transitive: must-link(A, B) ∧ must-link(B, C) ⇒ must-link(A, C), whereas cannot-link constraints have an entailment property: must-link(A, B) ∧ cannot-link(B, C) ⇒ cannot-link(A, C).
Thus, every time a constraint is queried and added to the set of constraints, transitivity and entailment can be applied to ex- pand the set.
This strategy is illustrated in Figure 1b.
Exploit- ing transitivity and entailment signiﬁcantly reduces the num- ber of pairwise queries needed to obtain a clustering, without a loss in clustering quality.
The order in which constraints are queried strongly inﬂu- ences the number of constraints that can be derived.
In gen- eral, it is better to obtain must-link constraints early on.
That way, any future query involving one of the instances con- nected by a must-link also applies to all others.
This suggests querying the closest pairs ﬁrst, as they are more likely to be- long to the same cluster and hence be connected by a must- link constraint.
This is strategy is illustrated in Figure 1c.
The previous strategies all obtain a perfect clustering, but require a high number of queries which makes them inap- plicable for reasonably sized datasets.
To further reduce the number of queries, COBRA groups similar instances into super-instances and only clusters their representatives, i.e. medoids.
It assumes that all instances within a super-instance are connected by a must-link constraint.
While clustering the medoids, COBRA uses both previously discussed strategies of querying the closest pairs and exploiting transitivity and entailment.
This strategy, illustrated in Figure 1d, results in a substantial reduction of the number of queries.
It does not always result in a perfect clustering as it is possible that the instances within a particular super-instance should not be grouped together w.r.t. the target clustering.
Table 1 illustrates to what extent each of the improvements described above reduces the number of queries.
We perform an extensive evaluation of the quality of the clusterings that COBRA produces in section 4.
3.1 Algorithmic Description After presenting the main motivations for each step of CO- BRA, we now give a more detailed description in Algorithm (a) (b) (c) (d) Figure 1: Illustration of different querying strategies.
Colors indicate the desired clustering.
Solid green lines indicate must-link, red ones cannot-link constraints.
Dashed lines indicate derived constraints with the same color code.
The number next to the solid line indicates the ordering of queried constraints, whereas the number next to the dashed line indicates the constraint number from which the constraint was derived.
(a) Querying all 10 constraints (b) Exploiting entailment and transitivity results in querying 6 constraints.
(c) Querying the closest pairs ﬁrst results in 4 constraints.
(d) Introducing super-instances (dashed ellipses) results in only 2 queries.
dataset # instances total # pairs # queries # queries # queries iris wine dermatology hepatitis ecoli opdigits389 147 178 358 112 336 1151 10731 15753 63903 6216 56280 661825 transitivity −−−−−→ entailment 409 457 1660 173 1343 3380 closest pairs −−−−−−→ ﬁrst super−−−−→ instances 155 187 379 140 440 1164 34 35 42 39 51 54 Table 1: This table shows the total number of pairwise relations in several datasets (column 3), as well the number of pairwise queries that is required when exploiting transitivity and entailment (column 4, these numbers are averages of runs for 5 random orderings of pairwise queries), and additionally querying the closest pairs ﬁrst (column 5).
The last column shows the number of pairwise queries (averaged over 5 runs) when COBRA is run with 25 super-instances.
(cid:83) disjoint subsets {C1, .
.
.
,CNC} of S ((cid:83) i=1, xi ∈ Rm the instances to be clus- 1.
Let X = {xi}N tered.
The set of instances X is ﬁrst over-clustered into NS disjoint subsets, namely super-instances {Si}NS i=1 , such that i Si = X .
This over-clustering is obtained by running K- means with a K that may be signiﬁcantly larger than the ac- tual number of clusters.
Each super-instance Si is represented by its medoid si, forming a set of super-instance representa- tives S = {s1, .
.
.
, sNS}.
All pairwise queries that are per- formed are between these super-instance representatives.
The goal of COBRA is now to cluster these representatives into Each cluster Ci is a set of super-instance representatives, but conceptually contains all the points in the corresponding super-instances.
The number of clusters NC is unknown a priori and will be determined during the clustering procedure.
Initially, there are NS clusters, each containing a single super- instance representative, as shown on line 2 in Algorithm 1.
These clusters are merged (if necessary) in the subsequent while loop.
i Ci = S).
In each iteration, COBRA ﬁrst sorts all pairs of clusters between which there is no cannot-link constraint (line 6).
The distance between clusters, by which the pairs are sorted, is deﬁned as follows (as in single-linkage clustering): d(C1,C2) = min s1∈C1,s2∈C2 (cid:107)s1 − s2(cid:107)2 (1) Next, COBRA loops over the pairs of clusters and checks whether they should be merged.
It starts by selecting the clos- est pair of super-instance representatives of the clusters (line 9), and asks whether they should be in the same cluster (line 10).
If this is the case, the clusters are merged (lines 11 and 12), and the while-loop is restarted (as the set of clusters is changed).
If that is not the case, the pair of representatives is added to the set of cannot-link constraints (line 16), and the inner loop continues by inspecting the next pair of clusters.
The execution stops when all clusters are complete and no merge is to be done anymore.
Number of Super-instances and Number of Queries The exact number of queries COBRA needs depends on the extent to which querying the closest pairs ﬁrst leads to must- link constraints and on the actual number of clusters.
It is thus difﬁcult to determine it before the execution.
However, an estimate in terms of a lower and upper bound can be posed: with NS the number of super-instances, and NC the number of clusters in the target clustering.
In the worst case, CO- BRA will need to query all pairwise relations between super- NS − NC +(cid:0)NC instances, which requires(cid:0)NS tice, COBRA typically needs much less than(cid:0)NS BRA needs exactly NS−NC +(cid:0)NC into the NC clusters, followed by an additional(cid:0)NC (cid:1) (cid:46) # queries ≤(cid:0)NS (cid:1) queries.
This happens if there (cid:1) queries.
If (cid:1) queries.
NS−NC must- (cid:1) queries (i) super-instances are perfectly homogeneous w.r.t. the tar- get clustering and (ii) the distances between must-link pairs are smaller than the distances between cannot-link pairs, CO- link constraints are needed to merge the NS super-instances is a cannot-link between each pair of super-instances.
In prac- to ensure nothing can be merged anymore.
This formula is in- applicable in practice as the number of clusters is not known (cid:1) Algorithm 1 COBRA Require: X : a dataset, NS: the number of super-instances, i=1 = K-means(X , NS) Ensure: C, a clustering of D 1: {Si}NS 2: ∀i = 1, .
.
.
, NS : Ci = {si} 3: CL = ∅ 4: mergeHappened = T rue 5: while mergeHappened do 6: P = {C1,C2 : (cid:64)x ∈ C1, y ∈ C2 : (x, y) ∈ CL}, ordered by d(C1,C2) for C1,C2 in P do sa, sb = arg mins1∈C1,s2∈C2(cid:107)s1 − s2(cid:107)2 if must-link(sa, sb) then 7: mergeHappened = F alse 8: 9: 10: 11: 12: 13: 14: 15: 16: end if 17: end for 18: 19: end while 20: return C C1 = C1 ∪ C2 C = C \ {C2} mergeHappened = T rue break CL = CL ∪ {(sa, sb}) else beforehand, and thus serves as a means to understand the number of queries that might be needed.
Figure 2 compares this lower bound to the actual number of queries that was needed by COBRA for 21 clustering tasks (these will be de- scribed in more detail in section 4).
For most tasks, the actual number of pairwise queries is relatively close to the lower bound.
It is possible to get a smaller number of queries than that suggested by the lower bound: this happens if a single super-instance contains the instances of two actual clusters, (cid:1) factor inaccurate.
rendering the(cid:0)NC Figure 2: The estimated lower bound on the number of queries vs.
the number of queries needed by COBRA (averaged over 5 CV folds).
Each dot corresponds to one of the 21 clustering tasks.
4 Experimental Evaluation In this section, we discuss the experimental evaluation of CO- BRA.
Existing Constraint-based Algorithms We compare COBRA to the following state-of-the-art constraint-based clustering algorithms: • MPCKMeans [Bilenko et al., 2004] is a hybrid constraint-based extension of K-means: it uses metric learning and an adapted clustering procedure combining the within-cluster sum of squares with the cost of violat- ing constraints in the objective.
We use the implementa- tion that is available in the WekaUT package1.
• Constrained Spectral Clustering (COSC) [Rangapuram and Hein, 2012] is based on spectral clustering, but opti- mizes for a modiﬁed objective that also takes constraint violation into account.
We use the code provided by the authors on their web page2.
It is important to note that, in contrast to COBRA, COSC and MPCKMeans require the number of clusters as an input parameter.
In our experiments, the true number of clusters is provided to these algorithms.
In many clustering applica- tions, however, this number is typically not known before- hand.
Thus, COSC and MPCKMeans are at an advantage.
Active Selection Strategies Each of these algorithms is combined with the following two active selection strategies: • MinMax [Mallapragada et al., 2008] starts with an ex- ploration phase in which K (the number of clusters, assumed to be known in advance) neighborhoods with cannot-links between them are sought.
In the subse- quent consolidation phase these neighborhoods are ex- panded by selecting the most uncertain instances and de- termining their neighborhood membership by means of pairwise constraints.
We set the width parameter of the Gaussian kernel to the 20th percentile of the distribution of pairwise Euclidean distances, as in [Mallapragada et al., 2008].
• NPU [Xiong et al., 2014] is also based on the concept of neighborhoods, but in contrast to MinMax it is an it- erative method: the data is clustered multiple times, and each clustering is used to determine the next set of pairs to query.
Xiong et al.
[2014] show that NPU typically outperforms MinMax in terms of clustering quality, at the cost of increased runtime.
Datasets We experiment with 15 UCI datasets: iris, wine, der- matology, hepatitis, glass, ionosphere, optdigits389, ecoli, breast-cancer-wisconsin, segmentation, column 2C, parkin- sons, spambase, sonar and yeast.
Most of these datasets have been used in earlier work on constraint-based cluster- ing [Bilenko et al., 2004; Xiong et al., 2014].
Optdigits389 contains digits 3, 8 and 9 of the UCI handwritten digits data 1http://www.cs.utexas.edu/users/ml/risc/code/ 2http://www.ml.uni-saarland.de/code/cosc/cosc.htm 050100150200250300approximate lower bound on # queries050100150200250300# queries25 super-instances50 super-instances100 super-instancesTable 2: Wins and losses aggregated over all 21 clustering tasks.
After each win (loss) count, we report the average margin by which COBRA wins (loses).
For win counts marked with an asterisk, the differences are signiﬁcant according to the Wilcoxon test with p < 0.05.
COBRA vs.
MPCKM-MinMax COBRA vs.
MPCKM-NPU COBRA vs.
COSC-MinMax COBRA vs.
COSC-NPU 25 super-instances win loss 50 super-instances loss win 100 super-instances loss win 13 (0.14) 11 (0.16) 15* (0.21) 15* (0.20) 8 (0.12) 10 (0.11) 6 (0.05) 6 (0.04) 13 (0.16) 17* (0.12) 16* (0.21) 14* (0.23) 8 (0.09) 4 (0.09) 5 (0.06) 7 (0.04) 12 (0.19) 12 (0.17) 14* (0.21) 13* (0.23) 9 (0.05) 9 (0.06) 7 (0.04) 8 (0.03) [Bilenko et al., 2004; Mallapragada et al., 2008].
Duplicate instances were removed for all these datasets, and all data was normalized between 0 and 1.
We also perform exper- iments on the CMU faces dataset, which contains 624 im- ages of 20 persons taking different poses, with different ex- pressions, with and without sunglasses.
Hence, this dataset has 4 target clusterings: identity, pose, expression and sun- glasses.
We extract a 2048-value feature vector for each im- age by running it through the pre-trained Inception-V3 net- work [Szegedy et al., 2015] and storing the output of the second last layer.
Finally, we also cluster the 20 news- groups text data.
For this dataset, we consider two tasks: clustering documents from 3 newsgroups on related topics (the target clusters are comp.graphics, comp.os.ms-windows and comp.windows.x, as in [Basu et al., 2004a; Mallapra- gada et al., 2008]), and clustering documents from 3 news- groups on very different topics (alt.atheism, rec.sport.baseball and sci.space, as in [Basu et al., 2004a; Mallapragada et al., 2008]).
We ﬁrst extract tf-idf features, and next apply latent semantic indexing (as in [Mallapragada et al., 2008]) to re- duce the dimensionality to 10.
This brings the total to 17 datasets, for which 21 clustering tasks are deﬁned (15 UCI datasets with a single target, CMU faces with 4 targets, and 2 subsets of the 20 newsgroups data).
Experimental Methodology We use a cross-validation procedure that is highly similar to the ones used in e.g. [Basu et al., 2004a] and [Mallapragada et al., 2008].
In each of 5 folds, 20% of the instances are set aside as the test set.
The clustering algorithm is then run on the entire dataset, but can only query pairwise constraints for which both instances are in the training set.
To evaluate the quality of the resulting clustering, we compute the Ad- justed Rand index (ARI, [Hubert and Arabie, 1985]) only on the instances in the test set.
The ARI measures the similarity between two clusterings, in this case between the one pro- duced by the constraint-based clustering algorithm and the one indicated by the class labels.
An ARI of 0 means that the clustering is not better than random, 1 indicates a perfect clustering.
The ﬁnal score for an algorithm for a particular dataset is computed as the average ARI over the 5 folds.
The exact number of pairwise queries is not known before- hand for COBRA, but more super-instances generally results in more queries.
To evaluate COBRA with varying amounts of user input, we run it with 25, 50 and 100 super-instances.
For each fold, we execute the following steps: • Run COBRA and count how many constraints it needs.
• Run the competitors with the same number of con- • Evaluate the resulting clusterings by computing the ARI straints.
on the test set.
To make sure that COBRA only queries pairs of which both instances are in the training set, the medoid of a super- instance is calculated based on only the training instances in that super-instance (and as such, test instances are never queried during clustering).
In the rare event that a super- instance contains only test instances, it is merged with the nearest super-instance that does contain training instances.
For the MinMax and NPU selection strategies, pairs involv- ing an instance from the test set are simply excluded from selection.
Results The results over all 21 clustering tasks are summarized in Ta- bles 2 and 3.
Table 2 reports wins and losses against each of the 4 competitors.
It shows that COBRA tends to pro- duce better clusterings than its competitors.
The difference with COSC is signiﬁcant according to the Wilcoxon test with p < 0.05, whereas the difference with MPCKMeans is not.
Table 3 shows the average ranks for COBRA and its competi- tors.
The Friedman aligned rank test [Hodges and Lehmann, 1962], which has more power than the Friedman test when the number of algorithms under comparison is low [Garca et al., 2010], indicates that for 50 and 100 super-instances, the differences in rank between COBRA and all competitors are signiﬁcant, using a posthoc Holm test with p < 0.05.
Table 3: For each dataset, all algorithms are ranked from 1 (best) to 5 (worst).
This table shows the average ranks for 25, 50 and 100 super-instances.
Algorithms for which the difference with COBRA is signiﬁcant according to the Friedman aligned rank test and a post- hoc Holm test with p < 0.05 are marked with an asterisk.
25 super-instances 50 super-instances 100 super-instances COBRA MPCK-NPU MPCK-MM COSC-MM* COSC-NPU* COBRA 2.43 MPCK-MM* 3.00 COSC-NPU* 3.07 3.12 COSC-MM* 3.40 MPCK-NPU* COBRA 2.14 3.00 COSC-NPU* 3.02 MPCK-NPU* MPCK-MM* 3.26 3.57 COSC-MM* 2.52 2.98 3.00 3.19 3.31 Running Competitors with Different Numbers of Queries In the previous experiments, the competitors are run with the same number of queries that COBRA required, as for COBRA this cannot be ﬁxed beforehand.
One might won- der whether this constitutes an advantage for COBRA, and whether the above conclusions also hold when competitors (a) (b) (c) (d) Figure 3: Comparing the clustering qualities for COBRA and its competitors for a wider range of numbers of constraints.
For COBRA, each black marker shows the average number of questions that COBRA required for a particular number of super-instances, and its average ARI (over 5-fold CV).
We only show the results for the MinMax selection strategy, but the conclusions that are drawn also hold for NPU.
The number of super-instances for COBRA is 25, 50, 100 (as in the experiments before), 150, 200, 250, .
.
.
can be run with different numbers of constraints.
To answer this question, we run COBRA with a wider range of super- instances, and its competitors with more numbers of con- straints.
Figure 3 shows the results for 4 datasets, but the con- clusions that are drawn here also hold for the others.
A ﬁrst conclusion is that for the datasets for which COBRA outper- forms its competitors in the experiments discussed above, it also does so for larger numbers of constraints (e.g. in Figures 3a and 3b).
As such, the results discussed in the previous sec- tion are representative.
Secondly, clustering quality quickly plateaus for many datasets (e.g. in Figure 3d) .
This is espe- cially true for MPCKMeans, which can be explained by its strong spherical bias.
In contrast, for several datasets both COBRA and COSC produce increasingly better clusterings as more constraints are given (e.g. in Figures 3a and 3c).
Selecting the Right Number of Clusters COBRA does not require specifying the number of clusters K beforehand.
Most often, it produces the correct K (i.e. the one indicated by the class labels).
When it does not, the K it ﬁnds is very close to the correct one.
In contrast, COSC and MPCKMeans do require specifying K.
In the experiments discussed above, both of them were given the correct K.
We have found experimentally that, in the majority of cases, run- ning them with a different K reduces clustering quality, and often by a signiﬁcant amount.
Occasionally a different K improves results, but when this was the case it was typically only by a small margin.
These results are omitted from the paper due to lack of space.
Runtime Figure 4 compares the runtimes of COBRA to those of the competitors.
COBRA consists of two steps: constructing super-instances, and grouping them together to form clusters.
Both are fast, as K-means is used for the ﬁrst step and the sec- ond step is only applied to the small set of super-instances.
As can be seen in Figure 4, the runtimes of MPCKMeans- MinMax are comparable to those of COBRA, which is not surprising as it is built on K-means.
In contrast, COSC- MinMax is signiﬁcantly more expensive, as it is built on spec- tral clustering.
When used with the NPU selection strategy, both MPCKMeans and COSC become much slower, as NPU requires several runs of the clustering algorithm.
Figure 4: Each dot shows the runtime (averaged over 5 CV folds) of a method on one of the datasets.
This ﬁgure shows the results for COBRA with 50 super-instances, the ﬁgures for 25 and 100 super- instances are comparable.
COBRA and MPCKMeans-MinMax are highly scalable and always ﬁnish in under 10 seconds.
5 Conclusion We have introduced COBRA, an active constraint-based clus- tering method.
Unlike other methods, it is not built as an ex- tension of an existing unsupervised algorithm.
Instead, CO- BRA is inherently constraint-based.
With its selection strat- egy, it aims to maximally exploit constraint transitivity and entailment.
Our experiments show that COBRA outperforms the state of the art in terms of clustering quality and runtime, even when the other methods have the advantage of being given the right number of clusters.
In future work, we will investigate whether an appropriate number of super-instances can be determined automatically, e.g. by incrementally reﬁn- ing them when necessary.
Acknowledgements Toon Van Craenendonck is supported by the Agency for In- novation by Science and Technology in Flanders (IWT).
Se- bastijan Dumanˇci´c is supported by Research Fund KU Leu- ven (GOA/13/010).
References [Basu et al., 2004a] Sugato Basu, Arindam Banerjee, and Raymond J.
Mooney.
Active semi-supervision for pair- In Proceedings of the 2004 wise constrained clustering.
100200300400500600700800# queries0.00.20.40.60.81.0ARIsegmentationMPCKMeans-MinMaxCOSC-MinMaxCOBRA50100150200250300350400# queries0.00.20.40.60.81.0ARIglassMPCKMeans-MinMaxCOSC-MinMaxCOBRA50100150200250300350400# queries0.00.20.40.60.81.0ARIfaces eyesMPCKMeans-MinMaxCOSC-MinMaxCOBRA50100150200250300# queries0.00.20.40.60.81.0ARIbreast-cancer-wisconsinMPCKMeans-MinMaxCOSC-MinMaxCOBRAdatasets10-1100101102103104105runtime (s)MPCKMeans-MinMaxMPCKMeans-NPUCOSC-MinMaxCOSC-NPUCOBRASIAM International Conference on Data Mining (SDM- 04), April 2004.
cal Clustering Using Conﬁdence-Based Active Learning.
(1):139–153, 2012.
[Rangapuram and Hein, 2012] Syama S.
Rangapuram and Matthias Hein.
Constrained 1-spectral clustering.
In Proc.
of the 15th International Conference on Artiﬁcial Intelli- gence and Statistics, 2012.
[Shental et al., 2004] Noam Shental, Aharon Bar-Hillel, Tomer Hertz, and Daphna Weinshall.
Computing Gaussian mixture models with EM using equivalence constraints.
In In Advances in Neural Information Processing Systems 16, 2004.
[Szegedy et al., 2015] Christian Szegedy, Vincent Van- houcke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.
Rethinking the inception architecture for computer vision.
CoRR, abs/1512.00567, 2015.
[von Luxburg et al., 2014] Ulrike von Luxburg, Robert C.
Williamson, and Isabelle Guyon.
Clustering: Science or Art?
In Workshop on Unsupervised Learning and Trans- fer Learning, JMLR Workshop and Conference Proceed- ings 27, 2014.
[Wagstaff et al., 2001] Kiri Wagstaff, Claire Cardie, Seth Rogers, and Stefan Schroedl.
Constrained K-means Clus- tering with Background Knowledge.
In Proc.
of the Eigh- teenth International Conference on Machine Learning, pages 577–584, 2001.
[Wang et al., 2014] Xiang Wang, Buyue Qian, and Ian Davidson.
On constrained spectral clustering and its appli- cations.
Data Mining and Knowledge Discovery, 28(1):1– 30, 2014.
[Xing et al., 2003] Eric P.
Xing, Andrew Y.
Ng, Michael I.
Jordan, and Stuart Russell.
Distance metric learning, with application to clustering with side-information.
In Advances in Neural Information Processing Systems 15, pages 505–512, 2003.
[Xiong et al., 2014] Sicheng Xiong, Javad Azimi, and Xi- aoli Z.
Fern.
Active learning of constraints for semi- supervised clustering.
IEEE Transactions on Knowledge and Data Engineering, 26(1):43–54, 2014.
[Xu et al., 2005] Qianjun Xu, Marie desJardins, and Kiri L.
Wagstaff.
Active Constrained Clustering by Examining Spectral Eigenvectors, pages 294–307.
Springer Berlin Heidelberg, Berlin, Heidelberg, 2005.
[Yan et al., 2009] Donghui Yan, Ling Huang, and Michael I.
In Pro- Jordan.
Fast approximate spectral clustering.
ceedings of the 15th ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining, KDD ’09, pages 907–916, New York, NY, USA, 2009.
ACM.
[Basu et al., 2004b] Sugato Basu, Misha Bilenko, and Ray- mond J.
Mooney.
A probabilistic framework for semi- In Proceedings of the 10th ACM supervised clustering.
SIGKDD International Conference on Knowledge Discov- ery and Data Mining (KDD-2004), page 5968, January 2004.
[Bilenko et al., 2004] Mikhail Bilenko, Sugato Basu, and Raymond J.
Mooney.
Integrating constraints and metric learning in semi-supervised clustering.
In Proc.
of 21st In- ternational Conference on Machine Learning, pages 81– 88, July 2004.
[Campello et al., 2013] Ricardo J.
G.
B.
Campello, Davoud Moulavi, Arthur Zimek, and J¨org Sander.
A framework for semi-supervised and unsupervised optimal extraction of clusters from hierarchies.
Data Mining and Knowledge Discovery, 27(3):344–371, 2013.
[Caruana et al., 2006] Rich Caruana, Mohamed Elhawary, In Proc.
of the In- and Nam Nguyen.
Meta clustering.
ternational Conference on Data Mining, 2006.
[Davidson and Ravi, 2009] I Davidson and SS Ravi.
Us- ing instance-level constraints in agglomerative hierarchi- cal clustering: theoretical and empirical results.
Data min- ing and knowledge discovery, pages 1–30, 2009.
[Davis et al., 2007] Jason V.
Davis, Brian Kulis, Prateek Jain, Suvrit Sra, and Inderjit S.
Dhillon.
Information- theoretic metric learning.
In Proceedings of the 24th In- ternational Conference on Machine Learning, ICML ’07, pages 209–216, New York, NY, USA, 2007.
ACM.
[Garca et al., 2010] Salvador Garca, Alberto Fernndez, Julin Luengo, and Francisco Herrera.
Advanced nonparamet- ric tests for multiple comparisons in the design of ex- periments in computational intelligence and data mining: Information Sciences, Experimental analysis of power.
180(10):2044 – 2064, 2010.
Special Issue on Intelligent Distributed Information Systems.
[Hodges and Lehmann, 1962] J.
L.
Hodges and E.
L.
Lehmann.
Rank methods for combination of independent experiments in analysis of variance.
The Annals of Math- ematical Statistics, 33(2):482–497, 1962.
[Hubert and Arabie, 1985] Lawrence Hubert and Phipps Arabie.
Comparing partitions.
Journal of Classiﬁcation, 2(1):193–218, 1985.
[Lelis and Sander, 2009] Levi Lelis and J¨org Sander.
Semi- supervised density-based clustering.
In 2009 Ninth IEEE International Conference on Data Mining, pages 842–847, Dec 2009.
[Mallapragada et al., 2008] Pavan K.
Mallapragada, Rong Jin, and Anil K.
Jain.
Active query selection for semi- supervised clustering.
In Proc.
of the 19th International Conference on Pattern Recognition, 2008.
[Nogueira et al., 2012] Bruno M Nogueira, M Jorge, and Solange O Rezende.
HCAC : Semi-supervised Hierarchi-
As the results of the 2016 US Presidential Election were ﬁnalized, it was clear that the majority of pro- fessional polling groups, many of whom had pre- dicted the probability of a Clinton victory to be well over 90%, were had signiﬁcantly overestimated their predictions ([1, 2, 3])).
While it could be argued that the underlying models were correct and that the par- ticular result was just a very rare event, post-mortem analyses have revealed ﬂaws that led to large pre- diction biases.
According to multiple post-election analyses, it was concluded that a leading cause of er- ror in the majority of election forecasting models was a lack of correlation between individual states pre- dictions ([4, 5, 6, 7]).
Uncorrelated models, though much simpler to build and train, cannot capture the more complex behavior of a fully-connected system.
To capture these higher-order relationships, a fully- connected graphical model would be ideal.
While these models are more powerful, practical roadblocks have prevented their widespread adoption due to dif- ﬁculties in implementation using classical computa- tion.
However, recent studies have shown that quan- tum computing is a competitive alternative when generating such networks ([8, 9, 10, 11, 12]).
Quantum machine learning (QML) is a blossom- ing ﬁeld.
As summarized in the comprehensive re- view of QML in [13], machine learning applications from support vector machines to principal compo- nent analysis are being reimagined on various quan- tum devices.
One of the most exciting research ar- eas within QML is deep quantum learning, which fo- cuses on the impact quantum devices and algorithms can have on classical deep neural networks (DNNs) and graphical models.
A particular class of DNNs is the Boltzmann machine (BM), which is an incred- ibly powerful fully-connected graphical model that can be trained to learn arbitrary probability distri- butions.
A downside of these networks is that BMs are incredibly costly to train, a fact that has limited their practical application.
This large computational training cost has drawn attention to the implemen- tation of quantum computation to help train such networks.
BMs realized on quantum devices (partic- ularly adiabatic quantum devices such as those pro- duced by D-Wave Systems ([9]) may possess inherent beneﬁts compared to those realized on classical de- vices.
Research groups have realized various forms of BMs (fully-connected BM, restricted Boltzmann machines (RBMs), and Helmholtz machines) trained using quantum computation, and this research has shown quantum computation can be used to eﬀec- tively train neural networks for image recognition tasks ([9, 11, 12]).
In this work, we will leverage the power of adia- batic quantum computation to eﬃciently train fully- connected BMs for the novel purpose of election modeling.
Additionally, we have systematically ex- plored a number of the assumptions underlying the approach of using adiabatic quantum computers (AQC) to model BMs, and we have demonstrated that for most systems of interest (such as this one) the approach does appear to be valid.
We believe the methods proposed in this paper could bring an in- teresting new factor into the conversation of election forecasting at large, one in which quantum compu- tation could play a future role.
2 Methodology 2.1 Modeling Boltzmann Machines with AQC In this work, we will be generating fully-connected BMs trained using a D-Wave 2X 1152 qubit quan- tum device using the general method described in [9].
While the methodology for training both RBMs and BMs using a D-Wave machine have been laid out in previous papers ([9, 11]), we will brieﬂy re- view the logic and methodology here.
A BM is a fully-connected graph of (N ) binary units (neurons).
These neurons can be either “visi- ble” (directly model some aspect of a data distribu- tion) or “hidden” (not tied to any particular aspect of the data distribution and used only for captur- ing features from the data distribution).
Each net- work has 2N possible states, and the probability of sampling a particular state s = (s1, ..., sN ) from the model is p(s) = (1) e−E(s) wherein Z it the well-known partition function and E is an energy function deﬁned as E(s) = −(cid:88) bisi − (cid:88) si∈s si,sj∈s Wijsisj, (2) wherein b represent the linear “bias” on each unit and W represents the “weight” of the coupling be- tween two units (b and W will be referred to as our “model parameters”).
To properly train the net- work, we need to adjust the model parameters so that the model distribution produced by repeatedly sampling the model is as close as possible to the un- derlying data distribution; more precisely, we want to maximize the log-likelihood, L, of the data dis- tribution.
To calculate the model parameters for maximizing L, we use the familiar gradient descent method and learning rate η to get model parameter update equations (3) ∆Wij = (cid:17) (cid:16)(cid:104)sisj(cid:105)D − (cid:104)sisj(cid:105)M (cid:16)(cid:104)si(cid:105)D − (cid:104)si(cid:105)M (cid:17) ∆bi = (4) In equations 3 and 4, the values inside (cid:104)∗(cid:105) represent expectation values over the data (D) and model (M ) distributions.
The model would be perfectly trained if ﬁrst ((cid:104)si(cid:105)) and second ((cid:104)sisj(cid:105)) order moments were identical for both the data and model distribution.
To properly adjust the model parameters we need to calculate expectation values over the model it- self.
Getting the “true” values would thus require a calculation for all 2N possible states of the model, which is clearly intractable as the system size in- creases.
These particular calculations are where the use of quantum computation is ideal, and we see a potential for a speedup in our overall training algo- rithm.
The quantum devices produced by D-Wave Sys- In tems perform a quantum annealing algorithm.
theory, this algorithm can leverage quantum eﬀects to take an initial quantum system that is in a well- known ground state and transform this into a ﬁnal Hamiltonian - one in which the system should still be in the ground state (assuming the annealing pro- cess was slow enough, as well as many other factors discussed elsewhere [14]).
The original use case of this algorithm lies in the fact that if you can prop- erly map a computationally diﬃcult problem of in- terest into this ﬁnal Hamiltonian, then measuring the ground state of this ﬁnal Hamiltonian should produce the optimal solution to the original prob- lem.
However, this use case has been elusive at scale; as shown in the research of [15], which focuses on fundamental limitations of quantum devices at ﬁ- nite temperatures.
Even taking some optimistic as- sumptions (such as perfect, instant thermalization), as the system (problem) size grows, the probabil- ity of measuring the optimal (ground) state of the system decreases exponentially.
Rather than return- ing the ground state solution, repeatedly measuring from such a device returns a Boltzmann distribution of energies.
While these results prove challenging for using such hardware for optimization, it presents an ideal opportunity for training BMs. At a high level, in- stead of trying to calculate (cid:104)si(cid:105)M and (cid:104)sisj(cid:105)M di- rectly, we can instead map our network onto the D- Wave quantum device.
By obtaining a ﬁnite num- ber of samples from the hardware device, the goal is to generate better approximations of (cid:104)si(cid:105)M and (cid:104)sisj(cid:105)M than classical heuristics.
This method seems all the more natural as the form of the Hamiltonian H of the D-Wave device is H(S) = − (cid:88) hiSi − (cid:88) JijSiSj, (5) Si∈S Si,Sj∈S which is the same functional form as the BM en- ergy in equation 2.
In this equation, S is the vector of qubit spin states, hi are the bias terms on each qubit, and Jij are the (anti)ferromagnetic couplings between the qubits.
By mapping the model param- eters of a BM to the hardware parameters of a D- Wave device and making a set of measurements of the device, one can use these measurements to con- struct approximations of (cid:104)si(cid:105)M and (cid:104)sisj(cid:105)M .
Ad- vantages have been shown in using fully-connected BM on QC devices because using the methods of [9], the eﬀective temperature of the device does not have to be taken into account.
Equation 1 is a spe- cial case of the more general representation; rather than raising the exponential to −E(s), the more gen- eral expression raises its to −E(s)β, where β is the “eﬀective” temperature of the system (parameter re- lated to temperature of the system).
If β = 1 then we arrive at equation 1, but in general when using a quantum device one will not know the eﬀective temperature beforehand, which can experience large ﬂuctuations between measurements.
While this can be problematic for training RBMs using quantum annealers, and requires diﬀerent techniques to esti- mate this parameter ([10, 8, 11]), fully connected BMs do not require these additional calculations for eﬀective training ([9]).
Though the structure of the BM graph to embed on the device is fully connected, we are in practice limited by the graph structure physically realized in the hardware.
The adiabatic quantum device we used for this research was a D-Wave 2X, which has 1,152 qubits connected in a Chimera graph archi- tecture consisting of 8 qubit cells arranged as K4,4 bipartite graphs.
The qubits within each cells are cross connected, and each cell is connected to four adjacent cells (with the exception of cells along the boundaries) as shown in Figure 1.
To properly map the BM energy function of (2) to the device, the graph minor-embedding problem must be solved; we need a hardware embedding which uses a chain of multiple physical qubits to realize a single logical qubit in the problem Hamiltonian of (5).
Using the same method as [9], we ﬁnd embeddings using the embedding heuristic provided by D-Wave’s API and resolve discrepancies of the qubit chains using ma- jority vote (a post-processing step of the measure- ments).
Figure 1: Four bipartite cells of a Chimera graph architecture showing how the cells interconnect.
In each cell there are four horizontal and four verti- cal qubits, colored alternately blue and burnt orange.
Within cells, where two qubits overlap they are cou- pled by means of a Josephson junction, indicated by green circles.
Each qubit can be coupled to two ad- dtional qubits from adjacent cells, also by means of Josephson junctions, indicated by light green circles.
2.2 Quantum Boltzmann Machines for Forecasting Elections The methodology outlined in section 2.1 lays out our approach for training fully-connected BM using a D-Wave quantum device.
This section will detail our procedure for implementing these networks to forecast elections.
In this research, we study the US Presidential election, and each binary unit in the BM represents a single US state.
The winner of a particular election simulation is determined by the candidate with the most electoral college votes.
Each US state has a particular number of electoral college votes to award to a candidate (2 + an integer which scales as a function of the state’s population), and these votes are awarded entirely to one candi- date (winner-take-all).
We assert that each sample returned from a fully-connected BM will in eﬀect be a simulation of a US presidential election.
Each sample from the BM returns a binary vector, where each entry in the vector corresponds to the vot- ing results of a particular US state.
These individ- ual state voting results are mapped to a particular candidate/party (i.e., 1 = Democrat, 0 = Republi- can).
To determine the election simulation outcome, we weight each of these US state outcomes accord- ing to their net weight in the national vote (each state’s electoral votes).
The winner of each simu- lation (sample) is determined by the sum of each party’s overall national vote, which is calculated us- ing the binary results (from sample) and national weight (electoral votes) for each state.
The goal is to train the BM which is being sam- pled from so that the ﬁrst and second order mo- ment terms of the model distribution approach those of the data distribution.
This training process has already been discussed in section 2.1, and in this section we will expand on how we determined the ﬁrst and second order moment terms for the data distribution of our election model.
The ﬁrst order moment terms represent the probability that each state will vote for a particular candidate.
As an example, if we believe that there is a 80% chance that the Democratic candidate wins Colorado, then the ﬁrst order moment for the binary variable as- signed to represent Colorado should be equal to 0.8. To determine all the ﬁrst order moments for each state in our model, we use the current time-averaged polling results made publicly available on FiveThir- tyEight ([16]).
We obtain a projected vote share for both candidates for each day that data is avail- able (6 months before, and including, November 8th 2016).
These projected vote shares are then used as input to a sigmoidal model (same model used by FiveThirtyEight [17]) which rightly assumes that elections are stochastic, and that the result for each state/country follows a probabilistic rather than de- terministic curve based on the popular-vote / pro- jected vote share margin.
This method for convert- ing a popular-vote margin to a probability of victory is shown for a particular state in Figure 2.
Figure 2: Model for interpreting projected vote share to probabilities.
A.
Plot of Maine’s polling projec- tions over time, where the solid lines are the time- averaged projected vote share for both candidates, and the dashed lines are the resulting probabilities of victory for each candidate, calculated using the best ﬁt function shown in B.
Given the underlying projected vote shares for each state and the best ﬁt function shown in Fig- ure 2B, calculating the ﬁrst order moment terms for each state is straightforward.
Calculating the second order terms, the eﬀective “correlations” be- tween states, is much more diﬃcult.
These correla- tions express the likelihood that two states will end up with the same (or diﬀerent) voting result in an election.
States that vote the same are more cor- related (higher second order moment), and states that don’t have a lower correlation (lower second or- der moment).
These correlations are inﬂuenced by a plethora of demographic (race, age, education), geo- graphic, and additional factors.
Professional model- ers (such as those at FiveThirtyEight) have com- plex methodologies for determining these correla- tions; however, a rigorous analysis of these corre- lations is outside the scope of this particular work.
We used data obtained publicly, which is suﬃcient to validate the general approach of our model.
To calculate the second order moment terms, we use one source of data and make two particular ansatz.
The data source we use is presidential elec- tion results from the last 11 US Presidential elec- tions.
This data contains the date and results per state for each election.
Our ﬁrst ansatz is that if we consider two states, these states should have higher correlations (second order terms) if they had voted similarly in previous elections.
This correlation is agnostic towards which candidate was voted for in each of these previous elections; the only important factor for the two states in question is if the vote was for the same candidate or a diﬀerent one.
The second ansatz is that in terms of weighting previ- ous election results, more recent elections are more relevant.
This means recent elections increase corre- lations between two states more than those that hap- pened longer ago.
We assume a linear relationship between time and importance.
The raw correlations (cid:104)sisj(cid:105)Draw between states i and j are calculated as follows: n=1:11 n(cid:0)2injn − in − jn + 1(cid:1) (cid:80) (cid:104)sisj(cid:105)Draw = (cid:80) n=1:11 n (6) wherein n refers to a particular election year in the set [1968, 1972, ...
, 2008, 2012] (higher n is more re- cent) and in and jn are the results for election n for both respective states.
We then enforce a hard con- straint that second order correlations should never contradict ﬁrst order moments (which are calculated directly from current polling data).
This is accom- plished by ﬁnally calculating the second order mo- ments between states i and j as (cid:104)sisj(cid:105)D = (cid:104)sisj(cid:105)Draw min((cid:104)si(cid:105)D,(cid:104)sj(cid:105)D).
(7) We have now a methodology for mapping election forecasting models, speciﬁcally the 2016 US Pres- idential election, to BM by deﬁning mathematical models for calculating both ﬁrst and second order data distribution terms.
In the following section, we validate that this approach holds true for small, nonexistent countries and then attempt to simulate a “real time” forecast for 2016 Presidential election using quantum-trained BMs. 2.3 Caveats and limitations In section 2.1 and 2.2, we reviewed the methodology for training fully-connected BM with a D-Wave ma- chine as well as describe our approach for mapping election forecast models to the (to be trained) BM.
While this work uses this approach as described, a few caveats and limitations deserve some additional attention here.
2.3.1 Hardware constraints The hardware size limitations of the D-Wave 2X does not allow us to fully embed a 50 state model as well as the DC province, which are the 51 fundamen- tal voting blocks for the US Presidential election.
Using the virtual full-yield Chimera capability of- fered by D-Wave, which uses a combination of the quantum device hardware in tandem with classical software for simulating missing qubits and couplers, we were able to embed 49 states and omitted DC and Maryland.
These were omitted because they were ranked as the most “deﬁnite” by model stan- dards (both were approaching 100% likelihood to vote Democrat), as well as geographically adjacent.
2.3.2 Assert that all states are winner-take- all While the US Presidential election is winner-take- all at the state level, two states are exceptions to this rule: Maine and Nebraska.
Instead of winner- take-all, these states award delegates by district.
To simplify the model and ﬁt within the hardware con- straints, we treat these states as winner-take-all re- gardless.
This decision was made for three reasons.
First, the primary purpose of this paper is to val- idate the overall election methodology for model- ing such elections using QC-trained neural networks; such state speciﬁc rules fall outside this scope of this work.
Second, these states have small weight (electoral college votes) in the broader election, so treating them as winner-take-all has a reduced ef- fect compared to a much larger state under the same voting system.
Third, in the future we could treat the provinces as individual states themselves, each awarding electoral college votes with a winner-take- all system.
However, due to our limitation already expressed in the previous issue, this experiment will be left to future studies on a larger quantum device.
2.3.3 Inability to model national errors in same model The strength in models with correlations as de- scribed here is simple; they can account for a form of error that is inaccessible to independent models.
However, there are also two other primary types of error that we would want our ﬁnal model to consider: national and state-speciﬁc errors.
Both of these er- ror arise from the fact that polling is never perfect; there are always voting blocks that are under or over- represented based on the types of people that are both polled and respond to the poll.
National error arises from the fact that all states could have system- atically missed a particular type of voting block in a similar, characteristic manner.
This leads to errors that aﬀect each state in a similar way.
State-speciﬁc error is the same concept, but on a state-by-state level.
The latter (as discussed in future results sec- tion) can be addressed naturally by the nature of the QC-training algorithm; however the former cannot.
Since we wish to emulate the best possible model, in- corporating state-speciﬁc, state, and national errors, we choose to create a meta-model which aggregates results from several diﬀerent models build on the as- sumption of diﬀerent national error.
In our case, we take 25 equally-spaced samples from a t-distribution with 10 degrees of freedom; this is the same distri- bution and degrees of freedom used for national and state-speciﬁc error used by FiveThirtyEight ([16]).
These points deﬁne the national errors we use to train 25 diﬀerent models.
For instance, one national model may have a national error favoring Clinton by 1% point, while another might favor Trump by 1%.
These national errors are ultimately incorpo- rated into the ﬁrst order moment terms for each state, leading to models which are slightly biased towards either candidate to a relative degree.
The average of these 25 models is calculated after simu- lating each model independently, and weighing the results of each by the probability of occurrence for each national error.
2.3.4 Limited time windows of D-Wave ac- cess In a production environment, it would be ideal to produce updates to forecasts daily (or sometimes several times a day) for particularly high-proﬁle elec- tions.
These updates occur as new polls come in, changing the particular predictions for each state, and thus ultimately the national results.
Applying our proposed methodology could assuredly be used for these purposes, but a limiting factor for simu- lating this daily forecast over 6 months is access to the D-Wave quantum device.
Due to limited access time to run experiments on the D-Wave device, and that we have to simulate multiple error models (as explained in section 2.3.3), we choose to only model every 2 weeks of data rather than daily.
This allowed us to generate an appropriate number of simulations for these days across all national error models.
3 Results 3.1 Eﬀect of Chain Length As mentioned in section 2.1, using a well posed Hamiltonian and the right environmental variables, an AQC should theoretically be capable of ﬁnding the ground state of the Hamiltonian.
In practice, thermal ﬂuctuations, environmental interactions, in- suﬃciently short annealing times, and a plethora of other physical and engineering challenges result in a low probability of measuring the ground state, but instead some other low energy (potentially near- optimal) state.
This is especially true for larger Hamiltonian systems, as shown in [15]; for ﬁnite- temperature AQCs, as the system size increases, the probability of measuring a non-optimal low energy state approaches 1.
In contrast, if we wish to use an AQC as a sampling engine for sampling from BMs, we can potentially face a diﬀerent set of ob- stacles when using small embeddings (system sizes).
In terms of using an AQC for machine learning pur- poses, returning a distribution of low energy solu- tions rather than the optimal conﬁguration drives the learning process, as the ﬁrst and second order statistics of these measurements determines the up- date terms for the model.
At small physical em- bedding size, the probability of measuring the op- timal state increases signiﬁcantly, and at very con- cise embedding sizes the probability of measuring the ground state energy can approach 1.
This be- havior is that of a Hopﬁeld network, which is a BM at T = 0.
Unlike a BM, the Hopﬁeld net- work can only return ground state energy solutions.
This would imply that our training algorithm as de- scribed in section 2 would not work for such a sys- tem.
Each time model updates in equations 3 and 4 are made, the energy function of equation 2 changes as well.
This new energy function would lead to new ground state solutions, which in turn could have completely diﬀerent model parameters.
While train- ing a BM leads to model updates “smoothly” guid- ing the model parameters ((cid:104)si(cid:105)M , (cid:104)sisj(cid:105)M ) towards the data distribution ((cid:104)si(cid:105)D, (cid:104)sisj(cid:105)D), slight changes in the model parameters of a Hopﬁeld network can completely change the ground state solutions, lead- ing to chaotic model parameter updates.
Figure 3: Training results for arbitrary Boltzmann machines realized on D-Wave device using (A) 1x and (B) 2x embedding qubit embedding chains.
In each subplot, the horizontal red lines are the respec- tive target values.
One potential way to mitigate these eﬀects is to deliberately increase the size of the qubit chains for embedding the problem.
For optimization purposes, the goal would be to ﬁnd the minimum chain length for embedding the problem Hamiltonian onto the physical device.
By keeping the embedding chains minimal, the system size is as small as possible which increases the chance of measuring an optimal ground state.
The opposite should be true as well: the more we increase the chain lengths for embedding the logical qubits onto the hardware, the more low energy states become available to system, increasing the probability that the system will transition away from the ground state during the annealing process.
This should enable one to properly train BM for any number of nodes, given that the qubit chain lengths are long enough.
By validating this assertion, we can argue that our approach here for using AQC for realizing BMs for election modeling could be ap- plied to any sized system, as well as validate that our particular experiments are in a regime where proper learning is possible.
To test this hypothesis, we performed experiments with fully connected graphs of size 5 through 9, em- bedded with three diﬀerent embeddings of various chain lengths, and studied how well we could train the systems to reproduce activation probability dis- tributions deﬁned by graphs with arbitrary ﬁrst and second order terms.
For each run, the activation (hi) and correlation (Ji,j) probabilities were selected ran- domly such that for node i the activation probability hi ∈ (0, 1) and for two nodes i and j, the correlation probability Ji,j = ci,jhihj where ci,j is the correla- tion strength and ci,j ∈ (0, 1).
Three embeddings were used for each graph: a maximally concise embedding, an embedding de- rived from a maximally concise graph of twice as many nodes (denoted by “2x”), and an embedding derived from a maximally concise graph of three times as many nodes (denoted by “3x”).
The de- cision to approach the problem in this way was done because the D-Wave API has been set up for op- timization problems, and as such the hardware em- bedding functions in general attempt to return max- imally concise embeddings.
The 2x and 3x embed- dings returned from the API were for graphs of 2x and 3x the size of the problem graph, so they were reduced to the correct size by joining the physical qubits representing pairs (in the case of 2x) and triples (in the case of 3x) of logical qubits (usually represented by chains of physical qubits) into single logical qubits of chains of physical qubits 2x and 3x times as long as in the original embedding.
An ex- ample of training by the shortest (1x) and medium (2x) chain lengths are shown in Figure 3.
For all the subgraphs of Figure 3, the x-axis of each graph is the number of completed iterations in the training algorithm while the y-axis is the acti- vation probability when sampling the graph multi- ple times.
The graphs on the diagonal are single node activation probabilities(ﬁrst order moments) and the oﬀ-diagonal graphs are the two node cor- relations (second order moments).
In Figure 3A, the activation probabilities fail to converge to the desired values, indicating that the qubit chains are not allowing suﬃcient degrees of freedom for the sys- tem to model a Boltzmann machine.
However, using the same network but with the 2x embedding qubit chains, the network was able to converge over time towards the target ﬁrst and second order moment values.
In Table 1, we show the root mean squared error (RMSE) for training iterations 191-200 for dif- ferent QC-trained networks at diﬀerent embedding chain lengths.
Nodes Chain 1x 2x 3x 1x 2x 3x RMSE 0.437 ± 0.072 0.106 ± 0.036 0.060 ± 0.038 0.149 ± 0.101 0.038 ± 0.028 0.045 ± 0.034 Table 1: RMSE for QC-trained networks at diﬀer- ent embedding chain lengths.
As the networks grow larger, the chain length diﬀerences grows more negli- gible as chains are naturally getting longer to satisfy the embedding.
Given the current D-Wave qubit connectivity graph, as the problem size grows larger, the average embedding chain length similarly grows.
As most studies embed as large a problem as possible onto the device, this has naturally led to longer chain lengths in previous research.
As future hardware improvements are made and shorter qubit chains are feasible (through increased connectivity), it may be- come important to validate that the individual log- ical qubits are properly learning the respective tar- get terms.
The lengthening technique shown here could provide a simple but eﬃcient tool for ensuring Boltzmann-like behavior for all nodes in the logical graph without having to perturb any of the individ- ual energy scalings.
3.2 Modeling the Presidential Elec- tion The primary experiment we conducted was to at- tempt to simulate a “real-time election model fore- cast” using QC-trained Boltzmann machines.
Start- ing on the date 2016-06-08 and continuing until elec- tion day 2016-11-08, we trained multiple fully con- nected Boltzmann machines using the D-Wave adi- abatic device.
Figure 4: Summed error as a function of training iterations for one national error model.
The small spikes of error occur deep into the training process are simply an artifact of the updates in the ﬁrst or- der moments that happen at 2 week (25 iteration) intervals.
As mentioned in section 2.3.3 and 2.3.4, due to limitations we retrained the network every two weeks rather than daily, and used 25 diﬀerent networks to model diﬀerent national errors (derived from a t- distribution with 10 degrees of freedom).
The net- works starting on 2016-06-08 were initialized with small, random coeﬃcients and then subsequently trained for 150 iterations each.
Then, at each 2 week interval, the ﬁrst order moment terms were updated and trained for an additional 25 iterations.
The changes to the ﬁrst order moments were small, so fewer training iterations were necessary to converge to a stable summed error (sum of squared ﬁrst and second moment errors) across the networks.
This led to 400 total training iterations per national er- ror (150 for 2016-06-08 + 25*10 for the next ten Figure 5: Comparing 2016 Presidential election forecasting results from QC-trained methodology to those of FiveThirtyEight.
QC-trained networks each had a national bias towards Clinton (CB), Trump (TB), or neither candidate.
two-week updates).
An example of the training er- ror for a particular national error model is shown in Figure 4.
Knowing from section 3.1 that our qubit chains are suﬃciently long enough to learn properly, the training error results of Figure 4 are to be expected.
Similar plots were observed across all national error models, as this translates into nothing more than scaling the ﬁrst order moment terms.
We can then take the samples from these networks at diﬀerent iterations as our election forecasting simulation re- sults.
We choose to take samples for the last 10 iterations of each forecasting date (this would be it- erations 141-150 for 2016-06-08 and 16-25 for the next 10 forecasting dates).
This allows us to sam- ple from the network once it has reached a general steady-state in terms of summed training error.
As discussed in section 2.2, each logical qubit is mapped to a particular state and each sample is equivalent to an election forecast.
To determine which candidate “won” a particular sample, we simply map the qubit results back to the particular state it represents, and add each state’s number of electoral votes to the can- didate that state voted for in the sample.
Since the democratic candidate was the heavy favorite in most election models, we choose to express our forecast- ing results in terms of the probability of a Clinton victory.
In this way, each sample results in a partic- ular candidate winning (270 electoral votes or more) or losing (we combined ties into this category for simplicity, although an exact tie is very unlikely).
For our experiments, we took 1,000 samples from the D-Wave device at every iteration for each na- tional error model.
This gave us 10,000 samples for each national error model for each forecasting date (10 training iterations, 1,000 samples per training iteration).
The probability of a Clinton victory for each national error model was simply the sum of the individual samples which were won by Clinton divided by the number of total samples (10,000 in our case, per national error model and time step).
Finally, to get an average election forecast as a func- tion of time (shown in Figure 5), we calculated the weighted arithmetic mean across all national error models for each forecasting date.
The weights for each national error were deﬁned as the t-distribution probability density function of each national error (t-distribution with 10 degrees of freedom).
As evidenced in Figure 5, the QC-trained network results followed trends similar to the trends of the professional FiveThirtyEight forecasts.
The overall probabilities of the diﬀerent national error networks also follows naturally; networks that had a national error in favor of Clinton increased the probability of a Clinton victory, and networks with a national error in favor of Trump decreased the probability of a Clinton victory.
The largest apparent diﬀer- ence between the QC-trained models was the overall probability of a Clinton victory.
While the Aver- age result line of the QC-trained networks follows a very similar pattern to the predictions of FiveThir- tyEight, the QC-trained results are almost uniformly 20% lower.
This result in no way says the quan- tum methodology is “better”, but rather highlights the diﬀerences in the overall approach.
It is likely that these results are mostly dependent on the un- derlying diﬀerences in how we calculated our sec- ond order moments terms between the states.
An interesting future study would be to replicate the quantum-training protocol described here but using second order moments driven by demographic data of the individual state inhabitants.
An important factor forecasters also desire from a forecasting model is to know which states are the most important for predicting a particular outcome.
A straightforward approach is to generate a vector for each state (1 = state voted Democrat, 0 = state voted Republican) and a similar vector for the out- come (1 = Democratic victory, 0 = Republican vic- tory) of each simulation for the date November 8, 2016.
Then, we can calculate the Pearson correla- tion coeﬃcient between the two vectors and take the absolute value of these correlations.
Table 2 shows the 10 states with the highest and lowest correlation coeﬃcients.
As expected, states that leaned heavily Democratic or Republican had very low correlation coeﬃcients; regardless of the outcome of the election, states like Illinois and Nebraska were virtual locks for the Democratic and Republican candidates, respec- tively.
Similarly, the states with the highest corre- lation coeﬃcients contained many of the most con- tested states in the election.
FiveThirtyEight’s fore- casts have a similar “tipping-point chance” metric which they deﬁne as “the probability that a state will provide the decisive vote in the Electoral College” ([16]).
On election day, 7 out of the 10 states they ranked as the highest tipping-point chance states were similarly in the list of 10 most correlated states in Table 2 (the diﬀerences: FiveThirtyEight included Virginia, Minnesota, and Wisconsin, while ours in- cluded New Hampshire, Iowa, and Arizona).
Finally, we take into consideration the individual state errors observed in our QC-trained models.
As mentioned previously, modelers (such as FiveThir- tyEight) will apply some degree of noise for individ- State Ohio Florida Nevada New Hampshire Pennsylvania Iowa Michigan North Carolina Colorado Arizona Illinois Nebraska Alabama Oklahoma California West Virginia Delaware Oregon Idaho Arkansas Correlation coeﬃcients 0.204 0.163 0.178 0.167 0.155 0.152 0.145 0.137 0.130 0.127 0.002 0.004 0.005 0.006 0.008 0.008 0.008 0.009 0.015 0.016 Table 2: Pearson correlation coeﬃcients for the 10 states most (top) and least (bottom) correlated with the election forecasting results.
ual states, such as adding in state-speciﬁc error from sampling.
It would be useful to know how the natu- ral sampling of the quantum device during training lends itself to state-speciﬁc error.
For each iteration that we used for determining the national averages, we calculated the diﬀerence between the target (cid:104)s(cid:105)D and current model output (cid:104)s(cid:105)M .
If this diﬀerence is negative, this would be a state-speciﬁc error in fa- vor of the Democratic candidate, and vice versa a positive value translates to error beneﬁting the Re- publican candidate.
By taking all these errors per state, we can form diﬀerent state-speciﬁc error dis- tributions per state.
These distributions vary con- siderably, depending on the underlying target (cid:104)s(cid:105)D value, as evidenced in Figure 6.
At the extremes, we see that the error distribu- tions of states leaning heavily Democratic or Repub- lican are asymmetrical.
This occurs naturally, due to (cid:104)s(cid:105)M being bound between 0 and 1.
If (cid:104)s(cid:105)D ≈ 0 (state leaning heavily Republican), all error will be biased in the negative direction; similarly, states with (cid:104)s(cid:105)D ≈ 1 (state leaning heavily Democrat) will have positively-biased error distributions.
For swing states, we see a much more uniform spread of error, which shows that in the absence of bounds ((cid:104)s(cid:105)D ≈ 10 0 or 1), the error tends to be equally distributed.
Figure 6: Example distributions of state-speciﬁc error for states leaning heavily Republican (top), Democratic (bottom), or swing states (middle).
One interesting ﬁnding was that heavily-learning Democratic states seemed to have longer error dis- tribution tails compared to the heavily-leaning Re- publican states.
As seen in Figure 6, while almost all the probability mass of Alabama’s error distribution is contained within the range -5 to 5, a substantial amount of California’s error distribution falls out- side these bounds.
This phenomena can introduce an amount of bias in favor of one particular candi- date.
One potential mitigation technique for deal- ing with this issue is taking the average of multiple gauges ([11]), some of which could “ﬂip” the mea- surement value (ﬂip Republican = 1, Democrat = 0).
Additionally, some interesting new techniques using “shimming” ([18]) have been shown to reduce overall qubit error.
In future work, it would be an in- teresting topic to explore the evolution of individual logical qubit error distributions in QC-trained Boltz- mann machines by using shimming techniques (re- ducing error) or introducing random noise (increas- ing error) on a per-qubit basis.
4 Conclusions In this work, we have showed an initial implementa- tion of QC-trained Boltzmann machines, which can be employed for the diﬃcult task of sampling from correlated systems, an essential problem at the heart of many applications such as election forecast mod- eling.
We validated that this approach successfully learned various data distributions based on state polling results during the 2016 US Presidential cam- paign, and these QC-trained models generated fore- casts that had similar structural properties and out- comes compared to a best in class election modeling group.
While quantum computers and samplers are an emerging technology, we believe this application area could be of near-term interest.
This methodol- ogy could be an interesting technique to bring to the broader conversation of modeling in future election forecasts.
References [1] Survey ﬁnds Hillary Clinton has ‘more than 99% chance’ of winning election over Donald Trump, http://www.independent.co.uk/, 2016.
[2] The Huﬃngton Post Presidential Forecast, http://elections.huﬃngtonpost.com/2016/fore- cast/president, 2016.
[3] Key model predicts big election win for Clinton, http://money.cnn.com/2016/11/01/news/econ- omy/hillary-clinton-win-forecast-moodys- analytics/index.html, 2016.
[4] How the polls, including ours, missed trump’s http://www.reuters.com/article/us- victory, usa-election-polls-idUSKBN1343O6, 2016.
[5] Analysis: Early thoughts went wrong with http://www.wbur.org/politicker/2016/11/09/ pollster-early-thoughts, 2016.
election the about what polls, [6] The devil in the polling data, https://www.quantamagazine.org/why-nate- silver-sam-wang-and-everyone-else-were-wrong- part-2-20161111/, 2016.
11 [7] Epic fail, http://www.economist.com/news/ united-states/21710024-how-mid-sized-error- led-rash-bad-forecasts-epic-fail, 2016.
[8] J.
E.
Dorband, ArXiv e-prints (2016), 1606.06123.
[9] M.
Benedetti, J.
Realpe-G´omez, R.
Biswas, and A.
Perdomo-Ortiz, Phys.
Rev.
X 7, 041052 (2017).
[10] M.
Benedetti, J.
Realpe-G´omez, R.
Biswas, and A.
Perdomo-Ortiz, Phys.
Rev.
A (2016).
[11] S.
H.
Adachi and M.
P.
Henderson, ArXiv e- prints (2015), 1510.06356.
[12] M.
Benedetti, A.
Perdomo-Ortiz, 1708.09784.
J.
Realpe-G´omez, and ArXiv e-prints (2017), [13] J.
Biamonte et al., Nature 549, 195 (2017).
[14] E.
Farhi et al., Science 292, 472 (2001).
[15] T.
Albash, V.
Martin-Mayor, and I.
Hen, Phys.
Rev.
Lett.
119, 110502 (2017).
[16] A user’s guide to election ﬁvethirtyeight’s forecast, general 2016 https://ﬁvethirtyeight.com/features/a-users- guide-to-ﬁvethirtyeights-2016-general-election- forecast/, 2016.
[17] FiveThirtyEight, Election update: Clin- forecast, ton’s big lead means a steadier https://ﬁvethirtyeight.com/features/election- update-clintons-big-lead-means-a-steadier- forecast/, 2016.
[18] S.
Adachi, Qubit bias measurement and correc- tion, D-Wave Users Conference, 2017.
12
Despite the impressive (and sometimes even superhuman) accuracies of machine learning on diverse tasks such as object recognition (He et al., 2015), speech recognition (Xiong et al., 2016), and play- ing Go (Silver et al., 2016), classiﬁers still fail catastrophically in the presence of small imperceptible but adversarial perturbations (Szegedy et al., 2014; Goodfellow et al., 2015; Kurakin et al., 2016).
In addition to being an intriguing phenonemon, the existence of such “adversarial examples” exposes a serious vulnerability in current ML systems (Evtimov et al., 2017; Sharif et al., 2016; Carlini et al., 2016).
While formally deﬁning an “imperceptible” perturbation is difﬁcult, a commonly-used proxy is perturbations that are bounded in (cid:96)∞-norm (Goodfellow et al., 2015; Madry et al., 2017; Tramèr et al., 2017); we focus on this attack model in this paper, as even for this proxy it is not known how to construct high-performing image classiﬁers that are robust to perturbations.
While a proposed defense (classiﬁer) is often empirically shown to be successful against the set of attacks known at the time, new stronger attacks are subsequently discovered that render the defense useless.
For example, defensive distillation (Papernot et al., 2016c) and adversarial training against the Fast Gradient Sign Method (Goodfellow et al., 2015) were two defenses that were later shown to be ineffective against stronger attacks (Carlini & Wagner, 2016; Tramèr et al., 2017).
In order to break this arms race between attackers and defenders, we need to come up with defenses that are successful against all attacks within a certain class.
However, even computing the worst-case error for a given network against all adversarial pertur- bations in an (cid:96)∞-ball is computationally intractable.
One common approximation is to replace the worst-case loss with the loss from a given heuristic attack strategy, such as the Fast Gradient Sign Method (Goodfellow et al., 2015) or more powerful iterative methods (Carlini & Wagner, 2017a; Madry et al., 2017).
Adversarial training minimizes the loss with respect to these heuristics.
How- ever, this essentially minimizes a lower bound on the worst-case loss, which is problematic since points where the bound is loose have disproportionately lower objective values, which could lure and mislead an optimizer.
Indeed, while adversarial training often provides robustness against a speciﬁc attack, it often fails to generalize to new attacks, as described above.
Another approach is to compute the worst-case perturbation exactly using discrete optimization (Katz et al., 2017a; Carlini Under review as a conference paper at ICLR 2018 (a) (b) Figure 1: Illustration of the margin function f (x) for a simple two-layer network.
(a) Contours of f (x) in an (cid:96)∞ ball around x.
Sharp curvature near x renders a linear approximation highly inaccu- rate, and f (Afgsm(x)) obtained by maximising this approximation is much smaller than f (Aopt(x)).
(b) Vector ﬁeld for ∇f (x) with length of arrows proportional to (cid:107)∇f (x)(cid:107)1.
In our approach, we bound f (Aopt(x)) by bounding the maximum of (cid:107)∇f (˜x)(cid:107)1 over the neighborhood (green arrow).
In general, this could be very different from (cid:107)∇f (x)(cid:107)1 at just the point x (red arrow).
et al., 2017).
Currently, these approaches can take up to several hours or longer to compute the loss for a single example even for small networks with a few hundred hidden units.
Training a network would require performing this computation in the inner loop, which is infeasible.
In this paper, we introduce an approach that avoids both the inaccuracy of lower bounds and the intractability of exact computation, by computing an upper bound on the worst-case loss for neural networks with one hidden layer, based on a semideﬁnite relaxation that can be computed efﬁciently.
This upper bound serves as a certiﬁcate of robustness against all attacks for a given network and input.
Minimizing an upper bound is safer than minimizing a lower bound, because points where the bound is loose have disproportionately higher objective values, which the optimizer will tend to avoid.
Furthermore, our certiﬁcate of robustness, by virtue of being differentiable, is trainable—it can be optimized at training time jointly with the network, acting as a regularizer that encourages robustness against all (cid:96)∞ attacks.
In summary, we are the ﬁrst (along with the concurrent work of Kolter & Wong (2017)) to demon- strate a certiﬁable, trainable, and scalable method for defending against adversarial examples on two-layer networks.
We train a network on MNIST whose test error on clean data is 4.2%, and which comes with a certiﬁcate that no attack can misclassify more than 35% of the test examples using (cid:96)∞ perturbations of size  = 0.1. Notation.
For a vector z ∈ Rn, we use zi to denote the ith coordinate of z.
For a matrix Z ∈ Rm×n, Zi denotes the ith row.
For any activation function σ : R → R (e.g., sigmoid, ReLU) and a vector z ∈ Rn, σ(z) is a vector in Rn with σ(z)i = σ(zi) (non-linearity is applied element-wise).
We use B(z) to denote the (cid:96)∞ ball of radius  around z ∈ Rd: B(z) = {˜z | |˜zi − zi| ≤  for i = 1, 2, .
.
.
d}.
Finally, we denote the vector of all zeros by 0 and the vector of all ones by 1.
2 SETUP Score-based classiﬁers.
Our goal is to learn a mapping C : X → Y, where X = Rd is the input space (e.g., images) and Y = {1, .
.
.
, k} is the set of k class labels (e.g., object categories).
Assume C is driven by a scoring function f i : X → R for all classes i ∈ Y, where the classiﬁer chooses the class with the highest score: C(x) = arg maxi∈Y f i(x).
Also, deﬁne the pairwise margin f ij(x) def= f i(x) − f j(x) for every pair of classes (i, j).
Note that the classiﬁer outputs C(x) = i iff f ij(x) > 0 for all alternative classes j (cid:54)= i.
Normally, a classiﬁer is evaluated on the 0-1 loss (cid:96)(x, y) = I[C(x) (cid:54)= y].
This paper focuses on linear classiﬁers and neural networks with one hidden layer.
For lin- i x, where Wi is the ith row of the parameter matrix W ∈ Rk×d.
ear classiﬁers, f i(x) def= W (cid:62) Under review as a conference paper at ICLR 2018 For neural networks with one hidden layer consisting of m hidden units, the scoring function is i σ(W x), where W ∈ Rm×d and V ∈ Rk×m are the parameters of the ﬁrst and second f i(x) = V (cid:62) layer, respectively, and σ is a non-linear activation function applied elementwise (e.g., for ReLUs, σ(z) = max(z, 0)).
We will assume below that the gradients of σ are bounded: σ(cid:48)(z) ∈ [0, 1] for all z ∈ R; this is true for ReLUs, as well as for sigmoids (with the stronger bound σ(cid:48)(z) ∈ [0, 1 4 ]).
Attack model.
We are interested in classiﬁcation in the presence of an attacker A : X → X that takes a (test) input x and returns a perturbation ˜x.
We consider attackers A that can perturb each feature xi by at most  ≥ 0; formally, A(x) is required to lie in the (cid:96)∞ ball B(x) def= {˜x | (cid:107)˜x − x(cid:107)∞ ≤ }, which is the standard constraint ﬁrst proposed in Szegedy et al.
(2014).
Deﬁne the adversarial loss with respect to A as (cid:96)A(x, y) = I[C(A(x)) (cid:54)= y].
We assume the white-box setting, where the attacker A has full knowledge of C.
The optimal (untargeted) attack chooses the input that maximizes the pairwise margin of an incorrect class i over the correct class y: Aopt(x) = arg max˜x∈B(x) maxi f iy(˜x).
For a neural network, computing Aopt is a non-convex optimization problem; heuristics are typically employed, such as the Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2015), which perturbs x based on the gradient, or the Carlini-Wagner attack, which performs iterative optimization (Carlini & Wagner, 2017b).
3 CERTIFICATE ON THE ADVERSARIAL LOSS For ease of exposition, we ﬁrst consider binary classiﬁcation with classes Y = {1, 2}; the multiclass extension is discussed at the end of Section 3.3. Without loss of generality, assume the correct label for x is y = 2.
Simplifying notation, let f (x) = f 1(x) − f 2(x) be the margin of the incorrect class over the correct class.
Then Aopt(x) = arg max˜x∈B(x) f (˜x) is the optimal attack, which is successful if f (Aopt(x)) > 0.
Since f (Aopt(x)) is intractable to compute, we will try to upper bound it via a tractable relaxation.
In the rest of this section, we ﬁrst review a classic result in the simple case of linear networks where a tight upper bound is based on the (cid:96)1-norm of the weights (Section 3.1).
We then extend this to general classiﬁers, in which f (Aopt(x)) can be upper bounded using the maximum (cid:96)1-norm of the gradient at any point ˜x ∈ B(x) (Section 3.2).
For two-layer networks, this quantity is upper bounded by the optimal value fQP(x) of a non-convex quadratic program (QP) (Section 3.3), which in turn is upper bounded by the optimal value fSDP(x) of a semideﬁnite program (SDP).
The SDP is convex and can be computed exactly (which is important for obtainining actual certiﬁcates).
To summarize, we have the following chain of inequalities: f (A(x)) ≤ f (Aopt(x)) (3.2)≤ f (x) +  max ˜x∈B(x) (cid:107)∇f (˜x)(cid:107)1 (3.3)≤ fQP(x) (3.3)≤ fSDP(x), (1) which implies that the adversarial loss (cid:96)A(x) = I[f (A(x)) > 0] with respect to any attacker A is upper bounded by I[fSDP(x) > 0].
Note that for certain non-linearities such as ReLUs, ∇f (˜x) does not exist everywhere, but our analysis below holds as long as f is differentiable almost-everywhere.
3.1 LINEAR CLASSIFIERS For (binary) linear classiﬁers, we have f (x) = (W1 − W2)(cid:62)x, where W1, W2 ∈ Rd are the weight vectors for the two classes.
For any input ˜x ∈ B(x), Hölder’s inequality with (cid:107)x − ˜x(cid:107)∞ ≤  gives: (2) f (˜x) = f (x) + (W1 − W2)(cid:62)(˜x − x) ≤ f (x) + (cid:107)W1 − W2(cid:107)1.
Note that this bound is tight, obtained by taking Aopt(x)i = xi +  sign(W1i − W2i).
3.2 GENERAL CLASSIFIERS For more general classiﬁers, we cannot compute f (Aopt(x)) exactly, but motivated by the above, we can use the gradient to obtain a linear approximation g: f (˜x) ≈ g(˜x) def= f (x) + ∇f (x)(cid:62)(cid:0)˜x − x(cid:1) ≤ f (x) + (cid:107)∇f (x)(cid:107)1.
(3) Under review as a conference paper at ICLR 2018 Using this linear approximation to generate A(x) corresponds exactly to the Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2015).
However, f is only close to g when ˜x is very close to x, and people have observed the gradient masking phenomenon (Tramèr et al., 2017; Papernot et al., 2016b) in several proposed defenses that train against approximations like g, such as saturat- ing networks (Nayebi & Ganguli, 2017), distillation (Papernot et al., 2016c), and adversarial training (Goodfellow et al., 2015).
Speciﬁcally, defenses that try to minimize (cid:107)∇f (x)(cid:107)1 locally at the train- ing points result in loss surfaces that exhibit sharp curvature near those points, essentially rendering the linear approximation g(˜x) meaningless.
Some attacks (Carlini & Wagner, 2016; Tramèr et al., 2017) evade these defenses and witness a large f (Aopt(x)).
Figure 1a provides a simple illustration.
We propose an alternative approach: use integration to obtain an exact expression for f (˜x) in terms of the gradients along the line between x and ˜x: (cid:90) 1 ∇f(cid:0)t˜x + (1 − t)x(cid:1)(cid:62)(cid:0)˜x − x(cid:1)dt f (˜x) = f (x) + ≤ f (x) + max ˜x∈B(x) (cid:107)∇f (˜x)(cid:107)1, (4) where the inequality follows from the fact that t˜x + (1 − t)x ∈ B(x) for all t ∈ [0, 1].
The key difference between (4) and (3) is that we consider the gradients over the entire ball B(x) rather than only at x (Figure 1b).
However, computing the RHS of (4) is intractable in general.
For two-layer neural networks, this optimization has additional structure which we will exploit in the next section.
3.3 TWO-LAYER NEURAL NETWORKS We now unpack the upper bound (4) for two-layer neural networks.
Recall from Section 2 that f (x) = f 1(x) − f 2(x) = v(cid:62)σ(W x), where v def= V1 − V2 ∈ Rm is the difference in second-layer weights for the two classes.
Let us try to bound the norm of the gradient (cid:107)∇f (˜x)(cid:107)1 for ˜x ∈ B(x).
If we apply the chain rule, we see that the only dependence on ˜x is σ(cid:48)(W ˜x), the activation derivatives.
We now leverage our assumption that σ(cid:48)(z) ∈ [0, 1]m for all vectors z ∈ Rm, so that we can optimize over possible activation derivatives s ∈ [0, 1]m directly independent of x (note that there is potential looseness because not all such s need be obtainable via some ˜x ∈ B(x)).
Therefore: (cid:107)∇f (˜x)(cid:107)1 (i) = (cid:107)W (cid:62) diag(v)σ(cid:48)(W ˜x)(cid:107)1 (ii)≤ max (cid:107)W (cid:62) diag(v)s(cid:107)1 s∈[0,1]m (iii) (5) where (i) follows from the chain rule, (ii) uses the fact that σ has bounded derivatives σ(cid:48)(z) ∈ [0, 1], and (iii) follows from the identity (cid:107)z(cid:107)1 = maxt∈[−1,1]d t(cid:62)z.
(Note that for sigmoid networks, where σ(cid:48)(z) ∈ [0, 1 4.) Substituting the bound (5) into (4), we obtain an upper bound on the adversarial loss that we call fQP: 4 ], we can strengthen the above bound by a corresponding factor of 1 max s∈[0,1]m,t∈[−1,1]d t(cid:62)W (cid:62) diag(v)s, f (Aopt(x)) ≤ f (x) +  max ˜x∈B(x) (cid:107)∇f (˜x)(cid:107)1 ≤ f (x) +  max s∈[0,1]m,t∈[−1,1]d t(cid:62)W (cid:62) diag(v)s def= fQP(x).
(6) Unfortunately, (6) still involves a non-convex optimization problem (since W (cid:62) diag(v) is not neces- sarily negative semideﬁnite).
In fact, it is similar to the NP-hard MAXCUT problem, which requires maximizing x(cid:62)Lx over x ∈ [−1, 1]d for a graph with Laplacian matrix L.
While MAXCUT is NP-hard, it can be efﬁciently approximated, as shown by the celebrated semidef- inite programming relaxation for MAXCUT in Goemans & Williamson (1995).
We follow a similar approach here to obtain an upper bound on fQP(x).
First, to make our variables lie in [−1, 1]m instead of [0, 1]m, we reparametrize s to produce: max s∈[−1,1]m,t∈[−1,1]d t(cid:62)W (cid:62) diag(v)(1 + s).
(7) Under review as a conference paper at ICLR 2018 Next pack the variables into a vector y ∈ Rm+d+1 and the parameters into a matrix M: 1(cid:62)W (cid:62) diag(v) W (cid:62) diag(v) M (v, W ) def= y def= diag(v)(cid:62)W 1 diag(v)(cid:62)W (cid:34) 1 (cid:35)   .
(8) In terms of these new objects, our objective takes the form: max y∈[−1,1](m+d+1) (9) Note that every valid vector y ∈ [−1, +1]m+d+1 satisﬁes the constraints yy(cid:62) (cid:23) 0 and (yy(cid:62))jj = 1.
Deﬁning P = yy(cid:62), we obtain the following convex semideﬁnite relaxation of our problem: y∈[−1,1](m+d+1) max y(cid:62)M (v, W )y = (cid:104)M (v, W ), yy(cid:62)(cid:105).
fQP(x) ≤ fSDP(x) def= f (x) + max P(cid:23)0,diag(P )≤1 (cid:104)M (v, W ), P(cid:105) .
(10) Note that the optimization of the semideﬁnite program depends only on the weights v and W and does not depend on the inputs x, so it only needs to be computed once for a model (v, W ).
Semideﬁnite programs can be solved with off-the-shelf optimizers, although these optimizers are somewhat slow on large instances.
In Section 4 we propose a fast stochastic method for training, which only requires computing the top eigenvalue of a matrix.
Generalization to multiple classes.
The preceding arguments all generalize to the pairwise margins f ij, to give: f ij(A(x)) ≤ f ij SDP(x) def= f ij(x) + (11) M ij(V, W ) is deﬁned as in (9) with v = Vi − Vj. The adversarial loss of any attacker, (cid:96)A(x, y) = SDP(x) ≥ f iy(A(x)).
In particular, I[maxi(cid:54)=y f iy(A(x)) > 0], can be bounded using the fact that f iy (12) (cid:96)A(x, y) = 0 if maxi(cid:54)=y f iy P(cid:23)0,diag(P )≤1 max SDP(x) < 0.
(cid:10)M ij(V, W ), P(cid:11) , where 4 TRAINING THE CERTIFICATE In the previous section, we proposed an upper bound (12) on the loss (cid:96)A(x, y) of any attack A, based on the bound (11).
Normal training with some classiﬁcation loss (cid:96)cls(V, W ; xn, yn) like hinge loss or cross-entropy will encourage the pairwise margin f ij(x) to be large in magnitude, but won’t necessarily cause the second term in (11) involving M ij to be small.
A natural strategy is thus to use the following regularized objective given training examples (xn, yn), which pushes down on both terms: (cid:10)M ij(V, W ), P(cid:11) , (13) (W (cid:63), V (cid:63)) = arg min (cid:96)cls(V, W ; xn, yn) + λij max P(cid:23)0,diag(P )≤1 (cid:88) W,V (cid:88) i(cid:54)=j i(cid:54)=j where λij > 0 are the regularization hyperparameters.
However, computing the gradients of the above objective involves ﬁnding the optimal solution of a semideﬁnite program, which is slow.
Duality to the rescue.
Our computational burden is lifted by the beautiful theory of duality, which provides the following equivalence between the primal maximization problem over P , and a dual minimization problem over new variables c (see Section A for details): (cid:0)M ij(V, W ) − diag(cij)(cid:1) + 1(cid:62) max(c, 0), (cid:10)M ij(V, W ), P(cid:11) = min D · λ+ max P(cid:23)0,diag(P )≤1 cij∈RD max (14) max(B) is the maximum eigenvalue of B, or 0 if all eigenvalues are where D = (d + m + 1) and λ+ negative.
This dual formulation allows us to introduce additional dual variables cij ∈ RD that are optimized at the same time as the parameters V and W , resulting in an objective that can be trained efﬁciently using stochastic gradient methods.
The ﬁnal objective.
Using (14), we end up optimizing the following training objective: (W (cid:63), V (cid:63), c(cid:63)) = arg min W,V,c λij ·(cid:2)D · λ+ max(M ij(V, W ) − diag(cij)) + 1(cid:62) max(cij, 0)(cid:3) .
(cid:96)cls(V, W ; xn, yn) + (cid:88) (cid:88) (15) Under review as a conference paper at ICLR 2018 The objective in (15) can be optimized efﬁciently.
The most expensive operation is λ+ max, which requires computing the maximum eigenvector of the matrix M ij − diag(cij) in order to take gradi- ents.
This can be done efﬁciently using standard implementations of iterative methods like Lanczos.
Further implementation details (including tuning of λij) are presented in Section 6.3. Dual certiﬁcate of robustness.
The dual formulation is also useful because any value of the dual is an upper bound on the optimal value of the primal.
Speciﬁcally, if (W [t], V [t], c[t]) are the parameters at iteration t of training, then (cid:0)M ij(V [t], W [t]) − diag(c[t]ij)(cid:1) + 1(cid:62) max(c[t]ij, 0)(cid:3) , (cid:2)D · λ+ (16) f ij(A(x)) ≤ f (x) + max for any attack A.
As we train the network, we obtain a quick upper bound on the worst-case adver- sarial loss directly from the regularization loss, without having to optimize an SDP each time.
5 OTHER UPPER BOUNDS In Section 3, we described a function f ij SDP that yields an efﬁcient upper bound on the adversarial loss, which we obtained using convex relaxations.
One could consider other simple ways to upper bound the loss; we describe here two common ones based on the spectral and Frobenius norms.
Spectral bound: Note that v(cid:62)(σ(W ˜x) − σ(W x)) ≤ (cid:107)v(cid:107)2(cid:107)σ(W ˜x) − σ(W x)(cid:107)2 by Cauchy- Schwarz.
Moreover, since σ is contractive, (cid:107)σ(W ˜x) − σ(W x)(cid:107)2 ≤ (cid:107)W (˜x − x)(cid:107)2 ≤ (cid:107)W(cid:107)2(cid:107)˜x − x(cid:107)2 ≤  d(cid:107)W(cid:107)2, where (cid:107)W(cid:107)2 is the spectral norm (maximum singular value) of W .
This yields the following upper bound that we denote by fspectral: f ij(A(x)) ≤ f ij spectral(x) def= f ij(x) +  d(cid:107)W(cid:107)2(cid:107)Vi − Vj(cid:107)2.
(17) This measure of vulnerability to adversarial examples based on the spectral norms of the weights of each layer is considered in Szegedy et al.
(2014) and Cisse et al.
(2017).
Frobenius bound: For ease in training, often the Frobenius norm is regularized (weight decay) instead of the spectral norm.
Since (cid:107)W(cid:107)F ≥ (cid:107)W(cid:107)2, we get a corresponding upper bound ffrobenius: (18) f ij(A(x)) ≤ f ij d(cid:107)W(cid:107)F(cid:107)Vi − Vj(cid:107)2.
frobenius(x) = f ij(x) +  In Section 6, we empirically compare our proposed bound using f ij SDP to these two upper bounds.
6 EXPERIMENTS We evaluated our method on the MNIST dataset of handwritten digits, where the task is to classify images into one of ten classes.
Our results can be summarized as follows: First, in Section 6.1, we show that our certiﬁcates of robustness are tighter than those based on simpler methods such as Frobenius and spectral bounds (Section 5), but our bounds are still too high to be meaningful for general networks.
Then in Section 6.2, we show that by training on the certiﬁcates, we obtain net- works with much better bounds and hence meaningful robustness.
This reﬂects an important point: while accurately analyzing the robustness of an arbitrary network is hard, training the certiﬁcate jointly leads to a network that is robust and certiﬁably so.
In Section 6.3, we present implementation details, design choices, and empirical observations that we made while implementing our method.
Networks.
In this work, we focus on two layer networks.
In all our experiments, we used neural networks with m = 500 hidden units, and TensorFlow’s implementation of Adam (Kingma & Ba, 2014) as the optimizer; we considered networks with more hidden units, but these did not substan- tially improve accuracy.
We experimented with both the multiclass hinge loss and cross-entropy.
All hyperparameters (including the choice of loss function) were tuned based on the error of the Pro- jected Gradient Descent (PGD) attack (Madry et al., 2017) at  = 0.1; we report the hyperparameter settings below.
We considered the following training objectives providing 5 different networks: 1.
Normal training (NT-NN).
Cross-entropy loss and no explicit regularization.
2.
Frobenius norm regularization (Fro-NN).
Hinge loss and a regularizer λ((cid:107)W(cid:107)F +(cid:107)v(cid:107)2) with λ = 0.08.
Under review as a conference paper at ICLR 2018 3.
Spectral norm regularization (Spe-NN).
Hinge loss and a regularizer λ((cid:107)W(cid:107)2 + (cid:107)v(cid:107)2) with λ = 0.09.
4.
Adversarial training (AT-NN).
Cross-entropy with the adversarial loss against PGD as a regularizer, with the regularization parameter set to 0.5. We found that this regularized loss works better than optimizing only the adversarial loss, which is the defense proposed in Madry et al.
(2017).
We set the step size of the PGD adversary to 0.1, number of iterations to 40, and perturbation size to 0.3. 5.
Proposed training objective (SDP-NN).
Dual SDP objective described in Equation 15 of Section 4.
Implementation details and hyperparameter values are detailed in Section 6.3. Evaluating upper bounds.
Below we will consider various upper bounds on the adversarial loss (cid:96)Aopt (based on our method, as well as the Frobenius and spectral bounds described in Section 5).
Ideally we would compare these to the ground-truth adversarial loss (cid:96)Aopt, but computing this exactly is difﬁcult.
Therefore, we compare upper bounds on the adversarial loss with a lower bound on (cid:96)Aopt instead.
The loss of any attack provides a valid lower bound and we consider the strong Projected Gradient Descent (PGD) attack run against the cross-entropy loss, starting from a random point in B(x), with 5 random restarts.
We observed that PGD against hinge loss did not work well, so we used cross-entropy even for attacking networks trained with the hinge loss.
6.1 QUALITY OF THE UPPER BOUND For each of the ﬁve networks described above, we computed upper bounds on the 0-1 loss based on our certiﬁcate (which we refer to as the “SDP bound” in this section), as well as the Frobenius and spectral bounds described in Section 5.
While Section 4 provides a procedure for efﬁciently obtaining an SDP bound as a result of training, for networks not trained with our method we need to solve an SDP at the end of training to obtain certiﬁcates.
Fortunately, this only needs to be done once for every pair of classes.
In our experiments, we use the modeling toolbox YALMIP (Löfberg, 2004) with Sedumi (Sturm, 1999) as a backend to solve the SDPs, using the dual form (14); this took roughly 10 minutes per SDP (around 8 hours in total for a given model).
In Figure 2, we display average values of the different upper bounds over the 10, 000 test examples, as well as the corresponding lower bound from PGD.
We ﬁnd that our bound is tighter than the Frobenius and spectral bounds for all the networks considered, but its tightness relative to the PGD lower bound varies across the networks.
For instance, our bound is relatively tight on Fro-NN, but unfortunately Fro-NN is not very robust against adversarial examples (the PGD attack exhibits large error).
In contrast, the adversarially trained network AT-NN does appear to be robust to attacks, but our certiﬁcate, despite being much tighter than the Frobenius and spectral bounds, is far away from the PGD lower bound.
The only network that is both robust and has relatively tight upper bounds is SDP-NN, which was explicitly trained to be both robust and certiﬁable as described in Section 4; we examine this network and the effects of training in more detail in the next subsection.
6.2 EVALUATING PROPOSED TRAINING OBJECTIVE.
In the previous section, we saw that the SDP bound, while being tighter than simpler upper bounds, could still be quite loose on arbitrary networks.
However, optimizing against the SDP certiﬁcate seemed to make the certiﬁcate tighter.
In this section, we explore the effect of different optimization objectives in more detail.
First, we plot on a single axis the best upper bound (i.e., the SDP bound) and the lower bound (from PGD) on the adversarial loss obtained with each of the ﬁve training objectives discussed above.
This is given in Figure 3a.
Neither spectral nor Frobenius norm regularization seems to be helpful for encouraging adversarial robustness—the actual performance of those networks against the PGD attack is worse than the upper bound for SDP-NN against all attacks.
This shows that the SDP certiﬁcate actually provides a useful training objective for encouraging robustness compared to other regularizers.
Separately, we can ask whether SDP-NN is robust to actual attacks.
We explore the robustness of our network in Figure 3b, where we plot the performance of SDP-NN against 3 attacks—the PGD attack from before, the Carlini-Wagner attack (Carlini & Wagner, 2017b) (another strong attack), and the weaker Fast Gradient Sign Method (FGSM) baseline.
We see substantial robustness against all 3 attacks, even though our method was not explicitly trained with any of them in mind.
Under review as a conference paper at ICLR 2018 (a) NT-NN (b) Fro-NN (c) Spe-NN (d) AT-NN (e) SDP-NN Figure 2: Upper bounds on adversarial error for different networks on MNIST.
(a) (b) Figure 3: (a) Upper bound (SDP) and lower bound (PGD) on the adversarial error for different networks.
(b) Error of SDP-NN against 3 different attacks.
Under review as a conference paper at ICLR 2018 Network SDP-NN LP-NN PGD error SDP bound LP bound 15% 22% 35% 93% 99% 26% Table 1: Comparison with the bound (LP bound) and training approach (LP-NN) of Kolter & Wong (2017).
Numbers are reported for  = 0.1. LP-NN has a certiﬁcate (provided by the LP bound) that no attack can misclassify more than 26% of the examples.
Next, we compare to other bounds reported in the literature.
A rough ceiling is given by the network of Madry et al.
(2017), which is a relatively large four-layer convolutional network adversarially trained against PGD.
While this network has no accompanying certiﬁcate of robustness, it was eval- uated against a number of attack strategies and had worst-case error 11% at  = 0.3. Another set of numbers comes from Carlini et al.
(2017), who use formal veriﬁcation methods to compute Aopt exactly on 10 input examples for a small (72-node) variant of the Madry et al.
network.
The authors reported to us that this network misclassiﬁes 6 out of 10 examples at  = 0.05 (we note that 4 out of 10 of these were misclassiﬁed to start with, but 3 of the 4 can also be ﬂipped to a different wrong class with some  < 0.07).
At the value  = 0.1 for which it was tuned, SDP-NN has error 16% against the PGD attack, and an upper bound of 35% error against any attack.
This is substantially better than the small 72-node network, but also much worse than the full Madry et al.
network.
How much of the latter looseness comes from conservatism in our method, versus the fact that our network has only two layers?
We can get some idea by considering the AT-NN network, which was trained similarly to Madry et al., but uses the same architecture as SDP-NN.
From Figure 3a, we see that the error of SDP-NN against PGD (16%) is not much worse than that of AT-NN (11%), even though AT-NN was explicitly trained against the PGD attack.
This suggests that most of the gap comes from the smaller network depth, rather than from conservatism in the SDP bound.
We are currently in the process of extending our approach to deeper networks, and optimistic about obtaining improved bounds with such networks.
Finally, we compare with the approach proposed in Kolter & Wong (2017) whose work appeared shortly after an initial version of our paper.
They provide an upper bound on the adversarial loss using linear programs (LP) followed by a method to efﬁciently train networks to minimize this upper bound.
In order to compare with SDP-NN, the authors provided us with a network with the same architecture as SDP-NN, but trained using their LP based objective.
We call this network LP-NN.
Table 1 shows that LP-NN and SDP-NN are comparable in terms of their robustness against PGD, and the robustness guarantees that they come with.
Interestingly, the SDP and LP approaches provide vacuous bounds for networks not trained to min- imize the respective upper bounds (though these networks are indeed robust).
This suggests that these two approaches are comparable, but complementary.
Finally, we note that in contrast to this work, the approach of Kolter & Wong (2017) extends to deeper networks, which allows them to train a four-layer CNN with a provable upper bound on adversarial error of 5.7% error.
6.3 IMPLEMENTATION DETAILS We implemented our training objective in TensorFlow, and implemented λ+ max as a custom operator using SciPy’s implementation of the Lanczos algorithm for fast top eigenvector computation; oc- casionally Lanczos fails to converge due to a small eigen-gap, in which case we back off to a full SVD.
We used hinge loss as the classiﬁcation loss, and decayed the learning rate in steps from 10−3 to 10−5, decreasing by a factor of 10 every 30 epochs.
Each gradient step involves computing top eigenvectors for 45 different matrices, one for each pair of classes (i, j).
In order to speed up com- putation, for each update, we randomly pick it and only compute gradients for pairs (it, j), j (cid:54)= it, requiring only 9 top eigenvector computations in each step.
For the regularization parameters λij, the simplest idea is to set them all equal to the same value; this leads to the unweighted regularization scheme where λij = λ for all pairs (i, j).
We tuned λ to 0.05, which led to reasonably good bounds.
However, we observed that certain pairs of classes tended to have larger margins f ij(x) than other classes, which meant that certain label pairs appeared in the maximum of (12) much more often.
That led us to consider a weighted regularization scheme with λij = wijλ, where wij is the fraction of training points for which the the label i (or j) appears as Under review as a conference paper at ICLR 2018 (a) (b) Figure 4: (a) Weighted and unweighted regularization schemes.
The network produced by weighting has a better certiﬁcate as well as lower error against the PGD attack.
(b) The dual certiﬁcate of robustness (SDP dual), obtained automatically during training, is almost as good as the certiﬁcate produced by exactly solving the SDP.
the maximizing term in (12).
We updated the values of these weights every 20 epochs.
Figure 4a compares the PGD lower bound and SDP upper bound for the unweighted and weighted networks.
The weighted network is better than the unweighted network for both the lower and upper bounds.
Finally, we saw in Equation 16 of Section 4 that the dual variables cij provide a quick-to-compute certiﬁcate of robustness.
Figure 4b shows that the certiﬁcates provided by these dual variables are very close to what we would obtain by fully optimizing the semideﬁnite programs.
These dual certiﬁcates made it easy to track robustness across epochs of training and to tune hyperparameters.
7 DISCUSSION In this work, we proposed a method for producing certiﬁcates of robustness for neural networks, and for training against these certiﬁcates to obtain networks that are provably robust against adversaries.
Related work.
In parallel and independent work, Kolter & Wong (2017) also provide provably robust networks against (cid:96)∞ perturbations by using convex relaxations.
While our approach uses a single semideﬁnite program to compute an upper bound on the adversarial loss, Kolter & Wong (2017) use separate linear programs for every data point, and apply their method to networks of depth up to four.
In theory, neither bound is strictly tighter than the other, and our experiments (Table 1) suggest that the two bounds are complementary.
Combining the approaches seems to be a promising future direction.
Katz et al.
(2017a) and the follow-up Carlini et al.
(2017) also provide certiﬁcates of robustness for neural networks against (cid:96)∞ perturbations.
That work uses SMT solvers, which are a tool from the formal veriﬁcation literature.
The SMT solver can answer the binary question “Is there an adversarial example within distance  of the input x?”, and is correct whenever it terminates.
The main drawback of SMT and similar formal veriﬁcation methods is that they are slow—they have worst-case exponential-time scaling in the size of the network; moreover, to use them during training would require a separate search for each gradient step.
Huang et al.
(2017) use SMT solvers and are able to analyze state-of-the-art networks on MNIST, but they make various approximations such that their numbers are not true upper bounds.
Bastani et al.
(2016) provide tractable certiﬁcates but require  to be small enough to ensure that the entire (cid:96)∞ ball around an input lies within the same linear region.
For the networks and values of  that we consider in our paper, we found that this condition did not hold.
Recently, Hein & An- driushchenko (2017) proposed a bound for guaranteeing robustness to (cid:96)p-norm perturbations, based on the maximum p p−1-norm of the gradient in the -ball around the inputs.
Hein & Andriushchenko (2017) show how to efﬁciently compute this bound for p = 2, as opposed to our work which focuses on (cid:96)∞ and requires different techniques to achieve scalability.
10 Under review as a conference paper at ICLR 2018 Madry et al.
(2017) perform adversarial training against PGD on the MNIST and CIFAR-10 datasets, obtaining networks that they suggest are “secure against ﬁrst-order adversaries”.
However, this is based on an empirical observation that PGD is nearly-optimal among gradient-based attacks, and does not correspond to any formal robustness guarantee.
Finally, the notion of a certiﬁcate appears in the theory of convex optimization, but means something different in that context; speciﬁcally, it corresponds to a proof that a point is near the optimum of a convex function, whereas here our certiﬁcates provide upper bounds on non-convex functions.
Addi- tionally, while robust optimization (Bertsimas et al., 2011) provides a tool for optimizing objectives with robustness constraints, applying it directly would involve the same intractable optimization for Aopt that we deal with here.
Other approaches to veriﬁcation.
While they have not been explored in the context of neural networks, there are approaches in the control theory literature for verifying robustness of dynamical systems, based on Lyapunov functions (Lyapunov, 1892; 1992).
We can think of the activations in a neural network as the evolution of a time-varying dynamical system, and attempt to prove stability around a trajectory of this system (Tedrake et al., 2010; Tobenkin et al., 2011).
Such methods typically use sum-of-squares veriﬁcation (Papachristodoulou & Prajna, 2002; 2005; Parrilo, 2003) and are restricted to relatively low-dimensional dynamical systems, but could plausibly scale to larger settings.
Another approach is to construct families of networks that are provably robust a priori, which would remove the need to verify robustness of the learned model; to our knowledge this has not been done for any expressive model families.
Adversarial examples and secure ML.
There has been a great deal of recent work on the security of ML systems; we provide only a sampling here, and refer the reader to Barreno et al.
(2010), Biggio et al.
(2014a), Papernot et al.
(2016b), and Gardiner & Nagaraja (2016) for some recent surveys.
Adversarial examples for neural networks were ﬁrst discovered by Szegedy et al.
(2014), and since then a number of attacks and defenses have been proposed.
We have already discussed gradient- based methods as well as defenses based on adversarial training.
There are also other attacks based on, e.g., saliency maps (Papernot et al., 2016a), KL divergence (Miyato et al., 2015), and elastic net optimization (Chen et al., 2017); many of these attacks are collated in the cleverhans repository (Goodfellow et al., 2016).
For defense, rather than making networks robust to adversaries, some work has focused on simply detecting adversarial examples.
However, Carlini & Wagner (2017a) recently showed that essentially all known detection methods can be subverted by strong attacks.
As explained in Barreno et al.
(2010), there are a number of different attack models beyond the test- time attacks considered here, based on different attacker goals and capabilities.
For instance, one can consider data poisoning attacks, where an attacker modiﬁes the training set in an effort to affect test-time performance.
Newsome et al.
(2006), Laskov & Šrndi`c (2014), and Biggio et al.
(2014b) have demonstrated poisoning attacks against real-world systems.
Other types of certiﬁcates.
Certiﬁcates of performance for machine learning systems are desirable in a number of settings.
This includes verifying safety properties of air trafﬁc control systems (Katz et al., 2017a;b) and self-driving cars (O’Kelly et al., 2016; 2017), as well as security applications such as robustness to training time attacks (Steinhardt et al., 2017).
More broadly, certiﬁcates of performance are likely necessary for deploying machine learning systems in critical infrastructure such as internet packet routing (Winstein & Balakrishnan, 2013; Sivaraman et al., 2014).
In robotics, certiﬁcates of stability are routinely used both for safety veriﬁcation (Lygeros et al., 1999; Mitchell et al., 2005) and controller synthesis (Ba¸sar & Bernhard, 2008; Tedrake et al., 2010).
In traditional veriﬁcation work, Rice’s theorem (Rice, 1953) is a strong impossibility result essen- tially stating that most properties of most programs are undecidable.
Similarly, we should expect that verifying robustness for arbitrary neural networks is hard.
However, the results in this work suggest that it is possible to learn neural networks that are amenable to veriﬁcation, in the same way that it is possible to write programs that can be formally veriﬁed.
Optimistically, given expressive enough certiﬁcation methods and model families, as well as strong enough speciﬁcations of robust- ness, one could even hope to train vector representations of natural images with strong robustness properties, thus ﬁnally closing the chapter on adversarial vulnerabilities in the visual domain.
11 Under review as a conference paper at ICLR 2018 REFERENCES M.
Barreno, B.
Nelson, A.
D.
Joseph, and J.
D.
Tygar.
The security of machine learning.
Machine Learning, 81(2):121–148, 2010.
T.
Ba¸sar and P.
Bernhard.
H-inﬁnity optimal control and related minimax design problems: a dy- namic game approach.
Springer Science & Business Media, 2008.
O.
Bastani, Y.
Ioannou, L.
Lampropoulos, D.
Vytiniotis, A.
Nori, and A.
Criminisi.
Measuring neural net robustness with constraints.
In Advances in Neural Information Processing Systems (NIPS), pp.
2613–2621, 2016.
D.
Bertsimas, D.
B.
Brown, and C.
Caramanis.
Theory and applications of robust optimization.
SIAM review, 53(3):464–501, 2011.
B.
Biggio, G.
Fumera, and F.
Roli.
Security evaluation of pattern classiﬁers under attack.
IEEE Transactions on Knowledge and Data Engineering, 26(4):984–996, 2014a.
B.
Biggio, K.
Rieck, D.
Ariu, C.
Wressnegger, I.
Corona, G.
Giacinto, and F.
Roli.
Poisoning be- havioral malware clustering.
In Workshop on Artiﬁcial Intelligence and Security (AISec), 2014b.
N.
Carlini and D.
Wagner.
Defensive distillation is not robust to adversarial examples.
arXiv, 2016.
N.
Carlini and D.
Wagner.
Adversarial examples are not easily detected: Bypassing ten detection methods.
arXiv, 2017a.
N.
Carlini and D.
Wagner.
Towards evaluating the robustness of neural networks.
In IEEE Sympo- sium on Security and Privacy, pp.
39–57, 2017b.
N.
Carlini, P.
Mishra, T.
Vaidya, Y.
Zhang, M.
Sherr, C.
Shields, D.
Wagner, and W.
Zhou.
Hidden voice commands.
In USENIX Security, 2016.
N.
Carlini, G.
Katz, C.
Barrett, and D.
L.
Dill.
Ground-truth adversarial examples.
arXiv, 2017.
P.
Chen, Y.
Sharma, H.
Zhang, J.
Yi, and C.
Hsieh.
EAD: Elastic-net attacks to deep neural networks via adversarial examples.
arXiv, 2017.
M.
Cisse, P.
Bojanowski, E.
Grave, Y.
Dauphin, and N.
Usunier.
Parseval networks: Improving robustness to adversarial examples.
In International Conference on Machine Learning (ICML), pp.
854–863, 2017.
I.
Evtimov, K.
Eykholt, E.
Fernandes, T.
Kohno, B.
Li, A.
Prakash, A.
Rahmati, and D.
Song.
Robust physical-world attacks on machine learning models.
arXiv, 2017.
J.
Gardiner and S.
Nagaraja.
On the security of machine learning in malware c&c detection: A survey.
ACM Computing Surveys (CSUR), 49(3), 2016.
M.
Goemans and D.
Williamson.
Improved approximation algorithms for maximum cut and satisﬁa- bility problems using semideﬁnite programming.
Journal of the ACM (JACM), 42(6):1115–1145, 1995.
I.
Goodfellow, N.
Papernot, and P.
McDaniel.
cleverhans v2.0.0: an adversarial machine learning library.
arXiv, 2016.
I.
J.
Goodfellow, J.
Shlens, and C.
Szegedy.
Explaining and harnessing adversarial examples.
In International Conference on Learning Representations (ICLR), 2015.
K.
He, X.
Zhang, S.
Ren, and J.
Sun.
Delving deep into rectiﬁers: Surpassing human-level perfor- mance on imagenet classiﬁcation.
arXiv preprint arXiv:1502.01852, 2015.
M.
Hein and M.
Andriushchenko.
Formal guarantees on the robustness of a classiﬁer against adver- sarial manipulation.
arXiv preprint arXiv:1705.08475, 2017.
X.
Huang, M.
Kwiatkowska, S.
Wang, and M.
Wu. Safety veriﬁcation of deep neural networks.
In Computer Aided Veriﬁcation (CAV), pp.
3–29, 2017.
12 Under review as a conference paper at ICLR 2018 G.
Katz, C.
Barrett, D.
Dill, K.
Julian, and M.
Kochenderfer.
Reluplex: An efﬁcient SMT solver for verifying deep neural networks.
arXiv preprint arXiv:1702.01135, 2017a.
G.
Katz, C.
Barrett, D.
L.
Dill, K.
Julian, and M.
J.
Kochenderfer.
Towards proving the adversarial robustness of deep neural networks.
arXiv, 2017b.
D.
Kingma and J.
Ba. Adam: A method for stochastic optimization.
arXiv preprint arXiv:1412.6980, 2014.
J.
Z.
Kolter and E.
Wong.
Provable defenses against adversarial examples via the convex outer adversarial polytope.
arXiv preprint arXiv:1711.00851, 2017.
A.
Kurakin, I.
Goodfellow, and S.
Bengio.
Adversarial examples in the physical world.
arXiv, 2016.
P.
Laskov and N.
Šrndi`c.
Practical evasion of a learning-based classiﬁer: A case study.
In Symposium on Security and Privacy, 2014.
J.
Löfberg.
YALMIP: A toolbox for modeling and optimization in MATLAB.
In CACSD, 2004.
A.
M.
Lyapunov.
The general problem of the stability of motion (in Russian).
PhD thesis, Kharkov Mathematical Society, 1892.
A.
M.
Lyapunov.
The general problem of the stability of motion.
International Journal of Control, 55(3):531–534, 1992.
J.
Lygeros, C.
Tomlin, and S.
Sastry.
Controllers for reachability speciﬁcations for hybrid systems.
Automatica, 35(3):349–370, 1999.
A.
Madry, A.
Makelov, L.
Schmidt, D.
Tsipras, and A.
Vladu.
Towards deep learning models resistant to adversarial attacks.
arXiv, 2017.
I.
M.
Mitchell, A.
M.
Bayen, and C.
J.
Tomlin.
A time-dependent Hamilton-Jacobi formulation of reachable sets for continuous dynamic games.
IEEE Transactions on Automatic Control, 50(7): 947–957, 2005.
T.
Miyato, S.
Maeda, M.
Koyama, K.
Nakae, and S.
Ishii.
Distributional smoothing with virtual adversarial training.
arXiv, 2015.
A.
Nayebi and S.
Ganguli.
Biologically inspired protection of deep networks from adversarial at- tacks.
arXiv preprint arXiv:1703.09202, 2017.
J.
Newsome, B.
Karp, and D.
Song.
Paragraph: Thwarting signature learning by training mali- ciously.
In International Workshop on Recent Advances in Intrusion Detection, 2006.
M.
O’Kelly, H.
Abbas, S.
Gao, S.
Shiraishi, S.
Kato, and R.
Mangharam.
APEX: Autonomous vehicle plan veriﬁcation and execution.
Technical report, University of Pennsylvania, 2016.
M.
O’Kelly, H.
Abbas, and R.
Mangharam.
Computer-aided design for safe autonomous vehicles.
Technical report, University of Pennsylvania, 2017.
A.
Papachristodoulou and S.
Prajna.
On the construction of lyapunov functions using the sum of squares decomposition.
In IEEE Conference on Decision and Control, 2002.
A.
Papachristodoulou and S.
Prajna.
Analysis of non-polynomial systems using the sum of squares decomposition.
Positive polynomials in control, 2005.
N.
Papernot, P.
McDaniel, S.
Jha, M.
Fredrikson, Z.
B.
Celik, and A.
Swami.
The limitations of deep learning in adversarial settings.
In Security and Privacy (EuroS&P), 2016 IEEE European Symposium on, pp.
372–387, 2016a.
N.
Papernot, P.
McDaniel, A.
Sinha, and M.
Wellman.
Towards the science of security and privacy in machine learning.
arXiv, 2016b.
N.
Papernot, P.
McDaniel, X.
Wu, S.
Jha, and A.
Swami.
Distillation as a defense to adversarial perturbations against deep neural networks.
In IEEE Symposium on Security and Privacy, pp.
582–597, 2016c.
13 Under review as a conference paper at ICLR 2018 P.
A.
Parrilo.
Semideﬁnite programming relaxations for semialgebraic problems.
Mathematical programming, 96(2):293–320, 2003.
H.
G.
Rice.
Classes of recursively enumerable sets and their decision problems.
Transactions of the American Mathematical Society, 74(2):358–366, 1953.
M.
Sharif, S.
Bhagavatula, L.
Bauer, and M.
K.
Reiter.
Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition.
In ACM SIGSAC Conference on Computer and Com- munications Security, pp.
1528–1540, 2016.
D.
Silver, A.
Huang, C.
J.
Maddison, A.
Guez, L.
Sifre, G.
V.
D.
Driessche, J.
Schrittwieser, I.
Antonoglou, V.
Panneershelvam, M.
Lanctot, et al.
Mastering the game of go with deep neural networks and tree search.
Nature, 529(7587):484–489, 2016.
A.
Sivaraman, K.
Winstein, P.
Thaker, and H.
Balakrishnan.
An experimental study of the learnabil- ity of congestion control.
In SIGCOMM, 2014.
J.
Steinhardt, P.
W.
Koh, and P.
Liang.
Certiﬁed defenses for data poisoning attacks.
In Advances in Neural Information Processing Systems (NIPS), 2017.
J.
F.
Sturm.
Using SeDuMi 1.02, a MATLAB toolbox for optimization over symmetric cones.
Optimization Methods and Software, 11:625–653, 1999.
C.
Szegedy, W.
Zaremba, I.
Sutskever, J.
Bruna, D.
Erhan, I.
Goodfellow, and R.
Fergus.
Intriguing properties of neural networks.
In International Conference on Learning Representations (ICLR), 2014.
R.
Tedrake, I.
R.
Manchester, M.
M.
Tobenkin, and J.
W.
Roberts.
LQR-trees: Feedback motion planning via sums of squares veriﬁcation.
International Journal of Robotics Research, 29:1038– 1052, 2010.
M.
M.
Tobenkin, I.
R.
Manchester, and R.
Tedrake.
Invariant funnels around trajectories using sum-of-squares programming.
IFAC Proceedings Volumes, 44, 2011.
F.
Tramèr, A.
Kurakin, N.
Papernot, D.
Boneh, and P.
McDaniel.
Ensemble adversarial training: Attacks and defenses.
arXiv preprint arXiv:1705.07204, 2017.
K.
Winstein and H.
Balakrishnan.
TCP ex machina: Computer-generated congestion control.
In SIGCOMM, 2013.
W.
Xiong, J.
Droppo, X.
Huang, F.
Seide, M.
Seltzer, A.
Stolcke, D.
Yu, and G.
Zweig.
Achieving human parity in conversational speech recognition.
arXiv, 2016.
A DUALITY In this section we justify the duality relation (14).
Recall that the primal program is maximize (cid:104)M, P(cid:105) subject to P (cid:23) 0, diag(P ) ≤ 1.
(19) Rather than taking the dual directly, we ﬁrst add the redundant constraint tr(P ) ≤ d + m + 1 (it is redundant because the SDP is in d + m + 1 dimensions and diag(P ) ≤ 1).
This yields maximize (cid:104)M, P(cid:105) subject to P (cid:23) 0, diag(P ) ≤ 1, tr(P ) ≤ d + m + 1.
(20) We now form the Lagrangian for the constraints diag(P ) ≤ 1, leaving the other two constraints as-is.
This yields the equivalent optimization problem (cid:104)M, P(cid:105) + c(cid:62)(1 − diag(P )) maximize min c≥0 subject to P (cid:23) 0, tr(P ) ≤ d + m + 1.
(21) 14 Under review as a conference paper at ICLR 2018 Now, we apply minimax duality to swap the order of min and max; the value of (21) is thus equal to (cid:104)M, P(cid:105) + c(cid:62)(1 − diag(P )) (22) minimize max P(cid:23)0, tr(P )≤d+m+1 subject to c ≥ 0.
The inner maximum can be simpliﬁed as 1(cid:62)c+(d+m+1)·(cid:16) (cid:104)M−diag(c), P(cid:105)(cid:17) max P(cid:23)0,tr(P )≤1 = 1(cid:62)c+(d+m+1)λ+ max(M−diag(c)).
(23) Therefore, (22) simpliﬁes to minimize 1(cid:62)c + (d + m + 1)λ+ subject to c ≥ 0.
max(M − diag(c)) (24) This is almost the form given in (14), except that c is constrained to be non-negative and we have 1(cid:62)c instead of 1(cid:62) max(c, 0).
However, note that for the λ+ max term, it is always better for c to be larger; therefore, replacing c with max(c, 0) means that the optimal value of c will always be non- negative, thus allowing us to drop the c ≥ 0 constraint and optimize c in an unconstrained manner.
This ﬁnally yields the claimed duality relation (14).
15
Recurrent neural networks (RNNs) are speciﬁc type of neural networks which are designed to model the sequence data.
In last decades, various RNN architectures have been proposed, such as Long- Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) and Gated Recurrent Units Cho et al.
(2014).
They have enabled the RNNs to achieve state-of-art performance in many applications, e.g., language models (Mikolov et al., 2010), neural machine translation (Sutskever et al., 2014; Wu et al., 2016), automatic speech recognition (Graves et al., 2013), image captions (Vinyals et al., 2015), etc.
However, the models often build on high dimensional input/output,e.g., large vocabulary in language models, or very deep inner recurrent networks, making the models have too many parameters to deploy on portable devices with limited resources.
In addition, RNNs can only be executed sequentially with dependence on current hidden states.
This causes large latency during inference.
For applications in the server with large scale concurrent requests, e.g., on-line machine translation and speech recognition, large latency leads to limited requests processed per machine to meet the stringent response time requirements.
Thus much more costly computing resources are in demand for RNN based models.
To alleviate the above problems, several techniques can be employed, i.e., low rank approximation (Sainath et al., 2013; Jaderberg et al., 2014; Lebedev et al., 2014; Tai et al., 2016), sparsity (Liu ∗Work performed while interning at Alibaba search algorithm team.
†Corresponding author.
Published as a conference paper at ICLR 2018 et al., 2015; Han et al., 2015; 2016; Wen et al., 2016), and quantization.
All of them are build on the redundancy of current networks and can be combined.
In this work, we mainly focus on quantization based methods.
More precisely, we are to quantize all parameters into multiple binary codes {−1, +1}.
The idea of quantizing both weights and activations is ﬁrstly proposed by (Hubara et al., 2016a).
It has shown that even 1-bit binarization can achieve reasonably good performance in some visual classiﬁcation tasks.
Compared with the full precision counterpart, binary weights reduce the memory by a factor of 32.
And the costly arithmetic operations between weights and activations can then be replaced by cheap XNOR and bitcount operations(Hubara et al., 2016a), which potentially leads to much acceleration.
Rastegari et al.
(2016) further incorporate a real coefﬁcient to compensate for the binarization error.
They apply the method to the challenging ImageNet dataset and achieve better performance than pure binarization in (Hubara et al., 2016a).
However, it is still of large gap compared with the full precision networks.
To bridge this gap, some recent works (Hubara et al., 2016b; Zhou et al., 2016; 2017) further employ quantization with more bits and achieve plausible performance.
Meanwhile, quite an amount of works, e.g., (Courbariaux et al., 2015; Li et al., 2016; Zhu et al., 2017; Guo et al., 2017), quantize the weights only.
Although much memory saving can be achieved, the acceleration is very limited in modern computing devices (Rastegari et al., 2016).
Among all existing quantization works, most of them focus on convolutional neural networks (CNNs) while pay less attention to RNNs. As mentioned earlier, the latter is also very demanding.
Recently, (Hou et al., 2017) showed that binarized LSTM with preconditioned coefﬁcients can achieve promising performance in some easy tasks such as predicting the next character.
However, for RNNs with large input/output, e.g., large vocabulary in language models, it is still very challenging for quantization.
Both works of Hubara et al.
(2016b) and Zhou et al.
(2017) test the effectiveness of their multi-bit quantized RNNs to predict the next word.
Although using up to 4-bits, the results with quantization still have noticeable gap with those with full precision.
This motivates us to ﬁnd a better method to quantize RNNs. The main contribution of this work is as follows: (a) We formulate the multi-bit quantization as an optimization problem.
The binary codes {−1, +1} are learned instead of rule-based.
For the ﬁrst time, we observe that the codes can be optimally derived by the binary search tree once the coefﬁcients are knowns in advance, see, e.g., Algorithm 1.
Thus the whole optimization is eased by removing the discrete unknowns, which are very difﬁcult to handle.
(b) We propose to use alternating minimization to tackle the quantization problem.
By separating the binary codes and real coefﬁcients into two parts, we can solve the subproblem efﬁciently when one part is ﬁxed.
With proper initialization, we only need two alternating cycles to get high precision approximation, which is effective enough to even quantize the activations on-line.
(c) We systematically evaluate the effectiveness of our alternating quantization on language models.
Two well-known RNN structures, i.e., LSTM and GRU, are tested with different quantization bits.
Compared with the full-precision counterpart, by 2-bit quantization we can achieve ∼16× memory saving and ∼6× real inference acceleration on CPUs, with a reasonable loss on the accuracy.
By 3-bit quantization, we can achieve almost no loss in accuracy or even surpass the original model with ∼10.5× memory saving and ∼3× real inference acceleration.
Both results beat the exiting quantization works with large margins.
To illustrate that our alternating quantization is very general to extend, we apply it to image classiﬁcation tasks.
In both RNNs and feedforward neural networks, the technique still achieves very plausible performance.
2 EXISTING MULTI-BIT QUANTIZATION METHODS Before introducing our proposed multi-bit quantization, we ﬁrst summarize existing works as follows: (a) Uniform quantization method (Rastegari et al., 2016; Hubara et al., 2016b) ﬁrstly scales its value in the range x ∈ [−1, 1].
Then it adopts the following k-bit quantization: (cid:32) (cid:33) − 1 qk(x) = 2 round[(2k − 1)( x+1 2 )] 2k − 1 (1) after which the method scales back to the original range.
Such quantization is rule based thus is very easy to implement.
The intrinsic beneﬁt is that when computing inner product Published as a conference paper at ICLR 2018 Figure 1: Illustration of the optimal 2-bit quantization when α1 and α2 (α1 ≥ α2) are known in advance.
The values are quantized into −α1 − α2, −α1 + α2, α1 − α2, and α1 + α2, respectively.
And the partition intervals are optimally separated by the middle points of adjacent quantization codes, i.e., −α1, 0, and α1, correspondingly.
of two quantized vectors, it can employ cheap bit shift and count operations to replace costly multiplications and additions operations.
However, the method can be far from optimum when quantizing non-uniform data, which is believed to be the trained weights and activations of deep neural network (Zhou et al., 2017).
(b) Balanced quantization (Zhou et al., 2017) alleviates the drawbacks of the uniform quantization by ﬁrstly equalizing the data.
The method constructs 2k intervals which contain roughly the same percentage of the data.
Then it linearly maps the center of each interval to the corresponding quantization code in (1).
Although sounding more reasonable than the uniform one, the afﬁne transform on the centers can still be suboptimal.
In addition, there is no guarantee that the evenly spaced partition is more suitable if compared with the non-evenly spaced partition for a speciﬁc data distribution.
(c) Greedy approximation (Guo et al., 2017) instead tries to learn the quantization by tackling the following problem: , with bi ∈ {−1, +1}n.
(2) min {αi,bi}k i=1 αibi (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)w − k(cid:88) (cid:107)ri−1 − αibi(cid:107)2 , with ri−1 = w − i−1(cid:88) i=1 min αi,bi For k = 1, the above problem has a closed-form solution (Rastegari et al., 2016).
Greedy approximation extends to k-bit (k > 1) quantization by sequentially minimizing the residue.
That is Then the optimal solution is given as αi = (cid:107)ri−1(cid:107)1 and bi = sign(ri−1).
j=1 αjbj.
(3) (4) Greedy approximation is very efﬁcient to implement in modern computing devices.
Although not able to reach a high precision solution, the formulation of minimizing quantization error is very promising.
(d) Reﬁned greedy approximation (Guo et al., 2017) extends to further decrease the quantization error.
In the j-th iteration after minimizing problem (3), the method adds one extra step to reﬁne all computed {αi}j i=1 with the least squares solution: [α1, .
.
.
, αj] =(cid:0)(BT j w(cid:1)T j Bj)−1BT (5) In experiments of quantizing the weights of CNN, the reﬁned approximation is veriﬁed to be better than the original greedy one.
However, as we will show later, the reﬁned method is still far from satisfactory for quantization accuracy.
, with Bj = [b1, .
.
.
, bj], Besides the general multi-bit quantization as summarized above, Li et al.
(2016) propose ternary quantization by extending 1-bit binarization with one more feasible state, 0.
It does quantization by tackling minα,t (cid:107)w − αt(cid:107)2 2 with t ∈ {−1, 0, +1}n.
However, no efﬁcient algorithm is proposed in (Li et al., 2016).
They instead empirically set the entries w with absolute scales less than 0.7/n(cid:107)w(cid:107)1 to 0 and binarize the left entries as (4).
In fact, ternary quantization is a special case of the 2-bit quantization in (2), with an additional constraint that α1 = α2.
When the binary codes are ﬁxed, the optimal coefﬁcient α1 (or α2) can be derived by least squares solution similar to (5).
Quantize to-α1α10Quantize toQuantize toQuantize toα1-α2-α1+α2α1+α2-α1-α2Published as a conference paper at ICLR 2018 Figure 2: Illustration of binary search tree to determine the optimal quantization.
Algorithm 1: Binary Search Tree (BST) to determine to optimal code BST(w, v) {w is the real value to be quantized} {v is the vector of quantization codes in ascending order} m = length(v) if m == 1 then return v1 end if w ≥ (vm/2 + vm/2+1)/2 then BST(w, vm/2+1:m) else end BST( w, v1:m/2) In parallel to the binarized quantization discussed here, vector quantization is applied to compress the weights for feedforward neural networks (Gong et al., 2014; Han et al., 2016).
Different from ours where all weights are directly constraint to {−1, +1}, vector quantization learns a small codebook by applying k-means clustering to the weights or conducting product quantization.
The weights are then reconstructed by indexing the codebook.
It has been shown that by such a technique, the number of parameters can be reduced by an order of magnitude with limited accuracy loss (Gong et al., 2014).
It is possible that the multi-bit quantized binary weight can be further compressed by using the product quantization.
3 OUR ALTERNATING MULTI-BIT QUANTIZATION in ascending order, i.e., v = {−(cid:80)k i=1 αi, .
.
.
,(cid:80)k Now we introduce our quantization method.
We tackle the same minimization problem as (2).
For simplicity, we ﬁrstly consider the problem with k = 2.
Suppose that α1 and α2 are known in advance with α1 ≥ α2 ≥ 0, then the quantization codes are restricted to v = {−α1 − α2,−α1 + α2, α1 − α2, α1 + α2}.
For any entry w of w in problem (2), its quantization code is determined by the least distance to all codes.
Consequently, we can partition the number axis into 4 intervals.
And each interval corresponds to one particular quantization code.
The common point of two adjacent intervals then becomes the middle point of the two quantization codes, i.e., −α1, 0, and α1.
Fig.
1 gives an illustration.
For the general k-bit quantization, suppose that {αi}k i=1 are known and we have all possible codes i=1 αi}.
Similarly, we can partition the number axis into 2k intervals, in which the boundaries are determined by the centers of two adjacent codes in v, i.e., {(vi + vi+1)/2}2k−1 i=1 .
However, directly comparing per entry with all the boundaries needs 2k comparisons, which is very inefﬁcient.
Instead, we can make use of the ascending property in v.
Hierarchically, we partition the codes of v evenly into two ordered sub-sets, i.e., v1:m/2 and vm/2+1:m with m deﬁned as the length of v.
If w < (vm/2 + vm/2+1)/2, its feasible codes are then optimally restricted to v1:m/2.
And if w ≥ (vm/2 +vm/2+1)/2 , its feasible codes become vm/2+1:m.
By recursively evenly partition the ordered feasible codes, we can then efﬁciently determine the -α1-α2 -α1+α2 α1-α2 α1+α2 -α1 α1 0Published as a conference paper at ICLR 2018 Algorithm 2: Alternating Multi-bit Quantization Require :Full precision weight w ∈ Rn, number of bits k, total iterations T Ensure :{αi, bi}k 1 Greedy Initialize {αi, bi}k 2 for iter ← 1 to T do i=1 as (4) i=1 i=1 as (5) Update {αi}k Construct v of all feasible codes in accending order Update {bi}k i=1 as Algorithm 1.
6 end optimal code for per entry by only k comparisons.
The whole procedure is in fact a binary search tree.
We summarize it in Algorithm 1.
Note that once getting the quantization code, it is straightforward to map to the binary code b.
Also, by maintaining a mask vector with the same size as w to indicate the partitions, we could operate BST for all entries simultaneously.
To give a better illustration, we give a binary tree example for k = 2 in Fig.
2.
Note that for k = 2, we can even derive the optimal codes by a closed form solution, i.e., b1 = sign(w) and b2 = sign(w − α1b1) with α1 ≥ α2 ≥ 0.
Under the above observation, let us reconsider the reﬁned greedy approximation (Guo et al., 2017) introduced in Section 2.
After modiﬁcation on the computed {αi}j i=2 are no longer optimal while the method keeps all of them ﬁxed.
To improve the reﬁned greedy approximation, i=1 becomes a natural choice.
Once getting {bi}k i=1 and {bi}k alternating minimizing {αi}k i=1 as described above, we can optimize {αi}k i=1 as (5).
In real experiments, we ﬁnd that by greedy initialization as (4), only two alternating cycles is good enough to ﬁnd high precision quantization.
For better illustration, we summarize our alternating minimization in Algorithm 2.
For updating {αi}k i=1, we need 2k2n binary operations and kn non-binary operations.
Combining kn non-binary operations to determine the binary code, for total T alternating cycles, we thus need 2T k2n binary operations and 2(T + 1)kn non-binary operations to quantize w ∈ Rn into k-bit, with the extra 2kn corresponding to greedy initialization.
i=1 as (5), {bi}j 4 APPLY ALTERNATING MULTI-BIT QUANTIZATION TO RNNS Implementation.
We ﬁrstly introduce the implementation details for quantizing RNN.
For simplicity, we consider the one layer LSTM for language model.
The goal is to predict the next word indexed by t in a sequence of one-hot word tokens (y∗ N ) as follows: 1, .
.
.
, y∗ e y∗ t−1, xt = WT it, ft, ot, gt = σ(Wixt + bi + Whht−1 + bh), ct = ft (cid:12) ct−1 + it (cid:12) gt, ht = ot (cid:12) tanh(ct), yt = softmax(Wsht + bs).
(6) where σ represents the activation function.
In the above formulation, the multiplication between the weight matrices and the vectors xt and ht occupy most of the computation.
This is also where we apply quantization to.
For the weight matrices, We do not apply quantization on the full but rather row by row.
During the matrix vector product, we can ﬁrstly execute the binary multiplication.
Then element-wisely multiply the obtained binary vector with the high precision scaling coefﬁcients.
Thus little extra computation results while much more freedom is brought to better approximate the weights.
We give an illustration on the left part of Fig.
3.
Due to one-hot word tokens, xt corresponds to one speciﬁc row in the quantized We. It needs no more quantization.
Different from the weight matrices, ht depends on the input, which needs to be quantized on-line during inference.
For consistent notation with existing work, e.g., (Hubara et al., 2016b; Zhou et al., 2017), we also call quantizing on ht as quantizing on activation.
For W ∈ Rm×n and ht ∈ Rn, the standard matrix-vector product needs 2mn operations.
For the quantized product between kw-bit W and kh-bit ht, we have 2kwkhmn + 4k2 hn binary operations and 6khn + 2kwkhm non-binary operations, where 6khn corresponds to the cost of alternating approximation (T = 2) and 2kwkhm corresponds to the ﬁnal product with coefﬁcients.
As the binary Published as a conference paper at ICLR 2018 Figure 3: Illustration of quantized matrix vector multiplication (left part).
The matrix is quantized row by row, which provides more freedom to approximate while adds little extra computation.
By reformulating as the right part, we can make full use of the intrinsic parallel binary matrix vector multiplication for further acceleration.
2mn 32 (2kwkhmn+4k2 multiplication operates in 1 bit, whereas the full precision multiplication operates in 32 bits, despite the feasible implementations, the acceleration can be 32× in theory.
For alternating quantization nn)+6khn+2kwkhm.
here, the overall theoretical acceleration is thus computed as γ = Suppose that LSTM has hidden states n = 1024, then we have Wh ∈ R4096×1024.
The acceleration ratio becomes roughly 7.5× for (kh, kw) = (2, 2) and 3.5× for (kh, kw) = (3, 3).
In addition to binary operations, the acceleration in real implementations can be largely affected by the size of the matrix, where much memory reduce can result in better utilizing in the limited faster cache.
We implement the binary multiplication kernel in CPUs. Compared with the much optimized Intel Math Kernel Library (MKL) on full precision matrix vector multiplication, we can roughly achieve 6× for (kh, kw) = (2, 2) and 3× for (kh, kw) = (3, 3).
For more details, please refer to Appendix A.
As indicated in the left part of Fig.
3, the binary multiplication can be conducted sequentially by associativity.
Although the operation is suitable for parallel computing by synchronously conducting the multiplication, this needs extra effort for parallelization.
We instead concatenate the binary codes as shown in the right part of Fig.
3.
Under such modiﬁcation, we are able to make full use of the much optimized inner parallel matrix multiplication, which gives the possibility for further acceleration.
The ﬁnal result is then obtained by adding all partitioned vectors together, which has little extra computation.
Training.
As ﬁrstly proposed by Courbariaux et al.
(2015), during the training of quantized neural network, directly adding the moderately small gradients to quantized weights will result in no change on it.
So they maintain a full precision weight to accumulate the gradients then apply quantization in every mini-batch.
In fact, the whole procedure can be mathematically formulated as a bi-level optimization (Colson et al., 2007) problem: min w,{αi,bi}k i=1 αibi (cid:32) k(cid:88) i=1 (cid:33) (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)w − k(cid:88) i=1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2 α(cid:48) ib(cid:48) (7) s.t. {αi, bi}k Denote the quantized weight as ˆw =(cid:80)k i=1 = arg min i}k i,b(cid:48) {α(cid:48) i=1 i=1 αibi.
In the forward propagation, we derive ˆw from the full precision w in the lower-level problem and apply it to the upper-level function f (·), i.e., RNN in this paper.
During the backward propagation, the derivative ∂f ∂ ˆw is propagated back to w through the lower-level function.
Due to the discreteness of bi, it is very hard to model the implicit dependence of ˆw on w.
So we also adopt the “straight-through estimate” as (Courbariaux et al., 2015), i.e., ∂f ∂ ˆw .
To compute the derivative on the quantized hidden state ht, the same trick is applied.
During the training, we ﬁnd the same phenomenon as Hubara et al.
(2016b) that some ∂w = ∂f    0.16   - 0.21  … - 0.05                        - 0.32     0.09  …   0.13                  0.07   - 0.17  … - 0.02 …     0.03    - 0.17                      0.20 * …    1       - 1   …  -1                         -1         1   …   1                  1       - 1  …  -1 …     0.15      0.30                      0.10    1       - 1   …   1                         -1        -1   …  -1                 -1       - 1  …    1     0.08      0.12                      0.06    -1     -1                   1 * ( + ) … …     1     -1                   1 …  0.13   0.05  ) ( +     0.15      0.30                      0.10    1       - 1   …  -1                         -1         1   …   1                  1       - 1  …  -1 …     0.08      0.12                      0.06 …    1       - 1   …   1                         -1        -1   …  -1                 -1       - 1  …    1    -1     -1                   1 …  0.05      1     -1                   1  0.13  … * Standard Matrix  Vector Product Multi-bit Binary Product Modified Multi-bit Binary Product Published as a conference paper at ICLR 2018 Table 1: Measurement on the approximation of different quantization methods, e.g., Uniform (Hubara et al., 2016b), Balanced (Zhou et al., 2017), Greedy (Guo et al., 2017), Reﬁned (Guo et al., 2017), and our Alternating method, see Section 2.
We apply these methods to quantize the full precision pre-trained weight of LSTM on the PTB dataset.
The best values are in bold.
W-bits represents the number of weight bits and FP denotes full precision.
W-Bits Uniform Balanced Greedy Reﬁned Alternating (ours) Relative MSE Testing PPW FP 1.070 0.891 0.146 0.137 0.125 0.404 0.745 0.071 0.060 0.043 0.302 0.702 0.042 0.030 0.019 283.2 10287.6 118.9 105.3 103.1 227.3 9106.4 99.4 95.4 93.8 216.3 8539.8 95.0 93.1 91.4 89.8 Table 2: Quantization on the full precision pre-trained weight of GRU on the PTB dataset.
W-Bits Uniform Balanced Greedy Reﬁned Alternating (ours) Relative MSE Testing PPW FP 6.138 1.206 0.377 0.128 0.120 3.920 1.054 0.325 0.055 0.044 3.553 1.006 0.304 0.030 0.021 3161906.6 2980.4 135.7 111.6 110.3 771259.6 3396.3 105.5 99.1 97.3 715781.9 3434.1 99.2 97.0 95.2 92.5 entries of w can grow very large, which become outliers and harm the quantization.
Here we simply clip w in the range of [−1, 1].
5 EXPERIMENTS ON THE LANGUAGE MODELS In this section, we conduct quantization experiments on language models.
The two most well-known recurrent neural networks, i.e., LSTM (Hochreiter & Schmidhuber, 1997) and GRU (Cho et al., 2014), are evaluated.
As they are to predict the next word, the performance is measured by perplexity per word (PPW) metric.
For all experiments, we initialize with the pre-trained model and using vanilla SGD.
The initial learning rate is set to 20.
Every epoch we evaluate on the validation dataset and record the best value.
When the validation error exceeds the best record, we decrease learning rate by a factor of 1.2. Training is terminated once the learning rate less than 0.001 or reaching the maximum epochs, i.e., 80.
The gradient norm is clipped in the range [−0.25, 0.25].
We unroll the network for 30 time steps and regularize it with the standard dropout (probability of dropping out units equals to 0.5) (Zaremba et al., 2014).
For simplicity of notation, we denote the methods using uniform, balanced, greedy, reﬁned greedy, and our alternating quantization as Uniform, Balanced, Greedy, Reﬁned, and Alternating, respectively.
Peen Tree Bank.
We ﬁrst conduct experiments on the Peen Tree Bank (PTB) corpus (Marcus et al., 1993), using the standard preprocessed splits with a 10K size vocabulary (Mikolov, 2012).
The PTB dataset contains 929K training tokens, 73K validation tokens, and 82K test tokens.
For fair comparison with existing works, we also use LSTM and GRU with 1 hidden layer of size 300.
To have a glance at the approximation ability of different quantization methods as detailed in Section 2, we ﬁrstly conduct experiments by directly quantizing the trained full precision weight (neither quantization on activation nor retraining).
Results on LSTM and GRU are shown in Table 1 and Table 2, respectively.
The left parts record the relative mean squared error of quantized weight matrices with full precision one.
We can see that our proposed Alternating can get much lower error across all Published as a conference paper at ICLR 2018 Table 3: Testing PPW of multi-bit quantized LSTM and GRU on the PTB dataset.
W-Bits and A-Bits represent the number of weight and activation bits, respectively.
W-Bits / A-Bits Uniform Balanced Reﬁned Alternating (ours) LSTM 2/3 220 123 95.6 91.9 2/2 126 100.3 95.8 3/3 91.3 87.9 4/4 100 114 FP/FP 97 107 89.8 2/2 142 105.1 101.2 2/3 100.3 97.0 GRU 3/3 95.9 92.9 4/4 116 FP/FP 100 92.5 Table 4: Testing PPW of multi-bit quantized LSTM and GRU on the WikiText-2 dataset.
LSTM GRU W-Bits / A-Bits 2/2 2/3 3/3 FP/FP 2/2 2/3 3/3 FP/FP Reﬁned Alternating (ours) 108.7 106.1 105.8 102.7 102.2 98.7 100.1 117.2 113.7 114.1 110.2 111.8 106.4 106.7 Table 5: Testing PPW of multi-bit quantized LSTM and GRU on the Text8 dataset.
LSTM GRU W-Bits / A-Bits 2/2 2/3 3/3 FP/FP 2/2 2/3 3/3 FP/FP Reﬁned Alternating (ours) 135.6 108.8 122.3 105.1 110.2 98.8 101.1 135.8 124.5 126.9 118.7 118.3 114.0 111.6 varying bit.
We also measure the testing PPW for the quantized weight as shown in the right parts of Table 1 and 2.
The results are in consistent with the left part, where less errors result in lower testing PPW.
Note that Uniform and Balanced quantization are rule-based and not aim at minimizing the error.
Thus they can have much worse result by direct approximation.
We also repeat the experiment on other datasets.
For both LSTM and GRU, the results are very similar to here.
We then conduct experiments by quantizing both weights and activations.
We train with the batch size 20.
The ﬁnal result is shown in Table 3.
Besides comparing with the existing works, we also conduct experiment for Reﬁned as a competitive baseline.
We do not include Greedy as it is already shown to be much inferior to the reﬁned one, see, e.g., Table 1 and 2.
As Table 3 shows, our full precision model can attain lower PPW than the existing works.
However, when considering the gap between quantized model with the full precision one, our alternating quantized neural network is still far better than existing works, i.e., Uniform (Hubara et al., 2016b) and Balanced (Zhou et al., 2017).
Compared with Reﬁned, our Alternating quantization can achieve compatible performance using 1-bit less quantization on weights or activations.
In other words, under the same tolerance of accuracy drop, Alternating executes faster and uses less memory than Reﬁned.
We can see that our 3/3 weights/activations quantized LSTM can achieve even better performance than full precision one.
A possible explanation is due to the regularization introduced by quantization (Hubara et al., 2016b).
WikiText-2 (Merity et al., 2017) is a dataset released recently as an alternative to PTB.
It contains 2088K training, 217K validation, and 245K test tokens, and has a vocabulary of 33K words, which is roughly 2 times larger in dataset size, and 3 times larger in vocabulary than PTB.
We train with one layer’s hidden state of size 512 and set the batch size to 100.
The result is shown in Table 4.
Similar to PTB, our Alternating can use 1-bit less quantization to attain compatible or even lower PPW than Reﬁned.
Published as a conference paper at ICLR 2018 Text8.
In order to determine whether Alternating remains effective with a larger dataset, we perform experiments on the Text8 corpus (Mikolov et al., 2014).
Here we follow the same setting as (Xie et al., 2017).
The ﬁrst 90M characters are used for training, the next 5M for validation, and the ﬁnal 5M for testing, resulting in 15.3M training tokens, 848K validation tokens, and 855K test tokens.
We also preprocess the data by mapping all words which appear 10 or fewer times to the unknown token, resulting in a 42K size vocabulary.
We train LSTM and GRU with one hidden layer of size 1024 and set the batch size to 100.
The result is shown in Table 5.
For LSTM on the left part, Alternating achieves excellent performance.
By only 2-bit quantization on weights and activations, it exceeds Reﬁned with 3-bit.
The 2-bit result is even better than that reported in (Xie et al., 2017), where LSTM adding noising schemes for regularization can only attain 110.6 testing PPW.
For GRU on the right part, although Alternating is much better than Reﬁned, the 3-bit quantization still has gap with full precision one.
We attribute that to the uniﬁed setting of hyper-parameters across all experiments.
With speciﬁcally tuned hyper-parameters on this dataset, one may make up for that gap.
Note that our alternating quantization is a general technique.
It is not only suitable for language models here.
For a comprehensive veriﬁcation, we apply it to image classiﬁcation tasks.
In both RNNs and feedforward neural networks, our alternating quantization also achieves the lowest testing error among all compared methods.
Due to space limitation, we deter the results to Appendix B.
6 CONCLUSIONS In this work, we address the limitations of RNNs, i.e., large memory and high latency, by quantization.
We formulate the quantization by minimizing the approximation error.
Under the key observation that some parameters can be singled out when others ﬁxed, a simple yet effective alternating method is proposed.
We apply it to quantize LSTM and GRU on language models.
By 2-bit weights and activations, we achieve only a reasonably accuracy loss compared with full precision one, with ∼16× reduction in memory and ∼6× real acceleration on CPUs. By 3-bit quantization, we can attain compatible or even better result than the full precision one, with ∼10.5× reduction in memory and ∼3× real acceleration.
Both beat existing works with a large margin.
We also apply our alternating quantization to image classiﬁcation tasks.
In both RNNs and feedforward neural networks, the method can still achieve very plausible performance.
7 ACKNOWLEDGEMENTS We would like to thank the reviewers for their suggestions on the manuscript.
Zhouchen Lin is supported by National Basic Research Program of China (973 Program) (grant no.
2015CB352502), National Natural Science Foundation (NSF) of China (grant nos.
61625301 and 61731018), Qual- comm, and Microsoft Research Asia.
Hongbin Zha is supported by Natural Science Foundation (NSF) of China (No. 61632003).
REFERENCES Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.
Learning phrase representations using RNN encoder-decoder for statistical machine translation.
arXiv:1406.1078, 2014.
Benoît Colson, Patrice Marcotte, and Gilles Savard.
An overview of bilevel optimization.
Annals of Operations Research, 153(1):235–256, 2007.
Tim Cooijmans, Nicolas Ballas, César Laurent, Ça˘glar Gülçehre, and Aaron Courville.
Recurrent batch normalization.
In ICLR, 2017.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David.
Binaryconnect: Training deep neural networks with binary weights during propagations.
In NIPS, pp.
3123–3131, 2015.
Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev.
Compressing deep convolutional networks using vector quantization.
arXiv:1412.6115, 2014.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton.
Speech recognition with deep recurrent neural networks.
In ICASSP, pp.
6645–6649.
IEEE, 2013.
Published as a conference paper at ICLR 2018 Yiwen Guo, Anbang Yao, Hao Zhao, and Yurong Chen.
Network sketching: Exploiting binary structure in deep cnns.
In CVPR, 2017.
Song Han, Jeff Pool, John Tran, and William Dally.
Learning both weights and connections for efﬁcient neural network.
In NIPS, pp.
1135–1143, 2015.
Song Han, Huizi Mao, and William J Dally.
Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding.
In ICLR, 2016.
Sepp Hochreiter and Jürgen Schmidhuber.
Long short-term memory.
Neural Computation, 9(8): 1735–1780, 1997.
Lu Hou, Quanming Yao, and James T Kwok.
Loss-aware binarization of deep networks.
In ICLR, 2017.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio.
Binarized neural networks.
In NIPS, pp.
4107–4115.
2016a.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio.
Quan- tized neural networks: Training neural networks with low precision weights and activations.
arXiv:1609.07061, 2016b.
Sergey Ioffe and Christian Szegedy.
Batch normalization: Accelerating deep network training by reducing internal covariate shift.
In ICML, pp.
448–456, 2015.
Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman.
Speeding up convolutional neural networks with low rank expansions.
arXiv:1405.3866, 2014.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization.
In ICLR, 2015.
Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, and Victor Lempitsky.
Speeding- up convolutional neural networks using ﬁne-tuned cp-decomposition.
arXiv:1412.6553, 2014.
Fengfu Li, Bo Zhang, and Bin Liu.
Ternary weight networks.
arXiv:1605.04711, 2016.
Zefan Li, Bingbing Ni, Wenjun Zhang, Xiaokang Yang, and Wen Gao.
Performance guaranteed network acceleration via high-order residual quantization.
In ICCV, pp.
2584–2592, 2017.
Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, and Marianna Pensky.
Sparse convolu- tional neural networks.
In CVPR, pp.
806–814, 2015.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini.
Building a large annotated corpus of english: The penn treebank.
Computational Linguistics, 19(2):313–330, 1993.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
Pointer sentinel mixture models.
In ICLR, 2017.
Tomáš Mikolov.
Statistical Language Models Based on Neural Networks.
PhD thesis, Brno University of Technology, 2012.
Tomáš Mikolov, Martin Karaﬁát, Lukáš Burget, Jan ˇCernocký, and Sanjeev Khudanpur.
Recurrent neural network based language model.
In INTERSPEECH, pp.
1045–1048, 2010.
Tomáš Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu, and Marc’Aurelio Ranzato.
Learning longer memory in recurrent neural networks.
arXiv:1412.7753, 2014.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi.
XNOR-Net: Imagenet classiﬁcation using binary convolutional neural networks.
In ECCV, pp.
525–542.
Springer, 2016.
Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran.
Low- rank matrix factorization for deep neural network training with high-dimensional output targets.
In ICASSP, pp.
6655–6659.
IEEE, 2013.
Karen Simonyan and Andrew Zisserman.
Very deep convolutional networks for large-scale image recognition.
In ICLR, 2015.
10 Published as a conference paper at ICLR 2018 Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In NIPS, pp.
3104–3112, 2014.
Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, and Weinan E.
Convolutional neural networks with low-rank regularization.
In ICLR, 2016.
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan.
Show and tell: A neural image caption generator.
In CVPR, pp.
3156–3164, 2015.
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks.
In NIPS, pp.
2074–2082, 2016.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.
Google’s neural machine translation system: Bridging the gap between human and machine translation.
arXiv:1609.08144, 2016.
Ziang Xie, Sida I Wang, Jiwei Li, Daniel Lévy, Aiming Nie, Dan Jurafsky, and Andrew Y Ng. Data noising as smoothing in neural network language models.
In ICLR, 2017.
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
Recurrent neural network regularization.
arXiv:1409.2329, 2014.
Shu-Chang Zhou, Yu-Zhi Wang, He Wen, Qin-Yao He, and Yu-Heng Zou.
Balanced quantization: An effective and efﬁcient approach to quantized neural networks.
Journal of Computer Science and Technology, 32(4):667–682, 2017.
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou.
Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients.
arXiv:1606.06160, 2016.
Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally.
Trained ternary quantization.
In ICLR, 2017.
11 Published as a conference paper at ICLR 2018 APPENDIX A BINARY MATRIX VECTOR MULTIPLICATION IN CPUS Table 6: Computing time of the binary matrix vector multiplication in CPUs, where Quant represents the cost to execute our alternating quantization on-line.
Weight Size W-Bits / A-Bits Total (ms) Quant (ms) Quant / Total Acceleration 4096 × 1024 42000 × 1024 2/2 3/3 FP/FP 2/2 3/3 FP/FP 0.35 0.72 1.95 3.17 6.46 19.10 0.07 0.11 0.07 0.11 20% 15% 2.2% 1.7% 5.6× 2.7× 1.0× 6.0× 3.0× 1.0× In this section, we discuss the implementation of the binary multiplication kernel in CPUs. The binary multiplication is divided into two steps: Entry-wise XNOR operation (corresponding to entry-wise product in the full precision multiplication) and bit count operation for accumulation (corresponding to compute the sum of all multiplied entries in the full precision multiplication).
We test it on Intel Xeon E5-2682 v4 @ 2.50 GHz CPU.
For the XNOR operation, we use the Single instruction, multiple data (SIMD) _mm256 _xor_ps, which can execute 256 bit simultaneously.
For the bit count operation, we use the function _popcnt64 (Note that this step can further be accelerated by the up-coming instruction _mm512 _popcnt_epi64 , which can execute 512 bits simultaneously.
Similarly, the XNOR operation can also be further accelerated by the up-coming _mm512 _xor_ps instruction to execute 512 bits simultaneously).
We compare with the much optimized Intel Math Kernel Library (MKL) on full precision matrix vector multiplication and execute all codes in the single-thread mode.
We conduct two scales of experiments: a matrix of size 4096× 1024 multiplying a vector of size 1024 and a matrix of size 42000 × 1024 multiplying a vector of size 1024, which respectively correspond to the hidden state product Whht−1 and the softmax layer Wsht for Text8 dataset during inference with batch size of 1 (See Eq. (6)).
The results are shown in Table 6.
We can see that our alternating quantization step only accounts for a small portion of the total executing time, especially for the larger scale matrix vector multiplication.
Compared with the full precision one, the binary multiplication can roughly achieve 6× acceleration with 2-bit quantization and 3× acceleration with 3-bit quantization.
Note that this is only a simple test on CPU.
Our alternating quantization method can also be extended to GPU, ASIC, and FPGA.
B IMAGE CLASSIFICATION Sequential MNIST.
As a simple illustration to show that our alternating quantization is not limited for texts, we conduct experiments on the sequential MNIST classiﬁcation task (Cooijmans et al., 2017).
The dataset consists of a training set of 60K and a test set of 10K 28 × 28 gray-scale images.
Here we divide the last 5000 training images for validation.
In every time, we sequentially use one row of the image as the input (28×1), which results in a total of 28 time steps.
We use 1 hidden layer’s LSTM of size 128 and the same optimization hyper-parameters as the language models.
Besides the weights and activations, the inputs are quantized.
The testing error rates for 1-bit input, 2-bit weight, and 2-bit activation are shown in 7, where our alternating quantized method still achieves plausible performance in this task.
MLP on MNIST.
The alternating quantization proposed in this work is a general technique.
It is not only suitable for RNNs, but also for feed-forward neural networks.
As an example, we ﬁrstly conduct a classiﬁcation task on MNIST and compare with existing work (Li et al., 2017).
The method proposed in (Li et al., 2017) is intrinsically a greedy multi-bit quantization method.
For fair comparison, we follow the same setting.
We use the MLP consisting of 3 hidden layers of 4096 units and an L2-SVM output layer.
No convolution, preprocessing, data augmentation or pre-training is 12 Published as a conference paper at ICLR 2018 Table 7: Testing error rate of LSTM on MNIST with 1-bit input, 2-bit weight, and 2-bit activation.
Methods Full Precision Reﬁned (Guo et al., 2017) Alternating (ours) Testing Error Rate 1.10 % 1.39 % 1.19 % Table 8: Testing error rate of MLP on MNIST with 2-bit input, 2-bit weight, and 1-bit activation.
Methods Full Precision Greedy (reported in (Li et al., 2017)) Reﬁned (Guo et al., 2017) Alternating (ours) Testing Error Rate 0.97 % 1.25 % 1.22 % 1.13 % Table 9: Testing error rate of CNN on CIFAR-10 with 2-bit weight and 1-bit activation.
Methods Full Precision (reported in (Hou et al., 2017)) XNOR-Net (1-bit weight & activation, reported in (Hou et al., 2017)) Reﬁned (Guo et al., 2017) Alternating (ours) Testing Error Rate 11.90 % 12.62 % 12.08 % 11.70 % used.
We also use ADAM (Kingma & Ba, 2015) with an exponentially decaying learning rate and Batch Normalization (Ioffe & Szegedy, 2015) with a batch size 100.
The testing error rates for 2-bit input, 2-bit weight, and 1-bit activation are shown in Table 8.
Among all the compared multi-bit quantization methods, our alternating one achieves the lowest testing error.
CNN on CIFAR-10.
We then conduct experiments on CIFAR-10 and follow the same setting as (Hou et al., 2017).
That is, we use 45000 images for training, another 5000 for validation, and the remaining 10000 for testing.
The images are preprocessed with global contrast normalization and ZCA whitening.
We also use the VGG-like architecture (Simonyan & Zisserman, 2015): (2 × 128 C3)−MP2−(2 × 256 C3)−MP2−(2 × 512 C3)−MP2−(2 × 1024 FC)−10 SVM where C3 is a 3 × 3 convolution layer, and MP2 is a 2 × 2 max-pooling layer.
Batch Normalization, with a mini-batch size of 50, and ADAM are used.
The maximum number of epochs is 200.
The learning rate starts at 0.02 and decays by a factor of 0.5 after every 30 epochs.
The testing error rates for 2-bit weight and 1-bit activation are shown in Table 9, where our alternating method again achieves the lowest test error rate among all compared quantization methods.
13
Deep Neural Network (DNN) architectures are promising solutions in achieving remarkable results in a wide range of machine learning applications, including, but not limited to computer vision, speech recognition, language modeling and autonomous cars.
Currently, there is a major growing trend in introducing more advanced DNN architectures and employing them in end-user ap- plications.
The considerable improvements in DNNs are usually achieved by increasing complexity which requires more compu- tational resources for training and inference.
Recent research di- rections to make this progress sustainable are: development of Graphical Processing Units (GPUs) as the vital hardware compo- nent of both servers and mobile devices [27], design of efficient algorithms for large-scale distributed training [7] and efficient in- ference [33], compression and approximation of models [38], and most recently introducing collaborative computation of cloud and fog as known as dew computing [36].
Using cloud servers for computation and storage is becoming extensively favorable due to technical advancements and improved accessibility.
Scalability, low cost, and satisfactory Quality of Service (QoS), made offloading to cloud the typical choice for computing intensive tasks.
On the other side, mobile-device are being equipped with more powerful general purpose CPUs and GPUs. Very recently there is a new trend in hardware companies to design dedicated chips to better tackle machine-learning tasks.
For example, Apple’s A11 Bionic chip [26] used in iPhone X uses a neural engine in its GPU to speed up DNN queries of applications such as face identification and facial motion capture [23].
There are currently two methods for DNN inference: mobile only and cloud only.
In simple models, a mobile device is responsible for performing all of the computation.
In case of complex models, the raw input data (image, video stream, voice, etc.) is uploaded and then computed on the cloud.
The results of the task are later downloaded to the device.
Besides the improvements of the mobiles devices mentioned ear- lier, the computational power of mobile devices are still considered significantly weaker than the cloud ones.
Therefore, mobile-only approach can cause large inference latency and failure in meeting QoS.
Moreover, embedded devices undergo major energy consump- tion constraints due to battery capacity limits.
On the other hand, cloud-only suffers communication overhead for uploading the raw data and downloading the outputs.
Moreover, slowdowns caused by service congestions, subscription costs, and network dependency should be considered as downsides of this approach.
The superiority and persistent improvement of DNNs is heavily dependent on providing huge amount of training data.
Typically, this data is collected from different resources and later fed into network for training.
The final model can then be delivered to dif- ferent devices for inference functions.
However, there is a trend of appearance of applications requiring adaptive learning in on- line environments, such as self driving cars and security drones [30][25].
Model parameters in these smart devices are constantly being changed based on their continuous interaction with surround- ings.
Complexity of these architectures with extended number of parameters and current cloud-only methods for DNN training, im- plies a constant communication cost and burden of increased power consumption for mobile device.
Automatic partitioning of computationally extensive tasks over the cloud for optimization of performance and energy consumption has been already well studied [2].
Most recently, scalable distributed hierarchy structures between end-user device, edge, and cloud have been suggested [39] which are specialized for DNN applications.
However, exploiting the layer granularity of DNN architectures for run time partitioning has not been studied throughly yet.
Figure 1: Different computation partitioning methods.
(a) Mobile only: computation is completely done on mobile de- vice.
(b) Cloud only: raw input data is sent to cloud, compu- tations is done on cloud and results are sent back to mobile device.
(c) JointDNN: DNN architecture is partitioned at the granularity of layers, each layer can be computed either on cloud or mobile.
In this work, we are investigating inference and training of DNNs in a joint platform of mobile and cloud as an alternatives to the current single-platform methods as illustrated in Figure 1.
Con- sidering DNN architectures as an ordered sequence of layers, and possibility of computation of every layer either on mobile or cloud, we can model the DNN structure as a directed acyclic graph (DAG).
The parameters of our real-time adaptive model are dependent on the following factors: mobile/cloud hardware and software re- sources, battery capacity, network specifications, and QoS.
Based on this modeling, we show that the problem of finding the optimal computation schedule for different scenarios, i.e. best performance or energy consumption, can be reduced to the polynomial time shortest path problem.
To present realistic results, we made experiments with real hard- wares as mobile device and cloud.
To model the communication between platform, we used different network technologies and the most recent reports on their specifications in the U.S. DNN architectures can be categorized based on functionality.
These differences enforce specific type and order of layers in archi- tecture, directly affecting the partitioning result in the collaborative method.
For discriminative models, used in recognition applica- tions, the layer size gradual decrease proceeding from input toward output 2.
This sequence suggests computation of the first few lay- ers on the mobile device to avoid excessive communication cost of uploading large raw input data.
On the other hand, growth of the layer size from input to output in generative models used for synthesizing new data, implies the possibility of uploading small input to the cloud and later downloading and computing the last layers on the mobile device for better efficiency.
Interesting mo- bile applications like image to image translation are implemented with autoencoder architectures, usually consisting of middle layers with smaller sizes compared to input and output.
Consequently we expect the first and last layers to be computed on the mobile de- vice in our collaborative approach.
We examined eight well-known DNN benchmarks selected from these categories to illustrate their differences in collaborative computation approach.
As we will see in Section ??, the communication between the mobile and cloud is the main bottleneck for both performance and energy in the collaborative approach.
We investigated the specific characteristics of CNN layer outputs and introduced a lossless com- pression method to reduce the communication costs.
Figure 2: Typical layer size architecture of (a) Discriminative (b) Autoencoder (c) Generative models.
State-of-the-art work for collaborative computation of DNNs [21] only considers one offloading point, assigning computation of its previous layers and next layers on the mobile and cloud platforms, respectively.
We show that this approach is non-generic and fails to be optimal, and introduced a new method granting the possibility of computation on either platforms for each layer independent of other layers.
Our evaluations show that JointDNN significantly improves the latency and energy up to 3× and 7× respectively com- pared to the status-quo single platform approaches without any compression.
The main contributions of this paper can be listed as: • Introducing a novel model for collaborative computation • Formulating the problem of optimal computation scheduling of DNNs at layer granularity in mobile cloud computing environment as shortest path problem and integer linear programming (ILP) • Examining compressibility of DNN layers and developing a lossless compression method to improve communication costs • Demonstrating the significant improvements of performance, mobile energy consumption, and cloud workload achieved by using JointDNN between the mobile and cloud 2 PROBLEM DEFINITION AND MODELING In this section, we explain the general architecture of DNN layers and our profiling method.
Moreover, we elaborate on how the cost optimization can be reduced to a shortest path problem by introducing the JointDNN graph model.
Finally, we show how the constrained problem is formulated by setting up ILP.
2.1 DNN Building Blocks DNNs are networks composed of several layers stacked to each other.
We briefly explain the functionality of each layers used in the state-of-the-art architectures: Convolution Layer (conv) consists of a set of filters with di- mensions relatively smaller than their input.
Each filter completely traverses through the input with a predefined step size and com- putes the dot product between it’s parameters and the correspond- ing part of the input.
This process creates different feature maps (referred to as channels) for different filters from the same input data.
This aspect of preserving the locality of input features has made Convolutional Neural Network (CNN) architectures the horse power of the state-of-the-art image classification models.
Because of dot product basis of conv, it can be formulated as General Matrix 5.6 ...
9.43.1 ...
2.2(a)(b)(c)(a)(b)(c)Input dataOutput dataMultiplication (GEMM), therefore capable of gaining performance improvement by using parallel computing devices (e.g. GPUs).
Fully Connected Layer (fc) is the main component of most regular neural networks in which every neuron is connected to all neurons of the previous layer.
This fully pairwise connection architecture comprises large portion of computation of the whole network.
Like conv, fc layer is also formulated as GEMM.
Pooling Layer (pool) performs a non-linear down sampling function over non-overlapping spatially local parts of input.
Max- pooling is the most common function used in this type of layer alongside other functions such as average or L2-norm pooling.
Activation Layer increases the non-linearity property of neu- ral network architectures.
This layer applies non-linear activation function on single data points of input to generate an output with the same size.
Among various non-linear functions, such as sigmoid and hyperbolic tangent, Rectified Linear Unit (relu) is currently the favorable choice in DNN architectures as it is simple and speeds up the tedious training process [10].
Local Response Normalization (lrn) performs local normal- ization by imposing a local competition for big activities between adjacent features in a channel, and also between features at the same spatial location in different channels.
lrn are inspired by in- hibition schemes observed in the brain helps with intention of generalization.
There are different formulations suggested for lrn, as shown in [19, 22] they may lead to slight improvements.
Dropout Layer (drop) As mentioned earlier, fc occupies most of the parameters of DNN models and thus vulnerable to overfitting.
Typically regularization methods are used to prevent overfitting by reducing high dependency of network on individual neurons during training.
In dropout [37] technique, at each training iteration every neurons can be removed (droped out) from network with a predetermined probability p or kept with probability 1 − p and the training is done on the remaining network.
The dropped out nodes will have their previous weight for the next training iteration.
Deconvolution Layer (deconv) also known as transposed con- volution is mostly used on generative and autoencoder models in applications such as building high-resolutions picture from low- resolution pictures and high-level descriptions.
The goal in decon- volution is to find f in the convolution equation of form f ∗ д = h.
In case of DNNs, д is the filter and f is the input of the convolu- tion [42].
Long Short-Term Memory Layer (lstm) is a building unit for layers of a recurrent neural network (RNN) and is widely used due to its promising results in speech recognition applications.
A typical LSTM unit is composed of a cell, an input gate, an output gate and a forget gate, which is responsible for remembering and forgetting specific values over arbitrary time intervals.
The whole LSTM unit can be thought as a typical artificial neuron, as in a feed-forward neural network.
Softmax (soft) is the last layer in multi-class architectures, usu- ally connected in a one-to-one correspondence way to a fc layer.
Softmax establishes a probability distribution by representing each class probability with a single neuron.
Figure 3: Latency of grouped and separated execution of con- volution operator.
2.2 Energy and Latency Profiling There are three methods in measuring the latency and energy con- sumption of each layer in neural networks: Statistical Modeling: In this method, a regression model over the configurable parameters of operators (e.g. filter size in convo- lution) can be used to estimate the associated latency and energy.
This method is prone to large error because of the inter-layer opti- mizations performed by DNN software packages.
Therefore, it is necessary to consider execution of several consecutive operators grouped with each other during profiling.
Many of these software packages are proprietary, making access to inter-layer optimization techniques impossible.
In order to illustrate this issue, we designed two experiments with 25 consecutive convolutions on NVIDIA Pascal™ GPU using cuDNN® library [1].
In the first experiment, we measure the latency of each convolution operator separately and set the total latency as sum of them.
In the second experiment, we group the convolutions together and measure the total latency.
All parameters are located on GPU’s memory in both experiments, avoiding any data transfer from the main memory to make sure results are exactly representing the actual computation latency.
As we see in Figure 3, there is a large error gap between separated and grouped execution experiments which grows as the number of convolutions is increased.
This observation confirms that we need to profile grouped operators to have more accurate estimations.
Considering various consecutive combination of operators and different input sizes, this method requires a very large number of measurements, not to mention the need for a complex regression model.
Analytical Modeling: To derive an analytical approach for es- timation of the latency and energy consumption, it is required to obtain the exact hardware and software specifications.
However, the state-of-the-art work in latency modeling of DNNs [31] fails to estimate layer-level delay within an acceptable error bound, for in- stance, underestimating the latency of a fully connected layer with 4096 neurons by around 900%.
Industrial developers do not reveal the detailed hardware architecture specifications and the propri- etary parallel computing architectures such as CUDA®, therefore, analytical approach could be quite challenging [15].
Application-specific Profiling: In this method, the DNN ar- chitecture of the application being used is profiled in run-time.
The number of applications in a mobile device using neural networks 0510152025Layer Number050100150Latency (ms)Separate executionGrouped executionFigure 4: Computation model in linear topology.
Figure 5: Graph representation of mobile cloud computing optimal scheduling problem for linear topology.
are generally limited.
In conclusion, this method is more feasi- ble, promising higher accuracy estimations.
We have chosen this method for estimation of energies and latencies in the experiments of this paper.
k =1 ωk.
(energy) of querying the DNN isn 2.3 JointDNN Graph Model First, we assume that a DNN is presented by a sequence of distinct layers with a linear topology as depicted in Figure 4.
Layers are executed sequentially, with output data generated by one layer feeds into the input of the next one.
We denote the input and output data sizes of kth layer as αk and βk, respectively.
Denoting the latency (energy) of layer k as ωk, where k = 1, 2, ..., n, the total latency The mobile cloud computing optimal scheduling problem can be reduced to a shortest path problem, from node S to F, in the graph of Figure 5.
Mobile Execution cost of the kth layer (C(MEk)) is the cost of executing the kth layer in the mobile while the cloud server is idle.
Cloud Execution cost of the kth layer (C(CEk)) is the executing cost of the kth layer in the cloud server while the mobile is idle.
Uploading the Input Data cost of the kth layer is the cost of uploading output data of the (k-1)th layer to the cloud server (U IDk).
Downloading the Input Data cost of the kth layer is the cost of downloading output data of the (k-1)th layer to the mobile (DIDk).
The costs can refer to either latency or energy.
However, as we showed in Section 2.2, the assumption of linear topology in DNNs is not true and we need to consider all the consecutive grouping of the layers in the network.
This fact suggests replacement of linear topology by a tournament graph as depicted in Figure 6.
We define the parameters of this new graph, JointDNN graph model, in Table 1.
In this graph, node Ci:j represents that the layers i to j are com- puted on the cloud server, while node Mi:j represents that the layers i to j are computed on the mobile device.
An edge between two adjacent nodes in JointDNN graph model is associated with four possible cases: 1) A transition from the mobile to the mobile, which only includes the mobile computation cost (MEi, j) 2) A transition from the cloud to the cloud, which only includes the cloud computa- tion cost (CEi, j) 3) A transition from the mobile to the cloud, which includes the mobile computation cost and uploading cost of the inputs of the next node (EUi, j = MEi, j + U IDj+1) 4) A transition from the cloud to the mobile, which includes the cloud compu- tation cost and downloading cost of the inputs of the next node (EDi, j = CEi, j +DIDj+1).
Under this formulation, we can transform the computation scheduling problem to finding the shortest path from S to F.
Residual networks are a class of powerful and easy-to-train ar- chitectures of DNNs [14].
In residual networks, as depicted in Figure 7 (a), the output of one layer is fed into another layer with distance of at least two.
Thus, we need to keep track of the source layer (node 2 in Figure 7) so as to know that this layer is computed on the mobile or the cloud.
Our standard graph model has a memory of one which is the very previous layer.
We provide a method to transform the computation graph of this type of network to our standard model, JointDNN graph.
In this regard, we add two additional chains of size k − 1, where k is the number of nodes in the residual block (3 in Figure 7).
One chain represents the case of computing layer 2 on the mobile and the other one represents the case of computing layer 2 on the cloud.
In Figure 7, we have only shown the weights that need to be modified, where D2 and U2 are the cost of downloading and uploading the output of layer 2, respectively.
By solving the shortest path problem in JointDNN graph model, we can obtain the optimal scheduling of inference in DNNs. Online training consists of one inference and one back-propagation step.
The total number of layers is noted by N consistently throughout this paper so there are 2N layers for modeling training, where the second N layers are the mirrored version of the first N layers, and their associated operations are the gradients of the error function with respect to the DNN’s weights.
The main difference between the mobile cloud computing graph of inference and online training is the need for updating the model by downloading the new weights from the cloud.
We assume that the cloud server performs the whole back-propagation step separately, even if it is scheduled to be done on the mobile, therefore, there is no need for mobile device to upload the weights that are updated by itself in order to save mobile energy consumption.
The modification in JointDNN graph Table 1: Parameter Definition of Graph Model Param.
CEi:j MEi:j EDi, j EUi, j ϕk Ωk Ψk Γk Πm Πc U1 Description of Cost Executing layers i to j on the cloud Executing layers i to j on the mobile CEi:j + DIDj MEi:j + U IDj All the following edges: ∀i = 1 : k − 1 EDi,k−1 All the following edges: ∀i = 1 : k − 1 MEi,k−1 All the following edges: ∀i = 1 : k − 1 EUi,k−1 All the following edges: ∀i = 1 : k − 1 CEi,k−1 All the following edges: ∀i = 1 : n MEi,n All the following edges: ∀i = 1 : n EDi,n Uploading the input of the first layer 12β1 = α2    α1kβ2 = α3βk-1 = αkn-1nβn-1 = αnαn-1βk = αk+1    βnω1 ω2ωkωn-1ωn 12C(CE1)    C(CE2)n-1nC(CEn-1)C(CEn-2)12C(ME1)    C(ME2)n-1nC(MEn-1)C(MEn-2)C(CEn)FSC(ME1)Figure 6: JointDNN graph model.
of size m ∈ {1, 2, .
.
.
, N}.
Thus, we will have N + (N − 1) + ...
+ 1 = N(N + 1)/2 (1) number of different profiling values for delay and energy.
Consider- ing layer i to layer j to be computed either on the mobile device or cloud server, we assign two binary variables mi, j and ci, j, respec- tively.
Download and upload communication delays needs to be added to the execution time, when switching from/to cloud to/from mobile, respectively.
+ ci, j .TcloudLi, j (2) mi, j .cj+1,k .TuploadLj ci, j .mj+1,k .TdownloadLj (3) model is adding the costs of downloading weights of the layers that are updated in the cloud to EDi, j.
efficiently.
The shortest path problem can be solved in polynomial time However, the problem of shortest path subjected to constraints has been shown to be NP-Complete [41].
For instance, assuming our standard graph is constructed for energy and we need to find the shortest path subject to the constraint of the total latency of that path being less than a time deadline (QoS).
However, there is an ap- proximation solution to this problem, "LARAC" algorithm [20], the nature of our application does not require to solve this optimization problem frequently, therefore, we aim to obtain the optimal solution.
We can constitute a small look-up table of optimization results for different set of parameters (e.g. network bandwidth, cloud server load, etc.).
We provide the ILP formulations of DNN partitioning in the following sections.
2.4 ILP Setup 2.4.1 Performance Efficient Computation Offloading ILP Setup for Inference.
We formulated the scheduling of inference in DNNs as an ILP with tractable number of variables.
In our method, first we profile the delay and energy consumption of consecutive layers Figure 7: (a) A residual building block (b) Transformation of a residual building block into shortest path problem.
n i =1 j=i i =1 n n n n n i =1 i =1 Tcomputation = Tcommunication = (mi, j .TmobileLi, j n n n n k =j+1 j=i j=i k =j+1 c1,i .TuploadLi ci,n .TdownloadLn i =1 TmobileLi, j Ttotal = Tcomputation + Tcommunication and TcloudLi, j and TuploadLi (4) represent the execution time of the ith layer to the jth layer on the mobile and cloud, respectively.
represent the latency of downloading TdownloadLi and uploading the output of the ith layer, respectively.
Consider- ing each set of the consecutive layers, whenever mi, j and one of {cj+1,k}k =j+1:n are equal to one, the output of the jth layer is up- loaded to the cloud.
The same argument applies to downloading.
We also note that the last two terms in Eq. 3 represent the condition by which the last layer is computed on the cloud and we need to download the output to the mobile device, and the first layer is computed on the cloud and we need to upload the input to the cloud, respectively.
To support for residual architectures, we need to add a pair of download and upload terms similar to the first two terms in Eq. 3 for the starting and ending layers of each residual block.
In order to guarantee that all layers are computed exactly once, we need to add the following set of constraints: M1:kk=1:nME1:kEU1,kM2:kk=2:nME2:kΩ1Φ1Mn-1:kk=n-1:nMEn-1:kΩn-2Φn-2Mn:kk=n:nMEn:kΩn-1Φn-1C1:kk=1:nCE1:kU1ED1,kC2:kk=2:nCE2:kΓ1ψ1Cn-1:kk=n-1:nCEn-1:kΓn-2ψn-2Cn:kk=n:nCEn:kΓn-1ψn-1ED2,kEDn-1,kEDn,kEU2,kEUn-1,kEUn,kSSFFΠcΠm.
.
.
.
.
..
.
.
.
.
.1234534234341234155(a)(b)m i =1 n j=m ∀m ∈ 1 : n : (mi, j + ci, j) = 1 (5) Because of the non-linearity of multiplication, an additional step is needed to transform Eq. 3 to the standard form of ILP.
We define two sets of new variables: EmobileLi, j and EcloudLi, j represent the amount of energy re- quired to compute the ith layer to the jth layer on the mobile and cloud, respectively.
EdownloadLi represent the energy required to download and upload the output of ith layer, respectively.
Similar to performance efficient ILP constraints, each layer should be executed exactly once: and EuploadLi ui, j = mi, j .
cj+1,k ∀m ∈ 1 : n : mi, j ≤ 1 (11) m n i =1 j=m n n k =j+1 k =j+1 di, j = ci, j .
mj+1,k with the following constraints: (6) (7) The ILP problem can be solved for different set of parameters (e.g. different uplink and download speeds), and then the scheduling re- sults can be stored as a look-up table in the mobile device.
Moreover because the number of variables in this setup is tractable solving ILP is quick.
For instance, solving ILP for AlexNet takes around 0.045 seconds on Intel(R) Core(TM) i7-3770 CPU with MATLAB®’s intlinprog() function using primal simplex algorithm.
2.4.3 Performance Efficient Computation Offloading ILP Setup for Training.
The ILP formulation of online training phase is very similar to that of inference.
In online training we have 2N layers instead of N obtained by mirroring the DNN, where the second N layers are backward propagation.
Moreover, we need to download the weights that are updated in the cloud to the mobile.
We assume that the cloud server always has the most updated version of the weights and does not require the mobile device to upload the up- dated weights.
The following terms need to be added for the ILP setup of training: Tcomputation = + ci, j .TcloudLi, j (12) 2n i =1 j=i 2n 2n 2n n 2n i =1 i =1 i =1 (mi, j .TmobileLi, j 2n 2n 2n 2n k =j+1 j=i j=i k =j+1 c1,i .TuploadLi 2n ci, j .TdownloadWi i =n+1 j=i mi, j .cj+1,k .TuploadLj ci, j .mj+1,k .TdownloadLj Tcommunication = Ttotal = Tcomputation + Tcommunication 2.4.4 Energy Efficient Computation Offloading ILP Setup for (13) (14) k =j+1 ui, j ≤ mi, j ui, j ≤ n n di, j ≤ n n mi, j + di, j ≤ ci, j ci, j + k =j +1 k =j+1 cj+1,k cj+1,k − ui, j ≤ 1 mj+1,k mj+1,k − di, j ≤ 1 k =j+1 orn will take value one if both binary variables, mi, j andn The first two constraints ensure that ui, j will be zero if either mi, j l =j+1 cj+1,l are zero.
The third inequality guarantees that ui, j l =j+1 cj+1,l , are set to one.
The same reasoning works for di, j.
In summary, the total number of variables in our ILP formulation will be 4N(N +1)/2, where N is total number of layers in the network.
2.4.2 Energy Efficient Computation Offloading ILP Setup for Inference.
Because of the nature of the application, we only care about the energy consumption on the mobile side.
We formulate ILP as follows: Ecomputation = Ecommunication = j=i i =2 i =1 n n n n n−1 n + ( n + ( n i =1 i =1 j=i j=i i =1 mi, j .EmobileLi, j mi, j .EdownloadLi mi, j .EuploadLj (8) (9) (1 − m1,i) − (n − 1)).EuploadL1 (1 − mi,n) − (n − 1)).EdownloadLn Etotal = Ecomputation + Ecommunication Training.
Ecomputation = (10) 2n 2n i =1 j=i mi, j .EmobileLi, j (15) Ecommunication = mi, j .EdownloadLi mi, j .EuploadLj j =i i =2 2n 2n 2n 2n−1 2n 2n i =1 i =1 j=i 2n + ( (1 − m1,i) − (2n − 1)).EuploadL1 defined for ILP as listed below: j=i + ( i =n+1 (1 − mi, j) − (n − 1)).EdownloadWi (16) (17) Etotal = Ecomputation + Ecommunication Scenarios.
There can be different optimization scenarios 2.4.5 • Performance efficient computation: In this case, it is suf- ficient to solve the ILP formulation for performance efficient computation offloading.
• Energy efficient computation: In this case, it is sufficient to solve the ILP formulation for energy efficient computation offloading.
• Battery budget limitation: In this case, based on the avail- able battery, the operating system can decide to dedicate a specific amount of energy consumption to each application.
By adding the following constraint to the performance effi- cient ILP formulation, our framework would adapt to battery limitations: Ecomputation + Ecommunication ≤ Eubound (18) • Cloud limited resources: In the presence of cloud server congestion or limitations on user’s subscription, we can ap- ply execution time constraints to each application to alleviate the server load: n n i =1 j=i ci, j .TcloudLi, j ≤ Tubound (19) • QoS: In this scenario, we minimize the required energy con- sumption while meeting a specified deadline: min{Ecomputation + Ecommunication} Tcomputation + Tcommunication ≤ TQoS (20) This constraint could be applied to both energy and perfor- mance efficient ILP formulations.
3 EVALUATION 3.1 Deep Architecture Benchmarks Since the architecture of neural networks depends on the type of the application, we have chosen three common application types of DNNs: (1) Discriminative neural networks are a class of models in machine learning for modeling the conditional probabil- ity distribution P(y|x).
This class generally is used in clas- sification and regression tasks.
AlexNet[22], OverFeat[34], Algorithm 1: JointDNN engine optimal scheduling of DNNs 1 function JointDNN (N , Li , Di , N B, N P); Input :1: N : number of layers in the DNN 2: Li|i = 1 : N : layers in the DNN 3: Di|i = 1 : N : data size at each layer 4: N B: mobile network bandwidth 5: N P: mobile network uplink and downlink power consumption Output:Optimal schedule of DNN end for j = 0; j < N ; j = j + 1 do Latencyi, j , Enerдyi, j = ProfileGroupedLayers(i, j); 2 for i = 0; i < N ; i = i + 1 do 6 end 7 G,S,F = ConstructShortestPathGraph(N ,Li,Di,N B,N P) //S and F are start and finish nodes and G is the JointDNN graph model schedule = ShortestPath(G,S,F) 8 if no constraints then 10 else 11 12 13 14 15 16 n i =1n if Battery Limited Constraint then Ecomm + Ecomp ≤ Eubound schedule = PerformanceEfficientILP(N ,Li,Di,N B,N P) end if Cloud Server Contraint then j=i ci, j .TcloudLi, j ≤ Tubound schedule = PerformanceEfficientILP(N ,Li,Di,N B,N P) Tcomm + Tcomp ≤ TQoS schedule = EnergyEfficientILP(N ,Li,Di,N B,N P) end if QoS then 17 18 19 20 21 end 22 23 24 end 25 return schedule; VGG16[35], Deep Speech[13], ResNet[14], and NiN[24] are well-known discriminative models we use as benchmarks in this experiment.
Except Deep Speech, used for speech recog- nition, all other benchmarks are used in image classification tasks.
(2) Generative neural networks model the joint probability distribution P(x, y), allowing generation of new samples.
These networks have applications in Computer Vision [11] and Robotics [9], which can be deployed on a mobile device.
Chair [8] is a generative model we use as benchmark in this work.
(3) Autoencoders are another class of neural networks used to learn a representation for a data set.
Their applications are image reconstruction, image to image translation, and denoising to name a few.
Mobile robots can be equipped with autoencoders to be used in their computer vision tasks.
We use Pix2Pix [18], as a benchmark from this class.
Figure 8: Latency and energy improvements for different batch sizes during inference.
Table 2: Benchmark Specifications Type Discriminative Generative Autoencoder Model AlexNet OverFeat Deep Speech ResNet VGG16 NiN Chair Pix2Pix Layers 21 14 10 70 37 29 10 32 Table 3: Mobile networks specifications in the U.S. Param.
Download speed (Mpbs) Upload speed (Mbps) αu (mW/Mpbs) αd (mW/Mpbs) β (mW) 3G 2.0275 1.1 868.98 122.12 817.88 4G Wi-Fi 54.97 13.76 18.88 5.85 283.17 438.39 137.01 51.97 1288.04 132.86 3.2 Mobile and Server Setup We used Jetson TX2 module developed by NVIDIA® [3], a fair representative of mobile computation power as our mobile device.
This module enables efficient implementation of DNN applications used in products such as robots, drones, and smart cameras.
It is equipped with NVIDIA Pascal®GPU with 256 CUDA cores and a shared 8 GB 128 bit LPDDR4 memory between GPU and CPU.
To measure the power consumption of the mobile platform, we used INA226 power sensor [17].
NVIDIA® Tesla® K40C [4] with 12 GB memory serves as our server GPU.
The computation capability of this device is more than one order of magnitude compared to our mobile device.
3.3 Communication Parameters To model the communication between platforms, we used the av- erage download and upload speed of mobile Internet [28, 29] for different networks (3G, 4G and Wi-Fi) as shown in Table 3.
The communication power for download (Pd) and upload (Pu) is dependent on the network throughput (td and tu).
Comprehensive examinations in [16] indicates that uplink and downlink power can be modeled with linear equations (Eq. 21) fairly accurate with less than 6% error rate.
Table 3 shows the parameter values of this equation for different networks.
Pu = αutu + β Pd = αd td + β (21) 4 RESULTS The latency and energy improvements of inference and online training with our engine for 8 different benchmarks are shown in 123456789100.0%25.0%50.0%Latency improvement (%)Pix2Pix, Performance Efficient Inference1234567891040.0%60.0%80.0%Energy improvement (%)Pix2Pix, Energy Efficient Inference12345678910-10.0%0.0%10.0%Deep Speech, Performance Efficient Inference12345678910-10.0%0.0%10.0%Deep Speech, Energy Efficient Inference123456789100.0%25.0%50.0%VGG16, Performance Efficient Inference123456789100.0%25.0%50.0%VGG16, Energy Efficient Inference123456789100.0%20.0%NiN, Performance Efficient Inference12345678910-10.0%0.0%10.0%NiN, Energy Efficient Inference12345678910-10.0%0.0%10.0%Latency improvement (%)Chair, Performance Efficient Inference12345678910Batch size-10.0%0.0%10.0%Energy improvement (%)Chair, Energy Efficient Inference123456789100.0%50.0%Overfeat, Performance Efficient Inference12345678910Batch size0.0%50.0%Overfeat, Energy Efficient Inference3G4GWi-Fi123456789100.0%50.0%AlexNet, Performance Efficient Inference12345678910Batch size0.0%50.0%AlexNet, Energy Efficient Inference123456789100.0%20.0%ResNet, Performance Efficient Inference12345678910Batch size0.0%25.0%50.0%ResNet, Energy Efficient InferenceFigure 9: Latency and energy improvements for different batch sizes during training.
Figures 8 and 9, respectively.
We considered the best case of mobile- only and cloud-only as our baseline.
JointDNN can achieve up to 66% and 86% improvements in latency and energy consumption, re- spectively during inference.
Communication cost increases linearly with batch size while this is not the case for computation cost and it grows with much lower rate, as depicted in 10(b).
Therefore, a key observation is that as we increase the batch size, the mobile-only approach becomes more preferable.
During online training, the huge communication overhead of transmitting the updated weights will be added to the total cost.
Therefore, in order to avoid downloading this large data, only a few back-propagation steps are computed in the cloud server.
We per- formed a simulation by varying the percentage of updated weight.
As the percentage of updated weights increases, the latency and energy consumption becomes constant which is shown in Figure 10.
This is the result of the fact that all the back-propagations will be performed on the mobile device and weights are not transfered from the cloud to the mobile.
JointDNN can achieve improvements up to 73% in latency and 56% in energy consumption during inference.
Different patterns of scheduling are demonstrated in Figure 11.
They represent the optimal solution in Wi-Fi network while opti- mizing for latency.
They show how the computations in DNN is divided between the mobile and the cloud.
As it can be seen, dis- criminative models (e.g. AlexNet), inference follows a mobile-cloud pattern and training follows a mobile-cloud-mobile pattern.
The intuition is that the last layers are computationally intensive (fc) with small data sizes, which require a low communication cost, therefore, last layers tend to be computed on the cloud.
For gen- erative models (e.g. Chair), the execution schedule of inference is Figure 10: (a) Latency of one epoch of online training using JointDNN algorithm vs percentage of updated weights (b) La- tency of mobile-only inference vs.
batch size.
123456789100.0%20.0%40.0%Latency improvement (%)Pix2Pix, Performance Efficient Training1234567891020.0%40.0%Energy improvement (%)Pix2Pix, Energy Efficient Training123456789100.0%25.0%50.0%Deep Speech, Performance Efficient Training123456789100.0%50.0%Deep Speech, Energy Efficient Training123456789100.0%25.0%50.0%VGG16, Performance Efficient Training123456789100.0%25.0%50.0%VGG16, Energy Efficient Training123456789100.0%20.0%NiN, Performance Efficient Training12345678910-10.0%0.0%10.0%NiN, Energy Efficient Training123456789100.0%50.0%Latency improvement (%)Chair, Performance Efficient Training12345678910Batch size0.0%20.0%Energy improvement (%)Chair, Energy Efficient Training123456789100.0%25.0%50.0%Overfeat, Performance Efficient Training12345678910Batch size0.0%25.0%50.0%Overfeat, Energy Efficient Training3G4GWi-Fi123456789100.0%20.0%40.0%AlexNet, Performance Efficient Training12345678910Batch size0.0%25.0%50.0%AlexNet, Energy Efficient Training123456789100.0%20.0%40.0%ResNet, Performance Efficient Training12345678910Batch size0.0%20.0%40.0%ResNet, Energy Efficient Training020406080100Percentage of the weights updated during backpropagation(a)020004000Latency (ms)12345678910Batch size during inference(b)0100020003000Latency (ms)Pix2PixDeep SpeechVGG16NiNChairOverfeatAlexNetResNetFigure 11: Interesting schedules of execution for three types of DNN architectures.
the opposite of discriminative networks, in which the last layers are generally huge and in the optimal solution they are computed on the mobile.
Lastly, for autoencoders, where both the input and output data sizes are large, the first and last layers are computed on the mobile.
JointDNN pushes some parts of the computations toward the mobile device.
As a result this will lead to less workload on the cloud server.
As we see in Table 4, we can reduce the cloud server’s workload up to 84% and 53% on average, which enables the cloud provider to service more users, while obtaining higher performance and lower energy consumptions compared to single-platform ap- proaches.
Table 4: Workload reduction of the cloud server in different mobile networks Optimization Target Latency Energy 3G (%) 84 73 49 49 4G (%) Wi-Fi (%) 12 51 4.1 Communication Dominance Execution time and energy breakdown for AlexNet, which is noted as a representative for the state-of-the-art architectures deployed in cloud servers, is depicted in Figure 12.
The cloud-only approach is dominated by the communication costs.
As demonstrated in Figure 12, 99%, 93% and 81% of the total execution time is used for communication in case of 3G, 4G, and Wi-Fi, respectively.
This relative portion also applies to energy consumption.
Comparing the latency and energy of the communication to those of mobile- only approach, we notice that mobile-only approach for AlexNet is better than the cloud-only approach in all the mobile networks.
We apply lossless compression methods in order to reduce the effect of the communication, which will be covered in the next section.
4.2 Layer Compression The preliminary results of our experiments show that more than 75% of the total energy and delay cost in DNNs are caused by communication in the collaborative approach.
This cost is directly proportional to the size of the layer being downloaded to or up- loaded from the mobile device.
Because of the complex feature extraction process of DNNs, the size of some of the intermediate layers are even larger than network’s input data.
For example, this ratio can go as high as 10× in VGG16.
To address this bottleneck, Figure 12: (a) Execution time of AlexNet optimized for per- formance (b) Mobile energy consumption of AlexNet opti- mized for energy (c) Data size of the layers in AlexNet and the scheduled computation, where the first nine layers are computed on the mobile and the rest on the cloud, which is the optimal solution w.r.t. both performance and energy.
we investigated compression of the data before any communica- tion.
This process can be applied to different DNN architecture types; however, we only considered CNNs due to their specific characteristics explained later in details.
CNN architectures are mostly used for image and video recogni- tion applications.
Because of the spatially local preservation char- acteristics of conv layers, we can assume that the output of the first convolution layers are following the same structure as the Figure 13: Layer output after passing the input image through conv, relu and lrn.
Channels are preserving the gen- eral structure of the input image and large ratio of the out- put data is black (zero) due to existence of relu.
Tiling is used to put all 96 channels together.
10 AlexNet, Wi-Fi, Latency Efficient, TrainingMobileCloudChair, Wi-Fi, Latency Efficient, TrainingPix2Pix, Wi-Fi, Latency Efficient, TrainingAlexNet, Wi-Fi, Latency Efficient, InferencePix2Pix, Wi-Fi, Latency Efficient, InferenceChair, Wi-Fi, Latency Efficient, InferenceMobile-onlyCloud-only (3G)JointDNN (3G)Cloud-only (4G)JointDNN (4G)Cloud-only (Wi-Fi)JointDNN (Wi-Fi)(a)02004006008001000Latency (ms)Mobile computationCloud computationCommunicationMobile-onlyCloud-only (3G)JointDNN (3G)Cloud-only (4G)JointDNN (4G)Cloud-only (Wi-Fi)JointDNN (Wi-Fi)(b)02004006008001000Energy (mJ)Mobile computationCommunicationdataconv1relu1lrn1pool1conv2relu2lrn2pool2conv3relu3conv4relu4conv5relu5pool5fc6relu6fc7relu7fc8softmax(c)0100K200KData size (bytes)Executed on the mobileExecuted on the cloudinput image, as shown in Figure 13.
Moreover, a big ratio of layer outputs are expected to be zero due to the presence of the relu layer.
Our observations shows that the ratio of neurons equal to zero (ZR) varies from 50% to 90% after relu in CNNs. These two characteristics, layers being similar to the input image, and large proportion of their data being a single value, suggest that we can employ existing image compression techniques to their output.
There are two general categories of compression techniques, lossy and lossless [5].
In lossless techniques it is possible to recon- struct the original information completely.
On the contrary, lossless techniques use approximations and the original data cannot be reconstructed.
In our experiments, we examined the impact of com- pression using PNG, a lossless technique, based on encoding of frequent sequences in an image.
Even though the data type of DNN parameters in typical im- plementations are 32-bits floating-points, most image formats are based on 3-bytes RGB color triples.
Therefore, to compress the layer in the same way as 2D pictures, the floating-point data should be quantized into 8-bits fixed-point.
Recent studies show representing the parameters of DNNs with only 4-bits affect the accuracy not more than 1% [38].
In this work, we implemented our architectures with 8-bits fixed-point and presented our baseline without any compression.
The layers of CNN contain numerous channels of 2D matrices, each similar to an image.
A simple method is to compress each channel separately.
In addition to extra overhead of file header for each channel, this method will not take the best of the frequent sequence decoding of PNG.
One alternative is locating different channels side by side, referred to as tiling, to form a large 2D matrix representing one layer as shown in Figure 13.
It should be noted that 1D fc layers are very small and we did not apply compression on them.
The Compression Ratio (CR) is defined as the ratio of the size of the layer (8-bit) to the size of the compressed 2D matrix in PNG.
Looking at the results of compression for two different CNN archi- tectures in Figure 14, we can observe a high correlation between ratio of pixels being zero (ZR) and CR.
PNG can compress the layer data up to 5.8× and 3.5× by average.
These results confirm the ef- fectiveness of the proposed compression method.
By replacing the compressed layers output and adding the cost of compression pro- cess itself in JointDNN formulations, we achieve an extra 4.9× and 4.6× improvements in energy and latency on average, respectively.
5 RELATED WORK AND COMPARISON General Task Offloading Frameworks.
There are existing prior arts focusing on offloading computation from the mobile to the cloud[2, 6, 12, 32, 40, 43].
However, all these frameworks share a limiting feature that makes them impractical for computation partitioning of the DNN applications.
These frameworks are programmer annotations dependent as they make decisions about pre-specified functions, whereas JointDNN makes scheduling decisions based on the model topology and mo- bile network specifications in run-time.
Offloading in function level, cannot lead to efficient partition decisions due to layers of a given type within one architecture can have significantly different compu- tation and data characteristics.
For instance, a specific convolution 11 Figure 14: Compression Ratio (CR) and ratio of zero val- ued neurons (ZR) for different layers of (a) AlexNet and (b) VGG16.
layer structure can be computed on mobile or cloud in different models in the optimal solution.
Neurosurgeon is the only prior art exploring a similar computa- tion offloading idea in DNNs between the mobile device and the cloud server at layer granularity.
Neurosurgeon assumes that there is only one data transfer point and the execution schedule of the efficient solution starts with mobile and then switches to the cloud, which performs the whole rest of the computations.
Our results show this is not true especially for online training, where the opti- mal schedule of execution often follows the mobile-cloud-mobile pattern.
Moreover, generative and autoencoder models follow a multi data transfer points pattern.
Also, the execution schedule can start with the cloud especially in case of generative models where the input data size is large.
Furthermore, inter-layer optimizations performed by DNN libraries are not considered in Neurosurgeon.
Moreover, Neurosurgeon only schedules for optimal latency and en- ergy, while JointDNN adapts to different scenarios including battery limitation, cloud server congestion, and QoS.
Lastly, Neurosurgeon only targets simple CNN and ANN models, while JointDNN utilizes a graph based approach to handle more complex DNN architectures like ResNet and RNNs. 6 CONCLUSIONS In this paper, we demonstrated that the status-quo approaches, cloud-only or mobile-only, are not optimal with regard to latency and energy.
We reduced the problem of partitioning the compu- tations in a DNN to shortest path problem in a graph.
Adding constraints to the shortest path problem makes it NP-Complete, therefore, we also provided ILP formulations to cover different pos- sible scenarios of limitations of mobile battery, cloud congestion, and QoS.
One can solve this problem for different set of parameters beforehand (e.g. network bandwidth, cloud server load, etc.) and use a look-up table accordingly to avoid the overhead of solving conv1relu1lrn1maxpool1conv2relu2lrn2maxpool2conv3relu3conv4relu4conv5relu5maxpool5(a)0123456Compression Ratioconv1_1conv1_2pool1conv2_1conv2_2pool2conv3_1conv3_2conv3_3pool3conv4_1conv4_2conv4_3pool4conv5_1conv5_2conv5_3pool5(b)0246Compression RatioCRZR020406080Zero Portion(%)020406080Zero Portion(%)the optimization problem.
The output data size in discriminative networks is typically smaller than other layers in the network, therefore, last layers are expected to be computed on the cloud, while first layers are expected to be computed on the mobile.
A reverse reasoning works for Generative models.
Autoencoders have large input and output data sizes, which implies that the first and last layers are expected to be computed on the mobile.
With these insights, the execution schedule of DNNs can possibly have various patterns depending on the model architecture.
ACKNOWLEDGMENTS This research was supported by grants from NSF SHF and DARPA MTO.
REFERENCES [1] Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, and others.
2014.
cuDNN: Efficient Primitives for Deep Learning.
CoRR abs/1410.0759 (2014).
arXiv:1410.0759 http://arxiv.org/abs/1410.0759 [2] Byung-Gon Chun, Sunghwan Ihm, Petros Maniatis, Mayur Naik, and Ashwin Patti.
2011.
CloneCloud: Elastic Execution Between Mobile Device and Cloud.
(2011), 301–314.
[3] Nvidia Corporation.
2018.
Jetson TX2 Module.
https://developer.nvidia.com/ embedded/buy/jetson-tx2.
(2018).
[Online; accessed 15-January-2018].
[4] Nvidia Corporation.
2018.
TESLA DATA CENTER GPUS FOR SERVERS.
http:// www.nvidia.com/object/tesla-servers.html.
(2018).
[Online; accessed 15-January- 2018].
[5] Thomas M.
Cover and Joy A.
Thomas.
2006.
Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing).
Wiley-Interscience.
[6] Eduardo Cuervo, Aruna Balasubramanian, Dae-ki Cho, and others.
2010.
MAUI: Making Smartphones Last Longer with Code Offload.
(2010), 49–62.
https: //doi.org/10.1145/1814433.1814441 [7] Jeffrey Dean, Greg S.
Corrado, Rajat Monga, and others.
2012.
Large Scale Distributed Deep Networks.
In Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1 (NIPS’12).
Curran Associates Inc., USA, 1223–1231.
http://dl.acm.org/citation.cfm?id=2999134.2999271 [8] Alexey Dosovitskiy, Jost Tobias Springenberg, and Thomas Brox.
2014.
Learning to Generate Chairs with Convolutional Neural Networks.
CoRR abs/1411.5928 (2014).
arXiv:1411.5928 http://arxiv.org/abs/1411.5928 [9] Chelsea Finn and Sergey Levine.
2016.
Deep Visual Foresight for Planning Robot Motion.
CoRR abs/1610.00696 (2016).
arXiv:1610.00696 [10] Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011.
Deep Sparse Rectifier Neural Networks.
In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (Proceedings of Machine Learning Research), Vol.
15.
PMLR, Fort Lauderdale, FL, USA, 315–323.
http://proceedings.mlr.press/ v15/glorot11a.html [11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, and others.
2014.
Gener- ative Adversarial Nets.
In Advances in Neural Information Processing Systems 27, Z.
Ghahramani, M.
Welling, C.
Cortes, N.
D.
Lawrence, and K.
Q.
Wein- berger (Eds.).
Curran Associates, Inc., 2672–2680.
http://papers.nips.cc/paper/ 5423-generative-adversarial-nets.pdf [12] Mark S.
Gordon, D.
Anoushe Jamshidi, Scott Mahlke, Z.
Morley Mao, and Xu Chen.
2012.
COMET: Code Offload by Migrating Execution Transparently.
(2012), 93–106.
http://dl.acm.org/citation.cfm?id=2387880.2387890 [13] Awni Y.
Hannun, Carl Case, Jared Casper, and others.
2014.
Deep Speech: Scaling up end-to-end speech recognition.
CoRR abs/1412.5567 (2014).
arXiv:1412.5567 http://arxiv.org/abs/1412.5567 [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
2015.
Deep Residual Learning for Image Recognition.
CoRR abs/1512.03385 (2015).
arXiv:1512.03385 http://arxiv.org/abs/1512.03385 [15] Sunpyo Hong and Hyesoon Kim.
2010.
An Integrated GPU Power and Per- formance Model.
SIGARCH Comput.
Archit.
News 38, 3 (June 2010), 280–289.
https://doi.org/10.1145/1816038.1815998 [16] Junxian Huang, Feng Qian, Alexandre Gerber, and others.
2012.
A Close Ex- amination of Performance and Power Characteristics of 4G LTE Networks.
In Proceedings of the 10th International Conference on Mobile Systems, Applications, and Services (MobiSys ’12).
ACM, New York, NY, USA, 225–238.
[17] Texas Instruments Incorporated.
2018.
INA Current/Power Monitor.
http://www.
ti.com/product/INA226.
(2018).
[Online; accessed 15-January-2018].
[18] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A.
Efros.
2016.
Image-to- Image Translation with Conditional Adversarial Networks.
CoRR abs/1611.07004 (2016).
arXiv:1611.07004 http://arxiv.org/abs/1611.07004 12 [19] K.
Jarrett, K.
Kavukcuoglu, M.
Ranzato, and Y.
LeCun.
2009.
What is the best multi-stage architecture for object recognition?.
In 2009 IEEE 12th International Conference on Computer Vision.
2146–2153.
[20] A.
Juttner, B.
Szviatovski, I.
Mecs, and Z.
Rajko.
2001.
Lagrange relaxation based method for the QoS routing problem.
2 (2001), 859–868 vol.2. [21] Yiping Kang, Johann Hauswald, Cao Gao, and others.
2017.
Neurosurgeon: Col- laborative Intelligence Between the Cloud and Mobile Edge.
In Proceedings of the Twenty-Second International Conference on Architectural Support for Program- ming Languages and Operating Systems (ASPLOS ’17).
ACM, New York, NY, USA, 615–629.
https://doi.org/10.1145/3037697.3037698 [22] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
2012.
ImageNet Classifi- cation with Deep Convolutional Neural Networks.
In Advances in Neural Infor- mation Processing Systems 25, F.
Pereira, C.
J.
C.
Burges, L.
Bottou, and K.
Q.
Wein- berger (Eds.).
Curran Associates, Inc., 1097–1105.
http://papers.nips.cc/paper/ 4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf abs/1312.4400 (2013).
arXiv:1312.4400 http://arxiv.org/abs/1312.4400 [23] Hao Li, Jihun Yu, Yuting Ye, and Chris Bregler.
2013.
Realtime Facial Animation with On-the-fly Correctives.
ACM Trans.
Graph.
32, 4, Article 42 (July 2013), 10 pages.
[24] Min Lin, Qiang Chen, and Shuicheng Yan.
2013.
Network In Network.
CoRR [25] Mahdi Nazemi, Amir Erfan Eshratifar, and Massoud Pedram.
2018.
A Hardware- Friendly Algorithm for Scalable Training and Deployment of Dimensionality Reduction Models on FPGA.
In Proceedings of the 19th IEEE International Sympo- sium on Quality Electronic Design.
[26] Apple Newsroom.
2017.
The future is here: iPhone X.
https://www.apple.com/ newsroom/2017/09/the-future-is-here-iphone-x/.
(2017).
[Online; accessed 15-January-2018].
[27] Kyoung-Su Oh and Keechul Jung.
2004.
GPU implementation of neural networks.
37 (06 2004), 1311–1314.
[28] OpenSignal.com.
2017.
State of Mobile Networks: USA.
https://opensignal.com/ [Online; accessed reports/2017/08/usa/state-of-the-mobile-network.
(2017).
15-January-2018].
[29] OpenSignal.com.
2017.
United States Speedtest Market Report.
http://www.
speedtest.net/reports/united-states/.
(2017).
[Online; accessed 15-January-2018].
[30] Yunpeng Pan, Ching-An Cheng, Kamil Saigol, and others.
2017.
Agile Off- Road Autonomous Driving Using End-to-End Deep Imitation Learning.
CoRR abs/1709.07174 (2017).
arXiv:1709.07174 http://arxiv.org/abs/1709.07174 [31] Hang Qi, Evan R.
Sparks, and Ameet Talwalkar.
2017.
Paleo: A Performance Model for Deep Neural Networks.
(2017).
[32] Moo-Ryong Ra, Anmol Sheth, Lily Mummert, and others.
2011.
Odessa: Enabling Interactive Perception Applications on Mobile Devices.
(2011), 43–56.
https: //doi.org/10.1145/1999995.2000000 [33] M.
S.
Razlighi, M.
Imani, F.
Koushanfar, and T.
Rosing.
2017.
LookNN: Neural network with no multiplication.
In Design, Automation Test in Europe Conference Exhibition (DATE), 2017.
1775–1780.
https://doi.org/10.23919/DATE.2017.7927280 [34] Pierre Sermanet, David Eigen, Xiang Zhang, and others.
2013.
OverFeat: Inte- grated Recognition, Localization and Detection using Convolutional Networks.
CoRR abs/1312.6229 (2013).
arXiv:1312.6229 http://arxiv.org/abs/1312.6229 [35] Karen Simonyan and Andrew Zisserman.
2014.
Very Deep Convolutional Networks for Large-Scale Image Recognition.
CoRR abs/1409.1556 (2014).
arXiv:1409.1556 http://arxiv.org/abs/1409.1556 [36] Karolj Skala, Davor Davidovic, Enis Afgan, Ivan Sovic, and Zorislav Sojat.
2015.
Scalable Distributed Computing Hierarchy: Cloud, Fog and Dew Computing.
Open Journal of Cloud Computing (OJCC) 2, 1 (2015), 16–24.
http://nbn-resolving.
de/urn:nbn:de:101:1-201705194519 [37] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
2014.
Dropout: A Simple Way to Prevent Neural Networks from Overfitting.
J.
Mach.
Learn.
Res.
15, 1 (Jan.
2014), 1929–1958.
http://dl.acm.org/ citation.cfm?id=2627435.2670313 [38] Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel S.
Emer.
2017.
Efficient Processing of Deep Neural Networks: A Tutorial and Survey.
CoRR abs/1703.09039 (2017).
arXiv:1703.09039 http://arxiv.org/abs/1703.09039 [39] Surat Teerapittayanon, Bradley McDanel, and H.
T.
Kung.
2017.
Distributed Deep Neural Networks over the Cloud, the Edge and End Devices.
CoRR abs/1709.01921 (2017).
arXiv:1709.01921 http://arxiv.org/abs/1709.01921 [40] Xudong Wang, Xuanzhe Liu, Ying Zhang, and Gang Huang.
2012.
Migration and Execution of JavaScript Applications Between Mobile Devices and Cloud.
(2012), 83–84.
https://doi.org/10.1145/2384716.2384750 [41] Zheng Wang and J.
Crowcroft.
1996.
Quality-of-service routing for supporting multimedia applications.
IEEE Journal on Selected Areas in Communications 14, 7 (Sep 1996), 1228–1234.
https://doi.org/10.1109/49.536364 [42] M.
D.
Zeiler, D.
Krishnan, G.
W.
Taylor, and R.
Fergus.
2010.
Deconvolutional networks.
In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.
2528–2535.
https://doi.org/10.1109/CVPR.2010.5539957 [43] Ying Zhang, Gang Huang, Xuanzhe Liu, and others.
2012.
Refactoring Android Java Code for On-demand Computation Offloading.
SIGPLAN Not.
47, 10 (Oct.
2012), 233–248.
https://doi.org/10.1145/2398857.2384634
The massive development of connected devices and highly precise instruments has introduced the world to vast volumes of high-dimensional data.
Traditional data analytics cannot cope with these massive amounts, which motivates well inves- tigating dimensionality reduction schemes capable of gleaning out efﬁciently low-dimensional information from large-scale datasets.
Dimensionality reduction is a vital ﬁrst step to render tractable critical learning tasks, such as large-scale regression, classiﬁcation, and clustering, and allows for processing of datasets that might otherwise not be tractable.
[16], [10], Dimensionality reduction methods have been extensively studied by the signal processing and machine learning communities [2], [17].
Principal component analysis (PCA) [10] is the ‘workhorse’ method yielding low-dimensional representations that preserve most of the high-dimensional data variance.
Multi-dimensional scaling (MDS) [12] on the other hand, maintains pairwise distances between data when going from high- to low-dimensional spaces, while local linear embedding (LLE) [16] only pre- serves relationships between neighboring data.
Information from non-neighboring data is lost in LLE’s low-dimensional representation, which may in turn inﬂuence the performance of ensuing tasks such as classiﬁcation or clustering [6], [21].
It is also worth stressing that all aforementioned approaches cap- ture and preserve linear relationships between data.
However, for data residing on highly nonlinear manifolds using only lin- ear relations might produce low-dimensional representations that are not accurate.
Generalizing PCA, Kernel PCA [9] can capture nonlinear relationships between data, for a preselected Work in this paper was supported by NSF 1500713, and NIH 1R01GM104975-01.
kernel function.
In addition, Laplacian eigenmaps [2] preserve nonlinear similarities between neighboring data.
While all the aforementioned approaches have been suc- cessful in reducing the dimensionality of various types of data, they do not consider additional information during the dimensionality reduction process.
This prior information may be task speciﬁc, e.g. provided by some “expert” or the physics of the problem, or it could be inferred from alternate views of the data, and can provide additional insights for the desired properties of the low-dimensional representations.
In fMRI signals for instance, in addition to time series collected at different brain regions, one may also have access to the connectivity patterns among these regions.
As shown in [8], [9], [18], [19] for PCA, this additional information can be encoded in a graph, and incorporated into the dimensionality reduction process through graph-aware regularization.
The present paper puts forth a novel framework for di- mensionality reduction that can capture nonlinear relations between data, and also exploit additional information through graph regularization.
This framework encompasses all afore- mentioned approaches, and markedly broadens their scope.
II.
PRELIMINARIES AND PROBLEM STATEMENT Consider a dataset with N vectors of dimension D col- lected as columns of the matrix X := [x1, .
.
.
, xN ].
Without loss of generality, it will be assumed that the sample mean N −1 PN n=1 xn has been removed from each xi.
Dimensional- ity reduction looks for a set of d-dimensional vectors {yi}N i=1, with d < D, that preserve certain properties of {xi}.
MDS for instance aims to preserve pairwise distances among {xi} when obtaining the corresponding low-dimensional represen- tations {yi}, while LLE attempts to preserve neighborhoods.
As will be shown in the ensuing subsections, all these dimen- sionality reduction schemes are special cases of kernel-based PCA.
A.
Principal component analysis Given X, PCA ﬁnds a linear subspace of dimension d so that hopefully all the data lie on or close to it (in the least- squares sense).
Speciﬁcally, PCA solves min Ud,{yi} i=1 kxi − Udyik2 s.
to U⊤ d Ud = I (1) where Ud ∈ RD×d is an orthonormal matrix.
The optimal solution of (1) is yi = U⊤ d xi, where Ud is formed by the eigenvectors of XX⊤ = UΣU⊤ corresponding to the d largest eigenvalues [7].
For future use, consider the singular value decomposition (SVD) X = UΣV⊤.
Given {yi}, the original vectors can be recovered as xi = Udyi.
PCA thrives when data lie close to a d-dimensional hyperplane.
Its complexity is that of eigendecomposing XX⊤, i.e., O(N D2), which means PCA is more affordable when D ≪ N .
In contrast, dimensionality reduction of small sets of high- dimensional vectors (D ≫ N ) becomes more tractable with dual PCA.
B.
Dual PCA and Kernel PCA The SVD of Y implies that Ud = YVΣ−1, which in d yi = turn yields the low-dimensional vectors as ψi = U⊤ Σ−1 d Y⊤yi.
Collecting Ψ := [ψ1, .
.
.
, ψN ], we have d V⊤ Y = U⊤ d X = ΣdV⊤ (2) where Σd ∈ Rd×d is a diagonal matrix containing the d leading eigenvalues of X⊤X, and Vd ∈ RN ×d is the submatrix of V collecting the corresponding eigenvectors of X⊤X.
The complexity of dual PCA is O(DN 2); therefore, it is preferable when D ≫ N .
Moreover, it can be readily veriﬁed that besides (1), Y in (2) is also the optimal solution to the following optimization problem min kKx − Y⊤Yk2 s.
to YY⊤ = Λd (3) where Kx := X⊤X, and Λd denotes a d × d diagonal matrix containing the d largest eigenvalues of Kx. Compared to PCA, dual PCA needs only the inner products {x⊤ i xj } in order to obtain the low-dimensional representations, but not X itself.
Hence, dual PCA can yield low-dimensional vectors {yi} of general (non-metric) objects that are not necessarily expressed using vectors {xi}, so long as inner products (a.k.a correlations) of the latter are known.
Furthermore, expanding the cost in (3), we can equivalently express it as tr(YK −1 ⊤).
(4) min −tr(YKxY ⊤) ⇔ min Y:YY⊤=Λd Y:YY⊤=Λd While PCA performs well for data that lie close to a hyper- plane, this property might not be true for many datasets [9].
In such cases one may resort to kernel PCA.
Kernel PCA “lifts” {xi} using a nonlinear function φ, onto a higher (possibly inﬁnite) dimensional space, where the data may lie on or near a linear hyperplane, and then ﬁnds low-dimensional representations {yi}.
Kernel PCA is obtained by solving (3) with [Kx]i,j = κ(xi, xj) = φ⊤(xi)φ(xj), where κ(xi, xj) denotes a prescribed kernel function [5].
C.
Local linear embedding Another popular method that deals with data that cannot be presumed close to a hyperplane is LLE.
It assumes that {xi} lie on a smooth manifold, which can be locally approximated by tangential hyperplanes.
Speciﬁcally, LLE assumes that each datum can be expressed as a linear combination of its neighbors; that is, xi = Pj∈Ni wij xj + ei, where Ni is a set containing the indices of the nearest neighbors of xi, in the Euclidean distance sense.
In order to solve for {wij }, the following optimization problem is considered W = arg min ˇW kX − X ˇWk2 s.
to ˇwij = 0, ∀i /∈ Nj, X ˇwij = 1 (5) where ˇwij denotes the (i, j)-th entry of ˇW.
Upon obtaining W as the constrained least-squares solution in (5), LLE ﬁnds {yi} that best preserve the neighborhood relationships encoded in W, by solving min Y:Y⊤Y=Λd ⇔ min Y:Y⊤Y=Λd kY − YWk2 tr[Y(I − W)(I − W)⊤Y⊤] (6) where Λd is a diagonal matrix.
Conventional LLE adopts Λd = I, which is subsumed by the constraint in (6).
Nonethe- less, the difference is just a scaling of {yi} when Λd 6= I.
If the diagonal of Λd collects the d smallest eigenvalues of matrix (I − W)(I − W)⊤, then (6) is a special case of kernel PCA with [cf.
(4)] Kx = [(I − W)(I − W)⊤]† (7) where † denotes pseudo inverse.
Similarly, other popular dimensionality reduction methods such as MDS and Laplacian eigenmaps can also be viewed as special cases of kernel PCA, by appropriately selecting Kx [4].
Thus, (4) can be viewed as an encompassing framework for nonlinear dimensionality reduction.
This framework is the foundation of the general graph-aware methods we develop in the ensuing section.
III.
GRAPH-AWARE NONLINEAR DIMENSIONALITY REDUCTION Matrices Kx for dual PCA, kernel PCA and LLE depend only on X.
However, as mentioned in Sec.
I, additional structural information potentially useful for the dimensionality reduction task may be available.
This knowledge can be en- coded in a graph and embodied in Y via graph regularization.
Speciﬁcally, suppose there exists a graph G over which the data is smooth; that is, vectors {xi} on nodes of G are also close to each other in Euclidean distance.
With A denoting the adjacency matrix of G, we have [A]ij = aij 6= 0 if node i is connected with node j.
The Laplacian of G is deﬁned as LG := D − A, where D is a diagonal matrix with entries [D]ii = dii = Pj aij .
Now consider tr(YLG Y⊤) = i=1 j6=i aij(yi − yj)2 (8) which is a weighted sum of the distances of adjacent yi’s on the graph.
By minimizing (8) over Y, the low-dimensional representations corresponding to adjacent nodes with large edge weights aij will be close to each other.
A.
Kernel PCA on graphs Introducing (8) as a regularization term to the original kernel PCA problem in (4), we arrive at min Y:YY⊤=Λd tr(YK−1 x Y⊤) + γtr(YLG Y⊤) (9) where γ is a positive scalar, and Λd collects the d smallest x + γLG = ¯VΛ ¯V⊤.
Combining the eigenvalues of K−1 Laplacian regularization with the kernel PCA formulation, (9) is capable of ﬁnding {yi} that preserve the “lifted” covariance captured by Kx, while at the same time, promoting the Algorithm 1 Local nonlinear embedding over graphs Input: X, γ S1.
Estimate W from X.
S2.
Obtain kernel matrix Kx via (7).
S3.
Find low-dimensional representations Y using (10).
smoothness of the low-dimensional representations over the graph G.
Problem (9) admits a closed-form solution as Y = Λ1/2 ¯V⊤ q=1 θqK(q) (10) where ¯Vd denotes the sub-matrix of ¯V containing columns corresponding to eigenvalues in Λd.
When the γ is set to 0, one obtains the solution of kernel PCA [cf.
(2)].
Remark 1: When the kernel function needed to form Kx is not known a priori, one can use Kx = PQ in (9), where {K(q) x } is a known dictionary of kernel matrices, and {θq} are estimated as variables in (9) [1].
In addition, instead of directly using LG, a family of graph kernels r(LG ) := UGr(Λ)U⊤ G can be employed, where r(.) is a scalar function of the eigenvectors of LG .
By appropriately selecting r(.), different graph properties can be accounted for.
As an example, when r sets eigenvalues above a certain threshold to 0, it acts as a sort of “low pass” ﬁlter over the graph.
A detailed discussion of how to choose graph kernels can be found in [14], [15].
Furthermore, instead of prescribing an r(LG ), a data-driven dictionary-based approach could also be employed to learn the proper graph kernel for the task at hand [15].
Remark 2: Even though only a single graph regularizer is introduced in (9), our method is ﬂexible to include multiple graph regularizers based on different graphs.
Therefore, the proposed method offers a powerful tool for dimensionality reduction on so-called multi-layer graphs, which encode the relationships among data across multiple graphs [11], [22].
B.
Local nonlinear embedding on graphs Broadening the premise of LLE, we pursue here a more general dimensionality reduction framework that captures nonlinear correlations between neighboring data, in addition to the structure induced by the graph G.
To this end, suppose that each datum can be represented by its neighbors as [xi]m = X j∈Ni where {hij(·)}N admitting a P th-order expansion hij(cid:0)[xj]m(cid:1) + [ei]m, m = 1, .
.
.
, D (11) i,j=1 are prescribed scalar nonlinear functions hij(z) = p=1 wij [p]zp (12) where coefﬁcients {wij[p]} are to be determined.
Taylor’s expansion asserts that for P sufﬁciently large, (12) offers an accurate approximation for all memoryless differentiable nonlinear functions.
Such a nonlinear model has been used for gragh topology identiﬁcation [20].
With the estimated W at hand, the low-dimensional representations can be obtained via (10), with Kx as in (7); see also Algorithm 1.
(a) (b) (c) (d) (e) (f) Fig.
1: Embedding results of two manifolds: linear hyperplane and trefoil (a) visualization of {xi}N i=1 obtained from (b) PCA; (c) LLE with K = 20; (d) LNEG with K = 20; (e) LLE with K = 40; (f) LNEG with K = 40.
i=1; and {yi}N IV.
NUMERICAL TESTS The performance of the generalized version of LLE was assessed using tests on synthetic data.
Alg.
1 is tested using Kx as in (7) for the locally nonlinear embedding (LNE) both without and with graph regularization (the latter abbreviated as LNEG), and is compared with LLE and PCA [10].
For all experiments, the graph G is constructed with adjacecy matrix As with (i, j)th entry aij = xix⊤ j /kxikkxjk.
Two types of tests are carried out in order to: a) evaluate embedding performance for a single manifold; and b) assess how infor- mative the low-dimensional embeddings are for distinguishing different manifolds.
Embedding experiment.
In the ﬁrst experiment, the em- bedding performance of the proposed method is assessed.
A 3-dimensional Swiss roll manifold is generated, and 1, 000 data are randomly sampled from the manifold as shown in Fig.
1 (a).
Fig.
1 (b) showcases the 2-dimensional embeddings obtained from PCA, while Figs.
1 (c) and (d) illustrate the resulting embeddings from LLE and LNEG respectively, where neighborhoods of K = 20 data are considered.
Figs.
1 (e) and (f) illustrate embeddings obtained by LLE and LNEG with K = 40.
The regularization parameter of LNEG is set to P = 2.
Clearly, by exploiting the nonlinear relationships to γ = 0.1, and the polynomial order is set 1 -1 -2 -1 -2 -3 (a) 1.00000015 1.0000001 1.00000005 0.99999995 0.9999999 0.99999985 -2.5 -2 -1.5 -1 -0.5 0.5 1.5 2.5 -1 -2 -3 -4 -5 -4 0.1 0.08 0.06 0.04 0.02 -0.02 -0.04 -0.06 -0.08 -3 -2 -1 -0.1 -0.02 0.02 0.04 0.06 0.08 0.1 (b) (c) (d) Fig.
2: Embedding results of two manifolds: a linear hyperplane with hole and a trefoil (a) visualization of two manifolds; and {yi}N i=1 obtained from (b) LLE with K = 40; and, (c) LNEG with K = 40 and P = 2; (d) PCA.
Trefoil-knots  -1 -2 -4 -2 (a) 1.00000004 1.00000003 1.00000002 1.00000001 0.99999999 0.99999998 0.99999997 0.99999996 0.99999995 0.99999994 -2 -1 -0.4 -0.6 -0.8 -1 -1.2 -1.4 -1.6 -1.8 -2 -2.2 -2.4 -1.5 0.12 0.1 0.08 0.06 0.04 0.02 -0.02 -0.04 -0.06 -1 -0.5 0.5 1.5 -0.08 -0.04 -0.02 0.02 0.04 0.06 0.08 (b) (c) (d) Fig.
3: Embedding results of two manifolds: a nonlinear sphere and a trefoil (a) visualization of two manifolds; and {yi}N obtained from (b) LLE with K = 40; (c) LNEG with K = 40 and P = 3; and (d) PCA.
i=1 Plane-hole-trefoil Sphere-trefoil LLE LNE LNEG LLE LNE LNEG 0.17 0.25 0.16 0.44 0.14 0.15 0.21 0.19 0.20 0.36 0.18 0.18 0.17 0.17 0.17 0.18 0.21 0.13 0.20 0.20 0.49 0.29 0.39 0.48 0.46 0.39 0.21 0.27 0.26 0.28 0.21 0.43 10 20 30 40 PCA TABLE I: Clustering error rate on low-dimensional represen- tations obtained from: LLE, LNE, LNEG and PCA.
between data, the resulting low-dimensional representations are capable of better preserving the structure of the manifold, thus allowing for more accurate visualization.
Clustering experiment.
In this experiment, the ability of Alg.
1 to provide meaningful embeddings for clustering of different manifolds is assessed.
Two 3-dimensional manifolds, a linear hyperplane with a hole around the origin and a trefoil are generated on the same ambient space as per [3], and 200 and 400 data are sampled from them, respectively.
Here each manifold corresponds to a different cluster.
Fig.
2(a) illustrates the sampled points from the generated manifolds.
Z1 ∈ R3×200 and Z2 ∈ R3×400 contain the data gener- ated from the linear hyperplane and the trefoil, respectively.
Both manifolds are then linearly embedded in R100, that is Xi = PZi +Ei, where P ∈ R100×3 is an orthonormal matrix, and E is a noise matrix with entries sampled from a zero mean Gaussian distribution with variance 0.01.
Afterwards, the 100-dimensional data in X := [X1 X2] are embedded into 2-dimensional representations Y ∈ R2×600 using LLE, LNEG and PCA.
Figures.
2(b), (c), and (d) depict the 2- dimensional embeddings Y provided by LLE, LNEG, and PCA, respectively.
Similarly, Fig.
3 illustrates the resulting embeddings when Z2 is sampled from a nonlinear sphere.
In both cases, the nonlinear methods result in embeddings that separate the two manifolds.
To further assess the performance, K-means is carried out on the resulting Y [13].
Table I shows the clustering error when running K-means on the low-dimensional embeddings given by PCA, LLE, LNE and LNEG, across different values of K.
The proposed approaches provide embeddings that enhance separability of the two man- ifolds, resulting in lower clustering error compared to LLE and PCA.
In addition, greater performance gain is observed when both manifolds are nonlinear, as in the case of Fig.
3.
V.
CONCLUSIONS This paper introduced a general framework for nonlinear dimensionality reduction over graphs.
By leveraging nonlinear relationships between data, low-dimensional representations were obtained to preserve these nonlinear correlations.
Graph regularization was employed to account for additional prior knowledge when seeking the low-dimensional representations.
An efﬁcient algorithm that admits closed-form solution was developed, and several tests were conducted on simulated data to demonstrate its effectiveness.
To broaden the scope of this study, several intriguing directions open up: a) extensive numerical tests on real data; b) development of data-dependent schemes that are capable of selecting appropriate kernels; c) online implementations that can handle streaming data; and d) generalizations to cope with large-scale graphs and high- dimensional datasets.
REFERENCES [1] F.
R.
Bach, G.
R.
Lanckriet, and M.
I.
Jordan, “Multiple kernel learning, conic duality, and the smo algorithm,” in Proc.intl.
Conf.
on Machine learning, New York, USA, 2004, pp.
6–13.
[2] M.
Belkin and P.
Niyogi, “Laplacian eigenmaps for dimensionality reduction and data representation,” Neural Computation, vol.
15, no.
6, pp.
1373–1396, Mar.
2003.
[3] E.
Elhamifar and R.
Vidal, “Sparse manifold clustering and embedding,” in Advances in Neural Information Processing Systems, Granada, Spain, 2011, pp.
55–63.
[4] A.
Ghodsi, “Dimensionality reduction -A short tutorial,” Department of Statistics and Actuarial Science, Univ.
of Waterloo, Ontario, Canada, vol.
37, p.
38, 2006.
[5] J.
Ham, D.
D.
Lee, S.
Mika, and B.
Sch¨olkopf, “A kernel view of the dimensionality reduction of manifolds,” in Proc.
Intl.
Conf.
on Machine Learning.
Alberta, Canada: ACM, Jul.
2004, p.
47.
[6] J.
A.
Hartigan and M.
A.
Wong, “Algorithm AS 136: A K-means clustering algorithm,” Journal of the Royal Statistical Society, vol.
28, no.
1, pp.
100–108, Jan.
1979.
[7] T.
Hastie, R.
Tibshirani, and J.
Friedman, The Elements of Statistical Learning.
Springer, 2009.
[8] B.
Jiang, C.
Ding, and J.
Tang, “Graph-Laplacian PCA: Closed-form solution and robustness,” in Proc.
Conf.
on Computer Vision and Pattern Recognition, Portland, Oregon, Jun.
2013, pp.
3492–3498.
[9] T.
Jin, J.
Yu, J.
You, K.
Zeng, C.
Li, and Z.
Yu, “Low-rank matrix factorization with multiple hypergraph regularizer,” Pattern Recognition, vol.
48, no.
3, pp.
1011–1022, Mar.
2015.
[10] I.
Jolliffe, Principal Component Analysis.
Wiley Online Library, 2002.
[11] M.
Kivel¨a, A.
Arenas, M.
Barthelemy, J.
P.
Gleeson, Y.
Moreno, and M.
A.
Porter, “Multilayer networks,” Journal of Complex Networks, vol.
2, no.
3, pp.
203–271, 2014.
[12] J.
B.
Kruskal and M.
Wish, Multidimensional Scaling.
Sage, 1978, vol.
11.
[13] S.
Lloyd, “Least-squares quantization in PCM,” IEEE Trans.
Info.
Theory, vol.
28, no.
2, pp.
129–137, 1982.
[14] D.
Romero, V.
N.
Ioannidis, and G.
B.
Giannakis, “Kernel-based Re- construction and Kalman Filtering of Space-time Functions on Dynamic Graphs,” IEEE Journal on Special Topics in Signal Processing, vol.
11, no.
6, 2017.
[15] D.
Romero, M.
Ma, and G.
B.
Giannakis, “Kernel-based reconstruction of graph signals,” IEEE Transactions on Signal Processing, vol.
65, no.
3, pp.
764–778, Feb.
2017.
[16] S.
T.
Roweis and L.
K.
Saul, “Nonlinear dimensionality reduction by locally linear embedding,” Science, vol.
290, no.
5500, pp.
2323–2326, Dec.
2000.
[17] B.
Sch¨olkopf, A.
Smola, and K.-R.
M¨uller, “Kernel principal component analysis,” in Proc.
Intl.
Conf.
on Artiﬁcial Neural Networks, Lausanne, Switzerland, Oct.
1997, pp.
583–588.
[18] N.
Shahid, N.
Perraudin, V.
Kalofolias, G.
Puy, and P.
Vandergheynst, “Fast robust PCA on graphs,” IEEE Journal of Selected Topics in Signal Processing, vol.
10, no.
4, pp.
740–756, Feb.
2016.
[19] F.
Shang, L.
Jiao, and F.
Wang, “Graph dual regularization non-negative matrix factorization for co-clustering,” Pattern Recognition, vol.
45, no.
6, pp.
2237–2250, Jun.
2012.
[20] Y.
Shen, B.
Baingana, and G.
B.
Giannakis, “Kernel-based structural equation models for topology identiﬁcation of directed networks,” IEEE Trans.
Sig.
Proc., vol.
65, no.
10, pp.
2503–2516, May 2017.
[21] J.
A.
Suykens and J.
Vandewalle, “Least squares support vector machine classiﬁers,” Neural Processing Letters, vol.
9, no.
3, pp.
293–300, Jun.
1999.
[22] P.
A.
Traganitis, Y.
Shen, and G.
B.
Giannakis, “Topology inference of multilayer networks,” in Intl.
Workshop on Network Science for Comms., Atlanta, GA, May 2017.

Autonomous driving has attracted considerable interest in the past two decades and signiﬁcant progress has been achieved.
According to Douges [Donges, 1999], autonomous driving tasks can be roughly classiﬁed into three categories: naviga- tion, guidance, and stabilization.
Strategic navigation tasks are responsible for generating road-level routes.
Tactical- level guidance tasks are responsible for guiding ego vehi- cle along these routes in complex environments by generat- ing tactical maneuver decisions.
And operational-level stabi- lization tasks are responsible for translating tactical decisions into reference trajectories and then low-level control signals.
Among these three classes of tasks, tactical-level decision making is especially important due to its central role and has been an active ﬁeld of research.
Early successes of decision making systems typically rely on human-designed rules to control the decision process, us- ing methods such as heuristic rules, decision trees, ﬁnite state machines, or fuzzy-logic [Montemerlo et al., 2008; Urmson et al., 2008; Miller et al., 2008].
These methods are often tailored for speciﬁc environments and do not generalize robustly.
More recently, the problem of tactical decision making has been cast into the Partially Observable Markov Decision Pro- cess (POMDP) framework and various approximate methods have been proposed to solve the theoretically intractable mod- els for tactical decision making [Ulbrich and Maurer, 2013; Brechtel et al., 2014; Galceran et al., 2015].
One common problem faced with POMDP-based work is the strong depen- dency to a relatively simple environment model, usually with delicately hand-crafted (discrete) observation spaces, transi- tion dynamics, and observation mechanisms.
These strong assumptions limit the generality of these methods to more complex scenarios.
In recent years, the success of deep learning has revived the interest in end-to-end driving agent which decides low-level control directly from image inputs, using supervised learning [Bojarski et al., 2017] or reinforcement learning (RL) [Sallab et al., 2017; Plessen, 2017].
But the black-box driving poli- cies learned by these methods are susceptible to inﬂuence un- der drifted inputs.
Although efforts have been made to iden- tify a more robust and compact subset of prediction targets than control outputs (e.g. in [Chen et al., 2015]), most prac- tical autonomous driving systems to date still only use deep learning as a restricted part of the whole system.
Deep RL is a natural way to incorporate deep learning into traditional POMDP or RL-based decision making meth- ods.
The use of function approximators makes it possible to directly use high-dimensional raw observations.
This al- leviates the strong dependency to hand-crafted simple mod- els in traditional POMDP and RL-based work.
Along this [Isele et al., 2017] and [Mukadam et line of research, al., 2017] apply the deep Q-network (DQN) [Mnih et al., 2013] to learn tactical decision policies for intersection cross- ing and lane changing on freeway, respectively.
Hierarchi- cal RL is combined with Monte-Carlo tree search (MCTS) in [Paxton et al., 2017] to simultaneously learn a high- level option policy for decision making and a low-level pol- [Shalev-Shwartz et al., 2016; icy for option execution.
Shalev-Shwartz et al., 2017] combine a high-level RL pol- icy with a non-learnable low-level policy to balance between efﬁciency and safety.
However, many commonly-used techniques for deep RL are originally proposed for low-level control tasks and can be less efﬁcient and/or robust for high-level tactical decision making from our observation.
Firstly for temporal abstrac- tion, frame skipping with action repetition will cause unstable low-level behavior due to the discontinuity in high-level ac- tion semantics.
Secondly for the multi-dimensional reward- ing systems, we ﬁnd that the commonly used sparse global goal indicators and dense local goal metrics are in general redundant and harmful, and a constant value lane penalty is hard to induce favorable lane switching behavior in multi-lane scenarios.
Thirdly, decision agents relying mere on a learned RL policy may accidentally issue disastrous action under the inﬂuence of observation noise.
In this paper, we aim to tackle the above difﬁculties for deep RL.
Our main contributions are a set of practical yet effective elements for deep RL agents towards tactical driv- ing decision making tasks.
We propose non-uniform action skipping as a more stable alternative to action repetition.
A counter-based lane penalty is also proposed to encourage de- sired behavior for multi-lane scenarios.
During inference, the learned RL agent can be further enhanced with heuristic rules that ﬁlter out obviously undesirable actions.
These elements are meant to make as less modiﬁcation to existing methods as possible for the sake of simplicity, and target the peculiarity of high-level tactical decision making for effectiveness.
The proposed elements are equipped in a hierarchical autonomous driving system and the effectiveness is demonstrated in realis- tic driving scenarios presenting two-way trafﬁc and signaled intersections.
2 Method We consider a hierarchical autonomous driving system that orchestrates learning-based and non-learning modules: the tactical decision making module is implemented as a deep RL agent for efﬁciency while the routing, planning, and control modules are realized with non-learning methods for safety and comfort.
The routing module calculates local lane sug- gestions towards the global goal according to a road map.
The decision module takes into consideration both the rout- ing suggestions and other information such as the status of ego vehicle and road structure to make high-level maneuver directions.
The planning module then translates those direc- tions into the trajectories of vehicle poses.
The control mod- ule ﬁnally implements the planning trajectories into low-level control signals.
Note the planning module has certain built-in safety functions to avoid hazards such as collisions.
From a agent-centric perspective, the tactical decision agent makes sequential decisions in the dynamic environ- ment composed of all non-learning modules and the rest of the world.
In time step t, information about ego vehicle and the surrounding environment is compiled into an observation ot and presented to the agent.
The agent then selects a tacti- cal decision action at = π(ot) according to the decision pol- icy π(·).
Downstream modules will receive this decision and control the movement of ego vehicle accordingly.
A reward- ing module will then assess the movement in current time step to provide a scalar reward rt.
And the system will evolve for- ward in time into the next time step t + 1.
The goal is to learn a policy that maximizes the expected total discounted reward (cid:40) T(cid:88) (cid:41) E{Gt} = E γτ−tRτ (1) Note the world state is only partially observable to the de- cision agent due to imperfect sensing and unpredictable be- τ =t (a) Before crossing lane (b) After crossing lane.
Figure 1: Illustration for the discontinuous action semantics of lane switching tasks: the meaning of “switching to right lane” changes after crossing the lane marking.
havior of other agents.
Therefore, we extend the observa- tion vector ot into history by means of frame stacking.
Other methods, e.g. recursive neural networks, can also be used to ﬁll more information into the observation vector.
2.1 Action Skipping Action-repeated frame skipping [Mnih et al., 2013] is a commonly-used technique to speed up reinforcement learn- ing algorithms.
The beneﬁts are multi-fold.
For explo- ration, non-trivial high-level maneuvers can be more easily explored with random perturbation.
Otherwise the difﬁculty of forming a series of low-level movements that correspond to a high-level maneuver can be exponential in the length of that series.
Also, the effective horizon of the semi-MDP re- sulting from action repetition is proportionally shorter than the original MDP.
And bootstrapped value estimation meth- ods, such as temporal difference learning, will receive pro- portional speedup.
Moreover, the reward signal can become more resilient to noises and delays thanks to the extended ef- fective period of each action.
However, action repetition can be less stable for high-level decision making tasks due to the discontinuity in action se- mantics.
Consider driving on a multi-lane road shown in Fig- ure 1: when ego vehicle is just about to cross the marking between the current lane (L0) and the lane immediately to the right (L1), the action switching to the right lane means switching from L0 to L1.
But after ego car has crossed the lane marking, the semantics of that same action is radically changed to switching from L1 to L2, in which L2 is the lane further to the right of L1.
If the frame skipping period con- tains such moments of non-continuous action semantics, the resulting high-level direction is doomed to result in unfavor- able lower-level behaviors, e.g. frequent overshot and correc- tion for the above scenario.
We propose to skip instead of repeat actions.
Concretely, each agent action is now a meta action consisting of an actual decision followed by several null actions (No-Op).
We de- note this operation as action skipping.
During skipping, the agent can continue to collect observations and rewards.
The overall reward for the meta action is then calculated as the average of all rewards collected during the skipping period.
Note to implement action skipping in the decision module, the lower-level modules need to continue operation under the null action.
This is not a problem as long as the trajectory planning module plans over a horizon longer than the skip- ping period, which is relatively easy to implement.
One drawback of action skipping is the decrease in deci- sion frequency which will delay or prevent the agent’s reac- tion to critical events.
To improve the situation, the actions can take on different skipping factors during inference.
For instance in lane changing tasks, the skipping factor for lane keeping can be kept short to allow for swift maneuvers while the skipping factor for lane switching can be larger so that the agent can complete lane changing actions.
When perform- ing non-uniform action skipping, the agent may observe time phases that are skipped during training and cause domain drift in observations.
As a solution, we uniformly randomly extend the skipping factor of the ﬁrst agent step by a factor between zero and one so that the agent can observe all possible time phases during training.
2.2 Reward Function Tactical decision making needs to balance among efﬁciency, safety, and comfort.
Therefore the scalar reward used for tac- tical decision making is usually composed of multiple com- ponents, most often through linear combination.
Roughly speaking, these reward signals can be classiﬁed into sparse global goal indicators, sparse constraint violation alerts, and dense heuristic components.
The reward components we use is shown in Table 1 and our choices are explained below.
Global Goal Indicators Global goal indicators are very sparse signals that only take on non-zero values when a long-term goal if achieved or missed.
For tactical decision making, the true long-term goal is reaching the destination given by the navigation module as fast as possible.
Therefore the most common form of global goal indicators is a signal given at the end of each episode, positive if ego car reaches the destination and negative other- wise.
In this way, the discounted total reward will be larger for episodes in which ego car reach the destination earlier.
We argue that global goal indicators are not only unnec- essary but also burdens to the tactical decision making agent.
The preferences described by global goal indicators (i.e. what is wanted) can be implicitly expressed with some denser re- ciprocal constraint violation alerts (i.e. what is unwanted).
This is possible because all behaviors that will stop ego ve- hicle from reaching the destination can be deﬁned as vio- lating constraints and harshly penalized.
The use of dense per-step cost can also further devalue behaviors that will mis- lead ego vehicle into dead-end situations.
As a result, the behavior that will help achieve a global goal will naturally result in low penalty.
In comparison with sparse indicators, their denser counterparts will usually result in faster credit assignment during value estimation and is therefore more desirable.
Moreover, global goal indicators emitted at the end of episodes will generally increase the absolute value of expected accumulated rewards in comparison with the indicator-free counterpart.
In turn, the approximators used for value functions needs to have larger dynamic range, which mean more learning steps during training and larger variance during inference.
For these reasons we do not use any global goal indicator components in our experiments.
Constraint Violation Alerts Constraint violation alerts are sparse signals that penalize the agent for being in a risky situation or performing a risky ac- tion.
The most common situation considered is collision with other road objects, e.g. other vehicles or pedestrian.
Note re- ward signals that fall into this category need not to be sparse during constraint violations events.
They are sparse in the sense that risky situation should be very rare under a properly cautious driving policy.
We consider three types of risky situations in our experi- ments: 1) entering intersection during red light, 2) collision with other vehicles, and 3) treading onto biking and opposite lanes on which ego vehicle has less priority.
The former two components will also termination the episode.
Trafﬁc light: The trafﬁc light alert is triggered when ego vehicle enters an intersection when the corresponding con- nection road is covered by a red light.
Note although we ren- der longitudinal speed control to a rule-based planning mod- ule and it will automatically stop ego vehicle in most situa- tions, there are still corner cases that may accidentally grant ego vehicle’s access into intersection during red lights.
There- fore a trafﬁc light alert should be in place to penalize these corner-case behaviors.
Collision risk: The collision risk component is active when ego vehicle is about to crash into other vehicles.
It is the sum of risk components contributed by each of the other vehicles in the region of interest.
Each component is further deﬁned as the product of an isotropic distance-based factor and a di- rectional factor related to the heading direction of cars: (cid:88) rc = u · ri ri d, (2) u is the isotropic factor by target i and ri d is the direc- where ri tional factor.
The distance-based risk factor is Laplacian in the distance between the target car and ego car: u(d) = e−d+d0, ri (3) u(d) is the distance-based risk for target car i at dis- where ri tance d and d0 is a normalizing distance.
The directional risk factor is the product of two narrow-band raised-cosine pat- terns, the center of which are aligned with the heading direc- tion of ego car and the target car, respectively: d(θego, θtarget) = rcos(θ − θego) · rcos(θ − θtarget), ri (4) where rcos(·) is the narrow-band raised cosine function, θego and θtarget are the heading angle of ego and target vehicle, and θ is the direction of the vector connecting ego car and target car.
The overall effect of these two factors is high risk only when ego car and a target car is about to drive head-to-head Category Goal indicators Constraint violation Name N/A Collision risk Trafﬁc light Dangerous lane Dense heuristics Speed Lane switching Step cost Description N/A Directional risk of crashing immediate into other cars.
Entering road section invalidated by red light.
Risk factor linear in the duration of staying on a undesired lane.
The ego car velocity along lane center.
Unit cost of switching to adjacent lanes.
Unit per-step cost.
Weight N/A −1.0 −1.0 Biking: −0.2 Opposite −0.4 0.1 −0.4 −0.1 Table 1: Description for reward components and their weights.
into each other.
Otherwise the risk is relatively low, e.g. when ego car and the target car is driving closely side-by-side.
Another commonly-used measure for crashing risk is Time-to-Collision (TTC).
TTC is roughly inversely propor- tional to the distance between ego and the target car.
There- fore it is relatively aggressive in risk prediction and tend to exaggerate crashing risk.
In contrast, the proposed risk for- mulation indicates risk only when it is about to happen and is thus more conservative.
To foresee upcoming hazards, the agent can use common value estimation methods to predict future risk values.
Dangerous lane: We use counter-based risk signals to re- ﬂect the ever-increasing empirical risk of staying on biking and opposite lanes.
Speciﬁcally, the indicator for each lane maintains a counter (with a maximum value cap) that keeps track of the time steps that ego vehicle has spent on that lane.
The risk value is then computed as a linear function of the corresponding counter value: R = (0.1x + 0.9) × (x > 1.0), (5) where R is the risk value and x is the counter value.
The lane risk deﬁned as such will be relatively small when ego vehi- cle just arrived on a dangerous lane and will gradually be- come intolerably large if ego vehicle remains there for a long time.
In this way, temporally switching onto dangerous lanes, which is required for overtaking slower trafﬁc on single lane roads, can be enabled.
And staying on dangerous lane will be prohibited in the long run.
Note since the reward component deﬁned in this way becomes stateful, it is important to aug- ment agent observations with history information so that it can roughly infer how long it has stayed on a particular lane.
In contrast, it is much more difﬁcult to design constant- value risk signals that has the same enabling effect on over- take maneuvers: a small risk value may encourage ego vehi- cle to switch onto dangerous lanes when necessary, but it may easily fail on encouraging backward lane switching because the small difference in risk value can be easily overwhelmed by the variance in approximated value functions.
The agent can only slowly learn the risk of dangerous lanes from sparse collision events.
Meanwhile, a large risk value will in effect prohibit switching onto dangerous lanes, even when doing so is beneﬁcial.
Dense heuristic metrics Reward signals belonging to this category are usually used to hard-code designer’s heuristic preference for some states and actions.
Unlike the former two reward categories, which are easier to design as they can reﬂect orthogonal aspects of desired and unwanted outcomes, dense heuristic metrics are harder to design because heuristic rules can easily conﬂict with each other or, even worse, fail to align with the global goals.
For this reason, we aim to employ only a minimal set of dense heuristic components.
We consider a component proportional to the speed of ego vehicle along the navigation route to encourage driving to- wards the goal as fast as possible.
The speed limits are mon- itored by the planning module to avoid over-speeding.
The second dense component we consider is a per-step penalty for lane changing actions to discourage unnecessary lateral maneuvers and improve passenger comfort.
The ﬁnal dense component applied is a trivial per-step cost to prefer short episodes.
We do not employ any dense penalties related to local goals (e.g. local target lane or headway to other vehi- cles) as they can easily conﬂict with other heuristic metrics and the global goal.
2.3 Rule-based Action Masking In some scenarios, undesirable tactical actions can be straightforwardly identiﬁed.
In such cases, we proposed to apply simple rules to ﬁlter out those actions during inference instead of only hoping the agent to learn to avoid those ac- tions.
The reason is that, on one hand, even if the agent can learn to avoid inferior actions, they can still be triggered due to the variance in observation and the learned model.
On the other hand, those simple rules designed for straightforward situations are less prone to unexpected false positives and eas- ier to debug if any happens.
This is in contrast to conventional rule-based decision policies which is comprised of a complex set of rules intended to work under complex scenarios.
Implementation As shown in Figure 2a, the decision agent observes tilted RGB top-down views of ego vehicle’s immediate surround- ing.
In each time step, the latest two frames from the ren- dered 10Hz image stream are max-pooled pixel-wise into a single image frame to combat ﬂickering.
The frames from the latest three time steps are further stacked channel-wise to form a single observation.
The reward components and lin- ear combination weights deﬁned in Section 2.2 are used to derive a scalar reward function.
For comparison, some of the components may be removed or replaced for comparison.
tered along this route with random starting points and cruising speed.
Ego vehicle will then start from the beginning of the selected route.
A global navigation module will constantly provide a reference lane (rendered with a red line in observed image) that leads to the route destination.
An episode is ter- minated if ego vehicle encounters any of the following con- ditions: 1) reaching destination; 2) receiving a crashing risk value > 1.0; 3) stop moving for 40 seconds1; 4) entering in- tersection on unpermitted lanes.
Each agent conﬁguration is trained from scratch for 10 simulation runs.
Each run ter- minates once the accumulated number of environment steps exceeds 250K.
The trained agents are then evaluated with- out exploration against 100 pre-generated test episodes.
The number of other vehicles present in each episode is 32 for both training and test.
Test results are shown in Table 2 and organized into 4 sec- tions.
We select success rate, longitudinal speed, and lateral speed as the performance metrics to reﬂect safety, efﬁciency, and comfort.
The agent needs to switch to the correct lane and avoid collisions to successfully ﬁnish each episode.
Overtak- ing slower trafﬁc is also required to achieve a high longitu- dinal speed.
Moreover, the agent needs to avoid unnecessary lateral maneuvers in order to reduce lateral speed.
The over- all metric for each conﬁguration is calculated as the median per-step metric of all 10 simulation runs.
The ﬁrst section presents the random and rule-based base- lines.
The simulation scenario is complex enough such that a random policy can only achieve a success rate of 8%.
And the rule-based decision agent can improve the success rate metric to a more reasonable number of 79.0%.
The longitu- dinal speed of rule-based of agent is higher than the random agent thanks to overtake maneuvers.
And the lateral speed is reduced because a reasonable decision policy will avoid lane changing and stay on the current lane for most of the time.
The second section compares different skipping conﬁgura- tions.
The RL agent without any skipping operation (ID03) performs rather poor, achieving a success rate of only 29.5%.
Action repetition operations (ID04) can help signiﬁcantly im- prove the success rate to 83%.
This demonstrates the impor- tance of temporal abstraction to high-level tactical decision making tasks.
The success rate of dynamic action skipping scheme (ID05) is surprisingly unsatisfactory even though its action space is a super-set to the action repetition scheme.
One possible explanation may be that the gains from broad- ening the action space is over-weighed by the disadvantage of extending the effective horizon .
As expected, action skipping (ID10) will result in smaller lateral speed than action repeti- tion (ID04) thanks to the elimination of overshoot-correction jitters while roughly preserving the success rate.
The third section compares alternatives rewarding schemes to the proposed one.
In comparison to experiment ID10, ex- periment ID06 superimpose a ±1 global goal indicator to the proposed reward function and cause both success rate and longitudinal speed metrics to deteriorate.
This corroborates our previous statement that global goal indicators are redun- dant under the presence of a complete set of constraint viola- tion penalties.
Experiment ID07 and ID08 replaces the pro- 1This happens when ego vehicle drives into dead-end lanes.
(a) (b) Figure 2: (a) Illustration of the tilted top-down view used for agent observation.
(b) Simulation scenario with signaled intersection and two-way trafﬁc.
Red line indicates the navigation route.
The agent is composed of a dueling deep Q-network (DQN) [Wang et al., 2015], with 3 convolutional layers fol- lowed by 2 dense layers and one linear output layer.
The last dense layer is divided into an action-wise advantage channel and a baseline channel.
The Q value for each action is the sum of its corresponding advantage plus the shared baseline.
The three convolutional layers has 16, 16, and 64 kernels with size 8, 5, 3 and stride 4, 3, and 2, respectively.
The ﬁrst dense layer has 256 hidden units while the latter dense layer has 256 hidden units for each of the two channels.
All convolutional layers apply 3 × 3 max pooling and all hidden layers apply ReLU activation.
The discount factor used is 0.9 and double Q learning [van Hasselt et al., 2015] is applied to build up the temporal differ- ence loss.
We use a mini-batch of 8 samples and an ADAM optimizer with learning rate 1e−4, β1 = 0.9, and β2 = 0.999 to train the learning network.
The target network uses a syn- chronization rate of 1e−3 to track the learning network.
The exploration factor is annealed linearly from 0.2 to 0.05 in 30K steps.
The training process is handled asynchronously by a rate-controlled training thread: for each data point col- lected from the environment, this thread can perform 8 up- dates for the learning network.
The training thread samples from a memory-mapped on-disk replay buffer of maximum size 300K.
The data is divided into consecutive fragments of size 100 and about 30 fragments are cached in memory for random sampling at any time.
Each fragment will be sampled for at most 200 times and a new fragment will be swapped in for replacement.
4 Experimental Results As shown in Fig.2b, we experiment with different agent con- ﬁgurations in simulated driving scenarios with two-way traf- ﬁc and signaled intersections.
The simulator is wrapped as an RL environment complying to the OpenAI Gym API [Brock- man et al., 2016] and the frequency of agent-environment in- teraction is regularized to 2Hz in terms of simulation clock.
Agents are trained and tested with episodic simulation.
In each episode, a route is sampled randomly from the road map of 20 routes.
Each route consists of two road segments con- nected by an intersection.
A number of vehicles are scat- Section Baseline Skipping Reward Proposed Skipping N/A None Repetition Dynamic ID 01 02 03 04 05 06 07 08 09 10 11 12 13 Non-Uniform 14 Uniform Uniform Reward N/A Proposed Global goal 1.0 lane penalty 0.1 lane penalty Local goal Proposed Other Random Rule-based N/A 2× / 6× skipping N/A N/A Action mask 1 Action mask 2 Action mask 1 Action mask 2 Success Rate Lon.
Speed Lat.
Speed 0.080 0.790 0.295 0.840 0.275 0.825 0.820 0.295 0.855 0.830 0.845 0.900 0.860 0.905 4.85 5.11 3.49 5.64 4.96 5.80 5.50 4.17 5.52 5.97 6.05 5.88 5.95 6.11 0.496 0.0822 0.00538 0.204 0.182 0.148 0.0893 0.0385 0.102 0.162 0.159 0.131 0.191 0.153 Table 2: Experimental results.
posed counter-based penalty with a constant penalty.
The re- sulting agent behavior is unfavorably sensitive to the penalty value: a larger value completely bans the access to dangerous lanes, resulting in lower longitudinal speed than experiment ID10; while a smaller value fails to reﬂect the risk of those lanes, resulting in a drastically reduced success rate.
Experi- ment ID09 adds a dense penalty for deviating from the navi- gation lane.
Although ego vehicle can achieve a higher suc- cess rate by sticking to the navigation lane, it can also miss the opportunity to temporarily deviate from that lane and over- take slower trafﬁc.
The result is signiﬁcantly lower longitu- dinal speed.
This exempliﬁes how dense local-goal related rewards can conﬂict from the global goal.
The effectiveness of rule-based action masking and non- uniform action skipping is illustrated in the fourth table sec- tion.
We experiment with two set of rules: rule #1 ﬁlters out lane switching behavior while ego vehicle is moving slowly on the navigation lane, while rule #2 also banns ego vehicle from treading onto the opposite lane or biking lane in addi- tion.
Note rule #2 is stricter than rule #1 but also requires more structural information about the environment.
A trade- off between safety and efﬁciency can be identiﬁed by com- paring Experiment ID11 and ID12: the stricter rule #2 can provide more signiﬁcant improvements on success rate than rule #1, at the price of reduced longitudinal speed.
Finally, observe from the last two experiments, non-uniform action skipping can further increase the success rate of both masking rules by giving the ego car more lane-changing opportunities during inference.
5 Related Work Temporal abstraction is an effective means for speeding up RL algorithms.
Although frame skipping with action repe- tition [Mnih et al., 2013] is a very simple form of temporal abstraction, it is extremely effective and has been shown to be key to state-of-art performance in many low-level control tasks [Braylan et al., 2000].
Due to the discontinuous seman- tics of tactical driving decisions, we propose to replace ac- tion repetition with the more stable action skipping method.
Dynamic frame skipping [Lakshminarayanan et al., 2017] is also investigated in [Isele et al., 2017] for high-level deci- sion learning, but it is not as effective as action skipping in our experiments.
Other more sophisticated form of temporal abstractions have also been proposed for RL-based decision making under the option framework [Sutton et al., 1999], e.g. [Shalev-Shwartz et al., 2016].
But they are mostly tailored for speciﬁc scenarios and hard to generalize.
An appropriate multi-dimensional rewarding system is in- dispensable for tactical decision making agents that are based on RL or POMDP.
Many existing work applies global goal indicators, e.g. [Brechtel et al., 2014; Isele et al., 2017; Mukadam et al., 2017; Paxton et al., 2017].
However, we show that it is unnecessary to use global goal indicators in tactical decision making tasks.
Constraint violation alerts in existing work are mostly one-shot or constant in time [Brech- tel et al., 2014; Isele et al., 2017; Paxton et al., 2017; Li et al., 2017].
We propose to use counter-based risk signals to speed up learning in multi-lane scenarios.
Dense local- goal penalties are also used in some previous work to reg- ulate driving policy heuristically [Ulbrich and Maurer, 2013; Galceran et al., 2015; Paxton et al., 2017].
We show that such penalties can easily mislead the agent towards sub-optimal policies and should be avoided if possible.
6 Conclusion Deep reinforcement learning is a promising framework to tackle the tactical decision making tasks in autonomous driv- ing systems.
In this paper, we propose several practical ingre- dients for efﬁcient deep reinforcement learning algorithms to- wards tactical decision making.
We propose action skipping as a more stable alternative for the commonly used action rep- etition scheme and investigate a necessary set of reward com- ponents that will guide decision making agent to learn effec- tively in complex trafﬁc environments.
For more reliable in- ference, a heuristic rule-based action masker is combine with the learned agent to ﬁlters out apparently unsafe actions.
The proposed ingredients is evaluated in a realistic driving simu- lator and results show that they outperform various baseline and alternative agent conﬁgurations.
References [Bojarski et al., 2017] Mariusz Bojarski, Philip Yeres, Anna Choromanska, Krzysztof Choromanski, Bernhard Firner, Lawrence Jackel, and Urs Muller.
Explaining how a deep neural network trained with end-to-end learning steers a car.
arXiv:1704.07911 [cs], Apr 2017.
arXiv: 1704.07911.
[Braylan et al., 2000] Alex Braylan, Mark Hollenbeck, El- liot Meyerson, and Risto Miikkulainen.
Frame skip is a powerful parameter for learning to play atari.
Space, 1600:1800, 2000.
[Brechtel et al., 2014] S.
Brechtel, T.
Gindele, and R.
Dill- mann.
Probabilistic decision-making under uncertainty for autonomous driving using continuous POMDPs, pages 392–399.
Oct 2014.
[Brockman et al., 2016] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba.
Openai gym, 2016.
[Chen et al., 2015] Chenyi Chen, Ari Seff, Alain Korn- hauser, and Jianxiong Xiao.
DeepDriving: Learning Affor- dance for Direct Perception in Autonomous Driving, pages 2722–2730.
2015.
[Donges, 1999] Edmund Donges.
A conceptual framework for active safety in road trafﬁc.
Vehicle System Dynamics, 32(2–3):113–128, Aug 1999.
[Galceran et al., 2015] Enric Galceran, Alexander G.
Cun- ningham, Ryan M.
Eustice, and Edwin Olson.
Mul- tipolicy Decision-Making for Autonomous Driving via Changepoint-based Behavior Prediction.
2015.
[Isele et al., 2017] David Isele, Akansel Cosgun, Kaushik Subramanian, and Kikuo Fujimura.
Navigating intersec- tions with autonomous vehicles using deep reinforcement learning.
arXiv preprint arXiv:1705.01196, 2017.
[Lakshminarayanan et al., 2017] Aravind Lakshmi- narayanan, Sahil Sharma, and Balaraman Ravindran.
Dynamic Action Repetition for Deep Reinforcement Learning., pages 2133–2139.
2017.
S.
[Li et al., 2017] N.
Li, D.
W.
Oyler, M.
Zhang, Y.
Yildiz, I.
Kolmanovsky, and A.
R.
Girard.
Game theoretic mod- eling of driver and vehicle interactions for veriﬁcation and validation of autonomous vehicle control systems.
IEEE Transactions on Control Systems Technology, PP(99):1– 16, 2017.
[Miller et al., 2008] Isaac Miller, Mark Campbell, Dan Hut- tenlocher, Frank-Robert Kline, Aaron Nathan, Sergei Lu- pashin, Jason Catlin, Brian Schimpf, Pete Moran, Noah Zych, and et al.
Team cornell’s skynet: Robust perception and planning in an urban environment.
Journal of Field Robotics, 25(8):493–527, Aug 2008.
[Mnih et al., 2013] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller.
Playing atari with deep reinforcement learning.
arXiv:1312.5602 [cs], Dec 2013.
arXiv: 1312.5602.
[Montemerlo et al., 2008] Michael Montemerlo, Jan Becker, Suhrid Bhat, Hendrik Dahlkamp, Dmitri Dolgov, Scott Et- tinger, Dirk Haehnel, Tim Hilden, Gabe Hoffmann, and Burkhard Huhnke.
Junior: The stanford entry in the urban challenge.
Journal of ﬁeld Robotics, 25(9):569–597, 2008.
[Mukadam et al., 2017] Mustafa Mukadam, Akansel Cos- gun, Alireza Nakhaei, and Kikuo Fujimura.
Tactical de- cision making for lane changing with deep reinforcement learning.
2017.
[Paxton et al., 2017] Chris Paxton, Vasumathi Raman, Gre- gory D.
Hager, and Marin Kobilarov.
Combining neural networks and tree search for task and motion planning in challenging environments.
arXiv:1703.07887 [cs], Mar 2017.
arXiv: 1703.07887.
[Plessen, 2017] Mogens Graf Plessen.
Automating vehicles by deep reinforcement learning using task separation with hill climbing.
arXiv:1711.10785 [cs], Nov 2017.
arXiv: 1711.10785.
[Sallab et al., 2017] Ahmad El Sallab, Mohammed Abdou, Etienne Perot, and Senthil Yogamani.
Deep reinforcement learning framework for autonomous driving.
Electronic Imaging, 2017(19):70–76, Jan 2017.
arXiv: 1704.02532.
Shalev-Shwartz, Safe, Shaked Shammah, multi-agent, learning for autonomous driving.
arXiv:1610.03295 [cs, stat], Oct 2016.
arXiv: 1610.03295.
[Shalev-Shwartz et al., 2016] Shai and Amnon Shashua.
reinforcement [Shalev-Shwartz et al., 2017] Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua.
On a formal model of safe and scalable self-driving cars.
arXiv:1708.06374 [cs, stat], Aug 2017.
arXiv: 1708.06374.
[Sutton et al., 1999] Richard S.
Sutton, Doina Precup, and Satinder Singh.
Between mdps and semi-mdps: A frame- work for temporal abstraction in reinforcement learning.
Artiﬁcial Intelligence, 112(1):181–211, Aug 1999.
[Ulbrich and Maurer, 2013] Simon Ulbrich and Markus Maurer.
Probabilistic online POMDP decision making for lane changes in fully automated driving, pages 2063– 2067.
IEEE, 2013.
[Urmson et al., 2008] Chris Urmson, Joshua Anhalt, Drew Bagnell, Christopher Baker, Robert Bittner, M.
N.
Clark, John Dolan, Dave Duggins, Tugrul Galatali, and Chris Geyer.
Autonomous driving in urban environments: Boss Journal of Field Robotics, and the urban challenge.
25(8):425–466, 2008.
[van Hasselt et al., 2015] Hado van Hasselt, Arthur Guez, and David Silver.
Deep reinforcement learning with dou- ble q-learning.
arXiv:1509.06461 [cs], Sep 2015.
arXiv: 1509.06461.
[Wang et al., 2015] Ziyu Wang, Tom Schaul, Matteo Hes- sel, Hado van Hasselt, Marc Lanctot, and Nando de Fre- itas.
Dueling network architectures for deep reinforce- ment learning.
arXiv:1511.06581 [cs], Nov 2015.
arXiv: 1511.06581.

Unsupervised discovery of common patterns is a long standing task for artiﬁcial intelligence as shown in Barlow (1989); Bengio, Courville, and Vincent (2012).
Recent deep learning approaches have offered major breakthroughs in classiﬁcation into multiple categories with millions of labeled examples (e.g. Krizhevsky (2009); Szegedy et al.
(2015); He et al.
(2016) and many others).
These methods rely on a lot of annotated data for training in order to perform well.
Unfortunately, labeling is an inefﬁcient and expensive progress, so learning from unlabeled data is desirable for many complex tasks.
At the same time, much of human knowledge and learning is obtained by unsupervised observations Grossberg (1994).
The goal of this work is to show that semantically meaningful classes can be learned with minimal supervision.
Given a set of objectness proposals, we use the activations of foreground objects in order to learn deep features to cluster the available data while simultaneously learning the embedding in an end-to-end manner.
More speciﬁcally, we propose a differentiable clustering approach that learns better separability of classes and embedding.
The main idea is to store the potential cluster means as weights in a neural network at the higher levels of feature representation.
This allows them to be learned jointly with the potential feature representation.
This differentiable clustering approach is integrated with Deep Neural Networks (e.g. Szegedy et al.
(2015)) to learn semantic classes in an end-to-end fashion without manual class labeling.
The idea of doing this ‘end-to-end’ is that gradient descent can not only learn good weights for clustering, it can also change the embedding to allow for better clustering without the use of labels.
We see that this leads to better feature representation.
Our results show also that different object categories emerge and can later be retrieved from test images never before seen by the network, resulting in clusters of meaningful categories, such as cars, persons, bicycles.
31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.
In this work we use given segmentation objectness masks, which are candidate objects without labels.
This can be extended by using an independent objectness-based object generation mechanism Pathak et al.
(2017); Faktor and Irani (2014) or by using unsupervised motion segmentation in videos or structure from motion Vijayanarasimhan et al.
(2017).
2 Related Work Unsupervised learning (Barlow (1989)) and unsupervised deep learning (Bengio, Courville, and Vincent (2012), Bengio (2012), Bengio and others (2009)) are central topics to Machine Learning.
Unsupervised deep learning has been shown to improve results on classiﬁcation tasks per Erhan et al.
(2010), especially given small datasets and complicated high dimensional data such as video.
This has been explored by many representations including sequence to sequence learning and textual representations (Radford, Jozefowicz, and Sutskever (2017), Ramachandran, Liu, and Le (2016)).
Our work focuses on unsupervised deep learning for discovering visual object categories.
This has also been shown to improve results such as in Doersch and Zisserman (2017).
Unsupervised discovery of visual objects has been a large topic of interest in computer vision (Sivic et al.
(2005); Russell et al.
(2006); Singh, Gupta, and Efros (2012); Bach and Jordan (2005); Kwak et al.
(2015); Pathak et al.
(2017)).
Building specialized, deep embeddings to help computer vision tasks is also a popular approach such as in Agrawal, Carreira, and Malik (2015).
Transfer learning from supervised tasks has proven to be very successful.
Further, Agrawal, Carreira, and Malik (2015) propose learning the lower dimensional embedding through unsupervised learning and show improved performance when transfered to other supervised tasks.
Despite the popularity of building different embeddings, there is little work investigating the use of clustering to modify the embedding in an end-to-end deep learning framework.
Bottou and Bengio (1995) investigate a differentiable version of the kmeans algorithm and examine its convergence properties.
Our work focuses on learnable feature representations (instead of ﬁxed ones as in Bottou and Bengio (1995)) and introduces memory units for the task.
3 Unsupervised deep clustering Our unsupervised deep clustering is inspired by Bottou and Bengio (1995), who consider differentiable clustering algorithms.
We differ from this approach because the features we cluster also change with backpropogation.
In our work, we add a kmeans-like loss that is integrated end-to-end.
Our idea is to store the potential cluster means as weights in the network and thus have them be learned.
The proposed clustering is done simultaneously while building an embedding.
Given information of a potential object vs background (binary labels), clustering in a differentiable way provides a better embedding for the input data.
We show that this method can be used for meaningful semantic retrieval of related objects.
3.1 Embedding with clustering We train a convolutional neural network (CNN) to predict foreground and background using oracle labels of patches of objects and background images.
Concurrently, we learn the clustering of objects by imposing constraints that will force the embedding to be partitioned into multiple semantically coherent clusters of objects without explicit labels for different objects.
For our experiments, we use random initialization on the fully-connected layers (the last two layers) and we add the differentiable clustering module after the second to last layer.
Note that we only cluster the foreground labels as background activations are not of interest for clustering; the classiﬁer can predict foreground vs background with high accuracy (above 90%).
The objective function is shown in Equation 1.
Lk = 2N mink[(xn − wk)2] (1) N(cid:88) n=1 K(cid:88) K(cid:88) (cid:26)0 N(cid:88) k=0 j=k n=0 LC = N K |countk − countj| countk = if argmink[xn] = 0 if argmink[xn] = 1 (3) (4) (5) In this equation, N is the number of samples, k is the number of deﬁned clusters, w is the “weight” (theoretically and typically the mean of the cluster) for each k, and x is the activations from the fully connected layer before the classiﬁcation fully connected layer.
This is differentiable and the gradient descent algorithm is shown in Equation 2.
δwk = w(cid:48) k − wk = if k = s(xn, w) otherwise (2) weights to the loss L2 =(cid:80) where s(xn, w) = argmink[xn] and lr is the learning rate.
We also add L2 regularization over the j .
Furthermore, we use a custom clustering regularization loss LC that enforces that the clusters are evenly distributed as deﬁned in Equation 3 and Equation 4.
j w2 N(cid:88) (cid:26)lr(xn − wk) n=1 The ﬁnal loss to be optimized is shown in (Equation 5) L = Lk + αrL2 + αcLC where αr and αc are hyperparameters which are tuned during the training.
For our method, we use αr = 0.25 and αc = 1.
We apply this loss to every point that is labeled as potentially an object and ignore the background ones when clustering.
This way we learn foreground vs background and then learn clustering of the foreground activations.
Optimization was performed with a ‘RMSProp’ optimizer, with a learning rate of 0.045, momentum 0.9, decay factor 0.9, and  of 1.0. 4 Experimental evaluation We experiment with a toy example using CIFAR10 and a more challenging example using Cityscapes.
4.1 CIFAR10 dataset We ﬁrst test the proposed unsupervised clustering approach on the CIFAR10 Krizhevsky (2009) dataset.
The goal of this experiment is to test if clustering can uncover separate categories in a simple toy problem with a two class setting.
Clusters Automobile Cluster 0 Cluster 1 68.5% 31.5% Dog 17.9% 82.1% Table 1: Unsupervised clustering results on CIFAR10 for discovery of two classes.
Per cluster accuracy for each of the two given classes on the test set (class labels are unknown during training).
We selected as an example the dog and automobile classes to label as foreground.
We then train a network from scratch based on the Network in Network architecture (NiN) of Lin, Chen, and Yan (2013) from scratch for our experiments.
All other classes of CIFAR are considered background for this experiment.
By attaching our modiﬁed clustering objective function to the next to last layer, we attempt to cluster dog and automobile without labels.
We can see in our simple experimental results that classes are naturally clustered with the majority of examples correctly assigned.
Table 1 shows quantitative results on the test set.
As seen 68.5% of the automobile classes and 82.1% of the dog examples are correctly assigned to separate clusters.
Note that in these cases, the concepts and classes of dog and automobile are unknown to the training algorithm and we are just looking at them after clustering for evaluation.
Classes Person Rider Car Truck Bus Train Motorcycle Bicycle Cluster 0 Cluster 1 4320 676 1491 60 49 17 88 795 138 138 4399 69 89 16 205 787 Table 2: Unsupervised clustering of objects from Cityscapes using our method.
The table shows number of examples assigned to each learned cluster (for K=2).
4.2 Cityscapes dataset The Cityscapes dataset (Cordts et al.
(2016)) is a large-scale dataset that is used for evaluating various classiﬁcation, detection, and segmentation algorithms related to autonomous driving.
It contains 2975 training, 500 validation, and 1525 test images, where the test set is provided for the purposes of the Cityscape competition only.
In this work, we used the training set for training and the validation set for testing and visualizing results (as the test set has no annotation results).
Annotation is provided for classes which represent moving agents in the scene, such as pedestrian, car, motorcycle, bicycle, bus, truck, rider, train.
In this work we only use foreground/background labels and intend to discover semantic groups from among the moving objects.
4.3 Weakly supervised discovery of classes In this experiment we considered the larger, real-life dataset, Cityscapes (Cordts et al.
(2016)), described above to see if important class categories, e.g. the moving objects in the scene can be clustered into semantically meaningful classes.
We extract the locations and extents of the moving objects and use that as weak supervision.
Note the classes are uneven and car and person dominate.
We show results clustering 8 categories into 2 and 3 clusters despite the rarity of some of them (such as bicycle).
All results below are presented on the validation set.
We report the results in terms of the number of object patches extracted from the available test images.
For this dataset, the CNN architecture is based on the Inception architecture proposed by Szegedy et al.
(2015).
Since there are a small number of examples, we pre-train only the convolutional layers of the network.
Results on clustering the 8 classes of moving objects into 2 and 3 clusters are presented in Table 2 and Table 3 respectively for the learned embedding by the proposed approach and the baseline embedding.
The baseline embedding is calculated by ﬁne-tuning the same architecture in the same manner, but without our loss (Equation 5); it uses the same amount of information as input as our embedding.
For this experiment, we apply standard kmeans on both activations after training is completed.
We see here that our method provides better clustering for the two dominant classes in the dataset (car and person).
On the other hand, the baseline embedding clusters on one class only, similar to the two class case.
We have consistently observed this behavior for different runs and hypothesize this is due to the sparse nature of the baseline embedding and it’s activations.
Figure 1 visualizes the three retrieved clusters (color-coded) when clustering into 3 clusters with our approach.
We can see that people (in blue) and cars (in green) are often correctly retrieved.
Bikes are more rare and may be more often mistaken, for example in cases where a portion of the patch contains part of a car, or since the bicycle very often has a person riding it.
Still this is exciting result, given that it is learned by not providing a single class label during training.
5 Conclusions We propose a differentiable clustering objective which learns to separate classes during learning and build a better embedding.
The key idea is to be able to learn the clusters which are stored as weights, and simultaneously learn the feature representation and the clustering of the data.
Our results show that the proposed approach is useful for extracting semantically related objects.
Our method Baseline Cluster 0 Cluster 1 Cluster 2 Cluster 0 Cluster 1 Cluster 2 Classes Person Rider Car Truck Bus Train Motorcycle Bicycle 151 258 5195 89 127 25 127 1128 4315 551 950 39 20 76 541 17 180 450 4482 816 6312 131 152 35 207 2119 13 Table 3: Unsupervised clustering on the Cityscapes dataset with 3 clusters.
The table shows the number of examples assigned to each learned cluster.
Our method (left) and baseline (right).
Our method results in 69.98% accuracy.
Figure 1: Visualization of clusters learned by our method (for K=3).
From the ﬁgure, the green class is responsible for retrieving cars, the blue one persons, and the red one bicycles.
We can see that both cars and persons are discovered well but bicycles, a rarer class, can be confused with a person or with a partially visible car in the background.
References Agrawal, P.; Carreira, J.; and Malik, J.
2015.
Learning to see by moving.
CVPR.
Bach, F.
R., and Jordan, M.
I.
2005.
Learning spectral clustering.
NIPS.
Barlow, H.
1989.
Unsupervised learning.
Neural computation.
Bengio, Y., et al.
2009.
Learning deep architectures for ai.
Foundations and trends R(cid:13) in Machine Learning 2(1):1–127.
Bengio, Y.; Courville, A.
C.; and Vincent, P.
2012.
Unsupervised feature learning and deep learning: A review and new perspectives.
CoRR, abs/1206.5538.
Bengio, Y.
2012.
Deep learning of representations for unsupervised and transfer learning.
In Proceedings of ICML Workshop on Unsupervised and Transfer Learning, 17–36.
Bottou, L., and Bengio, Y.
1995.
Convergence properties of the k-means algorithms.
In Advances in neural information processing systems, 585–592.
Cordts, M.; Omran, M.; Ramos, S.; Rehfeld, T.; Enzweiler, M.; Benenson, R.; Franke, U.; Roth, S.; and Schiele, B.
2016.
The cityscapes dataset for semantic urban scene understanding.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 3213–3223.
Doersch, C., and Zisserman, A.
arXiv:1708.07860.
2017.
Multi-task self-supervised visual learning.
arXiv preprint Erhan, D.; Bengio, Y.; Courville, A.; Manzagol, P.-A.; Vincent, P.; and Bengio, S.
2010.
Why does unsupervised pre-training help deep learning?
Journal of Machine Learning Research 11(Feb):625–660.
Faktor, A., and Irani, M.
2014.
Video segmentation by non-local consensus voting.
BMVC.
Grossberg, S.
1994.
3-d vision and ﬁgure-ground separation by visual cortex.
Perception and Psychophysics.
He, K.; Zhang, X.; Ren, S.; and Sun, J.
2016.
Deep residual learning for image recognition.
CVPR.
Krizhevsky, A.
2009.
Learning multiple layers of features from tiny images.
Kwak, S.; Cho, M.; Laptev, I.; Ponce2, J.; and Schmid, C.
2015.
Unsupervised object discovery and tracking in video collections.
ICCV.
Lin, M.; Chen, Q.; and Yan, S.
2013.
Network in network.
arXiv preprint arXiv:1312.4400.
Pathak, D.; Girshick, R.; Dollar, P.; Darrell, T.; and Hariharan, B.
2017.
Learning features by watching objects move.
CVPR.
Radford, A.; Jozefowicz, R.; and Sutskever, I.
2017.
Learning to generate reviews and discovering sentiment.
arXiv preprint arXiv:1704.01444.
Ramachandran, P.; Liu, P.
J.; and Le, Q.
V.
2016.
Unsupervised pretraining for sequence to sequence learning.
arXiv preprint arXiv:1611.02683.
Russell, B.
C.; Efros, A.
A.; Sivic, J.; Freeman, W.
T.; and Zisserman, A.
2006.
Using multiple segmentations to discover objects and their extent in image collections.
CVPR.
Singh, S.; Gupta, A.; and Efros, A.
A.
2012.
Unsupervised discovery of mid-level discriminative patches.
ECCV.
Sivic, J.; Russell, B.
C.; Efros, A.
A.; Zisserman, A.; and Freeman, W.
T.
2005.
Discovering objects and their location in images.
ICCV.
Szegedy, C.; Liu, W.; Jia, Y.; Sermanet, P.; Reed, S.; Anguelov, D.; Erhan, D.; Vanhoucke, V.; and Rabinovich, A.
2015.
Going deeper with convolutions.
CVPR.
Vijayanarasimhan, S.; Ricco, S.; Schmid, C.; Sukthankar, R.; and Fragkiadaki, K.
2017.
Sfm-net: Learning of structure and motion from video.

Emerging Trends.
Trafﬁc congestion in urban areas has become a signiﬁcant issue in recent years.
Because of trafﬁc congestion, people in the United States traveled an extra 6.9 billion hours and purchased an extra 3.1 billion gallons of fuel in 2014.
The extra time and fuel cost were valued up to 160 billion dollars [1].
Congestion that is caused by accidents, roadwork, special events, or adverse weather is called non- recurring congestion (NRC) [2].
Compared with the recurring congestion that happens repeatedly at particular times in the day, weekday and peak hours, NRC makes people unprepared and has a signiﬁcant impact on urban mobility.
For example, in the US, NRC accounts for two-thirds of the overall trafﬁc delay in urban areas with a population of over one million [3].
Driven by the concepts of the Internet of Things (IoT) and smart cities, various trafﬁc sensors have been deployed in urban environments on a large scale.
A number of tech- niques have been developed for knowledge discovery and data mining by integrating and utilizing the sensor data.
Trafﬁc data is widely available by using static sensors (e.g., loop detectors, radars, cameras, etc.) as well as mobile sensors (e.g., in-vehicle GPS and other crowdsensing techniques that use mobile phones).
The fast development of sensor techniques enables the possibility of in-depth analysis of congestion and causes.
The problem of ﬁnding anomalous trafﬁc patterns is called trafﬁc anomaly detection.
Understanding and analyzing trafﬁc anomalies, especially congestion patterns, is critical to help- ing city planners make better decisions to optimize urban transportation systems and reduce congestion conditions.
To identify faulty sensors, many data-driven and model-driven methods have been proposed to incorporate historical and real- time data [4], [5], [6], [7].
Some researchers [8], [9], [10], [11] have worked on detecting trafﬁc events such as car accidents and congestion using videos, trafﬁc, and vehicular ad hoc data.
There are also researchers who have explored the root causes of anomalous trafﬁc [12], [13], [14], [15], [16], [17].
Most existing work still mainly focuses on a road section or a small network region to identify trafﬁc congestion, but few studies explore non-recurring congestion and its causes for a large urban area.
Recently, deep learning techniques have gained great success in many research ﬁelds (including image processing, speech recognition, bioinformatics, etc.), and provide a great opportunity to potentially solve the NRC identiﬁcation and classiﬁcation problem.
There are still many open problems: (1) using feature vectors to represent trafﬁc conditions loses the spatial information of the road segments, (2) using small and unbalanced dataset (trafﬁc data with event labels is limited) to train neural networks downgrades the performance, a proper data augmentation mechanism is needed to balance the training data with different class labels, (3) building deep neural networks to model the trafﬁc conditions of both recurring and non-recurring congestion.
Contributions.
In this paper, we propose DxNAT, a deep neural network model to identify non-recurring trafﬁc conges- tion and explain its causes.
To the best of our knowledge, our work is one of the ﬁrst efforts to utilize deep learning techniques to study trafﬁc congestion patterns and explain non- recurring congestion using events.
The main contributions of our research are summarized as follows: • We present an algorithm to efﬁciently convert trafﬁc data • We introduce a crossover operator as a data augmentation • A convolutional neural network (CNN) is proposed to identify non-recurring trafﬁc anomalies that are caused by events.
• We create three scenarios to evaluate the performance in Trafﬁc Message Channel (TMC) format to images method for training class balancing.
of the proposed model by using real-world data of three events types (football games, hockey games, and trafﬁc incidents).
Paper Organization.
The remainder of this paper is orga- nized as follows: Section II compares our work with related work; Section III presents the dataset and a motivating example that explores the impact of football games on trafﬁc conges- tion; Section IV formulates the problem; Section V presents the solution approach; Section VI evaluates the performance of our model; Section VII gives concluding remarks; II.
RELATED WORK AND CHALLENGES This section presents an overview of the related work on trafﬁc anomaly detection, which includes studies about faulty trafﬁc sensor detection, trafﬁc event detection, and congestion cause indication.
Three key research challenges and our con- tributions for detecting NRC are discussed in the end.
Faulty Trafﬁc Sensor Detection.
Robinson et al.
[4] proposed an approach that used data from inductive loop detectors to estimate travel time on road segments.
His ap- proach included a data cleaning method to clean the collected trafﬁc data.
Lu et al.
[5] reviewed previous work on faulty inductive loops data analysis.
Widhalm et al.
[18] presented a trafﬁc anomaly detection method that used Floating-Car Data (FCD) as an independent information source.
They developed a non-linear regression model to ﬁt the trafﬁc sensor data and FCD data.
Zygouras et al.
[6] proposed a method comparing correlations among nearby sensors to identify faulty sensor readings.
Their system was based on MapReduce paradigm to work for crowdsourcing data.
Ghafouri et al.
[7] presented a faulty trafﬁc sensor detection model based on Gaussian Processes.
Particularly, they provided an effective approach for computing the parameters of detectors to minimize the loss due to false-positive and false-negative errors.
Event Detection Using Trafﬁc Data.
Monitoring trafﬁc ﬂow at intersections is important in the trafﬁc event detection research.
Kamijo et al.
[8] developed an algorithm based on spatiotemporal Markov random ﬁeld (MRF) for processing trafﬁc images and tracking vehicles at intersections.
Using the timeseries observed behaviors of vehicles, a hidden Markov model for accident detection is then proposed.
Veeraraghavan et al.
[9] presented a multiple cue-based approach combined with a switching Kalman ﬁlter for detecting vehicle events such as turning, stopping and slow moving.
Terroso-Senz et al.
[19] presented an event-driven architecture (EDA) that used vehicular ad hoc network and external data sources like weather conditions to detect trafﬁc congestions.
Yang et al.
[10] proposed a coupled Bayesian RPCA (BRPCA) model for detecting trafﬁc events that used multiple trafﬁc data streams.
Kong et al.
[11] proposed LoTAD to explore anomalous regions with long-term poor trafﬁc situations.
To model the trafﬁc condition, crowd-sourced bus data is grouped into spatiotemporal segments.
The segments with high anomaly indexes were combined to get anomalous regions.
Wang et al.
[20] proposed a two-stage solution to detect road trafﬁc anomalies: (1) a Collaborative Path Inference (CPI) model that performs path inference incorporating static and dynamic features into a Conditional Random Field (CRF); (2) a road Anomaly Test (RAT) model calculates the anomalous degree for each road segment.
Congestion Cause Indication.
Liu et al.
[12] studied both known (planned) and unknown (unplanned) events behaving differently from daily network trafﬁcs as anomalies, and pro- posed algorithms that construct outlier causality trees based on temporal and spatial properties of detected outliers.
Xu et al.
[13] introduced an approach to identify urban congestion patterns based on the data cube.
They proposed a multi- dimensional data analysis method for data cube.
Chow et al.
[14] presented an automatic number plate recognition technol- ogy to analyze urban trafﬁc congestions and introduced a linear regression model to indicate the causes of the congestions.
Kwoczek et al.
[16] proposed an Artiﬁcial Neural Network (ANN) based classiﬁer to detect the road segments affected by planned events.
Mallah et al.
[17] evaluated the performance of machine learning techniques for classifying congestions into different causes.
A.
Research Challenge 1: Representing Heterogeneous Trafﬁc Data and Event Labels Using Multi-Dimensional Images A feature vector is an n-dimensional vector and is the most popular representation of data objects.
Besides numerical values, feature vectors can also represent texts and images.
However, feature vectors may not be the best solutions for representing trafﬁc and corresponding event labels.
Trafﬁc conditions are highly affected by different inﬂu- encing factors [21], such as incidents, sports games, road work, weather, etc.
The events and their physical locations are used as the labels.
But since feature vectors have ﬁxed length, it is not practical to manually encode the labels to a speciﬁc ﬁxed length feature vector.
More importantly, in pattern recognition and machine learning, features matter the most.
When converting an image to a feature vector, you can directly convert the two-dimensional pixels to a one- dimensional vector, or you can ﬁrst take the histogram of the image and then construct a feature vector that has several comparison metrics, such as mean, standard deviation, etc.
Both methods will lose some relative spatial information in the original images.
In contrast to feature vectors, images can preserve the original spatial relations by locating points on different pixels and can integrate multiple data sources by simply adding layers.
Kwoczek et al.
[16] showed a factor representation that integrates multiple features like event and weather into different layers in a data cube.
However, though they men- tioned the idea as a possible future work, they did not present any concrete solution to it.
Ma et al.
[22] proposed a CNN- based approach for trafﬁc prediction.
They represented the trafﬁc speed and time using a time-space matrix.
The problem with the time-space matrix is that information between segments is lost, which is important in detecting trafﬁc patterns because nearby roads usually show similar or related patterns.
Additionally, their model simply considered trafﬁc data, but there are many other factors affecting the future trafﬁc conditions.
Thus representing heterogeneous trafﬁc and the spatial corresponding event labels using images remains a research gap.
One of the key differences between our proposed approach and the existing ones is that we are trying to visualize the wide area sensor data distribution as Trafﬁc Condition Images (TCIs), so that we can use CNN and other deep learning techniques for analyses.
B.
Research Challenge 2: Training Deep Learning Models Using Limited Data Instances The performance of deep learning techniques highly relies on the quality of training data instances.
However, the collected urban data may not provide enough data for training because of the data sampling rates.
For example, our proposed model ﬁrst converts trafﬁc data to Trafﬁc Condition Images (TCIs) and then trains different models using these images.
But the trafﬁc data we obtain from HERE [23] is requested every minute.
So for a day that consists of 1440 minutes, we will only have 1440 trafﬁc images, which are too few for effective training purposes.
The availability issue of data instances becomes worse considering there is also limited label data.
It remains a research challenge of getting more training data using the existing data.
Traditional ways of solving this problem are: (1) waiting and collecting until enough training data is collected, (2) manually labeling the data, (3) adding data sources, e.g., collecting more data from social media.
Our solution uses the idea of crossover from the genetic algorithm.
We assume that trafﬁc conditions within a short time range are associated with the same events.
So we can apply a crossover operator on the TCIs to generate more TCIs with the same event label.
C.
Research Challenge 3: Modeling Trafﬁc Patterns of Non- Recurring Events The existing work on trafﬁc event detection focused on analyzing trafﬁc videos or trafﬁc sensor data streams to detect events that are directly related to trafﬁc, such as vehicle stopping, car accidents, and road congestion.
But few studies explored the contextual non-recurrent events whose impacts are also highly associated with certain trafﬁc patterns.
Recently there has been an explosion in research of using deep neural networks.
But still, few have applied deep learning on studying trafﬁc patterns.
Deep learning techniques have gained great success in research ﬁelds like image processing, speech recognition, bioinformatics, etc.
Convolutional neural networks are similar to original neural networks but convo- lutional layers are added in the front of the model to learn patterns in the original images.
If trafﬁc and label data can be converted to images, then CNN can be employed to learn their labeled patterns.
It is still a research gap of how to develop an effective and efﬁcient deep learning network for identifying and classifying trafﬁc patterns of non-recurring events.
We formulate the problem of identifying the speciﬁc trafﬁc patterns associated with events in Section V-A, and then present the details of our proposed approach that uses con- volutional neural networks in Section V-C.
Trafﬁc Format TMC Source HERE [23] API Sports Game JSON ESPN, hockey- reference.com Update Every Minute Manually 28 Games Size 10/2016 Range 12/2016 155 GB 10/2016 Present TABLE I.
DATA Accident JSON Fire Department Manually 387 MB 03/2014 03/2017 III.
DATA AND MOTIVATING EXAMPLE This section ﬁrst introduces the datasets that we have integrated into the system, and then describes a motivating example in which we use the collected datasets to study the impact of football games on the trafﬁc congestion in the city.
A.
Datasets Since October 2016, we have been continuously collecting and storing real-time trafﬁc data from HERE API [23] for all major roads in the Nashville area.
In order to explore the impact of contextual events on urban mobility, we also collect the data about incidents and sports games.
We cooperate with the Nashville Fire Department [24] to access their incident datasets, and manually collect the information about sports games from the web.
As illustrated in Table I, the details of the datasets that we have integrated into the system are as follows: • The trafﬁc dataset provides the real-time trafﬁc infor- mation on road segments, such as speed limit, real-time speed, jam factor (JF), etc.
The dataset contains historical trafﬁc data for 3049 TMC road segments in the Nashville area.
• The sports game dataset contains the operation informa- tion about sports games, such as game type, start and end time, attendance, location, etc.
• The incident dataset provides the detailed records of incidents and the responding vehicles.
For each incident, it provides the coordinates, time, vehicle arrival and departure time, weather condition, etc.
type, alert incident B.
Motivating Example The motivation for our research comes from a brief experi- ment, in which we study the impact of football games on the trafﬁc congestion in the city.
During the studied period between Sept.
1, 2016 and Jan.
1, 2017, there were eight football games (as listed in Table II) at the Nissan Stadium at downtown Nashville.
During this time we collected data related to trafﬁc (speed limit, real- time speed) and the football games (date, start time, duration, location)1.
To indicate the congestion condition, HERE [23] provides a jam factor (JF) that ranges between 0.0 and 10.0 for each 1Our dataset is larger.
However, in this study we are focusing on these 4 months Fig.
1.
2 hours, (c) from 2 hours to 1 hour, (d) from 1 hour to 0 hour.
Impact of football games on trafﬁc congestion in four one-hour time windows before football games: (a) from 4 hours to 3 hours, (b) from 3 hours to TABLE II.
Date 1/1/17 12/11/16 11/13/16 10/27/16 10/23/16 10/16/16 9/25/16 9/11/16 3:11 68780 65205 Stadium Attendance Duration THE INFORMATION OF THE EIGHT FOOTBALL GAMES STUDIED IN THE MOTIVATING EXAMPLE Start Time (CST) 12:00 PM 12:00 PM 12:00 PM 7:26 PM 12:02 PM 12:02 PM 12:02 PM 12:05 PM Nissan Stadium Nissan Stadium Nissan Stadium Nissan Stadium Nissan Stadium Nissan Stadium Nissan Stadium Nissan Stadium 3:02 3:36 3:08 3:21 69116 3:12 3:03 2:57 61619 65470 60897 62370 63816 TMC road segment.
In this study we compare the JF between the days when there is a football game and the days when there is no football game, during four one-hour time window directly before the games: [−4,−3], [−3,−2], [−2,−1], and [−1, 0] relative to the time when the game was scheduled2.
As shown in Figure 1, the results of the JF difference on road segments in different time windows are visualized using heat maps.
In the ﬁgure, colors ranging from green to red are used to indicate the small and big JF differences.
The results show that the impact of football games on trafﬁc congestion begins to increase from 4 hours before games.
We have observed this pattern across several game events in the city.
Our hypothesis is that every event has a unique pattern and we can learn that pattern over time and use it to identify if a current congestion pattern matches with the expected pattern.
If the pattern does not match then we can classify it as an anomaly.
2 Most football games are scheduled at 1 PM.
IV.
PROBLEM FORMULATION In this section, we ﬁrst provide a formal deﬁnition of the problem and then describe the assumptions for solving the problem.
A.
Deﬁnition The goal of this research is to model trafﬁc patterns around the locations of non-recurrent events so that we can use the model to identify non-recurring congestion and its causes.
The trafﬁc pattern that we use here refers to the spatiotemporal relations of trafﬁc speeds on many road segments in an area, which can be modeled and detected by a classiﬁer.
The deﬁnitions of all relative notions can be found in Table III.
The inputs to the system are data about trafﬁc and events.
Since the trafﬁc data that we collected from HERE API is deﬁned using Trafﬁc Message Channel Location Code [25] format (a standard for encoding geographic information), the road segments used in this study are also deﬁned using the same TMC location codes.
Event data is categorized with labels for training and validating purposes.
The labels used are as follows: • Event-related: event indicator levent, time window relative • Time-related: time in the day tday, weekday tweek One of the key differences between our approach and the existing ones is that we are trying to visualize the wide area sensor data distribution as Trafﬁc Condition Images (TCI), so that we can use CNN and other deep learning techniques to analyze and model the spatiotemporal relations.
TCI is a Iw by Iw pixels image.
Each pixel p corresponds to a road segment in the real world and the grayscale value of each pixel represents the real-time trafﬁc speed vr of the road segment r.
to the event tevent, Formulation of the Non-recurring Congestion Identiﬁca- tion Problem.
Given a set of trafﬁc data St that contains speed limit and real-time speed for a set of road segments Sr at a speciﬁc time tday on weekday tweek, and a set of event labels Se, the model should determine levent that indicates whether the given trafﬁc data contains congestion caused by a subset (cid:48) e of event set Se. If levent is true, the model should also TABLE III.
SYMBOLS USED IN THE FORMULATED PROBLEM tday tweek tevent levent Se Sr St T M Ckey T CI Iw vr T H a timestamp time in the day in seconds weekdays encoded using integers (e.g., 0 for Sunday, 1 for Monday, etc.) time windows relative to events (e.g., 1 for the 1-hour time window before events) indicator of whether the current time is within a time window near the occurrence of an event a set of events in the city a road segment a set of road segments deﬁned by TMC location codes a set of trafﬁc data that contains speed limit and real-time speed for a set of road segments Sr a string representing a road segment in Sr trafﬁc Condition Image, a gray-scale image to represent trafﬁc conditions in a bounding box the width of a TCI a pixel in TCI.
Its value shows the normalized trafﬁc speed on a road segment the real-time trafﬁc speed (miles per hour) on a road segment r a threshold for the classiﬁer to determine whether the input trafﬁc data contains recur- ring or non-recurring congestion provide the time window tevent relative to events (i.e. tevent can be used to estimate the event occurrence time).
Figure 2 illustrates an example of the problem.
Given raw the model should identify trafﬁc data at a speciﬁc time, the possible non-recurring congestion and also provide an estimation of the event occurrence time.
B.
Assumptions event information for the studied area and period.
in a direction is the same anywhere on the segment.
The following assumptions are made when we design and formulate the non-recurring trafﬁc congestion identiﬁcation system: • We assume the availability of both trafﬁc speed data and • We assume the trafﬁc condition on a short road segment • We assume that an event happening in the urban envi- ronment will affect the trafﬁc conditions of nearby road segments.
• We assume that there is a robust correlation between the road segments affected by an event, and that the patterns can be identiﬁed by the image classiﬁcation techniques of deep learning.
V.
OUR APPROACH In order to identify the speciﬁc trafﬁc patterns associated with non-recurring events as deﬁned in Section IV, we present the details of our proposed approach in this section.
The overall workﬂow of the system is shown in Figure 2.
There are three key components in the system: (1) an algorithm that converts raw trafﬁc data to images, (2) a convolutional neural network that classiﬁes the trafﬁc condition images, (3) ROC analysis that tunes the classiﬁcation threshold to reduce the false positive and false negative rates.
A.
Feature Extraction by Mapping Trafﬁc Data to Images Research challenge 1 describes the problem that feature vectors have limitations when representing urban data.
To solve this issue, the ﬁrst step is to convert the collected trafﬁc data into images.
We have been collecting real-time trafﬁc data of Nashville area from Here Trafﬁc API [23] since Oct.
2016.
The trafﬁc data is encoded in TMC location codes.
Since the TMC database is not open to the public, here we present an algorithm to convert trafﬁc data for a speciﬁc time T coded by TMC locations to trafﬁc images.
In order to project the trafﬁc conditions to the pixels of images, we ﬁrst initialize a gridded map and then re-sample the road segments deﬁned by TMC location codes to the grids.
The algorithm’s input, output, and step details are as follows (for a set of road segments Sr, step 1 and 2 will run only once, but step 3 will run once for each timestamp): Input: Trafﬁc dataset St, road segment set Sr, and times- tamp T .
The raw trafﬁc data of road segments Sr for timestamp T is queried from Trafﬁc dataset St in the database.
Output: A Trafﬁc Condition Image (TCI).
Step 1: Map grid initialization.
The map of the area containing the road segment set Sr is divided into a map grid of squares.
The length of each square is about 8.97 meters, so each grid cell covers about 80.51 square meters on the map.
Step 2: Road segment path re-sampling and smoothing.
The points from road segments are re-sampled to the centers of grid cells if the points are covered by the cell.
Also, if the distance between two original points is large enough that there are blank cells between the two cells projected by the two points, then points will be interpolated to ﬁll the blank cells.
After this step, we get a two-dimensional array, in which each cell contains a list of TMC keys T M Ckey corresponding to points from road segments.
Step 3: Trafﬁc data projection to the images.
The two- dimensional array acts as a projecting table from original road segments to the image pixels.
Now we can ﬁll the images with trafﬁc data by querying the trafﬁc data using segment keys T M Ckey and timestamp T .
We use the following equation to convert a trafﬁc speed to a pixel value: (cid:40) (80−vt p = 0, s)∗255 80 (cid:54) 80 if 0 (cid:54) vt otherwise (1) where p denotes the pixel value (0-255) and vt real-time speed (miles per hour).
s denotes the After getting initial projected TCI, simple image processing techniques are used to resize TCI to the desired size (Iw by Iw).
Fig.
2.
Overall workﬂow of the non-recurring congestion identiﬁcation system B.
Data Augmentation by Crossover Operations C.
Classifying Non-Recurring Congestion TCI is our image representation of trafﬁc speeds on road segments.
Since trafﬁc data is collected every minute, without data augmentation we can only get 1440 (the number of minutes in a day) TCIs for one day, which is usually not enough for training deep learning image processing models.
To address research challenge 2 (i.e., the lack of enough trafﬁc data with labels), we create a crossover operator to generate more labeled trafﬁc condition images for training deep learning models.
Crossover is originally a genetic operator from genetic algorithms to vary the chromosomes of individuals from one generation to another.
We are motivated by a similar idea and present the crossover operator for our system: 1) Getting TCI candidates.
For a given timestamp T , instead of only getting one TCI for T , we generate n TCIs for time range [T − t, T ] (t denotes a time length to extend T , e.g., 3 minutes).
While these TCIs are the same in image size, they differ in the pixel values because they correspond to trafﬁc speeds at different times.
2) Generating new labeled TCIs. While looping through the pixels in TCI, for each pixel row there is a probability pm that its values will mutate and randomly select a new row from the same corresponding pixels in other TCI candidates.
After the second step, we get a new TCI.
Because we assume trafﬁc conditions within a small time range are caused by the same events, we can give the new TCI the same event label.
The crossover operator can be executed for many times to generate many new data instances.
Through crossover, we not only have more labeled data, but also reduce the probability of over-ﬁtting in the training phase.
In the previous section, we have described an algorithm that converts raw trafﬁc data to TCI.
Since the inputs contain images, it makes sense to apply convolutional neural networks.
This section introduces our CNN model to classify the TCI using event labels.
CNN is a class of deep and feed-forward artiﬁcial neural networks that have shown great success in image analysis tasks.
Here we apply CNN to our problem that assigns event and congestion labels to a TCI.
CNN.
The architecture of the proposed CNN is shown in Figure 3.
Generally, the model consists of a stack of convolu- tional, fully-connected neural, dropout and max-pooling layers.
Dropout layers are used throughout the model to prevent over ﬁtting.
Max pooling layers are used for spatial down-sampling.
In the middle of the CNN, feature vectors that represent time of day and day of week that correspond to the TCI are concatenated to be input into the CNN to help it make better decisions.
Details of the layer conﬁguration, such as dimension, activation function, and dropout rate, can be found in the Figure 3.
Since we use one-hot encoding and the vectors are in categorical format (i.e., dimensional vector is all-zeros except for a one at the index corresponding to the class of the sample), categorical cross entropy is used as the loss function to train the model.
One-hot Encoding.
In the proposed CNN model, the input feature vectors are time in the day and weekday, and output labels are (1) whether the congestion in input TCI is recurring or non-recurring (2) the relative time windows that the TCI belongs to if it is non-recurring congestion.
We use one-hot encoding to convert both input and output vectors to binary class matrix.
The input matrix has 31 classes, in which 24 classes correspond to 24 hours and 7 classes correspond to 7 days of the week.
As illustrated in Figure 4, the output matrix has several classes, of which the ﬁrst class represents D.
Tuning the Model Sensitivity by ROC Analysis Our approach uses receiver operating characteristic (ROC) analysis to tune the sensitivity of the CNN classiﬁer.
ROC is a statistical plot that illustrates the diagnostic ability of a classiﬁer system [26].
The ROC curve is a fundamental tool for diagnostic test evaluation.
In an ROC curve, the true positive rate (TPR) is plotted in a function of the false positive rate (FPR).
In machine learning, TPR represents sensitivity, recall or probability of detection, and FPR represents fall-out or false alarm [27].
By choosing a point from the curve, corresponding classiﬁcation threshold can be decided.
In our model, the non-recurring congestion is considered as positive output and the recurring congestion is negative output.
We use the ROC analysis to tune the classiﬁcation threshold that decides whether the trafﬁc congestion in the input trafﬁc data is recurring congestion or non-recurring congestion.
We choose thresholds that range from 0.01 to 1.00 and the corre- sponding FPR and TPR of the training dataset are plotted (an example is shown in Figure 5).
The curve’s nearest point to the point (FPR: 0.0, TPR: 1.0) will be selected.
Fig.
3.
Our proposed convolutional neural network (CNN).
Fig.
4.
An example of the one-hot encoding format.
Event labels are encoded using 9 classes: (1) ﬁrst digit represents whether the trafﬁc condition belongs to recurring congestion or non-recurring congestion, (2) if it’s non-recurring congestion, the next 8 digits represent 8 time windows before and after events.
whether the trafﬁc condition belongs to recurring congestion or non-recurring congestion, and the next classes represent time windows before and after events.
The ﬁrst class is tunable since it directly determines whether the input trafﬁc condition contains non-recurring congestion or not.
If the value of the ﬁrst class output is higher than a predeﬁned threshold T H, then the classiﬁer will output that the input trafﬁc data does not contain non-recurring congestion (even if the values of other classes are higher than the ﬁrst class).
The details of the tuning steps are presented in the following section.
Fig.
5.
prediction threshold.
Receiver operating characteristics (ROC) curve analysis on the VI.
EXPERIMENTS In this section, we evaluate the proposed deep neural net- work’s ability to identify non-recurring trafﬁc anomalies by using real-world data of three event types: football games, hockey games, and trafﬁc incidents.
Keras [28] Python deep learning library is used to construct the models and TensorFlow [29] is selected as the tensor manipulation library.
A.
Scenarios As illustrated in Figure 6, we create three scenarios to test the performance of the proposed model.
In each scenario, we consider one of the three event categories for training and validating the proposed model: • Football Games.
Between Oct.
11, 2016, and Jan.
1, 2017, there were 8 NFL football games at the Nissan Stadium in Nashville.
The trafﬁc data in the bounding box (latitude TABLE IV.
EXPERIMENT RESULTS IN SCENARIO 1: TRAINING DXNAT FPR FOR IDENTIFYING NRC CAUSED BY FOOTBALL GAMES Accuracy FNR 98.73% 1.57% 0.17% 84.06% 6.25% 2.17% Random Forest DxNAT TABLE V.
EXPERIMENT RESULTS IN SCENARIO 2: TRAINING DXNAT FOR IDENTIFYING NRC CAUSED BY HOCKEY GAMES DxNAT Accuracy FNR 90.76% 8.11% 23.19% FPR over 60,000 people, which shows a great impact on causing non-recurring trafﬁc congestion.
In the ﬁrst scenario, we use the trafﬁc data collected in 1-minute intervals between Oct.
11, 2016 and Jan.
1, 2017.
Trafﬁc data of 5 non-game days and two game days are used as the training dataset, and one non-game day and one game day are used as the validating dataset.
As a comparison with the traditional machine learning techniques, we build a random forest model that uses the same training and validating dataset.
Because random forests cannot use images directly as input, we ﬁrst convert the trafﬁc condition images to one-dimensional vectors, and then concentrate the trafﬁc vectors with time of the day and day of the week vectors, and ﬁnally use the combined feature vector as input to the random forest model.
The accuracy, false positive rate (FPR) and false negative rate (FNR) of our model and the random forest model are shown in Table VI.
Our model outperforms the random forest model with higher accuracy and lower FPR and FNR.
C.
Experiment 2: Identifying NRC Caused by Hockey Games Compared with NFL football games, NHL hockey games in Nashville usually have less attendance (NHL 10,000 v.s. NFL 60,000).
So we assume that an NHL hockey game has less impact on trafﬁc conditions and it will be more difﬁcult to detect the NRC related to hockey games.
In Scenario 2, we use trafﬁc and hockey games data between Oct.
14, 2016, and Nov.
30, 2016, as the training dataset, and data of Dec.
15, 2016 (game day) and Dec.
16, 2016 (non-game day) as the validating dataset.
The accuracy, FPR and FNR results are shown in Table V.
Compared with the results in Scenario 1, the model has lower accuracy and higher FNR.
Our assumption is validated that the NRC associated with hockey games with less attendance is harder to be detected.
D.
Experiment 3: Identifying NRC Caused by Trafﬁc Accidents In scenario 3, we explore the model’s ability to detect NRC caused by road accidents.
For the selected block area, there were eight trafﬁc accidents on 7 different days between Oct.
18, 2016, and Dec.
13, 2016.
We use six days with accidents as the training dataset and one day with an accident as the validating dataset.
The DxNet model archives an accuracy of 86.59% with FPR of 13.71% and FNR of 4.44% Fig.
6.
Experimental scenarios and their coverage areas: (1) detecting NRC caused by football games, (2) detecting NRC caused by hockey games, (3) detecting NRC caused by trafﬁc accidents.
range: [36.1120, 36.2052], longitude range: [-86.8475, - 86.7543]) is used.
• Hockey Games.
Between Oct.
14, 2016, and Jan.
03, 2017, there were 20 NHL hockey games at the Bridgestone Arena in Nashville.
The trafﬁc data in the bounding box (latitude range: [36.1237, 36.1936], longitude range: - 86.8359, -86.7660]) is used.
• Trafﬁc Accidents.
Between Oct.
15, 2016, and Mar.
10, 2017, the selected block area.
The trafﬁc data in the bounding box (latitude range: [36.1470, 36.1586], longitude range: [-86.8126, - 86.8009]) is used.
there were 23 trafﬁc accidents at The trafﬁc and event datasets are divided into two subsets for training and validation.
The event and time information is encoded using one-hot encoding as described in Section V.
An example of the output of our model is shown in Figure 4.
Particularly, the following metrics are used to deﬁne if an output is positive or negative: (1) an output is considered to be positive if it determines that the input TCI contains non- recurring congestion, (2) an output is negative if it determines the input TCI only contains recurring congestion.
B.
Experiment 1: Identifying NRC Caused by Football Games As shown in the motivating example in Section III-B, the 8 selected NFL football games have an average attendance of TABLE VI.
EXPERIMENT RESULTS IN SCENARIO 3: TRAINING DXNAT FOR IDENTIFYING NRC CAUSED BY TRAFFIC ACCIDENTS DxNAT Accuracy FNR 86.59% 13.71% 4.44% FPR and a TIPS grant from Vanderbilt University.
We acknowledge the support provided by our partners from Nashville Metropoli- tan Transport Authority.
TABLE VII.
SUMMARY OF ARCHITECTURAL DECISIONS REFERENCES and Challenge Representing Heterogeneous Trafﬁc Data Event Labels Deep Training Learning Models Using Limited Data Instances Modeling Patterns Recurring Events Trafﬁc of Non- Approach Using Multi- dimensional Images Section II-A Developing crossover on original data operator Employing convolutional neural networks II-B II-C VII.
CONCLUSION includes: In this paper, we propose a deep neural network model to identify non-recurring trafﬁc congestion and explain its causes.
To our best knowledge, our work is one of the ﬁrst efforts to utilize deep learning techniques to study trafﬁc congestion patterns and explain non-recurring congestion using events.
Our main contributions are listed in Table VII.
We present an algorithm to efﬁciently convert trafﬁc data in Trafﬁc Message Channel (TMC) format to images, as well as a crossover operator as a data augmentation mechanism for class balancing.
A convolutional neural network is proposed to identify non-recurring trafﬁc anomalies that are caused by events.
We evaluate the proposed model by using three types of events (football games, hockey games, and trafﬁc incidents).
The future work to extend the current proposed model • Integrating more contextual features.
Existing work usu- ally focuses just on trafﬁc data, but there are many types of urban data available to help identify trafﬁc patterns, like real-time bus travel time, speed, and weather.
Be- sides events, trafﬁc conditions are affected by multiple environmental factors.
The current work only considers time of day and day of week as the environmental training features.
In the next step, we will include various features about weather conditions (such as humidity, nearest storm distance, visibility, etc.).
• Identifying sizes of block areas and length of time win- dows.
For each event type (e.g., sports games, accidents), the size of impacting block areas as well as the number of impacting time windows in the experimental scenarios are selected arbitrarily.
A mechanism is needed to au- tomatically select the best impacting area size and time windows for each event type.
ACKNOWLEDGMENTS This work is supported by The National Science Foundation under the award numbers CNS-1528799 and CNS-1647015 [1] D.
Schrank, B.
Eisele, T.
Lomax, and J.
Bak, “2015 urban mobility scorecard,” 2015.
[2] R.
W.
Hall, “Non-recurrent congestion: How big is the problem?
are traveler information systems the solution?” Transportation Research Part C: Emerging Technologies, vol.
1, no.
1, pp.
89–103, 1993.
[3] S.
Lockwood, “The 21st century operation oriented state dots, nchrp project 20–24,” Transportation research board, American Association of State Highway and Transportation Ofﬁcials, Washington, DC, 2006.
[4] S.
P.
Robinson, “The development and application of an urban link travel time model using data derived from inductive loop detectors,” Ph.D. dissertation, University of London, 2006.
[5] X.-Y.
Lu, P.
Varaiya, R.
Horowitz, and J.
Palen, “Faulty loop data analysis/correction and loop fault detection,” in 15th World Congress on Intelligent Transport Systems and ITS America’s 2008 Annual Meeting, 2008.
[6] N.
Zygouras, N.
Panagiotou, N.
Zacheilas, I.
Boutsis, V.
Kalogeraki, I.
Katakis, and D.
Gunopulos, “Towards detection of faulty trafﬁc sensors in real-time.” in MUD@ ICML, 2015, pp.
53–62.
[7] A.
Ghafouri, A.
Laszka, A.
Dubey, and X.
Koutsoukos, “Optimal detection of faulty trafﬁc sensors used in route planning,” in Proceedings of the 2nd International Workshop on Science of Smart City Operations and Platforms Engineering.
ACM, 2017, pp.
1–6.
[8] S.
Kamijo, Y.
Matsushita, K.
Ikeuchi, and M.
Sakauchi, “Trafﬁc monitoring and accident detection at intersections,” IEEE transactions on Intelligent transportation systems, vol.
1, no.
2, pp.
108–118, 2000.
[9] H.
Veeraraghavan, P.
Schrater, and N.
Papanikolopoulos, “Switching kalman ﬁlter-based approach for tracking and event detection at trafﬁc intersections,” in Intelligent Control, 2005.
Proceedings of the 2005 IEEE International Symposium on, Mediterrean Conference on Control and Automation.
IEEE, 2005, pp.
1167–1172.
[10] S.
Yang, K.
Kalpakis, and A.
Biem, “Detecting road trafﬁc events by coupling multiple timeseries with a nonparametric bayesian method,” IEEE Transactions on Intelligent Transportation Systems, vol.
15, no.
5, pp.
1936–1946, 2014.
[11] X.
Kong, X.
Song, F.
Xia, H.
Guo, J.
Wang, and A.
Tolba, “Lotad: long- term trafﬁc anomaly detection based on crowdsourced bus trajectory data,” World Wide Web, pp.
1–23, 2017.
[12] W.
Liu, Y.
Zheng, S.
Chawla, J.
Yuan, and X.
Xing, “Discovering spatio- temporal causal interactions in trafﬁc data streams,” in Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining.
ACM, 2011, pp.
1010–1018.
[13] L.
Xu, Y.
Yue, and Q.
Li, “Identifying urban trafﬁc congestion pattern from historical ﬂoating car data,” Procedia-Social and Behavioral Sciences, vol.
96, pp.
2084–2095, 2013.
[14] A.
H.
Chow, A.
Santacreu, I.
Tsapakis, G.
Tanasaranond, and T.
Cheng, “Empirical assessment of urban trafﬁc congestion,” Journal of advanced transportation, vol.
48, no.
8, pp.
1000–1016, 2014.
[15] S.
Kwoczek, S.
Di Martino, and W.
Nejdl, “Predicting and visualizing trafﬁc congestion in the presence of planned special events,” Journal of Visual Languages & Computing, vol.
25, no.
6, pp.
973–980, 2014.
[16] ——, “Stuck around the stadium?
an approach to identify road seg- ments affected by planned special events,” in Intelligent Transportation Systems (ITSC), 2015 IEEE 18th International Conference on.
IEEE, 2015, pp.
1255–1260.
[17] R.
Al Mallah, A.
Quintero, and B.
Farooq, “Distributed classiﬁcation of urban congestion using vanet,” IEEE Transactions on Intelligent Transportation Systems, 2017.
[23] [24] “Here trafﬁc api,” https://developer.here.com/rest-apis/documentation/ trafﬁc/topics v6.1/ﬂow.html.
.
M.
G.
of Nashville and D.
County, “Nashville ﬁre department,” 2017, [Online].
Available: http://www.nashville.gov/Fire-Department.aspx [Online; accessed 30-September-2017].
[25] O.
Wiki, “Tmc/location code list/location types,” 2017, 12-September-2017].
[Online].
Available: accessed openstreetmap.org/wiki/TMC/Location Code List/Location Types J.
A.
Hanley and B.
J.
McNeil, “The meaning and use of the area under a receiver operating characteristic (roc) curve.” Radiology, vol.
143, no.
1, pp.
29–36, 1982.
[Online; http://wiki.
[18] P.
Widhalm, H.
Koller, and W.
Ponweiser, “Identifying faulty trafﬁc detectors with ﬂoating car data,” in Integrated and Sustainable Trans- portation System (FISTS), 2011 IEEE Forum on.
IEEE, 2011, pp.
103–108.
[19] F.
Terroso-S´aenz, M.
Vald´es-Vela, C.
Sotomayor-Mart´ınez, R.
Toledo- Moreo, and A.
F.
G´omez-Skarmeta, “A cooperative approach to trafﬁc congestion detection with complex event processing and vanet,” IEEE Transactions on Intelligent Transportation Systems, vol.
13, no.
2, pp.
914–929, 2012.
[21] [20] H.
Wang, H.
Wen, F.
Yi, H.
Zhu, and L.
Sun, “Road trafﬁc anomaly detection via collaborative path inference from gps snippets,” Sensors, vol.
17, no.
3, p.
550, 2017.
J.
Kwon, M.
Mauch, and P.
Varaiya, “Components of congestion: Delay from incidents, special events, lane closures, weather, potential ramp metering gain, and excess demand,” Transportation Research Record: Journal of the Transportation Research Board, no.
1959, pp.
84–91, 2006.
[22] X.
Ma, Z.
Dai, Z.
He, J.
Ma, Y.
Wang, and Y.
Wang, “Learning trafﬁc as images: a deep convolutional neural network for large-scale transportation network speed prediction,” Sensors, vol.
17, no.
4, p.
818, 2017.
[26] [27] MathWorks, “Detector roc [On- curves,” line].
Available: https://www.mathworks.com/help/phased/examples/ detector-performance-analysis-using-roc-curves.html using 30-September-2017].
performance accessed analysis 2017, [Online; [28] Keras, “Keras: The python deep learning library,” 2017, [Online; accessed 30-September-2017].
[Online].
Available: https://keras.io/ [29] M.
Abadi, A.
Agarwal, P.
Barham, E.
Brevdo, Z.
Chen, C.
Citro, G.
S.
Corrado, A.
Davis, J.
Dean, M.
Devin et al., “Tensorﬂow: Large-scale machine learning on heterogeneous distributed systems,” arXiv preprint arXiv:1603.04467, 2016.

Matrix completion (e.g., [4,5,13]) is a fundamental problem in signal processing and machine learning, which studies the recovery of a low-rank matrix from an observation of a subset of its entries.
It has attracted a lot attention from researchers and practitioners and there are various motivating real-world applications including recommender systems and the Netﬂix challenge (see a recent overview in [6]).
A popular approach for matrix completion is to ﬁnd a matrix of minimal rank satisfying the observation constraints.
Due to the non-convexity of the rank function, popular approaches are convex relaxation (see, e.g., [8]) and nuclear norm minimization.
There is a rich literature, both in establishing performance bounds, developing eﬃcient algorithms and providing performance guarantees.
Recently there has also been new various results for non-convex formulations of matrix completion problem (see, e.g., [9]).
Existing conditions ensuring recovery of the minimal rank matrix are usually formulated in terms of missing-at-random entries and under an assumption of the so-called bounded- coherence (see a survey for other approaches in [6]; we do not aim to give a complete overview of the vast literature).
These results are typically aimed at establishing the recovery with a high probability.
The work [7] studies a related problem: the uniqueness conditions for minimum rank matrix recovery with random linear measurements of the true matrix (here the linear measurements correspond to inner product of a measurement mask matrix with the true matrix, and hence, the observations are diﬀerent from what one has in matrix completion).
With a deterministic pattern of observed entries, a complete characterization of the identiﬁable matrix for matrix completion remains an important yet open question: under what conditions for the pattern, there will be (at least locally) unique solution?
Recent work [12] provides some insights into this problem by studying the so-called completable problems and establishing conditions ensuring the existence of at most ﬁnitely many rank-r matrices that agree with all its observed entries.
A related work [1] studied this problem when there is a sparse noise that corrupts the entries.
In this paper, we aim to answer the question from a somewhat diﬀerent point of view and to give a geometric perspective.
In particular, we consider the solution of the Minimum Rank Matrix Completion (MRMC) formulation, which leads to a non-convex optimization problem.
We address the following questions: (i) Given observed entries arranged according to a (deterministic) pattern, by solving the MRMC problem, what is the minimum achievable rank?
(ii) Under what conditions, there will be a unique matrix that is a solution to the MRMC problem?
We give a suﬃcient and “almost necessary” condition (which we call the well-posedness condition) for the local uniqueness of MRMC solutions, and illustrate with special cases where such condition can be veriﬁed.
In addition, we also consider the convex relaxation and nuclear norm minimization formulations.
We argue that given m observation of an n1 × n2 matrix, if the minimal rank r∗ is less than R(n1, n2, m) := (n1 + n2)/2 − [(n1 + n2)2/4 − m]1/2, then the corresponding solution is unstable in the sense that an arbitrary small perturbation of the observed values can make this rank unattainable.
On the other hand if r∗ > R(n1, n2, m), then almost surely the solution is not (even locally) unique (cf., [18]).
This indicates that except in rare occasions, the MRMC problem cannot have both properties of possessing unique and stable solutions.
Consequently, what makes sense is to try to solve the minimum rank problem approximately and hence to consider low-rank approximation approaches (such as an approach mentioned in [6, 20]) as a better alternative to the MRMC formulation.
We also propose a sequential statistical testing procedure to determine the rank of the matrix from observed entries.
Such statistical approach can be useful for many existing low- rank matrix completion algorithms that require a pre-speciﬁcation of the matrix rank, such as the alternating minimization approach to solving the non-convex problem by representing the low-rank matrix as a product of two low-rank matrix factors (see, e.g., [6]).
The paper is organized as follows.
In the next section, we introduce the considered setting and some basic deﬁnitions.
In Section 3.1 we discuss rank reducibility from a generic point of view.
Section 3.2 is devoted to studying of the uniqueness of the MRMC solutions.
Low- rank approximations by the least squares method are discussed in Section 4.
In Section 3.4 we consider semideﬁnite relaxations of the MRMC problem.
Statistical testing of a low-rank model is discussed in Section 4.2. In Section 5 we present numerical results related to the developed theory.
Finally Section 6 concludes the paper.
All proofs are transferred to the Appendix.
2 Matrix completion and problem setup Consider the problem of recovering an n1×n2 data matrix of low rank when observing a small number m of its entries, which are denoted as Mij, (i, j) ∈ Ω.
Here Ω ⊂ {1, ..., n1}×{1, ..., n2} is an index set of cardinality m.
This goal can be written as the following optimization problem referred to as the Minimum Rank Matrix Completion (MRMC), (2.1) Let us introduce some necessary deﬁnitions.
Consider Ωc := {1, ..., n1} × {1, ..., n2} \ Ω, rank(Y ) subject to Yij = Mij, (i, j) ∈ Ω.
the complement of the index set Ω, and deﬁne min Y ∈Rn1×n2 VΩ :=(cid:8)Y ∈ Rn1×n2 : Yij = 0, (i, j) ∈ Ωc(cid:9) .
VΩc :=(cid:8)Y ∈ Rn1×n2 : Yij = 0, (i, j) ∈ Ω(cid:9) , This linear space represents the set of matrices that are ﬁlled with zeros at the locations of the unobserved entries.
Similarly deﬁne which represents the space of matrices that cannot be determined by the speciﬁed observa- tions.
We denote by M the n1 × n2 matrix with the speciﬁed entries Mij, (i, j) ∈ Ω, and all other entries equal zero.
By this construction, M + VΩc is the aﬃne space of all matrices that satisfy the observation constraints.
Note that M ∈ VΩ and the dimension of the linear space VΩ is dim(VΩ) = m, while dim(VΩc) = n1n2 − m.
By PΩ we denote the projection onto the space VΩ, i.e., [PΩ(Y )]ij = Yij for (i, j) ∈ Ω and [PΩ(Y )]ij = 0 for (i, j) ∈ Ωc.
We use conventional notations.
For a ∈ R we denote by (cid:100)a(cid:101) the least integer that is greater than or equal to a, and by (cid:98)a(cid:99) the largest integer less than or equal to a.
By A ⊗ B we denote the Kronecker product of matrices (vectors) A and B, and by vec(A) column vector obtained by stacking columns of matrix A.
We use the following matrix identity for matrices A, B, C of appropriate order vec(ABC) = (C(cid:62) ⊗ A)vec(B).
(2.2) By Sp we denote the linear space of p× p symmetric matrices and by writing X (cid:23) 0 we mean that matrix X ∈ Sp is positive semideﬁnite.
By σi(Y ) we denote the i-th largest singular value of matrix Y ∈ Rn1×n2.
By Ip we denote an identity matrix of dimension p.
• We say that a property holds for almost every (a.e) Mij, or almost surely, if the set of matrices Y ∈ VΩ for which this property does not hold has Lebesgue measure zero in the space VΩ.
3 Main theoretical results To gain insights into the identiﬁability issue of matrix completion, we aim to answer the following two questions: (i) what is achievable minimum rank (the optimal value of problem (2.1)), and (ii) whether the minimum rank matrix, i.e., the optimal solutions to (2.1), is unique given a problem set-up.
3.1 Rank reducibility We denote by r∗ the optimal value of problem (2.1).
That is, r∗ is the minimal rank of an n1 × n2 matrix with prescribed elements Mij, (i, j) ∈ Ω.
Clearly r∗ depends on the index set Ω and values Mij.
A natural question what values r∗ can attain.
Note that (2.1) is a non-convex problem and may have multiple solutions.
In the literature, the rank-objective function is often approximated by the nuclear norm function (which is convex), and various theoretical properties are derived; we will discuss this approach in Section 3.4. Theorem 3.1 (Upper bound for minimum achievable rank) The following upper bound always holds (3.1) r∗ ≤(cid:6)√ m(cid:7) .
The main idea of the proof is to show that two aﬃne spaces, one representing all matrices satisfying the observation constraints in (2.1), and another representing all matrices that have rank less than r, have non-empty intersection.
In a certain generic sense it is possible to give a lower bound for the minimal rank r∗.
Let us consider intersection of a set of low-rank matrices and the aﬃne space of matrices satisfying the observation constraints.
That is, consider the set of n1 × n2 matrices of rank r Mr :=(cid:8)Y ∈ Rn1×n2 : rank(Y ) = r(cid:9) (3.2) and the (aﬃne) mapping AM : VΩc → Rn1×n2 deﬁned as AM (X) := M + X, for X ∈ VΩc.
As it was pointed before, the image AM (VΩc) = M + VΩc of mapping AM deﬁnes the space of feasible points of the MRMC problem (2.1).
It is well known that Mr is a smooth, C∞, manifold with (3.3) It is said that the mapping AM intersects Mr transverally if for every X ∈ VΩc either AM (X) (cid:54)∈ Mr, or AM (X) ∈ Mr and the following condition holds dim(Mr) = r(n1 + n2 − r).
VΩc + TMr(Y ) = Rn1×n2, (3.4) where Y := AM (X) and TMr(Y ) denotes the tangent space to Mr at Y ∈ Mr. By using a classical result of diﬀerential geometry, it is possible to show that for almost every (a.e.) Mij, (i, j) ∈ Ω, the mapping AM intersects Mr transverally (this holds for every manifold Mr) (see [18] for a discussion of this result).
Transversality condition implies the following dimensionality condition dim(VΩc) + dim(TMr(Y )) ≥ dim(Rn1×n2).
In turn the above condition (3.5) can be written as r(n1 + n2 − r) ≥ m, or equivalently r ≥ R(n1, n2, m), where R(n1, n2, m) := (n1 + n2)/2 −(cid:112)(n1 + n2)2/4 − m.
(3.5) (3.6) (3.7) That is, if r < R(n1, n2, m), then the transversality condition (3.4) cannot hold and hence for a.e. Mij it follows that rank(M + X) (cid:54)= r for all X ∈ VΩc.
Now if AM intersects Mr transverally at AM (X) ∈ Mr (i.e., condition (3.4) holds), then the intersection AM (VΩc)∩Mr forms a smooth manifold near the point Y := AM (X).
When r > R(n1, n2, m), this manifold has dimension greater than zero and hence the corresponding rank r solution is not (locally) unique.
This leads to the following (for a formal discussion of these results we can refer to [18]).
Theorem 3.2 (Lower bound and non-uniqueness of solutions) For any index set Ω of cardinality m and almost every Mij, (i, j) ∈ Ω, the following holds: (i) for every feasible point Y of problem (2.1) it follows that (3.8) (ii) if r∗ > R(n1, n2, m), then problem (2.1) has multiple (more than one) optimal solutions.
rank(Y ) ≥ R(n1, n2, m), In particular when we have a square matrix n1 = n2 = n, it follows that R(n, n, m) = n − n2 − m.
(3.9) For n1 = n2 = n and small m/n2 we can approximate R(n, n, m) = n (cid:16) 1 −(cid:112)1 − m/n2 (cid:17) ≈ m/(2n).
For example, for n1 = n2 = 1000 and m = 20000 we have m = 141.4 and R(n, n, m) = 10.05, and hence the bounds (3.10) become 11 ≤ r∗ ≤ 142.
It follows from part (i) of Theorem 3.2 that r∗ ≥ R(n1, n2, m) for a.e. Mij.
Therefore for m ≤ min{n1, n2}, generically (almost surely) the minimal rank r∗ is between the bounds R(n1, n2, m) ≤ r∗ ≤(cid:6)√ m(cid:7) , sider data matrix M of the following form M =(cid:0) M1 0 (3.10) and (2.1) may have unique optimal solution only when r∗ = R(n1, n2, m).
Of course such equality could happen only if R(n1, n2, m) is an integer number.
As Example 3.1 below shows, for any integer r∗ satisfying (3.10) there exists an index set Ω such that the corresponding MRMC problem attains the minimal rank r∗ for a.e. Mij.
In particular this shows that the lower and upper bounds in (3.10) are tight.
Example 3.1 (Tightness of lower and upper bounds for r∗) For r < min{n1, n2} con- M2, M3, of the respective order r × r, (n1 − r) × r and (n1 − r) × (n2 − r), represent the observed entry values.
Cardinality m of the corresponding index set Ω is r(n1 + n2 − r), i.e., here r = R(n1, n2, m).
Suppose that the r × r matrix M1 is nonsingular, i.e., its rows are linearly independent.
Then any row of matrix M2 can be represented as a (unique) linear combination of rows of matrix M1.
It follows that the corresponding MRMC problem has (unique) solution of rank r∗ = r.
In other words, the rank of the completed matrix will be equal to r (the rank of the sub-matrix M1) and there will be a unique matrix that achieves this rank.
Now suppose that some of the entries of the matrices M2 and M3 are not ob- served, and hence cardinality of the respective index set Ω is less than r(n1 +n2−r), and thus r > R(n1, n2, m).
In that case the respective minimal rank still is r, provided matrix M1 is nonsingular, although the corresponding optimal solutions are not unique.
In particular, if (cid:1), i.e., only the entries of matrix M1 are observed, then m = r2 and the minimum (cid:1) Here, the three sub-matrices M1, M2 M3 M =(cid:0) M1 0 0 0 rank is r.
3.2 Uniqueness of solutions of the MRMC problem For a given matrix M ∈ VΩ and the corresponding minimal rank r∗ ≤ R(n1, n2, m) the question is whether the corresponding solution Y ∗ of rank r∗ is unique.
Although, as it was discussed in the previous section, the set of such matrices M is “thin” (in the sense that it has Lebesgue measure zero), this question of uniqueness is important, in particular for the statistical inference (discussed in Section 4.2) when entries Mij are observed with a noise.
Available results, based on the so-called Restricted Isometry Property (RIP) for low-rank matrix recovery from linear observations and based on the coherence property for low-rank matrix completion, assert that for certain probabilistic (Gaussian) models such uniqueness holds with high probability.
However for a given matrix M ∈ VΩ it is not clear how to verify whether the solution is unique.
Let us consider the following concept of local uniqueness of solutions.
Deﬁnition 3.1 We say that an n1×n2 matrix ¯Y is a locally unique solution of problem (2.1) if PΩ( ¯Y ) = M and there is a neighborhood V ⊂ Rn1×n2 of ¯Y such that rank(Y ) (cid:54)= rank( ¯Y ) for any Y ∈ V, Y (cid:54)= ¯Y .
Note that rank is a lower semicontinuous function of matrix, i.e., if {Yk} is a sequence of matrices converging to matrix Y , then lim inf k→∞ rank(Yk) ≥ rank(Y ).
Therefore local uniqueness of ¯Y actually implies existence of the neighborhood V such that rank(Y ) > rank( ¯Y ) for all Y ∈ V, Y (cid:54)= ¯Y , i.e., that at least locally problem (2.1) does not have optimal solutions diﬀerent from ¯Y .
To explain the main result, we will introduce some constructions associated with the manifold of low-dimensional matrices.
There are several equivalent forms how the tangent space to the manifold Mr at Y ∈ Mr can be represented.
In one way it can be written as TMr(Y ) =(cid:8)Q1Y + Y Q2 : Q1 ∈ Rn1×n1, Q2 ∈ Rn2×n2(cid:9) .
(3.11) In an equivalent form this tangent space can be written as TMr(Y ) =(cid:8)H ∈ Rn1×n2 : F HG = 0(cid:9) , (3.12) where F is an (n1 − r)× n1 matrix of rank n1 − r such that F Y = 0 (referred to as a left side complement of Y ) and G is an n2× (n2− r) matrix of rank n2− r such that Y G = 0 (referred to as a right side complement of Y ).
We also use the linear space of matrix orthogonal (normal) to Mr at Y ∈ Mr, denoted by NMr(Y ).
A matrix Z is orthogonal to Mr at Y ∈ Mr if and only if tr(Z(cid:62)Y (cid:48)) = 0 for all Y (cid:48) ∈ TMr(Y ).
By (3.11) this means that Since tr(cid:2)Z(cid:62)(Q1Y + Y Q2)(cid:3) = tr(Y Z(cid:62)Q1)+tr(Z(cid:62)Y Q2) and matrices Q1 and Q2 are arbitrary, tr(cid:2)Z(cid:62)(Q1Y + Y Q2)(cid:3) = 0, ∀Q1 ∈ Rn1×n1, ∀Q2 ∈ Rn2×n2.
NMr(Y ) =(cid:8)Z ∈ Rn1×n2 : Z(cid:62)Y = 0 and Y Z(cid:62) = 0(cid:9) .
it follows that the normal space can be written as (3.13) (3.14) Deﬁnition 3.2 (Well-posedness condition) We say that a matrix ¯Y ∈ Mr is well-posed for problem (2.1) if PΩ( ¯Y ) = M and the following condition holds VΩc ∩ TMr( ¯Y ) = {0}.
(3.15) Condition (3.15) (illustrated in Figure 1) is a natural condition having a simple geometri- cal interpretation.
Intuitively, it means that the null space of the observation operator does not have any non-trivial matrix that lies in the tangent space of low-rank matrix manifold.
Hence, there cannot be any local deviation from the optimal solution that still satisﬁes the measurement constraints.
More formally, suppose that condition (3.15) does not hold, i.e., there exists nonzero matrix H ∈ VΩc ∩TMr( ¯Y ).
This means that there is a curve Z(t) ∈ Mr starting at ¯Y and tangential to H, i.e., Z(0) = ¯Y and (cid:107) ¯Y + tH − Z(t)(cid:107) = o(t).
Of course if moreover PΩ(Z(t)) = M for all t near 0 ∈ R, then solution ¯Y is not locally unique.
Although this is not guaranteed, i.e., the suﬃcient condition (3.15) may be not necessary for local uniqueness of the solution ¯Y , violation of this condition implies that solution ¯Y is unstable in the sense that for some matrices Y ∈ Mr close to ¯Y the distance (cid:107)PΩ(Y ) − M(cid:107) is of order o((cid:107)Y − ¯Y (cid:107)).
This motivates us to introduce the well-posedness condition that guarantees a matrix to be locally unique solution.
Figure 1: Illustration of well-posedness condition.
Now we can give suﬃcient conditions for local uniqueness: Theorem 3.3 (Suﬃcient conditions for local uniqueness) Matrix ¯Y ∈ Mr is a lo- cally unique solution of problem (2.1) if ¯Y is well-posed for (2.1).
Below we present an equivalent form of the well-posedness condition.
By Theorem 3.3 we have that if matrix ¯Y ∈ Mr is well-posed, then ¯Y is a locally unique solution of problem (2.1).
Note that condition (3.15) implies that dim(VΩc) + dim(TMr( ¯Y )) ≤ n1n2.
That is, condition (3.15) implies that r(n1 + n2 − r) ≤ m or equivalently r ≤ R(n1, n2, m).
By Theorem 3.2 we have that if r∗ > R(n1, n2, m), then the corresponding optimal solution !ℳ#$%$%ℳ&$%+()*cannot be locally unique almost surely.
Note that since the space VΩ is orthogonal to the space VΩc, by duality arguments condition (3.15) is equivalent to the following condition VΩ + NMr( ¯Y ) = Rn1×n2.
(3.16) By using formula (3.12) it is also possible to write condition (3.15) in the following form {X ∈ VΩc : F XG = 0} = {0}, (3.17) where F is a left side complement of ¯Y and G is a right side complement of ¯Y .
Recall that vec(F XG) = (G(cid:62)⊗F )vec(X).
Column vector of matrix G(cid:62)⊗F corresponding to component j ⊗ fi, where fi is the i-th column of matrix F and gj is the j-th xij of vector vec(X), is g(cid:62) j ⊗ fi, (i, j) ∈ Ωc, are row of matrix G.
Condition (3.17) means that the column vectors g(cid:62) linearly independent.
We obtain the following result which can be useful for checking the well-posedness condition.
Theorem 3.4 (Equivalent condition of well-posedness) Matrix ¯Y ∈ Mr is well-posed for problem (2.1) if and only if for any left side complement F and right side complement G of ¯Y , the column vectors g(cid:62) j ⊗ fi, (i, j) ∈ Ωc, are linearly independent.
This in turn implies the following necessary condition for well-posedness of ¯Y ∈ Mr in terms of the pattern of the index set Ω.
Theorem 3.5 (Necessary condition for well-posedness) If matrix ¯Y ∈ Mr is well- posed for problem (2.1), then at each row and each column of ¯Y there are at least r elements of the index set Ω.
However, although necessary, the condition for the index set Ω to have at each row and each column at least r elements is not suﬃcient to ensure well-posedness as shown by Theorem 3.6 below.
Note that by deﬁnition the matrices F and G are of full rank.
Remark 3.1 (Special case with veriﬁable globally unique solutions) In some rather special cases it is possible to verify global uniqueness of minimum rank solutions (actually such conditions work well for the so-called Minimum Rank Factor Analysis discussed in Sec- tion 3.4).
Let ¯Y ∈ Mr be such that PΩ( ¯Y ) = M .
Consider an index (k, l) ∈ Ωc.
Suppose that there exist I1 ⊂ {1, ..., n1} \ {k} and I2 ⊂ {1, ..., n2} \ {l} such that |I1| = |I1| = r and I1 × I2 ⊂ Ω.
Consider the r × r submatrix of M corresponding to rows i ∈ I1 and columns j ∈ I2.
Suppose that this submatrix is nonsingular.
Then it follows that the minimum rank r∗ ≥ r.
Suppose further that {k} × I2 ∈ Ω and {l} × I1 ∈ Ω.
Then for any matrix Y ∈ Mr such that PΩ(Y ) = M we have that Ykl = Mkl.
This follows by observing that the (r + 1) × (r + 1) submatrix of Y corresponding to rows {k} ∪ I1 and columns {l} ∪ I2 has rank r and hence zero determinant, and applying Shur complement for the element Ykl.
If this holds for every (k, l) ∈ Ωc, then the uniqueness of the solution ¯Y follows.
Remark 3.2 In Wilson and Worcester [21] was constructed an example of two 6 × 6 sym- metric matrices of rank 3 with the same oﬀ-diagonal and diﬀerent diagonal elements.
If we deﬁne the index set as Ω := {(i, j) : i (cid:54)= j, i, j = 1, ..., 6}, then this can be viewed as an R(6, 6, 30) = 6−√ example of two diﬀerent locally unique solutions of rank 3.
Note that here m = 30 and 6.
That is R(6, 6, 30) > 3 and generically (almost surely) rank cannot be reduced below r = 4.
We will discuss this example further in Section 5.
Note that uniqueness of the minimum rank solution is invariant with respect to per- mutations of rows and columns of matrix M .
This motivates to introduce the following deﬁnition.
Deﬁnition 3.3 We say that the index set Ω is reducible if by permutations of rows and columns, matrix M can be represented in a block diagonal form, i.e., (cid:21) (cid:20) M1 0 M2 Otherwise we say that Ω is irreducible.
M = (3.18) Theorem 3.6 (Reducible index set) If the index set Ω is reducible, then any minimum rank solution ¯Y , such that ¯Yij (cid:54)= 0 for all (i, j) ∈ Ωc, is not locally (and hence globally) unique.
As it was shown in Proposition 3.3, if ¯Y is not locally unique, then it cannot be well- posed.
Therefore if the index set Ω is reducible, then any minimum rank solution, diﬀerent from M itself, is not well-posed.
Of course even if Ω is reducible, it still can happen that in each row and column there are at least r elements of the index set Ω.
That is, the condition discussed in Theorem 3.5 may be not suﬃcient to ensure the well-posedness property.
3.3 Uniqueness of rank one solutions In this section we discuss uniqueness of rank one solutions of the MRMC problem (2.1).
We show that in case of the minimum rank one, irreducibility of Ω is suﬃcient for the global uniqueness.
We assume that all Mij (cid:54)= 0, (i, j) ∈ Ω, and that every row and every column of the matrix M has at least one element Mij.
Let ¯Y be a solution of rank one of problem (2.1), i.e., there are nonzero column vectors v and w such that ¯Y = vw(cid:62) with PΩ( ¯Y ) = M .
Recall that permutations of the components of vector v corresponds to permutations of the rows of the respective rank one matrix, and permutations of the components of vector w corresponds to permutations of the columns of the respective rank one matrix.
It was shown in Corollary 3.6 that if the index set Ω is reducible, then solution ¯Y cannot be locally unique: Theorem 3.7 (Global uniqueness for rank one solution) Suppose that Ω is irreducible, Mij (cid:54)= 0 for all (i, j) ∈ Ω, and every row and every column of the matrix M have at least one element Mij.
Then any rank one solution is globally unique.
3.4 Semideﬁnite relaxations and nuclear norm minimization In this section we consider alternative versions of MRMC problem that have been studied in literature, including the convex relaxation formulation (see, e.g. [8]) and nuclear norm minimization, and study solution uniqueness.
We also make a connection to existing results in Factor Analysis.
The MRMC problem (2.1) can be formulated in the following equivalent form where Ξ ∈ Sp, p = n1 + n2, is symmetric matrix of the form Ξ =(cid:0) 0 M rank(Ξ + X) subject to Ξ + X (cid:23) 0, min X∈WSc (3.19) (cid:1).
Minimization in (3.19) is performed over matrices X ∈ Sp which are complement to Ξ in the sense of having zero entries at all places corresponding to the speciﬁed values Mij, (i, j) ∈ Ω.
That is, let S ⊂ {1, ..., p} × {1, ..., p} be the symmetric index set corresponding to the index set Ω, i.e., (i, j) ∈ S when 1 ≤ i ≤ j ≤ n1, if and only if (i, j + n2) ∈ Ω; and if (i, j) ∈ S, then (j, i) ∈ S.
By Sc ⊂ {1, ..., p} × {1, ..., p} we denote the symmetric index set complement of S.
Deﬁne M(cid:62) 0 WS := {X ∈ Sp : Xij = 0, (i, j) ∈ Sc} and WSc := {X ∈ Sp : Xij = 0, (i, j) ∈ S}.
We consider a more general minimum rank problem of the form (3.19) in that we allow the index set S to be a general symmetric subset of {1, ..., p} × {1, ..., p}, with a given matrix Ξ ∈ WS.
Note that WS ∩ WSc = {0} and WS + WSc = Sp. As a heuristic it was suggested in [8] to approximate problem (3.19) by the following problem tr(X) subject to Ξ + X (cid:23) 0.
min X∈WSc (3.20) Together with the minimum rank problem (3.19) we consider the following SDP problem tr(CX) subject to Ξ + X (cid:23) 0, min X∈WSc (3.21) for some matrix C ∈ WSc. Of course problem (3.20) is a particular case of problem (3.21) for C := Ip. We have the following result (cf., [17]).
Theorem 3.8 (Global uniqueness of trace minimization) Suppose that the matrix C ∈ WSc is positive deﬁnite.
Then for a.e. Ξ ∈ WS problem (3.21) has unique optimal solution.
Recall that by saying that a property holds for a.e. Ξ ∈ WS we mean that it holds for all Ξ in WS except for a subset of WS of Lebesgue measure zero.
The classical Minimum Rank Factor Analysis (MRFA) can be viewed as a particular case of problem (3.19) with WSc being the space Dp of p × p diagonal matrices, and given symmetric matrix Ξ of oﬀ diagonal elements.
It is possible to show that generically (i.e., for a.e. Ξ) the reduced rank of the MRFA problem is bounded (cf., [14]): rank(Ξ + X) ≥ 2p + 1 − √ 8p + 1 , ∀X ∈ Dp. (3.22) 10 In Factor Analysis the respective minimum trace problem of the form (3.20) is called the Minimum Trace Factor Analysis (MTFA).
A relation between MRFA and MTFA problems is discussed in [14, 15].
In Factor Analysis the setting of Remark 3.1 can be used to show that in a certain generic sense, MRFA solution is unique if the respective minimal rank is less than p/2 (we can refer to [3], and references therein, for a discussion of uniqueness of MRFA solutions).
Consider the SDP problem (3.21), and assume that the matrix C ∈ WSc is positive deﬁnite.
The (Lagrangian) dual of problem (3.21) is the problem tr(CX) − tr[Λ(Ξ + X)].
(3.23) For Λ = C − Θ, with Θ ∈ WS, problem (3.23) can be written (note that tr(CΞ) = 0 for Ξ ∈ WS) as min X∈WSc max Λ(cid:23)0 tr(ΘΞ) subject to C − Θ (cid:23) 0.
max Θ∈WS (3.24) Note that for both problems (3.23) and (3.24) the Slater condition holds, and hence there is no duality gap between these problems, and both problems have nonempty bounded sets of optimal solutions.
Optimality conditions (necessary and suﬃcient) for problem (3.21) are C = Pτ (Λ), (Ξ + X)Λ = 0, Λ (cid:23) 0, Ξ + X (cid:23) 0, X ∈ WSc. (3.25) (3.26) (3.27) Now suppose that ¯X ∈ WSc is such that Ξ + ¯X (cid:23) 0 and rank(Ξ + ¯X) = r < p.
Let E be a p × (p − r) matrix of rank p − r such that (Ξ + ¯X)E = 0.
By the optimality conditions (3.25)–(3.27) we have that ¯X is an optimal solution of the SDP problem (3.21) if and only if the following condition holds: there exists Z ∈ Sp−r + such that Pτ (EZE(cid:62)) = C.
Equations Pτ (EZE(cid:62)) = C can be viewed as a system of dim(Wτ ) equations with (p − r)(p − r + 1)/2 unknowns (nonduplicated elements of matrix Z ∈ Sp−r).
When r is “small” and consequently (p − r)(p − r + 1)/2 > dim(Wτ ), it is likely that this system will have a solution Z (cid:23) 0, and hence ¯X is an optimal solution of problem (3.21).
This of course is a heuristic argument and a more careful analysis is needed.
We can also view this by adjusting weight matrix C to the considered matrix Ξ + ¯X by choosing Z (cid:23) 0 and deﬁning C := Pτ (EZE(cid:62)).
For such C the corresponding SDP problem has ¯X as an optimal solution.
Note that although matrix EZE(cid:62) is positive semideﬁnite when Z (cid:23) 0, there is no guarantee that the corresponding matrix Pτ (EZE(cid:62)) is positive semideﬁnite.
Next we consider the nuclear norm minimization problem.
Consider the following (con- vex) problem where (cid:107) · (cid:107)∗ is the nuclear norm (cid:107)Y (cid:107)∗ = (cid:80)min{n1,n2} (cid:107)X + M(cid:107)∗, min X∈VΩc σi(Y ).
For the nuclear norm its dual norm is given by σ1(·) and the respective unit ball of the dual norm can be written as i=1 (3.28) 11 B =(cid:8)Q ∈ Rn1×n2 : λ1(Q(cid:62)Q) ≤ 1(cid:9) , where λ1(Q(cid:62)Q) is the largest eigenvalue of matrix Q(cid:62)Q.
It follows that (cid:107)Y (cid:107)∗ is equal to the optimal value of the following SDP problem tr(Q(cid:62)Y ) s.t. max Q∈Rn1×n2 The Lagrangian dual of this problem is Q(cid:62) In2 (cid:20) In1 Q (cid:20) Λ1 − 1 2Y (cid:62) (cid:21) (cid:23) 0.
(cid:21) − 1 2Y Λ2 (cid:23) 0.
min Λ1∈Sn1 , Λ2∈Sn2 tr(Λ1 + Λ2) s.t. index set S being the symmetric index set corresponding to the index set Ω and Ξ :=(cid:0) 0 M It follows that here problem (3.28) can be written in the form (3.20) with p = n1 + n2, the M(cid:62) 0 (the coeﬃcient −1/2 can be absorbed into X).
Then by Theorem 3.8 we have that almost surely (i.e., for a.e. Mij) the nuclear norm minimization problem has unique optimal solution.
(cid:1) 4 Low-rank matrix approximation In this section, we discuss a formulation of the MRMC based on ﬁnding a low-rank ap- proximation with noisy and incomplete observations of matrix entries.
We refer to it as the “Low-Rank Matrix Approximation” (LRMA) problem.
Compared with the formulation of exact low rank recovery, the LRMA is more realistic in the presence of noise.
We discuss properties of the LRMA problem and then introduce a statistical test procedure for choosing the true rank of the matrix.
4.1 LRMA and its properties By Theorem 3.2 we have that if the minimal rank r∗ is less than R(n1, n2, m), then the corresponding solution is unstable in the sense that an arbitrary small perturbation of the observed values Mij can make this rank unattainable.
On the other hand if r∗ > R(n1, n2, m), then almost surely the solution is not (even locally) unique.
This indicates that except in rare occasions, problem (2.1) of exact rank minimization cannot have both properties of possessing unique and stable solutions.
Consequently, what makes sense is to try to solve the minimum rank problem approximately.
That is consider the problem min Y ∈Rn1×n2 , X∈VΩc F (X + M, Y ) subject to rank(Y ) = r, (4.1) where M ∈ VΩ is a given data matrix, and F (A, B) is a discrepancy between matrices A, B ∈ Rn1×n2.
For example if F (A, B) := (cid:107)A − B(cid:107)2 i,j Y 2 ij, being the Frobenius norm, then (4.1) becomes the least squares problem F with (cid:107)Y (cid:107)2 F = tr(Y (cid:62)Y ) = (cid:80) (cid:88) min Y ∈Mr (i,j)∈Ω 12 (Mij − Yij)2 .
(4.2) The least squares approach although is natural, is not the only one possible.
For example, in the statistical approach to Factor Analysis the discrepancy function is based on the Maximum Likelihood method and is more involved.
We discuss below the least squares approach.
There are several questions related to formulation of the approximation problem (4.2).
The ﬁrst question is how to solve problem (4.2) numerically.
In general problem (4.2) is not convex and could be diﬃcult to solve.
Proposition 4.1 (Necessary condition for LRMA) The following are necessary condi- tions for Y ∈ Mr to be an optimal solution of problem (4.2) (PΩ(Y ) − M )(cid:62)Y = 0 and Y (PΩ(Y ) − M )(cid:62) = 0.
(4.3) Remark 4.1 We can view the least squares problem (4.2) from the following point of view.
Consider function φ(Y, Θ) := 1 2tr[(PΩ(Y ) − Θ)(cid:62)(PΩ(Y ) − Θ)], (4.4) (4.5) with Θ ∈ VΩ viewed as a parameter.
Deﬁne (Yij − Mij)2 = 1 f (Y ) := 1 (cid:88) (i,j)∈Ω 2tr[(PΩ(Y ) − M )(cid:62)(PΩ(Y ) − M )], Hence, the problem (4.2) consists of minimization of f (Y ) subject to Y ∈ Mr. Note that for Θ = M we have f (·) = φ(·, M ), where f (·) is deﬁned in (4.5).
Let ¯Y ∈ Mr be such that φ( ¯Y , Θ0) = 0 for some Θ0 ∈ VΩ, i.e., PΩ( ¯Y ) = Θ0.
A suﬃcient condition for ¯Y to be a locally unique solution of problem (2.1), at M = Θ0, is tr(cid:2)PΩ(H)(cid:62)PΩ(H)(cid:3) > 0, ∀H ∈ TMr( ¯Y ) \ {0}.
(4.6) The above condition means that if H ∈ TMr( ¯Y ) and H (cid:54)= 0, then PΩ(H) (cid:54)= 0.
In other words this means that the kernel Ker(PΩ) := {H ∈ TMr( ¯Y ) : PΩ(H) = 0} is {0}.
Since PΩ(H) = 0 for any H ∈ VΩc, it follows that: condition (4.6) is equivalent to the suﬃcient condition (3.15) of Proposition 3.3. That is, condition (4.6) means that matrix ¯Y is well-posed for problem (2.1).
Assuming that condition (4.6) (or equivalently condition (3.15)) holds, by applying the Implicit Function Theorem to the ﬁrst order optimality conditions of the least squares prob- lem (4.2) we have the following result.
Proposition 4.2 Let ¯Y ∈ Mr be such that PΩ( ¯Y ) = Θ0 for some Θ0 ∈ VΩ and suppose that the well posedness condition (3.15) holds.
Then there exist neighborhoods V and W of ¯Y and Θ0, respectively, such that for any M ∈ W ∩ VΩ there exists unique Y ∈ V ∩ Mr satisfying the optimality conditions (4.3).
13 (4.3).
Then if PΩ( ¯Y ) is suﬃciently close to M (i.e., the ﬁt(cid:80) The above proposition implies the following.
Suppose that we run a numerical procedure which identiﬁes a matrix ¯Y ∈ Mr satisfying the (necessary) ﬁrst order optimality conditions (i,j)∈Ω (Yij − Mij)2 is suﬃciently small) and condition (3.15) holds at ¯Y , then we can say that f (Y ) > f ( ¯Y ) for all Y (cid:54)= ¯Y in a neighborhood of ¯Y .
That is, ¯Y solves the least squares problem at least locally.
Unfortunately it is not clear how to quantify the “suﬃciently close” condition, and this does not guarantee global optimality of ¯Y unless ¯Y is the unique minimum rank solution.
4.2 Statistical test for rank selection The second important question for LRMA is what rank r to use for a considered data matrix M ∈ VΩ.
By the above discussion it will be natural to take some value of r less than R(n1, n2, m), since otherwise we will not even have locally unique solution.
Can the ﬁt of Y ∈ Mr to X + M , and hence the choice of r, be tested in some statistical sense?
In this section we discuss statistical testing of value of the “true” minimal rank when the entries of the data matrix M are observed with noise.
In order to proceed we assume the following model with noisy and possibly biased obser- vations of a subset of matrix entries.
There is a (population) value Y ∗ of n1 × n2 matrix of ij, (i, j) ∈ Ω, rank r < R(n1, n2, m) and Mij are viewed as observed (estimated) values of Y ∗ based on a sample of size N .
The observed values are modeled as ij + N−1/2∆ij + εij, (i, j) ∈ Ω, (4.7) where Y ∗ ∈ Mr and ∆ij are some (deterministic) numbers.
The random errors εij are assumed to be independent of each other and such that N 1/2εij converge in distribution to ij, (i, j) ∈ Ω.
The additional terms N−1/2∆ij in (4.7) normal with mean zero and variance σ2 represent a possible deviation of population values from the “true” model and are often referred to as the population drift or a sequence of local alternatives (we can refer to [11] for a historical overview of invention of the local alternatives setting).
This is a reasonably realistic model motivated by many real applications.
Deﬁnition 4.1 We say that the model is globally identiﬁed (at Y ∗) if ¯Y ∈ Rn1×n2 of rank( ¯Y ) ≤ r and PΩ( ¯Y ) = PΩ(Y ∗) imply that ¯Y = Y ∗, i.e., Y ∗ is the unique solution of the respective matrix completion problem.
Similarly it is said that the model is locally identiﬁed if this holds for all such ¯Y in a neighborhood of Y ∗, i.e., Y ∗ is a locally unique solution.
Mij = Y ∗ Consider the following weighted least squares problem wij (Mij − Yij)2 , (cid:88) min Y ∈Mr (i,j)∈Ω for some weights wij > 0, (i, j) ∈ Ω.
(Of course, if wij = 1, (i, j) ∈ Ω, then problem (4.8) coincides with the least squares problem (4.2).) We have the following standard result about consistency of the least squares estimates.
14 (4.8) Proposition 4.3 Suppose that the model is globally identiﬁed at Y ∗ ∈ Mr and values Mij, (i, j) ∈ Ω, converge in probability to the respective values Y ∗ ij as the sample size N tends to inﬁnity.
Then an optimal solution ˆY of problem (4.8) converges in probability to Y ∗ as N → ∞.
Consider the following weighted least squares test statistic TN (r) := N min Y ∈Mr wij (Mij − Yij)2 , (4.9) (cid:88) (i,j)∈Ω (cid:88) (i,j)∈Ω (cid:88) (i,j)∈Ω (cid:88) (i,j)∈Ω (cid:0)Y ∗ ij with ˆσ2 where wij := 1/ˆσ2 ij converge in probability ij as N → ∞).
Recall that the respective condition of form (3.15), or equivalently (4.6), to σ2 is suﬃcient for local identiﬁablity of Y ∗.
The following asymptotic results can be compared with similar results in the analysis of covariance structures (cf., [19]).
ij being consistent estimates of σ2 ij (i.e., ˆσ2 Proposition 4.4 (Asymptotic properties of test statistic) Consider the noisy obser- vation model (4.7).
Suppose that the model is globally identiﬁed at Y ∗ ∈ Mr and Y ∗ is well-posed for problem (2.1).
Then the test statistic TN (r) converges in distribution to non- central chi square with degrees of freedom df r = m − r(n1 + n2 − r) and the noncentrality parameter δr = min H∈TMr (Y ∗) ij (∆ij − Hij)2 .
σ−2 (4.10) Note that the optimal (minimal) value of the weighted least squares problem (4.8) can be approximated by min H∈TMr (Y ∗) wij (Eij − Hij)2 + RN , (4.11) with Eij := N−1/2∆ij + εij and the error term RN = o ((cid:107)M − PΩ(Y ∗)(cid:107)2) being of stochastic order RN = op(N−1).
Hence, the noncentrality parameter, given in (4.10), can be approxi- mated as δr ≈ N min Y ∈Mr wij ij + N−1/2∆ij − Yij (4.12) (cid:1)2 That is, the noncentrality parameter is approximately equal to N times the ﬁt to the “true” model of the alternative population values Y ∗ ij + N−1/2∆ij under small perturbations of order O(N−1/2).
Remark 4.2 The above asymptotic results are formulated in terms of the “sample size N ” suggesting that the observed values are estimated from some data.
This allows to formulate mathematically precise convergence results.
One can take a more pragmatic point of view that when there is a “small” random noise in the observed values, the respective test statistics properly normalized with respect to magnitude of that noise have approximately a noncentral chi square distribution.
15 The asymptotics of the test statistic TN (r) depends on r and also on the cardinality m of the index set Ω.
Suppose now that more observations become available at additional entries of the matrix.
That is we are testing now the model with a larger index set Ω(cid:48), of cardinality m(cid:48), such that Ω ⊂ Ω(cid:48).
In order to emphasize that the test statistic also depends on the corresponding index set we add the index set in the respective notations.
Note that if Y ∗ is a solution of rank r for both sets Ω and Ω(cid:48) and the model is globally (locally) identiﬁed at Y ∗ for the set Ω, then the model is globally (locally) identiﬁed at Y ∗ for the set Ω(cid:48).
Note also that if the regularity condition (3.15) holds at Y ∗ for the smaller model (i.e. for Ω), then it holds at Y ∗ for the larger model (i.e. for Ω(cid:48)).
The following result can be proved in the same way as Theorem 4.4 (cf., [19]).
Proposition 4.5 Consider index sets Ω ⊂ Ω(cid:48) of cardinality m = |Ω| and m(cid:48) = |Ω(cid:48)|, and the noisy observation model (4.7).
Suppose that the model is globally identiﬁed at Y ∗ ∈ Mr and condition (3.15) holds at Y ∗ for the smaller model (and hence for both models).
Then the statistic TN (r, Ω(cid:48)) − TN (r, Ω) converges in distribution to noncentral χ2 with df r,Ω(cid:48) − df r,Ω = m(cid:48)−m degrees of freedom and the noncentrality parameter δr,Ω(cid:48)−δr,Ω, and TN (r, Ω(cid:48))−TN (r, Ω) is asymptotically independent of TN (r, Ω).
It is also possible to give asymptotic distribution of solutions of problem (4.8).
Suppose now that the assumptions of Theorem 4.4 hold with all ∆ij in equation (4.7) being zeros.
Let ˆYN be a solution of problem (4.8), i.e., ˆYN ∈ arg min Y ∈Mr (cid:88) wij (i,j)∈Ω (cid:16) Y ∗ ij + εij (cid:124) (cid:123)(cid:122) (cid:125) Mij −Yij (cid:17)2 Then N 1/2( ˆYN − Y ∗) converges in distribution to a random matrix A(Z), where Z ∈ VΩ is a random matrix with entries Zij ∼ N (0, σ2 ij), (i, j) ∈ Ω, having normal distribution and independent of each over, and A(Z) is the optimal solution of the problem ij (Zij − Hij)2 .
σ−2 (4.13) (cid:88) min H∈TMr (Y ∗) (i,j)∈Ω Note that A(·) is a linear operator and hence A(Z) has a multivariate normal distribution with zero means.
Note also that (compare with (7.5)) rank(A(Z)) = dim(cid:0)PΩ(TMr(Y ∗)(cid:1) = r(n1 + n2 − r).
(4.14) 5 Numerical experiments In this section we give a summary of numerical experiments illustrating the presented theory.
For a more extensive discussion of these results we refer to the supplementary material at https://github.com/ruizhang-ray/Matrix-Completion.
16 5.1 An example of 6×6 matrix considered in [21] As it was pointed in Remark 3.2, in [21] was given an example of two diﬀerent locally unique solutions of rank r∗ = 3 for a 6 × 6 matrix with the index set Ω corresponding to its oﬀ-diagonal elements.
The matrix M in that example is  M = 0.56 0.16 0.48 0.24 0.64 0.20 0.66 0.51 0.86 0.18 0.07 0.23 0.72 0.41 0.56 0.16 0.20 0.48 0.66 0.18 0.24 0.51 0.07 0.30 0.64 0.86 0.23 0.72 0.41 0.3  The two rank 3 solutions are given by diagonal matrices: D1 = Diag(0.64, 0.85, 0.06, 0.56, 0.50, 0.93), D2 = Diag(0.425616, 0.902308, 0.063469, 0.546923, 0.386667, 0.998).
Two numerical procedures - the soft-thresholded SVD [10] and a nuclear norm minimiza- tion [2], were applied to this example.
Both procedures didn’t recover any of these two solutions; the soft-thresholded SVD converged to another solution matrix and the nuclear norm minimization produced the following diagonal matrix solution D3 = Diag(0.4369, 0.7625, 0.0520, 0.5302, 0.1926, 0.9555), with the respective rank r = 4 and smaller nuclear norm than in the above rank 3 solutions and the solution produced by the soft-thresholded SVD procedure.
Note that here both optimal solutions are well posed, and yet these numerical procedures could not recover any one of them.
It is not clear how typical this example, of diﬀerent locally optimal solutions, is.
Recall that generically the nuclear norm minimization problem possesses unique optimal solution.
However it is not clear how well it approximates the ‘true’ minimal rank solution when it is observed with a noise.
5.2 Comparison of LRMA and nuclear norm minimization In ﬁgure 2, ﬁgure 3 and ﬁgure 4, there is a comparison between the performances of LRMA (using least square) and nuclear norm minimization when true matrices are observed with normal noise.
We generate Y ∗, an n1 × n2 matrix of rank r∗, by uniformly generated an n1 × r∗ matrix U , an n2 × r∗ matrix V and an r∗ × r∗ diagonal matrix D and setting Y ∗ = ˜U D ˜V T , where ˜U and ˜V are orthonormalization of U , V respectively.
We uniformly sample Ω, where |Ω| = m.
Observation matrix M is generated by Mij = Y ∗ ij + εij, (i, j) ∈ Ω, where εij ∼ N (0, σ2N−1).
Least square approximation is solved by a soft-threshholded SVD solver with setting no nuclear norm regularization.
The algorithm stops when either relative change in the Frobenius norm between two successive estimates, (cid:107)Y (t+1) − Y t(cid:107)F /(cid:107)Y (t)(cid:107)F , is 17 ij and Y nm less than some tolerance, denoted as tol or the iteration is larger than some number, denoted as it.
Nuclear norm minimization is solved via TFOCS [2].
The solutions of least square approximation and nuclear norm minimization are denoted as Y ls and Y nm respectively.
In the following, we will compare this two methods by looking into the absolute value of ij − Y ∗ Y ls In Figure 2, we compare solutions in the situation that well-posedness condition is sat- isﬁed.
In this experiment, n1 = 40, n2 = 50, r∗ = 10, m = 1000, σ = 5, N = 50 and Ω is sampled until well-posedness condition is satisﬁed.
Convergence parameter tol = 10−20 and it = 50000.
Least square approximation recover the true matrix Y ∗ with less error than the nuclear norm minimization.
ij − Y ∗ ij.
Figure 2: Absolute errors of two methods when well-posedness condition is sat- isﬁed.
The picture on the left is the structure of Ω with Ω denoted by yellow and Ωc denoted by blue.
The picture in the middle is the absolute error between true matrix and the completed ij|.
The picture on the right is the absolute error matrix by least square approximation, |Y ls between true matrix and the completed matrix by nuclear norm minimization, |Y nm ij|.
True matrix Y ∗ ∈ R40×50, rank(Y ∗) = 10, |Ω| = 1000, ∆ij = 0, εij ∼ N (0, 52 50 ) and observation matrix ij + εij, (i, j) ∈ Ω.
For the error, blue denotes small error and yellow denotes large error.
Mij = Y ∗ ij − Y ∗ ij − Y ∗ In Figure 3, we compare solutions in the situation that the necessary condition for well- posedness is violated.
In this experiment, n1 = 70, n2 = 40, r∗ = 11, m = 1300, σ = 5, N = 50 and Ω is sampled until the necessary condition for well-posedness is violated.
Convergence parameter tol = 10−16 and it = 50000.
Least square approximation fails to recover the true matrix at the rows (the row 3, 6, 30, 46, 50) violating the necessary condition for well- posedness.
Nuclear norm minimization can only recover with larger error than least square approximation.
In Figure 4, we compare solutions in the situation that Ω is reducible.
In this experiment, n1 = 40, n2 = 50, r∗ = 10, m = 1000, σ = 5, N = 50 and Ω = {(i, j) ∈ {1··· 20} × {1··· 20} ∪ {21··· 40} × {21··· 50}}.
Convergence parameter tol = 10−20 and it = 50000.
In this situation, both methods can’t recover the true matrix Y ∗ due to unidentiﬁability.
18 Figure 3: Absolute errors of two methods when necessary condition of well- posedness is violated.
The picture on the left is the structure of Ω.
The picture in the middle is the absolute error between true matrix and the completed matrix by least square ap- proximation, |Y ls ij|.
The picture on the right is the absolute error between true matrix and ij|.
True matrix Y ∗ ∈ R70×40, the completed matrix by nuclear norm minimization, |Y nm ij + εij, (i, j) ∈ Ω.
The rank(Y ∗) = 11, |Ω| = 1300, ε ∼ N (0, 52 necessary condition for well-posedness is violated, i.e. the numbers of observations are less than 11, at row 3, 6, 30, 46, 50.
Color scheme is the same as in Figure 2.
50 ) and observation matrix Mij = Y ∗ ij − Y ∗ ij − Y ∗ Figure 4: Absolute errors of two methods when Ω is reducible.
The picture on the left is the structure of Ω.
The picture in the middle is the absolute error between true matrix and the completed matrix by least square approximation, |Y ls ij|.
The picture on the right is the absolute error between true matrix and the completed matrix by nuclear norm minimization, |Y nm ij − Y ∗ 50 ) and observation ij + εij, (i, j) ∈ Ω.
Ω is reducible.
Only two diagonal block matrices M1 ∈ R20×20 matrix Mij = Y ∗ and M2 ∈ R20×30 are observed.
Note that the necessary condition of well-posedness is satisﬁed in this situation.
Color scheme is the same as in ﬁgure 2.
ij|.
True matrix Y ∗ ∈ R40×50, rank(Y ∗) = 10, |Ω| = 1000, εij ∼ N (0, 52 ij − Y ∗ 19 5.3 Numerical veriﬁcation of asymptotic theorems N (r)}200 N (r, Ω(cid:48)) − T (k) N (r, Ω)}200 ij , (i, j) ∈ Ω, where ε(k) Figure 6 is the Q-Q plot of {T (k) An idealized model, when a “true” solution of low rank is observed with a noise, was in- troduced in Section 4.2 (see equation (4.7)) and asymptotically chi-square test statistics for testing the “true” minimal rank were suggested.
Below are discussed numerical experiments validating these asymptotics.
We generate Y ∗, an n1 × n2 matrix of rank r∗, by uniformly generated an n1 × r∗ matrix U , an n2 × r∗ matrix V and an r∗ × r∗ diagonal matrix D and setting Y ∗ = ˜U D ˜V T , where ˜U and ˜V are orthonormalization of U , V respectively.
We uniformly sample Ω, where |Ω| = m.
To verify the asymptotics, observation matrix M is generated repeatedly by ij ∼ N (0, σ2N−1).
In each instance the corresponding ij = Y ∗ ij + ε(k) M (k) statistic T (k) N (r) is computed using equation (4.9).
Least square approximation is solved by a soft-threshholded SVD solver with setting no nuclear norm regularization and the algorithm stops when either relative change in the Frobenius norm between two successive estimates, (cid:107)Y (t+1) − Y t(cid:107)F /(cid:107)Y (t)(cid:107)F , is less than some tolerance, denoted as tol or the iteration is larger than some number, denoted as it.
Figure 5 is the Q-Q plot of {T (k) k=1 against the corresponding chi-square distribution.
In this experiment, n1 = 40, n2 = 50, r∗ = 11, m = 1000, σ = 5, N = 400 and Ω is sampled until well-posedness condition is satisﬁed.
Convergence parameter tol = 10−20 and it = 50000.
From the result, we can see TN (r) follows the central chi-square distribution with degree of freedom df r = m − r(n1 + n2 − r) = 131 as proved in Theorem 4.4. k=1 against the corresponding chi- square distribution.
In this experiment, n1 = 40, n2 = 50, r∗ = 11, m = 996, σ = 5, N = 400, m(cid:48) = |Ω(cid:48)| = 1001 and Ω is sampled until well-posedness condition is satisﬁed (Notes that Ω(cid:48) also satisﬁed well-posedness condition since Ω(cid:48)C ⊂ ΩC).
Convergence parameter tol = 10−20 and it = 50000.
From the result, we can see TN (r, Ω(cid:48))− TN (r, Ω) follows a central Chi-square distribution with degree of freedom df r,Ω(cid:48) − df r,Ω = m(cid:48) − m = 5 as proved in Theorem 4.5. Figure 7 and 8 shows a possible procedure that determines the true rank r∗ by sequential chi-square tests.
In this experiment, n1 = 40, n2 = 50, r∗ = 9, m = 1000, σ = 5, N = 400, and Ω is sampled until well-posedness condition is satisﬁed.
Convergence parameter tol = 10−12 and it = 50000.
Only one observation matrix M is generated in this experiment.
Then the least square approximation are sequentially solved by speciﬁed r from 1 to (cid:100)R(n1, n2, m)(cid:101) and for each TN (r), p-value of the corresponding central chi-square distribution (with degree of freedom df r = m − r(n1 + n2 − r)) can be computed to determine whether r should be accepted.
It shows that all the r < r∗ is rejected and the ﬁrst r accepted is the true r∗, which happens with high probability in the experiments as showed in the supplementary material.
The performed numerical results justify the asymptotic theorems and show that although the least square approximation can not be solved as computationally eﬃciently as the nu- clear norm minimization, current algorithms can still give a satisfactory statistical inference under the assumption of model (4.7).
Numerical experiments, shown in the supplementary material, also validate the noncentral chi-square asymptotics of the considered statistics un- der the assumption of the population drift with the additional terms representing a deviation 20 Figure 5: Q-Q plot of TN (r) against quantiles of chi-square distribution.
Y ∗ ∈ R40×50, rank(Y ∗) = 11, |Ω| = 1000.
Observation matrix M is generated ij , (i, j) ∈ Ω, where 200 times, M (k) ij ∼ N (0, 52 ε(k) N (r) is computed as equation 4.9. By Theorem 4.4, {T (k) N (r)} follows central chi-square with df r = m − r(n1 + n2 − r) = 131.
ij = Y ∗ 400 ).
For each M (k), T (k) ij +ε(k) Figure 6: Q-Q plot of TN (r, Ω(cid:48)) − TN (r, Ω) against quantiles of chi- Y ∗ ∈ R40×50, square distribution.
|Ω| = 996, rank(Y ∗) = 11, where Ω ⊂ Ω(cid:48).
Observation matrix M(cid:48) (cid:48)(k) ij = and M are generated 200 times, M ij, (i, j) ∈ Y ∗ ij + ε(k) Ω, where ε(k) 50 ).
For each time, N (r, Ω(cid:48)) and T (k) T (k) N (r, Ω) are computed.
By N (r, Ω)} fol- N (r, Ω(cid:48)) − T (k) Theorem 4.5, {T (k) lows central chi-square with df r,Ω(cid:48) − df r,Ω = m(cid:48) − m = 5 .
ij , (i, j) ∈ Ω(cid:48) and Mij = M(cid:48) |Ω(cid:48)| = 1001, ij ∼ N (0, 52 of population values from the “true” model.
6 Conclusion In this paper, we have examined the non-convex matrix completion from a geometric view- point and established a suﬃcient and “almost necessary” condition for local uniqueness of solutions.
Our characterization assumes deterministic patterns, and the results are general.
From this study, we found that the minimum rank matrix completion (MRMC) tends to lead to unstable or non-unique solutions and thus the alternative low-rank matrix approxi- mation (LRMA) is a better and more stable approach.
We proposed a new statistical test for rank selection, based on observed entries, which can be useful for many practical matrix completion algorithms.
21 Figure 8: p-value for the chi-square rank test.
The experiment is the same as the one in Figure 7.
The red dash line is p-value = 0.05.
This plot is equivalent to Figure 7.
Figure 7: Sequential test for rank se- Y ∗ ∈ R40×50, rank(Y ∗) = 9, lection.
|Ω| = 1000, εij ∼ N (0, 52 400 ) and observa- ij + εij, (i, j) ∈ Ω.
Least tion matrix Mij = Y ∗ square approximation are solved sequentially by speciﬁed r from 1 to (cid:100)R(n1, n2, m)(cid:101) and corresponding TN (r) is computed sequen- tially.
The grey region is the conﬁdence in- terval with α = 0.05.
The red point indicates the corresponding TN (r) is not in the conﬁ- dence interval while the blue point indicates the corresponding TN (r) is in the conﬁdence interval.
The data is plotted in log scale.
7 Appendix m is an integer.
Proof of Theorem 3.1 It suﬃces to verify (3.1) for the cases when m.
Let V ∈ Rn1×r and W ∈ Rn2×r be matrices of rank r, with respective Consider r := 1 × r row vectors v1, ..., vn1 and w1, ..., wn2.
Consider the following system of equations j = 0, (i, j) ∈ Ω, with unknowns given by elements of matrix Z ∈ Rr×r.
This is a viZw(cid:62) linear homogeneous system with m = r2 equations and r2 unknowns, which can be written as (wj ⊗ vi)vec(Z) = 0, (i, j) ∈ Ω.
Let matrices V and W be chosen in such a way that the row vectors wj ⊗ vi, (i, j) ∈ Ω, are linearly independent.
Then this system of linear equations has 22 only one solution Z = 0.
Consider the linear space of matrices L := {V ZW (cid:62) : Z ∈ Rr×r}.
Since vectors wj ⊗ vi, (i, j) ∈ Ω, are linearly independent we have that dim(L) = r2, and by the above construction L ∩ VΩc = {0}.
On the other hand dim(VΩc) + dim(L) = n1n2 and hence L + VΩc = Rn1×n2.
It follows that for any matrix M ∈ VΩ the linear (aﬃne) spaces M + VΩc and L have a nonempty intersection.
Note that M + VΩc represents the set of all matrices that satisfy the observation constraints.
Also note that for every matrix Y ∈ L its rank(Y ) ≤ r.
It follows that if r2 = m, then it is possible to ﬁnd an n1 × n2 matrix of rank r ≤ √ m with the prescribed elements Mij, (i, j) ∈ Ω.
Proof of Theorem 3.3 We argue by a contradiction.
Suppose that there is a sequence {Yk} ⊂ Mr converging to ¯Y such that PΩ(Yk) = M .
Note that it follows that Yk − ¯Y ∈ VΩc.
By passing to a subsequence if necessary we can assume that (Yk − ¯Y )/tk, where tk := (cid:107)Yk − ¯Y (cid:107), converges to some H ∈ VΩc (of course it is assumed that Yk (cid:54)= ¯Y ).
We have that Yk = ¯Y + tkH + o(tk) and hence H ∈ TMr( ¯Y ).
It remains to note that H (cid:54)= 0 to obtain the required contradiction with condition (3.15).
Proof of Theorem 3.5 Suppose that in row i ∈ {1, ..., n1} there are less than r elements of Ω.
This means that the set σi := {j : (i, j) ∈ Ωc} has cardinality greater than n2 − r.
Let F be a left side complement of ¯Y and G be a right side complement of ¯Y .
Since rows gj of G are of dimension 1× (n2 − r), we have then that vectors gj, j ∈ σi, are linearly dependent, i.e.,(cid:80) j∈σi λjgj = 0 for some λj, not all of them zero.
Then (cid:80) j ⊗ fi) =(cid:0)(cid:80) j∈σi λj(g(cid:62) j∈σi (cid:1)(cid:62) ⊗ fi = 0.
λjgj (7.1) j ⊗ fi, (i, j) ∈ Ωc, to be linearly independent.
This contradicts the condition for vectors g(cid:62) Similar arguments can be applied to the columns of matrix ¯Y .
(cid:3) Proof of Theorem 3.6 Suppose that Ω is reducible.
Then by making permutations of rows and columns if necessary, it can be assumed that M has the block diagonal form as in 1 , M2 = V2W (cid:62) (3.18).
Let ¯Y be a respective minimum rank solution.
That is M1 = V1W (cid:62) (cid:1) being n1 × r and n2 × r matrices of rank r.
(cid:1) and W =(cid:0) W1 2 and 1 are zero matrices.
By changing V1 to αV1 and W1 to α−1W1 for α (cid:54)= 0, we change .
Since ¯Yij (cid:54)= 0, (i, j) ∈ Ωc, it cannot happen that V1W (cid:62) V2W (cid:62) and ¯Y = V W (cid:62) with V =(cid:0) V1 (cid:16) M1 V1W (cid:62) (cid:17) (cid:16) M1 Note that ¯Y = V2W (cid:62) matrix ¯Y to matrix 1 M2 V2 .
This shows solution ¯Y is not locally unique.
αV1W (cid:62) (cid:17) W2 α−1V2W (cid:62) 1 M2 Proof of Theorem 3.7 Suppose that Ω is irreducible.
Consider a rank one solution ¯Y = vw(cid:62) with respective vectors v = (v1, ..., vn1)(cid:62) and w = (w1, ..., wn2)(cid:62).
We can assume that v1 is ﬁxed, say v1 = 1.
Consider an element M1j1, (1, j1) ∈ Ω, in the ﬁrst row of matrix M .
Since M1j1 = v1wj1, it follows that the component wj1 of vector w is uniquely deﬁned.
Next consider element Mi1,j1, (i1, j1) ∈ Ω.
Since Mi1j1 = vi1wj1, it follows that the component vi1 of vector v is uniquely deﬁned.
We proceed now iteratively.
Let ν ⊂ {1, ..., n1} and ω ⊂ {1, ..., n2} be index sets for which the respective components of vectors v and w are already uniquely deﬁned.
Let j (cid:54)∈ ω be such that there is (i, j(cid:48)) ∈ Ω with j(cid:48) ∈ ω and hence wj(cid:48) is already uniquely deﬁned.
Since Mij = viwj and Mij(cid:48) = viwj(cid:48), it follows that wj is 23 uniquely deﬁned and j can be added to the index set ω.
If such column j does not exist, take row i (cid:54)∈ ν such that there is (i(cid:48), j) ∈ Ω with i(cid:48) ∈ ν.
Then vi is uniquely deﬁned and hence i can be added to ν.
Since Ω is irreducible, this process can be continued until all components of vectors v and w are uniquely deﬁned.
Proof of Proposition 4.1 Consider function deﬁned in (4.5).
The diﬀerential of f (Y ) can be written as df (Y ) = tr[(PΩ(Y ) − M )(cid:62)dY ].
Therefore if Y ∈ Mr is an optimal solution of the least squares problem (4.2), then ∇f (Y ) = PΩ(Y ) − M is orthogonal to the tangent space TMr(Y ).
By (3.14) this implies optimality conditions (4.3) .
Proof of Proposition 4.2 Consider function φ deﬁned in (4.4), and the problem of mini- mization of φ(Y, Θ) subject to Y ∈ Mr with Θ viewed as a parameter.
Locally for Y near ¯Y ∈ Mr the manifold Mr can be represented by a system of K = n1n2−dim(Mr) equations gi(Y ) = 0, i = 1, ..., K, for an appropriate smooth mapping g = (g1, ..., gK).
That is, the above optimization problem can be written as min φ(y, θ) subject to gi(y) = 0, i = 1, ..., K, (7.2) where with some abuse of the notation we write this in terms of vectors y = vec(Y ) and θ = vec(Θ).
Note that the mapping g is such that the gradient vectors ∇g1(¯y), ...,∇gK(¯y) are linearly independent.
First order optimality conditions for problem (7.2) are ∇yL(y, λ, θ) = 0, g(y) = 0, (7.3) where L(y, λ, θ) := f (y, θ) + λ(cid:62)g(y) is the corresponding Lagrangian.
For θ = θ0 this system has solution ¯y and the corresponding vector ¯λ = 0 of Lagrange multipliers.
We can view (7.3) as a system of (nonlinear) equations in z = (y, λ) variables.
Jacobian matrix (cid:0) H G (cid:1) of the system (7.3) at (y, λ) = (¯y, ¯λ), where H := ∇yyφ(¯y, θ0) is We would like now to apply the Implicit Function Theorem to this system of equations to conclude that for all θ near θ0 it has unique solution near ¯z = (¯y, ¯λ).
Consider the the Hessian matrix of the objective function and G := ∇g(¯y) = [∇g1(¯y), ...,∇gK(¯y)].
We need to verify that this Jacobian matrix is nonsingular.
This is implied by condition (3.15), which is equivalent to condition (4.6).
Indeed suppose that G(cid:62) 0 (cid:20) H G G(cid:62) 0 (cid:21)(cid:20) v (cid:21) = 0, (7.4) for some vectors v and u of appropriate dimensions.
This means that Hv + Gu = 0 and G(cid:62)v = 0.
It follows that v(cid:62)Hv = 0.
Condition G(cid:62)v = 0 means that v is orthogonal to the tangent space TMr(¯y).
It follows then by condition (4.6) that v = 0.
Then Gu = 0 and hence, since G has full column rank, it follows that u = 0.
Since equations (7.4) have 24 only zero solution, it follows that this Jacobian matrix is nonsingular.
Now by implying the Implicit Function Theorem to the system (7.3) we obtain the required result.
This completes the proof.
Proof of Proposition 4.4 Note that under the speciﬁed assumptions, Mij − Y ∗ ij are of stochastic order Op(N−1/2).
We have by Proposition 4.3 that an optimal solution of problem (4.8) converges in probability to Y ∗.
By the standard theory of least squares (e.g., [16, Lemma 2.2]) we can write the following local approximation near Y ∗ as (4.11).
It follows that the limiting distribution of TN (r) is the same as the limiting distribution of N times the ﬁrst term in the right hand side of (4.11).
Note that N 1/2w1/2 ij Eij converges in distribution to normal with mean σ−1 ij ∆ij and variance one.
It follows that the limiting distribution of N times the ﬁrst term in the right hand side of (4.11), and hence the limiting distribution of TN (r), is noncentral chi-square with degrees of freedom ν = m − dim (PΩ(L)) and the noncentrality parameter δr.
Recall that dimension of the linear space L is equal to the sum of the dimension of its image PΩ (L) plus the dimension of the kernel Ker(PΩ).
It remains to note that condition (3.15) means that Ker(PΩ) = {0} (see Remark 4.1), and hence dim (PΩ(L)) = dim (L) = r(n1 + n2 − r).
(7.5) This completes the proof.
References [1] Morteza Ashraphijuo, Vaneet Aggarwal, and Xiaodong Wang.
On deterministic sam- pling patterns for robust low-rank matrix completion.
IEEE Signal Processing Letter, accepted, 2017.
[2] Stephen R Becker, Emmanuel J Cand`es, and Michael C Grant.
Templates for convex cone problems with applications to sparse signal recovery.
Mathematical programming computation, 3(3):165, 2011.
[3] P.A. Bekker and J.M.F. Ten Berge.
Generic global indentiﬁcation in factor analysis.
Linear Algebra and Its Applications, 264:255–263, 1997.
[4] Emmanuel J Cand`es and Benjamin Recht.
Exact matrix completion via convex opti- mization.
Foundations of Computational Mathematics (FOCS), 9(6):717–772, 2009.
[5] Emmanuel J Cand`es and Terence Tao.
The power of convex relaxation: Near-optimal matrix completion.
IEEE Trans.
Info.
Theory, 56(5):2053–2080, 2010.
[6] M.
Davenport and J.
Romberg.
An overview of low-rank matrix recovery from incom- plete observations.
IEEE Journal of Selected Topics in Signal Processing, 10(4):608–622, 2016.
25 [7] Y.
C.
Eldar, D.
Needell, and Y.
Plan.
Uniqueness conditions for low-rank matrix recov- ery.
Applied and Computational Harmonic Analysis, 33(2):309–314, Sept.
2012.
[8] M.
Fazel.
Matrix rank minimization with applications.
Ph.D. thesis, Stanford Univer- sity, 2002.
[9] Y.-P.
Hsieh, Y.-C.
Kao, R.
Mahabadi, Y.
Alp, and A.
Kyrillidis.
A non-euclidean gradient descent framework for non-convex matrix factorization.
submitted, 2017.
[10] Rahul Mazumder, Trevor Hastie, and Robert Tibshirani.
Spectral regularization algo- rithms for learning large incomplete matrices.
J.
Machine Learning Research, 11:2287– 2322, 2010.
[11] D.A. McManus.
Who invented local power analysis?
Econometric Theory, 7:265–268, 1991.
[12] D.
Pimentel-Alarcon, N.
Boston, and R.
D.
Nowak.
A characterization of deterministic sampling patterns for low-rank matrix completion.
IEEE Journal of Selected Topics in Signal Processing, 10(4):623–636, 2016.
[13] Benjamin Recht, Maryam Fazel, and Pablo A Parrilo.
Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization.
SIAM Review, 52(3):471–501, 2010.
[14] A.
Shapiro.
Rank reducibility of a symmetric matrix and sampling theory of minimum trace factor analysis.
Psychometrika, 47:187–199, 1982.
[15] A.
Shapiro.
Weighted Minimum Trace Factor Analysis.
Psychometrika, 47:243–264, 1982.
[16] A.
Shapiro.
Asymptotic distribution of test statistics in the analysis of moment struc- tures under inequality constraints.
Biometrika, 72:133–144, 1985.
[17] A.
Shapiro.
Extremal Problems on the Set of Nonnegative Deﬁnite Matrices.
Linear Algebra and Its Applications, 67:7–18, 1985.
[18] A.
Shapiro.
Statistical inference of semideﬁnite programming.
Technical report, Georgia Institute of Technology, 2017.
Preprint posted on Optimization Online, http://www.optimization-online.org/DB HTML/2017/01/5842.html.
[19] J.H. Steiger, A.
Shapiro, and M.W. Browne.
On the multivariate asymptotic distribution of sequential chi-square statistics.
Psychometrika, 50:253–254, 1985.
[20] J.
A.
Tropp, A.
Yurtsever, M.
Udell, and V.
Cevher.
Practical sketching algorithms for low-rank matrix approximation.
SIAM J.
Matrix Anal.
Appl., 38(4):1454–1485, Dec.
2017.
26 [21] E.B. Wilson and J.
Worcester.
The resolution of six tests into three general factors.
Proc.
Nat.
Acad.
Sci.
U.S.A., 25:73–77, 1939.
27
Spatio-temporal datasets are often very large and difﬁcult to analyse [1].
Traditional centralised data management and mining techniques are not adequate, as they do not consider all the issues of data-driven applications such as scalability in both response time and accuracy of solutions, distribution and heterogeneity [2].
In addition, transferring a huge amount of data over the network is not an efﬁcient strategy and may not be possible for security and protection reasons.
Distributed data mining (DDM) techniques constitute a better alternative as they are scalable and can deal efﬁciently with data heterogeneity.
Many DDM methods such as dis- tributed association rules and distributed classiﬁcation have been proposed in the last decade and most of them are based on performing partial analysis on local data at individual sites followed by the generation of global models by aggregating those local results [3], [4], [5], [6].
However, only a few of them concern distributed clustering.
Among these many parallel clustering algorithms have been proposed [7], [8], [9].
They are classiﬁed into two categories.
The ﬁrst consists of methods requiring multiple rounds of message passing and signiﬁcant amount of synchronisations.
The second category consists of methods that build local clustering models and then aggregate them to build global models [10].
Most of the parallel approaches need either multiple synchronisation constraints between processes or a global view of the dataset, or both [11], [12].
For distributed approaches, the aggregation phase is very costly, and therefore, it needs to be optimised.
In this paper, we present an approach that reduces the complexity of the aggregation phase.
It reduces signiﬁcantly the amount of information exchanged during the aggregation phase, and generates automatically the correct number of clusters.
In a case study, it was shown that the data exchanged is reduced by more than 98% of the original datasets.
II.
DYNAMIC DISTRIBUTED CLUSTERING The DDC approach includes two main steps.
In the ﬁrst step, as usual, we cluster the datasets located on each node and select good local representatives for each local cluster.
This phase is executed in parallel without communications between the nodes.
In this phase we can reach a super speed- up.
The next phase consists of aggregating the local clusters.
This operation is executed by certain nodes of the system, called leaders.
The leaders are elected according to some nodes’ characteristics such as their capacity, processing power, connectivity, etc.
The leaders are responsible for merging and regenerating the data objects based on the local cluster representatives.
However, the nodes need to communicate in order to send their local clusters to the leaders.
This process is shown in Figure 1.
Figure 1: An overview of the DDC Approach.
A.
Local Models Each node can use any technique of clustering on its local dataset.
This is one of the key features of this approach to deal with data heterogeneity.
Each node choses a technique that is more suitable for its data.
This approach relaxes the data pre-processing.
For instance we may not need to deal with data consistency between the nodes’ datasets.
All needed is the consistency of the local cluster’s representation.
Once extracted the local clusters will be used as inputs for the next phase to generate global clusters.
However exchanging the local clusters through the network will create signiﬁcant overheads and slow- down hugely the process.
This is one of the major problems of the majority of distributed clustering techniques.
To improve the performance we propose to exchange a minimum number of points/objects.
Instead of sending all the clusters datapoints, we only exchange their representative points, which constitute 1% to 2% of the total size of the dataset.
The best way to represent a spatial cluster is by its shape and density.
The shape of a cluster is represented by its boundary points (called contour) [11], [13], [14] (see Figure 4).
Many algorithms for extracting the boundaries from a cluster can be found in the literature [15], [16].
Recently, we developed a new algorithm for detecting and extracting the boundary of a cluster [10].
The main concepts of this technique are given below.
Neighbourhood: Given a cluster C ⊆ (cid:60)n ≡ {p1, p2, ..., pn} The neighbourhood N C(p) of a point p in the cluster C is deﬁned as the set of points pi ∈ C so that the distance between pi and p is less than or equal to ε: N C(p) = {pi ∈ C | dist(p, pi) ≤ ε} (1) In order to determine the boundary points of a cluster, we need to introduce the following concepts: Displacement vector: A displacement vector from pi ∈ N C(p) to the point p is deﬁned as: −→ V = (cid:88) pi∈N C (p) (p − pi) This vector points towards the area of the lowest density of the neighbourhood of p.
Balance vector: The balance vector relative to the point p is deﬁned as follows: −→ b p = −→ V p if (cid:107)−→ V p(cid:107) > 0 Otherwise (3) (cid:107)−→ V p(cid:107) −→ (cid:40) 1 Note that the balance vector of p points to the least dense area of the neighbourhood of p, the length of the vector does not hold any relevant information.
Figure 2 shows an example of a balance vector.
The neighbourhoods of p are inside the circle, the balance vector is represented by the blue colour.
Boundary points: If a point is a boundary point, there should be no points towards the direction of the balance vector.
This property allows us to separate boundary points and internal points.
For each point p, it checks for an empty area whose shape is the intersection of an hyper-cone of inﬁnite height, vertex, axis and aperture ρ, where ρ is a given angle.
As shown in 3, the area checked is highlighted in green.
Formally, a boundary point is described as a Boolean predicate.
Figure 2: Example of a balance vector.
Figure 3: Boundary point check (cid:40) Boundary(p) = if (∀q ∈ N C ε (p), (q − p) true f alse Otherwise −→ b b < cos(v) (4) We can deﬁne the boundary BC of a cluster C as the set of all boundary points in C: (2) BC = {p ∈ C : Boundary(p) is true} (5) The algorithm for selecting the boundary points is de- scribed in Algorithm 1, and its complexity is O(n log n).
Algorithm 1: Boundary Detection Algorithm −→ b pi, input : Cluster C, set of balance vectors parameter ν.
output: Boundary points BC of cluster C 1 BC ← C; 2 for every point p ∈ BC do for every point q ∈ N C(p) do −→ b ≥ cos(v) then if (q − p) Discard p from BC; Break; 7 return BC; Let Ci be a set of clusters of node i and Bcj be the boundary of a cluster cj ∈ Ci. The local model Li is deﬁned by: n(cid:91) Li = Bcj ∪ Pi (6) j=1 Where Pi is the set of internal representatives.
B.
DDC with DBSCAN B.
Global Models The global clusters are generated during the second phase.
This phase consists of two main steps: 1) each leader collects the local clusters of its neighbours, 2) the leaders merge the local clusters using the overlay technique.
The process of merging clusters will continue until we reach the root node.
The root node will contain the global clusters (see Figure 1).
During this phase we only exchange the boundaries of the clusters.
The merging process consists of two steps: boundary merging and regeneration.
The merging is performed by a boundary-based method.
Let Li be a local model received from the site i and Bi be the set of all boundaries in Li. The global model G is deﬁned by: G = Γ(∪Bi), Bi ∈ Li (7) where Γ is a merging function.
III.
DDC EVALUATION AND VALIDATION DDC is evaluated using different clustering algorithms.
In this paper we use a centroid-based algorithm (K-Means) and a density-based Algorithm (DBSCAN).
A.
DDC-K-Means The DDC with K-Means (DDC-K-Means) is characterised by the fact that in the ﬁrst phase, called the parallel phase, each node Ni of the system executes the K-Means algorithm on its local dataset to produce Li local clusters and calculate their contours.
The rest of the process is the same as described above.
It was shown in [17] that DDC-K-Means dynamically determines the number of the clusters without a priori knowl- edge about the data or an estimation process of the number of the clusters.
DDC-K-Means was compared to two well- known clustering algorithms: BIRCH [18] and CURE [19].
The results showed that it generates much better clusters.
Also, as expected, this approach runs much faster than the two other algorithms; BIRCH and CURE.
DDC-K-Means does not need the number of global clusters in advance.
It is calculated dynamically.
Moreover, each local clustering Li needs Ki. Let ˜Ki be the exact number of local clusters in the node Ni, all it is required is to set Ki such that Ki > ˜Ki. This is much simpler than giving Ki, especially when there is not enough knowledge about the local dataset characteristics.
Nevertheless, it is indeed better to set Ki as close as possible to ˜Ki in order to reduce the processing time in calculating the contours and also merging procedure.
However, DDC-K-Means fails to ﬁnd good clusters for datasets with Non-Covex shapes and also for datasets with noises, this is due to the fact that the K-Means algorithm tends to work with convex shape only, because it is based on the centroid principle to generate clusters.
Moreover, we can also notice that the results of DDC-K-Means are even worse with dataset which contains a big amount of noises (T3, and T4).
In fact it returns the whole dataset with the noise as one ﬁnal cluster for each dataset (see Figure 5).
This is because K-Means does not deal with noise.
DBSCAN (Density-Based spatial Clustering of Applica- tions with Noise) is a well-known density based clustering algorithm capable of discovering clusters with arbitrary shapes and eliminating noisy data [20].
1) DBSCAN Complexity: DBSCAN visits each point of the dataset, possibly multiple times, depending on the size of the neighbourhood.
If it performs a neighbourhood operations in O(log n), an overall average complexity of O(n log n) is obtained if the parameter Eps is chosen in a meaningful way.
The worst case execution time complexity remains O(n2).
The distance matrix of size O((n2 − n/2)) can be materialised to avoid distance re-computations, but this needs O(n2) of mem- ory, whereas a non-matrix based implementation of DBSCAN only needs O(n) of memory space.
2) DDC-DBSCAN Algorithm: The approach remains the same; instead of using K-Means for processing local clusters, we use DBSCAN.
Each node (ni) executes DBSCAN on its local dataset to produce Ki local clusters.
Once all the local clusters are determined, we calculate their contours.
The second phase is the same and the pseudo code of the algorithm is given in Algorithm 2.
Algorithm 2: DDC with DBSCAN.
input : Xi: Dataset Fragment, Epsi: Distance Epsi for N odei, M inP tsi: minimum points contain clusters generated by N odei, D: tree degree, Li: Local clusters generated by N odei output: Kg: Global Clusters (global results) 1 level = treeheight; 1) 2) 4) 5) DBSCAN(Xi. Epsi, M inP tsi); // N odei executes DBSCAN locally.
Contour(Li); // N odei executes the Contour algorithm to generate the boundary of each local cluster.
3) N odei joins a group G of D elements; // N odei joins its neighbourhood Compare cluster of N odei to other node’s clusters in the same group; // look for overlapping between clusters.
j = ElectLeaderNode(); // Elect a node which will merge the overlapping clusters.
if i <> j then Send (contour i, j); else if level > 0 then level - - ; Repeat 3, 4, and 5 until level=0; else return (Kg: N odei’ clusters); Figure 4 illustrates an example of DDC-DBSCAN.
Assume that the distributed computing platform contains ﬁve Nodes (N = 5).
Each Node executes DBSCAN algorithm with its local parameters (Epsi, M inP tsi) on its local dataset.
As it can be seen in Figure 4 the new approach returned exactly the right number of clusters and their shapes.
The approach is insensitive to the way the original data was distributed among the nodes.
It is also insensitive to noise and outliers.
As we can see, although each node executed DBSCAN locally with different parameters.
The global ﬁnal clusters were correct even on the noisy dataset (T3) (See Figure 4).
Figure 4: Example of DDC-DBSCAN execution.
IV.
EXPERIMENTAL RESULTS In this section, we study the performance of the DDC- DBSCAN approach and demonstrate its effectiveness com- pared to BIRCH, CURE and DDC-K-Means.
We choose these algorithms because either they are in the same category such as BIRCH which belongs to hierarchical clustering category, or have an efﬁcient optimisation approach, such as CURE.
BIRCH: We used the BIRCH implementation provided in [18].
It performs a pre-clustering and then uses a centroid- based hierarchical clustering algorithm.
Note that the time and space complexity of this approach is quadratic to the number of points after pre-clustering.
We set its parameters to the default values as suggested in [18].
CURE: We used the implementation of CURE provided in [19].
The algorithm uses representative points with shrinking towards the mean.
As described in [19], when two clusters are merged in each step of the algorithm, representative points for the new merged cluster are selected from the ones of the two original clusters rather than all the points in the merged clusters.
A.
Experiments We run experiments with different datasets.
We used four types of datasets (T1, T2, T3 and T4) with different shapes and sizes.
These datasets are very well-known benchmarks to evaluate density-based clustering algorithms.
All the four datasets are summarised in Table I.
The number of points and clusters in each dataset is also given.
These four datasets contain a set of shapes or patterns which are not easy to extract with traditional techniques.
Table I: The datasets used to test the algorithms.
Type Without Noise With Noise Dataset T1 T2 T3 T4 Description Round and Oval shape Different shapes including noise Different shapes with Noises Different shapes, with some clusters surrounded by others #Points #Clusters 700 321 10,000 8,000 B.
Quality of Clustering We run the four algorithms on the Four datasets in order to evaluate the quality of their ﬁnal clusters.
Figure 5 shows the clusters returned by each of the four algorithms for the datasets without noise (T1) and datasets with noise (T2, T3, and T4).
We use different colours to show the clusters returned by each algorithm.
As expected, BIRCH could not ﬁnd correct clusters; it tends to work slightly better with datasets without noise (T1), as BIRCH does not deal with noise.
The results of CURE are worse and it is not able to extract clusters with non-convex shapes.
We can also see that CURE does not deal with noise.
DDC-K-Means fails to ﬁnd the correct ﬁnal results.
In fact it returns the whole original dataset as one ﬁnal cluster for each dataset (T3 and T4) (Which contain signiﬁcant amount of noise).
This conﬁrms that the DDC technique is sensitive to the type of the algorithm chosen for the ﬁrst phase.
Because the second phase deals only with the merging of the local clusters whether they are correct or not.
This issue is corrected by the DDC-DBSCAN, as it is well suited for spatial datasets with or without noise.
For datasets with noise it eliminates the noise and outliers.
In fact, it generates good ﬁnal clusters in datasets that have signiﬁcant amount of noise.
As a ﬁnal observation, these results prove that the DDC framework is very efﬁcient with regard to the accuracy of its results.
The only issue is to choose a good clustering algorithm for the ﬁrst phase.
This can be done by exploring the initial datasets along with the question to be answered and choose an appropriate clustering algorithm accordingly.
Moreover, as for the DDC-K-Means, DDC-DBSCAN is dynamic (the correct number of clusters is returned automati- cally) and efﬁcient (the approach is distributed and minimises the communications).
C.
Speed-up The goal here is to study the execution time of the four algorithms and demonstrate the impact of using a parallel and distributed architecture to deal with the limited capacity of a centralised system.
As mentioned in Section III-B1, the execution time for the DDC-DBSCAN algorithm can be generated in two cases.
The ﬁrst case is to include the time required to generate the distance matrix calculation.
The second case is to suppose that the distance matrix has already been generated.
The reason for this is that the distance matrix is calculated only once.
Table II illustrates the execution times of the four tech- niques on different datasets.
Note that the execution times do BIRCH CURE DDC-K-Means DDC-DBSCAN T1 T2 T3 T4 Figure 5: Comparing the clusters generated across different datasets Table II: The execution times (ms) of BIRCH, CURE, DDC- K-Means and DDC-DBSCAN with (w) and without (w/o) distance matrix computation.
SIZE 700 321 10000 8000 BIRCH 150 72 250 218 T1 T2 T3 T4 Execution Time (ms) DDC-K-Means 120 143 270 257 CURE 70045 47045 141864 92440 DDC-DBSCAN W/O 302 500 312 504 836 470 304 608 not include the time for post-processing since these are the same for the four algorithms.
As mentioned in Section III-B1, Table II conﬁrmed the fact that the distance matrix calculation in DBSCAN is very sig- niﬁcant.
Moreover, DDC-DBSCAN’s execution time is much lower than CUREs execution times across the four datasets.
Table II shows also that the DDC-K-Means is very quick which is in line with its polynomial computational complexity.
BIRCH is also very fast, however, the quality of its results are not good, it failed in ﬁnding the correct clusters across all the four datasets.
The DDC-DBSCAN is a bit slower that DDC-K-Means, but it returns high quality results for all the tested benchmarks, much better that DDC-K-Means, which has reasonably good results for convex cluster shapes and very bad results for non-convex cluster shapes.
The overall results conﬁrm that the DDC-DBSCAN clustering techniques compares favourably to all the tested algorithms for the combined performance measures (quality of the results and response time).
D.
Computation Complexity Let n be the number of data objects in the dataset.
The complexity of our approach is the sum of the complexities of its three components: local mining, local reduction, and global aggregation.
a) Phase1: Local clustering: Assume that the local clustering algorithm is DBSCAN for all the nodes.
The cost of this phase is given by: Max i=1 (DBSCANi) + TP hase1 = Max i=1 (Reductioni) [5] N.-A.
Le-Khac, L.
Aouad, and M.-T.
Kechadi, “Performance study of distributed apriori-like frequent itemsets mining,” Knowledge and Information Systems, vol.
23, no.
1, pp.
55–72, 2010.
[6] N.
Le-Khac, L.Aouad., and M.-T.
Kechadi, Emergent Web Intelligence: Advanced Semantic Technologies.
Springer London, 2010, ch.
Toward Distributed Knowledge Discovery onGrid Systems, pp.
213–243.
[7] L.
Aouad, N.-A.
Le-Khac, and M.-T.
Kechadi., “Image analysis plat- in the meteorological domain,” in 7th form for data management Industrial Conference, ICDM 2007, Leipzig, Germany, July 14-18, 2007.
Proceedings, vol.
4597.
Springer Berlin Heidelberg, 2007, pp.
120–134.
I.
Dhillon and D.
Modha, “A data-clustering algorithm on distributed memory multiprocessor,” in large-Scale Parallel Data Mining, Work- shop on Large-Scale Parallel KDD Systems, SIGKDD.
Springer-Verlag London, UK, 1999, pp.
245–260.
[8] [9] M.
Ester, H.-P.
Kriegel, J.
Sander, and X.
Xu, “A density-based algorithm for discovering clusters in large spatial databases with noise.” in Kdd, vol.
96, no.
34, 1996, pp.
226–231.
J.-F.
Laloux, N.-A.
Le-Khac, and M.-T.
Kechadi, “Efﬁcient distributed approach for density-based clustering,” Enabling Technologies: Infras- tructure for Collaborative Enterprises (WETICE), 20th IEEE Interna- tional Workshops, pp.
145–150, 2011.
[10] [11] N.
Le-Khac, L.Aouad., and M.-T.
Kechadi, Data Management.
Data, Data Everywhere: 24th British National Conference on Databases.
Springer Berlin Heidelberg, 2007, ch.
A New Approach for Distributed Density Based Clustering on Grid Platform, pp.
247–258.
[12] L.
Aouad, N.-A.
L.
Khac, and M.-T.
Kechadi, Advances in Data Mining.
Theoretical Aspects and Applications: 7th Industrial Conference (ICDM 2007), Leipzig, Germany, July 14-18, 2007.
Proceedings.
Springer Berlin Heidelberg, 2007, ch.
Lightweight Clustering Technique for Distributed Data Mining Applications, pp.
120–134.
[13] L.Aouad, N.-A.
Le-Khac, and M-T.Kechadi, “Variance-based clustering technique for distributed data mining applications.” in DMIN, 2007, pp.
111–117.
[14] N.-A.
Le-Khac, M.
Bue, M.
Whelan, and M-T.Kechadi, “A knowl- edge based data reduction for very large spatio-temporal datasets,” International Conference on Advanced Data Mining and Applications, (ADMA2010), 2010.
[15] A.
Chaudhuri, B.
Chaudhuri, and S.
Parui, “A novel approach to computation of the shape of a dot pattern and extraction of its perceptual border,” Computer vision and Image Understranding, vol.
68, pp.
257– 275, 1997.
[16] M.
Melkemi and M.
Djebali, “Computing the shape of a planar points set,” Elsevier Science, vol.
33, p.
14231436, 2000.
[17] M.
Bendechache and M.-T.
Kechadi, “Distributed clustering algorithm for spatial data mining,” in Spatial Data Mining and Geographical Knowledge Services (ICSDM), 2015 2nd IEEE International Conference on, 2015, pp.
60–65.
[18] T.
Zhang, R.
Ramakrishnan, and M.
Livny, “Birch: An efﬁcient data clustering method for very large databases,” in SIGMOD-96 Proceed- ings of the 1996 ACM SIGMOD international conference on Manage- ment of data, vol.
25.
ACM New York, USA, 1996, pp.
103–114.
[19] S.
Guha, R.
Rastogi, and K.
Shim, “Cure: An efﬁcient clustering algorithm for large databases,” in Information Systems, vol.
26.
Elsevier Science Ltd.
Oxford, UK, 2001, pp.
35–58.
[20] M.Ester, H.
Kriegel, J.
Sander, and X.
Xu, “A density-based algorithm for discovering clusters in large spatial databases with noise,” In 2nd Int.
Conf., Knowledge Discovery and Data Mining (KDD 96), 1996.
Where N is the number of nodes in the system.
The complexity of DBSCAN in the best case scenario is O(n log n) if the parameter Eps is chosen in a meaningful way and also if we exclude the distance matrix computation.
Finally, the complexity of the local reduction algorithm is O(n log n).
b) Phase2: Aggregation: Our global aggregation de- pends on the hierarchical combination of contours of local clusters.
As the combination is based on the intersection of edges from the contours, the complexity of this phase is O(v log v + p).
Where v is the number of vertices and p is the number of intersections between edges of different contours (polygons).
c) Total complexity: The total complexity of our ap- proach is TT otal = O(n log n) + O(n log n) + O(v log v + p), which is: TT otal (cid:39) O(n log n) V.
CONCLUSION In this paper, we proposed an efﬁcient and ﬂexible dis- tributed clustering framework that can work with existing data mining algorithms.
The framework has been tested on spatial datasets using the K-Means and DBSCAN algorithms.
The proposed approach is dynamic, which solves one of the major shortcomings of K-Means or DBSCAN.
We proposed an efﬁcient aggregation phase, which reduces considerably the data exchange between the leaders and the system nodes.
The size of the data exchange is reduced by about 98%.
The DDC approach was tested using various benchmarks.
The benchmarks were chosen in such a way to reﬂect all the difﬁculties of clusters extraction.
These difﬁculties include the shapes of the clusters (convex and non-convex), the data vol- ume, and the computational complexity.
Experimental results showed that the approach is very efﬁcient and can deal with various situations (various shapes, densities, size, etc.).
As future work, we will try to extend the framework to non-spatial datasets.
We will also look at the problem of the data and communications reduction during phase two.
ACKNOWLEDGMENT The research work is conducted in the Insight Centre for Data Analytics, which is supported by Science Foundation Ireland under Grant Number SFI/12/RC/2289.
REFERENCES [1] M.
H.
Dunham and D.
Ming, “Introductory and advanced topics,” 2003.
[2] M.
Bertolotto, S.
Di Martino, F.
Ferrucci, and M.-T.
Kechadi, “Towards a framework for mining and analysing spatio-temporal datasets,” Inter- national Journal of Geographical Information Science, vol.
21, no.
8, pp.
895–906, 2007.
[3] L.
Aouad, N.-A.
Le-Khac, and M.-T.
Kechadi, “weight clustering technique for distributed data mining applications,” LNCS on advances in data mining – theoretical aspects and applications, vol.
4597, pp.
120–134, 2007.
[4] L.Aouad, N.-A.
Le-Khac, and M.-T.
Kechadi, “Grid-based approaches for distributed data mining applications,” Journal of Algorithms & Computational Technology, vol.
3, no.
4, pp.
517–534, 2009.

Artiﬁcial Neural Network (ANN), inspired by biological neural networks, is based on a collection of connected units or nodes called artiﬁcial neurons.
These systems are used as a learning algorithm which tries to mimic how the brain works.
ANNs are consider as universal function approximators, that is, it can approximate the function for the data sent through it.
It is based on the mul- tilayer perceptron [3] model which is a class of feedforward artiﬁcial neural networks, consisting of at least three layers of models.
Learning occurs in the perceptron by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result.
This is an example of supervised learning, and is carried out through back- propagation, a generalization of the least mean squares algorithm in the linear perceptron.
The multilayer perceptron model coupled with the backpropagation algorithm gave rise to the Artiﬁcial Neural Network, which can be eﬀectively and eﬃciently used as a learning algorithm.
Backpropagation [1] is a method used in artiﬁcial neural networks to calcu- late the error contribution of each neuron after a batch of data is processed.
It is commonly used by the gradient descent optimization algorithm to adjust the weight of neurons by calculating the gradient of the loss function.
This tech- nique is also sometimes called backward propagation of errors, because the error is calculated at the output and distributed back through the network layers.
It also requires a known, desired output for each input value — it is therefore considered to be a supervised learning method.
Gradient Descent [4] is an iterative approach that takes small steps to reach to the local minima of the function.
This is used to update the weights and biases of each neuron in a neural network.
Gradient descent is based on the observation that if the multivariable function F (x) is deﬁned and diﬀerentiable in a neighborhood of a point a, then F (x) decreases fastest if one goes from a in the direction of the negative gradient of F at a, −∆F (a).
It follows that, if an+1 = an – γ∆F (an) for γ small enough, then F (an) >= F (an+1).
In other words, the term γ∆F (a) is subtracted from a because we want to move against the gradient, toward the minimum.
With this observation in mind, one starts with a guess x0 for a local minimum of F , and considers the sequence x0, x1, x2, ...
such that xn+1 = xn – γn∆F (xn), for n >= 0.
We have F (x0) >= F (x1) >= F (x2) >= ..., so hopefully the sequence xn converges to the desired local minimum.
Even though this method works well in general, it has a few limitations.
Firstly, due to the iterative nature of the algorithm, it takes a lot of time to converge to the local minima of the function.
Secondly, gradient descent is relatively slow close to the minimum: technically, its asymptotic rate of convergence is inferior to many other methods.
Thirdly, the gradient methods are ill-deﬁned for non-diﬀerentiable functions.
During the paper we will be referring to the Moore-Penrose Pseudo Inverse [8].
In mathematics, and in particular linear algebra, a pseudoinverse A+ of a matrix A is a generalization of the inverse matrix.
The most widely known type of matrix pseudoinverse is the Moore–Penrose inverse, which was independently described by E.
H.
Moore in 1920, Arne Bjerhammar in 1951, and Roger Pen- rose in 1955.
A common use of the pseudoinverse is to compute a ‘best ﬁt’ (least squares) solution to a system of linear equations that lacks a unique solution.
Another use is to ﬁnd the minimum (Euclidean) norm solution to a system of linear equations with multiple solutions.
The pseudoinverse facilitates the state- ment and proof of results in linear algebra.
The pseudoinverse is deﬁned and unique for all matrices whose entries are real or complex numbers.
It can be computed using the singular value decomposition.
In this paper, we formulate another method of ﬁnding the errors in weights and biases of the neurons in a neural network.
But ﬁrst, we would like to present a few assumptions made in the model of the neural network, to make our method feasible.
2 Modiﬁcations to the neuron structure We have made one change to the structure of an artiﬁcial neuron.
We assume that there is a weight and bias associated for each input, that is, each element in the input vector is multiplied by a weight and a bias is added to it.
This is a slight alteration from the traditional artiﬁcial neuron where there is a common bias applied to the overall output of the neural network.
This change will not alter the goal or the end result of a neural network.
The proof for this statement is shown below: For input vector of size ‘n’: c1w1 + b1 + c2w2 + b2 + c3w3 + b3...cnwn + bn = c1w1 + c2w2 + c3w3...cnwn + b Where : b = b1 + b2 + b3..bn (1) (2) (3) Therefore, having a separate bias for each input element will make no diﬀerence to the end result.
Figure 1: Neuron 3 The New Backpropagation Algorithm 3.1 Calculating new weights and biases for a neuron Taking one neuron at a time, there is one input entering into the neuron, which is multiplied by some weight and a bias is added to this product.
This value is then sent through an activation function, and the output from activation func- tion is taken as the output of the neuron.
Let C be the input into the neuron, the original weight applied to that input is w and the original bias applied to that input is b.
Let x be the output given initially when the input C passes through the neuron.
Let xn be the output that we require.
Based on the required output, we will require a diﬀerent weight and bias value, say wn and bn respectively.
The original output is calculated as, But, we required xn as the output.
Therefore, Cw + b = x Let, Cwn + bn = xn wn = w − ∆w bn = b − ∆b Where, ∆w is the error in the weight and, ∆b is the error in the bias.
Cwn + bn = xn C(w − ∆w) + (b − ∆b) = xn C(w − ∆w) + (b − ∆b) = xn (Cw + b) − (C∆w + ∆b) = xn x − xn = (C∆w + ∆b) Therefore, C∆w + ∆b = (x − xn) (4) (5) (6) (7) (8) (9) (10) (11) (12) (13) Now, [ C 1 ] × [ ∆w ∆b ] = [ (x − xn) ] ∆w ∆b ] = [ C 1 ]−1 × [ (x − xn) ] (14) (15) But, [ C 1 ] is not a square matrix.
Therefore, We will have to ﬁnd the Moore-Penrose Pseudo-Inverse of the matrix [ C 1 ].
∆w ∆b ] = [ C 1 ]+ × [ (x − xn) ] (16) After obtaining ∆w and ∆b, change the original weight and bias to the new weight and bias in accordance to, wn = w − (∆w ∗ α) bn = b − (∆b ∗ α) (17) (18) where α is the learning rate.
3.2 Tackling multiple inputs The above mentioned method of changing weights and biases of the neuron can be extended for a vector input of length n.
Let the input vector C belong to the nth dimension.
In this case, each element of the input vector will be multiplied by its respective weight in the neuron, and a bias will be added to each of the products.
There- fore, there will be n input elements, n corresponding weights and biases, and n outputs from each weight-bias block.
These outputs are added up to give one single output and passed on the activation function.
During the backpropagation stage, the desired output is distributed amongst all the weight-bias pairs, such that, for a block of weight and bias i (wi, bi), the required output for that block will be 1/n of the required output.
That is, For all weight-bias blocks (wi, bi) xni = xn/n (19) The weights and biases are initialized to random values in the beginning, that is, absolute weightage given to each element in the input vector is randomized.
Figure 2: Neuron for vector length of ‘n’ The relative weightage given to each element in the input vector should be the same.
Each weight-bias block will give the same output, so that the cumulative output will give us the required answer.
Therefore, this method of dividing the weights will work.
3.3 Activation Function for non-linearity To achieve non-linearity, the general approach taken is to pass the summation of the output from all weight-bias pairs through a non-linear activation func- tion [6].
During the backpropagation phase, to correct the weights and biases values of the neuron, we cannot simply pass the actual output vector required.
If we do so, it will change the weights and biases as though there is no acti- vation function, and when the forward propagation of the same vector occurs, the neuron outputs will go through the activation function, and give a wrong result.
Therefore, we must pass the output vector required through the inverse of the activation function.
We need to make sure that we will have to choose an activation function such that its domain and range are the same, so as to avoid math errors and to avoid loss of data.
The new vector after applying the inverse activation function is the actual vector sent to the layers of the network during the backpropagation phase.
3.4 Network Architecture Figure 3 shows the representation of a neural network.
Each neuron outputs one value.
The output of every neuron in one layer is sent as the input to every Figure 3: Neural Network Representation neuron in the next layer.
Therefore, each layer can be associated with a buﬀer list, so that the output from each neuron in that layer can be stored and passed on to the next layer as input.
This would help in the implementation of a neural network by simplifying the forward propagation task.
Figure 4: Neural Network Implementation The input forward propagates through the network and at the last (output) layer it gives out an output vector.
Now, for this last layer, the required output is known.
Therefore the weights and biases of the neurons of the last layer can be easily changed.
We do not know the required output vectors for the previous layers.
We can only make a calculated guess.
Using a simple intuition by asking ourselves the question, ”What should be the input (which is the output vector of the previous layer) to the last layer such that the output would be correct?”, we can arrive at a conclusion that the input, which would be the correct required output for the previous layer, is a vector which should have given no error in the output of the last layer.
This can be illustrated by the following equations.
If C ∗ w + b = x Then what Cn vector will satisfy the equation Cn ∗ w + b = x Cn = (xn − b)/w (20) (21) This approach can be extended to all the previous layers.
Another issue arises that many neurons will give their own ‘required’ input, so that their outputs will be correct.
This could happen in a multiclass classi- ﬁcation problem, wherein the output vector required is one-hot encoded vector (where the element of the vector at the position of the required class is 1, and the other elements in the vector are 0).
Therefore, we take the average of all vectors.
This will give an equal weightage of all the feedbacks from each neuron.
Pass this averaged required input vector to the previous layers as the required output from that layer.
This concludes the complete working of the neural network with our devised backpropagation algorithm.
4 Diﬀerences with Extreme Learning Machines Extreme learning machines [7] are feedforward neural network for classiﬁcation, regression, clustering, sparse approximation, compression and feature learning with a single layer or multilayers of hidden nodes, where the parameters of hid- den nodes (not just the weights connecting inputs to hidden nodes) need not be tuned.
These hidden nodes can be randomly assigned and never updated (i.e. they are random projection but with nonlinear transforms), or can be inherited from their ancestors without being changed.
In most cases, the output weights of hidden nodes are usually learned in a single step, which essentially amounts to learning a linear model.
Even though both the method use the Moore-Penrose Pseudo Inverse, there are a few signiﬁcant diﬀerences between the ELM and the proposed backpropagtion method explained in this paper.
The ELM is a feedforward network which is aims at replacing the traditional artiﬁcial neural network, whereas this paper provides an alternative for the backpropagation algorithm used in traditional artiﬁcial neural networks.
The ELM algorithm provides only a forward prop- agation technique to change the weights and bias of the neurons in the last hidden layer, whereas we have provided a method of backpropagation to change the weights and biases of all neurons in every layer.
5 Results 5.1 Telling-Two-Spirals-Apart Problem Alexis P.
Wieland proposed a useful benchmark task for neural networks: distin- guishing between two intertwined spirals.
Although this task is easy to visualize, it is hard for a network to learn due to its extreme non-linearity.
In this report we exhibit a network architecture that facilitates the learning of the spiral task, and then compare the learning speed of several variants of the backpropagation algorithm.
In our experiment, we are using the spiral dataset which contains 193 data points of each class.
We have decided to model the network with a 16-32-64-32- 2 conﬁguration, with ‘Softplus’ activation function on all neurons of the network.
We trained the model for 1000 epochs, with a learning rate of 0.0002.
Figure 5: Training data points for the Two-Spirals problem From the above 2 ﬁgures, we can see that although it doesn’t distinguish between the two spirals very well, we are able to get an accuracy of about 63%.
This is due to the fact that the Softplus activation function is not the rec- Figure 6: Testing data points for the Two-Spirals problem ommended activation function for this particular problem.
The recommended activation function is ‘Tanh’ but, due to the fact that the domain of inverse of the Tanh function lies between (−1, 1) and not between (−∞, +∞), it cannot be used in our backpropagation method without causing some loss of data.
Looking at ﬁgure 6, we can observe the non-linearity in the classiﬁcation of the two sets of spirals, which proves that this backpropagation method is working.
5.2 Separating-Concentric-Circles Problem Another type of natural patterns is concentric rings.
As a test, we use the sklearn.dataset.make circles function to create 2 concentric circles with each 100 data points, which were respectively assigned to two classes.
We used an artiﬁcial neural network model with the conﬁgurations 16-64-32-2, again using the ‘Softplus’ activation function on all neurons of the network.
We trained the model for 1000 epochs with a learning rate of 0.00001.
Observing ﬁgure 8, we can see that there is a slight non-linearity in the classiﬁcation of the 2 points.
We can observe an accuracy rate of 61%.
This low accuracy can again be justiﬁed with the fact that the softplus activation function is not suitable for such types of data.
5.3 XOR Problem Continuing our tests on this alternate algorithm, we create a dataset with 1000 data points with each data sample containing 2 numbers, and 1 class number.
If the 2 numbers are both positive or negative, the class is 0, else, the class number is 1.
The XOR function is applied on the sign of the number.
10 Figure 7: Training data points for the Concentric-Circles problem Figure 8: Testing data points for the Concentric-Circles problem Our model was of conﬁguration 4-8-16-32-1 where ‘Softplus’ activation func- tion is applied by all neurons.
The learning rate was set to 0.0001 and the network was trained for 100 epochs.
A validation accuracy of 81% was achieved.
11 Figure 9: Training data points for the XOR problem Figure 10: Testing data points for the XOR problem 5.4 Wisconsin Breast Cancer Dataset To further test our neural network model, we used a real-world dataset in test- ing our neural network.
This dataset contains 699 samples, where each sample has 10 attributes as the features, and 1 class attribute.
This dataset is taken from the UCI Machine Learning Repository, where samples arrive periodically as Dr. Wolberg reports his clinical cases.
The model had a conﬁguration of 16-2, and the ‘Softplus’ activation function is applied by all neurons.
We trained the model for 1000 epochs with a learning rate of 0.0001.
We could observe that the validation accuracy reached upto 12 90.4% at the 78th epoch.
Even though the values of validation error and train- ing error are erratic in the start, they seem to reach an almost constant value after some number of epochs.
Figure 11: Validation Accuracy while training for Wisconsin Breast Cancer Dataset Figure 12: Training Error while training for Wisconsin Breast Cancer Dataset From the above experiments, we can conclude that the Softplus activation function is more suited to the Wisconsin Breast Cancer Dataset and that our proposed backpropagation algorithm truly works.
13 Figure 13: Validation Error while training for Wisconsin Breast Cancer Dataset 6 Discussions and Conclusion From the above stated facts and results, we can observe a few properties with this method.
This proposed method of backpropagation can be used very well with activation functions where the domain of the activation function matches the range of its inverse.
This property eases the requirement that the activation function must be diﬀerentiable.
Therefore, ReLU-like activation functions such as LeakyReLU, Softplus, S-shaped rectiﬁed linear activation unit (SReLU), etc.
will be a good match with this method.
Further optimizations must be made to this method, so that, it can be eﬃ- ciently used.
The requirement of a diﬀerent type of activation function could accelerate the discovery of many more activation functions which could ﬁt var- ious diﬀerent models.
We believe that because this backpropagation method suits ReLU-like [2] ac- tivation functions, it can be enhanced to be used in the ﬁelds of biomedical engineering, due to the asymmetric behaviour of data collected in such ﬁelds where the number of data points in diﬀerent classes are not balanced.
Possibly in the future, if a suitable replacement for activation functions, such as Sigmoid and Tanh, are created, this method could be used more frequently.
References [1] Rumelhart, David E.; Hinton, Geoﬀrey E.; Williams, Ronald J.
(8 October 1986).
”Learning representations by back-propagating errors”.
Nature.
323 (6088): 533–536.
doi:10.1038/323533a0 14 [2] arXiv:1710.05941 - Prajit Ramachandran, Barret Zoph, Quoc V.
Le - Search- ing for Activation Functions [3] Rosenblatt, Frank (1958), The Perceptron: A Probabilistic Model for Infor- mation Storage and Organization in the Brain, Cornell Aeronautical Labo- ratory, Psychological Review, v65, No. 6, pp.
386–408.
doi:10.1037/h0042519 [4] Snyman, Jan (3 March 2005).
Practical Mathematical Optimization: An In- troduction to Basic Optimization Theory and Classical and New Gradient- Based Algorithms.
Springer Science & Business Media.
ISBN 978-0-387- 24348-1 [5] arXiv:1606.04474 - Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W.
Hoﬀman, David Pfau, Tom Schaul, Brendan Shillingford, Nando de Freitas - Learning to learn by gradient descent by gradient de- scent [6] arXiv:1602.05980v2 - Bing Xu, Ruitong Huang, Mu Li - Revise Saturated Activation Functions [7] Guang-Bin Huang, Qin-Yu Zhu, Chee-Kheong Siew - Extreme learning ma- chine: a new learning scheme of feedforward neural networks.
- ISBN: 0- 7803-8359-1 [8] Weisstein, Eric W.
”Moore-Penrose Matrix Inverse.” From MathWorld– http://mathworld.wolfram.com/Moore- A Wolfram Web Resource.
PenroseMatrixInverse.html [9] https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original) 15
Model interpretability is a long-standing problem in machine learning that has become quite acute with the accelerating pace of widespread adoption of complex predictive algorithms.
There are multiple approaches to interpreting models and their predictions ranging from a variety of visualization techniques [1–3] to explanations by example [4, 5].
The approach that we consider in this paper thinks of explanations as models themselves that approximate the decision boundary of the original predictor but belong to a class that is signiﬁcantly simpler (e.g., local linear approximations).
Explanations can be generated either post-hoc or alongside predictions.
A popular method, called LIME [6], takes the ﬁrst approach and attempts to explain predictions of an arbitrary model by searching for linear local approximations of the decision boundary.
On the other hand, recently proposed contextual explanation networks (CENs) [7] incorporate a similar mechanism directly into deep neural networks of arbitrary architecture and learn to predict and to explain jointly.
Here, we focus on analyzing a few properties of the explanations generated by LIME, its variations, and CEN.
In particular, we seek answers to the following questions: 1.
Explanations are as good as the features they use to explain predictions.
We ask whether and how feature selection and feature noise affect consistency of explanations.
2.
When explanation is a part of the learning and prediction process, how does that affect perfor- mance of the predictive model?
3.
Finally, what kind of insight we can gain by visualizing and inspecting explanations?
2 Methods We start with a brief overview of the methods compared in this paper: LIME [6] and CENs [7].
Given a dataset of inputs, x ∈ X , and targets, y ∈ Y, our goal is to learn a predictive model, f : X (cid:55)→ Y.
To explain each prediction, we have access to another set of features, z ∈ Z, and construct explanations, gx : Z (cid:55)→ Y, such that they are consistent with the original model, gx(z) = f (x).
These additional features, z, are assumed to be more interpretable than x, and are called interpretable representation in [6] and attributes in [7].
Interpretable ML Symposium, 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.
2.1 LIME and Variations Given a trained model, f, and an instance with features (x, z), LIME constructs an explanation, gx, as follows: gx = argmin g∈G L(f, g, πx) + Ω(g) (1) where L(f, g, πx) is the loss that measures how well g approximates f in the neighborhood deﬁned by the similarity kernel, πx : Z (cid:55)→ R+, in the space of additional features, Z, and Ω(g) is the penalty on the complexity of explanation.
Now more speciﬁcally, Ribeiro et al.
[6] assume that G is the class of linear models: (2) and deﬁne the loss and the similarity kernel as follows: gx(z) := bx + wx · z πx(z(cid:48)) := exp(cid:8) −D(z, z(cid:48))2/σ2(cid:9) (3) L(f, g, πx) := πx(z(cid:48)) (f (x(cid:48)) − g(z(cid:48)))2 , where the data instance is represented by (x, z), z(cid:48) and the corresponding x(cid:48) are the perturbed features, D(z, z(cid:48)) is some distance function, and σ is the scale parameter of the kernel.
Ω(g) is further chosen to favor sparsity of explanations.
(cid:88) z(cid:48)∈Z 2.2 Contextual Explanation Networks K(cid:88) k=1 LIME is a post-hoc model explanation method.
This means that it justiﬁes model predictions by producing explanations which, while locally correct, are never used to make the predictions in the ﬁrst place.
Contrary to that, CENs use explanations as the integral part of the learning process and make predictions by applying generated explanations.
Now more formally, CENs construct the predictive model f : X ×Z (cid:55)→ Y via a composition: given x, an encoder, eθ : X (cid:55)→ G, produces an explanation g which is further applied to z to make a prediction.
In other words: (4) f (x, z) := gx(z), where gx := eθ(x) In [7] we introduced a more general probabilistic framework that allows to combine different deterministic and probabilistic encoders with explanations represented by arbitrary graphical models.
To keep our discussion simple and concrete, here we assume that explanations take the same linear form (2) as for LIME and the encoder maps x to (bx, wx) as follows: bx := αθ(x)(cid:62)B, wx := αθ(x)(cid:62)W, where α(k) θ (x) = 1,∀k : α(k) θ (x) ≥ 0 (5) In other words, explanation (bx, wx) is constrained to be a convex combination of K components from a global learnable dictionary, D := (B, W ), where the combination weights, αθ(x), also called attention, are produced by a deep network.
Encoder of such form is called constrained deterministic map in [7] and the model is trained jointly w.r.t. (θ, B, W ) to minimize the prediction error.
3 Analysis Both LIME and CEN produce explanations in the form of linear models that can be further used for prediction diagnostics.
Our goal is to understand how different conditions affect explanations generated by both methods, see whether this may lead to erroneous conclusions, and ﬁnally understand how jointly learning to predict and to explain affects performance.
We use the following 3 tasks in our analysis: MNIST image classiﬁcation1, sentiment classiﬁcation of the IMDB reviews [8], and poverty prediction for households in Uganda from satellite imagery and survey data [9].
The details of the setup are omitted in the interest of space but can be found in [7], as we follow exactly the same setup.
3.1 Consistency of Explanations Linear explanation assign weights to the interpretable features, z, and hence strongly depend their quality and the way we select them.
We consider two cases where (a) the features are corrupted with additive noise, and (b) selected features are incomplete.
For analysis, we use MNIST and IMDB data.
1http://yann.lecun.com/exdb/mnist/ (a) (b) Fig.
1: The effect of feature quality on explanations.
(a) Explanation test error vs.
the level of the noise added to the interpretable features.
(b) Explanation test error vs.
the total number of interpretable features.
We train baseline deep architectures (CNN on MNIST and LSTM on IMDB) and their CEN variants.
For MNIST, z is either pixels of a scaled down image (pxl) or HOG features (hog).
For IMDB, z is either a bag of words (bow) or a topic vector (tpc) produced by a pre-trained topic model.
The effect of noisy features.
In this experiment, we inject noise2 into the features z and ask LIME and CEN to ﬁt explanations to the noisy features.
The predictive performance of the produced explanations on noisy features is given on Fig.
1a.
Note that after injecting noise, each data point has a noiseless representation x and noisy ˜z.
Since baselines take only x as inputs, their performance stays the same and, regardless of the noise level, LIME “successfully” overﬁts explanations—it is able to almost perfectly approximate the decision boundary of the baselines using very noisy features.
On the other hand, performance of CEN gets worse with the increasing noise level indicating that model fails to learn when the selected interpretable representation is low quality.
The effect of feature selection.
Here, we use the same setup, but instead of injecting noise into z, we construct ˜z by randomly subsampling a set of dimensions.
Fig.
1b demonstrates the result.
While performance of CENs degrades proportionally to the size of ˜z, we see that, again, LIME is able to ﬁt explanations to the decision boundary of the original models despite the loss of information.
These two experiments indicate a major drawback of explaining predictions post-hoc: when con- structed on poor, noisy, or incomplete features, such explanations can overﬁt the decision boundary of a predictor and are likely to be misleading.
For example, predictions of a perfectly valid model might end up getting absurd explanations which is unacceptable from the decision support point of view.
3.2 Explanations as a Regularizer In this part, we compare CENs with baselines in terms of performance.
In each task, CENs are trained to simultaneously generate predictions and construct explanations.
Overall, CENs show very competitive performance and are able to approach or surpass baselines in a number of cases, especially on the IMDB data (see Table 1).
This suggests that forcing the model to produce explanations along with predictions does not limit its capacity.
(a) (b) Fig.
2: (a) Training error vs.
iteration (epoch or batch) for baselines and CENs. (b) Validation error for models trained on random subsets of data of different sizes.
Additionally, the “explanation layer” in CENs affects the geometry of the optimization problem and causes faster and better convergence (Fig.
2a).
Finally, we train the models on subsets of data (the size varied from 1% to 20% for MNIST and from 2% to 40% for IMDB) and notice that explanations play the role of a regularizer which strongly improves the sample complexity (Fig.
2b).
2We use Gaussian noise with zero mean and select variance for each signal-to-noise ratio level appropriately.
−30−20−10010SNR,dB141664Testerror(%)MNISTCNNLIME-pxlCEN-pxlCEN-hog−20−10010SNR,dB8163264IMDBLSTMLIME-bowCEN-bowCEN-tpc050100Featuresubsetsize(%)050100Testerror(%)MNISTCNNLIME-pxlCEN-pxlCEN-hog050100Featuresubsetsize(%)2040IMDBLSTMLIME-bowCEN-bowCEN-tpc01020Epochnumber0.250.51.0Trainerror(%)MNISTCNNCEN-pxlCEN-hog05001000Batchnumber204060IMDBLSTMCEN-tpc51015Trainingsetsize(%)510Validationerror(%)MNISTCNNCEN-pxlCEN-hog2040Trainingsetsize(%)10203040IMDBLSTMCEN-bowCEN-tpcTable 1: Performance of the models on classiﬁcation tasks (averaged over 5 runs; the std.
are on the order of the least signiﬁcant digit).
The subscripts denote the features on which the linear models are built: pixels (pxl), HOG (hog), bag-or-words (bow), topics (tpc), embeddings (emb), discrete attributes (att).
MNIST IMDB Satellite Model Err (%) Model Err (%) Model Acc (%) AUC (%) LRpxl LRhog CNN MoEpxl MoEhog CENpxl CENhog 8.00 LRbow 2.98 LRtpc 0.75 LSTM 1.23 MoEbow 1.10 MoEtpc 0.76 CENbow 0.73 CENtpc 13.3 LRemb 17.1 LRatt 13.2 MLP 13.9 MoE 12.2 CEN (cid:63)6.9 VCEN (cid:63)7.8 62.5 75.7 77.4 77.9 81.5 83.4 68.1 82.2 78.7 85.4 84.2 84.6 (cid:63)Best previous results for similar LSTMs: 8.1% (supervised) and 6.6% (semi-supervised) [10].
3.3 Visualizing Explanations Finally, we showcase the insights one can get from explanations produced along with predictions.
Particularly, we consider the problem of poverty prediction for household clusters in a Uganda from satellite imagery and survey data.
The x representation of each household cluster is a collection of 400 × 400 satellite images; z is represented by a vector of 65 categorical features from living standards measurement survey (LSMS).
The goal is binary classiﬁcation of households in Uganda into poor and not poor.
In our methodology, we closely follow the original study of Jean et al.
[9] and use a pretrained VGG-F network for embedding the images into a 4096-dimensional space on top of which we build our contextual models.
Note that this datasets is fairly small (642 points), and hence we keep the VGG-F frozen to avoid overﬁtting.
We note that quantitatively, by conditioning on the VGG features of the satellite imagery, CENs are able to signiﬁcantly improve upon the sparse linear models on the survey features only (known as the gold standard in remote sensing techniques).
After training CEN with a dictionary of size 32, we discover that the encoder tends to sharply select one of the two explanations (M1 and M2) for different household clusters in Uganda (see Fig.
3a and also Fig.
4a in appendix).
In the survey data, each household cluster is marked as either urban or rural; we notice that, conditional on a satellite image, CEN tends to pick M1 for urban areas and M2 for rural (Fig.
3b).
Notice that explanations weigh different categorical features, such as reliability of the water source or the proportion of houses with walls made of unburnt brick, quite differently.
When visualized on the map, we see that CEN selects M1 more frequently around the major city areas, which also correlates with high nightlight intensity in those areas (Fig.
3c,3d).
High performance of the model makes us conﬁdent in the produced explanations (contrary to LIME as discussed in Sec.
3.1) and allows us to draw conclusions about what causes the model to classify certain households in different neighborhoods as poor.
(a) (b) (c) (d) Fig.
3: Qualitative results for the Satellite dataset: (a) Weights given to a subset of features by the two models (M1 and M2) discovered by CEN.
(b) How frequently M1 and M2 are selected for areas marked rural or urban (top) and the average proportion of Tenement-type households in an urban/rural area for which M1 or M2 was selected.
(c) M1 and M2 models selected for different areas on the Uganda map.
M1 tends to be selected for more urbanized areas while M2 is picked for the rest.
(d) Nightlight intensity of different areas of Uganda.
M1M2Water:UnreliableWatersrc:PublictapWalls:UnburntbricksRoof:Thatch,StrawIswaterpayedVegetationHaselectricityNightlightintensity0.9-0.4-0.6-1.20.3-0.20.50.2-0.3-0.3-0.10.4-0.2-0.8-0.7-0.7−0.8−0.40.00.40.80.30.40.50.60.7Timesmodelselected(%)M1M2RuralUrban0.10.20.30.4HHtype:Tenement(%)M1M2AruaGuluKampala(capital)IgangaMasakaKaseseUganda:ContextualModelsM1M2AruaGuluKampala(capital)IgangaMasakaKaseseUganda:NightlightIntensity0%100%References [1] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.
Deep inside convolutional networks: Visualising image classiﬁcation models and saliency maps.
arXiv preprint arXiv:1312.6034, 2013.
[2] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson.
Understanding neural networks through deep visualization.
arXiv preprint arXiv:1506.06579, 2015.
[3] Aravindh Mahendran and Andrea Vedaldi.
Understanding deep image representations by invert- ing them.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5188–5196, 2015.
[4] Rich Caruana, Hooshang Kangarloo, JD Dionisio, Usha Sinha, and David Johnson.
Case-based explanation of non-case-based learning methods.
In Proceedings of the AMIA Symposium, page 212, 1999.
[5] Been Kim, Cynthia Rudin, and Julie A Shah.
The bayesian case model: A generative approach for case-based reasoning and prototype classiﬁcation.
In Advances in Neural Information Processing Systems, pages 1952–1960, 2014.
[6] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
Why Should I Trust You?: Explaining the predictions of any classiﬁer.
In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1135–1144.
ACM, 2016.
[7] Maruan Al-Shedivat, Avinava Dubey, and Eric P Xing.
Contextual explanation networks.
arXiv preprint arXiv:1705.10301, 2017.
[8] Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts.
Learning word vectors for sentiment analysis.
In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 142–150.
Association for Computational Linguistics, 2011.
[9] Neal Jean, Marshall Burke, Michael Xie, W Matthew Davis, David B Lobell, and Stefano Ermon.
Combining satellite imagery and machine learning to predict poverty.
Science, 353 (6301):790–794, 2016.
[10] Rie Johnson and Tong Zhang.
Supervised and semi-supervised text categorization using lstm for region embeddings.
In Proceedings of The 33rd International Conference on Machine Learning, pages 526–534, 2016.
A Appendix (a) Full visualization of explanations M1 and M2 learned by CEN on the poverty prediction task.
(b) Correlation between the selected explanation and the value of a particular survey variable.
Fig.
4: Additional visualizations for the poverty prediction task.
B Details on Consistency of Explanations We provide a detailed description of the experimental setup used for our analysis in Section 3.1. M1M216HHtype:BQ15Iswaterpayed14Waterusagep/day13Dist.towatersrc.12Num.ofrooms11Avg.dist.toroad10Avg.dist.tomarket09Avg.vegetationdec.08Avg.vegetationinc.07Vegetation06Avg.percipitation05Avg.temperature04Hasgenerator03Haselectricity02Isurban01Nightlightintensity-0.9-0.7-0.3-0.3-0.10.10.30.4-0.3-0.10.40.40.10.00.10.2-0.4-0.2-0.10.4-0.20.3-0.0-0.1-0.10.1-0.2-0.8-0.0-0.6-0.7-0.7M1M232Roof:Wood,Planks31Roof:Tin30Roof:Tiles29Roof:Thatch,Straw28Roof:Other27Roof:Mud26Roof:Ironsheets25Roof:Concrete24Roof:Asbestos23HHtype:Uniport22HHtype:Tenement21HHtype:Sharedhouse20HHtype:Other19HHtype:Privatehouse18HHtype:Privateapt17HHtype:Hut-0.7-0.4-0.20.0-0.6-0.30.50.20.30.50.60.5-0.5-0.4-0.5-0.3-0.1-0.3-0.0-0.0-0.6-0.6-0.6-0.7-0.5-0.6-0.2-0.3-0.4-0.70.30.5M1M248Floor:Stone47Floor:Other46Floor:Mosaic/tiles45Floor:Cowdung44Floor:Earth43Floor:Cement42Floor:Bricks41Walls:Stone40Walls:Unburntbricks39Walls:Timber38Walls:Thatch,Straw37Walls:Other36Walls:Mud,poles35Walls:Cementblocks34Walls:Brickw/mud33Walls:Brickw/cement-0.9-1.1-0.5-0.5-0.9-0.90.30.40.3-0.1-0.8-0.8-0.00.0-0.30.60.3-0.20.0-0.9-0.20.4-0.10.10.30.4-0.7-1.0-0.3-0.3-0.6-0.7M1M264Water:Unreliable63Water:Contribution62Water:Badtaste61Water:Unprotect.OK60Water:Longqueues59Water:Faraway58Watersrc:Vendortruck57Watersrc:Unprotectedwell56Watersrc:River/lake/pond55Watersrc:Rainwater54Watersrc:Publictap53Watersrc:Protectedwell52Watersrc:Privatetap51Watersrc:Other50Watersrc:Gravityﬂow49Watersrc:Bore-hole0.9-0.40.3-0.9-0.9-0.4-0.50.2-0.6-0.30.00.2-0.9-0.30.10.3-0.70.2-0.8-0.9-0.6-1.2-0.1-0.1-1.1-1.0-0.8-0.80.70.80.40.2−0.8−0.40.00.40.80123456789101112131415161718192021222324252627282930310.00.1Correlation323334353637383940414243444546474849505152535455565758596061626364Featurenumber−0.050.000.050.10Correlation-0.10.1
A language model (LM) computes the likelihood of a given sentence and is used to improve the accuracy of an automatic speech recogni- tion (ASR) system.
Recent research has focused on neural network (NN) based LMs [1] because of their outstanding performances in generalizing from sparse data, which traditional n-gram based LMs could not do.
In particular, recurrent neural network based LMs (RNNLMs) [2] do not even require Markov assumptions as they can model word histories of variable-length, and these virtues of them have helped improve the performance of many ASR systems [3, 4].
However, to our knowledge, they are not yet actively adopted in real- time ASR systems due to their high computational complexities.
Several attempts have been made to utilize RNNLMs for on- line decoding in real-time ASR systems [5, 6, 7] However, they ei- ther simulate only some aspects of RNNLMs into the traditional ar- chitectures [5, 6], or perform a 2-pass decoding [7] which innately could not be applied before the end of the utterance was reached.
There have also been attempts to apply RNNLM directly to on- line ASR without approximation by eliminating redundant compu- tations [8, 9, 10].
In our previous research [9], we were successful in applying moderate size RNNLMs directly to CPU-GPU hybrid on- line ASR systems with a cache strategy [10].
However, in order to apply it to a more complex task with bigger RNNLMs, we needed to ﬁnd a way to accelerate it further.
Recent studies indicate that one can reduce the number of distinct RNN computations by treating similar past hidden layer outputs, also referred to as history vectors, as same [11], and that 1http://www.openslr.org/12/ RNNLMs can be accelerated with GPU parallelization [12].
In this paper, we attempt two different approaches in order to achieve real- time performance in a large RNNLM based ASR system.
Firstly, a lossy compression is applied to the cache of the history vector.
The precision of the vectors can be controlled by either rounding up with a smaller number of signiﬁcant digits or at an extreme, by storing only the sign of each element.
Next, we propose GPU paralleliza- tion of RNNLM computations, but only on selected layers.
Instead of performing all RNNLM computations on the same platform, compute-intensive parts of the model are computed on GPUs, and the parts that need to utilize a large memory are calculated on CPUs. This method inherently increases the overhead of data transfer be- tween CPUs and GPUs. This is handled by coordinating a batch transfer method that reduces the number of communications and the size of the data blocks at the same time in the hybrid ASR systems.
The paper is organized as follows.
The architecture of our base- line ASR system is explained in Section 2.
The lossy compression method of the history vectors is explained in Section 3.
Section 4 explains how RNNLM rescoring is accelerated with CPU-GPU par- allelization.
Section 5 evaluates performance improvements of the proposed methods, followed by conclusion in Section 6.
2.
ARCHITECTURE OF OUR BASELINE CPU-GPU HYBRID RNNLM RESCORING In the CPU-GPU hybrid ASR system [13], the weighted ﬁnite state transducer (WFST) is composed of four layers each representing an acoustic model (AM), a context model, a pronunciation model, and an LM.
WFSTs output word hypotheses when they reach word boundaries during frame-synchronous Viterbi searches and the hy- potheses can be rescored by a separately stored RNNLM.
However, in order to speed up on-the-ﬂy rescoring based on RNNLMs, we needed to reduce redundant computations as much as possible.
In this section, we brieﬂy outline the architecture of our baseline CPU- GPU hybrid RNNLM rescoring proposed in [9].
The main high- lights of our baseline architecture are the use of gated recurrent unit (GRU) [14] based RNNLM, noise contrastive estimation (NCE) [15] at the output layer, n-gram based maximum entropy (MaxEnt) by- pass [16] from input to output layers, and cache based on-the-ﬂy rescoring.
2.1. GRU based RNN We employed a GRU which is a type of gated RNNs [14].
The GRU is a mechanism designed to prevent vanishing gradient problems re- lated to long-term dependencies over time by using reset gates and update gates.
For calculating a output vector of a GRU hidden layer, a total of six weight matrices and three bias vectors need to be loaded into memory since for each gate and a candidate activation, two Fig.
1.
On-the-ﬂy rescoring with the LM query cache (baseline).
weight matrices and one bias vector are required.
Thus the memory usage can go up to several megabytes even if the weights are stored in a single precision ﬂoating-point format.
The computational com- plexities of GRU computations are O(H × H) for a hidden layer of size H.
This is a highly compute-intensive task considering that the number of unique LM queries in decoding an utterance can reach several hundreds of thousands.
2.2. Noise contrastive estimation In order to guarantee that the scores calculated at the output layer of an RNNLM are valid probabilities, they need to be normalized over different word sequences.
The normalization is a highly com- putationally intensive task considering the vocabulary size V can reach millions.
In order to address this, we employ an NCE at the output layer [15].
NCE is a sampling-based approximation method that treats partition functions as separate parameters and learns them by non-linear logistic regression.
The variances of these partition functions estimated by NCE are often limited to small values [17], allowing us to use the unnormalized scores without signiﬁcant re- duction in the recognition accuracy.
Even though the only required computations are inner products between the GRU outputs and NCE weights corresponding to the current word, the NCE weight matrix of size H × V need to be loaded into memory.
2.3. Maximum Entropy The second strategy to reduce computation in our GRU based RNNLM is to use an n-gram based MaxEnt bypass connections from input to output layers [16].
The MaxEnt scheme helps in main- taining a relatively small size for the hidden layer without signiﬁcant reduction in recognition accuracy.
The two types of parallel models, the main network consisting of GRUs and NCE, and the other with MaxEnt bypass connections, operate as an ensemble model and can improve the overall recognition accuracy.
In order to reduce the computational overhead because of the bypass connections, we implemented a hash-based MaxEnt.
This method requires the load- ing of a large hash table proportional to the number of n-grams, to retrieve a probability for the given n-gram in constant time.
Fig.
2.
Proposed on-the-ﬂy rescoring with the cache of quantized history vectors.
2.4. On-the-ﬂy rescoring with cache The process ﬂow diagram of our baseline CPU-GPU hybrid RNNLM rescoring is shown in Figure 1.
The LM queries with same history as well as following words are deduplicated by applying a cache strategy at the start of the rescoring procedure [9].
After the dedupli- cation, the embedding vectors corresponding to indices are retrieved by using an “Index Table”.
The RNNLM computations are then performed with appropriate values in CPU memory.
The results of the calculations are converted to indices, cached, and returned to graph traversals.
3.
QUANTIZATION OF HISTORY VECTORS The cache-based strategy for deduplicating LM queries dramatically accelerated our baseline RNNLM rescoring with a cache hit ratio of around 89% and more than 10 times reduction in computation time [9].
However, there is still room for improvement by extending this caching strategy to the outputs of GRU hidden layers.
The current GRU hidden layer outputs computed based on the previous GRU hidden layer outputs (history vectors) could be shared between similar LM queries.
Therefore, in order to reuse the pre- computed history vectors, we created another cache for that vectors just before computing RNNLMs as shown in Figure 2.
The key of the cache is the GRU input which is a pair of a word embedding and a history vector, and the value of the cache is a GRU hidden layer output corresponding to that input.
The number of unique computa- tions is further reduced by assuming that close history vectors would result in similar GRU hidden layer outputs, with negligible effect on the overall ASR results.
Euclidean distance would be an easy way to measure the similarity [11], but it would still require a lot of compu- tations that can slow down the whole rescoring process.
Instead, we propose to quantize the history vectors by controlling the precision of history vector itself by rounding up to a speciﬁed decimal point.
We also consider an extreme case, in which we store only the signs of each element, as it would still capture some of the latent meanings which the hidden layers represent.
Table 1 shows the possible reduction of computations for a Table 1.
Redundancy rates of quantized history vectors.
Redundancy rate Precision (baseline) round-2 round-1 sign Count 103,904 102,776 102,776 88,749 0.0 % 1.09 % 1.09 % 14.59 % four-second utterance.
(Note that each element of the history vector ranges from -1 to 1.) The term “Precision” refers to the quantization of history vectors to a speciﬁed decimal place.
After the initial deduplication, in our baseline system, we have 103,904 unique LM queries as can be observed from the ﬁrst row of Table 1.
The “round- 2” row shows that only 1.09% of the computations can be reduced by caching the history vectors rounded to the second decimal place.
Rounding off the history vectors to the ﬁrst decimal place shows that there is no further redundancy.
However, as shown in the last row of Table 1, an extreme case of quantization where only signs of each element are stored, we were able to reduce 14.59% of the computations.
This relatively huge reduction may affect the accu- racy of RNNLM results to some extent since after the extreme sign quantization there are still 2256 possible unique history vectors for a hidden layer of size H = 256, but it is worthy to evaluate its effect on ASR systems.
4.
CPU-GPU HYBRID DEPLOYMENT OF RNNLM COMPUTATION As described in Section 2, the proposed RNNLM model cannot be readily deployed on a GPU processor due to its large memory re- quirement.
The word embedding step at the input layer requires space proportional to the size of vocabulary, and the MaxEnt step at the output layer need to maintain a large hash table that can store the n-grams and the corresponding scores.
Also, the NCE step at the output layer requires loading of an NCE weight matrix proportional to the size of vocabulary.
On the other hand, the hidden layer oc- cupies only a ﬁxed amount of memory but needs a large number of computations instead.
Table 2.
Operation times for each RNNLM computation step in seconds.
Processor CPU GPU GPU Data transfer Count Unit LM Query Frame 102,172 518 Hidden Output Layer Layer 0.04 6.23 0.06 2.15 2.26 0.03 Time 5.94 0.60 The ﬁrst row of Table 2 shows a proﬁling result of an RNNLM computation with a single layer of 128 GRU nodes based on a three- second utterance.
As is expected, the hidden layer takes 99% of the overall computation, which we aim to reduce in this section.
The high computational rates of neural networks are easily accelerated by utilization of GPUs, but their high memory requirements for word embeddings and MaxEnts prevent us from doing so.
Therefore, we deploy only the hidden layer part of the computation on the GPUs and keep the input embedding and output layer computations on the CPU side, as shown in Figure 3.
As can be observed from the second row of Table 2, the hybrid deployment reduces the computation time for the hidden layer to one-third of what was done on CPU alone.
Fig.
3.
Proposed GPU based RNNLM rescoring with frame-wise batch data transfer.
However, this method also introduces a setback.
Because only the middle layer of the RNNLM computations was deployed on the GPU side, and its surrounding layers are computed on CPUs, the information needs to be shared across the two heterogeneous pro- cessor units frequently.
As the number of data exchanges increases, the decoding speed of the hybrid ASR system inevitably decreases.
The second row of Table 2 also shows that there have been more than a hundred thousand data exchanges during an utterance, which delayed the overall computation by 5.94 seconds, which is twice as long as the original utterance.
The frequency of data transfers between CPUs and GPUs affects the decoding speed more critically than the data size in each transfer.
Therefore, we propose a method in which we reduce the number of data copies between CPUs and GPUs by concatenating the needed information to one block per frame.
During the batching step, the history vectors and their next word embeddings that are emitted for each frame are stored in a consecutive CPU memory block, and the whole data block is transferred to GPU memory at once.
The GRU outputs from the GPU are also copied back to the output layer com- putation in one data block.
This effect can be observed from the last row of Table 2, in which the data transfer time is reduced to 10% of the original.
In addition, this approach still works in multi-GPU en- vironments without additional operations by evenly distributing the block to GPUs since the hidden layer calculations for each segment of the CPU memory block are not sequentially related to each other.
5.
EXPERIMENTS 5.1. Experimental setup The LMs in our experiments were trained on the training corpus of LibriSpeech [18].
To compare the performance with n-grams, “4- gram full LM” in LibriSpeech was used.
Both vanilla-RNNLMs and GRU-RNNLMs consisted of a single hidden layer and 4-gram based MaxEnt connections.
The vocabularies used for all RNNLMs were the same as “4-gram full LM” (V = 200, 000).
A bi-directional re- current deep neural network (RDNN) based AM with three hidden Table 3.
Performances on LibriSpeech’s test sets; all evaluations were performed with same decoding options.
LM dev-clean test-clean dev-other test-other Processor Rescoring threads Precision 4-gram full GRU-RNNLM (H = 256) CPU CPU GPU (baseline) round-2 round-1 sign WER RTF WER RTF WER 11.92 4.28 4.05 11.70 11.69 4.06 11.69 4.05 11.69 4.06 11.70 4.05 4.06 11.70 11.69 4.05 4.05 11.70 0.18 2.16 1.85 1.82 1.79 1.10 0.71 0.58 0.52 4.95 4.69 4.69 4.69 4.69 4.69 4.69 4.68 4.69 0.33 2.19 1.89 1.87 1.80 1.08 0.70 0.63 0.52 RTF WER 11.87 0.54 3.58 11.47 11.49 2.91 11.48 2.95 11.47 2.82 11.49 1.94 1.24 11.47 11.47 0.98 0.88 11.47 RTF 0.26 3.37 2.85 2.89 2.75 2.29 1.20 0.97 0.94 long short term memory (LSTM) layers (500 nodes for each layer), and a softmax output layer was trained using about 7,600 hours of the fully transcribed in-house English speech data mostly consist- ing of voice commands and dialogs.
WFSTs were compiled with 2-gram LMs, and all the epsilon transitions were removed so that computations on GPUs could be optimized.
The hardware speciﬁcation for the evaluations was Intel Xeon E5-2680 with 12 physical CPU cores and four Nvidia Tesla K80 GPUs equipped with 12 GB memory.
We used CUDA for GPU par- allelization.
CUBLAS, which is a linear algebra library of CUDA, was used for matrix multiplications and kernel functions were im- plemented for relatively simple operations such as element-wise op- erations.
For RNNLM computations on CPUs such as output layer computations, we used EIGEN which is a C++ based linear algebra library.
5.2. Results perplexity of the 4-gram LM.
In all tasks except for “dev-other,” the GRU-RNNLM size of 256 showed the lowest LM perplexities.
Table 3 shows the word error rate (WER) and the real-time factor (RTF) for the proposed methods for accelerating the online RNNLM rescoring.
All decoding options otherwise mentioned in Table 3 are same for all the methods being compared.
The meanings of values in the column “Precision” are the same as Table 1.
Regarding recog- nition accuracies, the average WER of the baseline system was im- proved by 3.39% relatively than that of the 4-gram LM based system as can be observed from the ﬁrst two rows of Table 3.
As expected in Section 3, caching quantized history vectors rounded off in the ﬁrst and the second decimal points did not show noticeable improvement in recognition speed compared to the baseline system.
However, the proposed quantization strategy of caching only signs of the history vectors was 1.23 times faster compared to the baseline system with- out any accuracy degradations.
As shown in the ﬁfth and sixth rows of Table 3, with the pro- posed GPU parallelization method, even one thread was 1.43 times faster on an average than the fastest CPU based system (sign).
The recognition speed improves further with the use of multiple GPUs. In particular, when the number of GPUs increased to two, the speed was signiﬁcantly improved, which was 1.61 times faster than a sin- gle GPU-based system.
When three GPUs were utilized, we attained real-time speech recognition over all the test cases.
Finally, the RNNLM-based ASR system with four GPUs has shown the fastest average recognition speed of 0.72 RTF.
It was three times faster than the fastest CPU-based system and four times faster than the baseline system.
Fig.
4.
Perplexities depending on LM types.
6.
CONCLUSION In our experiments, LibriSpeech’s development and test cases were used for evaluations.
The performance of different LMs mea- sured in terms of perplexity is shown in Figure 4.
The term “other” in the evaluation cases means the speech data sets were recorded in noisy environments.
As can be seen in Figure 4, the vanilla- RNNLM of size 128 showed the worst accuracies over all the sets and was even worse than that of the 4-gram LM.
The accuracy of vanilla-RNNLM improved dramatically for a hidden layer size of 256 and showed the lowest perplexities, but still worse than a 128- size GRU-RNNLM.
Perplexities of GRU-RNNLMs were dropped by 7.81, 10.10, and 9.75 absolute (averaged over all four cases) for model sizes of 128, 256, and 512, respectively, as compared to the We devised a faster RNNLM based on-the-ﬂy rescoring on both CPU and GPU platforms by introducing a lossy compression strat- egy of history vectors and the novel hybrid parallelization method.
As cache hit ratios got higher by lowering decimal precisions of the vectors, speech recognition was speeded up by 1.23 times.
Although it was not a signiﬁcant improvement, the fact that recognition rates were not affected even if each dimension of the history vectors was stored by one bit representing the sign seemed to provide a clue to the efﬁcient compression way of embedding vectors while minimizing the loss of their information.
Finally, with the CPU-GPU hybrid par- allelization method, the decoding speed over all the cases has fallen within real-time.
[16] T.
Mikolov, A.
Deoras, D.
Povey, L.
Burget, and J.
Cernock´y, “Strategies for training large scale neural network language models,” in Proc.
ASRU, 2011, pp.
196–201.
[17] X.
Chen, X.
Liu, M.
Gales, and P.
Woodland, “Recurrent neu- ral network language model training with noise contrastive es- timation for speech recognition,” in Proc.
ICASSP, 2015, pp.
5411–5415.
[18] V.
Panayotov, G.
Chen, D.
Povey, and S.
Khudanpur, “Lib- rispeech: An ASR corpus based on public domain audio books,” in Proc.
ICASSP, 2015, pp.
5206–5210.
7.
REFERENCES [1] Y.
Bengio, R.
Ducharme, P.
Vincent, and C.
Jauvin, “A neural probabilistic language model,” Journal of Machine Learning Research, vol.
3, pp.
1137–1155, 2003.
[2] T.
Mikolov, M.
Karaﬁat, L.
Burget, J.
Cernocky, and S.
Khu- danpur, “Recurrent neural network based language model,” in Proc.
Interspeech, 2010, pp.
1045–1048.
[3] S.
Kombrink, T.
Mikolov, M.
Karaﬁat, and L.
Burget, “Re- current neural network based language modeling in meeting recognition,” in Proc.
Interspeech, 2011, pp.
5528–5531.
[4] O.
Tilk and T.
Alum¨ae, “Multi-domain recurrent neural net- work language model for medical speech recognition,” in Proc.
Human Language Technologies, 2014, vol.
268, pp.
149–152.
[5] L.
Gwnol and M.
Petr, “Conversion of recurrent neural net- work language models to weighted ﬁnite state transducers for automatic speech recognition,” in Proc.
Interspeech, 2012, pp.
131–134.
[6] E.
Arisoy, S.
Chen, B.
Ramabhadran, and A.
Sethy, “Con- verting neural network language models into back-off language models for efﬁcient decoding in automatic speech recognition,” IEEE Transactions on Acoustics, Speech, and Signal Process- ing, vol.
22, no.
1, pp.
184–192, 2014.
[7] Y.
Si, Q.
Zhang, T.
Li, J.
Pan, and Y.
Yan, “Preﬁx tree based n-best list re-scoring for recurrent neural network language in Proc.
Inter- model used in speech recognition system,” speech, 2013, pp.
3419–3423.
[8] T.
Hori, Y.
Kubo, and A.
Nakamura, “Real-time one-pass decoding with recurrent neural network language model for speech recognition,” in Proc.
ICASSP, 2014, pp.
6364–6368.
[9] K.
Lee, C.
Park, I.
Kim, N.
Kim, and J.
Lee, “Applying gpgpu to recurrent neural network language model based fast network search in the real-time lvcsr,” in Proc.
Interspeech, 2015, pp.
2102–2106.
[10] Z.
Huang, G.
Zweig, and B.
Dumoulin, “Cache based recurrent neural network language model inference for ﬁrst pass speech recognition,” in Proc.
ICASSP, 2014, pp.
6354–6358.
[11] X.
Liu, X.
Chen, Y.
Wang, M.
Gales, and P.
Woodland, “Two efﬁcient lattice rescoring methods using recurrent neural net- work language models,” IEEE/ACM Trans.
Audio, Speech & Language Processing, vol.
24, no.
8, pp.
1438–1449, 2016.
[12] X.
Chen, Y.
Wang, X.
Liu, M.
Gales, and P.
Woodland, “Ef- ﬁcient gpu-based training of recurrent neural network lan- guage models using spliced sentence bunch,” in Proc.
INTER- SPEECH, 2014, pp.
641–645.
[13] J.
Kim, J.
Chong, and I.
Lane, “Efﬁcient on-the-ﬂy hypothesis rescoring in a hybrid gpu/cpu-based large vocabulary contin- uous speech recognition engine,” in Proc.
INTERSPEECH, 2012, pp.
1035–1038.
[14] J.
Chung, C¸ .
G¨ulc¸ehre, K.
Cho, and Y.
Bengio, “Empiri- cal evaluation of gated recurrent neural networks on sequence modeling,” CoRR, vol.
abs/1412.3555, 2014.
[15] M.
Gutmann and A.
Hyv¨arinen, “Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics,” Journal of Machine Learning Research, vol.
13, pp.
307–361, 2012.

Cerebrovascular diseases are common causes of morbidity and mortality in the adult  population worldwide1-3.
Most cerebrovascular diseases are found during routine brain  imaging with CT or MRI, however 2D-DSA remains the gold standard for their accurate  angiographic evaluation and characterization, in particular for arteriovenous  malformations 4 , cerebral aneurysms 5, 6 and dural arteriovenous fistulas 7.
Additional  3D rotational angiography (3D-DSA) is used to improve the visualization and spatial  understanding of vascular structures during the diagnostic work-up of these conditions.
Currently, with many angiographic systems, obtaining a 3D-DSA still requires two  rotational acquisitions; one without injection of contrast (mask run) and one during  injection of contrast (fill run).
These two datasets are used to compute log-subtracted  projections which are then used to reconstruct a subtracted 3D-DSA volume 8, 9 .
Machine learning is a discipline within computer science, closely related to statistics and  mathematical optimization, that aims to learn patterns directly from a large set of  examples that demonstrate a desired outcome or behavior without the need of explicit  instructions 10.
In the context of medical imaging, machine learning methods have been  investigated since early 1990s, initially for computed aided detection and diagnosis in   mammography and pulmonary embolism 11-14, however recent advances in deep  learning 15 (i.e. a specific machine learning technique) have demonstrated  unprecedented performance in many applications, including detections of diabetic  retinopathy 16, breast cancer 17, 18, quantitative analysis of brain tumors in MRI 19, 20,  computed-aided detection of cerebral aneurysms in MR angiography21 and computer  aided detection and classification of thoracic diseases 22, 23.
With recent advances in deep learning and the universal approximation properties of  feedforward neural networks 24, 25, it is hypothesized that a deep neural network is  capable of computing cerebral angiograms with only the vascular information contained  in the fill scan of a 3D-DSA exam acquired with a C-arm cone beam CT system.
If  possible, potential benefits of eliminating the mask scan include 1) reduction of inter- sweep patient motion artifacts caused by the mis-registration of the mask and fill scans  and 2) radiation dose reduction by at least a factor of 2.
The purpose of this work was to develop and test the capability of a deep learning  angiography (DLA) method, based on convolutional neural networks (CNN), to generate  subtracted 3D cerebral angiograms from a single contrast-enhanced exam without the  need for a mask acquisition.
Methods  In the following sections, the patient inclusion criteria and image acquisition protocols  are first presented, followed by a description of the datasets and methods used to train  the DLA model.
Finally, the image analysis and statistical analysis are described.
The  overall study schema is shown in Figure 1.
Patient Cohort  All studies were HIPAA compliant and done under an Institutionally Review Board  approved protocol.
Clinically indicated rotational angiography exams for the assessment  of cerebrovascular abnormalities of 105 patients, scanned from August 2014 through  April 2016 were retrospectively collected.
Cases were selected in a random fashion to  reduce the potential bias in patient selection.
It was thought that the randomized  selection over this period would result in a data set that was representative of the  varieties of conditions that are referred for angiographic studies.
Imaging Acquisition and Reconstruction  All subjects were imaged with a standard 3D-DSA data acquisition protocol using a C- arm cone beam CT system (Axiom Artis zee; Siemens Healthineers).
The protocol  consists of the acquisition of two cone beam CT acquistions(i.e. mask and fill  acquisitions) with 172 or 304 projection images for a 6 or 13 second rotation time  respectively.
Angular coverage for all data acquisition is 260º, with a tube potential of 70  kVp, detector dose per projection image equal to 0.36 µGy per frame and angular  increments of 1.52º or 0.85 º per frame.
Iodinated contrast medium was injected in the  proximal internal carotid artery or vertebral artery just after the initiation of the fill  acquisition.
For each subject, “native fill” and subtracted 3D volumes were  reconstructed using vendor’s proprietary software (InSpace Reconstruction, syngo  Workplace; Siemens Healthineers).
All reconstructions were performed using the  standard filtered backprojection with edge enhancement, normal image characteristic,  full field of view (238x238 mm2) with 512x512 image matrix, and 0.46 mm image  thickness/increment for a 0.46 mm isotropic voxel size.
The effective dose for the  acquisition protocols used in this study is 1.1 mSv for the 6-second rotation acquisition  and 1.8 mSv for 13-second rotation acquisition which is similar to the dose level  reported by others26, 27.
Training Dataset  A training dataset consisting of 13790 axial images from 35 patients with over 150  million labeled voxels was generated using the information from both the cone beam CT  image of the fill scan and the subtracted images from the subtracted cone beam  projection data.
For each patient in the training dataset, vasculature extraction was  performed by a manual thresholding of the subtracted images.
The selection of the  threshold was based on the subjective assessment of complete vasculature  segmentation while excluding image artifacts and background noise with threshold  values typically in the range of 500-700 HU.
Large vessels, specifically the internal  carotid artery (ICA), middle cerebral artery (MCA), anterior cerebral artery (ACA), distal  branches of the MCA and ACA, vertebral artery (VA) and posterior cerebral artery  (PCA) were isolated through three-dimensional connected component analysis 28.
Small  regions not connected to a large vessel were assumed to be image artifacts and were  excluded from the final vasculature volume.
After the previous steps, in the event of  remaining inter-sweep patient motion and streak artifacts, the vasculature volume was  subject to manual artifact removal.
The extraction of bone tissue was performed by subtracting the vasculature volume  from the contrast-enhanced images (i.e. fill scan) and performing manual thresholding  and connectivity analysis (similar to that of vasculature extraction) in the resulting  images.
Only connected regions including the skull and mandible were considered to be  bone.
Remaining streaking artifacts and metal implants in the bone volume were  manually removed.
Finally, the soft tissue class was extracted by thresholding the fill  images with thresholds of [-400,500] HU and applying a morphologic erosion.
The procedure described above generates approximately 0.28 million, 8 million, and 15  million voxels of vasculature, bone, and soft tissue, respectively for each patient.
In  order to mitigate the class imbalance (i.e. different number of labeled voxels per tissue  class) and reduce redundant training data by similarity of adjacent voxels, only 4.3  million labeled voxels per patient were included for training, consisting of all vasculature  voxels and equal number of randomly extracted bone and soft tissue voxels (i.e.  random undersampling) 29.
Validation and Testing Datasets  A validation dataset and a testing dataset were created using the remaining image  volumes from 70 subjects divided into 8 exams for the validation dataset and 62 exams  for the testing dataset.
These datasets were created with the same procedure used to  generate the training dataset, however the tissue labels were constrained to a region  only containing the following anatomy: ICA, MCA, ACA, distal branches of the MCA and  ACA, VA, PCA, the base and anterior aspect of the skull, temporal bone, otic capsule  and surrounding soft tissue as opposed to the entire head in the training dataset.
Each  exam in the validation and testing dataset had approximately the same number of  labeled voxels for each tissue class.
Neural Network Architecture and Implementation  A 30-layer convolutional neural network (CNN) 30 with a ResNet architecture 31, 32   as  shown in Figure 2 was used.
All convolutional layers except the input layer, use 3x3  filters with rectified linear units for activation function.
The input of the network is a  41x41x5 volumetric image patch extracted from the contrast-enhanced image volume;  the network output consists of a 3-way fully connected layer with softmax activation.
Training and inference is performed in a voxel-wise basis where the input volumetric  image patch is labeled with the tissue class of its central voxel.
The DLA model was  implemented using TensorFlow (Google Inc, Mountain View, CA).
Network parameters  were initialized using the variance scaling method 33 and trained from scratch using  synchronous stochastic gradient descent method with a batch size of 512 volumetric  image patches using 2 GTX 1080 Ti (NVIDIA, Santa Clara, CA) GPUs (256 image  patches per GPU).
The time required to process one case in this study varies from 1 to  3 minutes, depending on the size of the image volume.
To account for class imbalance, each tissue class had equal probability of being  included in a single batch (i.e. data re-sampling) 29.
The learning rate was initially set to  be 1x10-3 with momentum of 0.9. The learning rate was reduced to 1x10-4 and 1x10-5  after 1 and 1.5 epochs respectively.
The validation dataset was used only to monitor the  convergence and generalization error during model training.
Early stopping was used  when the validation error reached a plateau at 2x105 iterations.
Statistical Analysis  The trained DLA model was applied for the task of tissue classification in the, validation  and testing cohorts consisting of image volumes from 8 and 62 subjects respectively.
The final vasculature tissue class was used to generate the 3D-DLA images.
To  quantify the generalization error of the trained model, vasculature classification was  evaluated for each labeled voxel in the reference standard for the validation and testing  datasets.
Two-by-two tables were generated for each patient and accuracy, sensitivity  (also known as recall), positive predictive value (also known as precision) and dice  similarity coefficients (also known as F1-score) were calculated.
The 95% CIs for each  performance metric were also reported.
Finally, the clinical 3D-DSA and the 3D-DLA  images were subject to a qualitative assessment for the presence of inter-sweep motion  artifacts and results were expressed as frequencies and percentages.
Results  Contrast-enhanced image volumes from 105 subjects (age 53.3 ± 13.5 years) who  underwent clinically indicated rotational angiography exams for the assessment of  cerebrovascular abnormalities were used in the study.
Contrast medium was injected  via the proximal ICA in 89 patients (85%) and injected via the VA in 16 patients (15%).
Average and 95% CIs for vasculature classification accuracy, sensitivity, positive  predictive value and Dice similarity coefficient in the testing dataset were 98.7% ([98.3,  99.1] %), 97.6% ([96.5, 98.6] %), 98.5% ([97.6, 99.3] %), 98.0% ([97.4, 98.7] %)  respectively.
Table 1 summarizes the performance metrics for vascular classification in  the training, validation and testing datasets.
No residual signal from osseous structures was observed for all testing cases generated  using 3D-DLA except for small regions in the otic capsule and nasal cavity compared to  37% (23/62) of the 3D-DSA cases that presented residual bone artifacts.
Figure 3  shows a comparison of MIP images derived from 3D-DSA and the 3D-DLA datasets of  a patient evaluated for posterior circulation.
One can see how residual bone artifacts  induced by inter-sweep patient motion are greatly reduced in 3D-DLA, improving the  conspicuity of small vessels.
Similarly, Figure 4 shows lateral and oblique MIP images  derived from 3D-DSA and the 3D-DLA datasets of a patient evaluated for anterior  circulation.
Results show reduced residual bone artifacts for 3D-DLA images, in  particular for the anterior aspect of the skull and the temporal bone.
Figure 5 shows a  comparison of volume rendering images for both the clinical 3D-DSA and the 3D-DLA of  a patient with a small aneurysm in the anterior communicating artery and a large  aneurysm in the MCA bifurcation.
Discussion  In this work, a deep CNN was used to learn generic opacified vasculature from contrast  enhanced C-arm cone beam CT datasets in order to generate a 3D cerebral angiogram,  without explicit definition of cerebrovascular diseases or specific vascular anatomy.
The  datasets used for model training, validation and testing, were created by applying  simple image processing techniques with minimum manual editing to a total of 82740  subtracted and contrast-enhanced cone beam CT images from 105 subjects.
The  proposed DLA method is used to improve image quality by reducing image artifacts  caused by mis-registration of mask and fill scans in 3D-DSA in addition to enabling  potential radiation dose reduction.
Many angiographic systems require two rotational acquisitions (mask and fill) for  reconstruction of a subtracted 3D-DSA.
Others, through the use of vascular  segmentation and thresholding algorithms allow a 3D vessel reconstruction to be done  without availability of a mask.
Those that require two rotations are susceptible to  artifacts caused by potential miss-registrations of the mask and fill projections.
Those  which require use of segmentation and thresholding algorithms may be subject to errors  related to too little contrast intensity and/or improper segmentation.
Together, these  techniques remain the standard of care for the diagnosis and treatment planning of  cerebrovascular diseases.
Mis-registration artifacts arise in conventional 3D-DSA  imaging primarily due to 1) small variations in the angular range differences occurring  from one rotational acquisition to another and 2) potential patient motion in both mask  and fill runs.
The mask-free DLA method, by eliminating the need for one of the  rotational acquisitions, in theory, would reduce the chance of motion from both  mechanical instability and patient motion and effectively reduces the radiation dose  required to obtain a 3D-DSA by half in those systems which require 2 rotations.
In the context of medical imaging, machine learning methods have been investigated  since early 1990s11-14, however the recent unprecedented performance of deep  learning, has made major advances in solving very difficult problems in science, that  were thought to be intractable when approached by other means 34, 35.
In addition to  clever mathematical techniques and availability of large annotated datasets, many  authors recognize that the massively parallel computing capabilities of GPUs have  played a key role in the success of deep learning applications, providing accelerations  of 40x – 250x compared to multicore and single core CPUs 10, 15.
For example, the  training procedure of the network used in this study took approximately 23 hours.
This  training procedure could have taken 4-5 weeks if only a multicore CPU computation  architecture was used, making this application unpractical.
Fortunately, the training  procedure is performed offline, it only needs to be done once and GPU computing is  already widely available within the medical imaging community or accessible via cloud  computing services such as the Google Cloud Platform (GCP) or the Amazon Web  Service (AWS).
Also, many standard open-source libraries used for deep learning  applications, are highly optimized to be used in conjunction with GPUs. Once the  parameters of the model have been learned, the process of analyzing new data that  was not used for training the model (i.e. inference) can be further optimized for  production.
The method proposed in this study, uses a voxel-wise training and inference  where the input of the network is a small image region of 41x41x5 voxels around the  voxel of interest.
This approach has multiple benefits, 1) inference can be parallelized,  in other words, the classification of multiple voxels can be performed at the same time.
Therefore, the time required to analyze a new case is directly proportional to the  number of voxels to be classified (e.g. the entire head or a targeted ROI) and the  number and generation of available GPUs. The throughput of the particular research  implementation of the CNN model used in this study is approximately 2500  voxels/s/GPU.
2) This approach results in a large training dataset consisting of 150  million labeled voxels derived from 13790 axial images and 35 exams.
In addition to a  large training dataset, it is important to have a large testing cohort in order to assure a  good model generalization that better reflects how this technique could be used in  practice.
Having a large testing cohort, also helps to determine whether the training  dataset is large enough to achive a desire level of performance.
Although DLA images were successfully created for all validation and testing cases and  were subject to quantitative and qualitative image analysis, this study still has some  limitations: First, the use of a very specific image acquisition protocol and reconstruction  with selective intra-arterial contrast media injection into the proximal internal carotid  artery or vertebral artery may limit its clincal applications.
Fine tuning of the model and  clinical validations with prospective reader studies are required to further generalize  these results to the vasculature of other organ systems, to  complex or uncommon  vascular abnormalities,  as well as to 3D-DSA studies acquired using different image  acquisition protocols and modalities (e.g. injection of IV contrast media, 4D-DSA,  MDCT, etc.).
This kind of prospective reader studies would also overcome the limitation  of the current qualitative evaluation in our study by a single reader (Dr. Strother).
Second, in this study a specific type of deep CNN with 30 layers, implemented with a  ResNet 31, 32  architecture was used to demonstrate the DLA application.
The selection  of the well-known 30-layer ResNet is purely due to the fact that this architecture won an  image contest among computer scientists (i.e. ILSVRC 2015 classification task),  namely, it outperformed other types of networks such as AlexNet, VGGNet and  GoogLeNet, for the task of  natural images classification using the ImageNet data set.
This type of network architecture has also outperformed other type of networks in  medical imaging classification tasks with deeper models (i.e. increased number of  layers) having improved classification accuracy 20, 22, 32, 36.
However, it remains unknown  whether other architectures can be used for DLA and what would be the advanteages or  disadvantages among all these networks.
Furthernmore, additional optimization and fine  tuning of the DLA model hyper-parameters (e.g. number of layers, number of hidden  units per layer, learning rate, regularization schemes, etc.) is required for optimal online  implementation and compatibility with clinical workflow.
Third, even though metallic  objects are automatically subtracted in the 3D-DSA images that were used to create the  training dataset, small movements of metallic implants (e.g. an aneurysm clip or a coil  mass) which occur during a cardiac cycle are, in the case of subtracted images, usually  sufficient to create enough mis-registration artifacts to allow detection of an implant  presence.
This situation, in addition to the high x-ray attenuation and proximity to  vasculature of metallic implants could result in their imitation in the final DLA images.
The presence of high attenuating object (e.g. metal or Onyx) is also known to be an  intrinsic limitation of mask-free angiography (vendors who provide a method to obtain  3D-DSAs without mask also offer the ability to perform a mask and fill acquisition in  situations where metal objects are known to be present) and its clinical implications  need to be addressed with an expert reader study.
Conclusions  A DLA method based in CNNs that generates 3D cerebral angiograms from a contrast  enhanced C-arm cone beam CT without mask data acquisition was developed.
Results  indicate that the proposed method can successfully reduce mis-registration artifacts  induced by inter-sweep patient motion and, by eliminating the need for a mask  acquisition, reduces radiation dose in future clinical 3D angiography.
Acknowledgments  The authors would like to thank Dr. John W.
Garrett for grateful technical and editorial  assistance.
References  1.
Go AS, Mozaffarian D, Roger VL, et al.
Executive Summary: Heart Disease and  Stroke Statistics—2013 Update.
A Report From the American Heart Association  2013;127:143-152  2.
Wiebers DO.
Unruptured intracranial aneurysms: natural history, clinical  outcome, and risks of surgical and endovascular treatment.
The Lancet 2003;362:103- 110  3.
Mohr JP, Parides MK, Stapf C, et al.
Medical management with or without  interventional therapy for unruptured brain arteriovenous malformations (ARUBA): a  multicentre, non-blinded, randomised trial.
The Lancet 2014;383:614-621  4.
Ogilvy CS, Stieg PE, Awad I, et al.
AHA Scientific Statement: Recommendations  for the management of intracranial arteriovenous malformations: a statement for  healthcare professionals from a special writing group of the Stroke Council, American  Stroke Association.
Stroke 2001;32:1458-1471  5.
Hacein-Bey L, Provenzale JM.
Current imaging assessment and treatment of  intracranial aneurysms.
AJR Am J Roentgenol 2011;196:32-44  6.
Anxionnat R, Bracard S, Ducrocq X, et al.
Intracranial Aneurysms: Clinical Value  of 3D Digital Subtraction Angiography in the Therapeutic Decision and Endovascular  Treatment.
Radiology 2001;218:799-808  7.
Gandhi D, Chen J, Pearl M, et al.
Intracranial dural arteriovenous fistulas:  classification, imaging findings, and treatment.
AJNR Am J Neuroradiol 2012;33:1007- 1013  8.
Fahrig R, Fox AJ, Lownie S, et al.
Use of a C-arm system to generate true three- dimensional computed rotational angiograms: preliminary in vitro and in vivo results.
American Journal of Neuroradiology 1997;18:1507-1514  9.
Strobel N, Meissner O, Boese J, et al.
3D Imaging with Flat-Detector C-Arm  Systems.
In: Reiser MF, Becker CR, Nikolaou K, et al., eds.
Multislice CT.
Berlin,  Heidelberg: Springer Berlin Heidelberg; 2009:33-51  10.
Erickson BJ, Korfiatis P, Akkus Z, et al.
Machine Learning for Medical Imaging.
RadioGraphics 2017;37:505-515  11.
Chan H-P, B.
LS-C, Berkman S, et al.
Computer-aided detection of  mammographic microcalcifications: Pattern recognition with an artificial neural network.
Medical Physics 1995;22:1555--1567  12.
Wu Y, Kunio D, L.
GM, et al.
Computerized detection of clustered  microcalcifications in digital mammograms: Applications of artificial neural networks.
Medical Physics 1992;19:555--560  13.
Zhang W, Kunio D, L.
GM, et al.
Computerized detection of clustered  microcalcifications in digital mammograms using a shift-invariant artificial neural  network.
Medical Physics 1994;21:517--524  14.
Wang S, Summers RM.
Machine learning and radiology.
Medical Image Analysis  2012;16:933-951  15.
LeCun Y, Bengio Y, Hinton G.
Deep learning.
Nature 2015;521:436-444  16.
Gulshan V, Peng L, Coram M, et al.
Development and validation of a deep  learning algorithm for detection of diabetic retinopathy in retinal fundus photographs.
JAMA 2016;316:2402-2410  17.
Huynh BQ, Li H, Giger ML.
Digital mammographic tumor classification using  transfer learning from deep convolutional neural networks.
Journal of Medical Imaging  2016;3:034501  18.
Antropova N, Q.
HB, L.
GM.
A deep feature fusion methodology for breast cancer  diagnosis demonstrated on three imaging modality datasets.
Medical Physics  2017;[Epub ahead of print]  19.
Akkus Z, Galimzianova A, Hoogi A, et al.
Deep Learning for Brain MRI  Segmentation: State of the Art and Future Directions.
Journal of Digital Imaging  2017;30:449-459  20.
Korfiatis P, Kline TL, Lachance DH, et al.
Residual Deep Convolutional Neural  Network Predicts MGMT Methylation Status.
Journal of Digital Imaging 2017  21.
Nakao T, Hanaoka S, Nomura Y, et al.
Deep neural network-based computer- assisted detection of cerebral aneurysms in MR angiography.
Journal of Magnetic  Resonance Imaging 2017 [Epub ahead of print]  22.
Wang X, Peng Y, Lu L, et al.
ChestX-ray8: Hospital-Scale Chest X-Ray Database  and Benchmarks on Weakly-Supervised Classification and Localization of Common  Thorax Diseases.
2017 IEEE Conference on Computer Vision and Pattern Recognition  (CVPR); 2017  23.
Lakhani P, Sundaram B.
Deep Learning at Chest Radiography: Automated  Classification of Pulmonary Tuberculosis by Using Convolutional Neural Networks.
Radiology 2017;284:574-582  24.
Cybenko G.
Approximation by superpositions of a sigmoidal function.
Mathematics of Control, Signals and Systems 1989;2:303-314  25.
Hornik K.
Approximation capabilities of multilayer feedforward networks.
Neural  Networks 1991;4:251-257  26.
Struffert T, Hauer M, Banckwitz R, et al.
Effective dose to patient measurements  in flat-detector and multislice computed tomography: a comparison of applications in  neuroradiology.
European Radiology 2014;24:1257-1265  27.
Lang S, Gölitz P, Struffert T, et al.
4D DSA for Dynamic Visualization of Cerebral  Vasculature: A Single-Center Experience in 26 Cases.
American Journal of  Neuroradiology 2017;38:1169  28.
Shapiro LG.
Connected Component Labeling and Adjacency Graph  Construction.
Machine Intelligence and Pattern Recognition 1996;19:1-30  29.
He H, Garcia EA.
Learning from Imbalanced Data.
IEEE Transactions on  Knowledge and Data Engineering 2009;21:1263-1284  30.
LeCun Y, Boser BE, Denker JS, et al.
Handwritten Digit Recognition with a Back- Propagation Network.
Advances in Neural Information Processing Systems; 1990:396-- 404  31.
He K, Zhang X, Ren S, et al.
Identity Mappings in Deep Residual Networks.
In:  Leibe B, Matas J, Sebe N, et al., eds.
Computer Vision – ECCV 2016: 14th European  Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV.
Cham: Springer International Publishing; 2016:630-645  32.
He K, Zhang X, Ren S, et al.
Deep Residual Learning for Image Recognition.
2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR);  2016:770-778  33.
He K, Zhang X, Ren S, et al.
Delving Deep into Rectifiers: Surpassing Human- Level Performance on ImageNet Classification.
2015 IEEE International Conference on  Computer Vision (ICCV); 2015:1026-1034  34.
Silver D, Huang A, Maddison CJ, et al.
Mastering the game of Go with deep  neural networks and tree search.
Nature 2016;529:484-503  35.
Silver D, Schrittwieser J, Simonyan K, et al.
Mastering the game of Go without  human knowledge.
Nature 2017;550:354  36.
Shin HC, Roth HR, Gao M, et al.
Deep Convolutional Neural Networks for  Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer  Learning.
IEEE Transactions on Medical Imaging 2016;35:1285-1298  Table 1.
Summary of performance metrics for vascular classification in the training,  validation and testing datasets.
Sensitivity (recall)   𝑻𝑷𝑻𝑷+𝑭𝑵  PPV (precision)   𝑻𝑷𝑻𝑷+𝑭𝑷  DSC (F1-score)   𝟐𝑻𝑷 𝟐𝑻𝑷+𝑭𝑷+𝑭𝑵  Dataset  Accuracy  𝑻𝑷+𝑻𝑵 𝑻𝑷+𝑻𝑵+𝑭𝑷+𝑭𝑵  Mean  95% CI  Mean  95% CI  Mean  95% CI  Mean  95% CI  Validation (n=8)  97.8%  [96.9, 98.7]%  97.2%  [96.4, 98.1]%  97.5%  [97.0, 98.0]%  98.4%  [98.0, 98.7]%  Testing (n=62)  98.5%  [97.6, 99.3]%  97.6%  [96.5, 98.6]%  98.0%  [97.4, 98.7]%  [98.3, 99.1]%  98.7%  Figure 1.
Overall study schema.
Figure 2.
Neural Network Architecture.
Figure 3.
Comparison of anterior and lateral views of MIP images derived from A) 3D- DSA and B) 3D-DLA datasets of a patient evaluated for posterior circulation.
Residual  bone artifacts induced by inter-sweep patient motion are greatly reduced in 3D-DLA,  improving the conspicuity of small vessels as pointed by the white arrows.
Figure 4.
Comparison of lateral and oblique MIP images derived from A) 3D-DSA and  B) 3D-DLA datasets of a patient evaluated for anterior circulation.
Results show  reduced residual bone artifacts for 3D-DLA images, in particular for the anterior aspect  of the skull and the temporal bone.
Figure 5.
Comparison of volume rendering images for both the A) clinical 3D-DSA and  B) the 3D-DLA of a patient with a small aneurysm in the anterior communicating artery  and a large aneurysm in the MCA bifurcation as pointed by the arrows.

Consider the following problem: Given an n-element set A ⊆ Bm := {0, 1}m from some class of sets A and a hidden element a ∈ A.
Given an oracle that answers queries of the type: “What is the value of ai?”.
Find a polynomial time algorithm that with an input A, asks minimum number of queries to the oracle and ﬁnds the hidden element a.
This is equivalent to constructing a minimum height decision tree for A.
A decision tree is a binary tree where each internal node is labeled with an index from [m] and each leaf is labeled with an assignment a ∈ Bm. Each internal node has two outgoing edges one that is labeled with 0 and the other is labeled with 1.
A node that is labeled with i corresponds to the query “Is ai = 0?”.
An edge that is labeled with ξ corresponds to the answer ξ.
This decision tree is an algorithm in an obvious way and its height is the worst case complexity of the number of queries.
A decision tree T is said to be a decision tree for A if the algorithm that corresponds to T predicts correctly the hidden assignment a ∈ A.
Our goal is to construct a small height decision tree for A ⊆ Bm in time polynomial in m and n := |A|.
We will denote by OPT(A) the minimum height decision tree for A.
This problem is related to the following problem in exact learning [1]: Given a class C of boolean functions f : X → {0, 1}.
Construct in poly(|C|,|X|) time an optimal adaptive algorithm that learns C from membership queries.
This learning problem is equivalent to constructing a minimum height decision tree j = fi(xj)} where fi is the ith function in C and xj is for the set A = {a(i)|a(i) the jth instance in X.
In computer vision the problem is related to minimizing the number of “probes” (queries) needed to determine which one of a ﬁnite set of geometric ﬁgures is present in an image [4].
In game theory the problem is related to the minimum number of turns required in order to win a guessing game.
1.1 Previous and New Results In [4], Arkin et al.
showed that (AMMRS-algorithm) if at every node the deci- sion tree chooses i that partitions the current set (the set of assignments that are consistent to the answers of the queries so far) as evenly as possible, then the height of the tree is within a factor of log |A| from optimal.
I.e., log |A|- approximation algorithm.
Moshkov [14] analysis shows that this algorithm is (ln|A|)-approximation algorithm.
This algorithm runs in polynomial time in m and |A|.
Hyaﬁl and Rivest, [11], show that the problem of constructing a minimum depth decision tree is NP-Hard.
The reduction of Laber and Nogueira, [12] to set cover with the inapproximability result of Dinur and Steurer [6] for set cover implies that it cannot be approximated to a factor of (1 − o(1)) · ln|A| unless P=NP.
Therefore, no better approximation ratio can be obtained if no constraint is added to the set A.
Moshkov, [13], studied the extended teaching dimension combinatorial mea- sure, ETD(A), of a set A ⊆ Bm. It is the maximum over all the possible assign- 3 ments b ∈ Bm of the minimum number of indices I ⊂ [m] in which b agrees with at most one a ∈ A.
Moshkov showed two results.
The ﬁrst is that ETD(A) is a lower bound for OPT(A).
The second is an exponential time algorithm that asks (2ETD(A)/ log ETD(A)) log n queries.
This gives a (ln 2) (ln|A|)/ log ETD(A) - approximation (exponential time) algorithm (since OPT(A) ≥ ETD(A)) and at the same time 2ETD (A)/ log ETD(A)-approximation algorithm (since OPT(A) ≥ log |A|).
Since many interesting classes have small ETD dimension, the latter re- sult gives small approximation ratio but unfortunately Moshkov algorithm runs in exponential time.
In this paper we further study the ETD measure.
We show that any poly- nomial time (1 − o(1))ETD(C)-approximation algorithm implies P=NP.
There- fore, Moshkov algorithm cannot run in polynomial time unless P=NP.
We then show that the above AMMRS-algorithm, [4], is polynomial time (ln 2)ETD(C)- approximation algorithm.
This gives a small approximation ratio for classes with small extended teaching dimension.
Another reason for studying the ETD of classes is the following: If you ﬁnd the ETD of the set A then you either get a lower bound that is better than the information theoretic lower bound log |A| or you get an approximation algorithm with a better ratio than ln|A|.
This is because if ETD(A) < log |A| then the AMMRS-algorithm has a ratio (ln 2)ETD(A) that is better than the ln|A| ratio and if ETD(A) > log |A| then Moshkov lower bound, ETD(A), for OPT(A) is better than the information theoretic lower bound log |A|.
To get the above results, we deﬁne a new combinatorial measure called the density DEN(A) of the set A.
If Q = DEN(A) then there is a subset B ⊆ A such that an adversary can give answers to the queries that eliminate at most 1/Q fraction of the number of elements in B.
This forces the learner to ask at least Q queries.
We then show that ETD(A) ≥ DEN(A) − 1.
On the other hand, we show that if Q = DEN(A) then a query in the AMMRS-algorithm eliminates at least (1 − 1/Q) fraction of the assignments in A.
This gives a polynomial time (ln 2)DEN(A)-approximation algorithm which is also a (ln 2)(ETD(A) + 1)- approximation algorithm.
In order to compare both algorithms we show that (ETD(A) − 1)/ ln|A| ≤ DEN(A) ≤ ETD(A) + 1 and for random uniform A (and therefore for almost all A), with high probability DEN(A) = Θ(ETD(A)/ ln|A|).
Since |A| > ETD(A), this shows that AMMRS-algorithm may get a better approximation ratio than Moshkov algorithm.
The inapproximability results follows from the reduction of Laber and Nogueira, [12] to set cover with the inapproximability result of Dinur and Steurer [6] and the fact that DEN(A) ≤ ETD(A) + 1 ≤ OPT(A) + 1.
We then apply the above results to learning the class of disjunctions of pred- icates from a set of predicates F from membership queries [5].
We show that the ETD of this class is bounded from above by the degree d of its Hasse diagram.
We then show that Moshkov algorithm, for this class, runs in polynomial time and is (d/ log d)-approximation algorithm.
Since |F| ≥ d (and in many applications, |F| (cid:29) d), this improves the |F|-approximation algorithm SPEX in [5] when the 4 size of Hasse diagram is polynomial.
This also gives optimal algorithms when the degree d is constant.
For example, learning axis parallel rays over constant dimension space.
2 Deﬁnitions and Preliminary Results In this section we give some deﬁnitions and preliminary results 2.1 Notation Let Bm = {0, 1}m.
Let A = {a(1), .
.
.
, a(n)} ⊆ Bm be an n-element set.
We will write |A| for the number of elements in A.
For h ∈ Bm we deﬁne A + h = {a + h|a ∈ A} where + (in the square brackets) is the bitwise exclusive or of elements in Bm. For integer q let [q] = {1, 2, .
.
.
, q}.
Throughout the paper, log x = log2 x.
2.2 Optimal Algorithm We denote by OPT(A) the minimum depth of a decision tree for A.
Our goal is to build a decision tree for A with small depth.
Obviously where n := |A|.
log n ≤ OPT(A) ≤ n − 1 (1) The following result is easy to prove (see Appendix A) Lemma 1.
We have OPT(A) = OPT(A + h).
2.3 Extended Teaching Dimension In this section we deﬁne the extended teaching dimension.
Let h ∈ Bm be any element.
We say that a set S ⊆ [m] is a specifying set for h with respect to A if |{a ∈ A | (∀i ∈ S)hi = ai}| ≤ 1.
That is, there is at most one element in A that is consistent with h on the entries of S.
Denote by ETD(A, h) the minimum size of a specifying set for h with respect to A.
The extended teaching dimension of A is ETD(A) = max h∈Bm ETD(A, h).
(2) We will write ETDz(A) for ETD(A, 0).
It is easy to see that ETD(A, h) = ETDz(A + h) and ETD(A) = ETD(A + h).
(3) We say that a set S ⊆ [m] is a strong specifying set for h with respect to A if either h ∈ A and |{a ∈ A | (∀i ∈ S)hi = ai}| = 1, or |{a ∈ A | (∀i ∈ 5 S)hi = ai}| = 0.
That is, if h ∈ A then there is exactly one element in A that is consistent with h on the entries of S.
Otherwise, no element in A is consistent with h on S.
Denote SETD(A, h) the minimum size of a strong specifying set for h with respect to A.
The strong extended teaching dimension of A is SETD(A) = max h∈Bm SETD(A, h).
(4) We will write SETDz(A) for SETD(A, 0).
It is easy to see that SETD(A, h) = SETDz(A + h) and SETD(A) = SETD(A + h).
(5) Obviously, ETD(A, h) ≤ min(m, n−1) and ETD(A, h) ≤ SETD(A, h) ≤ min(m, n) We now show Lemma 2.
We have ETD(A, h) ≤ SETD(A, h) ≤ ETD(A, h) + 1 and therefore ETD(A) ≤ SETD(A) ≤ ETD(A) + 1.
Proof.
The fact ETD(A, h) ≤ SETD(A, h) follows from the deﬁnitions.
Let S ⊆ [m] be a specifying set for h with respect to A.
Then for T := {a ∈ A | (∀i ∈ S)hi = ai} we have t := |T| ≤ 1.
If t = 0 or h ∈ A then S is a strong specifying set for h with respect to A.
If t = 1 and h (cid:54)∈ A then for the element a ∈ T there is j ∈ [m] such that aj (cid:54)= hj and then S ∪ {j} is a strong specifying set for h with respect to A.
This proves that SETD(A, h) ≤ ETD(A, h) + 1.
(cid:117)(cid:116) The other claims follows immediately.
Obviously, for any B ⊆ A ETD(B) ≤ ETD(A), SETD(B) ≤ SETD(A).
(6) 2.4 Hitting Set In this section we deﬁne the hitting set for A.
A hitting set for A is a set S ⊆ [m] such that for every non-zero element a ∈ A there is j ∈ S such that aj = 1.
That is, S hits every element in A except the zero element (if it exists).
The size of the minimum size hitting set for A is denoted by HS(A).
We now show Lemma 3.
We have HS(A) = SETDz(A).
In particular, SETD(A, h) = HS(A+ h) and SETD(A) = maxh∈Bm HS(A + h).
Proof.
If 0 ∈ A then SETDz(A) is the minimum size of a set S such that {a ∈ A | (∀i ∈ S)ai = 0} = {0} and if 0 (cid:54)∈ A then it is the minimum size of a set S such that {a ∈ A | (∀i ∈ S)ai = 0} = ∅.
Therefore the set S hits all the nonzero elements in A.
(cid:117)(cid:116) The other results follow from (5) and the deﬁnition of SETD.
6 2.5 Density of a Set In this section we deﬁne our new measure DEN of a set.
Let A = {a(1), .
.
.
, a(n)} ⊆ Bm. We deﬁne MAJ(A) ∈ Bm such that MAJ(A)i = 1 if the number of ones in (a(1) ) is greater or equal the number of zeros and MAJ(A)i = 0 otherwise.
We denote by MAX(A) the maximum number of ones in (a(1) ) over all i = 1, .
.
.
, m.
Let ,··· , a(n) ,··· , a(n) MAMI(A) = min h∈Bm MAX(A + h) = MAX(A + MAJ(A)).
(7) For j ∈ [m] and ξ ∈ {0, 1} let Aj,ξ = {a ∈ A | aj = ξ}.
Then MAMI(A) = max min(|Aj,0|,|Aj,1|).
We deﬁne the density of a set A ⊆ Bm by DEN(A) = max B⊆A |B| − 1 MAMI(B) (8) (9) Notice that since every j ∈ [m] can hit at most MAX(A) elements in A we have HS(A) ≥ |A| − 1 MAX(A) (10) 3 Bounds for OPT In this section we give upper and lower bounds for OPT.
3.1 Lower Bound Moshkov results in [10, 13] and the information theoretic bound in (1) give the following lower bound.
We give the proof in Appendix A for completeness.
Lemma 4.
[10, 13] Let A ⊆ Bm be any set.
Then OPT(A) ≥ max(ETD(A), log |A|).
Many lower bounds in the literature for OPT(A) are based on ﬁnding a subset B ⊆ A such that for each query there is an answer that eliminates at most small fraction E of B.
Then (|B|− 1)/E is a lower bound for OPT(A).
The best possible bound that one can get using this technique is exactly DEN(A) (Lemma 5), the density deﬁned in Section 2.5. Lemma 6 shows that the lower bound ETD(A) for OPT(A) exceeds any such bound.
In Appendix A we prove Lemma 5.
We have OPT(A) ≥ DEN(A).
Lemma 6.
We have ETD(A) ≥ DEN(A) − 1.
Proof.
By (7) and (9) there is B ⊆ A such that DEN(A) = |B| − 1 MAMI(B) |B| − 1 MAX(B + h) where h = MAJ(B).
Then (11) ETD(A) (2)≥ ETD(B, h) (6)≥ ETD(B) L2≥ SETD(B, h) − 1 L3= HS(B + h) − 1 (10)≥ |B| − 1 MAX(B + h) − 1 (11) = DEN(A) − 1.(cid:117)(cid:116) In Appendix A we also prove Lemma 7.
We have ETD(A) ≤ ln|A| · DEN(A) + 1.
It is also easy to see (by standard analysis using Chernoﬀ Bound) that for a random uniform A, with positive probability, DEN(A) = O(1) and ETD(A) = Θ(log |A|).
See the proof sketch in Appendix A.
So the bound in Lemma 7 is asymptotically best possible.
3.2 Upper Bounds Moshkov [10, 13] proved the following upper bound.
We gave the proof in the Appendix B for completeness.
Lemma 8.
[10, 13] Let A ⊆ {0, 1}m of size n.
Then OPT(A) ≤ ETD(A) + ETD(A) log ETD(A) log n ≤ 2 · ETD(A) log ETD(A) log n.
In [10,13], Moshkov gave an example of a n-set AE ⊆ {0, 1}m with ETD(AE) = E and OPT(AE) = Ω((E/ log E) log n).
So the upper bound in the above lemma is the best possible.
4 Polynomial Time Approximation Algorithm Given a a set A ⊆ Bm. Can one construct an algorithm that ﬁnds a hidden a ∈ A with OPT(A) queries?
Obviously, with unlimited computational power this can be done so the question is: How close to OPT(A) can one get when polynomial time poly(m, n) is allowed for the construction?
8 An exponential time algorithm follows from the following OPT(A) = min i∈[m] max(OPT(Ai,0), OPT(Ai,1)) where Ai,ξ = {a ∈ A | ai = ξ}.
This algorithm runs in time at least m! ≥ (m/e)m.
See also [3, 7].
Can one give a better exponential time algorithm?
In what follows (Theo- rem 1) we use Moshkov [10, 13] result (Lemma 8) to give a better exponential time approximation algorithm.
In Appendix B we give another simple proof of the Moshkov [10, 13] result that in practice uses less number of specifying sets.
When the extended teaching dimension is constant, the algorithm is O(1)- approximation algorithm and runs in polynomial time.
Theorem 1.
Let A be a class of sets A ⊆ Bm of size n.
If there is an algorithm that for any h ∈ Bm and any A ∈ A gives a specifying set for h with respect to A of size at most E in time T then there is an algorithm that for any A ∈ A constructs a decision tree for A of depth at most E + log E log n ≤ E + log E OPT(A) queries and runs in time O(T log n + nm).
Proof.
Follows immediately from Moshkov algorithm [10, 13].
See Appendix B.(cid:117)(cid:116) The following result immediately follows from Theorem 1.
Theorem 2.
Let A ⊆ Bm be a n-set.
There is an algorithm that ﬁnds the hidden column in time (cid:18) m (cid:19) · ETD(A) · n log n and asks at most ETD(A) 2 · ETD(A) · log n log ETD(A) ≤ 2 · min(ETD(A), log n) log ETD(A) OPT(A) queries.
In particular, if ETD(A) is constant then the algorithm is O(1)-approximation algorithm that runs in polynomial time.
Proof.
To ﬁnd a specifying set for h with respect to A we exhaustively check each ETD(A) row of A.
Each check takes time n.
Since the algorithm asks at most ETD(A) · log n queries, the time complexity is as stated in the Theorem.
Can one do it in poly(m, n) time?
Hyaﬁl and Rivest, [11], show that the problem of ﬁnding OPT is NP-Complete.
The reduction of Laber and Nogueira, [12], of set cover to this problem with the inapproximability result of Dinur and 9 Steurer [6] for set cover implies that it cannot be approximated to (1− o(1))· ln n unless P=NP.
In [4], Arkin et al.
showed that (the AMMRS-algorithm) if at the ith query the algorithm chooses an index j that partitions the current node set (the ele- ments in A that are consistent with the answers until this node) A as evenly as possible, that is, that maximizes min(|{a ∈ A|aj = 0}|,|{a ∈ A|aj = 1}|), then the query complexity is within a factor of (cid:100)log n(cid:101) from optimal.
The AMMRS- algorithm, [4], runs in time poly(m, n).
Moshkov [4, 14] analysis shows that this algorithm is ln n-approximation algorithm and therefore is optimal.
In this sec- tion we will give a simple proof.
In [10, 13], Moshkov gave a simple ETD(A)-approximation algorithm (Al- gorithm MEMB-HALVING-1 in [10]).
He then gave another algorithm that achieves the query complexity in Lemma 8 (Algorithm MEMB-HALVING-2 in [10]).
This is within a factor of 2 · min(ETD(A), log n) log ETD(A) from optimal.
This is better than the ratio ln n, but, unfortunately, both algo- rithms require ﬁnding a minimum size specifying set and the problem of ﬁnding a minimum size specifying set for h is NP-Hard, [2, 8, 15].
Can one achieve a O(ETD(A))-approximation.
In the following we give a sur- prising result.
We show that the AMMRS-algorithm is (ln 2)ETD(A)-approximation algorithm.
We also show that no better ratio can be achieved unless P=NP.
Theorem 3.
The AMMRS-algorithm runs in time O(mn) and ﬁnds the hidden element a ∈ A with at most DEN(A) · ln(n) ≤ min((ln 2)DEN(A), ln n) · OPT(A) ≤ min((ln 2)(ETD(A) + 1), ln n) · OPT(A) queries.
Proof.
Let B be any subset of A.
Then, DEN(B) (9)≥ |B| − 1 MAMI(B) and therefore MAMI(B) ≥ |B| − 1 DEN(B) ≥ |B| − 1 DEN(A) Since the AMMRS-algorithm chooses at each node in the decision tree the index j that maximizes min(|Bj,0|,|Bj,1|) where Bj,ξ = {a ∈ B|aj = ξ} and B is the set of elements in A that are consistent with the answers until this node, we have max(|Bj,0|,|Bj,1|) − 1 = |B| − 1 − min(|Bj,0|,|Bj,1|) (8) = |B| − 1 − MAMI(B) ≤ (|B| − 1) (cid:18) 1 − DEN(A) (cid:19) 10 Therefore, for a node v of depth h in the decision tree, the set B(v) of elements in A that are consistent with the answers until this node contains at most (cid:19)h + 1 (cid:18) (|A| − 1) 1 − DEN(A) elements.
Therefore the depth of the tree is at most DEN(A) ln|A|.(cid:117)(cid:116) We now show that our approximation algorithm is optimal Theorem 4.
Let  be any constant.
There is no polynomial time algorithm that ﬁnds the hidden element with less than (1 − )DEN(A) · ln|A| unless P=NP.
Proof.
Suppose such an algorithm exists.
Then (1 − )DEN(A) ln|A| L5≤ (1 − ) ln|A|OPT(A).
That is, the algorithm is also (1 − ) ln|A|-approximation algorithm.
Laber and Nogueira, [12] gave a polynomial time algorithm reduction of minimum depth decision tree to set cover and Dinur and Steurer [6] show that there is no poly- nomial time (1 − o(1)) · ln|A| for set cover unless P=NP.
Therefore, such an (cid:117)(cid:116) algorithm implies P=NP.
5 Applications to Disjunction of Predicates In this section we apply the above results to learning the class of disjunctions of predicates from a set of predicates F from membership queries [5].
Let C = {f1, .
.
.
, fn} be a set of boolean functions fi : X → {0, 1} where X = {x1, .
.
.
, xm}.
Let AC = {(fi(x1), .
.
.
, fi(xm)) | i = 1, .
.
.
, n}.
We will write OPT(AC), ETD(AC), etc.
as OPT(C), ETD(C), etc.
Let F be a set of boolean functions (predicates) over a domain X.
We consider the class of functions F∨ := {∨f∈Sf | S ⊆ F}.
5.1 An Equivalence Relation Over F∨ In this section, we present an equivalence relation over F∨ and deﬁne the repre- sentatives of the equivalence classes.
This enables us in later sections to focus on the representative elements from F∨.
Let F be a set of boolean functions over the domain X.
The equivalence relation = over F∨ is deﬁned as follows: two disjunctions F1, F2 ∈ F∨ are equivalent (F1 = F2) if F1 is logically equal to F2.
In other words, they represent the same function (from X to {0, 1}).
We write F1 ≡ F2 to denote that F1 and F2 are identical; that is, they have the same representation.
For example, consider f1, f2 : {0, 1} → {0, 1} where f1(x) = 1 and f2(x) = x.
Then, f1 ∨ f2 = f1 but f1 ∨ f2 (cid:54)≡ f1.
11 We denote by F∗ ∨ the set of equivalence classes of = and write each equiv- alence class as [F ], where F ∈ F∨.
Notice that if [F1] = [F2], then [F1 ∨ F2] = [F1] = [F2].
Therefore, for every [F ], we can choose the representative element to be GF := ∨F (cid:48)∈SF (cid:48) where S ⊆ F is the maximum size set that satisﬁes ∨S := ∨f∈Sf = F .
We denote by G(F∨) the set of all representative ele- ments.
Accordingly, G(F∨) = {GF | F ∈ F∨}.
As an example, consider the set F consisting of four functions f11, f12, f21, f22 : {1, 2}2 → {0, 1} where fij(x1, x2) = [xi ≥ j] where [xi ≥ j] = 1 if xi ≥ j and 0 otherwise.
There 2 := F∨ and ﬁve representative functions in G(F∨): are 24 = 16 elements in Ray2 G(F∨) = {f11∨f12∨f21∨f22, f12∨f22, f12, f22, 0} (where 0 is the zero function).
5.2 A Partial Order Over F∨ and Hasse Diagram In this section, we deﬁne a partial order over F∨ and present related deﬁnitions.
The partial order, denoted by ⇒, is deﬁned as follows: F1⇒F2 if F1 logically implies F2.
Consider the Hasse diagram H(F∨) of G(F∨) for this partial order.
The maximum (top) element in the diagram is Gmax := ∨f∈F f .
The minimum (bottom) element is Gmin := ∨f∈∅f , i.e., the zero function.
Figures 3 and 4 shows an illustration of the Hasse diagram.
In a Hasse diagram, G1 is a descendant (resp., ascendent) of G2 if there is a (nonempty) downward path from G2 to G1 (resp., from G1 to G2), i.e., G1⇒G2 (resp., G2⇒G1) and G1 (cid:54)= G2.
G1 is an immediate descendant of G2 in H(F∨) if G1⇒G2, G1 (cid:54)= G2 and there is no G ∈ G(F∨) such that G (cid:54)= G1, G (cid:54)= G2 and G1⇒G⇒G2.
G1 is an immediate ascendant of G2 if G2 is an immediate descendant of G1.
We denote by De(G) and As(G) the sets of all the immediate descendants and immediate ascendants of G, respectively.
The neighbours set of G is Ne(G) = De(G) ∪ As(G).
We further denote by DE(G) and AS(G) the sets of all G’s descendants and ascendants, respectively.
Deﬁnition 1.
The degree of G is deg(G) = |Ne(G)| and the degree deg(F∨) of F∨ is maxG∈G(F∨) deg(G).
For G1 and G2, we deﬁne their lowest common ascendent (resp., greatest com- mon descendant) G = lca(G1, G2) (resp., G = gcd(G1, G2)) to be the minimum (resp., maximum) element in AS(G1) ∩ AS(G2) (resp., DE(G1) ∩ DE(G2)).
Lemma 9.
Let G1, G2 ∈ G(F∨).
Then, lca(G1, G2) = G1 ∨ G2.
G1 ∨ G2 = G.
In particular, if G1, G2 are two distinct immediate descendants of G, then The following result is from [5] 5.3 Witnesses In this subsection we deﬁne the term witness.
Let G1 and G2 be elements in G(F∨).
An element a ∈ X is a witness for G1 and G2 if G1(a) (cid:54)= G2(a).
For a class of boolean functions C over a domain X and a function G ∈ C we say that a set of elements W ⊆ X is a witness set for G in C if for every G(cid:48) ∈ C and G(cid:48) (cid:54)= G there is a witness in W for G and G(cid:48).
12 5.4 The Extended Teaching Dimension of F∨ In this section we prove Lemma 10.
For every h : X → {0, 1} if h (cid:59) Gmax then ETD(F∨, h) = 1.
Otherwise, there is G ∈ G(F∨) such that ETD(F∨, h) ≤ |De(G)| + HS(As(G) ∧ ¯G) ≤ |Ne(G)| = deg(G) where As(G) ∧ ¯G = {s ∧ ¯G | s ∈ As(G)}.
In particular, (cid:0)|De(G)| + HS(As(G) ∧ ¯G)(cid:1) ≤ deg(F∨).
ETD(F∨) ≤ max G∈G(F∨) Proof.
Let h : X → {0, 1} be any function.
If h (cid:59) Gmax then there is an assignment a that satisﬁes h(a) = 1 and Gmax(a) = 0.
Since for all G ∈ G(F∨), G ⇒ Gmax we have G(a) = 0.
Therefore, the set {a} is a specifying set for h with respect to F∨ and ETD(F∨, h) = 1.
Let h ⇒ Gmax.
Consider any G ∈ G(F∨) such that h⇒G and for every imme- diate descendant G(cid:48) of G we have h (cid:59) G(cid:48).
Now for every immediate descendent G(cid:48) of G ﬁnd an assignment a such that G(cid:48)(a) = 0 and h(a) = 1.
Then a is a witness for h and G(cid:48).
Therefore, a is also a witness for h and every descendant of G(cid:48).
Let A be the set of all such assignments, i.e., for every descendant of G one witness.
Then |A| ≤ |De(G)| and A is a witness set for h and all the descen- dants of G.
We note here that if h = 0 then G = Gmin which has no immediate descendants and then A = ∅.
Consider a hitting set B for As(G)∧ ¯G of size HS(As(G)∧ ¯G).
Now for every immediate ascendant G(cid:48)(cid:48) of G ﬁnd an assignment b ∈ B such that G(cid:48)(cid:48)(b)∧ ¯G(b) = 1.
Then G(cid:48)(cid:48)(b) = 1 and G(b) = 0.
Since G(b) = 0 we have h(b) = 0 and then b is a witness for h and G(cid:48)(cid:48).
Therefore, b is also a witness for h and every ascendant of G(cid:48)(cid:48).
Thus B is a witness set for h in all the ascendants of G.
Let G0 be any element in G(F∨) (that is not a descendant or an ascendant).
Consider G1 = lca(G, G0).
By Lemma 9, we have G1 = G ∨ G0.
Since G1 is an ascendent of G there is a witness a ∈ B such that G1(a) = 1 and G(a) = 0.
Then G0(a) = 1, h(a) = 0 and a is a witness of h and G0.
Therefore A ∪ B is a specifying set for h with respect to G(F∨).
Since for every F ∈ F∨ we have F = GF ∈ G(F∨), A ∪ B is also a specifying set for h with respect to F∨.
Since ETD(F∨, h) ≤ |A| + |B| ≤ |De(G)| + HS(As(G) ∧ ¯G) the result follows.
In Appendix C we show that ETD(F∨) = max G∈G(F∨) (cid:0)|De(G)| + HS(As(G) ∧ ¯G)(cid:1) .
(cid:117)(cid:116) We could have replaced |De(G)| by HS(De(G)∧ G), but Lemma 14 in Appendix C shows that they are both equal.
The following result follows immediately from the proof of Lemma 10 Lemma 11.
For any h : X → {0, 1}, a specifying set for h with respect to F∨ of size deg(F∨) can be found in time O(nm).
By Theorem 1 we have 13 Theorem 5.
There is an algorithm that learns F∨ in time O(nm) and asks at most (cid:18) deg(F∨) log deg(F∨) (cid:19) + 1 OPT(F∨) deg(F∨) + deg(F∨) log deg(F∨) log n ≤ membership queries.
5.5 Learning Other Classes If a specifying set of small size cannot be found in polynomial time then from Theorem 2, 3 and Lemma 10, we have Theorem 6.
For a class C we have 1.
There is an algorithm that learns C in time (cid:18) m (cid:19) · ETD(C) · n log n deg(C) and asks at most 2 · ETD(C) · log n log ETD(C)) ≤ 2 · min(ETD(C)), log n) log ETD(C)) OPT(C) membership queries.
In particular, when ETD(C) is constant the algorithm runs in polynomial time and its query complexity is (asymptotically) optimal.
2.
There is an algorithm that learns C in time O(nm) and asks at most DEN(C) · ln(n) ≤ min((ln 2)DEN(C), ln n) · OPT(C) ≤ min((ln 2)(ETD(C) + 1), ln n) · OPT(C) membership queries.
References 1.
Dana Angluin.
Queries and concept learning.
Machine Learning, 2(4):319–342, 1988.
2.
Martin Anthony, Graham R.
Brightwell, David A.
Cohen, and John Shawe-Taylor.
On exact speciﬁcation by examples.
In Proceedings of the Fifth Annual ACM Conference on Computational Learning Theory, COLT 1992, Pittsburgh, PA, USA, July 27-29, 1992., pages 311–318, 1992.
14 3.
Esther M.
Arkin, Michael T.
Goodrich, Joseph S.
B.
Mitchell, David M.
Mount, Christine D.
Piatko, and Steven Skiena.
Point probe decision trees for geometric concept classes.
In Algorithms and Data Structures, Third Workshop, WADS ’93, Montr´eal, Canada, August 11-13, 1993, Proceedings, pages 95–106, 1993.
4.
Esther M.
Arkin, Henk Meijer, Joseph S.
B.
Mitchell, David Rappaport, and Steven Int.
J.
Comput.
Geometry Appl., Skiena.
Decision trees for geometric models.
8(3):343–364, 1998.
5.
Nader H.
Bshouty, Dana Drachsler-Cohen, Martin T.
Vechev, and Eran Yahav.
Learning disjunctions of predicates.
In Proceedings of the 30th Conference on Learning Theory, COLT 2017, Amsterdam, The Netherlands, 7-10 July 2017, pages 346–369, 2017.
6.
Irit Dinur and David Steurer.
Analytical approach to parallel repetition.
In Sym- posium on Theory of Computing, STOC 2014, New York, NY, USA, May 31 - June 03, 2014, pages 624–633, 2014.
7.
M.
R.
Garey.
Optimal binary identiﬁcation procedures.
SIAM Journal on Applied Mathematics, 23(2):173–186, 1971.
8.
Sally A.
Goldman and Michael J.
Kearns.
On the complexity of teaching.
J.
Comput.
Syst.
Sci., 50(1):20–31, 1995.
9.
Sally A.
Goldman, Ronald L.
Rivest, and Robert E.
Schapire.
Learning binary In 30th Annual Symposium on relations and total orders (extended abstract).
Foundations of Computer Science, FOCS 1989, pages 46–51.
10.
Tibor Heged¨us.
Generalized teaching dimensions and the query complexity of learning.
In Proceedings of the Eigth Annual Conference on Computational Learn- ing Theory, COLT 1995, Santa Cruz, California, USA, July 5-8, 1995, pages 108– 117, 1995.
11.
Laurent Hyaﬁl and Ronald L.
Rivest.
Constructing optimal binary decision trees is np-complete.
Inf.
Process.
Lett., 5(1):15–17, 1976.
12.
Eduardo Sany Laber and Loana Tito Nogueira.
On the hardness of the minimum height decision tree problem.
Discrete Applied Mathematics, 144(1-2):209–212, 2004.
13.
M.
Y.
Moshkov.
On conditional tests.
Problemy Kibernetiki.
and Sov.
Phys.
Dokl., 27(7):528–530, 1982.
14.
Mikhail Ju. Moshkov.
Greedy algorithm of decision tree construction for real data tables.
pages 161–168, 2004.
15.
Ayumi Shinohara.
Teachability in computational learning.
New Generation Com- put., 8(4):337–347, 1991.
16.
Ayumi Shinohara and Satoru Miyano.
Teachability in computational learning.
In Algorithmic Learning Theory, First International Workshop, ALT ’90.
15 6 Appendix A In this Appendix we give a proof of some lemmas Lemma 1.
We have OPT(A) = OPT(A + h).
Proof.
Since (A + h) + h = A, it is enough to prove that OPT(A + h) ≤ OPT(A).
Now given a decision tree T for A of depth OPT(A).
For each node, v, in T labeled with j, such that hj = 1, exchange the labels in their outgoing edges.
Then change the label of each leaf labeled with a to a + h.
It is easy to show (cid:117)(cid:116) that the new tree is a decision tree for A + h.
Lemma 4.
[10, 13] Let A ⊆ {0, 1}m be any set.
Then OPT(A) ≥ max(ETD(A), log |A|).
Proof.
The lower bound log |A| is the information theoretic lower bound.
We now prove the other bound.
Let T be a decision tree for A = {a(1), .
.
.
, a(n)} of minimum depth.
Consider the path P in T that at each level chooses the edge that is labeled with 0.
Let S be the set of labels in the internal nodes of P and a(j) be the label of the leaf of P .
Then a(j) is the only element in A that satisﬁes a(j) i = 0 for all i ∈ S.
Therefore S is a specifying set for 0 with respect to A.
Thus OPT(A) ≥ |S| ≥ ETDz(A).
Now, by Lemma 1, for any h ∈ {0, 1}m we have OPT(A) = OPT(A + h) ≥ ETDz(A + h) = ETD(A, h) and therefore OPT(A) ≥ (cid:117)(cid:116) maxh ETD(A, h) = ETD(A).
Lemma 5.
We have OPT(A) ≥ DEN(A).
Proof.
Let B ⊆ A be a set such that |B| − 1 MAMI(B) MAX(B + MAJ(B)) |B| − 1 DEN(A) (9) (7) For every query i ∈ [m] (what is “ai”?), the adversary answers MAJ(B)i.
This eliminates at most MAX(B+MAJ(B)) elements from B.
Therefore the algorithm is forced to ask at least (|B| − 1)/MAX(B + MAJ(B)) queries.
(cid:117)(cid:116) Lemma 7 We have ETD(A) ≤ ln|A| · DEN(A) + 1.
Proof.
There is h0 ∈ {0, 1}m such that ETD(A) L2≤ SETD(A) (4) = SETD(A, h0) L3= HS(A + h0).
(12) 16 For any C ⊆ A we have DEN(C) (9) = max B⊆C (7)≥ max B⊆C |B| − 1 MAMI(B) |B| − 1 MAX(B + h0) |C| − 1 MAX(C + h0) and therefore, for any C ⊆ A we have MAX(C + h0) ≥ |C| − 1 DEN(C) (9)≥ |C + h0| − 1 DEN(A) (13) We now consider the following sequence of subsets of A + h0, C0, C1, .
.
.
, Ct where C0 = A + h0 and the subset Ci+1 is deﬁned by Ci as follows: Since (13) is also true for Ci there is ji ∈ [m] such that ji hits at least (|Ci| − 1)/DEN(A) elements in Ci. Then Ci+1 contains all the elements in Ci that are not hit by ji.
Then |Ci+1| − 1 ≤ |Ci| − |Ci| − 1 DEN(A) Therefore |Ci| ≤ (|A| − 1) (cid:18) − 1 = (|Ci| − 1) 1 − DEN(A) 1 − DEN(A) + 1.
Let Ct be the ﬁrst set in this sequence that satisﬁes Ct = ∅ or Ct = {0}.
Deﬁne X = {ji|i = 0, 1, .
.
.
, t−1}.
Then X is a hitting set for A+h0 of size t.
Therefore, by (12) we have (cid:19) (cid:18) (cid:19)i ETD(A) ≤ HS(A + h0) ≤ t ≤ ln We now give proof sketch of (cid:16) ln(|A| − 1) 1 − DEN(A) (cid:17)−1 + 1 ≤ DEN(A) · ln|A| + 1.
(cid:117)(cid:116) Lemma 12.
There is a set A ⊆ Bm of size n where m = poly(n) such that ETD(A) = Ω(log n) and DEN(A) = O(1).
Proof.
Consider a random uniform set A ⊆ Bm of size n.
The probability that there are k = (log n)/2 entries i1, .
.
.
, tk ∈ [m] such that no a ∈ A satisﬁes (cid:19)(cid:18) ai1 = ai2 = ··· = aik = 0 is (cid:18)m (cid:19)n ≤ 1 1 − 1 2k 17 Therefore, with probability at least 3/4, ETDz(A) ≥ k and then ETD(A) = Ω(log n).
The probability that some subset B ⊆ A of size |B| > 100 has MAMI(B) ≤ |B|/100 is at most (cid:18) 1 (cid:19)m 2n Therefore with probability at least 3/4, MAMI(B) ≥ |B|/100 and DEN(A) = (cid:117)(cid:116) O(1).
7 Appendix B In this appendix we give the proof of Lemma 8 that is the same as the proof of Lemma 3.2 in [10].
Lemma 8 [10, 13] Let A ⊆ {0, 1}m of size n.
Then OPT(A) ≤ ETD(A) + ETD(A) log ETD(A) log n ≤ 2 · ETD(A) log ETD(A) log n.
Proof.
Consider the algorithm in Figure 1.
In Step 3, the algorithm deﬁnes a hypothesis that is the bitwise majority of all the vectors in A(i,1).
In Step 7 an index y is found that maximizes the size of (cid:110) (cid:111) A(i,k) (y,fy) := g ∈ A(i,k) | gy = fy Suppose the variable i (in the algorithm) gets the values 1, 2, .
.
.
, t + 1 and for each 1 ≤ i ≤ t the variable k gets the values 0, 1, 2, .
.
.
, ki.
Then the number of membership queries asked by the algorithm is k1 + ··· + kt.
We ﬁrst prove the following Claim For i = 1, .
.
.
, t − 1 we have |A(i+1,1)| ≤ |A(i,1)| max(2, ki) Proof.
Since S is a specifying set for h, either some y ∈ S satisﬁes hy (cid:54)= ay or a is the only column in A that is consistent with h on S.
Therefore, since h = Majority(A(i,1)), we have |A(i+1,1)| ≤ |A(i,1)| (14) Let D = A(i,1) and D(cid:48) = A(i+1,1).
Suppose y1, .
.
.
, yki are the queries that were asked in the ith stage and let δj = ayj for j = 1, .
.
.
, ki.
Then D(cid:48) = D(y1,δ1),(y2,δ2),...,(yki ,δki ) 18 and (disjoint union) D = D(y1,¯δ1) ∪ D(y1,δ1),(y2,¯δ2) ∪ ··· ∪ D(y1,δ1),(y2,δ2)...,(yki−1,δki−1),(yki ,¯δki ) ∪ D(cid:48).
Let D(j) = D(y1,δ1),(y2,δ2)...,(yj ,δj ), the set of columns in D that are consistent with the target column on the ﬁrst j assignments y1, .
.
.
, yj.
Then D = D(0) (y1,¯δ1) ∪ D(1) (y2,¯δ2) ∪ ··· ∪ D(ki−1) (yki ,¯δki ) ∪ D(cid:48).
For 0 ≤ j ≤ ki − 2, the fact that we took yj+1 for the (j + 1)th query and not yki implies that |D(j) )|.
Therefore, for 0 ≤ j ≤ ki − 2 (yj+1,hyj+1 )| ≤ |D(j) | = |D(j) (ykj ,hykj | ≥ |D(j) (yj+1,hyj+1 ) | = |D(j) (ykj ,δkj )| ≥ |D(cid:48)|.
(ykj ,hykj |D(j) (yj+1,δj+1) Therefore |D| = |D(0) (y1,¯δ1)| + |D(1) (y2,¯δ2)| + ··· + |D(ki−1) (yki ,¯δki )| + |D(cid:48)| ≥ ki · |D(cid:48)|.
With (14), the result of the claim follows.
(cid:117)(cid:116) Let zi = max(2, ki).
Then and therefore(cid:80)t−1 1 ≤ |A(t,1)| ≤ n(cid:81)t i=1 zi i=1 log zi ≤ log n.
Now for E ≥ 4 and since E ≤ n t(cid:88) t−1(cid:88) ki ki = kt + log zi i=1 i=1 log zi ≤ kt + max ≤ E + log E ki log n log zi log n ≤ 2E log E log n.
It is also easy to show that the above is also true for E = 2, 3.
We now prove the time complexity.
Finding a specifying set at each iteration of the While loop takes time T and the number of iterations in at most log n.
This takes T log n time.
Now at the ﬁrst iteration we deﬁne an array of length |S| ≤ E that contains |A(i,1) (z,hz)| for each z ∈ S.
This takes at most |A(i,1)| · E time.
Now if we have such array for A(i,k) (z,hz), we can ﬁnd y (in Step 7) in time E and update the array for A(i,k+1) = A(i,k) (y,hy) in time |A(i,k)\A(i,k) (y,hy)|·E.
Therefore the time of the Repeat loop is at most 2|A(i,1)| · E.
Since |A(i+1,1)| ≤ |A(i,1)|/2, (cid:117)(cid:116) the time of the While loop is at most 4n · E.
This gives the result.
Algorithm: Find the hidden column a ∈ A.
19 1.
i ← 1, k ← 0, A(1,1) ← A.
2.
While |A(i,1)| ≥ 2 do 3.
h ← Majority(A(i,1)) 4.
Find a specifying set S for h with respect to A(i,1) 5.
Repeat 6.
(cid:12)(cid:12)(cid:12)A(i,k) (cid:12)(cid:12)(cid:12) 7.
(z,hz ) k ← k + 1.
Find y ← arg minz∈S Ask query “What is ay”?
A(i,k+1) ← A(i,k) S ← S\{y}.
8.
9.
10.
11.
Until (hy (cid:54)= ay or |A(i,k+1)| = 1) 12.
A(i+1,1) ← A(i,k+1), i ← i + 1, k ← 0 13.
End While 14.
Output the column in A(i,k).
(y,ay ) Fig.
1.
An algorithm that ﬁnd the hidden column a ∈ A We now give another proof Proof.
of Theorem 1 Consider the following algorithm.
After the ith query, the algorithm deﬁnes a set Ai ⊆ A of all the columns that are consistent with the answers of the queries that were asked so far.
Consider any 0 <  < 1.
Now the algorithm searches for a j ∈ [m] such that |Ai| ≤ |{a ∈ Ai | aj = 0}| ≤ (1 − )|Ai|.
If such j ∈ [m] exists then the algorithm asks “What is aj?”.
Let the answer be ξ.
Deﬁne Ai+1 = {a ∈ Ai | aj = ξ}.
Obviously, in that case, |Ai+1| ≤ (1 − )|Ai|.
If no such j ∈ [m] exists then the algorithm ﬁnds a specifying set Th for h := Majority(Ai), where “Majority” is the bitwise majority function.
Then asks queries “What is aj” for all j ∈ Th. If the answers are consistent with h on Th then there is a unique column c ∈ Ai consistent with the answers and the algorithm outputs the index of this column.
Otherwise, there is j0 ∈ Th such that aj0 (cid:54)= hj0.
It is easy to see that in that case |Ai+1| ≤ |Ai|.
(cid:24) (cid:25) Now when  = ln E/E we get OPT(A) ≤ max (cid:24) log n (cid:25)(cid:19) (cid:18) log(1/) log n log(1/(1 − )) ≤ 2E log E log n.
20 The time complexity of this algorithm is O(T log n + mn).
(cid:117)(cid:116) (cid:19)(cid:19) log n (cid:18) E log log E log2 E In fact one can prove the bound (cid:18) E OPT(A) ≤ E log log E log2 E + o log E by substituting  = (ln E)/(E(1 + ln ln E/ ln E)).
8 Appendix C In this Appendix we ﬁnd ETD(F∨) exactly.
We prove ETD(F∨) = max G∈G(F∨) |De(G)| + HS(As(G) ∧ ¯G).
The following result is from [5].
Lemma 13.
Let De(G) = {G1, G2, .
.
.
, Gt} be the set of immediate descendants of G.
If a is a witness for G1 and G, then a is not a witness for Gi and G for all i > 1.
That is, G1(a) = 0, G(a) = 1, and G2(a) = ··· = Gt(a) = 1.
8.1 Teaching Dimension The minimum size of a witness set for G in C is called the witness size and is denoted by TD(C, G).
The value TD(C) := max G∈C TD(C, G) is called the teaching dimension of C, [8, 9, 16].
Obviously, ETD(C, G) ≥ TD(C, G), and ETD(C) ≥ TD(C).
8.2 The Proof Lemma 14.
For every G ∈ F∨ we have TD(F∨, G) ≥ |De(G)| + HS(As(G) ∧ ¯G).
In particular, ETD(F∨) = TD(F∨) = max G∈G(F∨) (cid:0)|De(G)| + HS(As(G) ∧ ¯G)(cid:1) .
Proof.
Let B be a witness set for G in Ne(G).
Take any G(cid:48) ∈ De(G).
Then there is a ∈ B such that G(cid:48)(a) = 0 and G(a) = 1.
Since for any ascendent G(cid:48)(cid:48) of G we have G(cid:48)(cid:48)(a) = 1, a is not a witness to G and any of its ascendants.
By Lemma 13, a cannot be a witness to any other descendent.
In the similar way, a witness for an ascendent of G and G cannot be a witness for any descendent of G and G.
Therefore, 21 TD(F∨, G) ≥ TD(Ne(G), G) = TD(De(G), G) + TD(As(G), G) (15) Now let S be a witness set for G in As(G).
Then for every G(cid:48)(cid:48) ∈ As(G) there is a ∈ S such that G(cid:48)(cid:48)(a) = 1 and G(a) = 0 which is equivalent to G(cid:48)(cid:48)(a)∧ ¯G(a) = 1.
Therefore, = |De(G)| + TD(As(G), G).
TD(As(G), G) ≥ HS(As(G) ∧ ¯G).
This with (15) gives the result.
9 Appendix D (cid:117)(cid:116) 9.1 Example of Classes where fi1,i2,...,im(x1, .
.
.
, xm) = (cid:86)m Deﬁne the class Raym n .
The functions are fi1,i2,...,im (x1, .
.
.
, xm) : [n]m → {0, 1} j=1[xj ≥ ij].
It is easy to see that this class contains O(nm) functions and its Hasse degree is 2m.
See Ray2 See ﬁgure 4 for another example of F with Hasse degree 3.
4 in Figure 2.
Fig.
2.
Hasse diagram of Ray2 [x2 ≥ i].
4.
The functions are fi(x1, x2) = [x1 ≥ i] and gi(x1, x2) = 𝑓1,𝑓2,𝑓3,𝑓4,𝑔1,𝑔2,𝑔3,𝑔4𝑓2,𝑓3,𝑓4,𝑔2,𝑔3,𝑔4𝑓3,𝑓4,𝑔2,𝑔3,𝑔4𝑓4,𝑔2,𝑔3,𝑔4𝑔2,𝑔3,𝑔4𝑔3,𝑔4𝑔4𝑓2,𝑓3,𝑓4,𝑔3,𝑔4𝑓2,𝑓3,𝑓4,𝑔4𝑓2,𝑓3,𝑓4𝑓3,𝑓4𝑓4𝑓3,𝑓4,𝑔3,𝑔4𝑓4,𝑔3,𝑔4𝑓3,𝑓4,𝑔4𝑓4,𝑔422 Fig.
3.
Hasse diagram of ...
Fig.
4.
Hasse diagram when F = {f1, f2, f3, g1, g2, g3, h1, .
.
.
, h5} of functions {1, 2, 3} × {1, 2, 3} → {0, 1} where fi(x1, y1) = [x1 ≥ i], gi(x1, x2) = [x2 ≥ i] and hi(x1, x2) = [x1 + x2 ≥ i + 1].
𝑥1∨𝑥2∨𝑥30𝑥1∨𝑥2𝑥1∨𝑥3𝑥2∨𝑥3𝑥1𝑥2𝑥3𝑥1∨𝑥2∨𝑥1∨𝑥20𝑥1∨𝑥2𝑥1∨𝑥2𝑥2∨𝑥1𝑥1𝑥2𝑥1𝑥1∨𝑥2𝑥2𝑓1,𝑓2,𝑓3,𝑔1,𝑔2,𝑔3,ℎ1,ℎ2,ℎ3,ℎ4,ℎ5𝑓2,𝑓3,𝑔2,𝑔3,ℎ2,ℎ3,ℎ4,ℎ5𝑓2,𝑓3,𝑔3,ℎ3,ℎ4,ℎ5𝑓2,𝑓3,ℎ4,ℎ5𝑓3,ℎ4,ℎ5𝑓3,ℎ5ℎ5𝑓3,𝑔2,𝑔3,ℎ3,ℎ4,ℎ5𝑔2,𝑔3,ℎ4,ℎ5𝑔3,ℎ4,ℎ5𝑔3,ℎ5𝑓3,𝑔3,ℎ3,ℎ4,ℎ5𝑓3,𝑔3,ℎ4,ℎ5ℎ4,ℎ5
Gaussian processes (GPs) naturally give rise to a function space view of modelling, whereby we place a prior distribution over functions, and reason about the properties of likely functions under this prior (Rasmussen & Williams, 2006).
Given data, we then infer a posterior distribution over functions to make predictions.
The generalisation behavior of the Gaussian process is determined by its prior support (which functions are a priori possible) and its inductive biases (which functions are a priori likely), which are in turn encoded by a kernel function.
However, popular kernels, and even multiple kernel learning procedures, typically cannot extract highly expressive hidden representations, as was envisaged for neural networks (MacKay, 1998; Wilson, 2014).
To discover such representations, recent approaches have advocated building more expressive ker- nel functions.
For instance, spectral mixture kernels (Wilson & Adams, 2013) were introduced for ﬂexible kernel learning and extrapolation, by modelling a spectral density with a scale-location mix- ture of Gaussians, with promising results.
However, Wilson & Adams (2013) specify the number of mixture components by hand, and do not characterize uncertainty over the mixture hyperparameters.
As kernel functions become increasingly expressive and parametrized, it becomes natural to also adopt a function space view of kernel learning—to represent uncertainty over the values of the kernel function, and to reﬂect the belief that the kernel does not have a simple form.
Just as we use Gaussian processes over functions to model data, we can apply the function space view a step further in a hierarchical model—with a prior distribution over kernels.
In this paper, we introduce a scalable distribution over kernels by modelling a spectral density, the Fourier transform of a kernel, with a L´evy process.
We consider both scale-location mixtures of Gaussians and Laplacians as basis functions for the L´evy process, to induce a prior over kernels that 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.
gives rise to the sharply peaked spectral densities that often occur in practice—providing a powerful inductive bias for kernel learning.
Moreover, this choice of basis functions allows our kernel func- tion, conditioned on the L´evy process, to be expressed in closed form.
This prior distribution over kernels also has support for all stationary covariances—containing, for instance, any composition of the popular RBF, Mat´ern, rational quadratic, gamma-exponential, or spectral mixture kernels.
And unlike the spectral mixture representation in Wilson & Adams (2013), this proposed process prior allows for natural automatic inference over the number of mixture components in the spectral density model.
Moreover, the priors implied by popular L´evy processes such as the gamma process and symmetric α-stable process result in even stronger complexity penalties than (cid:96)1 regularization, yielding sparse representations and removing mixture components which ﬁt to noise.
Conditioned on this distribution over kernels, we model data with a Gaussian process.
To form a predictive distribution, we take a Bayesian model average of GP predictive distributions over a large set of possible kernel functions, represented by the support of our prior over kernels, weighted by the posterior probabilities of each of these kernels.
This procedure leads to a non-Gaussian heavy- tailed predictive distribution for modelling data.
We develop a reversible jump MCMC (RJ-MCMC) scheme (Green, 1995) to infer the posterior distribution over kernels, including inference over the number of components in the L´evy process expansion.
For scalability, we pursue a structured kernel interpolation (Wilson & Nickisch, 2015) approach, in our case exploiting algebraic structure in the L´evy process expansion, for O(n) inference and O(1) predictions, compared to the standard O(n3) and O(n2) computations for inference and predictions with Gaussian processes.
Flexible distri- butions over kernels will be especially valuable on large datasets, which often contain additional structure to learn rich statistical representations.
The key contributions of this paper are summarized as follows: 1.
The ﬁrst fully probabilistic approach to inference with spectral mixture kernels — to incor- porate kernel uncertainty into our predictive distributions, for a more realistic coverage of extrapolations.
This feature is demonstrated in Section 5.3. 2.
Spectral regularization in spectral kernel learning.
The L´evy process prior acts as a sparsity- inducing prior on mixture components, automatically pruning extraneous components.
This feature allows for automatic inference over model order, a key hyperparameter which must be hand tuned in the original spectral mixture kernel paper.
3.
Reduced dependence on a good initialization, a key practical improvement over the original spectral mixture kernel paper.
4.
A conceptually natural and interpretable function space view of kernel learning.
2 Background We provide a review of Gaussian and L´evy processes as models for prior distributions over functions.
2.1 Gaussian Processes A stochastic process f (x) is a Gaussian process (GP) if for any ﬁnite collection of inputs X = {x1,··· , xn} ⊂ RD, the vector of function values [f (x1),··· , f (xn)]T is jointly Gaussian.
The distribution of a GP is completely determined by its mean function m(x), and covariance kernel k(x, x(cid:48)).
A GP used to specify a distribution over functions is denoted as f (x) ∼ GP(m(x), k(x, x(cid:48))), where E[f (xi)] = m(xi) and cov(f (x), f (x(cid:48))) = k(x, x(cid:48)).
The general- ization properties of the GP are encoded by the covariance kernel and its hyperparameters.
By exploiting properties of joint Gaussian variables, we can obtain closed form expressions for conditional mean and covariance functions of unobserved function values given observed function values.
Given that f (x) is observed at n training inputs X with values f = [f (x1),··· , f (xn)]T , the predictive distribution of the unobserved function values f∗ at n∗ testing inputs X∗ is given by (1) (2) (3) f∗|X∗, X, θ ∼ N (¯f∗, cov(f∗)), ¯f∗ = mX∗ + KX∗,X K−1 cov(f∗) = KX∗,X∗ − KX∗,X K−1 X,X (f − mX ), X,X KX,X∗ .
where KX∗,X for example denotes the n∗ × n matrix of covariances evaluated at X∗ and X.
The popular radial basis function (RBF) kernel has the following form: kRBF(x, x(cid:48)) = exp(−0.5(cid:107)x − x(cid:48)(cid:107)2 /(cid:96)2).
(4) GPs with RBF kernels are limited in their expressiveness and act primarily as smoothing interpo- lators, because the only covariance structure they can learn from data is the length scale (cid:96), which determines how quickly covariance decays with distance.
Wilson & Adams (2013) introduce the more expressive spectral mixture (SM) kernel capable of ex- tracting more complex covariance structures than the RBF kernel, formed by placing a scale-location mixture of Gaussians in the spectrum of the covariance kernel.
The RBF kernel in comparison can only model a single Gaussian centered at the origin in frequency (spectral) space.
2.2 L´evy Processes A stochastic process {L(ω)}ω∈R+ is a L´evy process if it has stationary, independent increments and it is continuous in probability.
In other words, L must satisfy 1.
L(0) = 0, 2.
L(ω0), L(ω1) − L(ω0),··· , L(ωn) − L(ωn−1) are independent ∀ω0 ≤ ω1 ≤ ··· ≤ ωn, 3.
L(ω2) − L(ω1) d= L(ω2 − ω1) ∀ω2 ≥ ω1, 4.
P(|L(ω + h) − L(ω)| ≥ ε) = 0 ∀ε > 0 ∀ω ≥ 0.
lim h→0 By the L´evy-Khintchine representation, the dis- tribution of a (pure jump) L´evy process is com- pletely determined by its L´evy measure.
That is, the characteristic function of L(ω) is given by: log E[eiuL(ω)] = (cid:0)eiu·β − 1 − iu · β1|β|≤1 (cid:1) ν(dβ).
(cid:90) Rd\{0} where the L´evy measure ν(dβ) is any σ-ﬁnite measure which satisﬁes the following integra- bility condition (cid:90) (1 ∧ β2)ν(dβ) < ∞.
Rd\{0} Figure 1: Annotated realization of a compound Poisson process, a special case of a L´evy process.
The ωj represent jump locations, and βj represent jump magnitudes.
A L´evy process can be viewed as a combination of a Brownian motion with drift and a superposition of independent Poisson processes with differing jump sizes β.
The L´evy measure ν(dβ) determines the expected number of Poisson events per unit of time for any particular jump size β.
The Brow- nian component of a L´evy process will not be considered for this model.
For higher dimension input spaces ω ∈ Ω, one deﬁnes the more general notion of L´evy random measure, which is also characterized by its L´evy measure ν(dβdω) (Wolpert et al., 2011) .
We will show that the sample realizations of L´evy processes can be used to draw sample parameters for adaptive basis expansions.
2.3 L´evy Process Priors over Adaptive Expansions Suppose we wish over adaptive class prior expansions: Through a simple manipulation, we can rewrite the f : X → R(cid:12)(cid:12)(cid:12) f (x) =(cid:80)J (cid:110) j=1 βjφ(x, ωj) f (x) into the form of a stochastic integral: specify of to (cid:111) (cid:90) J(cid:88) J(cid:88) (cid:90) J(cid:88) (cid:124) j=1 (cid:123)(cid:122) =dL(ω) (cid:125) f (x) = βjφ(x, ωj) = βj φ(x, ω)δωj (ω)dω = φ(x, ω) βjδωj (ω)dω j=1 j=1 Hence, by specifying a prior for the measure L(ω), we can simultaneously specify a prior for all of the parameters {J, (β1, ω1), ..., (βJ , ωJ )} of the expansion.
L´evy random measures provide a 0246810x-5051015f(x)β1β2β3ω1ω2ω3family of priors naturally suited for this purpose, as there is a one-to-one correspondence between the jump behavior of the L´evy prior and the components of the expansion.
To illustrate this point, suppose the basis function parameters ωj are one-dimensional and consider the integral of dL(ω) from 0 to ω.
(cid:90) ω We see in Figure 1 that(cid:80)J L(ω) = (cid:90) ω J(cid:88) j=1 J(cid:88) j=1 dL(ξ) = βjδωj (ξ)dξ = βj1[0,ω](ωj).
j=1 βj1[0,ω](ωj) resembles the sample path of a compound Poisson pro- cess, with the number of jumps J, jump sizes βj, and jump locations ωj corresponding to the number of basis functions, basis function weights, and basis function parameters respectively.
We can use a compound Poisson process to deﬁne a prior over all such piecewise constant paths.
More generally, we can use a L´evy process to deﬁne a prior for L(ω).
Through the L´evy-Khintchine representation, the jump behavior of the prior is characterized by a L´evy measure ν(dβdω) which controls the mean number of Poisson events in every region of the parameter space, encoding the inductive biases of the model.
As the number of parameters in this framework is random, we use a form of trans-dimensional reversible jump Markov chain Monte Carlo (RJ-MCMC) to sample the parameter space (Green, 2003).
Popular L´evy processes such as the gamma process, symmetric gamma process, and the symmetric α-stable process each possess desirable properties for different situations.
The gamma process is able to produce strictly positive gamma distributed βj without transforming the output space.
The symmetric gamma process can produce both positive and negative βj, and according to Wolpert et al.
(2011) can achieve nearly all the commonly used isotropic geostatistical covariance functions.
The symmetric α-stable process can produce heavy-tailed distributions for βj and is appropriate when one might expect the basis expansion to be dominated by a few heavily weighted functions.
While one could dispense with L´evy processes and place Gaussian or Laplace priors on βj to obtain (cid:96)2 or (cid:96)1 regularization on the expansions, respectively, a key beneﬁt particular to these L´evy process priors are that the implied priors on the coefﬁcients yield even stronger complexity penalties than (cid:96)1 regularization.
This property encourages sparsity in the expansions and permits scalability of our MCMC algorithm.
Refer to the supplementary material for an illustration of the joint priors on coefﬁcients, which exhibit concave contours in contrast to the convex elliptical and diamond contours of (cid:96)2 and (cid:96)1 regularization.
Furthermore, in the log posterior for the L´evy process there is a log(J!) complexity penalty term which further encourages sparsity in the expansions.
Refer to Clyde & Wolpert (2007) for further details.
3 L´evy Distributions over Kernels In this section, we motivate our choice of prior over kernel functions and describe how to generate samples from this prior distribution in practice.
3.1 L´evy Kernel Processes By Bochner’s Theorem (1959), a continuous stationary kernel can be represented as the Fourier dual of a spectral density: S(s)e2πis(cid:62)τ ds, S(s) = k(τ )e−2πis(cid:62)τ dτ.
(5) (cid:90) k(τ ) = RD (cid:90) RD Hence, the spectral density entirely characterizes a stationary kernel.
Therefore, it can be desirable to model the spectrum rather than the kernel, since we can then view kernel estimation through the lens of density estimation.
In order to emulate the sharp peaks that characterize frequency spectra of natural phenomena, we model the spectral density with a location-scale mixture of Laplacian components: φL(s, ωj) = e−λj|s−χj|, ωj ≡ (χj, λj) ∈ [0, fmax] × R+.
Then the full speciﬁcation of the symmetric spectral mixture is λj (cid:104) ˜S(s) + ˜S(−s) (cid:105) S(s) = (6) (7) J(cid:88) j=1 ˜S(s) = βjφL(s, ωj).
As Laplacian spikes have a closed form inverse Fourier transform, the spectral density S(s) repre- sents the following kernel function: J(cid:88) j=1 k(τ ) = βj λ2 j + 4π2τ 2 cos(2πχjτ ).
λ2 (8) The parameters J, βj, χj, λj can be interpreted through Eq. (8).
The total number of terms to the mixture is J, while βj is the scale of the jth frequency contribution, χj is its central frequency, and λj governs how rapidly the term decays (a high λ results in conﬁdent, long-term periodic extrapolation).
Other basis functions can be used in place of φL to model the spectrum as well.
For example, if a Gaussian mixture is chosen, along with maximum likelihood estimation for the learning procedure, then we obtain the spectral mixture kernel (Wilson & Adams, 2013).
As the spectral density S(s) takes the form of an adaptive expansion, we can deﬁne a L´evy prior over all such densities and hence all corresponding kernels of the above form.
For a chosen basis function φ(s, ω) and L´evy measure ν(dβdω) we say that k(τ ) is drawn from a L´evy kernel process (LKP), denoted as k(τ ) ∼ LKP(φ, ν).
Wolpert et al.
(2011) discuss the necessary regularity conditions for φ and ν.
In summary, we propose the following hierarchical model over functions f (x)|k(τ ) ∼ GP(0, k(τ )), τ = x − x(cid:48), k(τ ) ∼ LKP(φ, ν).
(9) Figure 2 shows three samples from the L´evy process speciﬁed through Eq. (7) and their cor- responding covariance kernels.
We also show one GP realization for each of the kernel func- tions.
By placing a L´evy process prior over spectral densities, we induce a L´evy kernel pro- cess prior over stationary covariance functions.
3.2 Sampling L´evy Priors We now discuss how to generate samples from the L´evy kernel process in practice.
In short, the kernel parameters are drawn accord- ing to {J,{(βj, ωj)}J j=1} ∼ L´evy(ν(dβdω)), and then Eq. (8) is used to evaluate k ∼ LKP(φL, ν) at values of τ.
Recall from Section 2.3 that the choice of L´evy measure ν is completely determined by the choice of the corresponding L´evy process and vice versa.
Though the processes men- tioned there produce sample paths with in- ﬁnitely many jumps (and cannot be sampled directly), almost all jumps are inﬁnitesimally small, and therefore these processes can be ap- proximated in L2 by a compound Poisson pro- cess with a jump size distribution truncated by ε.
Once the desired L´evy process is chosen and the truncation bound is set, the basis expansion parameters are generated by drawing J ∼ Poisson(ν+ samples β1,··· , βJ ∼ πβ(dβ), and J i.i.d. samples ω1,··· , ωJ ∼ πω(dω).
Refer to the supplementary ε = νε(R × Ω) for the gamma, symmetric gamma, material for L2 error bounds and formulas for ν+ and symmetric α-stable processes.
The form of πβ(βj) also depends on the choice of L´evy process and can be found in the supplemen- tary material, with further details in Wolpert et al.
(2011).
We choose to draw χ from an uninformed uniform prior over a reasonable range in the frequency domain, and λ from a gamma distribution, λ ∼ Gamma(aλ, bλ).
The choices for aλ, bλ, and the frequency limits are left as hyperparame- ters, which can have their own hyperprior distributions.
After drawing the 3J values that specify Figure 2: Samples from a L´evy kernel mix- ture prior distribution.
(top) Three spectra with Laplace components drawn from a L´evy process prior.
(middle) The corresponding stationary co- variance kernel functions and the prior mean with two standard deviations of the model, as deter- mined by 10,000 samples.
(bottom) GP samples with the respective covariance kernel functions.
ε ), and then drawing J i.i.d. 00.050.10.150.2Frequency024Power00.050.10.150.2Frequency00.51Power00.050.10.150.2Frequency00.511.5Power00.20.40.60.811.21.41.61.82τ-0.100.10.2K(τ)02468101214161820X-0.500.5f(X)a L´evy process realization, the corresponding covariance function can be evaluated through the an- alytical expression for the inverse Fourier transform (e.g. Eq. (8) for Laplacian frequency mixture components).
4 Scalable Inference Given observed data D = {xi, yi}N x∗ for interpolation and extrapolation.
We model observations y(x) with a hierarchical model: i=1, we wish to infer p(y(x∗)|D, x∗) over some test set of inputs y(x)|f (x) = f (x) + ε(x), f (x)|k(τ ) ∼ GP(0, k(τ )), k(τ ) ∼ LKP(φ, ν).
(10) (11) (12) Computing the posterior distributions by marginalizing over the LKP will yield a heavy-tailed non- Gaussian process for y(x∗) = y∗ given by an inﬁnite Gaussian mixture model: ε(x) τ = x − x(cid:48), iid∼ N (0, σ2), p(y∗|D) = p(y∗|k,D)p(k|D)dk ≈ 1 p(y∗|kh), kh ∼ p(k|D).
(13) (cid:90) H(cid:88) h=1 Initialization Considerations We compute this approximating sum using H RJ-MCMC samples (Green, 2003).
Each sample draws a kernel from the posterior kh ∼ p(k|D) distribution.
Each sample of kh enables us to draw a sample from the posterior predictive distribution p(y∗|D), from which we can estimate the predictive mean and variance.
Although we have chosen a Gaussian observation model in Eq. (10) (conditioned on f (x)), all of the inference procedures we have introduced here would also apply to non-Gaussian likelihoods, such as for Poisson processes with Gaussian process intensity functions, or classiﬁcation.
The sum in Eq. (13) requires drawing kernels from the distribution p(k|D).
This is a difﬁcult dis- tribution to approximate, particularly because there is not a ﬁxed number of parameters as J varies.
We employ RJ-MCMC, which extends the capability of conventional MCMC to allow sequential samples of different dimensions to be drawn (Green, 2003).
Thus, a posterior distribution is not limited to coefﬁcients and other parameters of a ﬁxed basis expansion, but can represent a chang- ing number of basis functions, as required by the description of L´evy processes described in the previous section.
Indeed, RJ-MCMC can be used to automatically learn the appropriate number of basis functions in an expansion.
In the case of spectral kernel learning, inferring the number of basis functions corresponds to automatically learning the important frequency contributions to a GP kernel, which can lead to new interpretable insights into our data.
4.1 The choice of an initialization procedure is often an important practical consideration for machine learning tasks due to severe multimodality in a likelihood surface (Neal, 1996).
In many cases, however, we ﬁnd that spectral kernel learning with RJ-MCMC can automatically learn salient fre- quency contributions with a simple initialization, such as a uniform covering over a broad range of frequencies with many sharp peaks.
The frequencies which are not important in describing the data are quickly attenuated or removed within RJ-MCMC learning.
Typically only a few hundred RJ-MCMC iterations are needed to discover the salient frequencies in this way.
Wilson (2014) proposes an alternative structured approach to initialization in previous spectral ker- nel modelling work.
First, pass the (squared) data through a Fourier transform to obtain an empirical spectral density, which can be treated as observed.
Next, ﬁt the empirical spectral density using a standard Gaussian mixture density estimation procedure, assuming a ﬁxed number of mixture com- ponents.
Then, use the learned parameters of the Gaussian mixture as an initialization of the spectral mixture kernel hyperparameters, for Gaussian process marginal likelihood optimization.
We observe successful adaptation of this procedure to our L´evy process method, replacing the approximation with Laplacian mixture terms and using the result to initialize RJ-MCMC.
4.2 Scalability As with other GP based kernel methods, the computational bottleneck lies in the evaluation of the log marginal likelihood during MCMC, which requires computing (KX,X + σ2I)−1y and log |KX,X + σ2I| for an n × n kernel matrix KX,X evaluated at the n training points X.
A di- rect approach through computing the Cholesky decomposition of the kernel matrix requires O(n3) computations and O(n2) storage, restricting the size of training sets to O(104).
Furthermore, this computation must be performed at every iteration of RJ-MCMC, compounding standard computa- tional constraints.
However, this bottleneck can be readily overcome through the Structured Kernel Interpolation approach introduced in Wilson & Nickisch (2015), which approximates the kernel matrix as ˜KX,X(cid:48) = MX KZ,ZM(cid:62) X(cid:48) for an exact kernel matrix KZ,Z evaluated on a much smaller set of m (cid:28) n inducing points, and a sparse interpolation matrix MX which facilitates fast computations.
The calculation reduces to O(n + g(m)) computations and O(n + g(m)) storage.
As described in Wilson & Nickisch (2015), we can impose Toeplitz structure on KZ,Z for g(m) = m log m, allowing our RJ-MCMC procedure to train on massive datasets.
5 Experiments We conduct four experiments in total.
In order to motivate our model for kernel learning in later experiments, we ﬁrst demonstrate the ability of a L´evy process to recover—through direct regression—an observed noise-contaminated spectrum that is characteristic of sharply peaked nat- urally occurring spectra.
In the second experiment we demonstrate the robustness of our RJ- MCMC sampler by automatically recovering the generative frequencies of a known kernel, even in presence of signiﬁcant noise contamination and poor initializations.
In the third experiment we demonstrate the ability of our method to infer the spectrum of airline passenger data, to per- form long-range extrapolations on real data, and to demonstrate the utility of accounting for un- certainty in the kernel.
In the ﬁnal experiment we demonstrate the scalability of our method through training the model on a 100,000 data point sound waveform.
Code is available at https: //github.com/pjang23/levy-spectral-kernel-learning.
5.1 Explicit Spectrum Modelling We begin by applying a L´evy process di- rectly for function modelling (known as LARK regression), with inference as described in Wolpert et al.
(2011), and Laplacian basis func- tions.
We choose an out of class test function proposed by Donoho & Johnstone (1993) that is standard in wavelet literature.
The spatially inhomogeneous function is deﬁned to represent spectral densities that arise in scientiﬁc and en- gineering applications.
Gaussian i.i.d. noise is added to give a signal-to-noise ratio of 7, to be consistent with previous studies of the test func- tion Wolpert et al.
(2011).
The noisy test function and LARK regression ﬁt are shown in Figure 3.
The synthetic spectrum is well characterized by the L´evy process, with no “false positive” basis function terms ﬁtting the noise owing to the strong regularization properties of the L´evy prior.
By contrast, GP regression with an RBF kernel learns a length scale of 0.07 through maximum marginal likelihood training: the Gaussian process posterior can ﬁt the sharp peaks in the test function only if it also overﬁts to the additive noise.
The point of this experiment is to show that the L´evy process with Laplacian basis functions forms a natural prior over spectral densities.
In other words, samples from this prior will typically look like the types of spectra that occur in practice.
Thus, this process will have a powerful inductive bias when used for kernel learning, which we explore in the next experiments.
Figure 3: L´evy process regression on a noisy test function (black).
The ﬁt (red) captures the lo- cations and scales of each spike while ignoring noise, but falls slightly short at its modes since the black spikes are parameterized as (1 + |x|)−4 rather than Laplacian.
012345678910x01020304050f(x)Figure 4: Ground truth recovery of known fre- quency components.
(left) The spectrum of the Gaussian process that was used to generate the noisy training data is shown in black.
From these noisy data and the erroneous spectral initialization shown in dashed blue, the maximum a posteriori estimate of the spectral density (over 1000 RJ- MCMC steps) is shown in red.
A SM kernel also identiﬁes the salient frequencies, but with broader support, shown in magenta.
(right) Noisy training data are shown with a scatterplot, with withheld testing data shown in green.
The learned posterior predictive distribution (mean in black, with 95% credible set in grey) captures the test data.
5.2 Ground Truth Recovery We next demonstrate the ability of our method to recover the generative frequencies of a known kernel and its robustness to noise and poor initializations.
Data are generated from a GP with a kernel having two spectral Laplacian peaks, and partitioned into training and testing sets containing 256 points each.
Moreover, the training data are contaminated with i.i.d. Gaus- sian noise (signal-to-noise ratio of 85%).
Based on these observed training data (depicted as black dots in Figure 4, right), we estimate the kernel of the Gaussian process by inferring its spectral density (Figure 4, left) using 1000 RJ-MCMC iterations.
The empirical spectrum initialization described in section 4.1 results in the discovery of the two generative frequencies.
Critically, we can also recover these salient fre- quencies even with a very poor initialization, as shown in Figure 4 (left).
For comparison, we also train a Gaussian SM kernel, initializing based on the empirical spec- trum.
The resulting kernel spectrum (Figure 4, magenta curve) does recover the salient frequencies, though with less conﬁdence and higher over- head than even a poor initialization and spectral kernel learning with RJ-MCMC.
5.3 Spectral Kernel Learning for Long-Range Extrapolation We next demonstrate the ability of our method to perform long-range extrapolation on real data.
Figure 5 shows a time series of monthly airline passenger data from 1949 to 1961 (Hyn- dman, 2005).
The data show a long-term ris- ing trend as well as a short term seasonal wave- form, and an absence of white noise artifacts.
As with Wilson & Adams (2013), the ﬁrst 96 monthly data points are used to train the model and the last 48 months (4 years) are withheld as testing data, indicated in green.
With an initial- ization from the empirical spectrum and 2500 RJ-MCMC steps, the model is able to automat- ically learn the necessary frequencies and the shape of the spectral density to capture both the rising trend and the seasonal waveform, allow- ing for accurate long-range extrapolations with- out pre-specifying the number of model compo- nents in advance.
This experiment also demonstrates the impact of accounting for uncertainty in the kernel, as the withheld data often appears near or crosses the upper bound of the 95% predictive bands of the SM ﬁt, whereas our model yields wider and more conservative predictive bands that wholly capture the test data.
As the SM extrapolations are highly sensitive to the choice of parameter values, ﬁxing the parameters of the kernel will yield overconﬁdent predictions.
The L´evy process prior allows us to account for a range of possible kernel parameters so we can achieve a more realistically broad coverage of possible extrapolations.
Note that the L´evy process over spectral densities induces a prior over kernel functions.
Figure 6 shows a side-by-side comparison of covariance function draws from the prior and posterior distribu- tions over kernels.
We see that sample covariance functions from the prior vary quite signiﬁcantly, but are concentrated in the posterior, with movement towards the empirical covariance function.
Figure 5: Learning of Airline passenger data.
Training data is scatter plotted, with withheld test- ing data shown in green.
The learned posterior distribution with the proposed approach (mean in black, with 95% credible set in grey) captures the periodicity and the rising trend in the test data.
The analogous 95% interval using a GP with a SM kernel is illustrated in magenta.
00.20.4Frequency100200300400Power01020304050X-10-505f(X)Figure 6: Covariance function draws from the kernel prior (left) and posterior (right) distributions, with the empirical covariance function shown in black.
After RJ-MCMC, the covariance distribution centers upon the correct frequencies and order of magnitude.
Figure 7: Learning of a natural sound tex- ture.
A close-up of the training interval is displayed with the true waveform data scat- ter plotted.
The learned posterior distribu- tion (mean in black, with 95% credible set in grey) retains the periodicity of the signal within the corrupted interval.
Three samples are drawn from the posterior distribution.
5.4 Scalability Demonstration A ﬂexible and fully Bayesian approach to kernel learning can come with some additional computa- tional overhead.
Here we demonstrate the scalability that is achieved through the integration of SKI (Wil- son & Nickisch, 2015) with our L´evy process model.
We consider a 100,000 data point waveform, taken from the ﬁeld of natural sound modelling (Turner, 2010).
A L´evy kernel process is trained on a sound texture sample of howling wind with the middle 10% removed.
Training involved initialization from the signal empirical covariance and 500 RJ-MCMC samples, and took less than one hour using an In- tel i7 3.4 GHz CPU and 8 GB of memory.
Four distinct mixture components in the model were au- tomatically identiﬁed through the RJ-MCMC proce- dure.
The learned kernel is then used for GP inﬁlling with 900 training points, taken by down-sampling the training data, which is then applied to the origi- nal 44,100 Hz natural sound ﬁle for inﬁlling.
The GP posterior distribution over the region of interest is shown in Figure 7, along with sample realizations, which appear to capture the qualitative behavior of the waveform.
This experiment demonstrates the applicability of our proposed kernel learning method to large datasets, and shows promise for extensions to higher dimensional data.
6 Discussion We introduced a distribution over covariance kernel functions that is well suited for modelling quasi- periodic data.
We have shown how to place a L´evy process prior over the spectral density of a sta- tionary kernel.
The resulting hierarchical model allows the incorporation of kernel uncertainty into the predictive distribution.
Through the spectral regularization properties of L´evy process priors, we found that our trans-dimensional sampling procedure is suitable for automatically performing infer- ence over model order, and is robust over initialization strategies.
Finally, we incorporated structured kernel interpolation into our training and inference procedures for linear time scalability, enabling experiments on large datasets.
The key advances over conventional spectral mixture kernels are in being able to interpretably and automatically discover the number of mixture components, and in representing uncertainty over the kernel.
Here, we considered one dimensional inputs and station- ary processes to most clearly elucidate the key properties of L´evy kernel processes.
However, one could generalize this process to multidimensional non-stationary kernel learning by jointly infer- ring properties of transformations over inputs alongside the kernel hyperparameters.
Alternatively, one could consider neural networks as basis functions in the L´evy process, inferring distributions over the parameters of the network and the numbers of basis functions as a step towards automating neural network architecture construction.
0.0080.0090.010.0110.0120.0130.0140.015X (Seconds)-0.4-0.200.20.4f(X)Acknowledgements.
This work is supported in part by the Natural Sciences and Engineering Re- search Council of Canada (PGS-D 502888) and the National Science Foundation DGE 1144153 and IIS-1563887 awards.
References Bochner, S.
Lectures on Fourier Integrals.(AM-42), volume 42.
Princeton University Press, 1959.
Clyde, Merlise A and Wolpert, Robert L.
Nonparametric function estimation using overcomplete dictionaries.
Bayesian Statistics, 8:91–114, 2007.
Donoho, D.
and Johnstone, J.M. Ideal spatial adaptation by wavelet shrinkage.
Biometrika, 81(3): 425–455, 1993.
Green, P.J. Reversible jump monte carlo computation and bayesian model determination.
Biometrika, 89(4):711–732, 1995.
Green, P.J. Trans-dimensional Markov chain Monte Carlo, chapter 6.
Oxford University Press, 2003.
Hyndman, R.J. Time series data library.
2005.
http://www-personal.buseco.monash.
edu.au/˜hyndman/TSDL/.
MacKay, David J.C. Introduction to Gaussian processes.
In Bishop, Christopher M.
(ed.), Neural Networks and Machine Learning, chapter 11, pp.
133–165.
Springer-Verlag, 1998.
Neal, R.M. Bayesian Learning for Neural Networks.
Springer Verlag, 1996.
ISBN 0387947248.
Rasmussen, C.
E.
and Williams, C.
K.
I.
Gaussian processes for Machine Learning.
The MIT Press, 2006.
Turner, R.
Statistical models for natural sounds.
PhD thesis, University College London, 2010.
Wilson, Andrew Gordon.
Covariance kernels for fast automatic pattern discovery and extrapolation with Gaussian processes.
PhD thesis, University of Cambridge, 2014.
Wilson, Andrew Gordon and Adams, Ryan Prescott.
Gaussian process kernels for pattern discovery and extrapolation.
International Conference on Machine Learning (ICML), 2013.
Wilson, Andrew Gordon and Nickisch, Hannes.
Kernel interpolation for scalable structured Gaus- sian processes (KISS-GP).
International Conference on Machine Learning (ICML), 2015.
Wolpert, R.L., Clyde, M.A., and Tu, C.
Stochastic expansions using continuous dictionaries: L´evy adaptive regression kernels.
The Annals of Statistics, 39(4):1916–1962, 2011.
10 7 Supplementary Materials 7.1 Sampling Levy Process Priors The following formulas in this section are taken from Wolpert et al.
(2011) for reference.
Suppose the hyperparameters θ of the prior distributions for J, β, ω, are drawn from a hyperprior distribution, πθ(dθ).
Then in order to sample the L´evy prior, the follow steps are taken: θ ∼ πθ(dθ) J|θ ∼ Po(ν+ ε ), ε ≡ νε(R × Ω) ν+ {(βj, ωj)}J j=1|J, θ i.i.d.∼ πβ(βj)dβjπω(dωj) ε and πβ are determined by the speciﬁc choice of L´evy process and are given The formulas for ν+ below.
For computational purposes, the βj’s are truncated at |βjη| > ε for a Poisson approximation to the true L´evy process, and E|L[φ] − Lε[φ]|2 represents the L2 error of the approximation for a given basis function φ.
Below, E1(z) =(cid:82) ∞ t−1e−tdt.
7.1.1 Gamma Process J ∼ Po(ν+ ε ), ε = γ|Ω|E1(ε) ν+ i.i.d.∼ πβ(βj)dβj, βj πβ(βj) = β−1 j e−βj η E1(ε) 1{βjη>ε} E|L[φ] − Lε[φ]|2 = γη−2(cid:107)φ(cid:107)2 2[1 − (1 + ε)e−ε] 7.1.2 Symmetric Gamma Process J ∼ Po(ν+ ε ), ε = 2γ|Ω|E1(ε) ν+ i.i.d.∼ πβ(βj)dβj, βj πβ(βj) = |βj|−1e−|βj|η 2E1(ε) 1{|βjη|>ε} E|L[φ] − Lε[φ]|2 = 2γη−2(cid:107)φ(cid:107)2 E|L[φ] − Lε[φ]|2 = 2γη−2(cid:107)φ(cid:107)2 11 7.1.3 Symmetric α-Stable Process J ∼ Po(ν+ ε ), ε = γ|Ω| 2 ν+ i.i.d.∼ πβ(βj)dβj, βj πβ(βj) = 2[1 − (1 + ε)e−ε] (cid:17) (cid:16) πα Γ(α)sin ε−α αεα 2ηα |βj|−α−11{|βjη|>ε} (cid:21) (cid:20) Γ(α + 1) (cid:16) πα (cid:17) ε2−α π(2 − α) sin 7.2 Sparsity Inducing Properties of L´evy Priors Figure 8 illustrates the contours of the joint distribution for two independent draws of β under different priors πβ(dβ).
The contours for the gamma process would be taken from the upper-right quadrant of those for the symmetric gamma process.
Gaussian and Laplace priors on β result in (cid:96)2 and (cid:96)1 regularization respectively.
The L´evy processes in contrast yield inward curving contours, leading to a sparsity inducing effect similar to (cid:96)p regular- ization with p < 1.
Intuitively, this discourages simultaneous large values of β more strongly than (cid:96)1 regularization unless the added basis functions signiﬁcantly improve the ﬁt.
Figure 8: Contour plot of the joint probability density function of two β draws under different priors.
Initialization and Hyperparameter Tuning 7.3 Initialization and hyperparameter tuning can be automated by ﬁtting the empirical spectrum of the data.
It is done in the following steps: 1.
If needed, de-mean the training data by subtracting a deterministic mean function such as the sample mean or best ﬁt line.
Doing so will eliminate large peaks at the origin which dominate the rest of the spectrum.
The de-meaned training data {yj}n j=1 will be the input for RJ-MCMC.
2.
Compute the empirical spectral density Semp(s) = 2 In MATLAB, this is calculated as the ﬁrst (cid:98) n 2(cid:99) entries from 2*abs(fft(y)).ˆ2/n; 3.
Sample the empirical spectral density and ﬁt a Gaussian mixture with J0 components to the sampled data.
A good initial guess for J0 can be done by examining the number of peaks in the empirical spectrum.
, s ∈ [0, 0.5].
j=1 yje−2πis(j−1)(cid:12)(cid:12)(cid:12)2 (cid:12)(cid:12)(cid:12)(cid:80)n J0(cid:88) j=1 1(cid:113) 2πσ2 − (s−χj )2 2σ2 SGaussian(s) = αj 4.
Keep the frequencies χj from the Gaussian ﬁt, and using least squares, ﬁt a Laplacian basis function to each individual Gaussian component.
For each j, one could minimize the 12 following objective over a sample grid of points sk in [−3σj, 3σj] βj (cid:88) sk min λj ,βj e−λj|sk| − αj λj 2 1(cid:113) 2πσ2 − s2 2σ2 5.
Form the initial spectrum with the ﬁtted parameters J0(cid:88) j=1 Sinitial(s) = βj λj e−λj|s−χj| The initial spectrum ﬁt for the airline data is shown in Figure 9.
6.
Tune the hyperparameters: likelihood on the λ parameters of the initial spectrum.
• λ is modelled with prior Gamma(aλ, bλ), so aλ and bλ can be estimated by maximum • η−1 ∼ Gamma(aη, bη) controls the expected value of coefﬁcients βj.
For basis func- tions which integrate to 1, the sum of βj’s is equal to the total area underneath the spectrum, which by Parseval’s identity represents total variance of the data.
Hence the sample variance of the training data can be used as an upper bound on coefﬁcient values, and aη and bη can be set accordingly.
• γ ∼ Gamma(aγ, bγ) is proportional to the expected number of basis functions as shown in Section 7.1 and controls the sparsity of the expansions.
aγ and bγ can be set to cover a range of values which encourage sparsity.
• For the symmetric α-stable process, 0 < α < 2 controls the heaviness of the tails in the distribution for βj with smaller values of α yielding heavier tails.
α can be set by maximum likelihood on the initial βj’s.
• ε can be set based on L2 truncation errors as described in Section 7.1. Figure 9: Initial Spectrum Fit 13
In this work, we consider a seller oﬀering N products, where N is very large, and the pricing of certain products may inﬂuence the demand for others in unknown ways.
We let pt P RN denote the vector of selected prices at which each product is sold during time period t P t1, .
.
.
, Tu, which results in total demands for the products over this period represented in the vector qt P RN .
Note that qt represents a (noisy) evaluation of the aggregate demand curve at the chosen prices pt, but we never observe the counterfactual demand that would have resulted had we selected a diﬀerent price-point.
This is referred to as bandit feedback in the online optimization literature [1].
Our goal is ﬁnd a setting of the prices for each time period to maximize the total revenue of the seller (over all rounds).
This is equivalent to minimizing the negative revenue over time: Rtpptq where Rtpptq “ ´xqt, pty (1) Rpp1, .
.
.
, pTq “ Tÿ t“1 ∗Work done in part while the author was at Microsoft Research We can alternatively maximize total proﬁts instead of revenue by simply redeﬁning pt as the diﬀerence between the product-prices and the cost of each product-unit.
In practice, the seller can only consider prices within some constraint set S Ă RN , which we assume is convex throughout.
To ﬁnd the optimal prices, we introduce the following linear model of the aggregate demands, which is allowed to change over time in a nonstationary fashion: qt “ ct ´ Btpt ` t (2) Here, ct P RN denotes the baseline demand for each product in round t.
Bt P RNˆN is an asymmetric matrix of demand elasticities which represents how changing the price of one product may aﬀect the demand of not only this product, but also demand for other products as well.
By conventional economic wisdom, Bt will have the largest entries along its diagonal because demand for a product is primarily driven by its price rather than the price of other possibly unrelated products.
Since a price increase usually leads to falling demand, it is reasonable to assume all Bt ľ 0 are positive semi-deﬁnite (but not necessarily Hermitian), which implies that at each round: Rt is a convex function of pt.
The observed aggregate demands over each time period are additionally subject to random ﬂuctuations driven by the noise term t P RN , and we use  to represent the full set of random eﬀects t1, .
.
.
, Tu. Throughout, we suppose the noise in each round t is sampled i.i.d. from some mean-zero distribution with ﬁnite variance.
A wealth of past work on dynamic pricing has posited similar demand models, although most existing research has not considered settings where the underlying model is changing over time [2, 3, 4, 5, 6].
Unlike standard statistical (or predictive) approaches to this pricing problem which rely on stationary formulations, we allow ct, Bt to change in each round and they may even be adversarially chosen.
This consideration is particularly important in dynamic markets where the seller faces new competitors and consumers with ever-changing preferences who are actively seeking out the cheapest prices for products [7].
Our goal is to select prices p1, .
.
.
, pT which minimize the expected regret ErRpp1, .
.
.
, pTq ´ Rpp˚, .
.
.
, p˚qs com- pared to always selecting the single best conﬁguration of the prices p˚ “ argmin Rtppq pPS chosen in hindsight after the functions Rt have all been revealed.
Tÿ t“1 Low regret algorithms ensure that in the case of a stationary underlying model, our chosen prices quickly converge to the optimal choice, and in nonstationary settings, our pricing procedure will naturally adapt to the intrinsic diﬃculty of the dynamic revenue- optimization problem without being overly conservative [8].
While low (ie.
opTq) regret regret of existing methods is bounded below by Ωp?
is achievable using algorithms for online convex optimization with bandit feedback, the Nq, which is undesirable large when one is dealing with a vast number of products [1, 8, 9].
To attain better bounds, we adopt a low-rank structural assumption that the variation in demands changes over time only due to d ! N underlying factors.
Under this setting, we develop algorithms whose regret depends only on d rather than N by combining existing bandit methods with low- dimensional projections selected via online singular value decomposition.
The primary contributions of this work include: • A nonstationary formulation of dynamic pricing as an online convex optimization with bandit feedback and side-information in the observed demands • A low-rank model of high-dimensional demands based on (latent) product features • Eﬃcient algorithms that enable low-regret dynamic pricing in high-dimensional, nonstationary (adversarial) settings, while simultaneously revealing product features As far as we are aware, our main result (Theorem 4) is the ﬁrst online bandit optimization algorithm whose regret does not scale with the ambient dimensionality of the action space.
2.
Related Work While numerous bandit optimization techniques have been successfully applied to dynamic pricing problems, research in this area has been primarily restricted to the stationary setting [2, 4, 10, 11, 12, 13].
Most similar to our work, Javanmard [6] recently developed a bandit pricing-method where they also assume demand depends linearly on prices and product-speciﬁc features.
Like most of the literature in this area, their work does not consider the more realistic setting where the demands for a product may depend (in a time-varying fashion) on the price and features of the other products.
As previously mentioned, the use of standard bandit algorithms in such settings will scale poorly for sellers who wish to price a vast number of related products [1].
High-dimensional dynamic pricing was recently considered in [5], who employ a sparse maximum likelihood framework that presumes stationarity of the underlying model and is thus far less robust in adversarial environments in comparison to online optimization algorithms, which come equipped with strong performance guarantees.
More generally, existing algorithms that combine bandits with low-dimensional sub- space estimation [14, 15] are only designed for stationary settings rather than the full online optimization problem (where the underlying reward function is allowed to vary over time).
While the ﬁeld of online bandit optimization has seen many advances since the pioneering work of Flaxman et al.
[9], none of the recent improvements achieves regret that is independent of the dimensionality of the action-space [16, 17].
To our knowledge, Hazan et al.
[18] is the only existing work to present online convex optimization algo- rithms whose regret depends on an intrinsic low rank structure rather than the ambient dimension.
However, their approach is not suited for dynamic pricing since it is restricted to settings with: full-information (rather than bandit feedback), linear and noise-free (or stationary) reward functions, and actions that are specially constrained within the probability-simplex.
3.
Low-rank Model We now introduce a model in which both ct and Bt in (2) display only low-rank changes over time.
In practice, each product i may be described by some vector of features ui P Rd (where presumably d ! N ), which can be used to determine the similarity between products as well as their baseline demands.
Traditionally, a natural method to gauge the underlying similarity between products i and j is via their inner product xui, ujyV “ i Vuj under some linear transformation of the feature-space given by V ľ 0.
For uT example, ui might be a binary vector indicating that product i falls into certain product- categories (where the number of categories d is far less than the number of products N ), and V might be a diagonal matrix specifying the cross-elasticity of demand within each i Vuj ¨ pj would thus be the marginal eﬀect on the product category.
In this example, uT demand for product i that results from selecting pj as the price for product j.
By introducing time-varying metric transformations Vt, our model allows these product-similarities to evolve over time.
Given the features ui that represent each prod- uct, we assume the following demand model, in which the variation over time naturally exhibits low-rank structure: qt “ Uzt ´ UVtUT pt ` t (3) Here, the rows of U P RNˆd contain the featurization of each of our N products, the t represent random noise in the observed demands, zt P Rd explain the variation in baseline- demand over time, and the (asymmetric) matrices Vt P Rdˆd specify latent changes in the demand-price relationship over time.
Under this model, the aggregate demand for product i at time t is governed by the prices of all products, weighted by their current feature-similarity to product i.
To ensure our revenue-optimization remains convex, we restrict the adversary to choices that satisfy Vt ľ 0 for all t.
Note that while the structural variation in our model is assumed to be low-rank, the noise in the observed demands may be fundamentally N -dimensional.
In each round, pt and qt are the only quantities observed, while t, zt, Vt all remain unknown (and we consider both cases where the product features U are known or unknown).
4.
Methods Our basic dynamic pricing strategy is to employ the gradient-descent without a gradient (GDG) online bandit optimization technique of [9].
While a naive application of this algorithm produces regret dependent on the number of products N , we ensure the updates of this method are only applied in the d-dimensional subspace spanned by U, which leads to regret bounds that depend only on d rather than N .
When U is unknown, this subspace is simultaneously estimated online, in a somewhat similar fashion to the approach of [18] for online learning with low-rank experts.
If we deﬁne x “ UT p P Rd, then under the low-rank model in (3) with Erts “ 0, the expected value of our revenue-objective in round t can be expressed as: ErRtppqs “ pT UVtUT p ´ pT Uzt “ xT Vtx ´ xT zt :“ ftpxq (4) Thus, the intrinsic dimensionality of the problem is only d, and we can maximize expected revenues by merely considering a restricted set of d-dimensional actions x and functions ft over the projected constraint set: (5) (cid:32) UTpSq “ x P Rd : x “ UT p for some p P S 5.
Dynamic Pricing with Known Product Features In certain markets, it is clear how to featurize products [4].
Under the low-rank model in (3) when U is given, we can apply the OPO-K method (Algorithm 1) below to select prices.
This algorithm employs subroutines FindPrice, ProjectToFeasible which both solve convex optimization problems in order to compute certain projections.
Here, we use Unifptx P Rd : ||x||2 “ 1uq to denote the uniform distribution over the unit Euclidean sphere in Rd. Intuitively, our algorithm adapts the GDG approach of [9] to select low-dimensional actions xt P Rd at each time point, and then seeks out a feasible price vector pt correspond- ing to the chosen xt.
Note that when d ! N , there are potentially many price-vectors p P RN that map to the same low-dimensional vector x P Rd via UT , and out of these, we select the one that is closest to our previously-chosen prices (via FindPrice), ensuring additional stability in our dynamic pricing procedure.
In practice, the initial prices p0 should be selected based on external knowledge or historical demand data.
Algorithm 1 Online Pricing Optimization with Known Product Features (OPO-K) Inputs: η, δ, α ą 0, product feature matrix U P RNˆd, and initial prices p0 P S Output: Sequence of prices p1, .
.
.
, pT which seek to maximize total revenue 1: Set prices to p0 P S and observe (negative) revenue R0pp0q and demands q0pp0q 2: Deﬁne x1 “ UT p0 3: for t “ 1, .
.
.
, T : Draw ξt „ Unifptx P Rd : ||x||2 “ 1uq and setrxt :“ xt ` δξt Set round t prices: pt “ FindPrice(rxt, U,S, pt´1), and observe Rtpptq, qtpptq xt`1 “ ProjectToFeasible(xt ´ ηRtpptqξt, α, U, S) 4: 5: 6: Algorithm 2 FindPrice(x; U,S, pt´1) Inputs: x P Rd (vector), U P RNˆd (matrix), S Ă RN (convex set) Output: Closest price to pt´1 which is within S and maps to x via UT 1: Return argmin pPRN ||p ´ pt´1||2 subject to UT p “ x, p P S Algorithm 3 ProjectToFeasible(x, α, U, S) Inputs: x P Rd (vector), α ą 0 (scalar), U P RNˆd (matrix), S Ă RN (convex set) Output: Projection of x onto set tp1 ´ αqUT p : p P Su Ă Rd ˇˇˇˇp1 ´ αqUT p ´ x 1: pp “ argmin 2: Return p1 ´ αqUTpp pPS ˇˇˇˇ2 Under mild conditions, Theorem 1 below states that the OPO-K algorithm incurs OpT 3{4 dq regret when product features are a priori known.
This result is derived from Lemma 1 which shows that Step 6 of our algorithm corresponds to online projected gradient descent (in expectation) on a smoothed version of our objective deﬁned as: pftpxq “ Eζ ftpx ` ζq Rtppqξ BpftBx “ d ¨ E,ξ where ft is the alternative objective function (which is equivalent to Rt) given in (4), and ζ is sampled uniformly from within the unit sphere in Rd. Lemma 1.
For ξ „ Unifptx P Rk : ||x||2 “ 1uq and p P RN with UT p “ x ` δξ P Rd: (6) (7) Proof.
Since ErRtppqs “ ftpx` δξq, this result directly follows from Lemma 1 in [9].
To bound the regret of our pricing algorithm, we adopt the following assumptions: (A1) UTpSq contains a ball of radius rmin and is contained within a ball of radius (the constraint set is bounded and well-scaled) rmax ě rmin 3drmax 2rmin (the number of pricing rounds is suﬃciently large) (A2) T ą (A3) ´B ď ErRtppqs ď 0 for all p P S for t “ 1, .
.
.
, T (revenues are bounded) (A4) ftpxq is L-Lipschitz over x P UTpSq for t “ 1, .
.
.
, T (smooth revenue-functions) Theorem 1.
If conditions (A1)-(A4) are met and we choose η “ rmax α “ δ , then for any p P S, there exists a universal constant C such that: , δ “ T ´1{4 rmin Bdrmaxrmin 3pLrmin`Bq, Tÿ t“1 Rtpptq ´ Tÿ t“1 Rtppq E,ξ ď C ¨ T 3{4 Brmax L ` B rmin (8) for the prices p1, .
.
.
, pT selected by the OPO-K algorithm.
Proof.
´B ď Rtppq ď 0 for all p P S implies the range of ft is constrained to r´B, 0s over x P UTpSq. Recall that each ft is a convex function of x (as we required each Vt ľ 0) and for any p P S, we can deﬁne x “ UT p P UTpSq such that: ErRtppqs “ ftpxq.
Since convexity of S implies UTpSq is also convex, the proof of our result immediately follows from Theorem 2 below.
Finally, we note that since both S and UTpSq are convex, our choice of η, δ, α ensuresrxt P UTpSq and hence pt P S for all t.
Theorem 2 (Flaxman et al.
[9]).
Suppose each ft P r´B, Bs is a convex, L-Lipschitz function of x P Rd, and the set of feasible actions U is convex, with Euclidean balls of radius rmax and rmin containing and contained-within U, respectively.
Then, the low- dimensional actions x1, .
.
.
, xT P Rd which correspond (in expectation) to the iterates of the online projected gradient descent algorithm applied to pft (as deﬁned in (6)) must satisfy: ftpxtq Tÿ t“1 ´ min xPU ftpxq ď 2T 3{4 3Brmax L ` B rmin (9) if we choose η, δ, α as in Theorem 1.
Rather than relying on implicit characteristics of the Rt, ft, we show that same OpT 3{4 dq regret bound holds for the OPO-K algorithm if we instead impose the following structural assumptions on our original linear low-rank model in (3): 4d2 for all t (A5) ||zt||2 ď b (A6) ||Vt||op ď b for all t (spectral norm ||¨||op is magnitude of largest singular value) (A7) T ą 9 (A8) U is an orthogonal matrix such that UT U “ Idˆd (A9) S “ tp P RN : ||p||2 ď ru (with r ě 1) Requiring that the columns of U form an orthonormal basis for Rd, condition (A8) can be easily enforced by ﬁrst orthonormalizing the given product features.
Note that this orthogonality condition does not really restrict the overall class of models speciﬁed in (3), and describes the case where the features used to encode each product are uncorrelated between products (ie.
a minimally-redundant encoding) and have been normalized across all products.
To further simplify our analysis, we also from now adopt (A9) presuming the constraint set of feasible product-prices is a centered Euclidean ball (implying our p vectors now represent appropriately shifted/scaled prices).
Corollary 1.
Under assumptions (A5)-(A9), if we choose η “ α “ δ r , then for any p P S, there exists a universal constant C such that: , δ “ T ´1{4 bp1`dq?
dr2p1`rq 9r`6 ď Cbrpr ` 1qT 3{4d1{2 (10) Tÿ t“1 E,ξ Rtppq Rtpptq ´ Tÿ t“1 for the prices p1, .
.
.
, pT selected by the OPO-K algorithm.
Proof.
We show that (A5)-(A9) imply the necessary conditions of Theorem 1 hold with rmax “ rmin “ r, B “ rbp1`rq, and L “ p2r`1qb.
Bounding and simplifying the Theorem 1 inequality that arises under these deﬁnitions then produces the desired result.
Lemma 2 implies (A1) holds with rmax “ rmin “ r.
Note that (A3) holds since: ftpxq “ xT Vtx ´ xT zt ď ||x||2 2||Vt||op ` ||zt||2||x||2 ď r2b ` rb Finally, we show that our structural assumptions imply the Lipshitz continuity of each ft, as required in (A4): ||∇xftpxq||2 “ ||pVT Lemma 2.
Under assumption (A9), for any orthogonal N ˆ d matrix U: UTpSq “ tx P Rd : ||x||2 ď ru and UUTppq P S for any p P S.
t ` Vtqx ´ zt||2 ď 2||Vt||op||x||2 ` ||zt||2 ď 2br ` b @x P UTpSq Proof.
Consider the orthogonal extension of U, a matrix W “ rU,rUs P RNˆN formed by appending N ´ d additional orthonormal columns to U that are also orthogonal to the columns of U.
For any p P RN , we have: since orthogonality implies U is an isometry because ||WT p||2 2 “ ||UT p||2 2 ` ||rUT p||2 ||UUT p||2 “ ||UT p||2 ď ||WT p||2 “ ||WWT p||2 “ ||p||2 since W is also an isometry due to the fact that WT “ W´1 as W is square and orthogonal Combined with (A9), this implies UUTppq P S and ||x||2 ď r for any x P UTpSq. Now ﬁx arbitrary x P Rd which satisﬁes ||x||2 ď r.
By orthogonality of U: ||Ux||2 “ ||x||2 ď r ùñ Ux P S, and UT Ux “ x ùñ x P UTpSq. 6.
Dynamic Pricing with Latent Product Features In many settings, it is not clear how to best represent products as feature-vectors.
Again adopting the low-rank demand model in (3), we now consider the case where U is unknown and must be estimated.
To improve the identiﬁability of the resulting model, we adopt (A8) throughout this section.
Orthogonality implies U is both an isometry as well as the right-inverse of UT .
Thus, given any low-dimensional action x P UTpSq, we can set the corresponding prices as p “ Ux such that UT p “ x.
Lemma 3 shows that this price selection-method is feasible and corresponds to changing Step 5 in the OPO-K algorithm rather than the previous price pt´1.
Because prices pt are multiplied by the noise term t within each revenue-function Rt, choosing minimum-norm prices can help reduce variance in the total revenue generated by our approach.
As U is unknown, we instead employ an to pt “ FindPrice(rxt, U,S, 0q, where the next price is regularized toward the origin estimate pU P RNˆd, which is always restricted to be an orthogonal matrix.
Lemma 3.
For any orthogonal matrix pU and any x P pUTpSq, deﬁne pp “ pUx P RN .
Under (A9), pp P S and pp “ FindPrice(x,pU,S, 0q.
Proof.
Given x P pUTpSq, there exists p P S with pUT p “ x.
Lemma 2 implies ||pp||2 ď ||p||2 and pp “ pUx “ pUpUT p P S when this set is a centered Euclidean ball.
Finally, we note that pUTpp “ x since pUTpU “ Idˆd, so rp is the minimum-norm vector in S which is mapped to x by pUT .
E,ξ (11) 6.1. Product Features with Known Span In Theorem 3, we consider a minor modiﬁcation to the OPO-K algorithm where price- previous price pt´1.
Even without knowing the true latent features, this result implies that the regret of our modiﬁed OPO-K algorithm may still be bounded independently of selection in Step 5 is done using pt “ pUrxt rather than being regularized toward the the number of products N , as long as pU accurately estimates the column span of U.
Theorem 3.
Suppose spanppUq “ spanpUq, ie.
our orthogonal estimate has the same with pU used in place of the underlying U and parameters η, δ, α chosen as in Corollary column-span as the underlying (rank d) latent product-feature matrix.
Let p˚ P S denote the optimal pricing and p1, .
.
.
, pT be the prices selected by our modiﬁed OPO-K algorithm 1.
Under conditions (A5)-(A9), there exists universal constant C such that: Tÿ t“1 Rtpp˚q Rtpptq ´ Tÿ Tÿ Rtppq, we letrp “ UUT p˚.
Note that E ď Cbrpr ` 1qT 3{4d1{2 t“1 t“1 Proof.
Deﬁning p˚ “ argmin pPS ”ř t“1 Rtprpq ”ř and rp P S by Lemma 2, so rp is an equivalently optimal setting of the product-prices (in expectation).
Since U and pU share the same column-span, there t“1 Rtpp˚q exist low-dimensional action rx P Rk such that rp “ pUrx.
By orthogonality of pU: pUTrp “ pUTpUrx “rx, sorx P pUTpSq is a feasible solution to our modiﬁed OPO-K algorithm.
For x P Rd and p “ pUx P RN , we can re-express the expected revenue at this price vector by introducing ft,pU as a function of x parameterized by pU, as similarly done in (4): Convexity of Rt in p implies ft,pU is convex in x for any pU.
Note that our modiﬁed OPO-K version of each ft,pU, deﬁned similarly as in (6).
Thus, by employing the same argument show that for any x P pUTpSq: Tÿ ft,pUpxq “ xTpUT UVtUTpUx ´ xTpUT Uzt “ ErRtppqs based on Theorem 2 and deﬁning B, L, rmax, rmin as in the proof of Corollary 1, we can algorithm is (in expectation) running online projected gradient descent on a smoothed (12) ´ ft,pUpxq ď Cbrpr ` 1qT 3{4d1{2 ft,pUpxtq Eξ t“1 where xt are the low-dimensional actions chosen in Step 4 of the modiﬁed OPO-K algo- 10 (13) rithm, such that pt “ pUxt for the prices output by this method.
To conclude the proof, Tÿ t“1 “ Tÿ t“1 ft,pUprxq and E Rtpp˚q we recall that for the chosen pt: Rtpptq Tÿ t“1 E,ξ ft,pUpxtq Tÿ t“1 “ Eξ 6.2. Product Features with Unknown Span and Noise-free Demands In practice, span(U) may be entirely unknown.
If we assume the adversary is restricted to strictly positive deﬁnite Vt ą 0 for all t and there is no statistical noise in the observed demands (ie.
qt “ Uzt ´ UVtUT pt in each round), then Lemma 4 below shows we can ensure span(U) is revealed within the ﬁrst d observed demand vectors by simply adding a minuscule random perturbation to all of our initial prices selected in the ﬁrst d rounds.
Thus, even without knowing the latent produce feature subspace, an absence of noise in the observed demands enables us to realize a low regret pricing strategy via the same OPO-K algorithm (applied after the ﬁrst d rounds).
Lemma 4.
Suppose that t “ 0 for each round and assume that each Vt ą 0.
If for t “ 1, .
.
.
, d: each pt is independently uniformly distributed within some (uncentered) Euclidean ball of strictly positive radius, then spanpq1, .
.
.
, qdq “ spanpUq almost surely.
Proof.
In Lemma 4, we suppose that each pt “rpt ` ζt, where each ζt is uniformly drawn from a centered Euclidean ball of nonzero radius in RN and zt, Vt,rpt are ﬁxed indepen- To show linear independence holds almost surely, we proceed inductively by proving dently of the randomness in ζt.
Note that each qj “ Usj where sj “ zj ´ VjUT pj P Rd. Thus, spanpq1, .
.
.
, qdq Ď spanpUq and the two spans must be equal if s1, .
.
.
, sd are lin- early independent.
Prpsj P spanps1, .
.
.
, sj´1qq “ 0 for any 1 ă j ď d.
We ﬁrst note that sj “ zj ´ VjUTrpj ´ s1 ` VjUTrpj ´ zj, .
.
.
, sj´1 ` VjUTrpj ´ zj, this subspace has measure zero under the VjUT ζj.
Since Vj ą 0 is invertible and U is orthogonal, VjUT ζj is uniformly distributed over a nondegenerate ellipsoid E Ă Rd with nonzero variance under any projection in Rd. Since this includes directions orthogonal to the j ´ 1 dimensional subspace spanned by uniform distribution over E (for j ď d).
6.3. Product Features with Unknown Span and Noisy Demands When the observed demands are noisy and spanpUq is unknown, we select prices using the OPO-L algorithm presented below.
The approach is similar to our previous OPO-K algorithm, except we now additionally maintain an estimate of the latent product features’ 11 span.
The estimator is updated in an online fashion via an averaged singular value decomposition (SVD) of the previously observed demands.
Algorithm 4 Online Pricing Optimization with Latent Product Features (OPO-L) Inputs: η, δ, α ą 0, initial prices p0 P S, and rank of demand-variation d P r1, Ns Output: Sequence of prices p1, .
.
.
, pT which seek to maximize total revenue 3: Set prices to p0 P S and observe (negative) revenue R0pp0q and demands q0pp0q 5: for t “ 1, .
.
.
, T : 1: Initialize pQ as N ˆ d matrix of zeros 2: Initialize pU as random N ˆ d matrix and orthonormalize all columns 4: Deﬁne x1 “ pUT p0 Draw ξt „ Unifptx P Rd : ||x||2 “ 1uq and setrxt :“ xt ` δξt Set round t prices: pt “ pUrxt and observe Rtpptq, qtpptq xt`1 “ ProjectToFeasible(xt ´ ηRtpptqξt, α, pU, S) 9: With j “ 1 ` rpt ´ 1q mod ds, k “ ﬂoorpt{dq, update: pQ˚,j Ð 1 Update columns of pU as the top d left singular vectors of pQ k qt ` k´1 pQ˚,j 6: 7: 8: 10: Step 9 in our OPO-L algorithm corresponds to online averaging of the currently ob- served demand vector qt with the historical observations stored in the jth column of matrix pQ.
After computing the singular value decomposition of pQ “ rUrSrVT , Step 10 is performed by setting pU equal to the ﬁrst d columns of rU (the indices corresponding to the largest singular values inrS).
Since pQ is only minorly changed within each round, the as singular vectors, the columns of pU remain orthonormal throughout the execution of update operation in Step 10 can be carried out with much greater computational eﬃciency by leveraging existing fast SVD-update procedures [19, 20].
Note that by their deﬁnition our algorithm.
To quantify the regret incurred by our algorithm, we assume the entries of the noise vector t are i.i.d. samples from a sub-Gaussian distribution for each t “ 1, .
.
.
, T .
Re- call that random variable X follows the sub-Gaussian(σ2) distribution if ErXs “ 0 and Prp|X| ą xq ď 2 expp´ x2 2σ2q for all x ą 0.
Our assumption of sub-Gaussian noise is quite general, covering common settings where the noise is either Gaussian, bounded, of strictly log-concave density, or any ﬁnite mixture of sub-Gaussian variables [21].
Furthermore, our regret bound remains valid even if the demands for diﬀerent products are indepen- dent but heteroscedastic, in which case we simply assume the sub-Gaussian parameter σ2 speciﬁes the maximal variation in the observed demand for any of the products.
Intuitively, the averaging in step 9 of our OPO-L algorithm ensures statistical con- centration of the noise in our observed demands, such that the true column span of the underlying U may be better revealed.
More concretely, if we let st “ zt ´ VtUT pt and 12 t ` t, where q˚ t “ Ust, then the observed demands can be written as: qt “ q˚ q˚ t are the (unobserved) expected demands at our chosen prices.
At round T (assuming T is divisible T{dÿ by d), the jth column of pQ is given by: T{dÿ j`dpi´1q where sQ˚ ˚,j “ d sj`dpi´1q i“1 (14) pQ˚,j “ sQ˚ ˚,j ` d i“1 T{d i“1 j`dpi´1q exhibits rapid concentration of measure, results from Because the average d random matrix theory imply that the span-estimator obtained from the ﬁrst d singular vectors of pQ in Step 10 of our OPO-L algorithm will rapidly converge to the column span of sQ˚ P RNˆd (the average-matrix of underlying expected demands whose jth column is deﬁned above).
This is useful since sQ˚ shares the same span as the underlying U.
Theorem 4 below shows that our OPO-L algorithm achieves low-regret in the setting of unknown product features, and the regret again depends only on the intrinsic rank d (rather than the number of products N ).
The required sub-Gaussianpσ2{Nq noise-level may be ensured via either rescaling the observed demands, or by extending the time- duration of each round t to allow for a suﬃcient number of (potential) customers, which will ensure that the random eﬀects in each individual’s behavior rapidly concentrate in the aggregate observed demand.
Note that the regret in Theorem 4 depends on the constant Q whose value is determined by the noise-level σ and the extreme singular values of sQ˚ deﬁned in (14).
In general, these quantities thus measure just how adversarial of an environment the seller is faced with.
For example, when the underlying low-rank variation is of much smaller magnitude than the noise in our observations, it will be diﬃcult to accurately estimate the span of the latent product features.
In control theory, a signal- to-noise expression similar to Q has also been recently proposed to quantify the intrinsic diﬃculty of system identiﬁcation for the linear quadratic regulator [22].
Theorem 4.
Suppose the unknown U is orthogonal and rank d.
Let p˚ “ argmin pPS denote the optimal product pricing and p1, .
.
.
, pT be the prices selected by the OPO-L al- iid„ sub-Gaussianpσ2{Nq gorithm with parameters η, δ, α set as in Corollary 1.
If all t and(A5)-(A9) hold, then there exists universal constant C such that: Tÿ t“1 Rtppq Tÿ t“1 Rtpptq ´ Tÿ ¯) t“1 E,ξ 13 Rtpp˚q ď CQrbp4r ` 1qdT 3{4 where Q “ max nonzero singular values of the underlying rank d matrix sQ˚ deﬁned in (14).
2σ1`1 σ2 with σ1 (and σd) deﬁned as the largest (and smallest) 1, σ2 E Tÿ t“1 Rtppq product features obtained in Step 10 of our OPO-L algorithm at round t.
Note that the Proof.
For notational convenience, suppose that T is divisible by d, T 3{4 ě d ě 3, and the noise-variation parameter σ ě 1 throughout our proof.
Recall from the proof of Theorem 3 that under our low-rank demand model, we can redeﬁne p˚ Ð UUT p˚ P S and still ensure p˚ “ argmin .
Thus, we suppose without loss of generality that the pPS optimal prices can be expressed as p˚ “ Ux˚ for some corresponding low-dimensional action x˚ P UTpSq. For additional clarity, we use pUt to denote the current Nˆd estimate of the underlying pUt are random variables which are determined by both the noise in the observed demands and the randomness employed within our pricing algorithm.
Letting pt “ pUtxt denote the prices chosen by the OPO-L algorithm in each round (and xt P pUT Tÿ Rtpptq ´ Rtpp˚q T 3{4ÿ ft,pUtpxtq ´ ft,pUtprxq ft,pUtpxtq ´ ft,Upx˚q where ft,U is deﬁned as in (12) and we letrx “ argmin The proof of Corollary 1 ensures both |ft,U| and |ft,pUt| (for any orthogonal pUt) are bounded ft,pUtprxq ´ ft,Upx˚q ft,pUtpxq by rbp1 ` rq over all x P UTpSq, so we can trivially bound the ﬁrst summand in (15): t pSq the corresponding low-dimensional actions), we have: ` E Tÿ Tÿ Tÿ t“1 t“1 xPUTpSq ` E t“T 3{4 t“T 3{4 (15) t“T 3{4 T 3{4ÿ ft,pUtpxtq ´ ft,Upx˚q ď rbp1 ` rq ¨ T 3{4 t“1 To bound the second summand in (15), we ﬁrst point out that UTpSq “ pUT 2 (since all pUt are restricted to be orthogonal).
Thus, Algorithm 4 is essentially running the classic gradient-free bandit method of [9] to optimize the functions ft,pUt over the low- dimensional action-space UTpSq, and the second term is exactly the regret of this method stated in Corollary 1: t pSq by Lemma Tÿ ft,pUtpxtq ´ ft,pUtprxq t“T 3{4 ‰3{4 ď Cbrpr ` 1q T ´ T 3{4 d1{2 Finally, we complete the proof by bounding the third summand in (15).
Deﬁning 14 ı inf OPO t“T 3{4 t“T 3{4 ď inf OPO ďpT ´ T 3{4q ¨ E ft,pUtprxq ´ ft,Upx˚q O Ă Rdˆd as the set of orthogonal d ˆ d matrices, we have: ft,pUtpOx˚q ´ ft,Upx˚q where now choose pO P O as the orthogonal matrix such that E||pUt bound of Lemma 5 for the t ě T 3{4 ﬁxed above.
Deﬁning ∆ “ Ux˚ ´ pUt plug in the deﬁnition of ft,pU from (12) and simplify to obtain the following bound: Tÿ ft,pUtpOx˚q ´ ft,Upx˚q where we’ve ﬁxed t “ argmax ft,pUtppOx˚q ´ ft,Upx˚q t1PrT 3{4,Ts Tÿ since x˚ P UTpSq ùñ Ox˚ P UTpSq by Lemma 2, andrx is an argmin over UTpSq OPO pT ´ T 3{4q ¨ E ď inf ft1,pUt1pOx˚q ´ ft1,Upx˚q pO ´ U||F satisﬁes the pOx˚ P Rd, we pO||opq||x˚||2 ď 2||x˚||2 by orthogonality of pO,pUt, U pO ´ U||F ft,pUtppOx˚q ´ ft,Upx˚q ďE ďE ďp4||x˚||2Vt||op ` ||zt||2q ¨ Er||∆||2s 2||U||op||Vt||op||UT||op ` 2||∆||2||x˚||2||Vt||op||UT||op ` ||∆||2||zt||2||U||op 2||Vt||op ` 2||∆||2||x˚||2||Vt||op ` ||∆||2||zt||2 since ||∆||2 ď p||U||op ` ||pUt ||pUt ďCrbp4r ` 1q dσ2 since Er||∆||2s ď ||x˚||2 ¨ E by Lemma 5 (recall that we ﬁxed t ě T 3{4).
||∆||2 ||∆||2 ‰´1{2 ‰´1{2 under (A5)-(A6) ď C T 3{4 2σ1 ` 1 2σ1 ` 1 σ2 T 3{4 dσ2 ||x˚||2 σ2 Crb p1 ` rqT 3{4 ` p1 ` rqd1{2 Combining our bounds for each of the three summands in (15) yields the following upper bound, from which the inequality presented in Theorem 4 can be derived: ˘ Lemma 5.
For the pU produced in Step 10 of the OPO-L algorithm after T rounds and any feasible low-dimensional action x P pUTpSq, there exists orthogonal d ˆ d matrix pO ˘3{4 ` p4r ` 1qdσ2 T 5{8 ´ T 3{8 T ´ T 3{4 2σ1 ` 1 ˙` σ2 and universal constant C such that: 15 ” ||pUpO ´ U||F ď CT ´1{2dσ2 2σ1 ` 1 σ2 sQ˚ deﬁned in (14).
where σ1 and σd denote the largest and smallest singular values of the underlying matrix Proof.
Our proof relies on standard random matrix concentration results and a general- ization of the Davis-Kahan-Wedin theory [23] presented below.
Theorem 5 (Yu et al., 2015 [23]).
Let σ1 ą ¨¨¨ ą σd ą 0 denote the nonzero singular values of rank d matrix Q P RNˆd.
The left singular vectors of Q are represented as columns in matrix U P RNˆd (such that Q has SVD: UΣVT ) and pU P RNˆd similarly denotes the left singular vectors of some other N ˆ d matrix pQ.
For any Q,pQ P RNˆd, there exists orthogonal matrix pO P Rdˆd such that ||pUpO ´ U||F ď 2 2σ1 ` ||pQ ´ Q||op Lemma 6 (variant of Lemma 4.2 in [24]).
Let E be a N ˆ d matrix (with N ě d) of i.i.d. entries drawn from a sub-Gaussianpσ2{Nq distribution.
Then, with probability 1 ´ δ: ||pQ ´ Q||op 2d σ2 ||E||op ď 2σ logp12q ` 2 logp1{δq sE “ pQ´sQ˚ is the mean of T{d sub-Gaussianpσ2{Nq samples, which must be distributed Due to the averaging performed in Step 9 of our OPO-L algorithm, each value in as sub-Gaussian E||sE||op “ σ2d N T E||sE||2 exp x“0 .
Lemma 6 implies: ż 8 Prp||sE||op ą xq dx ¨˝´1 ˜c ż 8 ´a ż 8 x ¨ Prp||sE||op ą xq dx ¨˝´1 ˜c ż 8 1 ` erf x ¨ exp πd 2T 2σ x“0 2 log 12 ´ 2 x“0 “ 2σ op “ 2 ď 2 2σ x“0 ¯ı ˛‚ dx log 12 ď 4σ ´ 2 πd 2T ˛‚ dx log 12 16 a 2π log 12 ¨ erfp 2 log 12q ` 1 144 2π log 12 ` „a “ 8σ2d ď 24σ2d 6πd{T .
Combing Theorem 5 with these concentration bounds implies that there exists d ˆ d op are upper-bounded by 24σ2 2π log 12 When T ě d, σ ě 1, both E||sE||op and E||sE||2 orthogonal matrix pO such that: ||pQ ´sQ˚||op 2σ1 ` 1 ||pUpO ´ U||F ď 2 2d 2σ1E σ2 ď 96 3π dσ2 σ2 ||pQ ´sQ˚||2 op ı¯ ` E 6.4. Pricing against an Imprecise Adversary In Theorem 6 below, we illustrate a basic scenario under which an explicit bound for the constant Q in Theorem 4 can be obtained.
We now assume that the adversary can only coarsely control the underlying baseline demand parameters zt in (3).
More speciﬁcally, we suppose that in each round: zt “ z1 t (and Vt) may be adversarially selected and the γt are purely stochastic terms outside of the adversary’s control.
In this scenario, we presume a random dˆ d matrix Γ is drawn before the initial round such that: t ` γt, where only z1 (A10) Each entry Γi,j is independently sampled with mean zero and magnitude bounded almost surely by b{2 (ErΓi,js “ 0, |Γi,j| ď b{2 for all i, j).
Recall that the constant b ą 0 upper bounds the magnitude of each zt as speciﬁed in (A5).
Once the values of Γ have been sampled, we suppose that in round t: γt “ Γ˚,j is simply taken to be the jth column of this matrix with j “ 1 ` pt ´ 1q mod d (traversing the columns of Γ in order).
Throughout our discussion, the largest and smallest nonzero singular values of a rank-d matrix A will be denoted as σ1pAq and σdpAq, respectively.
Since boundedness of the values in Γ implies these entries follow a sub-Gaussianpb2{4q distribution, the following result applies: Lemma 7 (variant of Theorem 1.2 in [25]).
With probability at least 1 ´ Cb ´ cb d: σdpΓq ě {?
where Cb ą 0 and cb P p0, 1q are constants that depend (polynomially) only on b.
In selecting z1 t, Vt, we assume this imprecise adversary is additionally restricted to ensure: 17 d Cbdb ă 1 such that for all t: (A11) There exists s ă 1 ´ cb ||Γ˚,j||2 where constants cb, Cb are given by Lemma 7 (see [25] for details), and r ě 1 is still used to denote the radius of the set of feasible prices S.
Note that these additional assumptions do not conﬂict with condition (A5) required in Theorem 4, since (A10), (A11) together t ` γt.
With these assumptions in place, we now provide ensure that ||zt||2 ď b for zt “ z1 an explicit bound for the constant Q in Theorem 4.
t||2`r¨||Vt||op ď s¨ min 1ďjďd ||z1 Theorem 6.
Under this setting of an imprecise adversary where conditions (A10) and (A11) are met, for any τ P p 1 d, 1q, Theorem 4 holds with: 2Cbsbd ` cb Q ď 2σdCbp2b ` 1q dq ´ Cbsbd with probability ě 1 ´ τ (over the random sampling of Γ).
2pτ ´ cb c2 Proof.
Recall that σ1 (and σd) denote the largest (and smallest) nonzero singular values that σ1 ď c1 and σd ě c2 with high probability, which then implies the upper bound: Q ď maxt1, σ of the underlying rank d matrix sQ˚ deﬁned in (14).
For suitable constants c1, c2: we show p2c1 ` 1qu.
We ﬁrst note that the orthogonality of U implies sQ˚ “ UsS has the same nonzero singular values as the square matrixsS, whose jth column is given by: sS˚,j “ d σ1psQ˚q “ σ1psSq ď ?
T{dÿ AssS has d columns, we have: j`dpi´1q ` γj`pi´1qd ´ Vj`pi´1qdUT pj`pi´1qd z1 ||sQ˚,j||2 ď bp1 ` sq?
d ¨ max (16) i“1 where the latter inequality derives from the fact that (A9) and orthogonality of U imply: ||sS˚,j||2 ď d T{dÿ ||γj`pi´1qd||2 ` ||z1 i“1 j`pi´1qd||2 ` r||Vj`pi´1qd||op ď p1 ` sq ¨ ||Γ˚,j||2 ď b p1 ` sq by conditions (A10), (A11) Via similar reasoning, we also obtain the bound: σ1psS ´ Γq ď sb 18 (17) Subsequently, we invoke Lemma 7, which implies that with probability 1 ´ τ : σdpΓq ě τ ´ cb Cb (18) Combining (17) and (18), we obtain a high probability lower bound for σd via the additive Weyl inequality (cf.
Theorem 3.3.16 in [26]): σdpsQ˚q “ σdpsSq ě σdpΓq ´ σ1psS ´ Γq ě τ ´ cb ´ sb The proof is completed by deﬁning c1 “ bp1 ` sq?
, c2 “ τ ´ cb Cb simplifying the resulting bound via the fact that d ě 1 and s ă 1.
Cb with probability ě 1 ´ τ ´ sb , and then 7.
Results We evaluate the performance of our method in settings where noisy demands are generated according to equation (3), and the underlying structural parameters of the demand curves are randomly sampled from Gaussian distributions.
Throughout this section, we assume pt and qt represent rescaled prices and aggregate demands, such that the feasible set S can be simply ﬁxed as a centered sphere of radius r “ 20.
Furthermore, the noise in the (rescaled) demands for each product is always drawn from a ﬁxed Gaussian distribution: t „ Np0, 10q.
Our proposed algorithms are compared against the GDG method for online bandit optimization of [9], as well as a simple explore-then-exploit (Explore it ) technique.
In this latter method, we randomly sample pt during the ﬁrst T 3{4 rounds (uniformly over S) and then for all remaining rounds, pt is ﬁxed at the best prices found during this explo- ration period.
Explore it thus reﬂects the typical approach used to price products: initially experiment with diﬀerent price-settings and eventually settle on the prices that previously exhibited the best results.
Throughout our experiments, the ﬁrst set of prices p0 used to initialize each method is always taken to be the center of the feasible set S.
7.1. Stationary Demand Model First, we apply our methods in a stationary setting where the underlying structural pa- rameters zt,“ z, Vt “ V are ﬁxed over time.
At the start of each simulation, we sample the entries of z, V independently as zij „ Np100, 20q, Vij „ Np0, 2q and U is ﬁxed as a random sparse binary matrix that reﬂects which of d possible categories each prod- 19 uct belongs to.
Subsequently, we orthogonalize the columns of U and project V into V “ tV : VT ` V ľ λIu with λ “ 10 to ensure cross-product price elasticities are strictly positive deﬁnite (and our resulting online optimization problem is strongly convex).
Figure 1 shows that our OPOK and OPOL algorithms strongly outperform the basic GDG algorithm when the dimensionality N greatly exceeds the intrinsic rank d.
When N “ d and there is no low-rank structure to exploit, our OPOK/OPOL algorithms match the performance of standard GDG bandit optimization.
Even in this stationary setting on which it is highly reliant, the standard Explore it approach does not perform as well as our more sophisticated bandit optimization techniques.
Surprisingly, our OPOL method (which must infer the latent product features along with the pricing strategy) even slightly outperforms the OPOK approach, which has access to the ground-truth product features.
This is because our SVD-estimated features actually better represent the subspace where projected pricing variation can maximally impact the overall observed demands.
In contrast, the dimensionality-reduction of the OPOK algorithm ignores the eﬀects of noise in both our selected prices and the observed demands.
(A) N “ 10, d “ 10 (B) N “ 100, d “ 10 Figure 1: Average cumulative regret (over 10 repetitions with standard-deviations shaded) of diﬀerent pricing strategies when underlying demand model is stationary.
7.2. Model with Demand Shocks Next, we examine the performance of these methods in a non-stationary setting where the underlying demand model changes drastically at times T{3 and 2T{3.
At the start of each period r0, T{3s, rT{3, 2T{3s, r2T{3, Ts: we simply redraw the underlying structural parameters zt, Vt from the same Gaussian distributions described in the previous section.
Figure 2 shows that our bandit techniques quickly adapt to the changes in the underlying 20 0200040006000800010000T0.00.20.40.60.8Regret1e7ExploreitGDGOPOKOPOL0200040006000800010000T010000002000000300000040000005000000600000070000008000000RegretExploreitGDGOPOKOPOLdemand curves.
In fact, the regret of the bandit algorithms starts decreasing over time, indicating that they begin outperforming the optimal ﬁxed price chosen in hindsight.
Once again, our low-rank methods are able to achieve low regret for a large number of products unlike the existing approaches, while exhibiting the same performance as the GDG algorithm when there is no low-rank structure to exploit.
(A) N “ 10, d “ 10 (B) N “ 100, d “ 10 Figure 2: Average regret (over 10 repetitions with standard-deviations shaded) of pricing strategies when underlying demand model contains structural shocks at times T{3, 2T{3.
7.3. Drifting Demand Model Finally, we study these methods in another non-stationary setting where the demand curves slowly change over time.
Here, the underlying structural parameters zt, Vt are initially drawn from the same previously described Gaussian distributions at time t “ 0, but then begin to drift over time via the following process: zt`1 “ zt ` w, Vt`1 “ ΠVpVt ` Wq (19) where the entries of w and W are i.i.d. samples from the Np0, 1q and Np0, 0.1q distribu- tions, respectively.
ΠV denotes the projection of a matrix into the strongly positive-deﬁnite set V deﬁned previously.
Figure 3 shows how our bandit pricing approach can nicely adapt to slowly changing demand curves, quickly identifying a better pricing strategy than the optimal ﬁxed price chosen in hindsight.
Under this setting, our low-rank methods again exhibit much stronger performance than the GDG and Explore it algorithms when there is a large number of products to handle.
21 0200040006000800010000T010000002000000300000040000005000000600000070000008000000RegretExploreitGDGOPOKOPOL0200040006000800010000T010000002000000300000040000005000000600000070000008000000RegretExploreitGDGOPOKOPOL(A) N “ 10, d “ 10 (B) N “ 100, d “ 10 Figure 3: Average regret (over 10 repetitions with standard-deviations shaded) of various pricing strategies when underlying demand model drifts over time.
8.
Discussion By exploiting a low-rank structural condition which naturally emerges in dynamic pricing problems, this work introduces the ﬁrst online bandit optimization algorithm whose regret depends only on the intrinsic rank of the problem rather than the ambient dimensionality of the action space.
Our low-rank bandit approach to dynamic pricing scales to a large number of products with highly intercorrelated demand curves, and the underlying de- mand model is allowed to vary over time or even be adversarially chosen.
When applied to various high-dimensional dynamic pricing systems involving stationary and changing demand curves, our approach empirically outperforms standard bandit methods.
Future extensions of this work could include adaptations for predictable sequences in which future demands can be partially forecasted [27], or generalizing our convex formulation and linear demand model [28].
Note the GDG algorithm [9] upon which our approach is based only relies on convexity of the revenue function, and it is only our SVD procedure for learning latent product features that necessitates a linear demand curve.
While our work focused on dynamic pricing, it remains interesting to explore other bandit applications where similar low-rank structural conditions might prove useful.
22 0200040006000800010000T0100000020000003000000400000050000006000000RegretExploreitGDGOPOKOPOL0200040006000800010000T100000001000000200000030000004000000RegretExploreitGDGOPOKOPOLReferences [1] Dani V, Hayes TP, Kakade SM (2007) The price of bandit information for online optimization.
Neural Information Processing Systems [2] Keskin NB, Zeevi A (2014) Dynamic pricing with an unknown demand model: asymptotically optimal semi-myopic policies.
Operations Research 62: 1142–67 [3] Besbes O, Zeevi A (2015) On the surprising suﬃciency of linear models for dynamic pricing with demand learning.
Management Science 61: 723–39 [4] Cohen M, Lobel I, Leme RP (2016) Feature-based dynamic pricing.
ACM Conference on Economics and Computation [5] Javanmard A, Nazerzadeh H (2016) Dynamic pricing in high-dimensions.
arXiv:arXiv:160907574 [6] Javanmard A (2017) Perishability of data: Dynamic pricing under varying-coeﬃcient models.
Journal of Machine Learning Research 18: 1–31 [7] Witt U (1986) How can complex economical behavior be investigated?
The example of the ignorant monopolist revisited.
Behavioral Science 31: 173–188 [8] Shalev-Shwartz S (2011) Online learning and online convex optimization.
Founda- tions and Trends in Machine Learning 4: 107–194 [9] Flaxman AD, Kalai AT, McMahan HB (2005) Online convex optimization in the bandit setting: Gradient descent without a gradient.
Proceedings of the 16th Annual ACM-SIAM Symposium on Discrete Algorithms [10] Kleinberg R, Leighton T (2003) The value of knowing a demand curve: Bounds on regret for online posted-price auctions.
Proceedings of the 44th Annual IEEE Symposium on Foundations of Computer Science [11] Besbes O, Zeevi A (2009) Dynamic pricing without knowing the demand function: Risk bounds and near-optimal algorithms.
Operations Research 57: 1407–20 [12] den Boer AV, Bert Z (2013) Simultaneously learning and optimizing using controlled variance pricing.
Management Science 60: 770–83 [13] Misra K, Schwartz EM, Abernethy J (2017) Dynamic online pricing with incomplete information using multi-armed bandit experiments.
Available at SSRN: https: // ssrncom/ abstract= 2981814 [14] Gopalan A, Maillard O, Zaki M (2016) Low-rank bandits with latent mixtures.
arXiv:160901508 [15] Djolonga J, Krause A, Cevher V (2013) High-dimensional gaussian process bandits.
Neural Information Processing Systems [16] Hazan E, Levy KY (2014) Bandit convex optimization: Towards tight bounds.
Neural Information Processing Systems 23 [17] Bubeck S, Lee YT, Eldan R (2017) Kernel-based methods for bandit convex opti- mization.
Proceedings of 49th Annual ACM SIGACT Symposium on the Theory of Computing [18] Hazan E, Koren T, Livni R, Mansour Y (2016) Online learning with low rank experts.
Conference on Learning Theory [19] Brand M (2006) Fast low-rank modiﬁcations of the thin singular value decomposition.
Linear Algebra and its Applications 415: 20–30 [20] Stange P (2008) On the eﬃcient update of the singular value decomposition.
Pro- ceedings in Applied Mathematics and Mechanics 8: 10827–28 [21] Honorio J, Jaakkola T (2014) Tight bounds for the expected risk of linear classiﬁers and PAC-Bayes ﬁnite-sample guarantees.
Artiﬁcial Intelligence and Statistics [22] Dean S, Mania H, Matni N, Recht B, Tu S (2017) On the sample complexity of the linear quadratic regulator.
arXiv:171001688 [23] Yu Y, Wang T, Samworth R (2015) A useful variant of the Davis-Kahan theorem for statisticians.
Biometrika 102: 315–323 [24] Rigollet P (2015).
High dimensional statistics.
MIT Opencourseware: http://ocw.mit.edu/courses/mathematics/18-s997-high-dimensional- statistics-spring-2015/lecture-notes/ [25] Rudelson M, Vershynin R (2008) The Littlewood-Oﬀord problem and invertibility of random matrices.
Advances in Mathematics 218: 600–33 [26] Horn R, Johnson CR (1991) Topics in Matrix Analysis.
Cambridge Univ.
Press [27] Rakhlin A, Sridharan K (2013) Online learning with predictable sequences.
Confer- ence on Learning Theory [28] Hazan E, Levy KY, Shalev-Shwartz S (2016) On graduated optimization for stochas- tic non-convex problems.
International Conference on Machine Learning 24
Recently deep learning has emerged as a useful technique for data classiﬁcation as well as ﬁnding feature representations.
We consider the scenario of multi-class classiﬁca- tion.
A deep neural network maps each feature vector to one of the class labels by the connection of nodes in a multi-layer structure.
Between two adjacent layers a weight matrix maps the inputs (values in the previous layer) to the outputs (values in the current layer).
Assume the training set includes (yi, xi), i = 1, .
.
.
, l, where xi ∈ (cid:60)n0 is the feature vector and yi ∈ (cid:60)K is the label vector.
If xi is associated with label k, then yi = [0, .
.
.
, 0 , 1, 0, .
.
.
, 0]T ∈ (cid:60)K, (cid:124) (cid:123)(cid:122) (cid:125) k−1 where K is the number of classes and {1, .
.
.
, K} are possible labels.
After collecting all weights and biases as the model vector θ and having a loss function ξ(θ; x, y), a neural-network problem can be written as the following optimization problem.
where min f (θ), f (θ) = 2C θT θ + l(cid:88) i=1 ξ(θ; xi, yi).
(1) (2) The regularization term θT θ/2 avoids overﬁtting the training data, while the parameter C balances the regularization term and the loss term.
The function f (θ) is non-convex because of the connection between weights in different layers.
This non-convexity and the large number of weights have caused tremendous difﬁculties in training large-scale deep neural networks.
To apply an optimization algorithm for solving (2), the calcula- tion of function, gradient, and Hessian can be expensive.
Currently, stochastic gradient (SG) methods are the most commonly used way to train deep neural networks (e.g., Bot- tou, 1991; LeCun et al., 1998b; Bottou, 2010; Zinkevich et al., 2010; Dean et al., 2012; Moritz et al., 2015).
In particular, some expensive operations can be efﬁciently con- ducted in GPU environments (e.g., Ciresan et al., 2010; Krizhevsky et al., 2012; Hinton et al., 2012).
Besides stochastic gradient methods, some works such as Martens (2010); Kiros (2013); He et al.
(2016) have considered a Newton method of using Hessian infor- mation.
Other optimization methods such as ADMM have also been considered (Taylor et al., 2016).
When the model or the data set is large, distributed training is needed.
Following the design of the objective function in (2), we note it is easy to achieve data paral- lelism: if data instances are stored in different computing nodes, then each machine can calculate the local sum of training losses independently.1 However, achieving model parallelism is more difﬁcult because of the complicated structure of deep neural net- works.
In this work, by considering that the model is distributedly stored we propose a novel distributed Newton method for deep learning.
By variable and feature-wise data partitions, and some careful designs, we are able to explicitly use the Jacobian matrix for matrix-vector products in the Newton method.
Some techniques are incorporated to reduce the running time as well as the memory consumption.
First, to reduce the communication cost, we propose a diagonalization method such that an approximate Newton direction can be obtained without communication between machines.
Second, we consider subsampled Gauss-Newton matrices for reducing the running time as well as the communication cost.
Third, to reduce the synchronization cost, we terminate the process of ﬁnding an approximate Newton direction even though some nodes have not ﬁnished their tasks.
To be focused, among the various types of neural networks, we consider the stan- dard feedforward networks in this work.
We do not consider other types such as the convolution networks that are popular in computer vision.
This work is organized as follows.
Section 2 introduces existing Hessian-free New- ton methods for deep learning.
In Section 3, we propose a distributed Newton method for training neural networks.
We then develop novel techniques in Section 4 to reduce running time and memory consumption.
In Section 5 we analyze the cost of the pro- 1Training deep neural networks with data parallelism has been considered in SG, Newton and other optimization methods.
For example, He et al.
(2015) implement a parallel Newton method by letting each node store a subset of instances.
posed algorithm.
Additional implementation techniques are given in Section 6.
Then Section 7 reviews some existing optimization methods, while experiments in Section 8 demonstrate the effectiveness of the proposed method.
Programs used for experiments in this paper are available at http://www.csie.ntu.edu.tw/˜cjlin/papers/dnn.
Supplementary materials including a list of symbols and additional experiments can be found at the same web address.
2 Hessian-free Newton Method for Deep Learning In this section, we begin with introducing feedforward neural networks and then review existing Hessian-free Newton methods to solve the optimization problem.
2.1 Feedforward Networks A multi-layer neural network maps each feature vector to a class vector via the con- nection of nodes.
There is a weight vector between two adjacent layers to map the input vector (the previous layer) to the output vector (the current layer).
The network in Figure 1 is an example.
Let nm denote the number of nodes at the mth layer.
We use n0(input)-n1- .
.
.
-nL(output) to represent the structure of the network.3 The weight 2This ﬁgure is modiﬁed from the example at http://www.texample.net/tikz/ examples/neural-network.
3Note that n0 is the number of features and nL = K is the number of classes.
A0 B0 C0 A1 B1 Figure 1: An example of feedforward neural networks.2 matrix W m and the bias vector bm at the mth layer are  wm 1nm 2nm wm ...
··· ··· ...
··· wm  bm bm ...
bm nm and bm = wm nm−11 wm nm−12 nm−1nm nm−1×nm  wm 11 wm 21 ...
wm 12 wm 22 ...
W m = Let A2 B2 C2  nm×1 s0,i = z0,i = xi be the feature vector for the ith instance, and sm,i and zm,i denote vectors of the ith instance at the mth layer, respectively.
We can use sm,i = (W m)T zm−1,i + bm, m = 1, .
.
.
, L, i = 1, .
.
.
, l zm,i j = σ(sm,i ), j = 1, .
.
.
, nm, m = 1, .
.
.
, L, i = 1, .
.
.
, l (3) to derive the value of the next layer, where σ(·) is the activation function.
If W m’s columns are concatenated to the following vector (cid:20) wm = wm 11 .
.
.
wm nm−11 wm 12 .
.
.
wm nm−12 .
.
.
wm 1nm .
.
.
wm nm−1nm (cid:21)T then we can deﬁne θ =  w1 b1 ...
wL bL  as the weight vector of a whole deep neural network.
The total number of parameters is L(cid:88) n = (nm−1 × nm + nm) .
m=1 Because zL,i is the output vector of the ith data, by a loss function to compare it with the label vector yi, a neural network solves the following regularized optimization problem where min f (θ), f (θ) = 2C θT θ + l(cid:88) i=1 ξ(zL,i; yi), (4) C > 0 is a regularization parameter, and ξ(zL,i; yi) is a convex function of zL,i.
Note that we rewrite the loss function ξ(θ; xi, yi) in (2) as ξ(zL,i; yi) because zL,i is decided by θ and xi.
In this work, we consider the following loss function ξ(zL,i; yi) = ||zL,i − yi||2.
The gradient of f (θ) is ∇f (θ) = θ + l(cid:88) i=1 (J i)T∇zL,iξ(zL,i; yi), (5) (6) where  J i = ∂zL,i ∂θ1 ...
∂zL,i nL ∂θ1 ··· ...
··· ∂zL,i ∂θn ...
∂zL,i nL ∂θn  nL×n , i = 1, .
.
.
, l, (7) is the Jacobian of zL,i, which is a function of θ.
The Hessian matrix of f (θ) is ∇2f (θ) = I + (J i)T BiJ i l(cid:88) ∂ξ(zL,i; yi) i=1 j=1 ∂zL,i where I is the identity matrix and Bi ts = ∂2ξ(zL,i; yi) t ∂zL,i ∂zL,i , t = 1, .
.
.
, nL, s = 1, .
.
.
, nL.
From now on for simplicity we let ξi ≡ ξi(zL,i; yi).
2.2 Hessian-free Newton Method l(cid:88) nL(cid:88) i=1  ∂2zL,i ∂θ1∂θ1 ...
∂2zL,i ∂θn∂θ1 ··· ...
··· ∂2zL,i ∂θ1∂θn ...
∂2zL,i ∂θn∂θn  , (8) (9) For the standard Newton methods, at the kth iteration, we ﬁnd a direction dk minimizing the following second-order approximation of the function value: min dT H kd + ∇f (θk)T d, (10) where H k = ∇2f (θk) is the Hessian matrix of f (θk).
To solve (10), ﬁrst we calculate the gradient vector by a backward process based on (3) through the following equations: ), i = 1, .
.
.
, l, m = 1, .
.
.
, L, j = 1, .
.
.
, nm wm tj , i = 1, .
.
.
, l, m = 1, .
.
.
, L, t = 1, .
.
.
, nm−1 (11) (12) ∂ξi ∂zm,i nm(cid:88) j=1 ∂ξi ∂sm,i ∂ξi ∂zm−1,i ∂f ∂wm tj σ(cid:48)(sm,i ∂ξi ∂sm,i l(cid:88) i=1 wm tj + zm−1,i ∂ξi ∂sm,i , m = 1, .
.
.
, L, j = 1, .
.
.
, nm, t = 1, .
.
.
, nm−1 l(cid:88) i=1 ∂ξi ∂sm,i ∂f ∂bm bm j + , m = 1, .
.
.
, L, j = 1, .
.
.
, nm.
(13) (14) Note that formally the summation in (13) should be l(cid:88) l(cid:88) i=1 i(cid:48)=1 zm−1,i(cid:48) ∂ξi ∂sm,i(cid:48) but it is simpliﬁed because ξi is associated with only sm,i If H k is positive deﬁnite, then (10) is equivalent to solving the following linear system: H kd = −∇f (θk).
(15) Unfortunately, for the optimization problem (10), it is well known that the objective function may be non-convex and therefore H k is not guaranteed to be positive deﬁnite.
Following Schraudolph (2002), we can use the Gauss-Newton matrix as an approxima- tion of the Hessian.
That is, we remove the last term in (8) and obtain the following positive-deﬁnite matrix.
G = I + l(cid:88) (J i)T BiJ i.
i=1 (16) Note that from (9), each Bi, i = 1, .
.
.
, l is positive semi-deﬁnite if we require that ξ(zL,i; yi) is a convex function of zL,i.
Therefore, instead of using (15), we solve the following linear system to ﬁnd a dk for deep neural networks.
(Gk + λkI)d = −∇f (θk), (17) where Gk is the Gauss-Newton matrix at the kth iteration and we add a term λkI be- cause of considering the Levenberg-Marquardt method (see details in Section 4.5).
For deep neural networks, because the total number of weights may be very large, it is hard to store the Gauss-Newton matrix.
Therefore, Hessian-free algorithms have been applied to solve (17).
Examples include Martens (2010); Ngiam et al.
(2011).
Specif- ically, conjugate gradient (CG) methods are often used so that a sequence of Gauss- Newton matrix vector products are conducted.
Martens (2010); Wang et al.
(2015) use R-operator (Pearlmutter, 1994) to implement the product without storing the Gauss- Newton matrix.
Because the use of R operators for the Newton method is not the focus of this work, we leave some detailed discussion in Sections II–III in supplementary materials.
3 Distributed Training by Variable Partition The main computational bottleneck in a Hessian-free Newton method is the sequence of matrix-vector products in the CG procedure.
To reduce the running time, parallel matrix-vector multiplications should be conducted.
However, the R operator discussed in Section 2 and Section II in supplementary materials is inherently sequential.
In a forward process results in the current layer must be ﬁnished before the next.
In this 10 section, we propose an effective distributed algorithm for training deep neural networks.
3.1 Variable Partition Instead of using the R operator to calculate the matrix-vector product, we consider the whole Jacobian matrix and directly use the Gauss-Newton matrix in (16) for the matrix- vector products in the CG procedure.
This setting is possible because of the following reasons.
1.
A distributed environment is used.
2.
With some techniques we do not need to explicitly store every element of the Jaco- bian matrix.
Details will be described in the rest of this paper.
To begin we split each J i to P partitions (cid:20) J i = (cid:21) ··· J i J i Because the number of columns in J i is the same as the number of variables in the optimization problem, essentially we partition the variables to P subsets.
Speciﬁcally, we split neurons in each layer to several groups.
Then weights connecting one group of the current layer to one group of the next layer form a subset of our variable partition.
For example, assume we have a 150-200-30 neural network in Figure 2.
By splitting the three layers to 3, 2, 3 groups, we have a total number of partitions P = 12.
The partition (A0, A1) in Figure 2 is responsible for a 50 × 100 sub-matrix of W 1.
In addition, we distribute the variable bm to partitions corresponding to the ﬁrst neuron sub-group of the 11 A0 B0 C0 A0,A1 A0,B1 B0,A1 B0,B1 C0,A1 C0,B1 A1 B1 A1,A2 A1,B2 A1,C2 B1,A2 B1,B2 B1,C2 A2 B2 C2 Figure 2: An example of splitting variables in Figure 1 to 12 partitions by a split struc- ture of 3-2-3.
Each circle corresponds to a neuron sub-group in a layer, while each square is a partition corresponding to weights connecting one neuron sub-group in a layer to one neuron sub-group in the next layer.
mth layer.
For example, the 200 variables of b1 is split to 100 in the partition (A0, A1) and 100 in the partition (A0, B1).
By the variable partition, we achieve model parallelism.
Further, because z0,i = xi from (2.1), our data points are split in a feature-wise way to nodes corresponding to partitions between layers 0 and 1.
Therefore, we have data parallelism.
With the variable partition, the second term in the Gauss-Newton matrix (16) for the 12 ith instance can be represented as  (J i 1)T BiJ i (J i P )T BiJ i  .
··· ...
··· (J i 1)T BiJ i (J i P )T BiJ i (J i)T BiJ i = In the CG procedure to solve (17), the product between the Gauss-Newton matrix and a vector v is Gv =  (cid:80)l (cid:80)l 1)T Bi((cid:80)P P )T Bi((cid:80)P ...
i=1(J i p=1 J i pvp) + 1 C v1 i=1(J i p=1 J i pvp) + 1 C vP  (18)  v1 ...
vP  , where v = 2 (cid:17) if t = s, 0 otherwise.
is partitioned according to our variable split.
From (9) and the loss function deﬁned in (5), Bi ts = ∂2(cid:16)(cid:80)nL j − yi j=1(zL,i ∂zL,i t ∂zL,i j)2(cid:17) (cid:16) 2(zL,i t − yi t) ∂zL,i However, after the variable partition, each J i may still be a huge matrix.
The total space for storing J i p, ∀i is roughly nL × n × l.
If l, the number of data instances, is so large such that l × nL > n, than storing J i p, ∀i requires more space than the n× n Gauss-Newton matrix.
To reduce the memory consumption, we will propose effective techniques in Sections 3.3, 4.3, and 6.1. 13 With the variable partition, function, gradient, and Jacobian calculations become complicated.
We discuss details in Sections 3.2 and 3.3. 3.2 Distributed Function Evaluation From (3) we know how to evaluate the function value in a single machine, but the im- plementation in a distributed environment is not trivial.
Here we check the details from the perspective of an individual partition.
Consider a partition that involves neurons in sets Tm−1 and Tm from layers m − 1 and m, respectively.
Thus Tm−1 ⊂ {1, .
.
.
, nm−1} and Tm ⊂ {1, .
.
.
, nm}.
Because (3) is a forward process, we assume that sm−1,i , i = 1, .
.
.
, l, ∀t ∈ Tm−1 are available at the current partition.
The goal is to generate , i = 1, .
.
.
, l, ∀j ∈ Tm sm,i and pass them to partitions between layers m and m + 1.
To begin, we calculate zm−1,i = σ(sm−1,i ), i = 1, .
.
.
, l and t ∈ Tm−1.
(19) Then, from (3), the following local values can be calculated for i = 1, .
.
.
, l, j ∈ Tm tj zm−1,i wm + bm if Tm−1 is the ﬁrst neuron sub-group of layer m − 1, tj zm−1,i wm otherwise.
(20) 14  (cid:80) (cid:80) t∈Tm−1 t∈Tm−1 After the local sum in (20) is obtained, we must sum up values in partitions between layers m − 1 and m.
(cid:88) (cid:16) sm,i j = Tm−1∈Pm−1 (cid:17) local sum in (20) (21) where i = 1, .
.
.
, l, j ∈ Tm, and Pm−1 = {Tm−1 | Tm−1 is any sub-group of neurons at layer m − 1}.
The resulting sm,i values should be broadcasted to partitions between layers m and m + 1 that correspond to the neuron subset Tm. We explain details of (21) and the broadcast operation in Section 3.2.1. 3.2.1 Allreduce and Broadcast Operations The goal of (21) is to generate and broadcast sm,i values to some partitions between layers m and m + 1, so a reduce operation seems to be sufﬁcient.
However, we will explain in Section 3.3 that for the Jacobian evaluation and then the product between Gauss-Newton matrix and a vector, the partitions between layers m − 1 and m corre- sponding to Tm also need sm,i for calculating zm,i j = σ(sm,i ), i = 1, .
.
.
, l, j ∈ Tm. (22) To this end, we consider an allreduce operation so that not only are values reduced from some partitions between layers m − 1 and m, but also the result is broadcasted to them.
After this is done, we make the same result sm,i available in partitions between layers m and m + 1 by choosing the partition corresponding to the ﬁrst neuron sub-group of layer m − 1 to conduct a broadcast operation.
Note that for partitions between layers L − 1 and L (i.e., the last layer), a broadcast operation is not needed.
15 Consider the example in Figure 2.
For partitions (A1, A2), (A1, B2), and (A1, C2), all of them must get s1,i j , j ∈ A1 calculated via (21): (cid:88) (cid:123)(cid:122) (cid:124) (cid:88) (cid:124) t + b1 (cid:123)(cid:122) tjz0,i t∈B0 t∈A0 (cid:125) w1 (A0,A1) (cid:125) s1,i j = (cid:88) (cid:124) t∈C0 w1 (cid:123)(cid:122) (cid:125) (B0,A1) (C0,A1) w1 tjz0,i tjz0,i (23) The three local sums are available at partitions (A0, A1), (B0, A1) and (C0, A1) re- j , j ∈ A1 are available spectively.
We ﬁrst conduct an allreduce operation so that s1,i at partitions (A0, A1), (B0, A1), and (C0, A1).
Then we choose (A0, A1) to broadcast values to (A1, A2), (A1, B2), and (A1, C2).
Depending on the system conﬁgurations, suitable ways can be considered for im- plementing the allreduce and the broadcast operations (Thakur et al., 2005).
In Section IV of supplementary materials we give details of our implementation.
To derive the loss value, we need one ﬁnal reduce operation.
For the example in , j ∈ A2, B2, C2 respectively available in partitions Figure 2, in the end we have z2,i (A1, A2), (A1, B2), and (A1, C2).
We then need the following reduce operation ||z2,i − yi||2 = j − yi (z2,i j)2 + (cid:88) j∈A2 (cid:88) j∈B2 j − yi (z2,i j)2 + (cid:88) j∈C2 j − yi (z2,i j)2 (24) and let (A1, A2) have the loss term in the objective value.
We have discussed the calculation of the loss term in the objective value, but we also need to obtain the regularization term θT θ/2.
One possible setting is that before the loss-term calculation we run a reduce operation to sum up all local regularization terms.
For example, in one partition corresponding to neuron subgroups Tm−1 at layer 16 m − 1 and Tm at layer m, the local value is(cid:88) (cid:88) t∈Tm−1 j∈Tm (wm tj )2.
(25) On the other hand, we can embed the calculation into the forward process for obtaining the loss term.
The idea is that we append the local regularization term in (25) to the vector in (20) for an allreduce operation in (21).
The cost is negligible because we only increase the length of each vector by one.
After the allreduce operation, we broadcast the resulting vector to partitions between layers m and m + 1 that corresponding to the neuron subgroup Tm. We cannot let each partition collect the broadcasted value for subsequent allreduce operations because regularization terms in previous layers would be calculated several times.
To this end, we allow only the partition corresponding to Tm in layer m and the ﬁrst neuron subgroup in layer m + 1 to collect the value and include it with the local regularization term for the subsequent allreduce operation.
By continuing the forward process, in the end we get the whole regularization term.
We use Figure 2 to give an illustration.
The allreduce operation in (23) now also calculates (cid:88) (cid:124) t∈A0 (cid:88) j∈A1 (w1 tj)2 + (cid:123)(cid:122) (A0,A1) (cid:88) j∈A1 (cid:88) (cid:124) t∈B0 (b1 j )2 (cid:125) (cid:88) (cid:123)(cid:122) j∈A1 (cid:88) (cid:124) t∈C0 (cid:88) (cid:123)(cid:122) j∈A1 (cid:125) (cid:125) (B0,A1) (C0,A1) (w1 tj)2 (w1 tj)2 (26) The resulting value is broadcasted to (A1, A2), (A1, B2), and (A1, C2).
Then only (A1, A2) collects the value and generate the following local sum: (26) + In the end we have (cid:88) (cid:88) t∈A1 j∈A2 (cid:88) j∈A2 (b2 j )2.
(w2 tj)2 + 17 1.
(A1, A2) contains regularization terms from (A0, A1), (B0, A1), (C0, A1), (A1, A2), (A0, B1), (B0, B1), (C0, B1), (B1, A2).
2.
(A1, B2) contains regularization terms from (A1, B2), (B1, B2).
3.
(A1, C2) contains regularization terms from (A1, C2), (B1, C2).
We can then extend the reduce operation in (24) to generate the ﬁnal value of the regu- larization term.
3.3 Distributed Jacobian Calculation From (7) and similar to the way of calculating the gradient in (11)-(14), the Jacobian matrix satisﬁes the following properties.
∂zL,i ∂wm tj ∂zL,i ∂bm ∂zL,i ∂sm,i ∂zL,i ∂sm,i ∂sm,i ∂wm tj ∂sm,i ∂bm (28) (29) where i = 1, .
.
.
, l, u = 1, .
.
.
, nL, m = 1, .
.
.
, L, j = 1, .
.
.
, nm, and t = 1, .
.
.
, nm−1.
However, these formulations do not reveal how they are calculated in a distributed set- ting.
Similar to Section 3.2, we check details from the perspective of any variable par- tition.
Assume the current partition involves neurons in sets Tm−1 and Tm from layers m − 1 and m, respectively.
Then we aim to obtain the following Jacobian components.
∂zL,i ∂wm tj and ∂zL,i ∂bm , ∀t ∈ Tm−1, ∀j ∈ Tm, u = 1, .
.
.
, nL, i = 1, .
.
.
, l.
18 Before showing how to calculate them, we ﬁrst get from (3) that ∂zL,i ∂sm,i ∂sm,i ∂wm tj ∂zL,i ∂zm,i = zm−1,i ∂zm,i ∂sm,i ∂zL,i ∂zm,i σ(cid:48)(sm,i ), and ∂sm,i ∂bm = 1, ∂zL,i ∂zL,i if j = u, 0 otherwise.
(30) (31) (32) 1   From (28)-(32), the elements for the local Jacobian matrix can be derived by and ∂zL,i ∂wm tj ∂zL,i ∂bm ∂zL,i ∂zm,i ∂zL,i ∂zm,i ∂zm,i ∂sm,i ∂zm,i ∂sm,i ∂sm,i ∂wm tj ∂sm,i ∂bm σ(cid:48)(sm,i )zm−1,i if m < L, ∂zL,i ∂zm,i σ(cid:48)(sL,i u )zL−1,i if m = L, j = u, (33) if m = L, j (cid:54)= u, σ(cid:48)(sm,i if m < L, ∂zL,i ∂zm,i σ(cid:48)(sL,i u ) if m = L, j = u, if m = L, j (cid:54)= u, (34) where u = 1, .
.
.
, nL, i = 1, .
.
.
, l, t ∈ Tm−1, and j ∈ Tm. We discuss how to have values in the right-hand side of (33) and (34) available at the current computing node.
From (19), we have zm−1,i , ∀i = 1, .
.
.
, l, ∀t ∈ Tm−1 available in the forward process of calculating the function value.
Further, in (21)-(22) to obtain zm,i for layers m and m+1, we use an allreduce operation rather than a reduce operation so that , ∀i = 1, .
.
.
, l, ∀j ∈ Tm sm,i 19 are available at the current partition between layers m − 1 and m.
Therefore, σ(cid:48)(sm,i in (33)-(34) can be obtained.
The remaining issue is to generate ∂zL,i u /∂zm,i .
We will show that they can be obtained by a backward process.
Because the discussion assumes that currently we are at a partition between layers m − 1 and m, we show details of and dispatching them to partitions between m− 2 and m− 1.
generating ∂zL,i u /∂zm−1,i From (3) and (30), ∂zL,i can be calculated by u /zm−1,i nm(cid:88) j=1 ∂zL,i ∂zm−1,i ∂zL,i ∂sm,i ∂sm,i ∂zm−1,i nm(cid:88) j=1 Therefore, we consider a backward process of using ∂zL,i In a distributed system, from (32) and (35), ∂zL,i ∂zm,i σ(cid:48)(sm,i )wm tj .
(35) u /∂zm,i to generate ∂zL,i u /∂zm−1,i (cid:80)  (cid:80) (cid:80) ∂zL,i ∂zm−1,i Tm∈Pm j∈Tm ∂zL,i ∂zm,i σ(cid:48)(sm,i )wm tj if m < L, (36) Tm∈Pm σ(cid:48)(sL,i u )wL tu if m = L, where i = 1, .
.
.
, l, u = 1, .
.
.
, nL, t ∈ Tm−1, and Pm = {Tm | Tm is any sub-group of neurons at layer m}.
(37) Clearly, each partition calculates the local sum over j ∈ Tm. Then a reduce operation is needed to sum up values in all corresponding partitions between layers m − 1 and m.
Subsequently, we discuss details of how to transfer data to partitions between layers m − 2 and m − 1.
Consider the example in Figure 2.
The partition (A0, A1) must get , t ∈ A1, u = 1, .
.
.
, nL, i = 1, .
.
.
, l.
∂zL,i ∂z1,i From (36), ∂zL,i ∂z1,i (cid:88) (cid:124) j∈A2 (cid:88) (cid:124) j∈B2 (cid:125) σ(cid:48)(s2,i j )w2 tj ∂zL,i ∂z2,i (cid:123)(cid:122) (A1,A2) (cid:88) (cid:124) j∈C2 (cid:125) σ(cid:48)(s2,i j )w2 tj ∂zL,i ∂z2,i (cid:123)(cid:122) (A1,B2) 20 σ(cid:48)(s2,i j )w2 tj (38) ∂zL,i ∂z2,i (cid:123)(cid:122) (A1,C2) (cid:125) Note that these three sums are available at partitions (A1, A2), (A1, B2), and (A1, C2), respectively.
Therefore, (38) is a reduce operation.
Further, values obtained in (38) are needed in partitions not only (A0, A1) but also (B0, A1) and (C0, A1).
Therefore, we need a broadcast operation so values can be available in the corresponding partitions.
For details of implementing reduce and broadcast operations, see Section IV of supplementary materials.
Algorithm 2 summarizes the backward process to calculate ∂zL,i u /∂zm,i 3.3.1 Memory Requirement We have mentioned in Section 3.1 that storing all elements in the Jacobian matrix may not be viable.
In the distributing setting, if we store all Jacobian elements corresponding to the current partition, then |Tm−1| × |Tm| × nL × l (39) space is needed.
We propose a technique to save space by noting that (28) can be written as the product of two terms.
From (30)-(31), the ﬁrst term is related to only Tm, while the second is related to only Tm−1: ∂zL,i ∂wm tj = [ ∂zL,i ∂sm,i ][ ∂sm,i ∂wm tj ] = [ ∂zL,i ∂zm,i σ(cid:48)(sm,i )][zm−1,i ].
(40) They are available in our earlier calculation.
Speciﬁcally, we allocate space to receive ∂zL,i u /∂zm,i from previous layers.
After obtaining the values, we replace them with ∂zL,i ∂zm,i σ(cid:48)(sm,i (41) for the future use.
Therefore, the Jacobian matrix is not explicitly stored.
Instead, we use the two terms in (40) for the product between the Gauss-Newton matrix and a vector 21 in the CG procedure.
See details in Section 4.2. Note that we also need to calculate and , ∀t ∈ store the local sum before the reduce operation in (36) for getting ∂zL,i Tm−1, ∀u, ∀i.
Therefore, the memory consumption is proportional to u /∂zm−1,i l × nL × (|Tm−1| + |Tm|).
This setting signiﬁcantly reduces the memory consumption of directly storing the Jaco- bian matrix in (39).
3.3.2 Sigmoid Activation Function In the discussion so far, we consider a general differentiable activation function σ(sm,i ).
In the implementation in this paper, we consider the sigmoid function except the output  j = σ(sm,i zm,i ) = −s 1+e m,i if m < L, sm,i if m = L.
(42) layer: Then,  (cid:18) σ(cid:48)(sm,i ) = −s m,i −s m,i 1+e (cid:19)2 = zm,i (1 − zm,i if m < L, if m = L.
and (33)-(34) become  ∂zL,i ∂wm tj zm,i (1 − zm,i )zm−1,i ∂zL,i ∂bm ∂zL,i ∂zm,i zL−1,i 0,  zm,i (1 − zm,i if m < L, ∂zL,i ∂zm,i if m = L, j = u, if m = L, j (cid:54)= u, where u = 1, .
.
.
, nL, i = 1, .
.
.
, l, t ∈ Tm−1, and j ∈ Tm. 22 3.4 Distributed Gradient Calculation For the gradient calculation, from (4), ∂f ∂wm tj wm tj + ∂ξi ∂wm tj wm tj + l(cid:88) i=1 l(cid:88) nL(cid:88) i=1 u=1 ∂ξi ∂zL,i ∂zL,i ∂wm tj (43) where ∂zL,i u /∂wm tj , ∀t, ∀j are components of the Jacobian matrix; see also the matrix form in (6).
From (33), we have known how to calculate ∂zL,i u /∂wm tj .
Therefore, if ∂ξi/∂zL,i is passed to the current partition, we can easily obtain the gradient vector via (43).
This can be ﬁnished in the same backward process of calculating the Jacobian matrix.
On the other hand, in the technique that will be introduced in Section 4.3, we only consider a subset of instances to construct the Jacobian matrix as well as the Gauss- Newton matrix.
That is, by selecting a subset S ⊂ {1, .
.
.
, l}, then only J i,∀i ∈ S are considered.
Thus we do not have all the needed ∂zL,i u /∂wm tj for (43).
In this situation, we can separately consider a backward process to calculate the gradient vector.
From a derivation similar to (33), ∂ξi ∂wm tj ∂ξi ∂zm,i σ(cid:48)(sm,i )zm−1,i , m = 1, .
.
.
, L.
(44) to be like ∂zL,i By considering ∂ξi/∂zm,i in (36), we can apply the same back- ward process so that each partition between layers m − 2 and m − 1 must wait for ∂ξi/∂zm−1,i from partitions between layers m − 1 and m: u /∂zm,i (cid:88) (cid:88) Tm∈Pm j∈Tm ∂ξi ∂zm−1,i σ(cid:48)(sm,i )wm tj , ∂ξi ∂zm,i (45) where i = 1, .
.
.
, l, t ∈ Tm−1, and Pm is deﬁned in (37).
For the initial ∂ξi/∂zL,i in the 23 backward process, from the loss function deﬁned in (5), = 2 ×(cid:16) ∂ξi ∂zL,i (cid:17) j − yi zL,i From (43), a difference from the Jacobian calculation is that here we obtain a sum over all instances i.
Earlier we separately maintain terms related to Tm−1 and Tm to avoid storing all Jacobian elements.
With the summation over i, we can afford to store ∂f /∂wm tj and ∂f /∂bm j , ∀t ∈ Tm−1, ∀j ∈ Tm. 4 Techniques to Reduce Computational, Communica- tion, and Synchronization Cost In this section we propose some novel techniques to make the distributed Newton method a practical approach for deep neural networks.
4.1 Diagonal Gauss-Newton Matrix Approximation In (18) for the Gauss-Newton matrix-vector products in the CG procedure, we notice that the communication occurs for reducing P vectors 1v1, .
.
.
, J i J i P vP , each with size O(nL), and then broadcasting the sum to all nodes.
To avoid the high communication cost in some distributed systems, we may consider the diagonal blocks 24 of the Gauss-Newton matrix as its approximation: i=1(J i 1)T BiJ i ˆG = I + ...
(cid:80)l i=1(J i P )T BiJ i Then (17) becomes P independent linear systems (cid:80)l  l(cid:88) i=1 (J i 1)T BiJ i 1 + l(cid:88) (J i P )T BiJ i P + i=1 I + λkI)dk 1 = −gk 1, ...
I + λkI)dk P = −gk P , The matrix-vector product becomes where gk 1, .
.
.
, gk P are local components of the gradient:  .
 gk ...
gk ∇f (θk) =  (cid:80)l (cid:80)l i=1(J i 1)T BiJ i 1v1 + 1 C v1 Gv ≈ ˆGv = ...
i=1(J i P )T BiJ i P vP + 1 C vP  ,  .
(49) (50) (51) in which each (Gv)p can be calculated using only local information because we have independent linear systems.
For the CG procedure at any partition, it is terminated if the following relative stopping condition holds ||1 (J i p)T BiJ i pvp + ( + λk)vp + gk p|| ≤ σ||gk p|| (52) or the number of CG iterations reaches a pre-speciﬁed limit.
Here σ is a pre-speciﬁed tolerance.
Unfortunately, partitions may ﬁnish their CG procedures at different time, a 25 l(cid:88) i=1 situation that results in signiﬁcant waiting time.
To address this synchronization cost, we propose some novel techniques in Section 4.4. Some past works have considered using diagonal blocks as the approximation of the Hessian.
For logistic regression, Bian et al.
(2013) consider diagonal elements of the Hessian to solve several one-variable sub-problems in parallel.
Mahajan et al.
(2017) study a more general setting in which using diagonal blocks is a special case.
4.2 Product Between Gauss-Newton Matrix and a Vector In the CG procedure the main computational task is the matrix-vector product.
We present techniques for the efﬁcient calculation.
From (51), for the pth partition, the product between the local diagonal block of the Gauss-Newton matrix and a vector vp takes the following form.
(J i p)T BiJ i pvp.
Assume the pth partition involves neuron sub-groups Tm−1 and Tm respectively in layers j , ∀j ∈ Tm. m− 1 and m, and this partition is not responsible to handle the bias term bm Then p ∈ RnL×(|Tm−1|×|Tm|) and vp ∈ R(|Tm−1|×|Tm|)×1.
J i Let mat(vp) ∈ R|Tm−1|×|Tm| be the matrix representation of vp.
From (40), the uth component of (J i pvp)u is (cid:88) (cid:88) ∂zL,i ∂wm tj (mat(vp))tj = t∈Tm−1 j∈Tm zm−1,i (mat(vp))tj.
(53) ∂zL,i ∂sm,i (cid:88) (cid:88) t∈Tm−1 j∈Tm 26 j∈Tm While calculating  .
zm−1,i (mat(vp))tj (cid:88)  (cid:88) t∈Tm−1 ∂zL,i ∂sm,i (cid:88) t∈Tm−1 zm−1,i (vp)tj, ∀j ∈ Tm A direct calculation of the above value requires O(|Tm−1| × |Tm|) operations.
Thus to get all u = 1, .
.
.
, nL components, the total computational cost is proportional to nL × |Tm−1| × |Tm|.
We discuss a technique to reduce the cost by rewriting (53) as still needs O(|Tm−1| × |Tm|) cost, we notice that these values are independent of u.
pvp)u, ∀u.
Therefore, the total That is, they can be stored and reused in calculating (J i computational cost is signiﬁcantly reduced to |Tm−1| × |Tm| + nL × |Tm|.
(54) The procedure of deriving (J i p)T (BiJ i pvp) is similar.
Assume ¯v = BiJ i pvp ∈ RnL×1.
From (40), Because p)T ¯v(cid:1) mat(cid:0)(J i tj nL(cid:88) nL(cid:88) u=1 u=1 ∂zL,i ∂wm tj ¯vu ∂zL,i ∂sm,i (cid:32) nL(cid:88) zm−1,i ¯vu = zm−1,i ∂zL,i ∂sm,i u=1 nL(cid:88) u=1 ∂zL,i ∂sm,i ¯vu, ∀j ∈ Tm 27 (cid:33) ¯vu (55) (56) are independent of t, we can calculate and store them for the computation in (55).
Therefore, the total computational cost is proportional to |Tm−1| × |Tm| + nL × |Tm|, (57) which is the same as that for (J i pvp).
In the above discussion, we assume that diagonal blocks of the Gauss-Newton ma- trix are used.
If instead the whole Gauss-Newton matrix is considered, then we calculate (J i p1)T (Bi(J i p2vp2)), for any two partitions p1 and p2.
The same techniques introduced in this section can be applied because (53) and (55) are two independent operations.
4.3 Subsampled Hessian Newton Method From (16) we see that the computational cost between the Gauss-Newton matrix and a vector is proportional to the number of data.
To reduce the cost, subsampled Hessian Newton method (Byrd et al., 2011; Martens, 2010; Wang et al., 2015) have been pro- posed for selecting a subset of data at each iteration to form an approximate Hessian.
Instead of ∇2f (θ) in (15) we use a subset S to have (cid:88) i∈S |S| ∇2 θθξ(zL,i; yi).
Note that zL,i is a function of θ.
The idea behind this subsampled Hessian is that when a large set of points are under the same distribution, (cid:88) i∈S |S| ξ(zL,i; yi).
28 is a good approximation of the average training losses.
For neural networks we consider the Gauss-Newton matrix, so (16) becomes the following subsampled Gauss-Newton matrix.
GS = |S| (cid:88) i∈S (J i)T BiJ i.
(58) Now denote the subset at the kth iteration as Sk. The linear system (17) is changed to (GSk + λkI)dk = −∇f (θk).
After variable partitions, the independent linear systems are (cid:32) (cid:33) (cid:88) i∈Sk λkI + I + |Sk| (J i 1)T BiJ i 1 = −gk dk 1, (cid:32) λkI + (cid:88) i∈Sk I + |Sk| (J i P )T BiJ i (cid:33) ...
P = −gk dk P .
(59) (60) While using diagonal blocks of the Gauss-Newton matrix avoids the communication between partitions, the resulting direction may not be as good as that of using the whole Gauss-Newton matrix.
Here we extend an approach by Wang et al.
(2015) to pay some extra cost for improving the direction.
Their idea is that after the CG procedure of using a sub-sampled Hessian, they consider the full Hessian to adjust the direction.
Now in the CG procedure we use a block diagonal approximation of the sub-sampled matrix GSk, so after that we consider the whole GSk for adjusting the direction.
Speciﬁcally, if dk is obtained from the CG procedure, we solve the following two-variable optimization problem that involves GSk. min β1,β2 (β1dk + β2 ¯dk)T GSk(β1dk + β2 ¯dk) + ∇f (θk)T (β1dk + β2 ¯dk), (61) 29 where ¯dk is a chosen vector.
Then the new direction is dk ← β1dk + β2 ¯dk.
Here we follow Wang et al.
(2015) to choose ¯dk = dk−1.
Notice that we choose ¯d0 to be the zero vector.
A possible advantage of considering dk−1 is that it is from the previous iteration of using a different data subset Sk−1 for the subsampled Gauss-Newton matrix.
Thus it provides information from instances not in the current Sk. To solve (61), because GSk is positive deﬁnite, it is equivalent to solving the follow- ing two-variable linear system.
 (dk)T GSkdk (¯dk)T GSkdk   β1 β2  =  −∇f (θk)T dk −∇f (θk)T ¯dk  .
(62) (¯dk)T GSkdk (¯dk)T GSk ¯dk Note that the construction of (62) involves the communication between partitions; see detailed discussion in Section V of supplementary materials.
The effectiveness of using (61) is investigated in Section VII.
In some situations, the linear system (62) may be ill-conditioned.
We set β1 = 1 and β2 = 0 if where ε is a small number.
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(dk)T GSkdk (¯dk)T GSkdk (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ ε, (¯dk)T GSkdk (¯dk)T GSk ¯dk 30 (63) 4.4 Synchronization Between Partitions While the setting in (51) has made each node conduct its own CG procedure without communication, we must wait until all nodes complete their tasks before getting into the next Newton iteration.
This synchronization cost can be signiﬁcant.
We note that the running time at each partition may vary because of the following reasons.
1.
Because we select a subset of weights between two layers as a partition, the number of variables in each partition may be different.
For example, assume the network structure is 50-100-2.
The last layer has only two neurons because of the small number of classes.
For the weight matrix W m, a partition between the last two layers can have at most 200 variables.
In contrast, a partition between the ﬁrst two layers may have more variables.
Therefore, in the split of variables we should make partitions as balanced as possible.
A example will be given later when we introduce the experiment settings in Section 8.1. 2.
Each node can start its ﬁrst CG iteration after the needed information is available.
From (30)-(34), the calculation of the information needed for matrix-vector products involves a backward process, so partitions corresponding to neurons in the last layers start the CG procedure earlier than those of the ﬁrst layers.
To reduce the synchronization cost, a possible solution is to terminate the CG pro- cedure for all partitions if one of them reaches its CG stopping condition: (cid:88) i∈Sk ||(λk + )vp + |Sk| (J i p)T BiJ i pvp + gp|| ≤ σ||gp||.
(64) 31 However, under this setting the CG procedure may terminate too early because some partitions have not conducted enough CG steps yet.
To strike for a balance, in our implementation we terminate the CG procedure for all partitions when the following conditions are satisﬁed: 1.
Every partition has reached a pre-speciﬁed minimum number of CG iterations, CGmin.
2.
A certain percentage of partitions have reached their stopping conditions, (64).
In Section 8.1, we conduct experiments with different percentage values to check the effectiveness of this setting.
4.5 Summary of the Procedure We summarize in Algorithm 3 the proposed distributed subsampled Hessian Newton algorithm.
Besides materials described earlier in this section, here we explain other steps in the algorithm.
First, in most optimization algorithms, after a direction dk is obtained, a suitable step size αk must be decided to ensure the sufﬁcient decrease of f (θk + αkdk).
Here 4, .
.
.} such we consider a backtracking line search by selecting the largest αk ∈ {1, 1 2, 1 that the following sufﬁcient decrease condition on the function value holds.
f (θk + αkdk) ≤ f (θk) + ηαk∇f (θk)T dk, (65) where η ∈ (0, 1) is a pre-deﬁned constant.
Secondly, we follow Martens (2010); Martens and Sutskever (2012); Wang et al.
(2015) to apply the Levenberg-Marquardt method by introducing a term λkI in the 32 linear system (17).
Deﬁne ρk = f (θk + αkdk) − f (θk) αk∇f (θk)T dk + 1 2(αk)2(dk)T GSkdk as the ratio between the actual function reduction and the predicted reduction.
Based on ρk, the following rule derives the next λk+1.
 λk+1 = λk × drop ρk > 0.75, λk 0.25 ≤ ρk ≤ 0.75, (66) λk × boost otherwise, where (drop,boost) are given constants.
Therefore, if the predicted reduction is close to the true function reduction, we reduce λk such that a direction closer to the Newton direction is considered.
In contrast, if ρk is small, we enlarge λk so that a conservative direction close to the negative gradient is considered.
Note that line search already serves as a way to adjust the direction according to the function-value reduction, so in optimization literature line search and Levenberg- Marquardt method are seldom applied concurrently.
Interestingly, in recent studies of Newton methods for neural networks, both techniques are considered.
Our preliminary investigation in Section VI of supplementary materials shows that using Levenberg- Marquardt method together with line search is very helpful, but more detailed studies can be a future research issue.
In Algorithm 3 we show a master-master implementation, so the same program is used at each partition.
Some careful designs are needed to ensure that all partitions get consistent information.
For example, we can use the same random seed to ensure that at each iteration all partitions select the same set Sk in constructing the subsampled 33 Gauss-Newton matrix.
5 Analysis of the Proposed Algorithm In this section, we analyze Algorithm 3 on the memory requirement, the computational cost, and the communication cost.
We assume that the full training set is used.
If the subsampled Hessian method in Section 4.3 is applied, then in the Jacobian calculation and the Gauss-Newton matrix vector product the “l” term in our analysis should be replaced by the subset size |S|.
5.1 Memory Requirement at Each Partition Assume the partition corresponds to the neuron sub-groups Tm−1 at layer m− 1 and Tm at layer m.
We then separately consider the following situations.
1.
Local weight matrix: Each partition must store the local weight matrix.
tj , ∀t ∈ Tm−1, and ∀j ∈ Tm. wm If Tm−1 is the ﬁrst neuron sub-group of layer m − 1, it also needs to store j , ∀j ∈ Tm. bm Therefore, the memory usage at each partition for the local weight matrix is propor- tional to |Tm−1| × |Tm| + |Tm|.
34 2.
Function evaluation: From Section 3.2, we must store part of zm−1,i and zm,i vec- tors.4 The memory usage at each partition is l × (|Tm−1| + |Tm|).
(67) 3.
Gradient evaluation: First, we must store ∂f ∂wm tj and ∂f ∂bm , t ∈ Tm−1, j ∈ Tm after the gradient evaluation.
Second, for the backward process, from (45), we must store ∂ξi ∂zm−1,i , ∀t ∈ Tm−1, ∀i and , ∀j ∈ Tm, ∀i.
∂ξi ∂zm,i Therefore, the memory usage in each partition is proportional to (|Tm−1| × |Tm| + |Tm|) + l × (|Tm−1| + |Tm|).
(68) 4.
Jacobian evaluation: From the discussion in Section 3.3.1, the memory consumption is proportional to l × nL × (|Tm−1| + |Tm|).
(69) In summary, the memory bottleneck is on terms that are related to the number of in- stances.
To reduce the memory use, we have considered a technique in Section 4.3 to replace the term l in (69) with a smaller subset size |Sk|.
We will further discuss a technique to reduce the memory consumption in Section 6.1. 4Note that the same vector is used to store the s vector before it is transformed to z by the activation function.
35 5.2 Computational Cost We analyze the computational cost at each partition.
For the sake of simplicity, we make the following assumptions.
• At the mth layer neurons are evenly split to several sub-groups, each of which has |Tm| elements.
• Calculating the activation function σ(s) needs 1 operation.
The following analysis is for a partition between layers m − 1 and m.
1.
Function evaluation: From Algorithm 1, after sm−1,i , i = 1, .
.
.
, l, t ∈ Tm−1 are available, we must calculate (19) and (20).
The dominant one is (20), so the compu- tational cost of function evaluation is O(l × |Tm| × |Tm−1|).
(70) 2.
Gradient evaluation: Assume that the current partition has received ∂ξi/∂zm,i , i = 1, .
.
.
, l, j ∈ Tm. From (44), we calculate l(cid:88) l(cid:88) i=1 i=1 ∂f ∂wm tj wm tj + wm tj + which costs ∂ξi ∂wm tj ∂ξi ∂zm,i σ(cid:48)(sm,i )zm−1,i , ∀t ∈ Tm−1, ∀j ∈ Tm, O(l × |Tm| × |Tm−1|).
Then for the reduce operation in (45), calculating the local sum (cid:88) j∈Tm σ(cid:48)(sm,i tj , i = 1, .
.
.
, l, t ∈ Tm−1.
)wm ∂ξi ∂zm,i 36 has a similar cost.
Thus the computational cost of gradient evaluation is O(l × |Tm| × |Tm−1|).
(71) 3.
Jacobian evaluation: From (46) and (47) in Algorithm 2, the computational cost is O(nL × l × |Tm| × |Tm−1|).
(72) 4.
Gauss-Newton matrix-vector products: Following (57) in Section 4.2, , the compu- tational cost for Gauss-Newton matrix vector products is # CG iterations × (l × (|Tm−1| × |Tm| + nL × |Tm|)) .
(73) From (70)-(73), we can derive the following conclusions.
1.
The computational cost is proportional to the number of training data, the number of classes, and the number of variables in a partition.
2.
In general, (72) and (73) dominate the computational cost.
Especially, when the number of CG iterations is large, (73) becomes the bottleneck.
3.
If the subsampling techniques in Section 4.3 is used, then l in (72)-(73) is replaced with the size of the subset.
Therefore, the computational cost at each partition in a Newton iteration can be effectively reduced.
However, the number of iterations may be increased.
4.
The computational cost can be reduced by splitting neurons at each layer to as many sub-groups as possible.
However, because each partition corresponds to a computing node, more partitions imply a higher synchronization cost.
Further, the total number of neurons at each layer is different, so the size of each partition may signiﬁcant vary, a situation that further worsens the synchronization issue.
37 5.3 Communication Cost We have shown in Section 3.1 that by using diagonal blocks of the Gauss-Newton matrix, each partition conducts a CG procedure without communicating with others.
However, communication cost still occurs for function, gradient, and Jacobian evalua- tion.
We discuss details for the Jacobian evaluation because the situation for others is similar.
To simplify the discussion we make the following assumptions.
1.
At the mth layer neurons are evenly split to several sub-groups, each of which has |Tm| elements.
Thus the number of neuron sub-groups at layer m is nm/|Tm|.
2.
Each partition sends or receives one message at a time.
3.
Following Barnett et al.
(1994), the time to send or receive a vector v is α + β × |v|, where |v| is the length of v, α is the start-up cost of a transfer and β is the transfer rate of the network.
4.
The time to add a vector v and another vector of the same size is γ × |v|.
5.
Operations (including communications) of independent groups of nodes can be con- ducted in parallel.
For example, the two trees in Figure IV.3 of supplementary ma- terials involve two independent sets of partitions.
We assume that the two reduce operations can be conducted in parallel.
38 From (36), for partitions between layers m − 1 and m that correspond to the same neuron sub-group Tm−1 at layer m − 1, the reduce operation on ∂zL,i 1, .
.
.
, nL, t ∈ Tm−1, i = 1, .
.
.
, l sums up u /∂zm−1,i , u = nm|Tm| vectors of l × nL × |Tm−1| size.
For example, the layer 2 in Figure 2 is split to three groups A2, B2 and C2, so for the sub-group A1 in layer 1, three vectors from (A1, A2), (A1, B2) and (A1, C2) are reduced.
Following the analysis in Pjeˇsivac-Grbovi´c et al.
(2007), the communication cost for the reduce operation is O((cid:100)(log2( nm|Tm|)(cid:101) × (α + (β + γ) × (l × nL × |Tm−1|)) .
(74) Note that between layers m − 1 and m nm−1 |Tm−1| reduce operations are conducted and each takes the communication cost shown in (74).
However, by our assumption they can be fully parallelized.
The reduced vector of size l × nL × |Tm−1| is then broadcasted to nm−2/|Tm−2| partitions.
Similar to (74), the communication cost is O((cid:100)(log2( nm−2 |Tm−2|)(cid:101) × (α + β × (l × nL × |Tm−1|))).
(75) The γ factor in (74) does not appear here because we do not need to sum up vectors.
Therefore, the total communication cost of the Jacobian evaluation is the sum of (74) and (75).
We can make the following conclusions.
1.
The communication cost is proportional to the number of training instances as well as the number of classes.
39 2.
From (74) and (75), a smaller |Tm−1| reduces the communication cost.
However, we can not split neurons at each layer to too many groups because of the following reasons.
First, we assumed earlier that for independent sets of partitions, their operations including communication within each set can be fully parallelized.
In practice, the more independent sets the higher synchronization cost.
Second, when there are too many partitions the block diagonal matrix in (49) may not be a good approximation of the Gauss-Newton matrix.
6 Other Implementation Techniques In this section, we discuss additional techniques implemented in the proposed algo- rithm.
6.1 Pipeline Techniques for Function and Gradient Evaluation The discussion in Section 5 indicates that in our proposed method the memory require- ment, the computational cost and the communication cost all linearly increase with the number of data.
For the product between the Gauss-Newton matrix and a vector, we have considered using subsampled Gauss-Newton matrices in Section 4.3 to effectively reduce the cost.
To avoid that function and gradient evaluations become the bottleneck, here we discuss a pipeline technique.
The idea follows from the fact that in (4) ξi,∀i 40 are independent from each other.
The situation is the same for (J i)T∇zL,iξ(zL,i; yi),∀i in (6).
Therefore, in the forward (or the backward) process, once results related to an instance xi are ready, they can be passed immediately to partitions in the next (or pre- vious) layers.
Here we consider a mini-batch implementation.
Take the function evalu- ation as an example.
Assume {1, .
.
.
, l} is split to R equal-sized subsets S1, .
.
.
, SR.
At a variable partition between layers m − 1 and m, we showed earlier that local values in (20) are obtained for all instances i = 1, .
.
.
, l.
Now instead we calculate (cid:88) t∈Tm−1 tj zm−1,i wm + bm j , j ∈ Tm, i ∈ Sr. The values are used to calculate , ∀i ∈ Sr. sm,i By this setting we achieve better parallelism.
Further, because we split {1, .
.
.
, l} to subsets with the same size, the memory space allocated for a subset can be reused by another.
Therefore, the memory usage is reduced by R folds.
6.2 Sparse Initialization A well-known problem in training neural networks is the easy overﬁtting because of an enormous number of weights.
Following the approach in Section 5 of Martens (2010), we implement the sparse initialization for the weights to train deep neural networks.
For each neuron in the mth layer, among the nm−1 weights connected to it, we randomly assign several weights to have values from the N (0, 1) distribution.
Other weights are kept zero.
41 We will examine the effectiveness of this initialization in Sections 8.2 and 8.3. 7 Existing Optimization Methods for Training Neural Networks Besides Newton methods considered in this work, many other optimization methods have been applied to train neural networks.
We brieﬂy discuss the most commonly used one in this section.
7.1 Stochastic Gradient Methods For deep neural networks, it is time-consuming to calculate the gradient vector because from (6), we must go through the whole training data set.
Instead of using all data instances, stochastic gradient (SG) methods randomly choose an example (yik, xik) to derive the following sub-gradient vector to update the weight matrix.
∇f ik(θk) = θk + (J ik)T∇zL,ik ξ(zL,ik; yik).
Algorithm 4 gives the standard setting of SG methods.
Assume that one epoch means the SG procedure goes through the whole training data set once.
Based on the frequent updates of the weight matrix, SG methods can get a reasonable solution in a few epochs.
Another advantage of SG methods is that Algorithm 4 is easy to implement.
However, if the variance of the gradient vector for each instance is large, SG methods may have slow convergence.
To address this issue, mini-batch SG method have been proposed to accelerate the convergence speed 42 (e.g., Bottou, 1991; Dean et al., 2012; Ngiam et al., 2011; Baldi et al., 2014).
Assume Sk ⊂ {1, .
.
.
, l} is a subset of the training data.
The sub-gradient vector can be as follows: ∇f Sk(θk) = θk |Sk| (cid:88) i∈Sk (J i)T∇zL,iξ(zL,i; yi).
However, when SG methods meet ravines which cause the particular dimension appar- ent to other dimensions, they are easier to drop to local optima.
Polyak (1964) proposes using the previous direction with momentum as part of the current direction.
This set- ting may decrease the impact of a particular dimension.
Algorithm 5 gives details of a mini-batch SG method with momentum implemented in Theano/Pylearn2 (Goodfellow et al., 2013).
Many other variants of SG methods have been proposed, but it has been shown (e.g., Sutskever et al., 2013) that the mini-batch SG with momentum is a strong baseline.
Thus in this work we do not include other types of SG algorithms for comparison.
Unfortunately, both SG and mini-batch SG methods have a well known issue in choosing a suitable learning rate and a momentum coefﬁcient for different problems.
We will conduct some experiments in Section 8.
8 Experiments We consider the following data sets for experiments.
All except Sensorless come with training and test sets.
We split Sensorless as described below.
• HIGGS: This binary classiﬁcation data set is from high energy physics applica- tions.
It is selected for our experiments because feedforward networks have been 43 successfully applied (Baldi et al., 2014).
Note that a scalar output y is enough to represent two classes in a binary classiﬁcation problem.
Based on this idea, we set nL = 1, and have each yi ∈ {−1, 1}.
The predicted outcome is the ﬁrst class if y ≥ 0 and is the second class if y < 0.
This data set is mainly used in Section 8.3 for a comparison with results in Baldi et al.
(2014).
• Letter: This set is from the Statlog collection (Michie et al., 1994) and we scale values of each feature to be in [−1, 1].
• MNIST: This data set for hand-written digit recognition (LeCun et al., 1998a) is widely used to benchmark classiﬁcation algorithms.
We consider a scaled ver- sion, where every feature value is divided by 255.
• Pendigits: This data set is originally from Alimoglu and Alpaydin (1996).
• Poker: This data set is from UCI machine learning repository (Lichman, 2013).
It has been studied by, for example, Li (2010).
• Satimage: This set is from the Statlog collection (Michie et al., 1994) and we scale values of each feature to be in [−1, 1].
• SensIT Vehicle: This data set, from Duarte and Hu (2004), includes signals from acoustic and seismic sensors in order to classify the different vehicles.
We use the original version without scaling.
• Sensorless: This data set is from Paschke et al.
(2013).
We scale values of each feature to be in [0, 1], and then conduct stratiﬁed random sampling to select 10, 000 instances to be the test set and the rest of the data to be the training set.
44 • SVHN: This data, originally from Google Street View images, consists of colored images of house numbers (Netzer et al., 2011).
We scale the data set to [0, 1] by considering the largest and the smallest feature values of the entire data set.
M ≡ max max (xi)p and m ≡ min min (xi)p.
Then the pth element of xi is changed to (xi)p ← (xi)p − m M − m • USPS: This data set, from Hull (1994), is used on recognizing handwritten ZIP codes and we scale values of each feature to be in [−1, 1].
All data sets, with statistics in Table 1, are publicly available.5 Detailed settings for each data such as the network structure are given in Table 2.
How to decide a suitable network structure is beyond the scope of this work, but if possible, we follow the setting in earlier works.
For example, we consider the structure in Wan et al.
(2013) for MNIST and Neyshabur et al.
(2015) for SVHN.
From Table 2, the model used for SVHN is the largest.
If the number of neurons in each layer is further increased, then the model must be stored in different machines.
We give parameters used in our algorithm.
For the sparse initialization discussed nm−1(cid:101) are in Section 6.2, among nm−1 weights connected to a neuron in layer m, (cid:100)√ selected to have non-zero values.
For the CG stopping condition (52), we set σ = 0.001 and CGmax = 250.
Further, the minimal number of CG steps run at each partition, CGmin, is set to be 3.
For the implementation of the Levenberg-Marquardt method, we 5All data sets used can be found at https://www.csie.ntu.edu.tw/˜cjlin/ libsvmtools/datasets/.
45 Table 1: Summary of the data sets: n0 is the number of features, l is the number of training instances, lt is the number of testing instances, and K is the number of classes.
n0 16 lt K 15,000 5,000 26 784 60,000 10,000 10 16 10 36 7,494 3,498 10 25,010 1,000,000 10 4,435 2,000 Data set Letter MNIST Pendigits Poker Satimage SensIT Vehicle 100 78,823 19,705 Sensorless 48 48,509 10,000 11 SVHN USPS HIGGS 3,072 73,257 26,032 10 256 7,291 2,007 10 28 10,500,000 500,000 set the initial λ1 = 1.
The (drop, boost) constants in (66) are (2/3, 3/2).
For solving (61) to get the update direction after the CG procedure, we set ε = 10−5 in (63).
8.1 Analysis of Distributed Newton Methods We have proposed several techniques to improve upon the basic implementation of the Newton method in a distributed environment.
Here we investigate their effectiveness by considering the following methods.
Note that because of the high memory consumption of some larger sets, we always implement the subsampled Hessian Newton method 46 Table 2: Details of the distributed network for each data.
Sampling rate is the percentage of training data used to calculate the subsampled Gauss-Newton matrix.
Data set Letter MNIST Pendigits Poker SensIT Vehicle Sensorless Satimage SVHN USPS Sampling rate Network structure Split structure # partitions 20% 20% 20% 20% 20% 20% 20% 10% 20% 16-300-300-300-300-26 1-2-1-1-1-1 784-800-800-10 16-300-300-10 1-1-3-1 1-2-2-1 10-200-200-200-10 1-1-1-1-1 100-300-300-3 1-2-2-1 48-300-300-300-11 1-2-1-2-1 36-1000-500-6 1-2-2-1 3072-4000-4000-10 3-2-2-1 256-300-300-10 1-2-2-1 12 discussed in Section 4.3. 1.
subsampled-GN: we use the whole subsampled Gauss-Newton matrix deﬁned in (58) to conduct the matrix-vector product in the CG procedure and then solve (61) to get the update direction after the CG procedure (Wang et al., 2015).
2.
diag: it is the same as subsampled-GN except that only diagonal blocks of the sub- sampled Gauss-Newton matrix are used; see (60).
3.
diag + sync 50%: it is the same as diag except that we consider the technique in Section 4.4 to reduce the synchronization time.
We terminate the CG procedure when 50% of partitions have reached their local stopping conditions (64).
47 4.
diag + sync 25%: it is the same as diag + sync 50% except that we terminate the CG procedure when 25% of partitions have reached their local stopping conditions (64).
For each of the above methods, we consider the following implementation details.
1.
We set C = l as the regularization parameter.
2.
We run experiments on G1 type instances on Microsoft Azure and let each instance use only one core.
If instances are not virtual machines on the same computer, our setting ensures that each variable partition corresponds to one machine.
3.
To make the computational cost in each partition as balanced as possible, in our ex- periments we choose our partitions such that the maximum ratio between the num- bers of variables (|Tm|×|Tm−1|) among any two partitions is as low as possible.
For example, in Pendigits, the largest partition has 150 × 150 = 22, 500 weight vari- ables, and the smallest partition has 150 × 10 = 1, 500 weight variables, with their ratio being 22500/1500 = 15.
For most data sets, the ratio is between 10 and 100 but not lower because the numbers of classes is relatively small, making the number of variables in the partitions involving the output layer smaller than those in other partitions.
In Figure 4, we show the comparison results and have the following observations.
1.
For test accuracy versus number of iterations, subsampled-GN in general has the fastest convergence rate.
The reason should be that the direction in subsampled-GN 48 by solving the linear system (59) is closer to the full Newton direction than other methods, which consider further approximations of the Gauss-Newton matrix or the early termination of the CG procedure.
However, the cost per iteration is high, so for training time we see that subsampled-GN may become worse than other approaches.
2.
The early termination of the CG procedure can effectively reduce the cost per iter- ation.
However, if we stop the CG procedure too early, the total training time may even increase.
For example, diag + sync 25% is generally the fastest in the beginning because of the least cost per iteration.
It is still the fastest in the end for MNIST, Letter, USPS, Satimage, and Pendigits.
However, it has the slowest ﬁnal convergence for SensIT Vechicle, Poker, and Sensorless.
Take the data set Poker as an example.
As listed in Table 2, the variables are split into four partitions, and the CG procedure stops if one partition (i.e., 25% of the partitions) reaches its local stopping condition.
This partition may have the lightest computational load or is the earliest one to start solving the local linear system.6 Thus the other partitions may not have run enough CG iterations.
The approach diag + sync 50% does not terminate the CG procedure that early.
Overall we ﬁnd that it is efﬁcient 6Note that because of the backward process in Section 3.3, the partitions corresponding to the last two layers begin their CG procedures earlier than the others.
49 and stable.
Therefore, in subsequent comparisons with stochastic gradient methods, we use it as the setting of our Newton method.
Because of the space consideration, we have evaluated only some techniques pro- posed in Section 4.
For the following two techniques we leave details in Sections VI and VII of the supplementary materials.
1.
In Section 4.3, we propose combining dk and dk−1 as the update direction.
We show that this technique is very effective.
2.
We mentioned in Section 4.5 that line search and the Levenberg-Marquardt (LM) method may not be both needed.
Our preliminary results show that the training speed is improved when both techniques are applied.
8.2 Comparison with Stochastic Gradient Methods and Support Vector Machines (SVM) In this section, we compare our methods with SG methods and SVMs, which are pop- ularly used for multi-class classiﬁcation.
Settings of these methods are described as follows.
1.
Newton: for our method we use the setting diag + sync 50% considered in Section 8.1 and let C = l.
2.
SVM (Boser et al., 1992): We consider the RBF kernel.
K(xi, xj) = e−γ||xi−xj||2, 50 where xi and xj are two data instances, and γ is the kernel parameter chosen by users.
Note that SVM solves an optimization problem similar to (4), so the regu- larization parameter, C, must be decided as well.
We conduct ﬁve-fold cross vali- dation on the training set to select the best C ∈ {2−5l, 2−3l, .
.
.
, 215l} and the best γ ∈ {2−15, 2−13, .
.
.
, 23}.7 We use the library LIBSVM (Chang and Lin, 2011) for training and prediction.
3.
SG: We use the code from Baldi et al.
(2014), which implements Algorithm 5.
The objective function is the same as (4).8 The network structure for each data set is identical to the corresponding one used in Newton, and we also set the regularization parameter C = l.
The major modiﬁcation we make is that we replace their activation functions with ours.
In Baldi et al.
(2014), the authors use tanh as their activation functions in layers 1, .
.
.
, L − 1 and the sigmoid function in layer L, while in our experiments of Newton methods in Section 8.1, we use the sigmoid function in layers 1, .
.
.
, L − 1 and the linear function in layer L.
The initial learning rate is selected from {0.05, 0.025, 0.01, 0.005, 0.002, 0.001} by the ﬁve-fold cross validation.
After the initial learning rate has been selected, we conduct the training process to generate a model for the prediction on the test set.
As regards the stopping condition for the training process, we terminate the Newton 7Here we consider an SVM formulation represented as (2).
In the form considered in LIBSVM, the two terms C and 1/l are combined together, so C/l is the actual parameter to be selected.
For SVHN because of the lengthy time for parameter selection, we selected only 10, 000 instances by stratiﬁed sampling to conduct the ﬁve-fold cross validation.
8Following Baldi et al.
(2014), we regularized only the weights but not the biases.
Through several experiments, we found that the performance is similar with/without the regularization of the biases.
51 method at the 100th iteration.
For SG, it terminates after a minimal number of epochs have been conducted and the objective function value on the validation set does not improve much within the last N epochs (see Algorithm 5).
To implement the stopping condition, for SG we split the input training set into 90% for training and 10% for validation.9 For SVM, we use the default stopping condition of LIBSVM.10 Here we also investigate the effect of the initialization by considering the following two settings.
1.
The sparse initialization discussed in Section 6.2. 2.
The dense initialization discussed in Baldi et al.
(2014).
The initial weights are drawn from the normal distribution N (0, 0.12) for the ﬁrst layer, N (0, 0.0012) for the output layer, and N (0, 0.052) for other hidden layers.
The biases are initialized as zeros.
To make a fair comparison, for each setting, Newton and SG are trained with the same initial weights and biases.
We present a comparison on test accuracy in Table 3, and make the following ob- servations.
1.
For neural networks, the sparse initialization usually results in better accuracy than the dense initialization does.
The difference can be huge in some cases, such as 9Note that in the CV procedure we also need a stopping condition in training each sub-problem.
We do an 80-20 split of every four folds of data so that the 20% of data are used to implement the stopping condition.
10LIBSVM terminates when the violation of the optimality condition calculated based on the gradient is smaller than a tolerance.
52 training using SG on the data set Letter.
The low accuracy of the densely initialized SG on Letter may be because of the poor differentiation between neurons in dense initialization (Martens, 2010).
Other possible causes include the vanishing gradient problem (Bengio et al., 1994), or that the activations are trapped in the saturation regime of the sigmoid function (Glorot and Bengio, 2010).
Note that the impact of the initialization scheme on the Newton method is much weaker.11 2.
Between SG and Newton, if sparse initialization is used, we can see that Newton generally gives higher accuracy.
3.
If sparse initialization is used, our Newton method for training neural networks gives similar or higher accuracy than SVM.
In particular, the results are much better for Poker and SVHN.
We compare our results on MNIST with those reported in earlier works.
Wan et al.
(2013) use a fully connected neural network with two 800-neuron hidden layers to de- rive an error rate 1.36%, under the setting of dense initialization,12 sigmoid activations, and the dropout technique.
By the same network structure and the same activation func- tion, our error rate is 1.34% at the 100th iteration.
For SVHN, we compare our results with Neyshabur et al.
(2015), in which the same network structure as ours is adopted, except that they use ReLU activations in the hidden layers.
They choose the cross-entropy as their objective function, and utilize the 11We observe similar phenomena in the experiments with HIGGS later in Section 8.3. See Table 5.
12In Wan et al.
(2013), the initial weights are drawn from N (0, 0.01), slightly different from the dense initialization we use.
53 dropout regularization.
Under dense initialization,13 they train their network with the Path-SGD method, which uses a proximal gradient method to solve the optimization problem.
They report an accuracy slightly below 87% (see their Figure 3), while the accuracy obtained by our Newton method with sparse initialization is 83.12%.
For Poker, we note that Li (2010) uses abc-logitboost to obtain a slightly higher accuracy, but his setting is different from ours.
He expands the training set by including half of the test set, with the remaining half of the test set used for evaluation.
An issue found out in our experiments is that SG is sensitive to the initial learning rate.
In Table 4, we present the test accuracy of SG under different initial rates for the Poker problem.
Clearly an inappropriate initial learning rate can lead to much worse accuracy.
8.3 Detailed Investigation on the HIGGS Data We compare AUC values obtained by our Newton and SG implementations with those reported in Baldi et al.
(2014) on HIGGS.
In our method, the sampling rate for cal- culating the subsampled Gauss-Newton matrix is set to be 1%.
Following the setting in Section 8.2, we consider two initializations (dense and sparse).
Then for each type of initialization, both SG and Newton start with the same initial weights and biases.
Note that our SG results are different from those in Baldi et al.
(2014) because we use different activation functions and initial values for weights and biases.14 Because of 13In Neyshabur et al.
(2015), the initial weights wm tj are drawn from N (0, 1/nm−1), slightly different from the dense initialization we use.
14Their initialization setting is the same as our dense initialization, but the values used by them are not available.
54 Table 3: Test accuracy of SVM, Newton and SG.
For SVM, we also show parameters (C, γ) used.
For SG, we show (the initial learning rate, number of epochs to reach the stopping criterion).
The bold-faced entries indicate the best accuracy obtained using the neural networks.
SVM Neural Networks Dense Initialization Sparse Initialization Newton SG Newton SG Letter MNIST 97.90% (27l, 2) 90.26% 8.02% (0.025, 245) 96.68% 96.28% (0.002, 906) 98.57% (23l, 2−5) 98.52% 98.26% (0.002, 801) 98.66% 98.33% (0.002, 909) Pendigits 98.06% (27l, 2−15) 97.51% 97.71% (0.001, 513) 97.83% 97.71% (0.002, 1179) Poker 58.78% (2−1l, 2−3) 99.25% 99.24% (0.005, 316) 99.25% 99.29% (0.002, 895) Satimage 91.85% (2l, 2) 89.35% 82.00% (0.01, 246) 89.85% 89.35% (0.001, 1402) SensIT Vehicle 83.90% (2l, 2−1) 85.16% 83.34% (0.01, 311) 84.60% 84.00% (0.01, 296) Sensorless 99.83% (25l, 23) 97.19% 97.64% (0.01, 412) 99.05% 98.24% (0.005, 382) SVHN USPS 74.54% (25l, 2−7) 95.32% (25l, 2−5) 80.96% 82.99% (0.001, 986) 83.12% 82.67% (0.001, 720) 95.17% 94.97% (0.025, 395) 95.27% 95.07% (0.001, 1617) resource constraints, we did not conduct a validation procedure to select SG’s initial learning rate.
Instead, we used the learning rate 0.05 by following Baldi et al.
(2014).
The results are shown in Table 5 and we can see that the Newton method often gives the best AUC values.
In Section 8.2 we have mentioned that SG’s performance may be sensitive to the initial learning rate.
The poor results of SG in Table 5 might be because we did not conduct a selection procedure.
Thus we decide to investigate the effect of the initial learning rate on the AUC value with the network structure 28-300-300-1 used in the 55 Table 4: Test accuracy on Poker using SG with different initial learning rates η.
Dense initialization is used.
Note that although η = 0.005 does not yield the highest test accuracy, it was selected for experiments in Table 3 because of giving the highest CV accuracy.
Initial learning rate η 0.05 0.025 0.01 0.005 0.002 0.001 Test accuracy 68.83% 98.81% 99.24% 99.24% 99.24% 99.25% earlier experiment in Table 5.
To compare the running time, both SG and Newton run on the same G3 type machine with 8 cores in Microsoft Azure.
The results of the AUC values versus the number of iterations and the training time are shown in Figure 5.
We clearly see again that the performance of SG depends signiﬁcantly on the initial learning rate.
Our experiments indicate that while SG can yield good performances under suitable parameters, the parameter selection procedure is essential.
In contrast, Newton methods are more robust because we do not need to ﬁne tune their parameters.
9 Discussion and Conclusions For the future works, we list the following directions.
1.
It is important to extend the proposed method for other types of neural networks.
For example, convolutional neural networks (CNNs) are popular for computer vision ap- plications (e.g., Krizhevsky et al., 2012; Simonyan and Zisserman, 2014).
Because 56 Table 5: A comparison between the AUC obtained by SG and that by the distributed Newton on the HIGGS data set.
We list the results in Baldi et al.
(2014) as a reference, where “NA” means that the result is not reported.
See explanation in Section 8.3 about the different results between our SG and Baldi et al.’s.
Network Split Dense Initialization Sparse Initialization Newton SG Newton SG Baldi et al.
(2014) 28-300-1 28-600-1 28-1000-1 28-2000-1 2-2-1 2-3-1 2-4-1 2-8-1 0.843 0.469 0.843 0.684 0.849 0.501 0.849 0.759 0.851 0.500 0.853 0.734 0.853 0.500 0.855 0.504 28-300-300-1 2-2-1-1 0.851 0.530 0.860 0.825 28-300-300-300-1 2-2-2-1-1 0.867 0.482 0.879 0.849 28-300-300-300-300-1 2-2-2-2-1-1 0.867 0.504 0.875 0.848 0.816 NA 0.841 0.842 NA 0.850 0.872 CNNs generally have fewer weights per layer, our method has the potential to train deep networks for large-scale image classiﬁcation.
2.
Instead of the Gauss-Newton matrix, we may consider other ways to use or approx- imate the Hessian such as the recent works by He et al.
(2016).
3.
For results in Tables 3 and 5, we consider the model after running 100 Newton iterations.
An advantage of Newton over stochastic gradient is that we can apply a gradient-based stopping condition.
We plan to investigate its practical use.
4.
It is known that using suitable preconditioners can effectively reduce the number of 57 CG steps in solving a linear system.
Studies of applying preconditioned CG methods in training neural networks include, for example, Chapelle and Erhan (2011).
We plan to investigate how to apply preconditioning in our distributed framework.
In summary, in this paper we proposed novel techniques to implement distributed Newton methods for training large-scale neural networks, and achieved both data and model parallelisms.
Acknowledgements This work was supported in part by MOST of Taiwan via the grant 105-2218-E-002-033 and Microsoft via Azure for Research programs.
References Alimoglu, F.
and Alpaydin, E.
(1996).
Methods of combining multiple classiﬁers based on different representations for pen-based handwritten digit recognition.
In Pro- ceedings of the Fifth Turkish Artiﬁcial Intelligence and Artiﬁcial Neural Networks Symposium.
Baldi, P., Sadowski, P., and Whiteson, D.
(2014).
Searching for exotic particles in high-energy physics with deep learning.
Nature Communications, 5.
Barnett, M., Gupta, S., Payne, D.
G., Shuler, L., van De Geijn, R., and Watts, J.
(1994).
Interprocessor collective communication library (InterCom).
In Proceedings of the Scalable High-Performance Computing Conference, pages 357–364.
58 Bengio, Y., Simard, P., and Frasconi, P.
(1994).
Learning long-term dependencies with gradient descent is difﬁcult.
IEEE Transactions on Neural Networks, 5(2):157–166.
Bian, Y., Li, X., Cao, M., and Liu, Y.
(2013).
Bundle CDN: a highly parallelized ap- proach for large-scale l1-regularized logistic regression.
In Proceedings of European Conference on Machine Learning and Principles and Practice of Knowledge Discov- ery in Databases (ECML/ PKDD).
Boser, B.
E., Guyon, I., and Vapnik, V.
(1992).
A training algorithm for optimal margin classiﬁers.
In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, pages 144–152.
ACM Press.
Bottou, L.
(1991).
Stochastic gradient learning in neural networks.
Proceedings of Neuro-Nımes, 91(8).
Bottou, L.
(2010).
Large-scale machine learning with stochastic gradient descent.
In Proceedings of COMPSTAT 2010, pages 177–186.
Byrd, R.
H., Chin, G.
M., Neveitt, W., and Nocedal, J.
(2011).
On the use of stochastic Hessian information in optimization methods for machine learning.
SIAM Journal on Optimization, 21(3):977–995.
Chang, C.-C.
and Lin, C.-J.
(2011).
LIBSVM: a library for support vector machines.
ACM Transactions on Intelligent Systems and Technology, 2(3):27:1–27:27.
Software available at http://www.csie.ntu.edu.tw/˜cjlin/libsvm.
Chapelle, O.
and Erhan, D.
(2011).
Improved preconditioner for Hessian free optimiza- tion.
In NIPS Workshop on Deep Learning and Unsupervised Feature Learning.
59 Ciresan, D.
C., Meier, U., Gambardella, L.
M., and Schmidhuber, J.
(2010).
Deep, big, simple neural nets for handwritten digit recognition.
Neural Computation, 22:3207– 3220.
Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Le, Q.
V., Mao, M.
Z., Ranzato, M., Senior, A.
W., Tucker, P.
A., et al.
(2012).
Large scale distributed deep networks.
In Advances in Neural Information Processing Systems (NIPS) 25.
Duarte, M.
and Hu, Y.
H.
(2004).
Vehicle classiﬁcation in distributed sensor networks.
Journal of Parallel and Distributed Computing, 64(7):826–838.
Glorot, X.
and Bengio, Y.
(2010).
Understanding the difﬁculty of training deep feed- forward neural networks.
In Proceedings of the 13th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), pages 249–256.
Goodfellow, I.
J., Warde-Farley, D., Lamblin, P., Dumoulin, V., Mirza, M., Pascanu, R., Bergstra, J., Bastien, F., and Bengio, Y.
(2013).
Pylearn2: a machine learning research library.
He, K., Zhang, X., Ren, S., and Sun, J.
(2015).
Delving deep into rectiﬁers: Surpass- ing human-level performance on ImageNet classiﬁcation.
In Proceedings of IEEE International Conference on Computer Vision (ICCV).
He, X., Mudigere, D., Smelyanskiy, M., and Tak´aˇc, M.
(2016).
Large scale distributed Hessian-free optimization for deep neural network.
arXiv preprint arXiv:1606.00511.
Hinton, G.
E., Deng, L., Yu, D., Dahl, G., rahman Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T., and Kingsbury, B.
(2012).
Deep neural 60 networks for acoustic modeling in speech recognition: The shared views of four research groups.
IEEE Signal Processing Magazine, 29(6):82–97.
Hull, J.
J.
(1994).
A database for handwritten text recognition research.
IEEE Transac- tions on Pattern Analysis and Machine Intelligence, 16(5):550–554.
Kiros, R.
(2013).
Training neural networks with stochastic Hessian-free optimization.
arXiv preprint arXiv:1301.3641.
Krizhevsky, A., Sutskever, I., and Hinton, G.
E.
(2012).
ImageNet classiﬁcation with deep convolutional neural networks.
In Pereira, F., Burges, C.
J.
C., Bottou, L., and Weinberger, K.
Q., editors, Advances in Neural Information Processing Systems 25, pages 1097–1105.
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
(1998a).
Gradient-based learning ap- plied to document recognition.
Proceedings of the IEEE, 86(11):2278–2324.
MNIST database available at http://yann.lecun.com/exdb/mnist/.
LeCun, Y., Bottou, L., Orr, G.
B., and M¨uller, K.-R.
(1998b).
Efﬁcient back- prop.
In Neural Networks, Tricks of the Trade, Lecture Notes in Computer Science LNCS 1524.
Springer Verlag.
Li, P.
(2010).
An empirical evaluation of four algorithms for multi-class classi- ﬁcation: Mart, abc-mart, robust logitboost, and abc-logitboost.
arXiv preprint arXiv:1001.1020.
Lichman, M.
(2013).
UCI machine learning repository.
61 Mahajan, D., Keerthi, S.
S., and Sundararajan, S.
(2017).
A distributed block coordinate descent method for training l1 regularized linear classiﬁers.
Journal of Machine Learning Research, 18(91):1–35.
Martens, J.
(2010).
Deep learning via Hessian-free optimization.
In Proceedings of the 27th International Conference on Machine Learning (ICML).
Martens, J.
and Sutskever, I.
(2012).
Training deep and recurrent networks with Hessian-free optimization.
In Neural Networks: Tricks of the Trade, pages 479–535.
Springer.
Michie, D., Spiegelhalter, D.
J., Taylor, C.
C., and Campbell, J., editors (1994).
Ma- chine learning, neural and statistical classiﬁcation.
Ellis Horwood, Upper Sad- dle River, NJ, USA.
Data available at http://archive.ics.uci.edu/ml/ machine-learning-databases/statlog/.
Moritz, P., Nishihara, R., Stoica, I., and Jordan, M.
I.
(2015).
SparkNet: Training deep networks in Spark.
arXiv preprint arXiv:1511.06051.
Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A.
Y.
(2011).
Reading digits in natural images with unsupervised feature learning.
In NIPS Workshop on Deep Learning and Unsupervised Feature Learning.
Neyshabur, B., Salakhutdinov, R.
R., and Srebro, N.
(2015).
Path-SGD: Path- normalized optimization in deep neural networks.
In Cortes, C., Lawrence, N.
D., Lee, D.
D., Sugiyama, M., and Garnett, R., editors, Advances in Neural Information Processing Systems 28, pages 2422–2430.
62 Ngiam, J., Coates, A., Lahiri, A., Prochnow, B., Le, Q.
V., and Ng, A.
Y.
(2011).
On optimization methods for deep learning.
In Proceedings of the 28th International Conference on Machine Learning, pages 265–272.
Paschke, F., Bayer, C., Bator, M., M¨onks, U., Dicks, A., Enge-Rosenblatt, O., and Lohweg, V.
(2013).
Sensorlose zustands¨uberwachung an synchronmotoren.
In Pro- ceedings of Computational Intelligence Workshop.
Pearlmutter, B.
A.
(1994).
Fast exact multiplication by the Hessian.
Neural Computa- tion, 6(1):147–160.
Pjeˇsivac-Grbovi´c, J., Angskun, T., Bosilca, G., Fagg, G.
E., Gabriel, E., and Dongarra, J.
J.
(2007).
Performance analysis of MPI collective operations.
Cluster Computing, 10:127–143.
Polyak, B.
T.
(1964).
Some methods of speeding up the convergence of iteration meth- ods.
USSR Computational Mathematics and Mathematical Physics, 4(5):1–17.
Schraudolph, N.
N.
(2002).
Fast curvature matrix-vector products for second-order gradient descent.
Neural Computation, 14(7):1723–1738.
Simonyan, K.
and Zisserman, A.
(2014).
Very deep convolutional networks for large- scale image recognition.
arXiv preprint arXiv:1409.1556.
Sutskever, I., Martens, J., Dahl, G., and Hinton, G.
(2013).
On the importance of ini- tialization and momentum in deep learning.
In Proceedings of the 30th International Conference on Machine Learning (ICML), pages 1139–1147.
63 Taylor, G., Burmeister, R., Xu, Z., Singh, B., Patel, A., and Goldstein, T.
(2016).
Train- ing neural networks without gradients: A scalable ADMM approach.
In Proceedings of The Thirty Third International Conference on Machine Learning, pages 2722– 2731.
Thakur, R., Rabenseifner, R., and Gropp, W.
(2005).
Optimization of collective com- munication operations in MPICH.
International Journal of High Performance Com- puting Applications, 19(1):49–66.
Wan, L., Zeiler, M., Zhang, S., LeCun, Y., and Fergus, R.
(2013).
Regularization of neural networks using DropConnect.
In Proceedings of the 30th International Conference on Machine Learning (ICML), pages 1058–1066.
Wang, C.-C., Huang, C.-H., and Lin, C.-J.
(2015).
Subsampled Hessian Newton meth- ods for supervised learning.
Neural Computation, 27:1766–1795.
Zinkevich, M., Weimer, M., Smola, A., and Li, L.
(2010).
Parallelized stochastic gra- dient descent.
In Lafferty, J., Williams, C.
K.
I., Shawe-Taylor, J., Zemel, R., and Culotta, A., editors, Advances in Neural Information Processing Systems 23, pages 2595–2603.
64 Algorithm 1 Function evaluation in a distributed system 1: Let Tm−1 and Tm be the subsets of neurons at the (m − 1)th and mth layers corre- sponding to the current partition.
from input, where i = 1, .
.
.
, l, and t ∈ Tm−1.
2: if m = 1 then Read sm−1,i 3: 4: else 5: Wait for sm−1,i Calculate zm−1,i , i = 1, .
.
.
, l, t ∈ Tm−1.
by (19).
6: 7: end if 8: After calculating (20), run an allreduce operation to have , i = 1, .
.
.
, l and j ∈ Tm, sm,i (27) available in all partitions between layers m − 1 and m corresponding to Tm. 9: if Tm−1 is the ﬁrst neuron sub-group of layer m − 1 then 10: if m < L then 11: 12: 13: 14: We broadcast values in (27) to partitions between layers m and m + 1 corresponding to the neuron subgroup Tm; see the description after (23) else Calculate l(cid:88) (cid:88) i=1 j∈TL ξ(zL,i ; yi j) + accumulated regularization terms If TL is the ﬁrst neuron sub-group of layer L, run a reduce operation to get the ﬁnal f; see (24).
end if 15: 16: end if 65 Algorithm 2 Calculation of ∂zL,i tributed system.
1: Let Tm−1 and Tm be the subsets of neurons at the (m − 1)th and mth layers corre- , u = 1, .
.
.
, nL, j = 1, .
.
.
,|Tm| in a dis- u /∂sm,i sponding to the current partition.
2: if m = L then Calculate 3: ∂zL,i ∂zm,i 2(zL,i u − yi u) if j = u, if j (cid:54)= u, , u = 1, .
.
.
, nL, i = 1, .
.
.
, l, and j ∈ Tm. Wait for ∂zL,i u /∂zm,i 4: else 5: 6: end if 7: Calculate , u = 1, .
.
.
, nL, i = 1, .
.
.
, l, and j ∈ Tm. ∂zL,i ∂sm,i ∂zL,i ∂zm,i σ(cid:48)(sm,i ), u = 1, .
.
.
, nL, i = 1, .
.
.
, l, and j ∈ Tm. (46) (47) (48) 8: if m > 1 then 9: Calculate the local sum (cid:88) j∈Tm tj , t ∈ Tm−1 wm ∂zL,i ∂sm,i and do the reduce operation to obtain ∂zL,i ∂zm−1,i , u = 1, .
.
.
, nL, i = 1, .
.
.
, l, and t ∈ Tm−1.
10: 11: if Tm is the ﬁrst neuron sub-group of layer m then Broadcast values in (48) to partitions between layers m − 2 and m − 1 corresponding to the neuron sub-group Tm−1 at layer m − 1; see the description after (38).
end if 12: 13: end if 66 Algorithm 3 A distributed subsampled Hessian Newton method with variable partition.
1: Given  ∈ (0, 1), λ1, σ ∈ (0, 1), η ∈ (0, 1), CGmax, CGmin, and r ∈ (0, 100].
2: Let p be the index of the current partition and generate the initial local model vector p.
θ1 3: Compute f (θ1).
4: for k = 1, .
.
.
, do 5: Choose a set Sk ⊂ {1, .
.
.
, l}.
p,∀i ∈ Sk. Compute gk Approximately solve the linear system in (60) by CG to obtain a direction dk after p and J i end while Update λk+1 based on (66).
24: 25: end for 67 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: |Sk|(cid:88) I + |Sk| p ≥ CGmax or i=1 is satisﬁed or #CGk ||(λkI + (J i p)T BiJ i p)dk p + gk p|| ≤ σ||gk p|| {# partitions ﬁnished ≥ r% × P and #CGk p ≥ CGmin}, p is the number of CG iterations that have been run so far.
p = β1dk p + β2dk−1 by solving (61).
where #CGk Derive dk αk = 1.
while true do p = θk Update θk+1 if Tm and Tm−1 are the ﬁrst neuron subgroups at layers L and L− 1, respec- p and then compute f (θk+1).
p + αkdk tively, then if (65) is satisﬁed then Notify all partitions to stop.
end if else Wait for the notiﬁcation to stop.
end if if the stop notiﬁcation has been received then break; end if αk = αk/2.
Algorithm 4 Standard stochastic gradient methods 1: Given a learning rate η.
2: for k = 0, .
.
.
do 3: Choose ik ∈ {1, .
.
.
, l}.
θk+1 = θk − η∇f ik(θk).
4: 5: end for Algorithm 5 Mini-batch stochastic gradient methods in Theano/Pylearn2 (Goodfellow et al., 2013).
1: Given epoch = 0, min epochs = 200, a learning rate η, a minimum learning rate ηmin = 10−6, α = 0, r = 0, X = 10−5, N = 10, a batch size b = |Sk| = 100, an initial momentum m0 = 0.9, a ﬁnal momentum mf = 0.99, an exponentially decay factor γ = 1.0000002, and an updating vector v ← 0.
2: counter ← N.
3: lowest value ← ∞.
4: while epoch < min epochs or counter > 0 do 5: Split the whole training data into K disjoint subsets, Sk, k = 1, .
.
.
, K.
α ← min(epoch/min epochs, 1.0).
m ← (1 − α)m0 + αmf.
for k = 1, .
.
.
, K do 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: v ← mv − max(η/γr, ηmin)∇f Sk(θ).
θ ← θ + v.
r ← r + 1.
end for epoch ← epoch + 1.
Calculate the function value h of the validation set.
if (h < (1 − X)× lowest value) then counter ← N.
else counter ← counter − 1.
end if lowest value ← min(lowest value, h).
20: 21: end while 68 (a) SensIT Vehicle (b) poker (c) MNIST (d) Letter 69  70 72 74 76 78 80 82 84 86 0 5 10 15 20 25 30 35 40Testing Accuracy (%)Iterationssubsampled-GNdiagdiag + sync 50%diag + sync 25% 70 72 74 76 78 80 82 84 86 0 500 1000 1500 2000Testing Accuracy (%)Training time in secondssubsampled-GNdiagdiag + sync 50%diag + sync 25% 0 20 40 60 80 100 0 5 10 15 20 25 30 35 40Testing Accuracy (%)Iterationssubsampled-GNdiagdiag + sync 50%diag + sync 25% 0 20 40 60 80 100 0 500 1000 1500 2000Testing Accuracy (%)Training time in secondssubsampled-GNdiagdiag + sync 50%diag + sync 25% 80 85 90 95 100 0 5 10 15 20 25 30 35 40Testing Accuracy (%)Iterationssubsampled-GNdiagdiag + sync 50%diag + sync 25% 80 85 90 95 100 0 5000 10000 15000 20000Testing Accuracy (%)Training time in secondssubsampled-GNdiagdiag + sync 50%diag + sync 25% 0 20 40 60 80 100 0 5 10 15 20 25 30 35 40Testing Accuracy (%)Iterationssubsampled-GNdiagdiag + sync 50%diag + sync 25% 0 20 40 60 80 100 0 1000 2000 3000 4000 5000 6000Testing Accuracy (%)Training time in secondssubsampled-GNdiagdiag + sync 50%diag + sync 25%(e) USPS (f) Pendigits (g) Sensorless (h) Satimage Figure 4: A comparison of different techniques to implement distributed Newton meth- ods.
Left: testing accuracy versus number of iterations.
Right: testing accuracy versus training time.
70  0 20 40 60 80 100 0 5 10 15 20Testing Accuracy (%)Iterationssubsampled-GNdiagdiag + sync 50%diag + sync 25% 0 20 40 60 80 100 0 20 40 60 80 100 120Testing Accuracy (%)Training time in secondssubsampled-GNdiagdiag + sync 50%diag + sync 25% 0 20 40 60 80 100 0 5 10 15 20Testing Accuracy (%)Iterationssubsampled-GNdiagdiag + sync 50%diag + sync 25% 0 20 40 60 80 100 0 20 40 60 80 100 120 140Testing Accuracy (%)Training time in secondssubsampled-GNdiagdiag + sync 50%diag + sync 25% 0 20 40 60 80 100 0 5 10 15 20 25 30 35 40Testing Accuracy (%)Iterationssubsampled-GNdiagdiag + sync 50%diag + sync 25% 0 20 40 60 80 100 0 1000 2000 3000 4000 5000 6000 7000 8000Testing Accuracy (%)Training time in secondssubsampled-GNdiagdiag + sync 50%diag + sync 25% 0 20 40 60 80 100 0 5 10 15 20 25 30Testing Accuracy (%)Iterationssubsampled-GNdiagdiag + sync 50%diag + sync 25% 0 20 40 60 80 100 0 50 100 150 200 250 300 350Testing Accuracy (%)Training time in secondssubsampled-GNdiagdiag + sync 50%diag + sync 25%(a) Dense initialization.
(b) Sparse initialization.
Figure 5: A comparison between SG and Newton.
A 28-300-300-1 network is applied to train HIGGS.
SG-x means that the initial learning rate x is used.
For Newton, each iteration means that we go through line 5 to line 24 in Algorithm 3, while for SG, each iteration means that we go through the whole training data once.
The curve of SG-0.03 in the dense initialization is not presented because the AUC value never exceeds 0.5. Left: AUC versus number of iterations.
Right: AUC versus training time in seconds (log-scaled).
71  0.75 0.78 0.81 0.84 0.87 0 100 200 300 400 500AUCIterationsdiag + sync 50%SG-0.02SG-0.01SG-0.005SG-0.001 0.75 0.78 0.81 0.84 0.87 100 1000 10000 100000AUCTraining time in seconds (log-scaled)diag + sync 50%SG-0.02SG-0.01SG-0.005SG-0.001 0.75 0.78 0.81 0.84 0.87 0 100 200 300 400 500AUCIterationsdiag + sync 50%SG-0.03SG-0.02SG-0.01SG-0.005SG-0.001 0.75 0.78 0.81 0.84 0.87 100 1000 10000 100000AUCTraining time in seconds (log-scaled)diag + sync 50%SG-0.03SG-0.02SG-0.01SG-0.005SG-0.001
Since the early work of Legendre and Gauss in the late XVIII century, linear or nonlinear regression has employed the space deﬁned by the input data to project the target or desired re- sponse and ﬁnd, in a training set, the optimal set of model parameters through mean square error minimization.
This ap- proach has been totally embraced by the adaptive signal pro- cessing [1], control theory, pattern recognition and machine learning communities [2], and has become the de facto stan- dard to perform function approximation.
The pursuit of this alternative is based on theoretic rea- sons, i.e. to expand the horizon of function approximation theory, but its impact on current machine learning applica- tions is perhaps even higher.
In the conventional modeling approach, when the system that created the input-desired data pairs is nonlinear, the linear model must be substituted by a nonlinear model (e.g. artiﬁcial neural networks), which means that optimization becomes nonlinear in the parameters.
Thanks to XYZ agency for funding.
This implies that local minima exist in the performance sur- face, and gradient search techniques become slow, cumber- some and there is no guarantee of ﬁnding the optimal solution.
This is one of the current bottlenecks of nonlinear modeling and machine learning.
All these methods, meanwhile, ignore the error after training the parameter, but the available error information can be better utilized to provide a novel approach to function approximation, as we will demonstrate here.
Our vision is to create universal learning systems that are easy to train and guarantee to converge to the optimum, which we called convex universal learning machines (CULMs) [3].
CULMs are universal mappers with architectures that either do not have a hidden layer or do not need to train the hidden layer weights.
One distinctive class of CULMs are Kernel Adaptive Filters (KAFs) which project the input data into a Reproducing Kernel Hilbert Space (RKHS) by using a strictly positive deﬁnite kernel function and use linear methods to train parameters [4].
The difﬁculty is that when employing the representer theorem in RKHS, the ﬁlter output is computed from all past data, so the ﬁlter computation grows linearly or super-linearly with respect to the sample number n, which is unrealistic for real word applications without sparseness pro- cedures [5].
The other class, including reservoir computing, uses stochastic approaches based on random hidden param- eters exempliﬁed by the Extreme Learning Machine (ELM), and it suffers from incomplete theoretic understanding and re- quires many tricks to achieve useful and reproducible results [6].
In spite of these shortcomings, ELMs are surprisingly very popular, which means that the need to achieve fast uni- versal processing with generalization capacity in function ap- proximation is still unmet.
Here we propose a new solution to design CULMs based on the conventional Finite Impulse Response (FIR) linear model extended with a table lookup.
Instead of using the input to span the projection space, we use the full joint space as the projection space, hence this approach is named Aug- mented Space Linear Model (ASLM).
Augmented with the desired signal, the framework of ASLM expends the data input space, assumed of dimension L, to L + 1 dimensional space.
Then the L + 1 independent bases can span any L + 1 space, which means the training set error can be as small as the adaptation method can achieve using a linear approach.
There are two difﬁculties that need to be addressed in Fig.
1.
The application and the geometric structure of ASLM the ASLM.
The ﬁrst is that all the weights go to zero af- ter training except the one that is connected to the desired, which approaches one.
This means we need regularization during the adaptation process.
The second difﬁculty is we don’t have the desired signal in the test phase! To address these issues here, we use the difference between the outputs in the input space and the desired in the joint space during the training phase (the training error) to augment the input space instead of expanding the input by the desired.
Then, we store all the training errors in a table indexed by the input data.
Our novel solution takes advantage of the extra infor- mation contained in training errors, which are wasted in con- ventional least squares, to approach nonlinear relationships with a linear model and a table.
Since ASLM is an adaptive linear architecture with convex optimization, and the train- ing error is orthogonal to the bases, the adaptation process no longer needs to be regularized.
Meanwhile, the computa- tional complexity of training and testing is much lower than nonlinear methods, which is well suited for online learning algorithms.
As a matter of fact, ASLM is an intermediate so- lution in the complexity-accuracy design space between the linear model (easy but not very accurate) and fully nonlin- ear models (complex but can be much more accurate).
Dif- ferent from the traditional linear and nonlinear models, the augmented space model framework makes full use of training errors, which may improve others models (linear and nonlin- ear) as well.
2.
AUGMENTED SPACE LINEAR MODEL The simplest implementation of the ASLM is presented be- low.
The left part of Fig.1 shows the conceptual least square solution of ﬁnding the best approximation (y) of the desired response (d) by projecting d in the space spanned by the mul- tidimensional input x.
The minimum error is achieved when y is the orthogonal projection of d in the input subspace.
It is sufﬁcient to add the error e to the output y to obtain exactly the desired response d in the training set, because it is by def- inition perpendicular to the input space.
When computing the output of the ASLM, obviously, we are using the joint space to evaluate the desired.
Consider a set of N pairs of training input vectors with desired output {xi, di}N i=1, where i denotes discrete time in- stant.
We ﬁrst compute the weights of the linear model in the input space with all the training data, which can be evaluated by the Least Squares (LS) solution in (1) w =(cid:0)δI + XT X(cid:1)−1 XT d (1) where X = [x1,··· , xN ]T , d = [d1,··· , dN ] and δ is a small value to prevent rank deﬁciency.
Then we create a table ad- dressed by the input which relates the input with the training error, and store this table.
The size of this table will be the training set size if no quantization is introduced.
In the test phase, we use the current input to ﬁnd the closest entry in the table, and then read back the corresponding training set error to approximate the desired response, i.e. equation (2) ˆyi = yi + e∗ = wT xi + e∗ (2) where yi is the current output of LS solution, and e∗ is the er- ror obtained from the training set, which is corresponding to the closest xi in index as we show in right part of Fig.1. The l2 norm is used to measure the distance of transformed sam- ples produced by Hadamard product w◦x.
Considering the inputs of the whole training set, this Hadamard metric can get more reasonable results when ﬁnding the closest sample.
The error e∗ in equation (2) will be a good approximation for the desired under two conditions: (1): e∗ is a good approximation for the current test sample xi from the training error; (2): the error in the test set for a given input remains stationary from training and testing.
The second assumption also must be im- posed in conventional functional approximation, although in ASLM, the requirement applies to instantaneous errors which is more demanding in practical noisy conditions.
In realistic application cases, we can use a quantization approach to cut the noise in the training data while also decreasing the table size.
ASLM is the simplest model in the augmented space, actually, we can also use it to augment the KAF nonlinear model.
Although KAFs are universal nonlinear models, it is Input SpaceAugmented SpaceTrainTeste*xdifﬁcult to achieve a good approximation to the desired by linear combination of Gaussian kernel, because of the rattling and insufﬁcient training data.
In order to compute the training error in the augmented space, we ﬁrst train a nonlinear model as usual, ﬁxed the weights, and compute the training errors all at once.
Then we create a table addressed by the input and store this table as mentioned before.
In the test phase, we use the current input to ﬁnd the closest entry in the table, and then read back the corresponding training error to approximate the desired response, i.e. equation (2).
Since we don’t have w in the nonlinear models, we measure the distance between input samples by l2 norm directly.
In order to improve the efﬁciency of ﬁnding the nearest neighbor, we use a kd tree to store the data in the table with searching complexity of O(log(N )) [7].
Hence, the testing computational complexity in the augmented space model con- sists of 2 parts.
One is the complexity of the algorithm (lin- ear or nonlinear) to compute the system output, and the other is the complexity of searching for the best error of the aug- mented space model.
The testing computational complexity of ASLM, for example, is O(L+log(N )).
As for the training, ASLM is very fast, since it only needs to create the table after the least squares algorithm, which is much faster than train- ing a nonlinear model.
We will compare the performance and computational complexity of the proposed ASLM with sev- eral linear or nonlinear models in the next section.
3.
SIMULATION RESULTS 3.1. Prediction Without Noise In order to evaluate the role of ASLM within the current methodologies for function approximation and system identi- ﬁcation, we select three competing models: the Least Squares (LS) as an example of the optimal linear projection, the Near- est Neighbor (KNN) algorithm [8] as a memory based ap- proach and the Kernel Least Mean Square (KLMS), using a Gaussian kernel, as a CULM that rivals in performance with the best nonlinear networks for prediction.
We also include KLMS with Augmented space model (KLMS-AM), as an extra comparison, to show the general capabilities of the Augmented space model.
All the hyper parameters were validated to get the best possible results including the kernel size σ, the step size η and the regularization factor δ.
For simplicity, we choose K = 1 for the KNN algorithm and all the parameters are showed in the last column of table 1.
The inclusion of the memory based method is judged important because ASLM also uses a table lookup that is similar in spirit to the memory based approaches for modeling.
The problem we selected is the prediction of the of the x component of the Lorenz system [4], which has been well studied in the liter- ature (order L=7 according to Takens embedding theorem) [9].
The Lorenz data set is generated from the differential equation with the parameters β = 8/3, δ = 1, ρ = 28.
A ﬁrst order approximation is used with a step size parameter 0.01.
Segments of 2000 samples are used as the train set and the following 400 samples are the testing set.
We normalize the time series to zero-mean with unit variance.
Performance is measured as the power of the error.
Results are averaged over 50 independent training-test runs obtained by sliding the window over the generated data by 50 samples each time.
Table 1.
The performance comparison of linear and nonlinear algorithm Algorithm Testing MSE Parameter Training MSE KLSM-AM 5.71×10−4 ± 2.83×10−4 2.74×10−3 ± 8.43×10−4 3.13×10−3 ± 6.97×10−4 1.02×10−2 ± 2.79×10−3 2.64×10−1 ± 1.20×10−2 KLMS ASLM KNN LS 2.22×10−3 σ = 1, η = 0.7 K = 1 2.65×10−1 K = 1 K = 1 δ = 0.1 We also show the testing MSE and training MSE in ta- ble 1.
Since KLMS is an online algorithm while the other algorithms are batch based, the testing MSE of KLMS is cal- culated from the last 100 points of the converged learning curve.
In terms of performance, we see that the LS is the worst performer.
Even KNN is better, but we notice that ASLM always improves KNN performance for the same stor- age.
KLMS is a little better than ASLM, which can be further improved by the augmented space model as well.
However, when we take into consideration accuracy and computation time, ASLM appears as a very good compromise between the performance of the nonlinear and the linear model.
Fig.
2 Fig.
2.
The computation time and storage comparison of lin- ear and nonlinear algorithm shows the computation time and storage in the test phase of the compared algorithms.
In terms of simplicity of resources, the LS solution is unbeatable both in terms of storage and computation time.
Compared with KLMS and KLMS-AM, ASLM is much faster with comparable performance, which is much better than the performance of LS algorithm.
The bottleneck of ASLM is the search for the best candidate in the table look up, which is very similar to KNN.
The general locations of linear and nonlinear model with respect to stor- age and computational time in this simulation are plotted as 100102104Storage10-610-410-2100Computation timeLSKNNASLMKLMSKLMS-AMan ellipsoid cloud around points.
It is obvious that ASLM is a linear model with nonlinear regression capacity, and its loca- tion in the Fig.
2 deviates from the diagonal linking the two linear and nonlinear models, which shows its efﬁciency.
In this simulation, the augmented space model also shows sur- prising potential to improve different models, which can make full use of the training error or desired in the augmented space to increase the performance.
Meanwhile, the computational complexity of searching in the table is much smaller than that of KLMS, so the performance improvement won’t bring a big computational burden and explains Fig.
2.
3.2. Prediction With Noise In this section, the data are the same as the last experiment and the desired signal of training data are corrupted by 20 dB zero mean Gaussian noise.
Performance is measured again as the power of the error.
Results are averaged over 50 in- dependent training-test runs obtained by sliding the window over the generated data by 50 samples each time.
The pur- pose of the experiment is to compare the performance when the training set is not clean.
It is obvious that the performance of ASLM will suffer when the noise is added in the table.
Therefore, we use a simple (sequential) Vector Quantization (VQ) method to cut the noise, which can build a small size codebook with a small threshold  instead of the original in- put values in the table.
Depending on the Euclidean distance, VQ is computationally simple (linear complexity in terms of the codebook size) [10, 5].
The error of one center in the codebook is computed by averaging the training errors whose indexes are quantized into the same center.
This method is ﬁrst used in KLMS to constrain the network size.
In ASLM, VQ not only decreases the table size and computational com- plexity, but also improves the performance by averaging the local training errors.
Three extra comparisons are added to show the improvements of algorithms brought by VQ, which is QKLMS, KLMS-QAM and QASLM.
To be fair, the ﬁnal size of codebooks are set to 500, and all the hyper parame- ters are validated to get the best possible results (kernel size σ, step size η, quantization radium  and regularization factor δ).
Table 2.
The performance comparison of linear and nonlinear algorithm Training MSE KNN LS Testing MSE Algorithm KLMS Parameter 8.24×10−3 ± 1.66×10−2 1.65×10−3 σ = 1, η = 0.7 8.28×10−3 ± 1.26×10−3 1.70×10−2 σ = 1, η = 0.7  = 0.085 σ = 1, η = 0.7 QKLMS KLSM-AM 1.04×10−2 ± 7.27×10−4 KLSM-QAM 4.42×10−3 ± 6.48×10−4 8.42×10−3 σ = 1, η = 0.7 K=1,= 0.085 1.32×10−2 ± 1.25×10−3 1.03×10−2 ± 1.20×10−3 1.10×10−2 K=1,= 0.032 2.02×10−2 ± 8.85×10−4 2.64×10−1 ± 1.52×10−2 2.74×10−1 ASLM QASLM K = 1 δ = 0.1 K = 1 K = 1 We show the testing MSE, training MSE and the corre- sponding parameters in the table 2.
It is easy to notice that the linear model shows powerful robustness in this comparison, since the performance of all the algorithms except LS are affected compared with the last result.
ASLM can beat KNN and both algorithms get better results than LS algorithm.
Without the quantization method, KLMS is the best predictor in this experiment, which is more robust than KLMS-AM and ASLM, because the sum of weighted Gaussian kernel can remove the noise to some extent.
VQ reduces the storage for KLMS, KLMS-AM and ASLM algorithms, while it im- proves the performance of KLMS-AM and ASLM by cutting the noise in the table.
However, noise removal by of VQ is not obvious for KLMS algorithm.
Hence, KLMS-QAM shows the best performance with the help of VQ.
QKLMS shows similar result with KLMS, which is still better than QASLM.
4.
CONCLUSION AND DISCUSSION We presented a new solution to the functional approximation problem, by taking advantage of the linear solution, and cor- recting this estimate with the training error from the input sample that is in the neighborhood of the current test phase input.
In essence, we combine the computational efﬁciency of the linear solution with a memory block encoding the training errors originating from the nonlinearity of the data generation process, which produce a nonlinear response.
In conventional nonlinear function approximation, one needs to ﬁnd appro- priate parameters of nonlinear mappers, which is full of dif- ﬁculties and also computationally expensive.
This is the rea- son ASLM displays an interesting compromise in the space of accuracy and computation complexity between the con- ventional linear and nonlinear solutions.
However, noticing that the ASLM only models the linear error in this case, we have also shown that the same approach can improve upon the modeling of the nonlinear error with the KLMS-AM.
ASLM is a member of the CULM family that has not been investigated in the past.
Conceptually, we are proposing to augment the input projection space with the desired response, so this opens the door to study many different implementa- tions of the simple ASLM discussed in this paper.
The thrust of research should focus on ways to improve the table look up performance which is very rudimentary.
In noisy situations, we can think of PCA to obtain a better deﬁnition of the input space, and ﬁlter the training error by local modeling.
In fact, It is very interesting to interpret the training errors as a sensi- tivity to the unknown desired response that can be exploited for Bayesian modeling.
Since we have an implicit model of the input, we can also speed up the search to ﬁnd the closest neighbor of the current input.
These simple modiﬁcations will improve ASLM performance and lead to new applications be- yond functional approximation.
We therefore believe that this will be a vibrant line of research for years to come.
5.
REFERENCES [1] Simon S Haykin, Adaptive ﬁlter theory, Pearson Edu- cation India, 2008.
[2] Richard O Duda, Peter E Hart, and David G Stork, Pat- tern classiﬁcation, John Wiley & Sons, 2012.
[3] Jose C Principe and Badong Chen, “Universal approx- imation with convex optimization: Gimmick or real- ity?[discussion forum],” IEEE Computational Intelli- gence Magazine, vol.
10, no.
2, pp.
68–77, 2015.
[4] Weifeng Liu, Jose C Principe, and Simon Haykin, Ker- nel adaptive ﬁltering: a comprehensive introduction, vol.
57, John Wiley & Sons, 2011.
[5] Weifeng Liu, Puskal P Pokharel, and Jose C Principe, “The kernel least-mean-square algorithm,” IEEE Trans- actions on Signal Processing, vol.
56, no.
2, pp.
543– 554, 2008.
[6] Guang-Bin Huang, Qin-Yu Zhu, and Chee-Kheong Siew, “Extreme learning machine: theory and appli- cations,” Neurocomputing, vol.
70, no.
1, pp.
489–501, 2006.
[7] Hanan Samet, The design and analysis of spatial data structures, vol.
199, Addison-Wesley Reading, MA, 1990.
[8] T.
Cover and P.
Hart, “Nearest neighbor pattern classiﬁ- cation,” IEEE Transactions on Information Theory, vol.
13, no.
1, pp.
21–27, 1967.
[9] Edward N Lorenz, “Deterministic nonperiodic ﬂow,” Journal of the atmospheric sciences, vol.
20, no.
2, pp.
130–141, 1963.
[10] Badong Chen, Songlin Zhao, Pingping Zhu, and Jos´e C “Quantized kernel least mean square algo- IEEE Transactions on Neural Networks and Pr´ıncipe, rithm,” Learning Systems, vol.
23, no.
1, pp.
22–32, 2012.

In survival analysis, the goal is to estimate the occurrence time and the risk of an unfavorable event in the future (e.g, death of a patient) that can inform our decisions at present time (e.g., help to select a treatment).
The classical models for this task are the Aalen’s additive model [1] and the Cox’s proportional hazard model [2], which linearly regress attributes of a patient to the hazard function.
While suitable for comparing populations of patients, these models were not designed for patient-speciﬁc prediction.
By reformulating survival analysis as a multi-task classiﬁcation problem, Yu et al.
[3] show that a set of temporally ordered linear classiﬁers provides much more accurate predictions.
Here, we follow the same classiﬁcation approach and show that using deep learning methods further improves predictive performance on survival data.
While promising, straightforward use of neural networks leads to black-box predictors that lack transparency offered by the linear models.
To overcome this issue, we employ contextual explanation networks [CEN, 4]—a class of models that learn to predict by generating and leveraging intermediate explanations.
Explanations here are deﬁned as instance-speciﬁc simple (linear) models that not only help to interpret predictions but are selected by the network to make predictions for each patient at each time interval.
CENs can be based on arbitrary deep architectures and can process a variety of input data modalities while interpreting predictions in terms of selected attributes.
As we demonstrate in experiments, this approach attains both the best performance as well as interpretability.
2 Background First, we present the setup used by Yu et al.
[3].
The data is represented by patient-speciﬁc attributes, X, and the times of the occurance of event, T.
These times are converted into m-dimensional binary vectors, Y := (y1, .
.
.
, ym), that indicate the corresponding follow up time.
If the death occurred at time t ∈ [ti, ti+1), then yj = 0, ∀j ≤ i and yk = 1, ∀k > i.
If the data point was censored (i.e., Machine Learning for Healthcare (ML4H) Workshop, NIPS 2017, Long Beach, CA, USA.
t ∈ [t2, t3) θ3 θ2 y2 x2 h2 y3 x3 h3 θ1 y1 x1 h1 t ∈ [t2, t3) θ3 θ2 y2 x2 h2 y3 x3 h3 θ1 y1 x1 h1 c1 c2 c3 h1 h2 h3 (a) Architecture used for SUPPORT2 (b) Architecture used for PhysioNet Figure 1: CEN architectures used in our survival analysis experiments.
Context encoders were time-distributed single hidden layer MLP (a) and LSTM (b) that produced inputs for another LSTM over the output time intervals (denoted with h1, h2, h3 hidden states respectively).
Each hidden state of the output LSTM was used to generate the corresponding θt that were further used to construct the log-likelihood for CRF.
we lack information for times after t ∈ [ti, ti+1)), the targets (yi+1, .
.
.
, ym) are regarded as latent variables.
Note that only m + 1 sequences are valid, i.e., assigned non-zero probability by the model, which allows to write the following linear model: p(Y = (y1, .
.
.
, ym) | x, Θ) = exp(cid:0)(cid:80)m k=0 exp(cid:0)(cid:80)m (cid:80)m t=1 ytx(cid:62)θt(cid:1) t=k+1 x(cid:62)θt(cid:1) The model is trained by optimizing a regularized log likelihood w.r.t. Θ := {θt}m t=1.
After training, we get a set of linear models, one for each time interval, used for predicting the survival probability.
(1) 3 Contextual Explanation Networks for Survival Analysis Here, we take the same structured prediction approach but consider a slightly different setup.
In particular, we assume that each data instance (patient record) is represented by three variables: the context, C, the attributes, X, and the targets, Y.
Our goal is to learn a model, pw(Y | X, C), parametrized by w that can predict Y from X and C.
Note that inputs have two representations, X and C, where X is a set of attributes that will be used to interpret predictions1.
Contextual explanation networks (CENs) are deﬁned as models that assume the following form: Y ∼ p(Y | X, θ), θ ∼ pw(θ | C), pw(Y | X, C) = (2) where p(Y | X, θ) is a predictor parametrized by θ.
Such predictors are called explanations, since they explicitly relate interpretable variables, X, to the targets, Y.
The conditional distribution pw(θ | C) is called the context encoder processes the context representation, C, and generates parameters for the explanation, θ.
For survival analysis, we want explanations to be in the form of linear CRFs as given in (1).
Hence, our contextual networks with CRF-based explanations are deﬁned as follows: p(Y | X, θ)pw(θ | C)dθ (cid:90) θt ∼ pw(θt | C), t ∈ {1, .
.
.
, m}, Y ∼ p(Y | X, θ1:m), p(Y = (y1, y2, .
.
.
, ym) | x, θ1:m) ∝ exp pw(θt | C) := δ(θt, φt w,D(c)), φt t=1 (cid:40) m(cid:88) yi(x(cid:62)θt) + ω(yt, yt+1) (3) w,D(c) := α(ht)(cid:62)D, ht := RNN(ht−1, c) (cid:41) A few things to note here.
First, the model generates explanations for each patient and for each time interval.
Second, depending on the nature of the context representation, C, CENs process it 1It is common to have the data to be of multiple representations some of which are low-level or unstructured (e.g., image pixels, sensory inputs), and other are high-level or human-interpretable (e.g., categorical variables).
To ensure interpretability, we would like to use deep networks to process the low-level representation (the context) and construct explanations as context-speciﬁc probabilistic models on the high-level features.
and generate θt for each time step using a recurrent encoder (Figure 1).
We use a deterministic RNN-based encoder, φt, that selects parameters for explanations from a global dictionary, D, using soft attention (for details on dictionary-based context encoding, see [4]).
Finally, the potentials between attributes, x, and targets, y1:m, are linear functions parameterized by θ1:m; the pairwise potentials between targets, ω(yi, yi+1), ensure that conﬁgurations (yi = 1, yi+1 = 0) are improbable (i.e., ω(1, 0) = −∞ and ω(0, 0) = ω00, ω(0, 1) = ω01, ω(1, 1) = ω10 are learnable parameters).
Given these constraints, the likelihood of an uncensored event at time t ∈ [tj, tj+1) is (cid:41) x(cid:62)θi (cid:40) m(cid:88) (cid:40) m(cid:88) i=k+1 exp (cid:41) x(cid:62)θi  m(cid:88) (cid:40) m(cid:88) i=j  k=0 exp (cid:44) m(cid:88) (cid:41)(cid:44) m(cid:88) (cid:88) k=0 j∈C m(cid:88) (cid:88) i∈NC p(T = t | x, Θ) = exp x(cid:62)θi and the likelihood of an event censored at time t ∈ [tj, tj+1) is p(T ≥ t | x, Θ) = exp x(cid:62)θi k=j+1 i=k+1 i=k+1 The joint log-likelihood of the data consists of two parts: p(T = ti | xi, Θ) + L(Y, X; Θ) = p(T > tj | xj, Θ) (4) (5) (6) where NC is the set of non-censored instances (for which we know the outcome times, ti) and C is the set of censored instances (for which only know the censorship times, tj).
The objective is optimized using stochastic gradient descent.
See [4] for more details.
4 Experiments In our experiments, we consider the datasets, models, and metrics as described below.
We compare CENs with a number of baselines quantitatively as well as visualize the learned explanations.
Datasets.
We use two publicly available datasets for survival analysis of of the intense care unit (ICU) patients: (a) SUPPORT22, and (b) data from the PhysioNet 2012 challenge3.
The data was preprocessed and used as follows: • SUPPORT2: The data had 9105 patient records and 73 variables.
We selected 50 variables for both C and X features.
Categorical features (such as race or sex) were one-hot encoded.
The values of all features were non-negative, and we ﬁlled the missing values with -1.
For CRF-based predictors, the survival timeline was capped at 3 years and converted into 156 discrete intervals of 7 days each.
We used 7105 patient records for training, 1000 for validation, and 1000 for testing.
• PhysioNet: The data had 4000 patient records, each represented by a 48-hour irregularly sampled 37-dimensional time-series of different measurements taken during the patient’s stay at the ICU.
We resampled and mean-aggregated the time-series at 30 min frequency.
This resulted in a large number of missing values that we ﬁlled with 0.
The resampled time-series were used as the context, C, while for the attributes, X, we took the values of the last available measurement for each variable in the series.
For CRF-based predictors, the survival timeline was capped at 60 days and converted into 60 discrete intervals.
Models.
For baselines, we use the classical Aalen and Cox models and the CRF from [3], where all used X as inputs.
Next, we combine CRFs with neural encoders in two ways: (i) We apply CRFs to the outputs from the neural encoders (denoted MLP-CRF and LSTM-CRF, all trainable end-to-end).
Similar models have been show very successful in the natural language applications [5].
Note that parameters of the CRF layer assign weights to the latent features and are no longer interpretable in terms of the attributes of interest.
(ii) We use CENs with CRF-based explanations, that process the context variables, C, using the same neural networks as in (i) and output parameters for CRFs that act on the attributes, X.
2http://biostat.mc.vanderbilt.edu/wiki/Main/DataSets.
3https://physionet.org/challenge/2012/.
Table 1: Performance of the classical Cox and Aalen models, CRF-based models, and CENs that use LSTM or MLP for context embedding and CRF for explanations.
The numbers are averages from 5-fold cross-validation; the std.
are on the order of the least signiﬁcant digit.
@K denotes the temporal quantile, i.e., the time point such that K% of the patients in the data have died or were censored before that point.
SUPPORT2 PhysioNet Challenge 2012 Model Acc@25 Acc@50 Acc@75 RAE Model Acc@25 Acc@50 Acc@75 RAE Cox Aalen CRF MLP-CRF MLP-CEN 84.1 87.1 84.4 87.7 85.5 73.7 66.2 89.3 89.6 47.6 45.8 79.2 80.1 0.90 0.98 0.59 0.62 Cox Aalen CRF LSTM-CRF 93.0 93.3 93.2 93.9 69.6 78.7 85.1 86.3 49.1 57.1 65.6 68.1 0.24 0.31 0.14 0.11 90.8 81.9 0.56 LSTM-CEN 94.8 87.5 70.1 0.09 Figure 2: Weights of the CEN-generated CRF explanations for two patients from SUPPORT2 dataset for a set of the most inﬂuential features: dementia (comorbidity), avtisst (avg.
TISS, days 3-25), slos (days from study entry to discharge), hday (day in hospital at study admit), ca yes (the patient had cancer), sfdm2 Coma or Intub (intubated or in coma at month 2), sfdm2 SIP (sickness impact proﬁle score at month 2).
Higher weight values correspond to higher feature contributions to the risk of death after a given time point.
Metrics.
Following Yu et al.
[3], we use two metrics speciﬁc to survival analysis: (a) accuracy of correctly predicting survival of a patient at times that correspond to 25%, 50%, and 75% population-level temporal quantiles (i.e., time points such that the corresponding percentage of the patients in the data had their time of the last follow up prior to that due to censorship or death) and (b) the relative absolute error (RAE) between the predicted and actual time of death for non-censored patients.
Quantitative results.
The results for all models are given in Table 1.
Our implementation of the CRF baseline reproduces (and even slightly improves) the performance reported by Yu et al.
[3].
CRFs built on representations learned by deep networks (MLP-CRF and LSTM-CRF models) improve upon the plain CRFs but, as we noted, can no longer be interpreted in terms of the original variables.
On the other hand, CENs outperform neural CRF models on certain metrics (and closely match on the others) while providing explanations for the survival Figure 3: CEN-predicted survival curves for 500 random patients from SUP- PORT2 test set.
Color indicates death within 1 year after leaving the hospital.
probability predictions for each patient at each point in time.
Qualitative results.
To inspect predictions of CENs qualitatively, for any given patient, we can visualize the weights assigned by the corresponding explanation to the respective attributes at each time interval.
Figure 2 shows explanation weights for a subset of the most inﬂuential features for two patients from SUPPORT2 dataset who were predicted as survivor and non-survivor.
These explanations allow us to better understand patient-speciﬁc temporal dynamics of the contributing factors to the survival rates predicted by the model (Figure 3).
This information can be used for model diagnostics (i.e., help us understand whether we can trust a particular prediction) and as more ﬁne-grained information useful for decision support.
01020304050Time after leaving hospital (weeks)sfdm2_SIP>=30sfdm2_Coma or Intubca_yeshdayslosavtisstdementiaPatient ID: 3520 (Died)01020304050Time after leaving hospital (weeks)Patient ID: 1100 (Survived)4202402550Time after leaving hospital (weeks)0.00.20.40.60.81.0Survival probabilitySurvivedDiedReferences [1] O.O. Aalen.
“A linear regression model for the analysis of life time”.
In: Statistics in Medicine, 8(8):907–925 (1989).
[2] DR Cox.
“Regression Models and Life-Tables”.
In: Journal of the Royal Statistical Society.
Series B (Methodological) (1972), pp.
187–220.
[3] Chun-Nam Yu, Russell Greiner, Hsiu-Chin Lin & Vickie Baracos.
“Learning patient-speciﬁc cancer survival distributions as a sequence of dependent regressors”.
In: Advances in Neural Information Processing Systems.
2011, pp.
1845–1853.
[4] Maruan Al-Shedivat, Avinava Dubey & Eric P Xing.
“Contextual Explanation Networks”.
In: arXiv preprint arXiv:1705.10301 (2017).
[5] Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu & Pavel Kuksa.
“Natural language processing (almost) from scratch”.
In: Journal of Machine Learning Research 12.Aug (2011).

Graphs are universal representations of pairwise relationship.
Many real world data come naturally in the form of graphs; e.g., social networks, gene expression networks, and knowledge graphs.
To improve the performance of graph-based learning tasks, such as node classiﬁcation and link prediction, recently much effort is made to extend well-established network architectures, including recurrent neural networks (RNN) and convolutional neural networks (CNN), to graph data; see, e.g., Bruna et al.
(2013); Duvenaud et al.
(2015); Li et al.
(2015); Jain et al.
(2015); Henaff et al.
(2015); Niepert et al.
(2016); Kipf & Welling (2016a;b).
Whereas learning feature representations for graphs is an important subject among this effort, here, we focus on the feature representations for graph vertices.
In this vein, the closest work that applies a convolution architecture is the graph convolutional network (GCN) (Kipf & Welling, 2016a;b).
Borrowing the concept of a convolution ﬁlter for image pixels or a linear array of signals, GCN uses the connectivity structure of the graph as the ﬁlter to perform neighborhood mixing.
The architecture may be elegantly summarized by the following expression: H (l+1) = σ( ˆAH (l)W (l)), where ˆA is some normalization of the graph adjacency matrix, H (l) contains the embedding (row- wise) of the graph vertices in the lth layer, W (l) is a parameter matrix, and σ is nonlinearity.
As with many graph algorithms, the adjacency matrix encodes the pairwise relationship for both training and test data.
The learning of the model as well as the embedding is performed for both data simultaneously, at least as the authors proposed.
For many applications, however, test data may not be readily available, because the graph may be constantly expanding with new vertices (e.g. new members of a social network, new products to a recommender system, and new drugs for functionality tests).
Such scenarios require an inductive scheme that learns a model from only a training set of vertices and that generalizes well to any augmentation of the graph.
∗These two authors contribute equally.
Published as a conference paper at ICLR 2018 A more severe challenge for GCN is that the recursive expansion of neighborhoods across layers incurs expensive computations in batched training.
Particularly for dense graphs and powerlaw graphs, the expansion of the neighborhood for a single vertex quickly ﬁlls up a large portion of the graph.
Then, a usual mini-batch training will involve a large amount of data for every batch, even with a small batch size.
Hence, scalability is a pressing issue to resolve for GCN to be applicable to large, dense graphs.
To address both challenges, we propose to view graph convolutions from a different angle and interpret them as integral transforms of embedding functions under probability measures.
Such a view provides a principled mechanism for inductive learning, starting from the formulation of the loss to the stochastic version of the gradient.
Speciﬁcally, we interpret that graph vertices are iid samples of some probability distribution and write the loss and each convolution layer as integrals with respect to vertex embedding functions.
Then, the integrals are evaluated through Monte Carlo approximation that deﬁnes the sample loss and the sample gradient.
One may further alter the sampling distribution (as in importance sampling) to reduce the approximation variance.
The proposed approach, coined FastGCN, not only rids the reliance on the test data but also yields a controllable cost for per-batch computation.
At the time of writing, we notice a newly published work GraphSAGE (Hamilton et al., 2017) that proposes also the use of sampling to reduce the computational footprint of GCN.
Our sampling scheme is more economic, resulting in a substantial saving in the gradient computation, as will be analyzed in more detail in Section 3.3. Experimental results in Section 4 indicate that the per-batch computation of FastGCN is more than an order of magnitude faster than that of GraphSAGE, while classiﬁcation accuracies are highly comparable.
2 RELATED WORK Over the past few years, several graph-based convolution network models emerged for address- ing applications of graph-structured data, such as the representation of molecules (Duvenaud et al., 2015).
An important stream of work is built on spectral graph theory (Bruna et al., 2013; Henaff et al., 2015; Defferrard et al., 2016).
They deﬁne parameterized ﬁlters in the spectral domain, in- spired by graph Fourier transform.
These approaches learn a feature representation for the whole graph and may be used for graph classiﬁcation.
Another line of work learns embeddings for graph vertices, for which Goyal & Ferrara (2017) is a recent survey that covers comprehensively several categories of methods.
A major category consists of factorization based algorithms that yield the embedding through matrix factorizations; see, e.g., Roweis & Saul (2000); Belkin & Niyogi (2001); Ahmed et al.
(2013); Cao et al.
(2015); Ou et al.
(2016).
These methods learn the representations of training and test data jointly.
Another category is random walk based methods (Perozzi et al., 2014; Grover & Leskovec, 2016) that compute node representations through exploration of neighborhoods.
LINE (Tang et al., 2015) is also such a tech- nique that is motivated by the preservation of the ﬁrst and second-order proximities.
Meanwhile, there appear a few deep neural network architectures, which better capture the nonlinearity within graphs, such as SDNE (Wang et al., 2016).
As motivated earlier, GCN (Kipf & Welling, 2016a) is the model on which our work is based.
The most relevant work to our approach is GraphSAGE (Hamilton et al., 2017), which learns node representations through aggregation of neighborhood information.
One of the proposed aggregators employs the GCN architecture.
The authors also acknowledge the memory bottleneck of GCN and hence propose an ad hoc sampling scheme to restrict the neighborhood size.
Our sampling approach is based on a different and more principled formulation.
The major distinction is that we sample vertices rather than neighbors.
The resulting computational savings are analyzed in Section 3.3. 3 TRAINING AND INFERENCE THROUGH SAMPLING One striking difference between GCN and many standard neural network architectures is the lack of independence in the sample loss.
Training algorithms such as SGD and its batch generalization are designed based on the additive nature of the loss function with respect to independent data samples.
For graphs, on the other hand, each vertex is convolved with all its neighbors and hence deﬁning a sample gradient that is efﬁcient to compute is beyond straightforward.
Published as a conference paper at ICLR 2018 Concretely, consider the standard SGD scenario where the loss is the expectation of some function g with respect to a data distribution D: L = Ex∼D[g(W ; x)].
Here, W denotes the model parameter to be optimized.
Of course, the data distribution is generally unknown and one instead minimizes the empirical loss through accessing n iid samples x1, .
.
.
, xn: n(cid:88) i=1 Lemp = g(W ; xi), xi ∼ D, ∀ i.
In each step of SGD, the gradient is approximated by ∇g(W ; xi), an (assumed) unbiased sample of ∇L.
One may interpret that each gradient step makes progress toward the sample loss g(W ; xi).
The sample loss and the sample gradient involve only one single sample xi.
For graphs, one may no longer leverage the independence and compute the sample gradient ∇g(W ; xi) by discarding the information of i’s neighboring vertices and their neighbors, recur- sively.
We therefore seek an alternative formulation.
In order to cast the learning problem under the same sampling framework, let us assume that there is a (possibly inﬁnite) graph G(cid:48) with the vertex set V (cid:48) associated with a probability space (V (cid:48), F, P ), such that for the given graph G, it is an induced subgraph of G(cid:48) and its vertices are iid samples of V (cid:48) according to the probability measure P .
For the probability space, V (cid:48) serves as the sample space and F may be any event space (e.g., the power set F = 2V (cid:48) To resolve the problem of lack of independence caused by convolution, we interpret that each layer of the network deﬁnes an embedding function of the vertices (random variable) that are tied to the same probability measure but are independent.
See Figure 1.
Speciﬁcally, recall the architecture of GCN ).
The probability measure P deﬁnes a sampling distribution.
˜H (l+1) = ˆAH (l)W (l), H (l+1) = σ( ˜H (l+1)), l = 0, .
.
.
, M − 1, L = (cid:90) For the functional generalization, we write ˜h(l+1)(v) = ˆA(v, u)h(l)(u)W (l) dP (u), h(l+1)(v) = σ(˜h(l+1)(v)), n(cid:88) g(H (M )(i, :)).
i=1 (1) l = 0, .
.
.
, M − 1, (2) (3) L = Ev∼P [g(h(M )(v))] = g(h(M )(v)) dP (v).
Here, u and v are independent random variables, both of which have the same probability measure P .
The function h(l) is interpreted as the embedding function from the lth layer.
The embedding functions from two consecutive layers are related through convolution, expressed as an integral transform, where the kernel ˆA(v, u) corresponds to the (v, u) element of the matrix ˆA.
The loss is the expectation of g(h(M )) for the ﬁnal embedding h(M ).
Note that the integrals are not the usual Riemann–Stieltjes integrals, because the variables u and v are graph vertices but not real numbers; however, this distinction is only a matter of formalism.
Writing GCN in the functional form allows for evaluating the integrals in the Monte Carlo manner, which leads to a batched training algorithm and also to a natural separation of training and test data, ∼ P to approximately as in inductive learning.
For each layer l, we use tl iid samples u(l) evaluate the integral transform (2); that is, 1 , .
.
.
, u(l) tl ˆA(v, u(l) j )h(l) tl (u(l) j )W (l), h(l+1) tl+1 (v) := σ(˜h(l+1) tl+1 (v)), l = 0, .
.
.
, M − 1, tl(cid:88) j=1 ˜h(l+1) tl+1 (v) := tl with the convention h(0) t0 ≡ h(0).
Then, the loss L in (3) admits an estimator (cid:90) tM(cid:88) i=1 Lt0,t1,...,tM := tM g(h(M ) tM (u(M ) )).
The follow result establishes that the estimator is consistent.
The proof is a recursive application of the law of large numbers and the continuous mapping theorem; it is given in the appendix.
Published as a conference paper at ICLR 2018 Figure 1: Two views of GCN.
On the left (graph convolution view), each circle represents a graph vertex.
On two consecutive rows, a circle i is connected (in gray line) with circle j if the two cor- responding vertices in the graph are connected.
A convolution layer uses the graph connectivity structure to mix the vertex features/embeddings.
On the right (integral transform view), the embed- ding function in the next layer is an integral transform (illustrated by the orange fanout shape) of the one in the previous layer.
For the proposed method, all integrals (including the loss function) are evaluated by using Monte Carlo sampling.
Correspondingly in the graph view, vertices are subsam- pled in a bootstrapping manner in each layer to approximate the convolution.
The sampled portions are collectively denoted by the solid blue circles and the orange lines.
Theorem 1.
If g and σ are continuous, then t0,t1,...,tM→∞ Lt0,t1,...,tM = L with probability one.
lim In practical use, we are given a graph whose vertices are already assumed to be samples.
Hence, we will need bootstrapping to obtain a consistent estimate.
In particular, for the network architecture (1), the output H (M ) is split into batches as usual.
We will still use u(M ) tM to denote a batch of vertices, which come from the given graph.
For each batch, we sample (with replacement) uniformly , i = 1, .
.
.
, tl, l = 0, .
.
.
, M − 1.
Such a procedure is equivalent each layer and obtain samples u(l) to uniformly sampling the rows of H (l) for each l.
Then, we obtain the batch loss , .
.
.
, u(M ) tM(cid:88) i=1 Lbatch = tM g(H (M )(u(M ) , :)), where, recursively, H (l+1)(v, :) = σ  n tl tl(cid:88) j=1 ˆA(v, u(l) j )H (l)(u(l) j , :)W (l) (4) (5) l = 0, .
.
.
, M − 1.
 , Here, the n inside the activation function σ is the number of vertices in the given graph and is used to account for the normalization difference between the matrix form (1) and the integral form (2).
The corresponding batch gradient may be straightforwardly obtained through applying the chain rule on each H (l).
See Algorithm 1.
3.1 VARIANCE REDUCTION tion(cid:82) ˆA(v, u)h(l) As for any estimator, one is interested in improving its variance.
Whereas computing the full variance is highly challenging because of nonlinearity in all the layers, it is possible to consider each single layer and aim at improving the variance of the embedding function before nonlinearity.
Speciﬁcally, consider for the lth layer, the function ˜h(l+1) (v) as an approximation to the convolu- tl+1 (u)W (l) dP (u).
When taking tl+1 samples v = u(l+1) tl+1 , the sample tl average of ˜h(l+1) (v) admits a variance that captures the deviation from the eventual loss contributed tl+1 by this layer.
Hence, we seek an improvement of this variance.
Now that we consider each layer separately, we will do the following change of notation to keep the expressions less cumbersome: , .
.
.
, u(l+1) Graph	convolution	viewIntegral	transform	viewbatchH(2)H(1)H(0)h(2)(v)h(1)(v)h(0)(v)Published as a conference paper at ICLR 2018 Algorithm 1 FastGCN batched training (one epoch) 1: for each batch do 2: 3: 4: For each layer l, sample uniformly tl vertices u(l) for each layer l do If v is sampled in the next layer, ∇ ˜H (l+1)(v, :) ← n tl end for W ← W − η∇Lbatch 5: 6: 7: end for 1 , .
.
.
, u(l) tl (cid:46) Compute batch gradient ∇Lbatch j )∇(cid:110) j , :)W (l)(cid:111) ˆA(v, u(l) H (l)(u(l) tl(cid:88) j=1 (cid:46) SGD step Num.
samples tl+1 → s tl → t Layer l + 1; random variable v Samples i → vi u(l+1) j → uj u(l) Under the joint distribution of v and u, the aforementioned sample average is Function (v) → y(v) (u)W (l) → x(u) Layer l; random variable u ˜h(l+1) tl+1 h(l) tl s(cid:88) i=1 s(cid:88)  1 t(cid:88) i=1 j=1 G := y(vi) = ˆA(vi, uj)x(uj)  .
First, we have the following result.
Proposition 2.
The variance of G admits (cid:18) where R = 1 − 1 (cid:19)(cid:90) Var{G} = R + e(v)2 dP (v) − 1 st (cid:90)(cid:90) (cid:18)(cid:90) ˆA(v, u)2x(u)2 dP (u) dP (v), (6) (cid:19)2 (cid:90) and e(v) = ˆA(v, u)x(u) dP (u).
e(v) dP (v) The variance (6) consists of two parts.
The ﬁrst part R leaves little room for improvement, because the sampling in the v space is not done in this layer.
The second part (the double integral), on the other hand, depends on how the uj’s in this layer are sampled.
The current result (6) is the consequence of sampling uj’s by using the probability measure P .
One may perform importance sampling, altering the sampling distribution to reduce variance.
Speciﬁcally, let Q(u) be the new probability measure, where the uj’s are drawn from.
We hence deﬁne the new sample average approximation yQ(v) := ˆA(v, uj)x(uj) dP (u) dQ(u) u1, .
.
.
, ut ∼ Q, and the quantity of interest j=1 yQ(vi) = s(cid:88) i=1 s(cid:88) i=1 GQ := (cid:32) (cid:33) .
(cid:12)(cid:12)(cid:12)(cid:12)uj dP (u) dQ(u) ˆA(vi, uj)x(uj) (cid:33) (cid:12)(cid:12)(cid:12)(cid:12)uj (cid:32)  1 t(cid:88) j=1 t(cid:88) Clearly, the expectation of GQ is the same as that of G, regardless of the new measure Q.
The following result gives the optimal Q.
Theorem 3.
If (cid:20)(cid:90) (cid:21) 1 ˆA(v, u)2 dP (v) (7) b(u)|x(u)| dP (u) (cid:82) b(u)|x(u)| dP (u) dQ(u) = where b(u) = Published as a conference paper at ICLR 2018 then the variance of GQ admits Var{GQ} = R + st (cid:20)(cid:90) (cid:21)2 b(u)|x(u)| dP (u) (8) where R is deﬁned in Proposition 2.
The variance is minimum among all choices of Q.
A drawback of deﬁning the sampling distribution Q in this manner is that it involves |x(u)|, which constantly changes during training.
It corresponds to the product of the embedding matrix H (l) and the parameter matrix W (l).
The parameter matrix is updated in every iteration; and the matrix product is expensive to compute.
Hence, the cost of computing the optimal measure Q is quite high.
As a compromise, we consider a different choice of Q, which involves only b(u).
The following proposition gives the precise deﬁnition.
The resulting variance may or may not be smaller than (6).
In practice, however, we ﬁnd that it is almost always helpful.
Proposition 4.
If dQ(u) = b(u)2 dP (u) (cid:82) b(u)2 dP (u) (cid:90) b(u)2 dP (u) (cid:90) where b(u) is deﬁned in (7), then the variance of GQ admits Var{GQ} = R + st where R is deﬁned in Proposition 2.
x(u)2 dP (u), (9) With this choice of the probability measure Q, the ratio dQ(u)/dP (u) is proportional to b(u)2, which is simply the integral of ˆA(v, u)2 with respect to v.
In practical use, for the network architec- ture (1), we deﬁne a probability mass function for all the vertices in the given graph: q(u) = (cid:107) ˆA(:, u)(cid:107)2/ (cid:107) ˆA(:, u(cid:48))(cid:107)2, u ∈ V (cid:88) u(cid:48)∈V  1 tl tl(cid:88) and sample t vertices u1, .
.
.
, ut according to this distribution.
From the expression of q, we see that it has no dependency on l; that is, the sampling distribution is the same for all layers.
To summarize, the batch loss Lbatch in (4) now is recursively expanded as H (l+1)(v, :) = σ ˆA(v, u(l) j )H (l)(u(l) j , :)W (l) j=1 q(u(l) j ) j ∼ q, u(l) l = 0, .
.
.
, M − 1.
(10) The major difference between (5) and (10) is that the former obtains samples uniformly whereas the latter according to q.
Accordingly, the scaling inside the summation changes.
The corresponding batch gradient may be straightforwardly obtained through applying the chain rule on each H (l).
See Algorithm 2.
 , Algorithm 2 FastGCN batched training (one epoch), improved version 1: For each vertex u, compute sampling probability q(u) ∝ (cid:107) ˆA(:, u)(cid:107)2 2: for each batch do 3: 4: 5: For each layer l, sample tl vertices u(l) for each layer l do If v is sampled in the next layer, 1 , .
.
.
, u(l) ∇ ˜H (l+1)(v, :) ← 1 tl end for W ← W − η∇Lbatch 6: 7: 8: end for tl according to distribution q (cid:46) Compute batch gradient ∇Lbatch ∇(cid:110) j , :)W (l)(cid:111) H (l)(u(l) (cid:46) SGD step tl(cid:88) j=1 ˆA(v, u(l) j ) q(u(l) j ) Published as a conference paper at ICLR 2018 3.2 INFERENCE The sampling approach described in the preceding subsection clearly separates out test data from training.
Such an approach is inductive, as opposed to transductive that is common for many graph algorithms.
The essence is to cast the set of graph vertices as iid samples of a probability distribution, so that the learning algorithm may use the gradient of a consistent estimator of the loss to perform parameter update.
Then, for inference, the embedding of a new vertex may be either computed by using the full GCN architecture (1), or approximated through sampling as is done in parameter learning.
Generally, using the full architecture is more straightforward and easier to implement.
3.3 COMPARISON WITH GRAPHSAGE GraphSAGE (Hamilton et al., 2017) is a newly proposed architecture for generating vertex embed- dings through aggregating neighborhood information.
It shares the same memory bottleneck with GCN, caused by recursive neighborhood expansion.
To reduce the computational footprint, the au- thors propose restricting the immediate neighborhood size for each layer.
Using our notation for the sample size, if one samples tl neighbors for each vertex in the lth layer, then the size of the expanded neighborhood is, in the worst case, the product of the tl’s.
On the other hand, FastGCN samples vertices rather than neighbors in each layer.
Then, the total number of involved vertices is at most the sum of the tl’s, rather than the product.
See experimental results in Section 4 for the order-of-magnitude saving in actual computation time.
4 EXPERIMENTS We follow the experiment setup in Kipf & Welling (2016a) and Hamilton et al.
(2017) to demon- strate the effective use of FastGCN, comparing with the original GCN model as well as Graph- SAGE, on the following benchmark tasks: (1) classifying research topics using the Cora citation data set (McCallum et al., 2000); (2) categorizing academic papers with the Pubmed database; and (3) predicting the community structure of a social network modeled with Reddit posts.
These data sets are downloaded from the accompany websites of the aforementioned references.
The graphs have increasingly more nodes and higher node degrees, representative of the large and dense set- ting under which our method is motivated.
Statistics are summarized in Table 1.
We adjusted the training/validation/test split of Cora and Pubmed to align with the supervised learning scenario.
Speciﬁcally, all labels of the training examples are used for training, as opposed to only a small portion in the semi-supervised setting (Kipf & Welling, 2016a).
Such a split is coherent with that of the other data set, Reddit, used in the work of GraphSAGE.
Additional experiments using the original split of Cora and Pubmed are reported in the appendix.
Table 1: Dataset Statistics Dataset Cora Pubmed Reddit Nodes 2, 708 19, 717 232, 965 Edges 5, 429 44, 338 11, 606, 919 Classes 41 Features 1, 433 500 602 Training/Validation/Test 1, 208/500/1, 000 18, 217/500/1, 000 152, 410/23, 699/55, 334 Implementation details are as following.
All networks (including those under comparison) contain two layers as usual.
The codes of GraphSAGE and GCN are downloaded from the accompany websites and the latter is adapted for FastGCN.
Inference with FastGCN is done with the full GCN network, as mentioned in Section 3.2. Further details are contained in the appendix.
We ﬁrst consider the use of sampling in FastGCN.
The left part of Table 2 (columns under “Sam- pling”) lists the time and classiﬁcation accuracy as the number of samples increases.
For illustration purpose, we equalize the sample size on both layers.
Clearly, with more samples, the per-epoch training time increases, but the accuracy (as measured by using micro F1 scores) also improves generally.
An interesting observation is that given input features H (0), the product ˆAH (0) in the bottom layer does not change, which means that the chained expansion of the gradient with respect to W (0) in Published as a conference paper at ICLR 2018 Table 2: Beneﬁt of precomputing ˆAH (0) for the input layer.
Data set: Pubmed.
Train- ing time is in seconds, per-epoch (batch size 1024).
Accuracy is measured by using micro F1 score.
Sampling F1 t1 10 25 50 Time 0.737 0.755 0.760 0.774 0.859 0.863 0.873 0.864 Precompute Time F1 0.139 0.141 0.144 0.142 0.849 0.870 0.879 0.880 Figure 2: Prediction accuracy: uniform versus impor- tance sampling.
The three data sets from top to bottom are ordered the same as Table 1.
the last step is a constant throughout training.
Hence, one may precompute the product rather than sampling this layer to gain efﬁciency.
The compared results are listed on the right part of Table 2 (columns under “Precompute”).
One sees that the training time substantially decreases while the accuracy is comparable.
Hence, all the experiments that follow use precomputation.
Next, we compare the sampling approaches for FastGCN: uniform and importance sampling.
Fig- ure 2 summarizes the prediction accuracy under both approaches.
It shows that importance sampling consistently yields higher accuracy than does uniform sampling.
Since the altered sampling distri- bution (see Proposition 4 and Algorithm 2) is a compromise alternative of the optimal distribution that is impractical to use, this result suggests that the variance of the used sampling indeed is smaller than that of uniform sampling; i.e., the term (9) stays closer to (8) than does (6).
A possible reason is that b(u) correlates with |x(u)|.
Hence, later experiments will apply importance sampling.
We now demonstrate that the proposed method is signiﬁcantly faster than the original GCN as well as GraphSAGE, while maintaining comparable prediction performance.
See Figure 3.
The bar heights indicate the per-batch training time, in the log scale.
One sees that GraphSAGE is a sub- stantial improvement of GCN for large and dense graphs (e.g., Reddit), although for smaller ones (Cora and Pubmed), GCN trains faster.
FastGCN is the fastest, with at least an order of magnitude improvement compared with the runner up (except for Cora), and approximately two orders of mag- nitude speed up compared with the slowest.
Here, the training time of FastGCN is with respect to the sample size that achieves the best prediction accuracy.
As seen from the table on the right, this accuracy is highly comparable with the best of the other two methods.
Micro F1 Score FastGCN Cora 0.850 GraphSAGE-GCN 0.829 0.822 GraphSAGE-mean 0.851 0.865 GCN (batched) GCN (original) Pubmed Reddit 0.937 0.880 0.923 0.849 0.946 0.888 0.930 0.867 0.875 NA Figure 3: Per-batch training time in seconds (left) and prediction accuracy (right).
For timing, GraphSAGE refers to GraphSAGE-GCN in Hamilton et al.
(2017).
The timings of using other ag- gregators, such as GraphSAGE-mean, are similar.
GCN refers to using batched learning, as opposed to the original version that is nonbatched; for more details of the implementation, see the appendix.
The nonbatched version of GCN runs out of memory on the large graph Reddit.
The sample sizes for FastGCN are 400, 100, and 400, respectively for the three data sets.
1025500.750.80.85F1 ScoreUniformImportance1025500.80.850.9F1 ScoreUniformImportance 25 50100Sample size0.90.920.94F1 ScoreUniformImportanceCoraPubmedReddit10-310-210-1100Time (seconds)FastGCNGraphSAGEGCNPublished as a conference paper at ICLR 2018 In the discussion period, the authors of GraphSAGE offered an improved implementation of their codes and alerted that GraphSAGE was better suited for massive graphs.
The reason is that for small graphs, the sample size (recalling that it is the product across layers) is comparable to the graph size and hence improvement is marginal; moreover, sampling overhead might then adversely affect the timing.
For fair comparison, the authors of GraphSAGE kept the sampling strategy but improved the implementation of their original codes by eliminating redundant calculations of the sampled nodes.
Now the per-batch training time of GraphSAGE compares more favorably on the smallest graph Cora; see Table 3.
Note that this implementation does not affect large graphs (e.g., Reddit) and our observation of orders of magnitude faster training remains valid.
Table 3: Further comparison of per-batch training time (in seconds) with new implementation of GraphSAGE for small graphs.
The new implementation is in PyTorch whereas the rest are in Ten- sorFlow.
FastGCN GraphSAGE-GCN (old impl) GraphSAGE-GCN (new impl) GCN (batched) Cora 0.0084 1.1630 0.0380 0.0166 Pubmed Reddit 0.0129 0.0047 0.3579 0.4260 0.3989 0.0815 2.1731 NA 5 CONCLUSIONS We have presented FastGCN, a fast improvement of the GCN model recently proposed by Kipf & Welling (2016a) for learning graph embeddings.
It generalizes transductive training to an inductive manner and also addresses the memory bottleneck issue of GCN caused by recursive expansion of neighborhoods.
The crucial ingredient is a sampling scheme in the reformulation of the loss and the gradient, well justiﬁed through an alternative view of graph convoluntions in the form of integral transforms of embedding functions.
We have compared the proposed method with additionally GraphSAGE (Hamilton et al., 2017), a newly published work that also proposes using sampling to restrict the neighborhood size, although the two sampling schemes substantially differ in both algorithm and cost.
Experimental results indicate that our approach is orders of magnitude faster than GCN and GraphSAGE, while maintaining highly comparable prediction performance with the two.
The simplicity of the GCN architecture allows for a natural interpretation of graph convolutions in terms of integral transforms.
Such a view, yet, generalizes to many graph models whose formulations are based on ﬁrst-order neighborhoods, examples of which include MoNet that applies to (meshed) manifolds (Monti et al., 2017), as well as many message-passing neural networks (see e.g., Scarselli et al.
(2009); Gilmer et al.
(2017)).
The proposed work elucidates the basic Monte Carlo ingredients for consistently estimating the integrals.
When generalizing to other networks aforementioned, an additional effort is to investigate whether and how variance reduction may improve the estimator, a possibly rewarding avenue of future research.
REFERENCES Amr Ahmed, Nino Shervashidze, Shravan Narayanamurthy, Vanja Josifovski, and Alexander J.
Smola.
Distributed large-scale natural graph factorization.
In Proceedings of the 22Nd Interna- tional Conference on World Wide Web, WWW ’13, pp.
37–48, 2013.
ISBN 978-1-4503-2035-1.
Mikhail Belkin and Partha Niyogi.
Laplacian eigenmaps and spectral techniques for embedding and clustering.
In Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic, NIPS’01, pp.
585–591, 2001.
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun.
Spectral networks and locally connected networks on graphs.
CoRR, abs/1312.6203, 2013.
Shaosheng Cao, Wei Lu, and Qiongkai Xu. Grarep: Learning graph representations with global structural information.
In Proceedings of the 24th ACM International on Conference on Informa- tion and Knowledge Management, CIKM ’15, pp.
891–900, 2015.
ISBN 978-1-4503-3794-6.
Published as a conference paper at ICLR 2018 Micha¨el Defferrard, Xavier Bresson, and Pierre Vandergheynst.
Convolutional neural networks on graphs with fast localized spectral ﬁltering.
CoRR, abs/1606.09375, 2016.
David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan Aspuru-Guzik, and Ryan P Adams.
Convolutional networks on graphs for learning molecular ﬁngerprints.
In C.
Cortes, N.
D.
Lawrence, D.
D.
Lee, M.
Sugiyama, and R.
Garnett (eds.), Advances in Neural Information Processing Systems 28, pp.
2224–2232.
Curran Associates, Inc., 2015.
J.
Gilmer, S.S. Schoenholz, P.F. Riley, O.
Vinyals, and G.E. Dahl.
Neural message passing for quantum chemistry.
In ICML, 2017.
Palash Goyal and Emilio Ferrara.
Graph embedding techniques, applications, and performance: A survey.
CoRR, abs/1705.02801, 2017.
Aditya Grover and Jure Leskovec.
Node2vec: Scalable feature learning for networks.
In Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’16, pp.
855–864, 2016.
ISBN 978-1-4503-4232-2.
William L.
Hamilton, Rex Ying, and Jure Leskovec.
graphs.
CoRR, abs/1706.02216, 2017.
Inductive representation learning on large Mikael Henaff, Joan Bruna, and Yann LeCun.
Deep convolutional networks on graph-structured data.
CoRR, abs/1506.05163, 2015.
Ashesh Jain, Amir Roshan Zamir, Silvio Savarese, and Ashutosh Saxena.
Structural-rnn: Deep learning on spatio-temporal graphs.
CoRR, abs/1511.05298, 2015.
Thomas N.
Kipf and Max Welling.
Semi-supervised classiﬁcation with graph convolutional net- works.
CoRR, abs/1609.02907, 2016a.
TN.
Kipf and M.
Welling.
Variational graph auto-encoders.
In NIPS Workshop on Bayesian Deep Learning.
2016b.
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S.
Zemel.
Gated graph sequence neural networks.
CoRR, abs/1511.05493, 2015.
Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore.
Automating the construction of internet portals with machine learning.
Inf.
Retr., 3(2):127–163, July 2000.
ISSN 1386-4564.
F.
Monti, D.
Boscaini, J.
Masci, E.
Rodala, J.
Svoboda, and M.M. Bronstein.
Geometric deep learning on graphs and manifolds using mixture model CNNs. In CVPR, 2017.
Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov.
Learning convolutional neural net- works for graphs.
CoRR, abs/1605.05273, 2016.
Mingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, and Wenwu Zhu.
Asymmetric transitivity pre- serving graph embedding.
In Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’16, pp.
1105–1114, 2016.
ISBN 978-1-4503- 4232-2.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena.
Deepwalk: Online learning of social repre- sentations.
In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’14, pp.
701–710, 2014.
ISBN 978-1-4503-2956-9.
Sam T.
Roweis and Lawrence K.
Saul.
Nonlinear dimensionality reduction by locally linear embed- ding.
Science, 290(5500):2323–2326, 2000.
ISSN 0036-8075.
doi: 10.1126/science.290.5500.
2323.
F.
Scarselli, M.
Gori, A.C. Tsoi, M.
Hagenbuchner, and G.
Monfardini.
The graph neural network model.
IEEE Transactions on Neural Networks, 20, 2009.
10 Published as a conference paper at ICLR 2018 Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei.
Line: Large-scale information network embedding.
In Proceedings of the 24th International Conference on World Wide Web, WWW ’15, pp.
1067–1077, 2015.
ISBN 978-1-4503-3469-3.
Daixin Wang, Peng Cui, and Wenwu Zhu.
Structural deep network embedding.
In Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’16, pp.
1225–1234, 2016.
ISBN 978-1-4503-4232-2.
A PROOFS Proof of Theorem 1.
Because the samples u(0) are iid, by the strong law of large numbers, t0(cid:88) j=1 ˜h(1) t1 (v) = t0 ˆA(v, u(0) j )h(0)(u(0) j )W (0) j=1 t1 ˜h(2) t2 (v) = t1 (v) = σ(˜h(1) and hence has nothing to do with that of the variable u or v in this statement.
Similarly, converges almost surely to ˜h(1)(v).
Then, because the activation function σ is continuous, the continuous mapping theorem implies that h(1) t1 (v)) converges almost surely to t1 (u)W (1) dP (u) converges almost surely to ˜h(2)(v) = h(1)(v) = σ(˜h(1)(v)).
Thus,(cid:82) ˆA(v, u)h(1) (cid:82) ˆA(v, u)h(1)(u)W (1) dP (u), where note that the probability space is with respect to the 0th layer t1(cid:88) converges almost surely to(cid:82) ˆA(v, u)h(1) (cid:90) (cid:18)(cid:90) t1 (u)W (1) dP (u) and thus to ˜h(2)(v).
A simple induction Proof of Proposition 2.
Conditioned on v, the expectation of y(v) is and the variance is 1/t times that of ˆA(v, u)x(u), i.e., completes the rest of the proof.
ˆA(v, u(1) j )h(1) t1 (u(1) j )W (1) ˆA(v, u)x(u) dP (u) = e(v), E[y(v)|v] = Var{y(v)|v} = (12) Instantiating (11) and (12) with iid samples v1, .
.
.
, vs ∼ P and taking variance and expectation in the front, respectively, we obtain ˆA(v, u)2x(u)2 dP (u) − e(v)2 (cid:19) (11) y(vi) = Var e(vi) e(v) dP (v) (cid:41) (cid:90) e(v)2 dP (v)− 1 (cid:18)(cid:90) (cid:19)2 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) v1, .
.
.
, vs (cid:35)(cid:41) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) v1, .
.
.
, vs (cid:34) (cid:40) (cid:41)(cid:35) y(vi) (cid:41) (cid:40) (cid:34) Var and (cid:34) (cid:40) Var s(cid:88) i=1 s(cid:88) i=1 (cid:40) s(cid:88) i=1 Then, applying the law of total variance s(cid:88) i=1 (cid:40) s(cid:88) i=1 (cid:90)(cid:90) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) v1, .
.
.
, vs Var y(vi) = Var y(vi) we conclude the proof.
11 st ˆA(v, u)2x(u)2 dP (u) dP (v) − 1 st e(v)2 dP (v).
(cid:90) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) v1, .
.
.
, vs (cid:41)(cid:35) (cid:35)(cid:41) (cid:34) +E Var (cid:40) s(cid:88) i=1 y(vi) Published as a conference paper at ICLR 2018 Proof of Theorem 3.
Conditioned on v, the variance of yQ(v) is 1/t times that of ˆA(v, u)x(u) dP (u) dQ(u) (where u ∼ Q), (cid:32)(cid:90) ˆA(v, u)2x(u)2dP (u)2 i.e., Var{yQ(v)|v} = Then, following the proof of Proposition 2, the overall variance is dQ(u) (cid:90)(cid:90) ˆA(v, u)2x(u)2 dP (u)2 dP (v) dQ(u) = R + st Var{GQ} = R + st − e(v)2 (cid:33) (cid:90) b(u)2x(u)2dP (u)2 dQ(u) Hence, the optimal dQ(u) must be proportional to b(u)|x(u)| dP (u).
Because it also must integrate to unity, we have in which case dQ(u) = Var{GQ} = R + b(u)|x(u)| dP (u) (cid:82) b(u)|x(u)| dP (u) (cid:20)(cid:90) b(u)|x(u)| dP (u) (cid:21)2 st Proof of Proposition 4.
Conditioned on v, the variance of yQ(v) is 1/t times that of i.e., ˆA(v, u)x(u) Var{yQ(v)|v} = dP (u) dQ(u) (cid:32)(cid:20)(cid:90) ˆA(v, u) sgn(x(u)) b(u) b(u)|x(u)| dP (u), (cid:90) (cid:21)2(cid:90) ˆA(v, u)2 b(u)|x(u)| dP (u) b(u)2 dQ(u) − e(v)2 (cid:33) The rest of the proof follows that of Proposition 2.
B ADDITIONAL EXPERIMENT DETAILS B.1 BASELINES GCN: The original GCN cannot work on very large graphs (e.g., Reddit).
So we modiﬁed it into a batched version by simply removing the sampling in our FastGCN (i.e., using all the nodes instead of sampling a few in each batch).
For relatively small graphs (Cora and Pubmed), we also compared the results with the original GCN.
GraphSAGE: For training time comparison, we use GraphSAGE-GCN that employs GCN as the aggregator.
It is also the fastest version among all choices of the aggregators.
For accu- racy comparison, we also compared with GraphSAGE-mean.
We used the codes from https: //github.com/williamleif/GraphSAGE.
Following the setting of Hamilton et al.
(2017), we use two layers with neighborhood sample sizes S1 = 25 and S2 = 10.
For fair comparison with our method, the batch size is set to be the same as FastGCN, and the hidden dimension is 128.
B.2 EXPERIMENT SETUP Datasets: The Cora and Pubmed data sets are from https://github.com/tkipf/gcn.
As we explained in the paper, we kept the validation index and test index unchanged but changed the training index to use all the remaining nodes in the graph.
The Reddit data is from http: //snap.stanford.edu/graphsage/.
Experiment Setting: We preformed hyperparameter selection for the learning rate and model di- mension.
We swept learning rate in the set {0.01, 0.001, 0.0001}.
The hidden dimension of Fast- GCN for Reddit is set as 128, and for the other two data sets, it is 16.
The batch size is 256 12 Published as a conference paper at ICLR 2018 for Cora and Reddit, and 1024 for Pubmed.
Dropout rate is set as 0.
We use Adam as the opti- mization method for training.
In the test phase, we use the trained parameters and all the graph nodes instead of sampling.
For more details please check our codes in a temporary git repository https://github.com/matenure/FastGCN.
Hardware: Running time is compared on a single machine with 4-core 2.5 GHz Intel Core i7, and 16G RAM.
C ADDITIONAL EXPERIMENTS C.1 TRAINING TIME COMPARISON Figure 3 in the main text compares the per-batch training time for different methods.
Here, we list the total training time for reference.
It is impacted by the convergence of SGD, whose contributing factors include learning rate, batch size, and sample size.
See Table 4.
Although the orders-of- magnitude speedup of per-batch time is slightly weakened by the convergence speed, one still sees a substantial advantage of the proposed method in the overall training time.
Note that even though the original GCN trains faster than the batched version, it does not scale because of memory limitation.
Hence, a fair comparison should be gauged with the batched version.
We additionally show in Figure 4 the evolution of prediction accuracy as training progresses.
Table 4: Total training time (in seconds).
FastGCN Cora 2.7 GraphSAGE-GCN 72.4 6.9 1.7 GCN (batched) GCN (original) Pubmed 15.5 259.6 210.8 21.4 Reddit 638.6 3318.5 58346.6 NA Figure 4: Training/test accuracy versus training time.
From left to right, the data sets are Cora, Pubmed, and Reddit, respectively.
C.2 ORIGINAL DATA SPLIT FOR CORA AND PUBMED As explained in Section 4, we increased the number of labels used for training in Cora and Pubmed, to align with the supervised learning setting of Reddit.
For reference, here we present results by using the original data split with substantially fewer training labels.
We also fork a separate version of FastGCN, called FastGCN-transductive, that uses both training and test data for learning.
See Table 5.
13 10-2100102Training time (seconds)00.20.40.60.81Training accuracyFastGCNGraphSAGEGCN (batched)10-2100102104Training time (seconds)0.80.820.840.860.880.9Training accuracyFastGCNGraphSAGEGCN (batched)100105Training time (seconds)0.70.750.80.850.90.95Training accuracyFastGCNGraphSAGEGCN (batched)10-2100102Training time (seconds)0.20.40.60.81Test accuracyFastGCNGraphSAGEGCN (batched)10-2100102104Training time (seconds)0.820.840.860.880.9Test accuracyFastGCNGraphSAGEGCN (batched)100105Training time (seconds)0.750.80.850.90.95Test accuracyFastGCNGraphSAGEGCN (batched)Published as a conference paper at ICLR 2018 The results for GCN are consistent with those reported by Kipf & Welling (2016a).
Because labeled data are scarce, the training of GCN is quite fast.
FastGCN beats it only on Pubmed.
The accuracy results of FastGCN are inferior to GCN, also because of the limited number of training labels.
The transductive version FastGCN-transductive matches the accuracy of that of GCN.
The results for GraphSAGE are curious.
We suspect that the model signiﬁcantly overﬁts the data, because perfect training accuracy (i.e., 1) is attained.
One may note a subtlety that the training of GCN (original) is slower than what is reported in Table 4, even though fewer labels are used here.
The reason is that we adopt the same hyperparameters as in Kipf & Welling (2016a) to reproduce the F1 scores of their work, whereas for Table 4, a better learning rate is found that boosts the performance on the new split of the data, in which case GCN (original) converges faster.
Table 5: Total training time and test accuracy for Cora and Pubmed, original data split.
Time is in seconds.
FastGCN FastGCN-transductive GraphSAGE-GCN GCN (original) Cora Pubmed Time 2.52 5.88 107.95 2.18 F1 0.723 0.818 0.334 0.814 Time 0.97 8.97 39.34 32.65 F1 0.721 0.776 0.386 0.795 D CONVERGENCE Strictly speaking, the training algorithms proposed in Section 3 do not precisely follow the existing theory of SGD, because the gradient estimator, though consistent, is biased.
In this section, we ﬁll the gap by deriving a convergence result.
Similar to the case of standard SGD where the convergence rate depends on the properties of the objective function, here we analyze only a simple case; a comprehensive treatment is out of the scope of the present work.
For convenience, we will need a separate system of notations and the same notations appearing in the main text may bear a different meaning here.
We abbreviate “with probability one” to “w.p.1” for short.
We use f (x) to denote the objective function and assume that it is differentiable.
Differentiability is not a restriction because for the nondifferentiable case, the analysis that follows needs simply change the gradient to the subgradient.
The key assumption made on f is that it is l-strictly convex; that is, there exists a positive real number l such that f (x) − f (y) ≥ (cid:104)∇f (y), x − y(cid:105) + (cid:107)x − y(cid:107)2, (13) for all x and y.
We use g to denote the gradient estimator.
Speciﬁcally, denote by g(x; ξN ), with ξN being a random variable, a strongly consistent estimator of ∇f (x); that is, N→∞ g(x; ξN ) = ∇f (x) w.p.1. lim Moreover, we consider the SGD update rule xk+1 = xk − γk g(xk; ξ(k) N ), (14) where ξ(k) N is an indepedent sample of ξN for the kth update.
The following result states that the update converges on the order of O(1/k).
Theorem 5.
Let x∗ be the (global) minimum of f and assume that (cid:107)∇f (x)(cid:107) is uniformly bounded by some constant G > 0.
If γk = (lk)−1, then there exists a sequence Bk with Bk ≤ max{(cid:107)x1 − x∗(cid:107)2, G2/l2} such that (cid:107)xk − x∗(cid:107)2 → Bk w.p.1. 14 Published as a conference paper at ICLR 2018 Proof.
Expanding (cid:107)xk+1 − x∗(cid:107)2 by using the update rule (14), we obtain (cid:107)xk+1 − x∗(cid:107)2 = (cid:107)xk − x∗(cid:107)2 − 2γk(cid:104)gk, xk − x∗(cid:105) + γ2 k(cid:107)gk(cid:107)2, where gk ≡ g(xk; ξ(k) conditioned on xk, N ).
Because for a given xk, gk converges to ∇f (xk) w.p.1, we have that (cid:107)xk+1 − x∗(cid:107)2 → (cid:107)xk − x∗(cid:107)2 − 2γk(cid:104)∇f (xk), xk − x∗(cid:105) + γ2 (15) On the other hand, applying the strict convexity (13), by ﬁrst taking x = xk, y = x∗ and then taking x = x∗, y = xk, we obtain k(cid:107)∇f (xk)(cid:107)2 w.p.1. (cid:104)∇f (xk), xk − x∗(cid:105) ≥ l(cid:107)xk − x∗(cid:107)2.
(16) Substituting (16) to (15), we have that conditioned on xk, (cid:107)xk+1 − x∗(cid:107)2 → Ck w.p.1 for some Ck ≤ (1 − 2lγk)(cid:107)xk − x∗(cid:107)2 + γ2 kG2 = (1 − 2/k)(cid:107)xk − x∗(cid:107)2 + G2/(l2k2).
(17) Now consider the randomness of xk and apply induction.
For the base case k = 2, the theorem clearly holds with B2 = C1.
If the theorem holds for k = T , let L = max{(cid:107)x1 − x∗(cid:107)2, G2/l2}.
Then, taking the probabilistic limit of xT on both sides of (17), we have that CT converges w.p.1 to some limit that is less than or equal to (1 − 2/T )(L/T ) + G2/(l2T 2) ≤ L/(T + 1).
Letting this limit be BT +1, we complete the induction proof.
15
Learning long-term dependencies is a key challenge for current machine learning approaches to artiﬁcial intelligence.
The ability of human beings to reconcile these long-term dependencies with the immediate context, i.e., to adapt and use knowledge that has been previously gained so as to be relevant to the current frame-of-reference, is indispensable.
An important example of this ability, if on a much smaller scale, is the ability to predict characters and words in a sentence or document based on one’s past experience (for example, in the form of commonly encountered constructions and phrases), the general subject dealt with in the document, and the precise wording of the speciﬁc sentence in question.
Recurrent neural network based architectures have made signiﬁcant progress towards having a machine mimic this ability.
Recurrent neural networks (RNNs) condition their present representation of the state of the world on their entire history of inputs (or “observations” in reinforcement learning parlance), and so are a natural ﬁt for learning temporally abstracted features.
In theory, a simple RNN can represent arbitrary functions and thus have the capacity to solve tasks involving dependencies at arbitrary time-scales.
In practice, more complex architectures have proven essential for solving many tasks.
One reason for this is the vanishing gradient problem (Hochreiter, 1991; Bengio et al., 1994), which makes it difﬁcult for simple RNNs to learn long-term dependencies.
Successful RNN architectures, such as LSTMs (Hochreiter and Schmidhuber, 1997) typically incorporate memory mechanisms which ameliorate the problem of vanishing gradient.
A more fundamental issue is that learning to detect long-term dependencies involves a funda- mentally difﬁcult credit assignment problem: in the absence of prior information, any past event ∗ Work begun while the author was at MILA and École Polytechnique de Montréal c(cid:13) 2017 J.R.A. Moniz & D.
Krueger.
MONIZ KRUEGER may plausibly be responsible for current events.
Architectural features such as memory mechanisms encode implicit priors which may help with the credit assignment problem.
Memory mechanisms allow a model to remember past information over arbitrarily long time-scales, so that credit can be assigned to events in the distant past.
We seek to encode an additional implicit prior of temporal hierarchy by the creation of a novel memory structure.
In particular, we suggest selective memory access via nesting as an approach to constructing temporal hierarchies in memory.
While some prior work on hierarchical memory exists, LSTM (and variants) are still the most popular deep learning model for sequential tasks, such as in character-level language modeling.
In particular, the default Stacked LSTM architecture uses a sequence of LSTMs stacked on top of each other to process the data, the input to a layer being the output of the previous layer.
In this work, we propose and explore a novel Nested LSTM architecture (NLSTM), which we envision as a potential drop-in replacement for a stacked LSTM.
In NLSTMs, the LSTM memory cells have access to an inner memory, which they selectively read and write to using the standard LSTM gates.
This key feature allows the model to implement a more effective temporal hierarchy than a conventional Stacked LSTM.
In NLSTM, the (outer) memory cell are free to selectively read and write relevant long-term information to their inner cell.
In contrast, in stacked LSTMs, the upper-level activations (analogous to the inner memories) are directly accessed to produce an output, and therefore must contain all the short-term information which is relevant to the current prediction.
In other words, the primary difference between stacked LSTMs and Nested LSTMs is the idea of selective access to inner memories which the NLSTM implements.
This frees the inner memories to remember and process events on longer time scales, even when these events are not relevant to the immediate present.
Our visualizations demonstrate that the inner memories of NLSTMs do in fact operate on longer time-scales than higher-level memories in a stacked LSTM.
Our experiments also show that NLSTMs outperform Stacked LSTMs in a wide range of tasks.
2.
Related Work The problem of learning effective temporal hierarchies for dealing with long-term dependencies is well studied in the context of both RNNs and reinforcement learning.
A comprehensive review of this topic is beyond our scope, we review some recent works and focus on the distinctive aspects of our approach.
Doing credit assignment over long time-scales is a central problem of reinforcement learning.
The options framework in RL (Sutton et al., 1999) enables long-term planning over sequences of temporally abstracted actions called options.
Selecting an option amounts to temporarily enacting a subpolicy which then selects the primitive actions at each time-step (or its own options).
Although learning options has received some attention (Stolle and Precup, 2002; Brunskill and Li, 2014), including some recent gradient-based approaches (Arulkumaran et al., 2016; Bacon et al., 2016), most successful applications so far have used hand-crafted options.
2.1. Deep learning approaches to temporal abstraction Currently, RNNs are frequently stacked creating a multi-layer feedforward network at each time-step.
Hermans and Schrauwen (2013) argue that stacking may results in more abstract, long-term features; Zhang et al.
(2016) argue that this may not be the case.
Unlike stacking, nesting also increases recurrent depth, which can improve performance Zhang et al.
(2016).
Pascanu et al.
(2013) add NESTED LSTMS multi-layer input, output, or recurrent connections as an alternative to stacking; their deep recurrent connections increase recurrent depth, but are not commonly used.
Multi-layer input connections have been used, however, for state-of-the-art speech-recognition (Hannun et al., 2014; Amodei et al., 2016) systems; these systems also incorporate stacked RNNs. Our model is based on the popular Long Short-term Memory (LSTM) (Hochreiter and Schmid- huber, 1997) architecture.
The hidden states of LSTMs include internal memory cells, which use identity connections to store long-term memories.
The LSTM forget/remember1 gate (Gers et al., 1999) allows memories to be forgotten with an (adaptive) multiplicative decay on these identity connections.
A wide variety of network architectures based on or inspired by LSTMs have been proposed (Graves et al., 2007; Cho et al., 2014; Chung et al., 2014; Kalchbrenner et al., 2015; Danihelka et al., 2016; Cheng et al., 2016).
Perhaps the most popular and well know is the Gated Recurrent Unit (GRU) (Cho et al., 2014; Chung et al., 2014).
GRUs function similarly to LSTMs, but they do not feature any internal memory; the entire hidden state is exposed to external computational units.
This moves in the opposite direction of our work, which is focused on creating more internal memory.
Some recent works also apportion more of the total hidden-state into internal memories (Cheng et al., 2016; Rocki, 2016), but not in a way which involves nesting.
Greff et al.
(2015); Jozefowicz et al.
(2015) evaluate architectural variants of LSTMs and GRUs; Greff et al.
(2015) remove components of standard LSTMs, whereas Jozefowicz et al.
(2015) use an evolutionary search procedure to search a wider space of possible models.
The LSTM remember gates allow the model to dynamically decay memories of different units at different rates, but do not explicitly encourage different units to model different levels of temporal dependency.
Some other works attempt to encode the temporal hierarchy in the prior of a recurrent model.
Temporal hierarchies among units can explicitly coded by hand, as in Clockwork RNNs (Koutnik et al., 2014) and hierarchical RNNs (Hihi and Bengio, 1996).
This approach seems brittle; it would be preferable for the model to learn to operate at the appropriate time-scales.
Chung et al.
(2015) present a fully differentiable approach to this problem, based on adding additional gating mechanisms.
A downside of this work is that the model size grows quadratically in the number of layers in the hierarchy.
More recently, Chung et al.
(2016) use the straight-through estimator (Hinton, 2012; Yoshua Bengio, 2013) to train a model which makes “crisp” binary decisions about when to update different recurrent units.
Recent work in Deep Learning considers augmenting RNN architectures with novel memory mechanisms inspired by computer memory architectures (Graves et al., 2014; Joulin and Mikolov, 2015; Grefenstette et al., 2015) and neural short-term memory mechanisms (Ba et al., 2016).
Storing and accessing memories provide paths for gradient ﬂow, but, just as in RNNs, using backpropagation through time becomes prohibitively computationally expensive when sequences become long.
The standard solution to this problem is truncate gradient ﬂow after some number of time-steps.
Zaremba and Sutskever (2015) attempt to use reinforcement learning (speciﬁcally, the REINFORCE algorithm (Williams, 1992)) to solve this problem in the context of training Neural Turing Machines (NTMs).
3.
Nested LSTMs The output gate in LSTMs encodes the intuition that memories which are not relevant at the present time-step may still be worth remembering.
Nested LSTMs use this intuition to create a temporal 1.
We support the efforts of Ba et al.
(2016) to reverse this counter-intuitive naming convention.
MONIZ KRUEGER hierarchy of memories.
Access to the inner memories is gated in exactly the same way, so that longer-term information which is only situationally relevant can be accessed selectively.
Figure 1: The Nested LSTM architecture 3.1. The architecture In an LSTM, the equations updating the cell state and the gates are given by: it = σi(xtW xi + ht−1W hi + bi) f t = σf (xtW xf + ht−1W hf + bf ) ct = f t (cid:12) ct−1 + it (cid:12) σc(xtW xc + ht−1W hc + bc) ot = σo(xtW xo + ht−1W ho + bo) ht = ot (cid:12) σh(ct) (1) (2) (3) (4) (5) (6) Note that these equations are similar to those deﬁned in Graves (2013), but do not include peephole connections.
Nested LSTMs replace the addition operation used to compute ct in LSTMs with a learned, stateful function, ct = mt(ft (cid:12) ct−1, it (cid:12) gt).
We refer to the state of the function, m at time t as the inner memory, and calling the function to compute ct also computes mt+1.
We chose to implement the memory function as another LSTM memory cell, producing a nested LSTM (see Figure 1 for an illustration).
The memory function could instead be another Nested LSTM cell, permitting arbitrarily deep nesting.
Given these architecture choices, the input and the hidden states of the memory function in an NLSTM become: (cid:101)ht−1 = f t (cid:12) ct−1 (cid:101)xt = it (cid:12) σc(xtW xc + ht−1W hc + bc) ct =(cid:101)ht−1 +(cid:101)xt In particular, note that if the memory function is addition, the entire system reduces to the classical LSTM, since the cell update becomes (7) (8) (9)   x x σi σc σo σf xt ht-1 xt ht-1 xt ht-1 ht xt ht-1 x x    σi                              ct                                    x NESTED LSTMS (a) LSTM (b) Stacked LSTM (c) Nested LSTM Figure 2: Computational graphs of the LSTM, Stacked LSTM and Nested LSTM.
The hidden state, outer memory cell, and inner memory cell are represented by h, c, and d, respectively.
While the current hidden state can inﬂuence the contents of the next inner memory cell directly, the inner memory inﬂuences the hidden state only via the outer memory.
In the architectural variant of the Nested LSTM proposed here, an LSTM is used as the memory function, and the working of the inner LSTM is governed by: (cid:101)it =(cid:101)σi((cid:101)xt(cid:102)W xi +(cid:101)ht−1(cid:102)W hi +(cid:101)bi) (cid:101)f t =(cid:101)σf ((cid:101)xt(cid:102)W xf +(cid:101)ht−1(cid:102)W hf +(cid:101)bf ) (cid:101)ct = (cid:101)f t (cid:12)(cid:101)ct−1 +(cid:101)it (cid:12)(cid:101)σc((cid:101)xt(cid:102)W xc +(cid:101)ht−1(cid:102)W hc +(cid:101)bc) (cid:101)ot =(cid:101)σo((cid:101)xt(cid:102)W xo +(cid:101)ht−1(cid:102)W ho +(cid:101)bo) (cid:101)ht =(cid:101)ot (cid:12)(cid:101)σh((cid:101)ct) ct =(cid:101)ht The cell state update of the outer LSTM now becomes: (10) (11) (12) (13) (14) (15) 4.
Experiments We evaluate Nested LSTMs on a wide variety of datasets and tasks: the Penn Treebank Corpus (Marcus et al., 1993) and the larger Text8 dataset (Mahoney, 2011) (both representing standard character-level language modeling, with Text8 being much larger than the Penn Treebank Corpus), the Chinese Poem Generation dataset (Zhang and Lapata, 2014) (which requires character-level language modeling on much smaller sequences with less temporal dependency than is common, but with a signiﬁcantly larger number of characters than would typically be found in English), and the MNIST Glimpses task (Ba et al., 2016) (which is a classiﬁcation task, but one that contains temporal                                                                                                                dt MONIZ KRUEGER dependencies).
We show that, in spite of these tasks representing diverse scenarios and objectives, nested LSTMs consistently improve performance over corresponding stacked LSTM baselines with a comparable number of parameters.
We set σi, σf , σo,(cid:101)σi,(cid:101)σf and(cid:101)σo to the sigmoid activation function,(cid:101)σc,(cid:101)σh and σh to tanh, and σc to the identity function in all our Nested LSTM experiments.
In all the following experiments, we initialize the hyperparameters of the Nested LSTM and the stacked LSTM baselines identically.
Although we explicitly specify the hyperparameters, unless otherwise mentioned, the hyperparameters we use are identical to those used in Krueger et al.
(2016); Cooijmans et al.
(2016).
We initialize the nested and stacked LSTMs’ ﬁrst input gates (which convert the input vector from having vocabulary number of elements to cell_size number of elements) using a Glorot initialization scheme Glorot and Bengio (2010), while all other gates in the LSTM (the other input, remember and output gates, the ﬁnal output gate) are initialized using orthogonal initialization Saxe et al.
(2013).
We also try to match the number of parameters of the different stacked LSTM baselines as closely as possible with our 2-layer Nested LSTM by adjusting the number of hidden units.
While this is possible precisely with the 2-layer stacked baseline, this is slightly harder to achieve with a single- layered or 3-layered stacked LSTM.
Correspondingly, we show the results of two single-layered LSTMs: one with the same number of parameters as in the reference paper, and one with a number of parameters larger than those used by our model.
We also choose the number of hidden units of the 3-layered stacked LSTM to surpass the number of parameters used by our model.
Thus, our model has an equal number of parameters as the 2-layered stacked LSTM, and is at a disadvantage to the larger single layered LSTM and the 3-layered stacked LSTM, but outperforms all these baselines.
4.1. Visualization To analyze what the cell activations look like and how they depend on each other, we visualize the changes in the cell activation states of both the inner and outer cell in the Nested LSTM as the sequence is fed into it.
We do this on the model trained on the Penn Treebank dataset as described in Section 4.2, and then visualize the cell states as a sequence from the test set is fed into the Nested LSTM as in Karpathy et al.
(2015).
We show our resulting visualization in Figure 3.
The cells for which the visualizations have been shown are the ﬁrst seven cells of the model.
From the visualization, we see that the inner LSTM’s cell activations tend to be relatively consistent across many time-steps, while the outer LSTM’s cell activations ﬂuctuate much more rapidly.
This visualization demonstrates that the NLSTM hierarchy works as expected: outer memory operates at a shorter time-scale and uses inner memory to store longer-term information.
We contrast this with a similar visualization of a 2-layer stacked LSTM baseline, in Figure 4.
While the higher layer memory (which is "further away" from the input) operates at a longer time- scale than the lower layer memory, it still ﬂuctuates more rapidly than the inner cells of the NLSTM.
This indicates that the NLSTM’s ability to selectively process and remember information across multiple levels of nested memories frees the model to remember information over longer-periods, and supports our intuition that nested memory structures can form more effective temporal hierarchies.
NESTED LSTMS Figure 3: A visualization of the cell activations corresponding to the input character for the inner cell (left) and the outer cell (right).
Red implies a negative cell state value, and blue a positive one.
A darker shade implies a larger magnitude.
In the case of the states of the inner LSTM, we visualize tanh((cid:101)ct) (since(cid:101)ct is not constrained), while in the case of the outer LSTM, we directly visualize ct.
Figure 4: A visualization of tanh(cn t ), representing the cell activations, corresponding to the input character for the ﬁrst (right) and second (left) stacked layers.
Red implies a negative cell state value, and blue a positive one.
A darker shade implies a larger magnitude.
4.2. Penn Treebank Character-level Language Modeling The Penn Treebank dataset (Marcus et al., 1993) contains around 1 million words, with a standard train:validation:test split.
We train models on this dataset to perform character-level prediction, given an input sequence, and measure the negative log likelihood (NLL) loss and the bits per character (BPC, deﬁned as the NLL divided by ln2).
MONIZ KRUEGER Figure 5: BPC as a function of epoch for character-level language modeling on PTB’s test and validation sets Model LSTM LSTM LSTM LSTM NLSTM 2 cell size 1000 1050 600 450 600 Params Valid 4.25M 1.489 4.68M 1.487 4.47M 1.473 4.17M 1.486 4.47M 1.437 Test 1.451 1.448 1.434 1.448 1.399 Table 1: BPC Losses for the Nested LSTM versus various baselines.
The test BPC losses correspond to the respective model’s loss at the epoch in which it had the minimum valid BPC (also shown).
For Penn Treebank, our ﬁrst baseline is a single layer LSTM of 1000 hidden units, following prior works (Graves, 2013; Krueger and Memisevic, 2015; Krueger et al., 2016; Cooijmans et al., 2016).
We compare this architecture with 2-layer and 3-layer stacked LSTMs and 2-layer nested LSTMs. The number of hidden units of each model is chosen to (approximately) balance the capacity at around 4 million parameters.
We also choose a single layered LSTM with a larger number of parameters than the 2-layered LSTM and NLSTM models.
We train using Adam (Kingma and Ba, 2014) with a learning rate of 0.002 in sequences of 100 and batches of 32, and clip gradients with a threshold of 1, as in the aforementioned papers.
However, we train on non-overlapping sequences, and without any normalization (which we believe could further improve these results).
We train each model for 35 epochs.
We ﬁnd that nested LSTMs yield an improvement of .035 BPC over stacked LSTMs using the same number of hidden units and layers, which, in turn, outperforms other baseline models.
Notably, both models and the 3-layered stack LSTM outperform the single-layer network, suggesting that the common use of single-layer nets for this task is sub-optimal.
Learning curves are presented in Figure 5.
4.3. Chinese Poetry Generation Here, we use the subset of the Chinese Poem Generation dataset Zhang and Lapata (2014) comprised of quatrains with 5-characters each, with the standard speciﬁed train:validation:test split.
This task is signiﬁcantly different from the PTB task: the sequence length (and as a result the length of the NESTED LSTMS Model LSTM LSTM LSTM LSTM NLSTM 2 cell size 32 40 32 32 32 Params Valid 868k 1.1M 877k 885k 877k 680.15 674.82 810.54 944.93 629.71 Test 669.99 670.27 771.88 925.47 625.19 Table 2: Perplexity for the Nested LSTM versus various baselines on the Chinese Poetry Generation dataset Figure 6: Perplexity as a function of epoch for character-level prediction on ChinesePG’s train and validation sets temporal dependency) is much shorter, but the number of characters (over 5000) is two orders of magnitude larger.
We follow the hyperparameter set used by the LSTM character-level prediction task used as the baseline in Yu et al.
(2017) (using a learning rate of 0.002).
Following suit, we keep one of our baselines as single-layered LSTM with a cell size of 32.
Our other baselines are a single layered LSTM with cell size 40, a 2-layered stacked LSTM with cell size 32 and a 3-layered stacked LSTM with cell size 32.
We compare these baselines against our Nested LSTM with a cell size of 32.
Note that because of the small cell sizes, the number of parameters in all these models is roughly the same (around 850k) in spite of the different numbers of layers (except in the case of the single layered LSTM with a cell size of 40, which has around 1.1M parameters).
All models that we compare against, however, have more or equal parameters when compared to the Nested LSTM (except for the 32-cell single layered LSTM, which is why we introduce the additional 40-cell single layered baseline).
We measure the performance using perplexity (which is eN LL), as in Che et al.
(2017).
We ﬁnd that the Nested LSTM outperforms all the baselines by a perplexity of ˜45 on the test set.
Surprisingly, we ﬁnd that both the single-layered LSTM baselines outperform the corresponding stacked LSTM baselines, even the one-layered LSTM with slightly fewer parameters.
This is possibly because of the relatively small cell size used in this experiment.
However, we observe that the Nested LSTM outperforms the single-layered LSTM in this case as well, pointing to its robustness with respect to the model size (i.e., the number of cells, and by extension, parameters, used in the model).
MONIZ KRUEGER Model LSTM LSTM LSTM LSTM NLSTM 2 cell size 100 130 75 75 75 Params Valid NLL Test NLL Valid Accuracy (%) Test Accuracy (%) 61.0k 94.9k 83.6k 85.1k 83.6k 97.19% 97.51% 97.45% 97.46% 97.60% 0.1007 0.1070 0.1040 0.1077 0.0836 0.1229 0.1242 0.1149 0.1242 0.1136 97.85% 97.89% 98.15% 98.07% 98.23% Table 3: NLL and percentage error for the Nested LSTM versus various baselines on the MNIST Glimpses task.
The epoch at which the percentage error has been show corresponds to that at which each model had the lowest percentage error on the validation set.
Similarly for NLL, the model’s validation NLL has been used to determine the epoch at which the test NLL is examined.
Figure 7: Plots of NLL (top) and percentage error (bottom) on the MNIST glimpses’ train and validation sets (versus the epoch) 4.4. MNIST Glimpses In the MNIST Glimpses task, introduced in Ba et al.
(2016), each 28x28 image (with the pixel values normalized to the range [0, 1]) is split into 4 quadrants.
Glimpses of each quadrant (in the form of alternate rows and columns), followed by the entire quadrant are then fed sequentially into the model (with 20 elements in the sequence, each element comprising of 49 pixels).
The model then predicts which integer the input represented.
The hyperparameter set we use is similar to that chosen in the pMNIST task in Krueger et al.
(2016): we train all models with an RMS Prop optimizer Tieleman and Hinton (2012) with a learning rate of 0.001 for 150 epochs (note that here, we use the more commonly used decay rate of 0.9 instead of 0.5 used there), and clip the gradients to a maximum norm of 1.
As in the aforementioned pMNIST task, we use a 100 cell single layer LSTM baseline, along with 130 cell single layer, 75 cell two-layered stacked and 75 cell three-layered stacked LSTM baseline, and compare these baselines with a 75 cell Nested LSTM.
The Nested LSTM outperforms the (stacked) LSTM baselines in terms of both NLL and error percentage, both on the validation and test datasets.
In particular, it reduces the validation error by 4.3% when compared to the next best performing model (to 1.77%, down from 1.85% for a 2-layered stacked LSTM), and the validation NLL by almost 17% (down to 0.0836 from 0.1007 in the case of a single-layered LSTM).
10 NESTED LSTMS 4.5. text8 Model LSTM LSTM LSTM LSTM NLSTM 2 cell size 2000 2100 1200 950 1200 Params Valid 16.28M 1.399 17.93M 1.396 17.45M 1.385 18.19M 1.389 17.45M 1.363 Test 1.482 1.480 1.466 1.471 1.445 Table 4: BPC for the Nested LSTM versus various baselines on the text8 task Figure 8: BPC vs epoch curves for character-level prediction on text8’s train and validation sets The text8 dataset comprises of the ﬁrst 100MB of a cleaned-up version of enwik8 (which is comprised of text from Wikipedia) (Mahoney, 2011).
As in our earlier experiments, we keep our hyperparameters identical to Krueger et al.
(2016); Cooijmans et al.
(2016), except that we do not use any normalization, and that we train on non- overlapping sequences: we use a learning rate of 0.001, batch size of 128, sequence length of 180, a gradient clipping threshold of 1, with an adam optimizer (Kingma and Ba, 2014).
Each model is trained for 40 epochs.
Our baselines include 2000 and 2100 celled single layered LSTMs, a 1200 celled two-layered stacked LSTM and a 950 celled three-layered stacked LSTM baseline, pitted against a 1200 celled Nested LSTM.
Here too, we observe that our model outperforms the closest baseline (a 2-layered stacked LSTM) on both the valid (1.363 vs 1.385, respectively) and test (1.445 vs 1.466, respectively) sets.
This indicates both that the proposed Nested LSTM is robust to different model sizes (as shown by the improvement it affords in the Chinese Poem generation task, where a relatively very small model was used), and that larger models trained on large datasets beneﬁt from a nested architecture.
5.
Conclusions Nested LSTMs (NLSTM) are a simple extension of the LSTM model that add depth via nesting, as opposed to via stacking.
The inner memory cells of an NLSTM form an internal memory, which is only accessible to other computational elements via the outer memory cells, implementing a form of temporal hierarchy.
NLSTMs outperform stacked LSTMs with similar numbers of parameters in our experiments, and result in more well deﬁned temporal hierarchies in the activations of their memory 11 MONIZ KRUEGER cells compares with stacked LSTMs. Thus, NLSTMs represent a promising alternative to stacked models.
ACKNOWLEDGMENTS We thank Christopher Beckham for help with early experiments, and Tegan Maharaj and Nicolas Ballas for helpful discussions.
We thank the developers of Theano (Theano Development Team, 2016), Fuel, and Blocks (van Merriënboer et al., 2015).
We acknowledge the computing resources provided by ComputeCanada and CalculQuebec.
References Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, et al.
Deep speech 2: End-to-end speech recognition in english and mandarin.
In International Conference on Machine Learning, pages 173–182, 2016.
Kai Arulkumaran, Nat Dilokthanakul, Murray Shanahan, and Anil Anthony Bharath.
Classifying options for deep reinforcement learning.
CoRR, abs/1604.08153, 2016.
URL http://arxiv.
org/abs/1604.08153.
J.
Ba, G.
Hinton, V.
Mnih, J.
Z.
Leibo, and C.
Ionescu.
Using Fast Weights to Attend to the Recent Past.
ArXiv e-prints, October 2016.
Jimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu.
Using fast weights to attend to the recent past.
In Advances in Neural Information Processing Systems, pages 4331–4339, 2016.
Pierre-Luc Bacon, Jean Harb, and Doina Precup.
The option-critic architecture.
CoRR, abs/1609.05140, 2016.
URL http://arxiv.org/abs/1609.05140.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
Learning long-term dependencies with gradient descent is difﬁcult.
Neural Networks, IEEE Transactions on, 5(2):157–166, 1994.
Emma Brunskill and Lihong Li. Pac-inspired option discovery in lifelong reinforcement learning.
In ICML, pages 316–324, 2014.
Tong Che, Yanran Li, Ruixiang Zhang, R Devon Hjelm, Wenjie Li, Yangqiu Song, and Yoshua Bengio.
Maximum-likelihood augmented discrete generative adversarial networks.
arXiv preprint arXiv:1702.07983, 2017.
Jianpeng Cheng, Li Dong, and Mirella Lapata.
Long short-term memory-networks for machine reading.
arXiv preprint arXiv:1601.06733, 2016.
Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.
Learning phrase representations using rnn encoder-decoder for statistical machine translation.
arXiv:1406.1078, 2014.
Junyoung Chung, Çaglar Gülçehre, KyungHyun Cho, and Yoshua Bengio.
Empirical evaluation of gated recurrent neural networks on sequence modeling.
CoRR, abs/1412.3555, 2014.
URL http://arxiv.org/abs/1412.3555.
12 NESTED LSTMS Junyoung Chung, Caglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio.
Gated feedback recurrent neural networks.
CoRR, abs/1502.02367, 2015.
Junyoung Chung, Sungjin Ahn, and Yoshua Bengio.
Hierarchical multiscale recurrent neural networks.
CoRR, abs/1609.01704, 2016.
URL http://arxiv.org/abs/1609.01704.
Tim Cooijmans, Nicolas Ballas, César Laurent, Caglar Gulcehre, and Aaron Courville.
Recurrent batch normalization.
arXiv preprint arXiv:1603.09025, 2016.
Ivo Danihelka, Greg Wayne, Benigno Uria, Nal Kalchbrenner, and Alex Graves.
Associative long short-term memory.
arXiv preprint arXiv:1602.03032, 2016.
Felix A.
Gers, Jürgen Schmidhuber, and Fred Cummins.
Learning to forget: Continual prediction with lstm.
Neural Computation, 12:2451–2471, 1999.
Xavier Glorot and Yoshua Bengio.
Understanding the difﬁculty of training deep feedforward neural networks.
In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics, pages 249–256, 2010.
A.
Graves.
Generating Sequences With Recurrent Neural Networks.
ArXiv e-prints, August 2013.
Alex Graves, Santiago Fernández, and Jürgen Schmidhuber.
Multi-dimensional Recurrent Neural Networks, pages 549–558.
Springer Berlin Heidelberg, Berlin, Heidelberg, 2007.
ISBN 978-3- 540-74690-4.
doi: 10.1007/978-3-540-74690-4\_56.
URL http://dx.doi.org/10.1007/ 978-3-540-74690-4_56.
Alex Graves, Greg Wayne, and Ivo Danihelka.
Neural turing machines.
CoRR, abs/1410.5401, 2014.
URL http://arxiv.org/abs/1410.5401.
Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom.
Learning to transduce with unbounded memory.
CoRR, abs/1506.02516, 2015.
URL http://arxiv.org/ abs/1506.02516.
Klaus Greff, Rupesh Kumar Srivastava, Jan Koutník, Bas R.
Steunebrink, and Jürgen Schmidhuber.
LSTM: A search space odyssey.
CoRR, abs/1503.04069, 2015.
URL http://arxiv.org/ abs/1503.04069.
Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al.
Deep speech: Scaling up end-to-end speech recognition.
arXiv preprint arXiv:1412.5567, 2014.
Michiel Hermans and Benjamin Schrauwen.
Training and analysing deep recurrent neural networks.
In Advances in Neural Information Processing Systems, pages 190–198, 2013.
Salah El Hihi and Yoshua Bengio.
Hierarchical recurrent neural networks for long-term dependencies.
In Advances in Neural Information Processing Systems.
1996.
Geoffrey Hinton.
Neural networks for machine learning coursera video lectures - geoffrey hinton.
2012.
URL https://www.coursera.org/course/neuralnets.
13 MONIZ KRUEGER Sepp Hochreiter.
Untersuchungen zu dynamischen neuronalen netzen.
Master’s thesis, Institut fur Informatik, Technische Universitat, Munchen, 1991.
Sepp Hochreiter and Jürgen Schmidhuber.
Long short-term memory.
Neural computation, 9(8): 1735–1780, 1997.
Armand Joulin and Tomas Mikolov.
Inferring algorithmic patterns with stack-augmented recurrent nets.
CoRR, abs/1503.01007, 2015.
URL http://arxiv.org/abs/1503.01007.
Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever.
An empirical exploration of recurrent network architectures.
Journal of Machine Learning Research, 2015.
Nal Kalchbrenner, Ivo Danihelka, and Alex Graves.
Grid long short-term memory.
CoRR, abs/1507.01526, 2015.
URL http://arxiv.org/abs/1507.01526.
Andrej Karpathy, Justin Johnson, and Li Fei-Fei.
Visualizing and understanding recurrent networks.
arXiv preprint arXiv:1506.02078, 2015.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization.
arXiv preprint arXiv:1412.6980, 2014.
Jan Koutnik, Klaus Greff, Faustino Gomez, and Juergen Schmidhuber.
A clockwork rnn.
arXiv preprint arXiv:1402.3511, 2014.
David Krueger and Roland Memisevic.
Regularizing rnns by stabilizing activations.
arXiv preprint arXiv:1511.08400, 2015.
David Krueger, Tegan Maharaj, János Kramár, Mohammad Pezeshki, Nicolas Ballas, Nan Rosemary Ke, Anirudh Goyal, Yoshua Bengio, Hugo Larochelle, Aaron C.
Courville, and Chris Pal.
Zoneout: Regularizing rnns by randomly preserving hidden activations.
CoRR, abs/1606.01305, 2016.
URL http://arxiv.org/abs/1606.01305.
Matt Mahoney.
About the test data, 2011.
URL http://mattmahoney.net/dc/textdata.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini.
Building a large annotated corpus of english: The penn treebank.
Computational linguistics, 19(2):313–330, 1993.
Razvan Pascanu, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio.
How to construct deep recurrent neural networks.
CoRR, abs/1312.6026, 2013.
URL http://arxiv.org/abs/ 1312.6026.
Kamil Rocki.
Recurrent memory array structures.
CoRR, abs/1607.03085, 2016.
URL http: //arxiv.org/abs/1607.03085.
Andrew M Saxe, James L McClelland, and Surya Ganguli.
Exact solutions to the nonlinear dynamics of learning in deep linear neural networks.
arXiv preprint arXiv:1312.6120, 2013.
Martin Stolle and Doina Precup.
Learning options in reinforcement learning.
In International Symposium on Abstraction, Reformulation, and Approximation, pages 212–223.
Springer, 2002.
14 NESTED LSTMS Richard S Sutton, Doina Precup, and Satinder Singh.
Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning.
Artiﬁcial intelligence, 112(1):181–211, 1999.
Theano Development Team.
Theano: A Python framework for fast computation of mathematical expressions.
arXiv e-prints, abs/1605.02688, May 2016.
Tijmen Tieleman and Geoffrey Hinton.
Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.
COURSERA: Neural Networks for Machine Learning, 4:2, 2012.
Bart van Merriënboer, Dzmitry Bahdanau, Vincent Dumoulin, Dmitriy Serdyuk, David Warde-Farley, Jan Chorowski, and Yoshua Bengio.
Blocks and fuel: Frameworks for deep learning.
CoRR, abs/1506.00619, 2015.
Ronald J.
Williams.
Simple statistical gradient-following algorithms for connectionist reinforcement learning.
Mach.
Learn., 8(3-4):229–256, May 1992.
ISSN 0885-6125.
doi: 10.1007/BF00992696.
URL http://dx.doi.org/10.1007/BF00992696.
Aaron Courville Yoshua Bengio, Nicholas Léonard.
Estimating or propagating gradients through stochastic neurons.
CoRR, abs/1305.2982, 2013.
URL http://arxiv.org/abs/1305.
2982.
Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: sequence generative adversarial nets with policy gradient.
In Thirty-First AAAI Conference on Artiﬁcial Intelligence, 2017.
Wojciech Zaremba and Ilya Sutskever.
Reinforcement learning neural turing machines.
CoRR, abs/1505.00521, 2015.
URL http://arxiv.org/abs/1505.00521.
Saizheng Zhang, Yuhuai Wu, Tong Che, Zhouhan Lin, Roland Memisevic, Ruslan Salakhutdinov, and Yoshua Bengio.
Architectural complexity measures of recurrent neural networks.
arXiv preprint arXiv:1602.08210, 2016.
Xingxing Zhang and Mirella Lapata.
Chinese poetry generation with recurrent neural networks.
In EMNLP, pages 670–680, 2014.
15
The development of accurate force fields1–3  for the efficient simulation of large and small systems  has been a cornerstone of modern computational chemistry.
The popularity of force fields is driven  by low computational cost relative to more accurate and transferable quantum mechanical (QM)  methods,  such  as  density  function  theory4  (DFT)  or  post-Hartree-Fock5–7  methods.
However,  parametrizing  universal  force  fields--applicable  to  any  chemical  system  in  any  chemical  environment--has remained an elusive goal due to the restrictive functional form of classical force  fields.
For  this  reason,  a  “zoo” of  force fields  has  been  developed  over  the last  30  years  with  applications  in  various  regions  of  chemistry  and  physics,  such  as  materials,  proteins,  carbohydrates, and small drug-like molecules.8–11  Drawing the line between where these system- specific force fields work and where they fail is a difficult task.
In recent years, machine learning (ML) methods have been successfully applied in many areas of  chemistry and physics research.12–19 Specifically, ML approaches for the prediction of interatomic  potential  energy  surfaces  (referred  to  as  ML  potentials)  have  exhibited  chemical  accuracy  compared  to  QM  models  at  roughly  the  computational  cost  of  classical  force  fields.20–31  ML  potentials promise to bridge the speed vs.
accuracy gap between force fields and QM methods.
Many recent studies rely on a philosophy of parametrization to one chemical system at a time22,26,  single  component  bulk  systems28,29  or  many  equilibrium  structures,  i.e.  QM7  and  QM9  datasets32,33.
While  parametrization  to  one  system  at  a  time  can  achieve  high  accuracy  with  relatively small amounts of QM, it has the downside that one must generate additional QM data  and  train  a  new  ML  model  for  every  new  chemical  system.
Using  this  approach  in  any  study  requires  extra  parametrization  time  due  to  the  non-universality  of  the  potentials.
Additionally,  parametrization  to  only  equilibrium  geometries  does  not  attempt  to  describe  the  range  of  conformations visited in atomistic simulation.
For these reasons, single system and equilibrium  dataset ML potentials do not aim to build an extensible and transferable (universal) ML potential.
Our work on the ANAKIN-ME (ANI) method for developing the ANI-1 potential34 is one example  of a universal ML atomistic potential for organic molecules.
The methodology is built upon the  concept of an atomic environment descriptor first developed by Behler and Parrinello35 and refined  to perform significantly better on large and diverse datasets of organic molecules.
A key aspect of  the ANI methodology was the focus on dataset diversity, which promotes the learning of low level  interactions (by utilizing localized descriptors) for better transferability.
For training the ANI-1  model, we calculated over 22 million structural conformations from 57,000 distinct small organic  molecules using DFT.36 The ANI-1 dataset was built through an exhaustive sampling of molecules  containing between one and eight C, N, and O atoms from the GDB-11 database, with H atoms  added  to  saturate  the  configurations.
It  is  based  on  a  philosophy  of  dataset  construction  that  samples small molecule conformational and configurational space at the same time.
The ANI-1  potential was shown to be chemically accurate for systems of 50 atoms and more, demonstrating  extensibility  and  transferability  to  much  larger  molecules  than  those  in  the  training  set.
This  phenomenon, whereby an ML model is trained on small systems (which could be thought of as  fragments of large systems), then demonstrated to be extensible to large systems has also been  confirmed in other recent studies.37–39 Other recent work had success in developing universal ML  property predictors for organic based chemical systems away from their local minima.30   When it comes to developing or optimizing ML model training datasets, human intuition currently  drives the experiment design.
The resulting datasets tend to be clustered, sparse, and incomplete;  recent work finds that people tend to favor inclusion of “successful” experiments and tend to forget  “failed”  experiments.40  The  comprehensive  incorporation  of  all  data  is  the  strength  of  ML  approaches to artificial intelligence (AI).
With sufficient data, an AI-driven machine can more  effectively  choose  the  next  step  in  experiments  or  simulations  than  humans,  speeding  up  the  optimization  of  a  given  dataset,  while  also  reducing  the  overall  amount  of  data  required.
As  robotics transforms chemical synthesis41, manufacturing, and transportation, constituting a modern  industrial  revolution,42,43  achieving  the  analogous  revolution  in  computational  methods  will  require AI and in particular the emulation of scientific intuition, reasoning and decision making.
Such  an  ambitious  program  will  not  be  accomplished  all  at  once  and  will  instead  require  incremental progress as AI algorithms are developed.
In this work we present a fully automated approach of dataset generation for training universal ML  potentials.
It is based on the concept of active learning (AL), which has been successfully applied  to  develop  single  system  ML  potentials.37,44–47    We  develop  a  two-component  technique  for  training  universal  ML  potentials.
The  first  component  is  a  dataset  reduction  algorithm  for  eliminating redundancy in an existing training set.
The second is an active learning algorithm based  on the query by committee48 (QBC) approach for selecting new training data.
For a complete and  rigorous validation of universal potentials, we also develop the COmprehensive Machine-learning  Potential  (COMP6)  benchmark  suite  for  organic  and  bio-molecules.
The  COMP6  benchmark  samples the chemical space (for molecules containing C, H, N, and O) of molecules larger than  those included in the training set, as well as non-covalent interactions via the S66x8 benchmark49.
The COMP6 benchmark is publicly available on GitHub [https://github.com/isayev/ASE_ANI].
Using the active learning scheme, a potential can be trained to the accuracy of ANI-1 using 90%  less  data,  even  while  sampling  from  smaller  molecules.
After  further  exploration  of  chemical  space,  our  potential  (dubbed  ANI-1x)  strongly  out-performs  ANI-1,  while  being  trained  on  a  dataset with only 25% of the size.
II.
METHODS  In the context of this work, the goal of active learning is to infer an accurate predictor from labeled  training data.
These labeled data are input-output pairs (X, y), where the output y represents the  correct answer to a question associated with the input X.
In the problem of ML potential training,  the label y may be the “yes” / “no” answer to whether the potential correctly describes a molecule  X.
As part of the active learning process, this question may be answered empirically for a given  substance.
The  Query  by  Committee  (QBC)  approach  uses  the  disagreement  between  models  trained to similar data to experimentally infer the correctness of an ensemble’s prediction.
This is  by the following reasoning: if an ensemble of predictors has a high standard deviation, then some  models  in  the  ensemble  must  have  a  relatively  high  error  from  the  ground  truth.
Therefore,  studies  provided  selection  of  compounds  that  have  a  high  standard deviation of ensemble predictions  in  search  of  chemical  space  can  be  employed to sample high error regions of  chemical space automatically, minimizing  the  need  for  redundant  QM  calculations.
Several  empirical  evidence  that  this  method  of  sampling  indeed improves the overall fitness of ML  potentials  for  single  systems.37,50  In  this  work, we apply this concept in a massive  search  of  chemical  space  to  develop  a  superior  training  set  for  universal  ML  ANI34 potentials.
These ANI potentials are  applicable to organic molecules containing  C,  H,  N,  and  O.
With  minimal  modification, the same approach could be  used for other areas of chemical sciences,  e.g. materials.
A.
Sample selection via Query by  Committee  We show how, in a rigorous statistical way,  one can obtain a priori information about  what  new  samples  should  be  included  in  subsequent generations of an ML potential  training set.
The a priori  information is obtained by the QBC48 algorithm.
QBC measures the  disagreement between students (models) of a committee (ensemble), then the algorithm selects  new examples where the students disagree by a preset inclusion criterion.
Finally, new reference  data for selected examples are obtained and included in the next committee training iteration.
As  a test of agreement, we choose to include new data point  𝑖 only for test cases which generate a  value 𝜌(cid:3036) greater than an inclusion criterion 𝜌(cid:3548), where 𝜌(cid:3036) is defined as,  𝜌(cid:3036) = 𝜎(cid:3036) (cid:3493)𝑁(cid:3036) 1.
In equation 1, 𝜎(cid:3036) is the standard deviation of predictions from an ensemble (see Section IIE for  details) of ANI potentials and 𝑁(cid:3036) is the number of atoms in the given test system.
The square root  is applied to 𝑁(cid:3036) since the potentials are atomistic, and the total energy error is assumed to be a  random distribution, centered around zero, per atom.
That is, cancellation of error on a per atom  basis can lead to artificially low per atom errors (and standard deviations in this case) on larger  molecules when a square root is not applied.
This is necessary when using a single value of 𝜌(cid:3548) to  test across molecules with varying numbers of atoms as is done in this work.
Figure  1:  Example  of  choosing  a  value  𝜌(cid:3548)  which  captures 98% of all errors (𝜀) over 1.5 kcal/mol on the  GDB07to09  benchmark  set  using  the  initial  (before  using active learning) ANI model ensemble.
The value  which accomplished this is found to be 𝜌(cid:3548) = 0.23.
This  value of 𝜌(cid:3548) used in query by committee results in the  selection of 58% of all test data.
Initially 26% of all 𝜀  are greater than 1.5. 44% of 𝜀 corresponding to ρ > 𝜌(cid:3548)  are greater than 1.5. Splitting the dataset along ρ = 𝜌(cid:3548)  results in a total energy RMSE of the ANI ensemble  prediction vs.
reference DFT of 7.4 kcal/mol for all  values ρ >   𝜌(cid:3548) and 1.5 kcal/mol for all values ρ ≤   𝜌(cid:3548).
(cid:3002)(cid:3015)(cid:3010)}(cid:3036) (cid:3032)(cid:3041)(cid:3046) − 𝐸(cid:3021),(cid:3036) Figure 1 provides an example of how the inclusion criterion 𝜌(cid:3548) is determined.
In this 2-dimensional  (cid:3019)(cid:3006)(cid:3007))(cid:3627)/(cid:3493)𝑁(cid:3036)  where  𝑁(cid:3036)  is  the  number  of  atoms  in  the  ith  density  plot,  𝜀(cid:3036) = (cid:3627)𝑀𝐴𝑋({𝐸(cid:3021) molecule.
Therefore,  𝜀(cid:3036) is the largest per atom prediction error of any model in an ensemble of  ANI models for test molecule  i.
The test data used in this example is the GDB07to09 test set,  which is described in Section IIC.
The ANI model used to determine 𝜌(cid:3548) in this example is the ANI  model which initialized the AL process (Section IIB).
The value 𝜌(cid:3548) is determined from the choice  of what value of  𝜀 is considered too large, and what percentage of epsilon over that should be  considered as fail cases.
Therefore,  𝜌(cid:3548) = 0.23 was empirically selected as it is the value which  allows selection of 98% of all 𝜀(cid:3036) > 1.5 kcal/mol.
The example from Figure 1 determines 𝜌(cid:3548) = 0.23 kcal/mol leads to the selection of 58% of all test  data as molecules that fail the agreement test.
As evidence that the choice of  𝜀(cid:3036) allows for the  statistical determination of poorly fit data, it is shown that before selecting any data (i.e. for all 𝜌(cid:3036)),  26% of the complete test set 𝜀(cid:3036) are greater than 1.5. However, this is 44% when considering all  𝜀(cid:3036) > 1.5  kcal/mol  which  correspond  to  𝜌(cid:3036) > 𝜌(cid:3548) .
This  shows  that  the  determined  𝜌(cid:3548) leads  to  a  selection  of  data  with  a  greater  number  of  𝜀(cid:3036) > 1.5  kcal/mol  within  its  population.
As  further  validation  of  the  approach,  the  application  of  the  concept  is  shown  to  choose  “bad”  data  by  calculating the RMSE of the potential energy (𝐸) for the mean prediction of the ensemble of ANI  models vs.
reference DFT calculations.
For all 𝑖 molecular structures corresponding to 𝜌(cid:3036) >   𝜌(cid:3548), the  𝐸 RMSE is 7.4 kcal/mol.
On the other hand, for all 𝑖 molecular structures corresponding to 𝜌(cid:3036) ≤ 𝜌(cid:3548) the 𝐸 RMSE is 1.5 kcal/mol.
Therefore, in a statistical way, the method chooses new data which  is significantly higher in error compared to GDB07to09, which is randomly generated.
With enough processing time on HPC resources, the rate-limiting step of a QBC data selection  cycle using ANI potentials is the training of a new ensemble of ANI models.
Complete training of  a single network takes 40 minutes per one million data points on a single NVIDIA Tesla V100  GPU.
To reduce the number of models trained, QBC is used in batches, searching configurational  and conformational (chemical) space for tens of thousands of new reference data points that fail  the  agreement  test.
Finally,  labels  (reference  potential  energies, 𝐸(cid:3019)(cid:3006)(cid:3007))  are  computed  for  all  molecules  in  the  selected  batch.
This  process  may  lead  to  some  redundant  data;  however,  the  alternative, retraining a new model ensemble after the addition of every new data point, will be  impractically slow.
B.
Automatic chemical space sampling via active learning    Figure 2.
Fully automated AL workflow for data generation.
The algorithm contains 3 steps: a) an  existing dataset reduction, b) a configurational search, and c) a conformational search.
Figure 2 shows the overall workflow of the iterative AL algorithm.
The algorithm is initialized  from  an  existing  random  sampling  generated  dataset,  which  may  contain  large  amounts  of  redundant  data.
This  initial  dataset  (ANI-1  in  this  work)  is  then  reduced  through  an  iterative  approach  with  the  goal  of  minimizing  the  overall  dataset  size  while  not  impacting  predictive  performance.
The reduction algorithm is provided in detail in Figure 2a.
Figure 2a is initialized  with a random subsampled 2% of the original ANI-1 dataset.
Then, iteratively, the remaining data  are tested, and 2% subsets of the fail cases are added to the training set.
Here, a fail case is defined  as |𝐸(cid:3002)(cid:3015)(cid:3010) − 𝐸(cid:3005)(cid:3007)(cid:3021)|/√𝑁   >  0.04 𝑘𝑐𝑎𝑙/𝑚𝑜𝑙, where N is the number of atoms in the molecule.
The  algorithm  is  terminated  when  less  than  5%  of  the  data  not  yet  added  to  the  training  set  are  considered as fail cases.
The remaining < 5% high error data are added to the final dataset.
Hyper- parameters for the reduction algorithm can be tuned to further reduce redundancies in the data, at  the  cost  of  more  cycles,  and  therefore,  longer  run  time.
The  final  reduced  dataset  is  used  to  bootstrap the remaining cycles of the active learning algorithm.
If a dataset such as ANI-1 is not  available, this step can be replaced with the generation of a small amount randomly sampled data  across many small, four to five C, N, O atoms, molecules.
However, this will lead to more active  learning cycles required before achieving the desired result.
With the reduced dataset, the configurational search (Figure 2b) is initialized.
The configurational  search is carried out by randomly sampling an external database of small molecules (e.g. GDB- 1151,52,  ChEMBL53–55,  algorithmically  generated  dipeptides  using  RDKit  [www.rdkit.org],  automatically  generated  dimers),  embedding  the  molecule  in  3D  space  with  RDKit,  then  optimizing  initial  structure  with  the  UFF56  force  field.
See  supplemental  information  Section  S1.2.3 for details on dimer generation.
Next, ANI energies are computed using an ensemble of five  ANI models trained to the current AL dataset (see Section IID for details on ensemble prediction  and training).
Finally, 𝜌(cid:3036) = 𝜎(cid:3036)/√𝑁, where 𝜎(cid:3036) is the standard deviation of the ensemble’s energy  predictions for molecule i and 𝑁 is the number of atoms in the molecule, is computed.
The test of  whether  to  include  the  molecule  corresponding  to  a  given  𝜌(cid:3036)  is  𝜌(cid:3036) > 𝜌(cid:3548).
The  selection  of  𝜌(cid:3548)  is  explained  in  Section  IIA.
All  molecules  that  fail  this  test  are  included  in  the  new  conformer  sampling set.
Any molecules added to the conformer sampling set are geometry optimized with  the correct reference QM level of theory using tight SCF and optimization convergence criteria.
With the configurational search complete, a conformational search cycle (Figure 2c) is initialized,  whereby the conformer sampling set (a set of equilibrium molecules generated in the configuration  sampling step) is used to generate a set of new non-equilibrium molecules (𝑋(cid:3552)).
The conformers in  𝑋(cid:3552)  are  generated  via  one  of  three  techniques,  which  are  designed  to  sample  various  regions  of  chemical space.
These sampling techniques are listed below.
  Diverse normal mode sampling (DNMS).
A version of normal mode sampling (NMS) as  presented in our previous work36, but with diversity selection used to reduce redundant data  and a bias towards near equilibrium structures.
A detailed description of DNMS is provided  in supplemental information (SI) section S1.2.1.    K random trajectory sampling (RTS).
We run short (4 ps) molecular dynamics simulations,  with an ensemble of ANI networks, starting  with random velocities  equal to 300K and  heated slowly to 1000K over the simulation time.
During the dynamics, every step QBC is  used to check whether the current structure fails the agreement test.
Once a structure is  reached that fails the test, the simulation is terminated, and new QM data is generated for  inclusion in the training set.
This is repeated to generate multiple new samples.
A detailed  description of RTS is provided in SI section S1.2.2.    Molecular dynamics generated dimer sampling.
Dimers are generated by randomly placing  and orienting molecules from the conformer sampling set into a box with periodic boundary  conditions.
A molecular dynamics simulation for 5ps is then carried out on the box.
Every  50 steps the box is fragmented into only dimer pairs within the desired cutoff radius.
Each  new dimer pair is tested using the QBC approach, failed tests are kept as new data, and QM  properties are generated for inclusion in the training set.
A detailed description of the dimer  sampling approach used here is provided in SI section S1.2.3.  After new data is selected, labels are computed and included in the training set, a new ensemble  of ANI potentials is trained.
The conformational search cycles are repeated until the model stops  improving within the COMP6 benchmarks (see details in Section IIC).
Finally, the entire cycle is  restarted from the configurational sampling step.
This process is carried out to produce a total of  37 cycles including many configurational and conformational searching cycles.
Throughout this  work we will refer to various intermediate active learned ANI models as AL1 through AL6.
The  AL6 potential is the final potential reached in this work and is referred to as the ANI-1x potential,  which is provided for free in a python package integrated with the atomic simulation environment  (ASE)  package57  [https://github.com/isayev/ASE_ANI].
The  first  row  in  Table  1  provides  information about the final dataset from this work, labeled as ANI-1x.
Notably, the size of the  ANI-1x dataset, at 5.5 million structures, is 25% the size of the dataset used in training the original  ANI-1 potential (22M).
C.
Development of the COMP6 benchmark suite  Table 1: Description of the final active learning generated training dataset (ANI-1x) and all six COMP6  benchmark datasets.
Mean relative energy range is the average range of relative energies for each set of  conformers.
Energy prediction range is the real prediction range in the benchmark; this is the range which  the ANI model predicts energies in.
[energy units:  kcal/mol]  Purpose  Dataset  Training  Testing  ANI-1x  S66x8  ANI-MD  GDB7to9  GDB10to13  Tripeptides  DrugBank  Molecule  Source  ANI-1 + AL  S66x8  PDB  GDB-11  GDB-13  RDKit  DrugBank  Configurations  (Conformations)  63,865 (5,496,771)  66 (528)  14 (1,791)  1,500 (36,000)  2,996 (47,670)  248 (1,984)  837 (13,379)  Atoms/Molecule  mean (std.
dev.)  Mean Relative  Energy Range   15 (5)  20 (7)  75 (72)  17 (3)  25 (4)  53 (7)  44 (20)  97.6  6.00  35.0  78.0  214.0  102.0  167.0  Energy Prediction  Range   6,400  2,800  31,000  1,900  2,300  4,200  14,000  To  validate  that  the active  learning process  generates  an ANI  potential  which  outperforms  the  original  ANI-1  potential,  and  that  each  cycle’s  resulting  AL  ANI  potentials  consistently  outperforms  previous  versions  of  AL  ANI  potentials,  we  develop  the  comprehensive  machine  learned  potential  (COMP6)  benchmark.
COMP6  is  a  benchmark  suite  composed  five  rigorous  benchmarks that cover broad regions of organic and bio-chemical space (for molecules containing  C,  N,  O,  and  H  atoms)  and  a  sixth  built  from  the  existing  S66x849  noncovalent  interaction  benchmark.
The five new benchmark sets are referred to as GDB7to9, GDB10to13, Tripeptides,  DrugBank, and ANI-MD.
See Table 1 for a detailed description.
The benchmarks range from a  mean  molecule  size  of  17  atoms  to  75  atoms,  with  the  largest  being  312  atoms.
Below  is  a  description  of  the  methods  used  to  develop  each  benchmark.
Energies  and  forces  for  all  non- equilibrium molecular conformations presented have been calculated using the ωB97x58 density  functional with the 6-31G(d) basis set59 as implemented in the Gaussian 0960 electronic structure  software.
Hirshfeld charges and molecular dipoles are also included in the benchmark.
An analysis  of these properties will be done in future work.
  S66x8  Benchmark.
This  dataset  is  built  from  the  original  S66x849  benchmark  for  comparing accuracy between different methods in describing noncovalent interactions of  biological molecules.
S66x8 is developed from 66 dimeric systems involving hydrogen  bonding, pi-pi stacking, London interactions, and mixed influence interactions.
While the  keen reader might question the use of this benchmark without dispersion corrections, since  dispersion  corrections  such  as  the  D361  correction  by  Grimme  et  al.
are  a  posteriori  additions to the produced energy, then a comparison without the correction is equivalent  to a comparison with the same dispersion corrections applied to both models.
  ANI Molecular Dynamics (ANI-MD) Benchmark.
Forces from the ANI-1x potential are  applied to run 1ns of vacuum molecular dynamics with a 0.25fs time step at 300K using  the Langevin thermostat on 14 well-known drug molecules and two small proteins.
System  sizes  range  from  20  to  312  atoms.
A  random  subsample  of  128  frames  from  each  1ns  trajectory is selected, and reference DFT single point calculations are performed to obtain  QM energies and forces.
  GDB7to9 Benchmark.
The GDB-11 subsets containing 7 to 9 heavy atoms (C, N, and O)  are subsampled and randomly embedded in 3D space using RDKit [www.rdkit.org].
A total  of 1500 molecule SMILES [opensmiles.org] strings are selected: 500 per 7, 8, and 9 heavy- atom set.
The resulting structures are optimized with tight convergence criteria, and normal  modes/force  constants  are  computed  using  the  reference  DFT  model.
Finally,  diverse  normal mode sampling (DNMS) is carried out to generate non-equilibrium conformations.
  GDB10to13  Benchmark.
Subsamples  of  500  SMILES  strings  each  from  the 10  and  11  heavy-atom subsets of GDB-1151,52 and 1000 SMILES strings from the 12 and 13 heavy- atom  subsets  of  the  GDB-1362  database  are  randomly  selected.
DNMS  is  utilized  to  generate random non-equilibrium conformations.
  Tripeptide Benchmark.
248 random tripeptides containing H, C, N, and O are generated  using  FASTA  strings  and  randomly  embedded  in  3D  space  using  RDKit.
As  with  GDB7to9, the molecules are optimized, and normal modes are computed.
DNMS is utilized  to generate random non-equilibrium conformations.
  DrugBank  Benchmark.
This  benchmark  is  developed  through  a  subsampling  of  the  DrugBank63 database of real drug molecules.
837 SMILES strings containing C, N, and O  are randomly selected.
Like the GDB7to9 benchmark, the molecules are embedded in 3D  space,  structurally  optimized,  and  normal  modes  are  computed.
DNMS  is  utilized  to  generate random non-equilibrium conformations.
D.
Error metrics for validation on the COMP6 benchmark suite  This work uses three error metrics for comparing different versions of ANI potentials: potential  energy (𝐸), conformer energy difference (∆𝐸), and atomic force component errors (𝐹).
  Potential energy (𝐸) error is a comparison of  𝐸(cid:3036) (cid:3014)(cid:2869), potential energies produced by model  M1 for molecule i, to 𝐸(cid:3036) (cid:3014)(cid:2870), the potential energies produced by model M2 for molecule i.
  The  conformer  energy  difference  (∆𝐸)  error is  calculated  per  set  of  conformers.
In  the  benchmark dataset K sets of conformers are supplied, one per molecular configuration.
For  a given set of conformers k, the conformer energy difference between conformers 𝑖 and 𝑗  (cid:3014),(cid:3038).
Finally, error is  for a given model M is obtained by computing ∆𝐸(cid:3036)(cid:3037) (cid:3014)(cid:2870),(cid:3038) for all k, 𝑖, and 𝑗 > 𝑖 + 1 for models M1 and M2.
calculated between ∆𝐸(cid:3036)(cid:3037)   The atomic force (𝐹) error metric is a comparison between the individual components (x,  (cid:3014)(cid:2869),(cid:3038) and ∆𝐸(cid:3036)(cid:3037) (cid:3014),(cid:3038)   =   𝐸(cid:3036) (cid:3014),(cid:3038)   −   𝐸(cid:3037) y, z) of each atom’s force vector for all conformations included in the given benchmark.
Comparisons  are  given  in  mean  absolute  error  (MAE),  and  root  mean  squared  error  (RMSE)  throughout this article.
The comparison of MAE along with RMSE can give information about  outliers in a model’s predictions.
For example, two models can have the same MAE for a prediction  on a given benchmark while the RMSE can be much higher for one than the another.
For this  reason, it is good practice to provide both MAE and RMSE when comparing two methods on some  benchmark.
E.
Property prediction with an ensemble of ANI models  For energy and force prediction we use the mean prediction of an ensemble of ANI potentials.
The  concept  of  using  an  ensemble  mean  for  ML  model  prediction  is  common  practice  in  the  ML  community.
Recently, it has been adopted in the area of ML molecular property prediction.23,37,64  All potentials used to generate results in this work utilize the mean prediction for an ensemble of  𝐿 = 5 ANI potentials trained to a 5-fold cross validation split of the training dataset.
The potential  energy (𝐸) is represented by,  𝐸 = (cid:3013) (cid:3533) 𝐸(cid:3036) (cid:3036)(cid:2880)(cid:2869) where 𝐸(cid:3036) is the potential energy prediction from each of an ensemble’s 𝐿 ANI models.
Since the  models are independent, atomic forces for the ensemble can be derived as the component wise  mean of the forces from the 𝐿 individual ANI models.
The use of an ensemble as described above  decreases ANI vs.
DFT 𝐸 RMSE by 0.67 kcal/mol, ∆𝐸 RMSE by 0.68 kcal/mol, and 𝐹 RMSE  by 2.1 kcal/mol ×  Å(cid:2879)(cid:2869) over the entire COMP6 benchmark.
III.
RESULTS AND DISCUSSIONS  The supplemental information (SI) provided with this work contains various tables detailing the  results obtained on the COMP6 benchmark by the ANI potentials discussed in this work.
SI tables  S1 through S7 provide an analysis of the ∆𝐸, 𝐸, and 𝐹 errors obtained for six subsequent active  learned ANI potentials, AL1 through AL6, and the original ANI-1 potential.
Note that the publicly  released ANI-1x potential is identical to the AL6 ANI potential.
SI Tables S8 through S10 supply  an  analysis  of  the  individual  ANI-MD  trajectory  results  for  the  ANI-1x  potential.
Table  S9  provides per atom energy errors for the ANI-1x potential vs.
DFT and shows that the mean energy  prediction RMSE per atom for all trajectories is 0.05 kcal/mol per atom.
This level of accuracy is  on par with single molecule or bulk metal ML potentials as described in recent work by J.
Behler.26  SI Table S11 provides details on the ANI models introduced in this work.
Finally, SI Tables S12  Figure 3.
Force correlation plots comparing ANI-1x, DFTB (3ob-3-1 parameter set for bio-molecules), and PM6  to DFT reference calculations are provided from left to right, respectively, for the complete ANI-MD benchmark.
Molecules  in  the  ANI-MD  benchmark  are  composed  of  a  mean  of  75  atoms  with  the  largest  being  Trp-cage  (1L2Y), a 20-residue (312-atom) protein.
DFTB and PM6 are provided as a baseline of comparison.
Mean absolute  errors (MAE) and root mean squared errors (RMSE) are provided in the bottom right of each figure.
The color bar  scale is the same for all figures allowing a proper density comparison.
through S17 give errors on conformers within select energy ranges for the ANI-1x potential.
These  tables  show  much  lower  errors  for  conformations  which  are  thermally  accessible  to  room  temperature  molecular  dynamics  simulations.
As  shown  in  Table  S17,  thermally  accessible  conformations (within 50kcal/mol) have a 𝐸 MAE/RMSE of 0.064/0.105 kcal/mol per atom and  ∆𝐸 MAE/RMSE of 0.049/0.070 kcal/mol per atom over the complete COMP6 benchmark.
Figure 3 provides evidence of the ANI-1x force prediction capabilities.
Also, most tables in the  supplemental information further establish the accuracy of ANI potential force prediction on the  COMP6  benchmark  suite.
By  construction,  ANI  potentials  provide  analytic  and  energy- conservative forces, a requirement for molecular dynamics simulations.
It is noteworthy that force  training,  which  can  be  computationally  expensive,  is  not  required  to  achieve  these  force  prediction results.
The forces compared in the DFT correlation density plots in Figure 3 are from  all trajectories combined in the COMP6 ANI-MD benchmark.
We compare the same figures for  ANI-1x (left), DFTB (center), and PM6 (right).
DFTB and PM6 are included as a baseline for the  comparison.
This  is  a  rigorous  test  case  for  any  ML  potential’s  force  prediction  because  the  molecules supplied in the dataset range from 20 to 312 atoms, with an average size of 75 atoms.
A  breakdown  of  the  errors  for  each  trajectory  in  the  ANI-MD  benchmark  is  supplied  in  the  supplemental information, Tables S8 through S10.
The closest comparison in literature can be found in recent work on a system specific ML potential  for an alanine tripeptide where a force RMSE of 3.4  kcal/mol × Å(cid:2879)(cid:2869) was achieved with test data  from  a  350K  MD  trajectory.37  The  force  error  from  this  work  was  obtained  by  training  directly  to  energies  and  analytic  forces from fragments of the molecule being  tested.
In the case of the ANI-1x potential,  which was used to predict the forces for the  creation  of  the  ANI-MD  benchmark,  a  MAE/RMSE  of  2.7/4.2  kcal/mol × Å(cid:2879)(cid:2869)  is  obtained  vs.
a  posteriori  DFT  calculations  on 128 random frames from each of the 14  molecule’s  dynamics  trajectories.
More  𝐹  MAE/RMSE for the neutralized 20-residue  Trp-cage (1L2Y) and 10-residue Chignolin  (1UAO)  proteins  are  3.1/4.6  kcal/mol × Å(cid:2879)(cid:2869)  kcal/mol × Å(cid:2879)(cid:2869),  respectively.
ANI-1x  also  exhibits  a  force  MAE/RMSE  of  2.3/3.3  kcal/mol × Å(cid:2879)(cid:2869)  within the energy range of 50 kcal/mol on  the  tripeptide  benchmark  (non-equilibrium  conformations  randomly  generated  tripeptides)  from  COMP6  (see  supplemental  information  Table  S14),  Figure  4.
Comparison  of  potential  energy  (E)  RMSE  obtained on the entire COMP6 benchmark vs.
training set  size (total molecular conformation included in the training  set).
The  x-axis  represents  the  progression  of  the  active  learning  process.
Plot  points  are  obtained  by  ANI  potentials (blue) trained to various versions of the active  learned dataset and an ANI potential (red) trained on the  original ANI-1 dataset.
1ns  molecular  from  248  impressive,  and  3.3/4.7  which is roughly the accessible energy range of 350K molecular dynamics simulations.
Finally,  considering  the  ANI-1x  potential  was  utilized  to  generate  1  ns  of  300K  molecular  dynamics  simulations,  from  which  the  ANI-MD  benchmark  geometries  were  sampled,  speaks  to  the  applicability of the forces in molecular dynamics for general molecular systems.
All the previously  mentioned  results  from  the  ANI-1x  potential  were  obtained  without  the  need  of  direct  force  training.
Figure 4 provides a plot of 𝐸 RMSE achieved on COMP6 vs.
dataset size for various active learned  datasets and the original ANI-1 dataset.
With only 2 million data points, the active learned ANI  potentials already outperform the original ANI-1 potential across the entire COMP6 benchmark.
Once the active learned ANI potential reaches 5.5 million data points it five times outperforms  ANI-1 and is approaching chemical accuracy from the reference DFT calculations.
In the new  COMP6 benchmark, diversity selection in the normal mode sampling helps ensure a more uniform  sampling of energy states within the energy range being fit to and tested within.
Therefore, general  errors on COMP6 vs.
the ANI-1 potential’s original results are expected to be much higher on this  complex benchmark than the results published on the less rigorous test sets from the original ANI- 1 work.
Table 1 provides the average energy ranges for each benchmark in COMP6 and the final  training set (ANI-1x), as well as the energy prediction (atomization energy) range.
Most  benchmarks  in  COMP6  (all  but  the  ANI-MD  benchmark)  were  used  during  the  active  learning process to validate the improvement in accuracy and universality of new active learned  ANI  models.
Figure  5  provides  the  learning  curves  for  six  intermediate  active  learned  ANI  potentials  on  each  benchmark  in  COMP6.
Supplemental  information  Table  S11  provides  information of the chemical space sampled in each of these datasets.
The horizontal dashed lines  Figure 5.
Individual COMP6 benchmark learning curves for successive versions of the active learned potentials.
RMSE  is  provided  for  three  properties:  potential  energy  (𝐸),  conformer  energy  differences  (∆𝐸),  and  force  components (𝐹).
The error bars on the solid lines represent one standard deviation of each of the five ANI models  in the ensemble used to make the mean prediction.
The horizontal lines represent the mean prediction of ANI-1.
in Figure 5 represent the original ANI-1 ensemble predictions on each of the benchmarks for the  property corresponding to its color.
AL1 is the ANI potential used to initialize the active learning  process.
It was trained to a reduced (Figure 2a) version of the one through six heavy atom subsets  of  the  ANI-1  dataset.
AL2  through  AL6  are  successive  versions  of  the  active  learned  ANI  potentials.
More details for each active learning cycle shown in Figure 5 is provided in SI Table  S11.
During the active learning process, small molecules (one to six C, N, and O atoms) were  initially sampled, with the size of the molecules sampled gradually increased as the active learning  process continued.
AL3 is where the AL models begin to statistically match or outperform the  original ANI-1 model in most metrics.
It is notable that AL3 accomplished this feat while only  having sampled 1.8 million conformations from molecules with up to 7-heavy atoms from GDB- 11.
This shows that the active learning techniques employed in this work sample chemical space  far better than random sampling techniques.
Especially considering the ANI-1 dataset includes 22  million conformations from larger, up to 8-heavy atom, molecules.
Eventually, between the AL4 and AL5 steps, amino acids, generated dipeptides, generated small  molecule dimers and small ChEMBL molecules were added to the sampling set.
This is apparent  from the large drop in error between AL4 and AL5 for the DrugBank, Tripeptides, and S66x8  benchmarks.
Active learning sampling was also driven into the GDB-11’s 9-heavy atom subset for  sampling during the production of AL6.
Supplemental information Tables S2 through S7 provide  the full tables used to make Figure 5 along with Table S1 which describes all benchmarks at once.
The latest ANI potential, ANI-1x (shown as AL6), achieves remarkable property prediction on the  complete benchmark with errors (MAE/RMSE) of 1.9/3.4 kcal/mol (𝐸), 1.8/3.0 kcal/mol (∆𝐸),  and 3.1/5.3 kcal/mol × Å(cid:2879)(cid:2869) (F) within the full energy range of the benchmark.
In general, as each ANI potential’s fitness improves in Figure 5, the standard deviation (shown as  vertical error bars) of each property prediction for a given ensemble decreases as well.
This is a  sign that each model in the ensemble is obtaining enough chemical interaction information through  active learning that the models begin agreeing on their predictions for these larger systems.
By the  final iteration of the active learning cycles, an active learned dataset of 5.5M data points is used in  training  the  ANI-1x  potential.
The  ANI-1x  potential  outperforms  the  ANI-1  potential  on  all  properties across all benchmarks.
Further, the ANI-1x dataset is 25% the size of the original ANI- 1 dataset, which contains a total of 22M data points.
In the process of submitting this paper we learned about recent work from Herr et al.65 It was  cautioned  that  using  a  specific  sampling  technique  (i.e.  MD,  normal  mode,  or  meta-dynamics  sampling) to measure the accuracy of an ML potential says nothing about how well that model  performs outside of that specific sampling technique: “No measure of a [ML potential] to a single  sampling  technique  contains  quantitative  information  about  general  performance  outside  that  sampling technique.”65 While we agree this statement is true for system specific potentials and test  sets where, for example, NMS will only sample near a very specific energy minima, it does not  hold  for  universal  ML  potentials  and  test  sets  that  span  chemically  relevant  regions  of  conformational and configurational (chemical) space.
With the following three points we argue  this statement is false with regards to the NMS sampling in the COMP6 benchmark.
First, the  configurational sampling conducted for COMP6 is a random sampling of out-of-sample molecules  with a random 3D embedding through RDKit.
Second, the NMS used to generate states away from  the energy minima overlaps with states produced by MD sampling techniques.
Third, restraints to  specific  energy  ranges  allow  controlled  tests  within  thermally  accessible  regions  of  chemical  space, the same space sampled by MD.
Therefore, if a large set of out-of-sample molecules is used  and the entire thermally accessible region of interest is considered, quantitative information about  how well the universal ML potential performs in MD simulations is obtained regardless of the  sampling technique used.
As empirical evidence of this, the ANI-1x potential tested on the ANI- MD  benchmark  (mean  energy  range  of  35 kcal/mol)  has  a  F  MAE/RMSE  of  2.49/3.77  kcal/mol × Å(cid:2879)(cid:2869) (SI Table S10).
Compare this with the complete COMP6 benchmark (95% NMS  sampled  test  data)  restricted  to  an  energy  range  of  50 kcal/mol;  it  exhibits  a  F  MAE/RMSE  kcal/mol × Å(cid:2879)(cid:2869) of 2.48/4.11 (SI Table S12).
The standard deviation of each model’s prediction  on MD vs.
NMS sampled data places them within a reasonable statistical fluctuation.
From this, a  conclusion can be drawn that the NMS test data provides quantitative information about how well  the model will perform on samples from random MD simulations.
IV.
CONCLUSIONS   In  pursuit  of  automated  dataset  generation  for  the  development  of  universal  machine  learned  potentials,  we  introduce  automatic  active  learning  techniques  for  sampling  sparsely  explored  regions of chemical space.
The algorithm begins with the reduction of an existing dataset to remove  redundant data without loss of accuracy.
New conformations of molecules are generated through  normal mode sampling, molecular dynamics sampling, and random dimer sampling.
Periodically  the  algorithm  samples  new  molecular  configurations  from  a  variety  of  sources  to  diversify  its  exploration of chemical space.
The result is a new potential (ANI-1x) developed though successive  generations of the active learning process.
The ANI-1x potential is packaged in a user-friendly  Python library, which is publicly available on GitHub [https://github.com/isayev/ASE_ANI].
We  also introduce the COMP6 benchmark for monitoring the progress of active learning cycles and  for comparison to future universal potentials.
The ANI-1x potential achieves errors (MAE/RMSE)  of 1.6/3.0 kcal/mol (𝐸), 1.4/2.3 kcal/mol (∆𝐸), and 2.7/4.5 kcal/mol × Å(cid:2879)(cid:2869) (F) when testing on  points within 100 kcal/mol of the energy minima for the complete COMP6 benchmark.
available  for  is  made  publicly  comparing  The COMP6 benchmark suite consists of six diverse benchmark test sets.
The COMP6 benchmark  suite  potentials  [https://github.com/isayev/ANI1_dataset].
As provided, properties are calculated using the ωB97x  density functional with the 6-31G(d) basis set, however, it could be recomputed using the desired  quantum level of theory.
For complete transparency, we provide the exact error metrics used to  measure accuracy on the COMP6 benchmark suite.
It is our hope that the COMP6 benchmark will  provide  the  universal  ML  potential  development  community  with  a  rigorous  benchmark  for  comparison  of  ML  potential  methods  on  organic  molecules  in  the  extrapolative  regime.
The  COMP6 benchmark suite constitutes a first benchmark of its kind for the comparison of universal  ML potentials in this rapidly changing and ever-growing field.
future  ML  The ANI-1x potential was trained to less than 100 conformations per molecular configuration in  its training set, compared to 400 for the ANI-1 dataset.
The accuracy of the ANI-1x potential is on  par  with  the  best  single  molecule  or  material  ML  potentials,  while  most  single  molecule  parametrized ML potentials require many hundreds to thousands of conformations to parametrize  a single system.
This further validates the configurational and conformational big data sampling  philosophy introduced in the original ANI-1 work.
Since the mean molecule size in the ANI-1x  active learning training set is only 15 total atoms (8 heavy atoms), the generation of more accurate  post-Hartree-Fock datasets plausible.
The high-level of universal accuracy achieved by the ANI-1x potential can be attributed to the  capacity of neural networks to learn low level interactions from properly developed descriptors.
We hypothesize the use of spatially localized descriptors (i.e., the atomic environment vector34  with modified angular symmetry function) within the cutoff to contribute greatly to this ability.
This contrasts with descriptor sets that represent the entire chemical environment at once, and thus  interactions must be inferred through the entire set of non-local descriptors by the ML model.
Given the prospects of high-throughput experiments, robotic synthesis, and intelligent software,  we  are  currently  witnessing  a  transformation  of  science  into  a  more  data-driven  automated  discovery.
The  envisioned  chemical  AI  imitates  human  decision  making  by  transferring  responsibility to an objective machine learning system.
If successful overall, the approach will  revolutionize the way computational methods are developed.
As one possible building block to  construct such AI, we introduced a fully automated workflow to select and calculate QM training  data  for  accurate,  transferable,  and  extensible  ML  potentials.
These  techniques  can  aid  in  the  generation of universal potentials for a wide variety of current and future ML models.
SUPPLEMENTARY INFORMATION   ACKNOWLEDGEMENTS   J.S.S thanks the University of Florida for the graduate student fellowship and the  Los Alamos  National Laboratory Center for Non-linear Studies for resources and hospitality.
This work was  performed,  in  part,  at  the  Center  for  Integrated  Nanotechnologies,  an  Office  of  Science  User  Facility  operated  for  the  U.S.  Department  of  Energy  (DOE)  Office  of  Science.
Los  Alamos  National Laboratory, an affirmative action equal opportunity employer, is operated by Los Alamos  National Security, LLC, for the National Nuclear Security Administration of the U.S. Department  of  Energy  under  contract  DE-AC52-06NA25396.
O.I.  acknowledges  support  from  DOD-ONR  (N00014-16-1-2311)  and  Eshelman  Institute  for  Innovation  award.
The  authors  acknowledge  Extreme Science and Engineering Discovery Environment (XSEDE) award DMR110088, which  is supported by National Science Foundation grant number ACI-1053575.
This research in part  was  done  using  resources  provided  by  the  Open  Science  Grid66,67  which  is  supported  by  the  National  Science  Foundation  award  1148698,  and  the  U.S.  Department  of  Energy's  Office  of  Science.
We gratefully acknowledge the support of the U.S. Department of Energy through the  LANL/LDRD Program for this work.
The authors thank Roman Zubatyuk and Kipton Barros for  invaluable discussions on the topics presented in this work.
APPENDIX  1 J.A. Maier, C.
Martinez, K.
Kasavajhala, L.
Wickstrom, K.E. Hauser, and C.
Simmerling, J.
Chem.
Theory Comput.
11, 3696 (2015).
2 K.
Vanommeslaeghe, E.
Hatcher, C.
Acharya, S.
Kundu, S.
Zhong, J.
Shim, E.
Darian, O.
Guvench, P.
Lopes, I.
Vorobyov, and A.D. Mackerell, J.
Comput.
Chem.
31, 671 (2010).
3 T.A. Halgren, J.
Comput.
Chem.
17, 490 (1996).
4 K.S. Thanthiriwatte, E.G. Hohenstein, L.A. Burns, and C.D. Sherrill, J.
Chem.
Theory Comput.
7, 88 (2011).
5 H.J. Monkhorst, Int.
J.
Quantum Chem.
12, 421 (1977).
6 G.D. Purvis and R.J. Bartlett, J.
Chem.
Phys.
76, 1910 (1982).
7 D.
Cremer, Wiley Interdiscip.
Rev.
Comput.
Mol.
Sci.
1, 509 (2011).
8 J.
Huang and A.D. Mackerell, J.
Comput.
Chem.
34, 2135 (2013).
9 H.
Sun, J.
Phys.
Chem.
B 102, 7338 (1998).
10 K.N. Kirschner, A.B. Yongye, S.M. Tschampel, J.
González-Outeiriño, C.R. Daniels, B.L.  Foley, and R.J. Woods, J.
Comput.
Chem.
29, 622 (2008).
11 J.A. Maier, C.
Martinez, K.
Kasavajhala, L.
Wickstrom, K.E. Hauser, and C.
Simmerling, J.
Chem.
Theory Comput.
11, 3696 (2015).
12 T.
Moot, O.
Isayev, R.W. Call, S.M. McCullough, M.
Zemaitis, R.
Lopez, J.F. Cahoon, and A.
Tropsha, Mater.
Discov.
6, 9 (2016).
13 M.
Ragoza, J.
Hochuli, E.
Idrobo, J.
Sunseri, and D.R. Koes, J.
Chem.
Inf.
Model.
57, 942  (2017).
14 B.
Liu, B.
Ramsundar, P.
Kawthekar, J.
Shi, J.
Gomes, Q.
Luu Nguyen, S.
Ho, J.
Sloane, P.
Wender, and V.
Pande, ACS Cent.
Sci.
3, 1103 (2017).
15 J.N. Wei, D.
Duvenaud, and A.
Aspuru-Guzik, ACS Cent.
Sci.
2, 725 (2016).
16 R.
Ramakrishnan, P.O. Dral, M.
Rupp, and O.A. Von Lilienfeld, J.
Chem.
Theory Comput.
11,  2087 (2015).
17 A.
Lavecchia, Drug Discov.
Today 20, 318 (2015).
18 O.
Isayev, C.
Oses, C.
Toher, E.
Gossett, S.
Curtarolo, and A.
Tropsha, Nat.
Commun.
8,  15679 (2017).
19 E.
Kim, K.
Huang, A.
Saunders, A.
McCallum, G.
Ceder, and E.
Olivetti, Chem.
Mater.
29,  9436 (2017).
20 B.
Kolb, B.
Zhao, J.
Li, B.
Jiang, and H.
Guo, J.
Chem.
Phys.
144, 224103 (2016).
21 M.
Hellström and J.
Behler, Phys.
Chem.
Chem.
Phys.
19, 82 (2017).
22 T.H. Ho, N.-N.
Pham-Tran, Y.
Kawazoe, and H.M. Le, J.
Phys.
Chem.
A 120, 346 (2016).
23 N.
Lubbers, J.S. Smith, and K.
Barros, Preprint at https://arxiv.org/abs/1710.00017 (2017).
24 K.T. Schütt, F.
Arbabzadah, S.
Chmiela, K.R. Müller, and A.
Tkatchenko, Nat.
Commun.
8,  13890 (2017).
25 K.
Yao, J.E. Herr, S.N. Brown, and J.
Parkhill, J.
Phys.
Chem.
Lett.
8, 2689 (2017).
26 J.
Behler, Angew.
Chemie Int.
Ed. 56, 12828 (2017).
27 V.
Botu, R.
Batra, J.
Chapman, and R.
Ramprasad, J.
Phys.
Chem.
C 121, 511 (2017).
28 S.
Kondati Natarajan, T.
Morawietz, and J.
Behler, Phys.
Chem.
Chem.
Phys.
17, 8356 (2015).
29 J.
Behler, R.
Martoňák, D.
Donadio, and M.
Parrinello, in Phys.
Status Solidi Basic Res.
(WILEY‐VCH Verlag, 2008), pp.
2618–2629.
30 K.
Yao, J.E. Herr, D.W. Toth, R.
Mcintyre, and J.
Parkhill, Chem.
Sci.
(2018).
31 K.
Schütt, P.-J.
Kindermans, H.E. Sauceda Felix, S.
Chmiela, A.
Tkatchenko, and K.-R.
Müller, in Adv.
Neural Inf.
Process.
Syst.
30, edited by I.
Guyon, U.
V Luxburg, S.
Bengio, H.
Wallach, R.
Fergus, S.
Vishwanathan, and R.
Garnett (Curran Associates, Inc., 2017), pp.
992– 1002.
32 R.
Ramakrishnan, P.O. Dral, M.
Rupp, and O.A. von Lilienfeld, Sci.
Data 1, 140022 (2014).
33 M.
Rupp, A.
Tkatchenko, K.-R.
Muller, and O.A. von Lilienfeld, Phys.
Rev.
Lett.
108, 58301  (2012).
34 J.S. Smith, O.
Isayev, and A.E. Roitberg, Chem.
Sci.
27, 479 (2017).
35 J.
Behler and M.
Parrinello, Phys.
Rev.
Lett.
98, 146401 (2007).
36 J.S. Smith, O.
Isayev, and A.E. Roitberg, Sci.
Data 4, 170193 (2017).
37 M.
Gastegger, J.
Behler, and P.
Marquetand, Chem.
Sci.
8, 6924 (2017).
38 B.
Huang and O.
Anatole Von Lilienfeld, Preprint at https://arxiv.org/abs/1707.04146 (2017).
39 K.
Yao, J.E. Herr, D.W. Toth, R.
Mcintyre, and J.
Parkhill, Preprint at  http://arxiv.org/abs/1711.06385 (2017).
40 P.
Raccuglia, K.C. Elbert, P.D.F. Adler, C.
Falk, M.B. Wenny, A.
Mollo, M.
Zeller, S.A.  Friedler, J.
Schrier, and A.J. Norquist, Nature 533, 73 (2016).
41 P.J. Kitson, G.
Marie, J.-P.
Francoia, S.S. Zalesskiy, R.C. Sigerson, J.S. Mathieson, and L.
Cronin, Science (80-.
).
359, 314 (2018).
42 T.
Chapman, Nature 421, 661 (2003).
43 R.D. King, J.
Rowland, S.G. Oliver, M.
Young, W.
Aubrey, E.
Byrne, M.
Liakata, M.
Markham, P.
Pir, L.N. Soldatova, A.
Sparkes, K.E. Whelan, and A.
Clare, Science (80-.
).
324,  85 (2009).
44 E.
V.
Podryabinkin and A.
V.
Shapeev, Comput.
Mater.
Sci.
140, 171 (2017).
45 N.J. Browning, R.
Ramakrishnan, O.A. von Lilienfeld, and U.
Roethlisberger, J.
Phys.
Chem.
Lett.
8, 1351 (2017).
46 P.O. Dral, A.
Owens, S.N. Yurchenko, and W.
Thiel, J.
Chem.
Phys.
146, 244108 (2017).
47 A.A. Peterson, R.
Christensen, and A.
Khorshidi, Phys.
Chem.
Chem.
Phys.
115, 1074 (2017).
48 H.S. Seung, M.
Opper, and H.
Sompolinsky, in Proc.
Fifth Annu.
Work.
Comput.
Learn.
Theory - COLT ’92 (ACM Press, New York, New York, USA, 1992), pp.
287–294.
49 B.
Brauer, M.K. Kesharwani, S.
Kozuch, and J.M.L. Martin, Phys.
Chem.
Chem.
Phys.
18,  20905 (2016).
50 I.
Kruglov, O.
Sergeev, A.
Yanilkin, and A.R. Oganov, Sci.
Rep.
7, 8512 (2017).
51 T.
Fink, H.
Bruggesser, and J.L. Reymond, Angew.
Chemie - Int.
Ed. 44, 1504 (2005).
52 T.
Fink and J.L. Raymond, J.
Chem.
Inf.
Model.
47, 342 (2007).
53 S.
Jupp, J.
Malone, J.
Bolleman, M.
Brandizi, M.
Davies, L.
Garcia, A.
Gaulton, S.
Gehant, C.
Laibe, N.
Redaschi, S.M. Wimalaratne, M.
Martin, N.
Le Nov??re, H.
Parkinson, E.
Birney, and  A.M. Jenkinson, Bioinformatics 30, 1338 (2014).
54 A.P. Bento, A.
Gaulton, A.
Hersey, L.J. Bellis, J.
Chambers, M.
Davies, F.A. Krüger, Y.
Light, L.
Mak, S.
McGlinchey, M.
Nowotka, G.
Papadatos, R.
Santos, and J.P. Overington,  Nucleic Acids Res.
42, D1083 (2014).
55 M.
Davies, M.
Nowotka, G.
Papadatos, F.
Atkinson, G.
van Westen, N.
Dedman, R.
Ochoa,  and J.
Overington, Challenges 5, 334 (2014).
56 A.K. Rappe, C.J. Casewit, W.A. Goddard III, K.S. Colwell, and W.M. Skiff, J.
Am. 114,  10024 (1992).
57 A.
Hjorth Larsen, J.
JØrgen Mortensen, J.
Blomqvist, I.E. Castelli, R.
Christensen, M.
Dułak,  J.
Friis, M.N. Groves, B.
Hammer, C.
Hargus, E.D. Hermes, P.C. Jennings, P.
Bjerre Jensen, J.
Kermode, J.R. Kitchin, E.
Leonhard Kolsbjerg, J.
Kubal, K.
Kaasbjerg, S.
Lysgaard, J.
Bergmann Maronsson, T.
Maxson, T.
Olsen, L.
Pastewka, A.
Peterson, C.
Rostgaard, J.
SchiØtz,  O.
Schütt, M.
Strange, K.S. Thygesen, T.
Vegge, L.
Vilhelmsen, M.
Walter, Z.
Zeng, and K.W.  Jacobsen, J.
Phys.
Condens.
Matter 29, 273002 (2017).
58 J.
Da Chai and M.
Head-Gordon, J.
Chem.
Phys.
128, 84106 (2008).
59 R.
Ditchfield, W.J. Hehre, and J.A. Pople, J.
Chem.
Phys.
54, 724 (1971).
60 G.
M.
J.
Frisch, W.
Trucks, H.B. Schlegel, G.E. Scuseria, M.A. Robb, J.R. Cheeseman, G.
Scalmani, V.
Barone, B.
Mennucci, G.A. Petersson, H.
Nakatsuji, M.
Caricato, X.
Li, H.P.  Hratchian, A.F. Izmaylov, J.
Bloino, G.
Zheng, and J.L. Sonnenberg, Gaussian, Inc.
Wallingford,  CT (2009).
61 S.
Grimme, J.
Antony, S.
Ehrlich, and H.
Krieg, J.
Chem.
Phys.
132, 154104 (2010).
62 L.C. Blum and J.-L.
Reymond, J.
Am. Chem.
Soc.
131, 8732 (2009).
63 V.
Law, C.
Knox, Y.
Djoumbou, T.
Jewison, A.C. Guo, Y.
Liu, A.
Maciejewski, D.
Arndt, M.
Wilson, V.
Neveu, A.
Tang, G.
Gabriel, C.
Ly, S.
Adamjee, Z.T. Dame, B.
Han, Y.
Zhou, and  D.S. Wishart, Nucleic Acids Res.
42, D1091 (2014).
64 J.
Gilmer, S.S. Schoenholz, P.F. Riley, O.
Vinyals, and G.E. Dahl, Preprint at  https://arxiv.org/abs/1704.01212 (2017).
65 J.E. Herr, K.
Yao, R.
McIntyre, D.
Toth, and J.
Parkhill, Preprint at  http://arxiv.org/abs/1712.07240 (2017).
66 R.
Pordes, D.
Petravick, B.
Kramer, D.
Olson, M.
Livny, A.
Roy, P.
Avery, K.
Blackburn, T.
Wenaus, F.
Würthwein, I.
Foster, R.
Gardner, M.
Wilde, A.
Blatecky, J.
McGee, and R.
Quick,  in J.
Phys.
Conf.
Ser.
(IOP Publishing, 2007), p.
12057.
67 I.
Sfiligoi, D.C. Bradley, B.
Holzman, P.
Mhashilkar, S.
Padhi, and F.
Würthwein, in 2009  WRI World Congr.
Comput.
Sci.
Inf.
Eng.
CSIE 2009 (IEEE, 2009), pp.
428–432.
Supplementary information for: “Less is more:  sampling chemical space with active learning”  Justin S.
Smith1, Ben Nebgen3, Nicholas Lubbers3, Olexandr Isayev2,*, Adrian E.
Roitberg1,*  1Department of Chemistry, University of Florida, Gainesville, FL 32611, USA  2UNC Eshelman School of Pharmacy, University of North Carolina at Chapel Hill, Chapel Hill,  NC 27599, USA  3Los Alamos National Laboratory, Los Alamos, NM 87545, USA  * Corresponding authors; email: OI (olexandr@olexandrisayev.com) and AER (roitberg@ufl.edu)  S1 Methods  S1.1 ANI Ensemble Preparation  Single network architectures vary by the size of the data set used to train the models during the  active learning  process.
Table  S11  describes  all  models  presented  in  this  work.
Network  sizes  (depth and number of parameters) were determined through hyper parameter searches conducted  at  every  configurational  sampling  step.
Parameters  for  the  atomic  environment  vector1  (a  numerical vector used to describe an atoms local chemical environment) used during the active  learning  process  were  constant  and  are  provided  with  the  released  ANI-1x  model.
An  initial  learning rate of 0.001 is used.
Early stopping is utilized in the training of each network, whereby  if a model fails to improve its validation set predictions within 75 epochs then training is stopped.
Learning rate annealing is utilized such that once a model stops early, training is restarted with a  learning rate 0.5 times that of the previous learning rate.
Termination of training is achieved when  the learning rate is less than 1.0 × 10(cid:2879)(cid:2873).
The adam2 update method is used to update the weights  during training.
Ensembles of ANI potentials are prepared using a 5-fold cross validation split of the data set and  all previously mentioned hyper parameters.
Training the ensemble to a 5-fold cross validation split  ensures that the ensemble was trained to the entire data set for maximum performance.
Rather than  testing models on a 10% hold out from the training data set, we use benchmarks from the COMP6  benchmark suite to determine the fitness of an ensembles prediction.
We do this because we are  more  interested  in  getting  potentials  which  are  not  just  accurate  but  also  transferable  and  extensible.
The COMP6 benchmarks provide more rigorous test case than could be achieved by  testing on a 10% hold out of the training data set since molecules in the benchmarks are on average  much larger than those included in the training set.
This allows testing of extrapolation to larger  structures,  which  is  of  great  importance  to  any  universal  ML  potential,  rather  than  testing  interpolation  to  already  seen  molecules.
However,  mean  10%  hold  out  test  set  performance  is  supplied in Table S11 for the AL models published in this work.
S1.2 Sampling methods  S1.2.1 Diverse Normal Mode Sampling (DNMS).
We modify the normal mode sampling (NMS) technique introduced by Smith el al.1 to avoid data  set clustering around equilibrium conformations.
This technique identically follows NMS, in that  a molecule is optimized at the desired QM level of theory, ωb97x with the 6-31g(d) basis set in  this work, then frequency calculations are performed to obtain normal mode coordinates and their  corresponding harmonic force constants.
As with NMS, N random non-equilibrium conformations  are  generated  by  randomly  perturbing  the  molecule  along  the  normal  mode  coordinates.
For  diverse normal mode sampling (DNMS), the atomic environment vector1  (referred to as AEV; a  numerical vector used to describe the chemical environment of an atom in a molecule) for all C,  N,  and  O  atoms  for  each  of  the  N  conformations  generated  with  NMS  is  stored.
The  squared  Euclidean  distance  matrix  between  each  of  the  N  AEVs  is  computed.
Finally,  K  diverse  conformers  is  selected  from  the  N  original  conformers  using  the  max-min  diversity  selector  algorithm implemented in the RDKit [http://www.rdkit.org/] cheminformatics software package.
For sampling, using the query by committee approach introduce in the main article we test all i  conformers from the K selected diverse conformers.
We generate QM energies and forces for all  𝜌(cid:3036) >   𝜌(cid:3548)  (Section  IIA  of  the  main  article)  and  add  this  new  data  to  the  training  set  in  the  next  iteration of the active learning algorithm.
S1.2.2 K Random Trajectory Sampling (KRTS).
Random trajectory sampling is carried out on a set of seed molecules from the conformational  sampling  data  set.
Given  a  molecular  configuration,  a  random  set  of  Boltzmann  distributed  velocities equal to 300K are generated.
Molecular dynamics using the Langevin thermostat with  0.25fs time step at 300K is then initialized.
The system is heated linearly over 4ps to 1000K.
Every  5 steps of dynamics, 𝜌(cid:3036) (Section IIA of the main article) is computed.
If 𝜌(cid:3036) >   𝜌(cid:3548) then dynamics is  terminated.
DFT  reference  data  is  then  computed  for  the  final  conformation  and  added  to  the  training set for the next iteration of the active learning cycle.
If the trajectory reaches 4ps without  encountering 𝜌(cid:3036) >   𝜌(cid:3548) then no new data is added to the training set.
Many trajectories can be run  for each of the seed molecules back to back to generate multiple new reference data.
S1.2.3 MD Generated Dimer Sampling  Dimers are generated in an active learning scheme where a large box of hundreds of randomly  selected  small  molecules  (from  the  conformational  sampling  set)  with  random  positions  and  orientation is generated.
Molecular dynamics at 300K with periodic boundary conditions using the  current version of the ANI active learned potential is ran on the box of molecules for X ps.
After  the molecular dynamics run, the box is decomposed into all dimers with intermolecular distances  less than 5.0Å.
Query by committee (Section S1.1) is then performed on all generated dimers,  selecting any dimer with 𝜌(cid:3036) > 𝜌(cid:3548).
DFT reference calculations are performed for all selected dimers  to obtain refence training data.
The new reference training data is added to the training data set for  the next iteration of the active learning cycle.
X was initially chosen to be small (10fs) but after  time the algorithm stops generating new dimers, thus X is increased iteratively.
As of the ANI-1x  data set X is set to 5ps.
𝝈  𝑬𝑹 𝝈   𝑬𝑴 Table S1.
Complete COMP6 benchmark suite results for various ANI potentials.
Errors for conformer  energy differences (∆E), potential energies (E), and force components (F) for the active learned ANI  potentials ANI-AL1 to ANI-1X compared with the original ANI-1 potential.
These results are from the  combination of all benchmarks within the COMP6 benchmark suite.
µ and σ are the arithmetic mean  and standard deviation, respectively.
M and R are the MAE and RMSE, respectively.
Units of energy  are kcal  × mol(cid:2879)(cid:2869) and units of force are kcal  × mol(cid:2879)(cid:2869) × Å(cid:2879)(cid:2869).
𝝁  𝑬𝑹 21.10  24.80  13.43  14.40  3.58  3.37  16.94  ANI  Model  AL1  AL2  AL3  AL4  AL5  AL6  ANI-1  Table S2.
DrugBank COMP6 benchmark results for various ANI potentials.
Errors for conformer  energy differences (∆E), potential energies (E), and force components (F) for the active learned ANI  potentials ANI-AL1 to ANI-1X compared with the original ANI-1 potential.
µ and σ are the arithmetic  mean and standard deviation, respectively.
M and R are the MAE and RMSE, respectively.
Units of  energy are kcal  × mol(cid:2879)(cid:2869) and units of force are kcal  × mol(cid:2879)(cid:2869) × Å(cid:2879)(cid:2869).
𝝈  𝑬𝑹 𝝁  ∆𝑬𝑹 𝝈  1.95  1.88  0.55  0.30  0.06  0.15  1.29  𝝁   ∆𝑬𝑴 0.31  0.47  0.09  0.09  0.02  0.04  0.20  𝝈   ∆𝑬𝑹 8.36  6.42  5.73  5.32  3.14  2.95  6.97  𝝁  𝑭𝑹 10.61  9.72  7.12  6.89  5.34  5.29  7.13  𝝁   𝑬𝑴 9.54  10.21  4.78  5.04  2.19  1.93  5.01  ∆𝑬𝑴 4.31  3.56  2.86  2.72  2.00  1.85  3.01  𝝁   𝑭𝑴 5.59  5.19  4.19  4.01  3.26  3.09  3.70  1.36  2.03  0.24  0.41  0.24  0.58  1.36  2.94  6.52  1.62  1.79  0.39  0.78  2.70  0.21  0.57  0.09  0.10  0.04  0.09  0.15  1.43  1.46  0.42  0.50  0.04  0.08  0.42  𝝈   𝑬𝑴 𝝈   𝑭𝑴 𝝈   𝑭𝑴 𝝈  𝑭𝑹 𝝈  𝑭𝑹 ANI  Model  AL1  AL2  AL3  AL4  AL5  AL6  ANI-1  ∆𝑬𝑴 4.69  4.94  3.49  3.51  2.17  2.09  3.41  𝝁   ∆𝑬𝑴 0.47  1.63  0.22  0.30  0.08  0.14  0.13  𝝈   ∆𝑬𝑹 6.69  7.16  4.89  4.89  3.05  3.18  4.94  𝝁  ∆𝑬𝑹 𝝈  0.90  2.92  0.29  0.53  0.66  0.83  0.42  𝝁   𝑬𝑴 29.08  30.35  12.99  14.78  2.79  2.65  13.89  8.59  8.71  2.88  2.23  0.16  0.28  2.75  𝝁  𝑬𝑹 40.25  42.80  17.84  19.33  5.61  6.01  20.65  11.15  14.31  3.47  2.96  1.92  3.01  3.07  𝝁   𝑭𝑴 5.90  5.97  4.58  4.49  2.99  2.86  4.04  𝝁  𝑭𝑹 10.70  11.17  7.78  7.84  4.83  5.35  7.62  0.35  1.47  0.17  0.34  0.07  0.16  0.12  1.01  4.16  0.38  1.20  1.08  1.82  1.55  𝝁   ∆𝑬𝑴 Table S3.
Tripeptide COMP6 benchmark.
Errors for conformer energy differences (∆E), potential  energies (E), and force components (F) for the active learned ANI potentials ANI-AL1 to ANI-1X  compared with the original ANI-1 potential.
µ and σ are the arithmetic mean and standard deviation,  respectively.
M and R are the MAE and RMSE, respectively.
Units of energy are kcal  × mol(cid:2879)(cid:2869) and units of force are kcal  × mol(cid:2879)(cid:2869) × Å(cid:2879)(cid:2869).
ANI  Model  ∆𝑬𝑴 AL1  4.75  AL2  4.08  AL3  3.54  AL4  2.92  AL5  1.78  AL6  1.65  ANI-1  2.86  21.08  27.82  15.72  15.35  4.51  3.77  16.47  11.45  11.04  9.18  7.77  4.43  4.79  6.67  15.25  20.92  11.56  13.17  3.50  2.92  13.10  13.79  10.02  8.75  7.22  2.90  2.58  4.99  3.29  4.70  2.27  3.33  0.10  0.22  1.15  0.36  0.76  0.10  0.12  0.05  0.07  0.31  3.94  7.78  2.38  3.00  0.16  0.47  1.70  4.79  4.90  4.26  3.95  2.67  2.49  3.46  2.25  1.50  0.96  1.07  0.30  0.48  0.69  0.21  0.89  0.14  0.14  0.05  0.04  0.34  𝝁  ∆𝑬𝑹 𝝈  𝝈   ∆𝑬𝑹 0.86  1.87  0.25  0.73  1.80  0.70  1.58  𝝁   𝑬𝑴 𝝁   𝑭𝑴 𝝁  𝑬𝑹 𝝁  𝑭𝑹 𝝈   𝑬𝑴 𝝈  𝑭𝑹 𝝈  𝑬𝑹 𝝈   𝑭𝑴 Table S4.
GDB07to09 COMP6 benchmark.
Errors for conformer energy differences (∆E), potential  energies (E), and force components (F) for the active learned ANI potentials ANI-AL1 to ANI-1X  compared with the original ANI-1 potential.
µ and σ are the arithmetic mean and standard deviation,  respectively.
M and R are the MAE and RMSE, respectively.
Units of energy are kcal  × mol(cid:2879)(cid:2869) and units of force are kcal  × mol(cid:2879)(cid:2869) × Å(cid:2879)(cid:2869).
ANI  Model  ∆𝑬𝑴 AL1  2.14  AL2  1.66  AL3  1.33  AL4  1.26  AL5  1.18  AL6  1.07  ANI-1  1.28  𝝁  ∆𝑬𝑹 𝝈  0.11  0.13  0.14  0.05  0.02  0.05  0.05  𝝁   ∆𝑬𝑴 0.03  0.03  0.02  0.01  0.02  0.03  0.01  𝝈   ∆𝑬𝑹 4.25  2.83  2.09  1.92  1.79  1.66  2.19  𝝁   𝑬𝑴 2.99  2.82  1.43  1.34  1.20  1.04  1.30  𝝁   𝑭𝑴 3.92  3.39  2.95  2.81  2.64  2.43  2.50  𝝁  𝑬𝑹 5.72  5.53  2.18  1.89  1.69  1.50  2.31  𝝁  𝑭𝑹 7.76  6.35  4.94  4.59  4.24  3.93  4.57  0.03  0.08  0.03  0.02  0.04  0.07  0.03  0.35  0.34  0.29  0.06  0.02  0.05  0.04  0.12  0.09  0.02  0.04  0.02  0.04  0.01  0.57  0.42  0.25  0.06  0.04  0.08  0.16  𝝈   𝑬𝑴 𝝈   𝑭𝑴 𝝈  𝑬𝑹 𝝈  𝑭𝑹 Table S5.
GDB10to13 COMP6 benchmark.
Errors for conformer energy differences (∆E), potential  energies (E), and force components (F) for the active learned ANI potentials ANI-AL1 to ANI-1X  compared with the original ANI-1 potential.
µ and σ are the arithmetic mean and standard deviation,  respectively.
M and R are the MAE and RMSE, respectively.
Units of energy are kcal  × mol(cid:2879)(cid:2869) and units of force are kcal  × mol(cid:2879)(cid:2869) × Å(cid:2879)(cid:2869).
ANI  Model  ∆𝑬𝑴 AL1  5.25  AL2  4.12  AL3  3.13  AL4  2.89  AL5  2.57  AL6  2.38  ANI-1  2.98  𝝁  𝝈   ∆𝑬𝑹 8.01  6.25  4.60  4.19  3.70  3.47  4.54  𝝁   ∆𝑬𝑴 0.14  0.24  0.05  0.04  0.04  0.05  0.05  𝝁  𝑭𝑹 11.72  10.03  7.42  7.03  6.23  6.01  7.09  𝝁  𝑬𝑹 11.00  13.43  4.69  4.48  3.58  3.21  4.74  𝝁   𝑬𝑴 7.18  7.70  3.29  3.32  2.62  2.30  3.12  𝝁   𝑭𝑴 6.38  5.65  4.56  4.31  3.85  3.67  3.96  0.43  0.98  0.10  0.21  0.05  0.06  0.11  0.19  0.39  0.09  0.06  0.05  0.09  0.06  0.60  2.38  0.25  0.27  0.05  0.14  0.24  0.30  0.52  0.14  0.08  0.06  0.11  0.16  0.69  1.10  0.26  0.16  0.08  0.17  0.24  𝝈  ∆𝑬𝑹 𝝈   𝑬𝑴 𝝈   𝑭𝑴 𝝈  𝑬𝑹 𝝈  𝑭𝑹 Table S6.
S66x8 COMP6 benchmark.
Errors for conformer energy differences (∆E), potential energies  (E), and force components (F) for the active learned ANI potentials ANI-AL1 to ANI-1X compared with  the original ANI-1 potential.
µ and σ are the arithmetic mean and standard deviation, respectively.
M  and R are the MAE and RMSE, respectively.
Units of energy are kcal  × mol(cid:2879)(cid:2869) and units of force are kcal  × mol(cid:2879)(cid:2869) × Å(cid:2879)(cid:2869).
ANI  Model  ∆𝑬𝑴 AL1  8.05  AL2  8.98  AL3  8.18  AL4  6.55  AL5  1.57  AL6  1.42  ANI-1  10.32  𝝁  𝝈   ∆𝑬𝑹 12.28  13.77  12.95  10.85  2.26  2.10  16.76  𝝁   ∆𝑬𝑴 0.73  1.40  0.84  0.29  0.08  0.09  0.76  𝝁  𝑬𝑹 16.47  17.09  14.99  12.71  3.45  3.01  20.12  𝝁   𝑬𝑴 11.63  11.95  9.99  8.54  2.51  2.06  13.25  𝝁   𝑭𝑴 2.67  2.63  2.37  2.41  1.72  1.60  3.17  𝝁  𝑭𝑹 5.77  5.87  4.78  4.54  3.07  2.76  9.08  0.12  0.31  0.14  0.23  0.06  0.15  0.36  0.44  2.47  1.57  0.74  0.21  0.18  1.38  0.34  1.60  1.01  0.52  0.17  0.13  0.96  1.25  2.37  1.49  0.71  0.11  0.26  1.07  0.21  2.58  0.82  0.39  0.17  0.30  4.07  𝝈  ∆𝑬𝑹 𝝈   𝑬𝑴 𝝈   𝑭𝑴 𝝈  𝑬𝑹 𝝈  𝑭𝑹 Table S7.
ANI-MD COMP6 benchmark.
Errors for conformer energy differences (∆E), potential  energies (E), and force components (F) for the active learned ANI potentials AL1 to AL6 compared  with the original ANI-1 potential.
µ and σ are the arithmetic mean and standard deviation,  respectively.
M and R are the MAE and RMSE, respectively.
Units of energy are kcal  × mol(cid:2879)(cid:2869) and units of force are kcal  × mol(cid:2879)(cid:2869) × Å(cid:2879)(cid:2869).
ANI  Model  ∆𝑬𝑴 AL1  8.47  AL2  7.15  AL3  6.80  AL4  6.61  AL5  2.82  AL6  2.59  ANI-1  8.93  𝝁  𝝈   ∆𝑬𝑹 17.11  12.46  13.43  12.52  4.61  4.17  18.08  𝝁   ∆𝑬𝑴 2.34  2.76  0.76  0.53  0.09  0.10  1.58  𝝁  𝑬𝑹 92.95  121.5  82.56  89.62  8.10  5.94  109.2  𝝁  𝑭𝑹 11.11  11.70  8.11  8.99  4.47  4.24  12.70  𝝁   𝑬𝑴 51.55  62.92  41.41  42.41  4.39  3.40  52.30  𝝈  𝑬𝑹 12.64  55.94  13.44  12.82  1.88  1.48  25.32  𝝈   𝑬𝑴 6.19  20.70  5.10  4.82  0.73  0.65  8.84  𝝈  ∆𝑬𝑹 7.89  6.76  2.33  1.01  0.28  0.26  4.39  𝝁   𝑭𝑴 5.95  6.37  4.99  5.01  2.89  2.68  5.80  𝝈   𝑭𝑴 1.47  3.02  0.37  0.28  0.06  0.16  1.61  𝝈  𝑭𝑹 10.58  10.29  1.17  1.54  0.10  0.63  7.80  Table S8.
Individual ANI-MD COMP6 benchmark trajectories.
Errors for conformer energy  differences (∆E), potential energies (E), and force components (F) for the ANI-1x potential vs DFT  reference calculations on the 128 conformations per molecule in the ANI-MD COMP6 benchmark.
Units of energy are kcal  × mol(cid:2879)(cid:2869) and units of force are kcal  × mol(cid:2879)(cid:2869) × Å(cid:2879)(cid:2869).
Per conformation  (conf.) energy and force prediction timings are also included for the ANI-1x potential.
System  Acetaminophen  Caffeine  Salbutamol  Atomoxetine  Lisdexamfetamine  Capsaicin  Oseltamivir  Retinol  Fentanyl  Tolterodine  Ranolazine  Atazanavir  Chignolin (1UAO)  TrpCage (1L2Y)  E  MAE  0.56  0.98  1.81  1.19  0.94  1.53  2.96  3.68  1.26  1.91  2.22  2.73  18.07  16.34  E  RMSE  0.70  1.24  2.10  1.50  1.18  1.98  3.45  4.72  1.58  2.28  2.62  3.52  18.47  18.37  E  range  15.7  17.1  21.2  19.7  27.9  25.8  40.7  41.0  29.8  28.3  29.1  52.0  45.5  102.5  F  MAE  2.06  3.56  2.16  1.84  1.63  2.01  2.41  3.88  2.03  2.10  2.20  2.53  3.25  3.13  F  RMSE  2.96  5.46  3.05  2.62  2.27  2.91  3.68  7.70  2.92  3.04  3.06  3.80  4.68  4.59  F  range  196.6  260.1  226.8  203.0  229.5  231.1  233.5  285.6  208.1  205.1  237.1  242.9  325.3  249.7  ∆E  MAE  0.80  1.31  1.23  1.51  1.29  2.07  1.95  3.99  1.69  1.66  1.95  3.92  4.36  9.80  ∆E  RMSE  1.00  1.66  1.55  1.90  1.66  2.61  2.57  5.08  2.13  2.07  2.45  4.94  5.42  12.10  ∆E  range  30.2  34.1  42.1  36.5  54.4  50.3  79.7  75.5  55.3  54.4  57.2  101.1  86.2  191.0  Time(ms)  per conf.
2.7  3.0  4.0  4.3  3.8  3.6  4.5  4.0  4.3  5.3  4.4  5.0  5.5  9.3  Table S9.
Individual ANI-MD COMP6 benchmark trajectories per atom.
Per atom errors for the  conformer energy differences (∆𝐸) and potential energies (𝐸) for the ANI-1x potential vs DFT  reference calculations on the 128 conformations per molecule in the ANI-MD COMP6 benchmark.
Units of energy are kcal  × mol(cid:2879)(cid:2869).
System  # of atoms  Acetaminophen  Caffeine  Salbutamol  Atomoxetine  Lisdexamfetamine  Capsaicin  Oseltamivir  Retinol  Fentanyl  Tolterodine  Ranolazine  Atazanavir  Chignolin (1UAO)  TrpCage (1L2Y)  Mean  20  24  38  40  44  49  50  51  53  55  64  103  149  312  75  𝐸(cid:3019)(cid:3014)(cid:3020) √𝑁 0.16  0.25  0.34  0.24  0.18  0.28  0.49  0.66  0.22  0.31  0.33  0.35  1.51  1.04  0.45  𝐸(cid:3019)(cid:3014)(cid:3020) 0.04  0.05  0.06  0.04  0.03  0.04  0.07  0.09  0.03  0.04  0.04  0.03  0.12  0.06  0.05  ∆𝐸(cid:3019)(cid:3014)(cid:3020) √𝑁 0.22  0.34  0.25  0.30  0.25  0.37  0.36  0.71  0.29  0.28  0.31  0.49  0.44  0.69  0.38  ∆𝐸(cid:3019)(cid:3014)(cid:3020) 0.05  0.07  0.04  0.05  0.04  0.05  0.05  0.10  0.04  0.04  0.04  0.05  0.04  0.04  0.05  Table S10.
Individual ANI-MD trajectories COMP6 benchmark for ANI, DFTB, and PM6.
ANI-1x, DFTB  and PM6 vs DFT reference calculation errors for the conformer energy differences (∆𝐸) and force  components (F) on the 128 conformations per molecule in the ANI-MD COMP6 benchmark.
Units of  energy are kcal  × mol(cid:2879)(cid:2869) and units of force are kcal  × mol(cid:2879)(cid:2869) × Å(cid:2879)(cid:2869).
𝐃𝐅𝐓𝐁  ∆𝐄𝐌𝐀𝐄 1.59  4.63  2.41  2.33  2.43  5.50  1.98  2.27  2.76  4.01  2.51  2.46  2.55  9.84  3.38  𝐃𝐅𝐓𝐁  ∆𝐄𝐑𝐌𝐒 𝐀𝐍𝐈   ∆𝐄𝐑𝐌𝐒 𝐏𝐌𝟔  2.00  2.80  9.27  5.71  3.85  3.02  6.08  2.93  3.02  4.67  12.1  7.05  4.76  2.47  5.29  2.84  4.94  3.47  4.97  7.49  3.85  3.16  4.46  3.05  4.36  3.16  12.3  16.3  6.45  4.23  𝐀𝐍𝐈   ∆𝐄𝐌𝐀𝐄 0.80  3.92  1.51  1.31  2.07  4.36  1.69  1.29  1.95  1.95  3.99  1.23  1.66  9.80  2.68  𝐏𝐌𝟔  ∆𝐄𝐌𝐀𝐄 2.23  7.43  3.12  4.92  3.76  9.62  3.81  4.24  4.00  6.03  3.05  3.56  3.44  13.1  5.16  System  Acetaminophen  Atazanavir  Atomoxetine  Caffeine  Capsaicin  Chignolin (1UAO)  Fentanyl  Lisdexamfetamine  Oseltamivir  Ranolazine  Retinol  Salbutamol  Tolterodine  TrpCage (1L2Y)  Mean  𝐀𝐍𝐈   𝐅𝐌𝐀𝐄 2.06  2.53  1.84  3.56  2.01  3.25  2.03  1.63  2.41  2.20  3.88  2.16  2.10  3.13  2.49  𝐏𝐌𝟔  𝐅𝐌𝐀𝐄 7.23  8.14  6.38  12.0  6.72  9.76  6.71  6.84  7.31  8.26  5.21  7.28  5.92  9.07  7.63  𝐀𝐍𝐈   𝐅𝐑𝐌𝐒 2.96  3.80  2.62  5.46  2.91  4.68  2.92  2.27  3.68  3.06  7.70  3.05  3.04  4.59  3.77  ∆𝐄𝐑𝐌𝐒 1.00  4.94  1.90  1.66  2.61  5.42  2.13  1.66  2.57  2.45  5.08  1.55  2.07  12.1  3.37  𝐃𝐅𝐓𝐁  𝐅𝐌𝐀𝐄 4.87  5.11  3.23  7.30  3.58  5.87  3.46  3.53  4.56  4.20  4.31  4.20  3.54  5.38  4.51  𝐃𝐅𝐓𝐁  𝐅𝐑𝐌𝐒 7.16  7.65  4.57  12.1  5.33  8.44  5.12  5.26  6.79  5.92  6.99  5.46  4.89  7.87  6.68  𝐏𝐌𝟔  𝐅𝐑𝐌𝐒 10.8  12.1  9.25  19.8  9.86  13.7  9.60  9.89  10.4  11.7  7.16  10.2  8.29  12.7  11.1  Table S11.
ANI model details.
Model details on the individual active learned ANI models (ANI-AL) and  a network ensemble trained to the original ANI-1 data set.
AL version is an internal versioning scheme  which allows tracking of the data included at any given model version.
AL cycles are the number of  active learning conformational search cycles completed to produce the given ANI-AL model.
Parameters is the total number of parameters in the models, all of which consisted of an input size of  386 and 3 total hidden layers with varying numbers of nodes per layer.
Test set potential energy (E)  RMSE are provided in kcal  × mol(cid:2879)(cid:2869).
Model  AL version  AL Cycles  Parameters  AL1  AL2  AL3  AL4  AL5  ANI-1X  (AL6)  ANI-1  6.0.0  6.2.4  7.0.4  8.0.5  8.3.4  9.0.5  -  0  9  13  18  32  37  -  57472  84096  84096  120000  184512  389376  270592  Test set  RMSE (E)  Configurational  Sampling  0.8  1.1  1.6  2.2  2.6  2.7  1.2  GDB-1 to 6  GDB-1 to 6  GDB-1 to 7  GDB-1 to 8  GDB-1 to 8; GDB-1 to 6 dimers; amino acids;  dipeptides; CheMBLE  GDB-1 to 9; GDB-1 to 6 dimers; amino acids;  dipeptides; CheMBLE  GDB-1 to 8  Table S12.
Complete COMP6 benchmark for ANI-1x within select energy ranges.
Errors for  conformer energy differences (∆E), potential energies (E), and force components (F) for the active  learned ANI potential ANI-1x over select energy ranges of the test set.
The test set for a given energy  range is built by only considering conformations of a given molecule within the energy range (shown  in column 2) from the minimum energy conformer in the set of conformations.
These results are from  the combination of all benchmarks within the COMP6 benchmark suite.
µ and σ are the arithmetic  mean and standard deviation, respectively.
M and R are the MAE and RMSE, respectively.
Units of  energy are kcal  × mol(cid:2879)(cid:2869) and units of force are kcal  × mol(cid:2879)(cid:2869) × Å(cid:2879)(cid:2869).
ANI  Model  𝝁   𝑭𝑴 𝝁  𝑬𝑹 𝝈  𝑬𝑹 𝝈   𝑭𝑴 𝝁  𝑭𝑹 𝝈  𝑭𝑹 Energy  Range  ∆𝑬𝑴 0.65  1.05  1.19  1.39  1.58  1.71  1.79  1.82  10  30  50  100  150  200  250  300  𝝁   ∆𝑬𝑴 0.01  0.02  0.03  0.03  0.04  0.04  0.04  0.04  𝝈   ∆𝑬𝑹 1.00  1.65  1.97  2.28  2.54  2.73  2.85  2.91  𝝁  ∆𝑬𝑹 0.05  0.06  0.08  0.11  0.13  0.13  0.15  0.15  𝝁   𝑬𝑴 𝝈  𝑬𝑴 𝝈   0.06  1.29  0.06  1.35  1.44  0.07  0.07  1.61  0.08  1.75  1.84  0.08  0.08  1.89  1.91  0.08  ANI-1x  2.60  2.53  2.71  3.01  3.19  3.28  3.33  3.35  0.93  0.70  0.81  0.91  0.88  0.84  0.81  0.79  2.21  2.35  2.48  2.70  2.87  2.98  3.04  3.07  0.08  0.08  0.08  0.09  0.09  0.09  0.10  0.10  3.68  3.84  4.11  4.53  4.85  5.06  5.18  5.23  0.49  0.41  0.56  0.65  0.64  0.60  0.58  0.57  Table S13.
DrugBank COMP6 benchmark for ANI-1x within select energy ranges.
Errors for  conformer energy differences (∆E), potential energies (E), and force components (F) for the active  learned ANI potential ANI-1x over select energy ranges of the test set.
The test set for a given energy  range is built by only considering conformations of a given molecule within the energy range (shown  in column 2) from the minimum energy conformer in the set of conformations.
µ and σ are the  arithmetic mean and standard deviation, respectively.
M and R are the MAE and RMSE, respectively.
Units of energy are kcal  × mol(cid:2879)(cid:2869) and units of force are kcal  × mol(cid:2879)(cid:2869) × Å(cid:2879)(cid:2869).
ANI  𝝁   𝑭𝑴 Model  𝝁  𝑬𝑹 𝝁  𝑭𝑹 𝝈   𝑭𝑴 𝝈  𝑬𝑹 𝝈  𝑭𝑹 Energy  Range  ∆𝑬𝑴 0.70  0.92  1.16  1.58  1.83  1.97  2.04  2.07  10  30  50  100  150  200  250  300  𝝁   ∆𝑬𝑴 0.10  0.08  0.13  0.13  0.14  0.14  0.14  0.14  𝝈   ∆𝑬𝑹 1.33  1.58  2.08  2.61  2.89  3.04  3.11  3.15  𝝁  ∆𝑬𝑹 0.67  0.66  1.02  1.05  1.00  0.92  0.87  0.85  𝝁   𝑬𝑴 𝝈  𝑬𝑴 𝝈   0.36  2.22  0.27  2.11  2.22  0.31  0.31  2.38  0.30  2.51  2.59  0.29  0.28  2.62  2.64  0.28  ANI-1x  6.83  5.82  6.06  6.15  6.12  6.05  6.02  6.01  4.25  3.37  3.57  3.45  3.28  3.13  3.05  3.02  2.30  2.36  2.48  2.66  2.77  2.82  2.85  2.86  0.19  0.16  0.18  0.18  0.17  0.17  0.16  0.16  4.75  4.73  5.20  5.37  5.35  5.37  5.36  5.35  1.98  1.74  2.23  2.24  2.09  1.95  1.87  1.84  Table S14.
Tripeptide COMP6 benchmark for ANI-1x within select energy ranges.
Errors for  conformer energy differences (∆E), potential energies (E), and force components (F) for the active  learned ANI potential ANI-1x over select energy ranges of the test set.
The test set for a given energy  range is built by only considering conformations of a given molecule within the energy range (shown  in column 2) from the minimum energy conformer in the set of conformations.
µ and σ are the  arithmetic mean and standard deviation, respectively.
M and R are the MAE and RMSE, respectively.
Units of energy are kcal  × mol(cid:2879)(cid:2869) and units of force are kcal  × mol(cid:2879)(cid:2869) × Å(cid:2879)(cid:2869).
ANI  𝝁   𝑭𝑴 Model  𝝁  𝑬𝑹 𝝁  𝑭𝑹 𝝈   𝑭𝑴 𝝈  𝑬𝑹 𝝈  𝑭𝑹 Energy  Range  ∆𝑬𝑴 0.64  0.90  1.07  1.35  1.48  1.53  1.56  1.59  10  30  50  100  150  200  250  300  𝝁   ∆𝑬𝑴 0.03  0.02  0.02  0.02  0.04  0.05  0.05  0.05  𝝈   ∆𝑬𝑹 0.91  1.20  1.43  1.80  2.00  2.09  2.19  2.34  𝝁  ∆𝑬𝑹 0.05  0.03  0.04  0.04  0.07  0.10  0.11  0.11  𝝁   𝑬𝑴 𝝈  𝑬𝑴 𝝈   0.24  2.48  2.59  0.25  0.23  2.67  0.20  2.75  0.19  2.81  2.84  0.19  0.19  2.86  2.88  0.19  ANI-1x  3.05  3.20  3.28  3.38  3.45  3.51  3.58  3.64  0.25  0.26  0.24  0.21  0.18  0.18  0.19  0.18  2.12  2.18  2.25  2.36  2.41  2.44  2.45  2.46  0.04  0.04  0.04  0.03  0.04  0.04  0.04  0.04  3.16  3.24  3.33  3.53  3.72  3.87  3.98  4.21  0.06  0.06  0.06  0.06  0.09  0.17  0.18  0.28  Table S15.
GDB07to09 COMP6 benchmark for ANI-1x within select energy ranges.
Errors for  conformer energy differences (∆E), potential energies (E), and force components (F) for the active  learned ANI potential ANI-1x over select energy ranges of the test set.
The test set for a given energy  range is built by only considering conformations of a given molecule within the energy range (shown  in column 2) from the minimum energy conformer in the set of conformations.
µ and σ are the  arithmetic mean and standard deviation, respectively.
M and R are the MAE and RMSE, respectively.
Units of energy are kcal  × mol(cid:2879)(cid:2869) and units of force are kcal  × mol(cid:2879)(cid:2869) × Å(cid:2879)(cid:2869).
ANI  𝝁   𝑭𝑴 Model  𝝁  𝑬𝑹 𝝁  𝑭𝑹 𝝈   𝑭𝑴 𝝈  𝑬𝑹 𝝈  𝑭𝑹 Energy  Range  ∆𝑬𝑴 0.46  0.71  0.84  0.99  1.05  1.07  1.07  1.07  10  30  50  100  150  200  250  300  𝝁   ∆𝑬𝑴 0.01  0.02  0.02  0.03  0.03  0.03  0.03  0.03  𝝈   ∆𝑬𝑹 0.62  0.95  1.15  1.41  1.58  1.65  1.66  1.66  𝝁  ∆𝑬𝑹 0.02  0.02  0.03  0.04  0.04  0.05  0.05  0.05  𝝁   𝑬𝑴 𝝈  𝑬𝑴 𝝈   0.04  0.80  0.04  0.88  0.93  0.04  0.04  1.00  0.04  1.03  1.04  0.04  0.04  1.04  1.04  0.04  ANI-1x  1.09  1.18  1.25  1.37  1.46  1.49  1.50  1.50  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05  1.93  2.12  2.24  2.37  2.42  2.43  2.43  2.43  0.06  0.06  0.06  0.07  0.07  0.07  0.07  0.07  2.96  3.22  3.42  3.71  3.87  3.92  3.93  3.93  0.08  0.09  0.09  0.10  0.10  0.08  0.08  0.08  Table S16.
GDB10to13 COMP6 benchmark for ANI-1x within select energy ranges.
Errors for  conformer energy differences (∆E), potential energies (E), and force components (F) for the active  learned ANI potential ANI-1x over select energy ranges of the test set.
The test set for a given energy  range is built by only considering conformations of a given molecule within the energy range (shown  in column 2) from the minimum energy conformer in the set of conformations.
µ and σ are the  arithmetic mean and standard deviation, respectively.
M and R are the MAE and RMSE, respectively.
Units of energy are kcal  × mol(cid:2879)(cid:2869) and units of force are kcal  × mol(cid:2879)(cid:2869) × Å(cid:2879)(cid:2869).
ANI  𝝁   𝑭𝑴 Model  𝝁  𝑬𝑹 𝝁  𝑭𝑹 𝝈   𝑭𝑴 𝝈  𝑬𝑹 𝝈  𝑭𝑹 Energy  Range  ∆𝑬𝑴 0.61  0.90  1.09  1.47  1.86  2.13  2.27  2.34  10  30  50  100  150  200  250  300  𝝁   ∆𝑬𝑴 0.01  0.02  0.02  0.03  0.04  0.04  0.05  0.05  𝝈   ∆𝑬𝑹 0.86  1.24  1.51  2.12  2.70  3.09  3.30  3.40  𝝁  ∆𝑬𝑹 0.02  0.03  0.03  0.06  0.06  0.08  0.11  0.11  𝝁   𝑬𝑴 𝝈  𝑬𝑴 𝝈   0.03  1.63  1.66  0.03  0.03  1.72  0.04  1.88  0.05  2.05  2.18  0.05  0.05  2.24  2.27  0.05  ANI-1x  2.18  2.26  2.34  2.60  2.85  3.03  3.13  3.18  0.04  0.09  0.08  0.11  0.10  0.11  0.14  0.14  2.48  2.61  2.75  3.04  3.32  3.50  3.60  3.64  0.07  0.07  0.07  0.08  0.08  0.09  0.09  0.09  3.94  4.13  4.34  4.85  5.35  5.67  5.87  5.95  0.12  0.12  0.12  0.13  0.14  0.16  0.17  0.17  Table S17.
Complete COMP6 benchmark per atom errors for ANI-1x within select energy ranges.
Per  atom errors for conformer energy differences (∆E) and potential energies (E) achieved by the active  learned ANI potential ANI-1x over select energy ranges of the entire COMP6 benchmark.
The test set  for a given energy range is built by only considering conformations of a given molecule within the  energy range (shown in column 2) from the minimum energy conformer in the set of conformations.
µ is the arithmetic mean.
M and R are the MAE and RMSE, respectively.
Units of energy are  kcal  × mol(cid:2879)(cid:2869) per atom.
ANI Model  Energy Range  𝝁   𝑬𝑴 0.059  0.061  0.064  0.070  0.075  0.077  0.078  0.079  𝝁  𝑬𝑹 0.101  0.099  0.105  0.117  0.125  0.129  0.130  0.131  ANI-1x  10  30  50  100  150  200  250  300  𝝁   ∆𝑬𝑴 0.031  0.042  0.049  0.059  0.067  0.072  0.074  0.075  𝝁  ∆𝑬𝑹 0.044  0.058  0.070  0.090  0.107  0.116  0.120  0.122  Smith, J.
S.; Isayev, O.; Roitberg, A.
E.
ANI-1: An Extensible Neural Network Potential with DFT  Accuracy at Force Field Computational Cost.
Chem.
Sci.
2017, 27, 479–496.
Kingma, D.; Ba, J.
Adam: A Method for Stochastic Optimization.
arXiv:1412.6980 [cs.LG] 2014, 1– 15.
Brauer, B.; Kesharwani, M.
K.; Kozuch, S.; Martin, J.
M.
L.
The S66x8 Benchmark for Noncovalent  Interactions Revisited: Explicitly Correlated Ab Initio Methods and Density Functional Theory.
Phys.
Chem.
Chem.
Phys.
2016, 18 (31), 20905–20925.
(1)   (2)   (3)
Our visual world is richly ﬁlled with a great variety of textures, present in images ranging from multispectral satellite views to mi- croscopic pictures of tissue samples (Fig.
1).
As a powerful visual cue like color, texture provides useful information in identifying objects or regions of interest in images.
Texture is different from color in that it refers to the spatial organization of a set of basic elements or primitives (i.e., textons), the fundamental microstruc- tures in natural images and the atoms of preattentive human visual   Li Liu (li.liu@oulu.ﬁ) Jie Chen (jie.chen@oulu.ﬁ) Paul Fieguth (pﬁeguth@uwaterloo.ca) Guoying Zhao (guoying.zhao@oulu.ﬁ) Rama Chellappa (rama@umiacs.umd.edu) Matti Pietik¨ainen (matti.pietikainen@oulu.ﬁ) 1 University of Oulu, Finland 2 National University of Defense Technology, China 3 University of Waterloo, Canada 4 University of Maryland, USA perception [81].
A textured region will obey some statistical prop- erties, exhibiting periodically repeated textons with some degree of variability in their appearance and relative position [51].
Tex- tures may range from purely stochastic to perfectly regular and everything in between (see Fig.
1).
As a longstanding, fundamental and challenging problem in the ﬁelds of computer vision and pattern recognition, texture anal- ysis has been a topic of intensive research since the 1960’s [80] due to its signiﬁcance both in understanding how the texture per- ception process works in human vision as well as in the impor- tant role it plays in a wide variety of applications.
The analysis of texture traditionally embraces several problems including clas- siﬁcation, segmentation, synthesis and shape from texture [202].
Signiﬁcant progress has been made since the 1990’s in the ﬁrst three areas, with shape from texture receiving comparatively less attention.
Typical applications of texture analysis include medi- cal image analysis [41, 138, 157], quality inspection [226], con- tent based image retrieval [128, 194, 242], analysis of satellite or aerial imagery [84, 71], face analysis [3, 42, 193, 241], biometrics [118, 165], object recognition [188, 153, 239], texture synthesis for computer graphics and image compression [56, 57], and robot vi- sion and autonomous navigation for unmanned aerial vehicles.
The ever-increasing amount of image and video data due to surveil- lance, handheld devices, medical imaging, robotics etc.
offers an endless potential for further applications of texture analysis.
Texture representation, i.e., the extraction of features that de- scribe texture information, is at the core of texture analysis.
After over ﬁve decades of continuous research, many kinds of theories and algorithms have emerged, with major surveys and some rep- resentative work as follows.
The majority of texture features be- fore 1990 can be found in comparative studies [32, 67, 140, 174, 202, 211, 221].
Tuceryan and Jain [202] identiﬁed ﬁve major cat- egories of features for texture discrimination: statistical, geometri- cal, structural, model based, and ﬁltering based features.
In 1996, Ojala et al.
[141] carried out a comparative study to evaluate tex- ture feature performance.
In 1999, Randen and Husøy [170] re- viewed most major ﬁltering based texture features and performed a comparative performance evaluation.
In 2002, Zhang and Tan [238] reviewed invariant texture feature extraction methods.
In 2007, Zhang et al.
[239] evaluated the performance of several major invariant lo- 2 Li Liu et al.
Fig.
1 Texture is an important characteristic of many types of images.
cal texture descriptors.
The book “Handbook of Texture Analysis” edited by Mirmehdi et al.
[137] in 2008 contains representative work on texture analysis — from 2D to 3D, from feature extrac- tion to synthesis, and from texture image acquisition to classiﬁ- cation.
The book “Computer Vision Using Local Binary Patterns” by Pietik¨ainen et al.
[165] in 2011 provides an excellent overview of the theory of Local Binary Pattern (LBP) and the use in solv- ing various kinds of problems in computer vision, especially in biomedical applications and biometric recognition systems.
Huang et al.
[74] presented a review of the LBP variants in the applica- tion area of facial image analysis.
The book “Local Binary Pat- terns: New Variants and Applications” by Brahnam et al.
[16] in 2014 is a collection of several new LBP variants and their applica- tions to face recognition.
More recently, Liu et al.
[114] conducted a taxonomy of recent LBP variants and performed a large scale performance evaluation of forty texture features.
Raad et al.
[169] presented a review of exemplar based texture synthesis approaches.
The published surveys [32, 67, 140, 173, 174, 141, 163, 202, 211] mainly reviewed or compared methods prior to 1995.
Simi- larly, the articles [170, 238] only covered approaches before 2000.
There are more recent surveys [16, 74, 114, 165], however they focused exclusively on texture features based on LBP.
The emer- gence of many powerful texture analysis techniques has given rise to a further increase in research activity in texture research since 2000, however none of these published surveys provides an exten- sive survey over that time.
Upon the time of change, we believe that there is a need for an updated survey, motivating this present work.
A thorough review and survey of existing work, the focus of this paper, will contribute to more progress in texture analysis.
Our goal is to overview the core tasks and key challenges in texture, to deﬁne taxonomies of representative approaches, to provide a re- view of texture datasets, and summarizing the performance of the state of the art on publicly available databases.
According to the different visual representations, this survey categorizes the texture classiﬁcation literature into three broad types: BoW-based, CNN- based and attribute-based.
The BoW-based methods are organized according to its key components.
The CNN-based methods are cat- egorized into using pretrained CNN models, using ﬁnetuned CNN models, as well as using handcrafted deep convolutional networks.
The remainder of this paper is organized as follows.
Related background, including the problem and its applications, the progress made during the past decades, and the challenges of the problem, are summarized in Section 2.
From Section 3 to Section 5, we give a detailed review of texture representation techniques for tex- ture classiﬁcation by providing a taxonomy to more clearly group the prominent alternatives.
A summarization of benchmark texture databases and state of the art performance is given in Section 6.
We conclude the paper with a discussion of several promising di- rections for texture representation in Section 7.
2 Background 2.1 The Problem Texture analysis can be divided into four areas: classiﬁcation, seg- mentation, synthesis, and shape from texture [202].
Texture classi- ﬁcation [96, 109, 202, 213, 214] deals with designing algorithms for declaring a given texture region or image as belonging to one of a set of known texture categories of which training samples have been provided.
Texture classiﬁcation may also be a binary hy- pothesis testing problem, such as differentiating a texture as being within or outside of a given class, such as distinguishing between healthy and pathological tissues in medial image analysis.
The goal of texture segmentation is to partition a given image into disjoint regions of homogeneous texture [77, 127, 173, 188].
Texture syn- thesis is the process of generating new texture images which are perceptually equivalent to a given texture sample [48, 56, 166, 169, 219, 245].
As textures provide powerful shape cues, approaches for shape from texture attempt to recover the three dimensional shape of a textured object from its image.
It should be noted that the con- cept of “texture” may have different connotations or deﬁnitions de- pending on the given objective.
Classiﬁcation, segmentation, and synthesis are closely related and widely studied, with shape from texture receiving comparatively less attention.
Nevertheless, tex- ture representation is at the core of these four problems.
Texture representation, together with texture classiﬁcation, will form the primary focus of this survey.
As a classical pattern recognition problem, texture classiﬁca- tion primarily consists of two critical subproblems: texture rep- resentation and classiﬁcation [78].
It is generally agreed that the extraction of powerful texture features plays a relatively more im- portant role, since if poor features are used even the best classiﬁer will fail to achieve good results.
While this survey is not explic- itly concerned with texture synthesis, studying synthesis can be instructive, for example, classiﬁcation of textures via analysis by synthesis [56] in which a model is ﬁrst constructed for synthesiz- ing textures and then inverted for the purposes of classiﬁcation.
As TexturesinnaturalimagesMicroscopicAerialLightFieldSyntheticApertureRadarProstatecancerNodularChestXRayNormalA Survey of Recent Advances in Texture Representation Fig.
2 The evolution of texture representation over the past decades (see discussion in Section 2.2).
a result, we will include representative texture modeling methods in our discussion.
as a Markov Random Field (MRF) [34, 129, 26, 103] or fractal models [85, 126].
2.2 Summary of Progress in the Past Decades Milestones in texture representation over the past decades are listed in Fig.
2.
The study of texture analysis can be traced back to the earliest work of Julesz [80] in 1962, who studied the theory of hu- man visual perception of texture and suggested that texture might be modelled using kth order statistics — the cooccurrence statistics for intensities at k-tuples of pixels.
Indeed, early work on texture features in the 1970s, such as the well known Gray Level Cooc- currence Matrix (GLCM) method [68, 67], were mainly driven by this perspective.
Aiming at seeking essential ingredients in terms of features and statistics in human texture perception, in the early 1980s Julesz [81, 82] proposed the texton theory to explain texture preattentive discrimination, which states that textons (composed of local conspicuous features such as corners, blobs, terminators and crossings) are the elementary units of preattentive human texture perception and only the ﬁrst order statistics of textons have percep- tual signiﬁcance: textures having the same texton densities could not be discriminated.
Julesz’s texton theory has been widely stud- ied and has largely inﬂuenced the development of texture analysis methods.
Research on texture features in the late 1980s and the early 1990s mainly focused on two well-established areas: 1.
Filtering approaches, which convolve an image with a bank of ﬁlters followed by some nonlinearity.
One pioneering ap- proach was that of Laws [94], where a bank of separable ﬁlters was applied, with subsequent ﬁltering methods including Ga- bor ﬁlters [15, 77, 203], Gabor wavelets [128], wavelet pyra- mids [52, 124], and simple linear ﬁlters like Differences of Gaussians [122].
2.
Statistical modelling, which characterizes texture images as arising from probability distributions on random ﬁelds, such At the end of the last century there was a renaissance of texton- based approaches, including Zhu’s work et al.
[224, 225, 245, 246, 244, 247] on the mathematical modelling of textures and textons.
A notable stride was the Bag of Textons (BoT) [100] and later Bag of Words (BoW) [35, 194] approaches, where a dictionary of textons is generated, and images are represented statistically as orderless histograms over the texton dictionary.
In the 1990s, the need for invariant feature representations was recognized, to reduce or eliminate sensitivity to variations such as illumination, scale, rotation, view point etc.
This gave rise to the development of local invariant descriptors, particularly mile- stone texture features such as Scale Invariant Feature Transform (SIFT) [116] and LBP [143].
Such local hand-crafted texture de- scriptors dominated many domains of computer vision until the turning point in 2012 when deep Convolutional Neural Networks (CNN) [90] achieved record-breaking image classiﬁcation accu- racy.
Since that time the research focus has been on deep learning methods for many problems in computer vision, including texture analysis [28, 29, 30].
Since the 2000s, texture representations (such as LBP [143], BoT [100], Fisher Vector (FV) [180], and wavelet Scattering Con- volution Networks (ScatNet) [19]) have been used extensively in most areas of image understanding and computer vision.
As a re- sult, the division between texture descriptors and more generic im- age or video descriptors has been narrowing.
The study of texture representation continues to play an important role in computer vi- sion and pattern recognition.
2.3 Key Challenges In spite of several decades of development, most texture features have not been capable of performing at a level sufﬁcient for real- world textures and are computationally too complex to meet the real-time requirements of many computer vision applications.
The TexturePerceptionModel(BelaJulesz)1970GLCM(Haralicketal.)19902014DCNNforImageNet(Krizhevskyetal.)FV-CNN(Cimpoietal.)1980198320152004201019621973Wavelet(StephaneMallat)SIFT(DavidLowe)20062002MultiScaleLBP(Ojalaetal.)VideoGoogle(SivicandZisserman)FractalModel(Kelleretal.)ImprovedFV(Perronninetal.)ScatteringConvolutionalNetwork(Bruna(cid:3)etal.)CNNforTextureSynthesis(Gatys(cid:3)etal.)UnifiedTheoryforTextureModeling(Zhuetal.)texturerepresentationsintheearlyyearstexturerepresentationsinthenewcentury(thefocusofthissurvey)20082012MRFTextureModel(CrossandJain)BagofTextons(LeungandMalik)2000GaborWavelets(ManjunathandMa)HarrisCornersandLaplacianBlobs(Lazebniketal.)LBP-TOPforDynamicTexture(ZhaoandPietikäinen)LBPforFacialTexture(Ahonenetal.)Textonboost(Shottonetal.)LawsFilterMasks(KennethLaws)GaussianMRF(ChellappaandChatterjee)DeCAFandIFV(Cimpoietal.)TheTextonTheory(BelaJulesz,Nature)GaborFilters(MarkTurner)4 Li Liu et al.
Fig.
3 Illustrations of challenges in texture recognition.
Dramatic intraclass variations: (a) Illumination variations, (b) View point and local nonrigid deformation, (c) Scale variations, and (d) Different instances from the same category.
Small interclass variations make the problem harder still: (e) Images from the FMD database, and (f) Images from the LFMD database (photographed with a light-ﬁeld camera).
The reader is invited to identify the material category of the foreground surfaces in each image in (e) and (f).
The correct answers are (from left to right): (e) glass, leather, plastic, wood, plastic, metal, wood, metal and plastic; (f) leather, fabric, metal, metal, paper, leather, water, sky and plastic.
Section 6 gives details regarding texture databases.
inherent difﬁculty in obtaining powerful texture representations lies in balancing two competing goals: high quality representation and high efﬁciency.
High Quality Related Challenges mainly consist of the large intraclass appearance variations caused by changes in illumination, rotation, scale, blur, noise, occlusion, etc.
and potentially small in- terclass appearance differences, requiring texture representations to be of high robustness and distinctiveness.
Illustrative examples are shown in Fig.
3.
A further difﬁculty is in obtaining sufﬁcient training data in the form of labeled examples, which are frequently available only in limited amounts due to collection time or cost.
High Efﬁciency Related Challenges include the potentially large number of different texture categories and their high dimen- sional representations.
Here we have polar opposite motivations: that of big data, with associated grand challenges and the scala- bility/complexity of huge problems, and that of tiny devices, the growing need for deploying highly compact and efﬁcient texture representations on resource limited platforms such as embedded and handheld devices.
3 Bag of Words based Texture Representation The goal of texture representation or texture feature extraction is to transform the input texture image into a feature vector that de- scribes the properties of a texture, facilitating subsequent tasks such as texture classiﬁcation, as illustrated in Fig.
4.
Since texture is a spatial phenomenon, texture representation cannot be based on a single pixel, and generally requires the analysis of patterns over local pixel neighborhoods.
Therefore, a texture image is ﬁrst transformed to a pool of local features, which are then aggregated into a global representation for an entire image or region.
Since the properties of texture are usually translationally invariant, most tex- Fig.
4 The goal of texture representation is to transform the input texture im- age into a feature vector that describes the properties of the texture, facilitating subsequent tasks such as texture recognition.
Usually a texture image is ﬁrst transformed into a pool of local features, which are then aggregated into a global representation for an entire image or region.
ture representations are based on an orderless aggregation of local texture features, such as a sum or max operation.
Early in 1981, Julesz [81] introduced “textons”, which refer to basic image features such as elongated blobs, bars, crosses, and terminators, as the elementary units of preattentive human texture perception.
However Julesz’s texton studies were limited by their exclusive focus on artiﬁcial texture patterns rather than natural tex- tures.
In addition, Julesz did not provide a rigorous deﬁnition for textons.
Subsequently, texton theory fell into disfavor as a model of texture discrimination until the inﬂuential work by Leung and Malik [100] who revisited textons and gave an operational deﬁni- tion of a texton as a cluster center in ﬁlter response space.
This not only enabled textons to be generated automatically from an image, but also opened up the possibility of learning a universal texton dictionary for all images.
Texture images can be statisti- cally represented as histograms over a texton dictionary, referred (a)CUReT(b)UIUC(d)DTD(c)KTHTIPS2b(e)FMD(f)LFMDATextureRegion/ImageIfApatchcenteredatlocationiLocalFeaturesGlobalRepresentation()I)()IIo)Goal:ApatchcenteredatlocationjixffjxA Survey of Recent Advances in Texture Representation to as the Bag of Textons (BoT) approach.
Although BoT was ini- tially developed in the context of texture recognition [100, 123], it was introduced / generalized to image retrieval [194] and classiﬁ- cation [35], where it was referred to as Bag of Features (BoF) or, more commonly, Bag of Words (BoW).
The research community has since witnessed the prominence of the BoW model for over a decade during which many improvements were proposed.
3.1 The BoW Pipeline The remainder of this section will introduce the methods in each component, as summarized in Table 1.
3.2 Local Texture Descriptors All local texture descriptors aim to provide local representations invariant to contrast, rotation, scale, and possibly other criteria.
The primary categorization is whether the descriptor is applied densely, at every pixel, as opposed to sparsely, only at certain lo- cations of interest.
The BoW pipeline is sketched in Fig.
5, consisting of the following basic steps.
1.
Local Patch Extraction.
For a given image, a pool of N image patches is extracted over a sparse set of points of interest [96, 239], over a ﬁxed grid [88, 130, 184], or densely at each pixel position [143, 213, 214].
i=1, 2.
Local Patch Representation.
Given the extracted N patches, local texture descriptors are applied to obtain a set or pool of tex- ture features of D dimension.
We denote the local features of N patches in an image as {xi}N i=1, xi ∈ RD.
Ideally, local descrip- tors should be distinctive and at the same time robust to a variety of possible image transformations, such as scale, rotation, blur, il- lumination, and viewpoint changes.
High quality local texture de- scriptors play a critical role in the BoW pipeline.
3.
Codebook Generation.
The objective of this step is to gen- erate a codebook (i.e., a texton dictionary) with K codewords {wi}K wi ∈ RD based on training data.
The codewords may be learned (e.g., by kmeans [95, 213]) or in a predeﬁned way (such as LBP [143]).
The size and nature of the codebook affects the represen- tation followed and thus the discrimination power.
The key here is how to generate a compact and discriminative codebook so as to enable accurate and efﬁcient classiﬁcation.
4.
Feature Encoding.
Given the generated codebook and the extracted local texture features {xi} from an image, the role of fea- ture encoding is to represent each local feature xi with the code- book, usually by mapping each xi to one or a number of code- words, resulting a feature coding vector vi (e.g. vi ∈ RK).
Of all the steps in the BoW pipeline, feature encoding is a core compo- nent which links local representation and feature pooling, greatly inﬂuencing texture classiﬁcation in terms of both accuracy and speed.
Thus, many studies have focused on developing powerful feature encoding, such as vector quantization / kmeans, sparse cod- ing [120, 121, 161], Locality constrained Linear Coding (LLC) [216], Vector of Locally Aggregated Descriptors (VLAD) [79], and Fisher Vector (FV) [30, 159, 180].
5.
Feature Pooling.
A global feature representation y is pro- duced by using a feature pooling strategy to aggregate the coded feature vectors {vi}.
Classical pooling methods include average pooling, max pooling, and Spatial Pyramid Pooling (SPM) [97, 201].
6.
Feature Classiﬁcation.
The global feature is used as the basis for classiﬁcation, for which many approaches are possible [78, 218]: Nearest Neighbor Classiﬁer (NNC), Support Vector Ma- chines (SVM), neural networks, and random forests.
SVM is one of the most widely used classiﬁers for the BoW based representa- tion.
3.2.1 Sparse Texture Descriptors To develop a sparse texture descriptor, a region of interest detec- tor must be designed which is able to reliably detect a sparse set of regions, reliably and stably, under various imaging conditions.
Typically, the detected regions undergo a geometric normalization, after which local descriptors are applied to encode the image con- tent.
A series of region detectors and local descriptors has been proposed, with excellent surveys [134, 135, 204].
The sparse ap- proach was introduced to texture recognition by Lazebnik et al.
[95, 96] and followed by Zhang et al.
[239].
In [96], two types of complementary region detectors, i.e. the Harris afﬁne detector of Mikolajczyk and Schmid [133] and the Laplacian blob detector of G˚arding and Lindeberg [55], were used to detect afﬁne covariant regions, meaning that the region content is afﬁne invariant.
Each detected region can be thought of as a tex- ture element having a characteristic elliptic shape and a distinc- tive appearance pattern.
In order to achieve afﬁne invariance, each elliptical region was normalized and then two rotation invariant descriptors, the spin image (SPIN) and the Rotation Invariant Fea- ture Transform (RIFT) descriptor, were applied.
As a result, for each texture image four feature channels were extracted (two de- tectors × two descriptors), and for each feature channel kmeans clustering is performed to form its signature.
The Earth Mover’s Distance (EMD) [177] was used for measuring the similarity be- tween image signatures and NNC was used for classiﬁcation.
Al- though SPIN and RIFT achieve rotation invariance, they lack dis- tinctiveness since some spatial information is lost due to their fea- ture pooling schemes.
Following Lazebnik et al.
[96], Zhang et al.
[239] presented an evaluation of multiple region detector types, levels of geometric invariance, multiple local texture descriptors, and SVM classiﬁer with kernels based on two effective measures for comparing distri- butions (signatures and EMD distance vs.
standard BoW and the Chi Square distance) for texture and object recognition.
Regarding local description, Zhang et al.
[239] also used the SIFT descriptor1 in addition to SPIN and RIFT.
They recommended that practical texture recognition should seek to incorporate multiple types of complementary features, but with local invariance properties not exceeding those absolutely required for a given application.
Other local region detectors have also been used for texture description, such as the Scale Descriptors which measure the scales of salient textons [83].
1 Originally, SIFT is comprised of a detector and descriptor, but which are used in isolation now; in this survey, if not speciﬁed, SIFT refers to the de- scriptor, a common practice in the community.
6 Li Liu et al.
Fig.
5 General pipeline of the BoW model.
See Table 1, and also refer to Section 3 for detail discussion.
Features are computed from handcrafted detectors for descriptors like SIFT and RIFT, and densely applied local texture descriptors like handcrafted ﬁlters or CNNs. The CNN features can also be computed in an end to end manner using ﬁnetuned CNN models.
These local features are quantized to visual words in a codebook.
3.2.2 Dense Texture Descriptors The number of features derived from a sparse set of interesting points is much smaller than the total number of image pixels, re- sulting a compact feature space.
However, the sparse approach can be inappropriate for many texture classiﬁcation tasks: ◦ Interest point detectors typically produce a sparse output and ◦ A sparse output in a small image might not produce sufﬁcient ◦ There are issues regarding the repeatability of the detectors, the stability of the selected regions and the instability of orienta- tion estimation [135].
regions for robust statistical characterization.
could miss important texture elements.
As a result, extracting local texture features densely at each pixel is the more popular representation, the subject of the following discussion.
(1) Gabor Filters are one of the most popular texture descrip- tors, motivated by their relation to models of early visual systems of mammals as well as their joint optimum resolution in time and frequency [77, 99, 128].
As illustrated in Fig.
6, Gabor ﬁlters can be considered as orientation and scale tunable edge and bar de- tectors.
The Gabor wavelets are generated by appropriate rotations and dilations from the following product of an elliptical Gaussian and a complex plane wave: exp(j2πω), (cid:19)(cid:21) φ(x, y) = (cid:18) x2 (cid:20) (cid:18) (u − ω)2 exp 2σ2 2πσxσy (cid:20) whose Fourier transform is ˆφ(x, y) = exp v2 2σ2 2σ2 y2 2σ2 (cid:19)(cid:21) where ω is the radial center frequency of the ﬁlter in the frequency domain, σx and σy are the standard deviations of the elliptical Gaussian along x and y.
Thus, a Gabor ﬁlter bank is deﬁned by its parameters including frequencies, orientations and the parameters of the Gaussian enve- lope.
In the literature, different parameter settings have been sug- gested, and ﬁlter banks created by these parameter settings work well in general.
Details for the derivation of Gabor wavelets and parameter selection can be found in [99, 128, 160].
(2) The Schmid Filters (SFilters) [181] consist of 13 rotation- ally invariant Gabor-like ﬁlters of the form (cid:18) x2 + y2 (cid:19)(cid:21) (cid:20) cos 2σ2 (cid:32) πβ(cid:112)x2 + y2 (cid:33) φ(x, y) = exp where β is the number of cycles of the harmonic function within the Gaussian envelope of the ﬁlter.
The ﬁlters are shown in Fig.
7; as can be seen, all of the ﬁlters have rotational symmetry.
(3) Filters by Leung and Malik (LM Filters) [100, 123] pi- oneered the problem of classifying textures under varying view- point and illumination.
The LM ﬁlters used for local texture fea- ture extraction are illustrated in Fig.
8.
In particular, they marked a milestone by giving an operational deﬁnition of textons: the cluster centers of the ﬁlter response vectors.
Their work has been widely followed by other researchers [35, 96, 188, 194, 213, 214].
To han- dle 3D effects caused by imaging, they proposed 3D textons which were cluster centers of ﬁlter responses over a stack of images with representative viewpoints and lighting, as illustrated in Fig.
9.
In their texture classiﬁcation algorithm, 20 images of each texture were geometrically registered and transformed into 48D local fea- tures with the LM Filters.
Then the 48D ﬁlter response vectors of 20 selected images of the same pixel were concatenated to obtain a 960D feature vector as the local texture representation.
Subse- quently, these 960D feature vectors were input into a BoW pipeline for texture classiﬁcation.
A downside of the method is that it is not suitable for classifying a single texture image under unknown imaging conditions, which usually arises in practical applications.
(4) Maximum Response (MR8) Filters of Varma and Zisser- man [213] consist of 38 root ﬁlters but only 8 ﬁlter responses.
The ﬁlter bank contains ﬁlters at multiple orientations but their outputs are pooled by recording only the maximum ﬁlter response across all orientations, in order to achieve rotation invariance.
The root ﬁlters are a subset of the LM Filters [100] of Fig.
8, retaining the two rotational symmetry ﬁlters, the edge ﬁlter, and the bar ﬁlter at 3 scales and 6 orientations.
Recording only the maximum response (cid:311)(cid:312)(cid:313)(cid:314)Feature Detectors and DescriptorsFeatureVectorsFeature Encoding (with a Codebook)OrderlessPoolingPredefined FiltersOne Pass CNN/MultiPass CNNd-dimp-dimq-dimmaps ofmaps ofHarrris CornersLaplacian BlobsRIFT, SIFTTexture Image (Size               )key pointsClassifierAveragePoolingFullyConnectedsoftmaxOrReorderingintoaFeatureVectorA Survey of Recent Advances in Texture Representation Table 1 A summary of components in the BoW representation pipeline, as was sketched in Fig.
5.
Step Approach Highlights Sparse Descriptors • (Harris+Laplacian)(RIFT+SPIN) [96] • (Harris+Laplacian)(RIFT+SPIN+SIFT) [239] Keypoint detectors plus novel descriptors SPIN and RIFT A comprehensive evaluation of multiple keypoint detectors, feature descriptors, and classiﬁer kernels.
Dense Descriptors • Gabor Wavelets • LMﬁlters [100] • Schmid Filters • MR8 [213] • Patch Intensity [214] • LBP [143] • Random Projection [109] • Sorted Random Projection [110] • Basic Image Features (BIFs) [33] • Weber Local Descriptor (WLD) [33] Fractal Based Descriptors • MultiFractal Spectrum [229] ) Predeﬁned [33, 143] kmeans clustering [35, 100] GMM modeling [30, 159, 187] Sparse Coding based learning [161, 195] Voting Based Methods • Hard Voting [100, 213] • Soft Voting [2, 175, 210] Fisher Vector (FV) Based Methods • FV [158] • Improved FV (IFV) [28, 159, 187] • VLAD [79, 28] Reconstruction Based Methods Joint optimum resolution in time and frequency; Multiscale and multiorientation analysis.
First to propose Bag of Texton (BoT) model (i.e. the BoW model) Gabor like ﬁlters; Rotation invariant.
Rotationally invariant ﬁlters and low-dimensional ﬁlter response space.
Challenge the dominant role of ﬁlter descriptors and propose image raw intensity feature.
Fast binary features with gray scale invariance; Predeﬁned codebook.
First to introduce compressive sensing and random projection into texture classiﬁcation.
Efﬁcient and effective approach for random projection to achieve rotation invariance.
Introduce BIFs of Grifﬁn and Lillholm into texture classiﬁcation; Predeﬁned codebook.
A descriptor based on Weber’s Law.
Invariant under the bi-Lipschitz mapping.
No codebook learning step; Computationally efﬁcient.
Most commonly used method; Cannot capture overlapping distributions in the feature space.
Considers both cluster centers and covariances which describe the spreads of clusters.
Sparse representation based; Minimize reconstruction error of data; Computationally expensive.
Require a large codebook (usually learned by kmeans); Usually combine with nonlinear SVM.
Quantize each feature to nearest codeword; Fast to compute; Codes are sparse and high dimensional.
Assigns each feature to multiple codewords; Does not minimize reconstruction error.
Require a small codebook; Very high dimension; Combines with efﬁcient linear SVM.
GMM-based; Encodes higher order statistics; Efﬁcient to compute.
Uses signed square rooting and L2 normalization; State of the art performance in texture classiﬁcation.
A simpliﬁed version of FV.
Enforce sparse representation; Explores the manifold structure of data; Minimize reconstruction error.
Leverage that fact that natural images are sparse; Optimization is computationally expensive.
• Sparse Coding [161, 195, 232] • Local constraint Linear Coding (LLC) [28, 216] Local smooth sparsity; Fast computation through approximated LLC.
) Average Pooling Max Pooling Spatial Pyramid Pooling (SPM) ) Nearest Neighbor Classiﬁer (NNC) [109, 213] Kernel SVM [239] Linear SVM [30] The most widely used pooling scheme in texture representation.
Usually used in combination with sparse coding and LLC.
Preserving more spatial information; Higher feature dimensionality.
Simple and elegant nonparametric classiﬁer; Popular in texture classiﬁcation.
Usually in combination with Chi Square for BoW based representation.
Suitable for high-dimensional feature representation like FV and VLAD.
Fig.
6 Illustration of the Gabor wavelets used in [128].
Fig.
7 Illustration of the rotationally invariant Gabor-like ﬁlters used in [181].
The parameter (σ, β) pair takes values (2,1), (4,1), (4,2), (6,1), (6,2), (6,3), (8,1), (8,2), (8,3), (10,1), (10,2), (10,3) and (10,4).
across orientations reduces the number of responses from 38 to 8 Fig.
8 The LMﬁlter bank has a mix of edge, bar and spot ﬁlters at multiple scales and orientations.
It has a total of 48 ﬁlters: 2 Gaussian derivative ﬁlters at 6 orientations and 3 scales, 8 Laplacian of Gaussian ﬁlters and 4 Gaussian ﬁlters.
(3 scales for 2 anisotropic ﬁlters, plus 2 isotropic), resulting the so-called MR8 ﬁlter bank.
Realizing the shortcomings of Leung and Malik’s method [100], Varma and Zisserman [213] attempted to improve the classiﬁcation of a single texture sample image under unknown imaging condi- tions, bypassing the registration step, instead learning 2D textons by aggregating ﬁlter responses over different images.
The other steps of the classiﬁcation process were roughly the same as those of Leung and Malik.
Experimental results showed that MR8 out- performed the LM Filters and Schmid Filters.
Later, Hayman et al.
(a) Real Part(b) Imaginary Part8 Li Liu et al.
Fig.
9 Illustration of the process of 3D texton dictionary learning proposed by Leung and Malik [100].
Each image at different lighting and viewing direc- tions is ﬁltered using the ﬁlter bank illustrated in Fig.
8.
The response vectors are concatenated together to form data vectors of length Nf ilNim.
These data vectors are clustered using the kmeans algorithm to obtain the 3D textons.
Fig.
11 An illustration of SRP descriptor: extracting SRP features on an exam- ple local image patch of size 7 × 7.
(a) Sorting pixel intensities; (b,c) Sorting pixel differences.
Fig.
10 Illustration for the Patch Descriptor proposed in [214]: the raw inten- sity vector is used directly as the local representation.
[70] showed that SVM could further enhance the texture classiﬁ- cation performance of MR8 features.
(5) Patch Descriptors of Varma and Zisserman [214] chal- lenged the dominant role of the ﬁlter banks [132, 170] in texture analysis, and instead developed a simple Patch Descriptor simply keeping the raw pixel intensities of a square neighborhood to form a feature vector, as illustrated in Fig.
10.
By replacing the MR8 ﬁlter responses with the Patch Descriptor in texture classiﬁcation [213], Varma and Zisserman [214] observed very good classiﬁca- tion performance using extremely compact neighborhoods (3× 3), and that for any ﬁxed size of neighborhood the Patch Descriptor leads to superior classiﬁcation as compared to ﬁlter banks with the same support.
A clear limitation of the Patch Descriptor itself is sensitivity to nearly any change (brightness, rotation etc.).
(6) Random Projection (RP) and Sorted Random Projec- tion (SRP) features of Liu and Fieguth [109] were inspired by theories of sparse representation and compressed sensing [21, 44].
Taking advantage of the sparse nature of textured images, a small set of random features is extracted from local image patches by projecting the local patch feature vectors to a lower dimensional feature subspace.
The random projection is a ﬁxed, distance-preserving embedding capable of alleviating the curse of dimensionality [8, 60].
The random features are embedded into BoW to perform tex- ture classiﬁcation.
Like the Patch Descriptors, the RP features remain sensitive to image rotation.
To further improve robustness, Liu et al.
[111, 110] proposed sorting the RP features, as illustrated in Fig.
11, whereby Fig.
12 A circular neighborhood used to derive an LBP code: a central pixel xc and its p circularly and evenly spaced neighbors on a circle of radius r.
rings of pixel values are sorted, without any reference orientation, ensuring rotation invariance.
Two kinds of local features are used, one based on raw intensities and the other on gradients (radial dif- ferences and angular differences).
Random functions of the sorted local features are taken to obtain SRP features.
It was shown that SRP outperformed RP signiﬁcantly for robust texture classiﬁcation [111, 110].
(7) Local Binary Patterns of Ojala et al.
[141] marked the be- ginning of the LBP methodology, followed by the simpler rotation invariant version of Pietik¨ainen et al.
[164], and later “uniform” patterns to reduce feature dimensionality [143].
Texture representation generally requires the analysis of pat- terns in local pixel neighborhoods, which are comprehensively de- scribed by their joint distribution.
However, stable estimation of joint distributions is often infeasible, even for small neighborhoods, because of the combinatorics of joint distributions.
Considering the joint distribution: g(xc, x0, .
.
.
, xp−1) (1) TextureImage1imN12filN12filN12filNkmeans3DTextonsLMfiltersFilteredimagesConcatenationfilimNNTextureImage2TextureImageLMfiltersLMfiltersTexturesamples(differentilluminationandviewpoint,registered)FeaturevectorsofdimensionalityfilNConcatenatedfeaturevectorofdimensionalityfilNfilNfilNALocalPatchPatchFeatureVectorCircularsortsortCircx sort0,0x2,{}ciix3,{}ciix1,{}ciixAngular-DiffRadial-DiffRad1,{}ii'Rad2,{}ii'Rad'sortsort sortRad3,{}ii'Ang1,{}ii'Ang2,{}ii'Ang'sortsort sortAng3,{}ii'(a)(b)(c)RPMatrix)CircCircyx )RadRady )'AngAngy )'SRPFeaturesSortedPixelValuesSortedPixelDifferencesLocalSRPFeatureExtraction61617180798478827700011111Binary:11110001Decimal:2412Sp1,0()0,0xsxxt­ ®¯()iicbsxx 102piiib ¦0b1pb1bExample:A Survey of Recent Advances in Texture Representation Fig.
13 LBP and its representative variants (see text for discussion).
Fig.
14 Illustration of the calculation of BIF features.
where gray value xc corresponds to the center pixel and {xn}p−1 n=0 to the gray values of p equally spaced pixels on a circle of radius r, as shown in Fig.
12.
Ojala et al.
[143] argued that much of the information in the original joint distribution (1) about the textural characteristics is conveyed by the joint difference distribution g(x0 − xc, x1 − xc, .
.
.
, xp−1 − xc).
(2) The size of the joint histogram was minimized by keeping only the sign of each difference, as illustrated in Fig.
12.
A certain degree of rotation invariance is achieved by cyclic shifts of the LBPs, i.e., grouping together those LBPs that are ac- tually rotated versions of the same underlying pattern.
Since the dimensionality of the representation (which grows exponentially with p) is still high, Ojala et al.
[143] introduced a uniformity measure to identify p(p − 1) + 2 uniform LBPs and classiﬁed all remaining nonuniform LBPs under a single group.
By changing parameters p and r, we can derive LBP for any quantization of the angular space and for any spatial resolution, such that multiscale analysis can be accomplished by combining multiple operators of varying r.
The most prominent advantages of LBP are invariance to monotonic gray scale change, very low computational complex- ity, and ease of implementation.
Since [143], LBP started to receive increasing attention in com- puter vision and pattern recognition, especially texture and facial analysis, with the LBP milestones presented in Fig.
13.
As Gabor ﬁlters and LBP provide complementary information (LBP captures small and ﬁne details, Gabor ﬁlters encode appearance information over a broader range of scales), Zhang et al.
[240] proposed Local Gabor Binary Pattern (LGBP) by extracting LBP features from im- ages ﬁltered by Gabor ﬁlters of different scales and orientations, to enhance the representation power, followed by subsequent Gabor- LBP approaches [74, 114, 165].
Additional important LBP vari- ants include LBP-TOP, proposed by Zhao and Pietik¨ainen [241], a milestone in using LBP for dynamic texture analysis; the Lo- cal Ternary Patterns (LTP) of Tan and Triggs [200], introducing a pair of thresholds and a split coding scheme which allows for encoding pixel similarity; the Local Phase Quantization (LPQ) by Ojansivu et al.
[144, 145] quantizing the Fourier transform phase in local neighborhoods which is, by design, tolerant to most com- mon types of image blurs; the Completed LBP (CLBP) of Guo et al.
[65], encoding not only the signs but also the magnitudes of lo- cal differences; and the Median Robust Extended LBP (MRELBP) of Liu et al.
[113] which enjoys high distinctiveness, low compu- Fig.
15 First-order square symmetric neighborhood for WLD computation.
tational complexity, and strong robustness to image rotation and noise.
After over 20 years of developments, LBP is no longer just a simple texture operator, but has laid the foundation for a direc- tion of research dealing with local image and video descriptors.
A large number of LBP variants have been proposed to improve its robustness and to increase its discriminative power and appli- cability to different types of problems, and interested readers are referred to excellent surveys [74, 114, 165].
The recent experimen- tal survey [114] compared the most recent promising LBP variants with deep convolutional networks, evaluating robustness in mul- tiple classiﬁcation challenges, where the best overall performance was obtained for MRELBP.
Although CNN-based methods are be- ginning to dominate, LBP research remains active, as evidenced by signiﬁcant recent work [66, 197, 179, 101, 117, 227, 235, 42].
(8) Basic Image Features (BIF) approach [33] is similar to LBP [143], in that it is based upon a predeﬁned codebook rather than one learned from training.
It therefore shares the advantages of LBP over methods based on codebook learning with clustering.
In contrast with LBP, BIF probes an image locally using Gaussian derivative ﬁlters [62, 61] whereas LBP computes the differences between a pixel and its neighbors.
Derivative of Gaussians (DtG), consisting of ﬁrst and second order derivatives of the Gaussian ﬁl- ter, can effectively detect the local basic and symmetry structure of an image, and allows achieving exact rotation invariance [52].
BIF feature extraction is summarized in Fig.
14: each pixel in the image is ﬁltered by the DtG ﬁlters, and then labeled as that class maximiz- ing of six values.
A simple six-dimensional BIF histogram can be used as a global texture representation, however the histogram over these six categories produces too coarse a representation, therefore others (e.g., Crosier and Grifﬁn [33]) have performed multiscale analysis and calculated joint histograms over multiple scales.
(9) Weber Law Descriptor (WLD) [27] is based on the fact that human perception of a pattern depends not only on the change of a stimulus but also on the original intensity of the stimulus.
The WLD consists of two components: differential excitation and ori- entation.
For a small patch of size 3 × 3, shown in Fig.
15, the 19962014LBPforFacialTexture(Ahonenetal.)20162010LBP(Ojalaetal.)20062002PairwiseRotationInvarianceCooccurrenceLBP(PRICoLBP)(Qietal.)CompletedLBP(CLBP)(Guoetal.)200820122000LocalTernaryPattern(LTP)(TanandTriggs)1998MedianRobustExtendedLBP(MRELBP)(Liuetal.)RotationInvariantLBP(Pietikainenetal.)2004MultiScaleRotationInvariantUniformLBP(Ojalaetal.)LocalGaborBinaryPattern(LGBP)(Zhangetal.)CenterSymmetricLBP(CSLBP)(Heikkilaetal.)LBP-TOPforDynamicTexture(ZhaoandPietikainen)HOG-LBP(Wangetal.),10s01s20s02s11sijijijsrV x(,)ijijrxc x:ALocalPatch=2002ssO 22200211()4sssJ 2210012,,max()/2,ssyOJOJ­½r°° ®¾r°°¯¿,yO label2xmIfthenyJ (cid:15)1(),2yJO 1()2yJO (cid:15)label4xmlabel5xmlabel6xmIfthenIfthenIfthen,yO label3xmIfthen2210012,ssy label1xmIfthenClassifyaccordingtothelargestvalueFilterResponsesCompute,OJ10 Li Liu et al.
differential excitation is the relative intensity ratio (cid:32)(cid:80)7 (cid:33) ξ(xc) = arctan i=0 (xi − xc) xc and the orientation component is derived from the local gradient orientation θ(xc) = arctan x7 − x3 x5 − x1 Both ξ and θ are quantiﬁed into a 2D histogram, offering a global representation.
Clearly the use of multiple neighborhood sizes sup- ports a multiscale generalization.
3.2.3 Fractal Based Descriptors Fractal Based Descriptors present a mathematically well-founded alternative to dealing with scale [126], however they have not be- come popular as texture features due to their lack of discrimina- tive power [212].
Recently, inspired by the BoW approach, re- searchers revisited the fractal method and proposed the MultiFrac- tal Spectrum (MFS) method [229, 228, 230], invariant to view- point changes, nonrigid deformations and local afﬁne illumination changes.
The basic MFS method was proposed in [229], where MFS was ﬁrst deﬁned for simple image features, such as intensity, gradient and Laplacian of Gaussian (LoG).
In speciﬁc, a texture image is ﬁrstly transformed into n feature maps such as intensity, gradient or LoG ﬁlter features.
For each of the n feature map, it is clustered into k clusters (i.e. k codewords) via kmeans.
Then, a codeword la- bel map is obtained and is decomposed into k binary feature maps: those pixels assigned to codeword i are labeled with 1 and the rest pixels as 0.
For each binary feature map, the box counting algo- rithm [230] is used to estimate a fractal dimension feature.
Thus, a total of k fractal dimension features are computed for each feature map, forming a kD feature vector (referred to as a fractal spec- trum) as the global representation of the image.
Finally, for the n different feature maps, n fractal spectrum feature vectors are con- catenated as the MFS feature.
Later MFS was improved by replacing [228, 230] the simple image intensity and gradient features with more advanced ones, such as SIFT and wavelets, and by combining [168] the ideas of MFS and LBP.
The downside of MFS approach is that it requires high resolution images to obtain sufﬁciently stable features.
3.3 Codebook Generation Textures characterization requires the analysis of spatially repeat- ing patterns, which sufﬁce to characterize textures and the pursuit of which has had important implications in a series of practical problems, such as dimensionality reduction, variable decoupling, and biological modelling [148, 247].
The extracted set of local tex- ture features is versatile, and yet overly redundant [100].
It can therefore be expected that a set of prototype features (i.e. code- words or textons) must exist which can be used to create global representations of textures in natural images [100, 146, 247], in a similar way as in speech and language (such as words, phrases and sentences).
Fig.
16 Contrasting the ideas of BoW, VLAD and FV.
(a) BoW: Counting the number of local features assigned to each codeword.
It encodes the zero order statistics of the distribution of local descriptors.
(b) VLAD: Accumulating the differences of local features assigned to each codeword.
(c) FV: The Fisher vector extends the BOW by encoding higher order statistics (ﬁrst and second order), retaining information about the ﬁtting error of the best ﬁt.
There exist a variety of methods for codebook generation.
Cer- tain approaches, such as LBP [143] and BIF [33], which we have already discussed, use predeﬁned codebooks, therefore entirely by- passing the codebook learning step.
For approaches requiring a learned codebook, kmeans cluster- ing [96, 100, 109, 214, 239] and Gaussian Mixture Models (GMM) [28, 30, 93, 79, 159, 187] are the most popular and successful methods.
GMM modeling considers both cluster centers and co- variances, which describe the location and spread/shape of clus- ters, whereas kmeans clustering cannot capture overlapping distri- butions in the feature space as it considers only distances to cluster centers, although generalizations to kmeans with multiple proto- types per cluster can allow this limitation to be relaxed.
The GMM and kmeans methods learn a codebook in an unsupervised manner, but some recent approaches focus on building more discriminative ones [233, 222].
In addition, another signiﬁcant research thread is reconstruc- tion based codebook learning [1, 161, 195, 216], under the assump- tion that natural images admit a sparse decomposition in some re- dundant basis (i.e., dictionary or codebook).
These methods focus on learning nonparametric redundant dictionaries that facilitate a sparse representation of the data and minimize the reconstruction error of the data.
Because discrimination is the primary goal of tex- ture classiﬁcation, researchers have proposed to construct discrim- inative dictionaries that explicitly incorporate category speciﬁc in- formation [120, 121].
Since the codebook is used as the basis for encoding feature vectors, codebook generation is often interleaved with feature en- coding, described next.
3.4 Feature Encoding As illustrated in Fig.
4, a given image is transformed into a pool of local texture features, from which a global image representation is derived by feature encoding with the generated codebook.
In the ﬁeld of texture classiﬁcation, we group commonly used encoding strategies into three major categories: ◦ Voting based [100, 213, 210], ◦ Fisher Vector based [79, 30, 159, 180], and ◦ Reconstruction based [120, 121, 147, 161, 216].
1w(a)BoW123(b)VLAD2w3w1w2w3w1u2u3uConcatenate1u2u3u(c)FV1x2x3xModellocalfeatureswithGMM;Compute gradient of the sample’s likelihoodwrttheparametersofGMM,scaledbytheinversesquarerootoftheFisherinformationmatrix.
A Survey of Recent Advances in Texture Representation 11 Comprehensive comparisons of encoding methods in image clas- siﬁcation can be found in [24, 28, 76].
Voting based methods.
The most intuitive way to quantize a local feature is to assign it to its nearest codeword in the codebook, also referred to as hard voting [100, 213].
A histogram of the quan- tized local descriptors can be computed by counting the number of local features assigned to each codeword; this histogram con- stitutes the baseline BoW representation (as illustrated in Fig.
16 (a)) upon which other methods can improve.
Formally, it starts by learning a codebook {wi}K i=1, usually by kmeans clustering.
Given a set of local texture descriptors {xi}N i=1 extracted from an image, the encoding representation of some descriptor x via hard voting is (cid:26) 1, if i = argminj((cid:107)x − wj(cid:107)) (3) v(i) = 0, otherwise.
The histogram of the set of local descriptors is to aggregate all encoding vectors {vi}N i=1 via sum pooling.
Hard voting overlooks codeword uncertainty, and may label image features by nonrepre- sentative codewords.
In an improvement to this hard voting scheme, soft voting [2, 175, 234] employs several nearest codewords to en- code each local feature in a soft manner, such that the weight of each assigned codeword is an inverse function of the distance from the feature, for some kernel deﬁnition of distance.
Voting based methods yield a histogram representation of dimensionality K, the number of bins in the histogram.
Fisher Vector based methods.
By counting the number of oc- currences of codewords, the standard BoW histogram representa- tion encodes the zeroth order statistics of the distribution of de- scriptors, which is only a rough approximation of the probabil- ity density distribution of the local features.
The Fisher vector ex- tends the histogram approach by encoding additional information about the distribution of the local descriptors.
Based on the origi- nal FV encoding [158], improved versions were proposed [31, 159] such as the Improved FV (IFV) [159] and VLAD [79].
We brieﬂy describe IFV [159] here, since to the best of our knowledge it achieves the best performance in texture classiﬁcation [28, 29, 30, 187].
Theory and practical issues regarding FV encoding can be found in [180].
IFV encoding learns a soft codebook with GMM, as shown in Fig.
16 (c).
An IFV encoding of a local feature is computed by as- signing it to each codeword, in turn, and computing the gradient of the soft assignment with respect to the GMM parameters2.
The IFV encoding dimensionality is 2DK, where D is the dimensionality of the feature space and K is the number of Gaussian mixtures.
BoW can be considered a special case of FV in the case where the gradient computation is restricted to the mixture weight parame- ters of the GMM.
Unlike BoW, which requires a large codebook size, FV can be computed from a much smaller codebook (typi- cally 64 or 256) and therefore at a lower computational cost at the codebook learning step.
On the other hand, the resulting dimension of the FV encoding vector (e.g. tens of thousands) is usually sig- niﬁcantly higher than BoW (thousands), which makes it unsuitable for nonlinear classiﬁers, however it offers good performance even with simple linear classiﬁers.
The VLAD encoding scheme proposed by J´egou et al.
[79] can be thought of as a simpliﬁed version of FV, in that it typically uses Fig.
17 Contrasting the ideas of hard voting, sparse coding, and LLC.
kmeans, rather than GMM, and records only ﬁrst-order statistics rather than second order.
In particular, it records the residuals (the difference between the local features and the codewords), as shown in Fig.
16 (b).
Reconstruction based methods.
Reconstruction based meth- ods aim to obtain an information-preserving encoding vector that allows for the reconstruction of a local feature with a small number of codewords.
Typical methods include sparse coding and Local constraint Linear Coding (LLC), which are contrasted in Fig.
17.
Sparse coding was initially proposed [147] to model natural image statistics, then to texture classiﬁcation [38, 120, 121, 161, 195] and later to other problems such as image classiﬁcation [232] and face recognition [223].
In sparse coding, a local feature x can be well approximated by a sparse decomposition x ≈ Wv over the learned codebook W = [w1, w2, .
.
.
wK], by leveraging the sparse nature of the underlying image [147].
A sparse encoding can be solved as argminv(cid:107)x − Wv(cid:107)2 (cid:107)v(cid:107)0 ≤ s.
(4) s.t. where s is a small integer denoting the sparsity level, limiting the number of non-zero entries in v, measured as (cid:107)v(cid:107)0.
Learning a re- dundant codebook that facilitate a sparse representation of the lo- cal features is of key importance in sparse coding [1].
Methods in [120, 121, 161, 195] are based on learning C class-speciﬁc code- books, one for each texture class and approximating each local feature using a constant sparsity s.
The C different codebooks pro- vides C different reconstruction errors, which can then be used as classiﬁcation features.
In [161, 195], the class speciﬁc codebooks were optimized for reconstruction, but signiﬁcant improvements have been shown by optimizing for discriminative power instead [38, 120, 121], an approach which is, however, associated with high computational cost, especially when the number of texture classes C is large.
Locality constrained linear coding (LLC) [216] projects each local descriptor x down to the local linear subspace spanned by q codewords in the codebook of size K closest to it (in Euclidean distance), resulting a K dimensional encoding vector whose en- tries are all zero except for the indices of the q codewords closest to x.
The projection of x down to the span of its q closest code- words is solved via the following: argminv(cid:107)x − Wv(cid:107)2 2 + λ (cid:107)x − wi(cid:107)2 v(i)exp (cid:18) K(cid:88) k=1 s.t. (cid:19)2 v(i) = 1, K(cid:88) k=1 2 The derivative to weights, which is considered to make little contribution to the performance, is removed in IFK [159].
where λ is a small regularization constant and σ adjusts the weight decay speed.
(a)EncodingwithHardVoting(b)EncodingwithSparseCoding(c)EncodingwithLLC12 Li Liu et al.
Fig.
18 Contrasting classical ﬁltering based texture features, CNN, BoW and LBP.
3.5 Feature Pooling and Classiﬁcation The goal of feature pooling [14] is to integrate or combine the coded feature vectors {vi}i, vi ∈ Rd of a given image into a ﬁ- nal compact global representation yi which is more robust to im- age transformations and noise.
Commonly used pooling methods include sum pooling, average pooling and max pooling [100, 214, 216].
Speciﬁcally, let V = [v1, ..., vN ] ∈ Rd×N denote the coded features from N locations.
For u denoting a row of V, u is reduced to a single scalar by some operation (sum, average, max), reducing V to a d-dimensional feature vector.
Realizing that pooling over the entire image disregards all information regarding spatial de- pendencies, Lazebnik et al.
[97] proposed a simple spatial pyramid pooling (SPM) scheme by partitioning the image into increasingly ﬁne subregions and computing histograms of local features found inside each subregion via average or max pooling.
The ﬁnal global representation is a concatenation of all histograms extracted from subregions, resulting in a higher dimensional representation that preserves more spatial information [201].
Given a pooled feature, a given texture sample can be clas- siﬁed.
Many classiﬁcation approaches are possible [78, 218], al- though Nearest Neighbor Classiﬁer (NNC) and Support Vector Machines (SVM) are the most widely-used classiﬁers for the BoW representation.
Different distance measures may be used, such as the EMD distance [96], KL divergence and the widely-used Chi Square distance [109, 214].
For very high-dimensional features, as with IFV and VLAD, linear SVM may represent a better choice [79, 159].
4 CNN based Texture Representation A large number of CNN-based texture representation methods have been proposed in recent years since the record-breaking image classiﬁcation result [90] achieved in 2012.
A key to the success of CNNs is their ability to leverage large labeled datasets to learn high quality features.
Learning CNNs, however, amounts to esti- mating millions of parameters and requires a very large number of annotated images, an issue which rather constrains the appli- cability of CNNs in problems with limited training data.
A key discovery, in this regard, was that CNN features pretrained on very large datasets were found to transfer well to many other problems, including texture analysis, with a relatively modest adaption effort [25, 30, 59, 150, 186].
In general, the current literature on tex- ture classiﬁcation includes examples of both employing pretrained generic CNN models or performing ﬁnetuning for speciﬁc texture classiﬁcation tasks.
In this survey we will classify CNN based texture representa- tion methods into three categories, and which form the basis of the following three sections: ◦ using pretrained generic CNN models, ◦ using ﬁnetuned CNN models, and ◦ using handcrafted deep convolutional networks.
These representations have had a widespread inﬂuence in image understanding; representative examples of each of these are given in Table 2.
4.1 Using Pretrained Generic CNN Models Given the behavior of CNN transfer, the success of pretrained CNN models lies in the feature extraction and encoding steps.
Similar to Section 3, we will describe ﬁrst some commonly used networks for pretraining and then the feature extraction process.
(1) Popular Generic CNN Models can serve as good choices for extracting features, including AlexNet [90], VGGNet [192], GoogleNet [198], ResNet [72] and DenseNet [75].
Among these networks, AlexNet was proposed the earliest, and in general the others are deeper and more complex.
A full review of these net- works is beyond the scope of this paper, and we refer readers to (d) Reformulation of the LBP  using convolutional filters(a) Traditional Multiscale and     Multiorientation Filtering(c) Random Projections and BoW based Texture Representation(b) Basic module in Standard DCNN Feature Maps from Previous LayerMaximunAverageGaussianReLUSigmoidConvolutionNonlinearityn filtersPooling       RSVMInput imageLabelFilterResponsesFeature VectorsAverageGaussianModulusSquaringSigmoidConvolutionNonlinearityClassifierFeatureMapsn filtersLocal Energy EstimatePooling Local Energy Function11111111Histogram poolingInput imageFilterResponsesFeature vectorHeavisideStepFunctionn binary fIlters (multiorientation)NonlinearityFeatureMapsn filtersWeighted Feature MapWeightingConvolutionClassifierSVMOrderlessPoolingSVMInput imageFilter ResponsesCodebookEncodingn random projections of size 5×5ConvolutionNonlinearityClassifierCodeword MapRandom ProjectionsOrderlessPooling-1-1-1-111111-1-1-1-1111-1-1-11111-1-1-111111111-1-1111111-1-1-1-11111Histogram PoolingFeature vectorFeature Maps to Next Layer-1-1-1-1-1-1-1A Survey of Recent Advances in Texture Representation Table 2 CNN based texture representation 13 Approach Using Pretrained Generic CNN Models [30] (Section 4.1) Traditional feature encoding and pooling; New pooling such as bilinear pooling [104, 107] and LFV Highlights • AlexNet [90] • VGGM [25, 30] • VGGVD [192] • GoogleNet [198] • ResNet [72] Using Finetuned CNN Models (Section 4.2) • TCNN [6] • BCNN [106, 104] • Compact BCNN [54] • FASON [39] • NetVLAD [7] • DeepTEN [237] [196] Achieved breakthrough image classiﬁcation result on ImageNet; The historical turning point of feature representation from handcrafted to CNN.
Similar complexity as AlexNet, but better texture classiﬁcation performance.
Much deeper than AlexNet; Much Larger model size than AlexNet and VGGM; Much better texture recognition performance than AlexNet and VGGM.
Much deeper than AlexNet; Small pretrained model size; Not often used in texture classiﬁcation.
Signiﬁcantly deeper than VGGVD; Smaller model size (ResNet 101) than AlexNet.
End-to-end learning Using global average pooling; Combining outputs from multiple CONV layers.
Introducing a novel and orderless bilinear feature pooling method; Generalizing Fisher Vector and VLAD; Good representation ability; Very high feature dimensionality.
Adopting Random Maclaurin Projection or Tensor Sketch Projection to reduce the dimensionality of bilinear features (e.g. from 262144 (5122) to 8192); Maintain similar performance to BCNN; Combining the ideas of TCNN [6] and Compact BCNN [54].
Plugging a VLAD like layer in a CNN network at the last CONV layer.
Similar to NetVLAD [7], integrating an encoding layer on top of CONV layers; Generalizing orderless pooling methods such as VLAD and FV in a CNN trained end to end.
Texture Speciﬁc Deep Convolutional Models (Section 4.3) • ScatNet [19] • PCANet [23] Use Gabor wavelets for comvolution; Mathematical interpretation of CNNs; Features being stable to deformations and preserving high frequency information; Inspired by ScatNet [19], using PCA ﬁlters to replace Gabor wavelets;Using LBP and histogramming as feature pooling; No local invariance.
the original papers [72, 75, 90, 192, 198] and to excellent surveys [11, 25, 64, 98] for additional details.
Brieﬂy, as shown in Fig.
18 (b), a typical CNN repeatedly applies the following three opera- tions: 1.
Convolution with a number of linear ﬁlters, 2.
Nonlinearities, such as sigmoid or rectiﬁcation, 3.
Local pooling or subsampling.
These three operations are highly related to traditional ﬁlter bank methods widely used in texture analysis [170], as shown in Fig.
18 (a), with the key differences that the CNN ﬁlters are learned di- rectly from data rather than handcrafted, and that CNNs have a hi- erarchical architecture learning increasingly abstract levels of rep- resentation.
These three operations are also closely related to the RP approach (Fig.
18 (c)) and the LBP (Fig.
18 (d)).
Several large-scale image datasets are usually used for CNN pretraining.
Among them the ImageNet dataset, with 1000 classes and 1.2 million images [178] is most commonly used, with a fur- ther source of scene-centric dataset MITPlaces [243].
Comprehensive evaluations of the feature transfer effect of CNNs for the purpose of texture classiﬁcation have been conducted in [28, 29, 30, 139], with the following critical insights.
During model transfer, features extracted from different layers exhibit different classiﬁcation performance.
Experiments conﬁrm that the fully-connected layers of the CNN, whose role is primarily that of classiﬁcation, tend to exhibit relatively worse generalization ability and trans- ferability, and therefore would need retraining or ﬁnetuning on the transfer target.
In contrast the convolutional layers, which act more as feature extractors, with coarser convolutional layers acting as progressively more abstract features, generally transfer well.
That is, the convolutional descriptors are substantially less committed to a speciﬁc dataset than the fully connected descriptors.
As a re- sult, the source training set is relevant to classiﬁcation accuracy on different datasets, and the similarity of the source and target plays a critical role when using a pretrained CNN model [10].
Finally, from [29, 30, 139] it was found that deeper models transfer better, and that the deepest convolutional descriptors give the best perfor- mance, superior to the fully-connected descriptors, when proper encoding techniques are employed (such as FVCNN←CNN fea- tures with Fisher Vector encoder).
(2) Feature Extraction A CNN can be viewed as a composi- tion fL ◦ ··· ◦ f2 ◦ f1 of L layers, where the output of each layer Xl = (fl ◦ ··· ◦ f2 ◦ f1)(I) consists of Dl feature maps of size W l × H l.
The Dl responses at each spatial location form a Dl dimensional feature vector.
The network is called convolutional if all the layers are implemented as (nonlinear) ﬁlters, in the sense that they act locally and uniformly on their input.
From bottom to top layers, the image undergoes convolution with (nonlinear) ﬁlters, and the receptive ﬁeld of these ﬁlters and the number of feature channels increases, whereas the size of the feature maps decreases.
Usually, the last several layers of a typical CNN are called Fully Connected (FC) because, if seen as ﬁlters, their sup- port is the same as the size of the input Xl−1, and therefore lack locality.
By contrast, earlier layers that act locally will be referred to as convolutional.
The most straightforward approach to CNN-based texture clas- siﬁcation is to extract the descriptor from the fully connected lay- ers of the network [29, 30], e.g., the FC6 or FC7 descriptors in AlexNet [90].
The fully connected layers are pretrained discrim- inatively, which can be either an advantage or disadvantage, de- pending on whether the information that they captured can be trans- ferred to the domain of interest [25, 30, 59].
The fully-connected descriptors have a global receptive ﬁeld and are usually viewed as global features suitable for classiﬁcation with an SVM classiﬁer.
In contrast, the convolutional layers of a CNN can be used as ﬁlter banks to extract local features [29, 30].
Compared with the global fully-connected descriptors, lower level convolutional descriptors are more robust to image transformations such as translation and 14 Li Liu et al.
occlusion.
In [29, 30], the features are extracted as the output of a convolutional layer, directly from the linear ﬁlters (excluding ReLU and max pooling, if any), and are combined with traditional encoders for global representation.
For instance, the last convolu- tional layer of VGGVD (very deep with 19 layers) [192] yields a set of 512 descriptor vectors; in [28, 29, 30] four types of CNNs AlexNet [90], DeCAF [43], VGGM [25] and VGGVD [192] were considered for feature extraction.
(3) Feature Encoding and Pooling A set of features extracted from convolutional or fully connected layers resembles a set of texture features as described in Section 3.2, so the traditional fea- ture encoding methods discussed in Section 3.4 can be directly employed.
In [30], Cimpoi et al.
evaluated several encoders, i.e. stan- dard BoW [100], LLC [216], VLAD [79] and IFV [159] (reviewed in Section 3.4), for CNN features, and showed that the best per- formance is achieved by IFV.
FVCNN descriptors can be com- pressed to the same dimensionality of FC descriptors via PCA while preserving classiﬁcation performance [30].
Later, Song et al.
[196] proposed a neural network to transform the FVCNN descrip- tors into a lower dimensional representation.
As shown in Fig.
20, locally transferred FVCNN (LFVCNN) descriptors are obtained by passing the 2KD dimensional FVCNN descriptors of images through a multilayer neural network consisting of fully connected, l2 normalization layers, and ReLU layers.
Recently, Gatys et al.
[56] showed that the Gram matrix rep- resentations extracted from various layers of VGGNet [192] can be inverted for texture synthesis.
The work of Gatys et al.
[56] ignited a renewed interest in texture synthesis [207].
Notably, the Gram matrix representation used in their approach is identical to the bilinear pooling of CNN features of Lin et al.
[106], which were proved to be good for texture recognition in [104].
Like the traditional encoders introduced in Section 3.4, the bilinear feature pooling is an orderless representation of the input image and hence is suitable for modeling textures.
As discussed previously, a ﬁeld of descriptors Xl ∈ RW l×H l×Dl can be obtained at a given layer l of a CNN.
Hence, a set of features {xl indexed by their location i can be extracted.
The Bilinear CNN (BCNN) de- scriptors are obtained by computing the outer product xl i)T of each feature xl i with itself, are reordered into feature vectors, and are subsequently pooled via sum to obtain the ﬁnal global repre- sentation.
The dimension of the bilinear descriptor is (Dl)2, which is very high (e.g. 5122).
In [104, 107], the BCNN descriptors of VGGNet [192] were evaluated with an SVM classiﬁer for texture recognition.
It was shown in [104, 107] that the texture classiﬁ- cation performance of BCNN and FVCNN were virtually identi- cal, indicating that bilinear pooling is as good as the Fisher vector pooling for texture recognition.
It was found in [104, 107] that the BCNN descriptor of the last convolutional layer performed the best, in agreement with [30].
i ∈ RDl}W lH l i=1 i(xl 4.2 Using Finetuned CNN Models Pretrained CNN models, discussed in Section 4.1, have achieved impressive performance in texture recognition, however training in these methods is a multistage pipeline that involves feature extrac- tion, codebook generation, feature encoding and classiﬁer training.
Consequently, these methods cannot take advantage of utilizing the full capability of neural networks in representation learning.
Gen- erally ﬁnetuning CNN models on task-speciﬁc training datasets (or learning from scratch if large-scale task-speciﬁc datasets are available) is expected to improve on already strong performance achieved by pretrained CNN models [25, 59].
When using a ﬁne- tuned CNN model, the global image representation is usually gen- erated in an end to end manner; that is, the network will produce a ﬁnal visual representation without additional explicit encoding or pooling steps, as illustrated in Fig.
5.
When ﬁnetuning a CNN, the last fully connected layer is modiﬁed to have B nodes corre- sponding to the number of classes in the target dataset.
The na- ture of the datasets used in ﬁnetuning is important to learning dis- criminative CNN features.
The pretrained CNN model is capable of discriminating images of different objects or scene classes, but may be less effective in discerning the difference between different textures (material types) since an image in ImageNet may contain different types of textures (materials).
The size of the dataset used in ﬁnetuning matters as well, since too small datasets may be inad- equate for complete learning.
To the best of our knowledge, the properties of texture rep- resentation of ﬁnetuning a large-scale CNN like VGGNet [192] or training it from scratch using a texture dataset has not been fully explored in texture analysis, due to the fact that a large-scale texture dataset of the same size as ImageNet [178] or MITPlaces [243] does not exist.
Most existing texture datasets are small, as discussed later in Section 6, and according to [6, 104] ﬁnetun- ing a VGGNet [192] or AlexNet [90] on existing texture datasets leads to negligible performance difference.
As shown in Fig.
19 (a), for a typical CNN like VGGNet [192], the output of the last convolutional layer is reshaped into a single feature vector (spa- tially sensitive) and fed into fully connected layers (i.e., order sen- sitive pooling).
The global spatial information is necessary for an- alyzing the global shapes of objects, however it has been realized [6, 30, 56, 104, 237] that it is not of great importance for analyzing textures due to the need for orderless representation.
The FVCNN descriptor shows higher recognition performance than FCCNN, even if the pretrained VGGVD model is ﬁnetuned on the texture dataset (i.e. the ﬁnetuned FCCNN descriptor) [30, 104].
Therefore, an orderless feature pooling from the output of a convolution layer is desirable for end to end learning.
In addition, orderless pooling doesn’t require an input image to be of a ﬁxed size, motivating a series of innovations in designing novel CNN architectures for texture recognition [6, 7, 39, 107, 237].
A Texture CNN (TCNN) based on AlexNet, as illustrated in Fig.
19 (b), was developed in [6].
It simply utilizes global average pooling to transform a ﬁeld of descriptor Xl ∈ RW l×H l×Dl at a given convolutional layer l of a CNN into a Dl dimension vector which is connected to a fully connected layer.
TCNN has fewer pa- rameters and lower complexity than AlexNet.
In addition, Andrea- rczyk and Whelan [6] proposed to fuse the global average pooled vector of an intermediate convolutional layer and that of the last convolutional layer via concatenation and introduced to later fully connected layers, a combination which resembles the hypercolumn feature developed in [69].
Andrearczyk and Whelan [6] observed that ﬁnetuning a network that was pretrained on a texture-centric dataset achieves better results on other texture datasets compared to a network pretrained on an object-centric dataset of the same A Survey of Recent Advances in Texture Representation 15 Fig.
19 Comparison of Fine Tuned CNNs: (a) standard CNN, (b) TCNN [6], (c) BCNN [107], (d) Compact Bilinear Pooling [54], and (e) FASON [39].
size, and that the size of the dataset on which the network is pre- trained or ﬁnetuned predominantly inﬂuences the performance of the ﬁnetuning.
These two observations suggest that a very large texture dataset could bring a signiﬁcant contribution to CNNs ap- plied to texture analysis.
that the classiﬁcation performance was inferior to FVCNN.
Similar to NetVLAD [7], a Deep Texture Encoding Network (DeepTEN) was introduced in [237] by integrating an encoding layer on top of convolutional layers, also generalizing orderless pooling methods such as VLAD and FV in a CNN trained end to end.
In BCNN [107], as shown in Fig.
19 (c), Lin et al.
proposed to replace the fully connected layers with an orderless bilinear pool- ing layer, which was discussed in Section 4.1. This method was successfully applied to texture classiﬁcation in [104] and obtained slightly better results than FVCNN while being trained end to end, however the representational power of bilinear features comes at the cost of very high dimensional feature representations, which in- duce substantial computational burdens and require large amounts of training data, motivating several improvements on BCNN.
Gao et al.
[54] proposed compact bilinear pooling, as shown in Fig.
19 (d), which utilizes Random Maclaurin Projection or Tensor Sketch Projection to reduce the dimensionality of bilinear representations while still maintaining similar performance to the full BCNN fea- ture [107] with a 90% reduction in the number of learned parame- ters.
To combine the ideas in [6] and [54], Dai et al.
[39] proposed an effective fusion network called FASON (First And Second Or- der information fusion Network) that combines ﬁrst and second order information ﬂow, as illustrated in Fig.
19 (e).
In FASON, the global average pooled vector and the compact bilinear pooled fea- ture of a convolutional layer was referred as the ﬁrst and second or- der statistics, respectively.
These two types of features were gener- ated from different convolutional layers and concatenated to form a single feature vector which was connected to a fully connected softmax layer for end to end training.
In [87], Kong and Fowlkes proposed to represent the bilinear features as a matrix and applied a low rank bilinear classiﬁer.
The resulting classiﬁer can be eval- uated without explicitly computing the bilinear feature map which allows for a large reduction in the computational time as well as decreasing the effective number of parameters to be learned.
The BCNN model [107] and its variants [39, 54, 87] are successful for texture classiﬁcation and ﬁne-grained image recognition.
There are some works attempting to integrate CNN and VLAD or FV pooling in an end to end manner.
In [7], a NetVLAD network was proposed by plugging a VLAD-like layer into a CNN network at the last convolutional layer and allows training end to end.
The model was initially designed for place recognition, however when applied to texture classiﬁcation by Song et al.
[196] it was found 4.3 Using Handcrafted Deep Convolutional Networks In addition to the CNN based methods reviewed in Sections 4.1 and 4.2, some handcrafted deep convolutional networks [19, 23] deserve attention.
Recall that a standard CNN architecture (as shown in Fig.
18 (b)) consists of multiple trainable building blocks stacked on top of one another followed by a supervised classiﬁer.
Each block generally consists of three layers: a convolutional ﬁlter bank layer, a nonlinear layer, and a feature pooling layer.
Similar to the CNN architecture, Bruna and Mallat [19] proposed a highly inﬂu- ential Scattering convolution Network (ScatNet), as illustrated in Fig.
21.
The key difference from CNN, where the convolutional ﬁlters are learned from data, is that the convolutional ﬁlters in ScatNet are predetermined — they are simply wavelet ﬁlters, such as Gabor or Haar wavelets, and no learning is required.
Moreover, the ScatNet usually cannot go as deep as a CNN; Bruna and Mallat [19] sug- gested two convolutional layers, since the energy of the third layer scattering coefﬁcients is negligible.
Speciﬁcally, as can be seen in Fig.
21, ScatNet cascades wavelet transform convolutions with modulus nonlinearity and averaging poolers.
It is shown in [19] that ScatNet computes translation-invariant image representations which are stable to deformations and preserve high frequency in- formation for recognition.
As shown in Fig.
21, the average pooled feature vector from each stage is concatenated to form the global feature representation of an image, which is input into a simple PCA classiﬁer for recognition.
Somewhat surprisingly, such a pre- ﬁxed network has demonstrated very high performance in texture recognition [19, 190, 191, 189].
A downside of ScatNet is that the feature extraction stage is very time consuming, although the di- mensionality of the global representation feature is relatively low (several hundreds).
ScatNet has been extended to achieve rotation and scale invariance [190, 191, 189] and applied to other problems besides texture such as object recognition [153].
Importantly, the mathematical analysis of ScatNet explains important properties of FC(4096)CONV(Dfilters)AveragePoolingFC(4096)ConcatenatingCONV(Cfilters)AveragePooling(b)TCNN(e)FASONSoftMaxCONV(Dfilters)CompactBilinearPoolingCONV(Cfilters)AveragePoolingCompactBilinearPoolingAveragePoolingSoftMaxCONV(Dfilters)CNN1CONV(Dfilters)CNN2BilinearPooling(c)BCNN(d)CompactBCNNCNN1andCNN2canbesameordifferentSoftMaxDim:C+DDim:D2SoftMaxRandomMatrix1(D(cid:104)D1)RandomMatrix2(D×D1)OuterProductElementwiseMultiplicationCONV(Dfilters)CNN1CONV(Dfilters)CNN2Dim:D1CompactBilinearPoolingDim:C+D+D1+D1(a)StandardCNNFC(4096)CONV(Dfilters)VectorizingFC(4096)CONV(Cfilters)SoftMaxDim:HWDOutput:H(cid:104)W(cid:104)D16 Li Liu et al.
5 Attribute-Based Texture Representation In recent years, the recognition of texture categories has been ex- tensively studied and has shown substantial progress, partly thanks to the texture representations reviewed in Sections 3 and 4.
De- spite the rapid progress, particularly with the development of deep learning techniques, we remain far from reaching the goal of com- prehensive scene understanding [89].
Although the traditional goal was to recognize texture categories based on their perceptual dif- ferences or their material types, textures have other properties, as shown in Fig.
22, where we may speak of a banded shirt, a striped zebra, and a striped tiger.
Here, banded and striped are referred to as visual texture attributes [28], which describe texture patterns using human-interpretable semantic words.
With texture attributes, the textures shown back in Fig.
3 (d) might all be described as braided, falling into a single category in the Describable Textures Dataset (DTD) database [28].
The study of visual texture attributes [13, 28, 131] was moti- vated by the signiﬁcant interest raised by visual attributes [49, 156, 155, 92].
Visual attributes allow the describing of objects in signif- icantly greater detail than a category label and are therefore impor- tant towards reaching the goal of comprehensive scene understand- ing [89], which would support important applications such as de- tailed image search, question answering, and robotic interactions.
Texture attributes are an important component of visual attributes, particularly for objects that are best characterized by a pattern.
It can support advanced image search applications, such as more spe- ciﬁc queries in image search engines (e.g. a striped skirt, rather than just any skirt).
The investigation of texture attributes and de- tailed semantic texture description offers a signiﬁcant opportunity to close the semantic gap in texture modeling and to support appli- cations that require ﬁne grained texture description.
Nevertheless, there are only several papers [13, 28, 131] investigating the texture attributes thus far, and there is no systematic study yet attempted.
There are three essential issues in studying texture attribute based representation: 1.
The identiﬁcation of a universal texture attribute vocabulary that can describe a wide range of textures; 2.
The establishment of a benchmark texture dataset, annotated by semantic attributes; 3.
The reliable estimation of texture attributes from images, based on low level texture representations, such as the methods re- viewed in Sections 3 and 4.
Tamura et al.
[199] proposed a set of six attributes for describing textures: coarseness, contrast, directionality, line-likeness, regular- ity and roughness.
Amadasun and King [5] reﬁned this idea with the ﬁve attributes of coarseness, contrast, business, complexity, and strength, that were tuned for general applicability and low compu- tational costs.
Later, Bhushan et al.
[12] studied texture attributes from the perspective of psychology, asking subjects to cluster a collection of 98 texture adjectives according to similarity and iden- tiﬁed eleven major clusters.
Recently, inspired by the work in [12, 49, 155, 92], Matthews et al.
[131] attempted to enrich texture analysis with semantic at- tributes.
They identiﬁed eleven commonly used texture attributes Fig.
20 Locally transferred Fisher Vector (LFV): use 2K neural networks for dimensionality reduction of FVCNN descriptor.
Fig.
21 Illustration of two similar handcrafted deep convolutional networks: ScatNet [19] and PCANet [23].
CNN architectures, and it is one of the few works that provides detailed theoretical understanding of CNNs. Inspired by ScatNet, Chan et al.
[23] proposed a very sim- ple convolutional network called PCANet.
As contrasted in Fig.
21, the key difference is that PCANet uses PCA ﬁlters which are learned from training data, instead of predeﬁned Gabor wavelets.
In addition, PCANet uses LBP encoding [143] and histogramming for feature pooling.
Two simple variations of PCANet, RandNet and LDANet, were also introduced in [23], sharing the same topol- ogy as PCANet, but their convolutional ﬁlters are either random ﬁlters as in [109] or learned from Linear Discriminant Analysis (LDA).
Compared with ScatNet, feature extraction in PCANet is much faster, however according to [114], PCANet representation does not have local invariance and its texture classiﬁcation perfor- mance is not so good.
Input:FVCNNDescriptorFCLayerl2NormLayerReLULayerFCLayerl2NormLayerReLULayerConcatenateOutput:LFVCNNDescriptorLocallytransferredFisherVector(LFV):Use2KNeuralNetworksforDimensionalityReductionInputImageAGaborWavelet(FrequencyDomain)ConvolutionConvolutionConvolutionGaussianSmoothingFeaturePoolingGlobalAveragePoolingFeaturePoolingFeaturePoolingFeaturePoolingConcatenatedVectorConvConvConvConvGaborWaveletsModulus(Nonlinearity)ConvolutionPCAfiltersConvolutionLBPPoolingFeaturePoolingScatteringConvolutionalNetwork(ScatNet)PCANetDeepNetworkArchitectureA Survey of Recent Advances in Texture Representation 17 6 Texture Datasets and Performance 6.1 Texture Datasets Throughout the history of computer vision research, datasets have played an essential role.
They not only serve as source of training data and as a means of evaluating algorithms, but also stimulate a ﬂood of research interest and drive research towards new and more challenging directions.
This is evidenced by the fact that the re- cent large scale ImageNet dataset [178] has enabled breakthroughs in visual recognition research using a new class of deep learning algorithms [90].
In the big data era, it becomes urgent to further enrich texture datasets to promote future research.
There are many texture datasets in the ﬁelds of medical analysis, robotics, recogni- tion, and analysis.
In this section, we discuss existing natural tex- ture image datasets that have been released and commonly used by the research community for texture classiﬁcation, as summarized in Table 3.
The Brodatz texture database [17], derived from the Brodatz Album [18], is the earliest, the most widely used and the most fa- mous texture database.
It has a relatively large number of classes (111), with each class having only one image.
Many texture repre- sentation approaches exploit the Brodatz database for evaluations [86, 109, 143, 167, 170, 209], however in most cases the entire database is not utilized, except in some recent studies [58, 96, 114, 162, 239].
The database has been criticized because of the lack of intraclass variations such as scale, rotation, perspective and illumi- nation.
The Vision Texture Database (VisTex) [115, 215] is an early and well-known database.
Built by the MIT Multimedia Lab, it has 167 classes of textures, each with only one image.
The VisTex textures are imaged under natural lighting conditions, and have ex- tra visual cues such as shadows, lighting, depth, perspective, thus closer in appearance to real-world images.
VisTex is often used for texture synthesis or segmentation, but rarely for image-level tex- ture classiﬁcation.
Since 2000, texture recognition has evolved to classifying real world textures with large intraclass variations due to changes in camera pose and illumination, leading to the development of a number of benchmark texture datasets based on various real-world material instances.
Among these, the most famous and widely used is the Columbia-Utrecht Reﬂectance and Texture (CUReT) dataset [40], with 61 different material textures taken under varying image conditions in a controlled lab environment.
Each of the materials in the database has been imaged under 205 different viewing and illumination conditions.
The effects of specularities, interreﬂec- tions, shadowing, and other surface normal variations are evident, as shown in Fig.
3 (a), where their impact is highlighted.
CUReT is a considerable improvement over Brodatz, where all such ef- fects are absent.
Based on the original CUReT, Varma and Zisser- man [213] built a subset for texture classiﬁcation, which became the widely used benchmark to assess classiﬁcation performance.
CUReT has limitations of no signiﬁcant scale change for most of the textures and limited in-plane rotation.
Thus, a discrimina- tive texture feature without rotational invariance can achieve high recognition rates [19].
Noticing the limited scale invariance in CUReT, researchers from the Royal Institute of Technology (KTH) introduced a dataset Fig.
22 Objects with rich textures in our daily life.
Visual texture attributes like mesh, spotted, striated, spotted and striped provide detailed and vivid de- scriptions of objects.
3 by selecting a single adjective from each of the eleven clus- ters identiﬁed by Bhushan et al.
[12].
Then, with the eleven tex- ture attributes, they released a publicly available human-provided labeling of over 300 classes of texture from the Outex database [142].
For each texture image, instead of asking a subject to sim- ply identifying the presence or absence of each texture attribute, Matthews et al.
[131] proposed a framework of pairwise compar- ison, in which a subject was shown two texture images simulta- neously and prompted to choose the image exhibiting more of some attribute, motivated by the use of relative attributes [155].
This approach allows the Outex textures to be placed along a con- tinuum according to how strongly they exhibit each attribute.
In order to bridge the semantic gap Matthews et al.
[131] proposed a method similar to a soft margin SVM to estimate the level of expression of each attribute (a quality they referred to as an at- tribute’s strength) within a texture image based on texture features like LBP [143], and the beneﬁts of texture attribute based repre- sentation were demonstrated for texture retrieval.
After performing a screening process on the 98 adjectives iden- tiﬁed by Bhushan et al.
[12], Cimpoi et al.
[28] obtained a texture attribute vocabulary of 47 English adjectives, including words such as banded, cobwebbed, freckled, knitted and zigzagged.
They col- lected a DTD from the internet, providing 120 example images for each attribute.
The DTD dataset serves as a good start to seek representation for recognizing texture attributes in images.
They furthermore provide a comparison of BoW- and CNN-based tex- ture representation methods for attribute estimation, demonstrat- ing that texture attributes are excellent texture descriptors, trans- ferring between datasets.
Bormann et al.
[13] introduced a set of seventeen human comprehensible attributes for color texture char- acterization.
These attributes include seven color properties and ten structural properties.
They collected a new database named Robotics Domain Attributes Database (RDAD) for the indoor ser- vice robotics context.
The RDAD dataset contains images of ev- eryday object and surface textures and is fully annotated with the seventeen attributes and the respective object or surface class.
They compared ﬁve low level texture representation approaches for at- tribute prediction, and found that not all objects can be described very well with the seventeen attributes.
Clearly, which attributes are best suited for a precise description of different object and tex- ture classes deserves further attention.
3 Blemished, bumpy, lined, marbled, random, repetitive, speckled, spiralled, webbed, woven, and wrinkled 18 Table 3 Summary of commonly-used texture databases.
References Total Images Texture Classes Image Size Texture Dataset Brodatz VisTex CUReT Outex KTHTIPS UIUC No. [18] [40] [142] [70, 53] [96] 7 KTHTIPS2a [22, 125] 8 KTHTIPS2b [22, 125] 10 11 UMD ALOT RawFooT [229] [20] [37] 12 13 FMD DreTex 14 UBO2014 15 OpenSurfaces 16 17 DTD MINC 18 MINC2500 19 20 21 GTOS LFMD RDAD [183, 184] [152] [220] [9] [28] [10] [10] [231] [217] [13] 111 167 5612 8640 810 1000 4608 4752 1000 25000 3128 1000 40000 1915284 10422 5640 2996674 57500 Wild Imaging Environment Controlled Gray or Color Gray Color Color Controlled Color Controlled Color Controlled Gray Color Controlled Color Controlled 640 × 640 786 × 512 200 × 200 746 × 538 200 × 200 640 × 480 200 × 200 200 × 200 1280 × 960 Gray Wild Wild 111 167 92 320 10 25 11 11 20 10 Wild 800 × 800 512 × 384 200 × 200 400 × 400 25 250 1536 × 1024 Color Controlled Color Controlled 68 Color Color Controlled Color Synthesis Color Color Color Color unﬁxed unﬁxed unﬁxed Wild Wild Wild Wild 362 × 362 22 47 23 23 34243 1200 1488 40 12 57 Color Partially 240 × 240 Controlled 3787 × 2632 Color Uncontrolled 2592 × 1944 Color Uncontrolled Illumination Changes Rotation Changes Viewpoint Changes Scale Changes Small Small Small Small Small Small Small Li Liu et al.
Download Link [17] [215] [36] [151] [91] [206] [91] [91] [208] [4] [172] [50] [45] [205] [149] [46] [136] [136] Image Instances or Categories Year Content Instances 1966 Objects Instances 1995 Objects Instances 1999 Materials Instances 2002 Materials Instances 2004 Materials Materials Instances 2005 Materials Categories 2006 Materials Categories 2006 Instances 2009 Objects Instances 2009 Materials Materials Instances 2016 Materials Categories 2009 Materials Instances 2012 Materials Categories 2014 Materials Attributes Categories 2014 Materials Materials Clutter Clutter Clutter 2013 2015 2015 Instances 2016 Materials Materials Categories 2016 Objects Instances 2016 [63] [102] [176] Fig.
23 Image examples from one category in KTHTIPS2.
called “KTH Textures under varying Illumination, Pose, and Scale” (KTHTIPS) [70, 154] by imaging ten CUReT materials at three different illuminations, three different poses, and nine different dis- tances, but with signiﬁcantly fewer settings for lighting and view- ing angle than CUReT.
KTHTIPS was created to extend CUReT in two directions: (i) by providing variations in scale, and (ii) by imaging different samples of the CUReT materials in different set- tings.
This supports the study of recognizing different samples of the CUReT materials; for instance, does training on CUReT enable good recognition performance on KTHTIPS?
Despite pose varia- tions, rotation variations in KTHTIPS is very small.
Further details of KTHTIPS images acquisition is described in [154].
Experiments with Brodatz or VisTex used different nonover- lapping subregions from the same image for training and testing; experiments with CUReT or KTHTIPS used different subsets of the images imaged from the identical sample for training and test- ing.
KTHTIPS2 was one of the ﬁrst datasets to offer considerable variation within each class.
It groups textures not only by the in- stance, but also by the type of materials (e.g., wool).
It is built on KTHTIPS and provides a considerable extension by imaging four physical, planar samples of each of eleven materials [154].
There are two versions of the KTHTIPS2 database, i.e. KTHTIPS2a and KTHTIPS2b, the latter having 144 more images.
The Oulu Texture (Outex) database was collected by the Ma- chine Vision Group at the University of Oulu [142].
It has the largest number of different texture classes (320), with each class having images photographed under three illuminations and nine rotation angles, but with limited scale variation.
Based on Ou- tex, a series of benchmark test suites were derived for evalua- tions of texture classiﬁcation or segmentation algorithms [142].
Among them, two benchmark datasets Outex TC00010 and Ou- tex TC00012 [143] designated for testing rotation and illumination invariance, appear commonly in papers.
The UIUC (University of Illinois Urbana-Champaign) dataset collected by Lazebnik et al.
[96] contains 25 texture classes, with each class having 40 uncalibrated, unregistered images.
It has sig- niﬁcant variations in scale and viewpoint as well as nonrigid defor- mations (see Fig.
3 (b)), but has less severe illumination variations than CUReT.
The challenges of this database are that there are few sample images per class, but with signiﬁcant variations within classes.
Though UIUC improves over CUReT in terms of large intraclass variations, it is much smaller than CUReT both in the number of classes and the number of images per class.
The UMD (University of Maryland) dataset [229] also contains 25 texture classes, with each class having 40 uncalibrated, unregistered high- resolution images.
Similar to UIUC, it has signiﬁcant viewpoint and scale variations and uncontrolled illumination conditions.
As textures are imaged under variable truncation, viewpoint, and illu- mination, the UIUC and the UMD have stimulated the creation of texture representations that are invariant to signiﬁcant viewpoint changes.
The Amsterdam Library of Textures (ALOT) database [20] con- sists of 250 texture classes, with each class having 100 images.
It was collected under controlled lab environment at eight different A Survey of Recent Advances in Texture Representation 19 lighting conditions.
Although it has a much larger number of tex- ture classes than the UIUC or UMD, it has little scale, rotation and viewpoint variations, and is therefore not difﬁcult to recognize.
It provides three versions with different resolutions.
The Drexel Texture (DreTex) dataset [152] contains 20 different textures, each of which was imaged approximately 2000 times under different (known) illumination directions, at multiple distances, and with different in-plane and out of plane rotations.
It contains stochas- tic and regular textures.
The Raw Food Texture database (RawFooT), has been spe- cially designed to investigate the robustness of texture represen- tation methods with respect to variations in the lighting conditions [37].
It consists of 68 texture classes of raw food, with each class having 46 images acquired under 46 lighting conditions which may differ in the light direction, in the illuminant color, in its intensity, or in a combination of these factors.
It has no variations in rotation, viewpoint and scale.
Due to the rapid progress of texture representation approaches, the performance of many methods on the datasets described above are close to saturation, with KTHTIPS2b being an exception due to its increased complexity.
However, most datasets introduced above (excluding KTHTIPS2) make the simplifying assumption that tex- tures ﬁll images, and often, there is limited intraclass variability, due to a single or limited number of instances, captured under controlled scale, viewpoint and illumination.
In recent years, re- searchers have set their sights on more complex recognition prob- lems where textures appear under poor viewing conditions, low resolution, and in realistic cluttered backgrounds.
The Flickr Mate- rial Database (FMD) [183, 184] was built to address some of these limitations, by collecting many different object instances from the Internet grouped in 10 different material Categories, with exam- ples shown in Fig.
3 (e).
The FMD [183] focuses on identify- ing materials such as plastic, wood, ﬁber and glass.
The MIT re- searchers pointed out that although humans can quickly identify materials, automatic identiﬁcation of materials by computers is very challenging [185].
It is worth mentioning that although mate- rial recognition mostly employs texture features, it is in fact differ- ent from texture identiﬁcation, object recognition and scene recog- nition [185].
The limitations of the FMD dataset is that its size is quite small, containing only 10 material classes and the 100 images in each class.
The UBO2014 dataset [220] contains 7 material categories, with each having 12 different physical instances.
Each material instance was measured by a full bidirectional texture function with 22,801 images (a sampling of 151 viewing and 151 lighting di- rections), resulting in a total of more than 1.9 million synthesized images.
This synthesized material dataset allows classifying mate- rials under complex real-world scenarios.
Motivated by recent interests in visual attributes [49, 156, 155, 92], Cimpoi et al.
[28] identiﬁed a vocabulary of 47 texture at- tributes based on the seminal work of Bhusan et al.
[12] who stud- ied the relationship between commonly used English words and the perceptual properties of textures, identifying a set of words sufﬁcient to describing a wide variety of texture patterns.
These human-interpretable texture attributes can vividly characterize tex- tures, as shown in Fig.
24.
Based on the 47 texture attributes, they introduced a corresponding DTD dataset consisting of 120 texture images per attribute, by downloading images from the Internet in Fig.
24 Describing textures with attributes: The goal of DTD is to understand and generate automatically human interpretable descriptions such as the ex- amples above.
Fig.
25 Examples of material segments in the OpenSurfaces dataset.
an effort to support directly real world applications.
The large intr- aclass variations in the DTD are different from traditional texture datasets like CUReT, UIUC and UMD, in the sense that the im- ages shown in Fig.
3 (d) all belong to the braided class, whereas in a traditional sense these textures should belong to rather different texture categories.
Subsequently to FMD, Bell et al.
[9] released OpenSurfaces (OS) which has over 20,000 images from consumer photographs, each containing a number of high-quality texture or material seg- ments.
Images in the OS have real-world context, in contrast to prior databases like the CUReT, KTHTIPS and FMD, where each image belong to one texture category and the texture ﬁlls the whole image.
The OS has over 100,000 segments (as shown shown in Fig.
25) that can support a variety of applications.
Many of these segments are annotated with material names, the viewpoint, re- ﬂectance, the object names and scene class, but it still lacks ex- haustive annotations.
The number of segments in each material category can also be highly unbalanced in the OS; for example, the wood class has about 20,000 segments whereas the water class has only 18.
Nevertheless, OS has a large number of categories and the most samples of all prior databases.
Using the OS dataset as the seed, Bell et al.
[10] introduced a large material dataset named the Materials in Context Database (MINC) for material recognition and segmentation in the wild, with some image samples shown in Fig.
26.
MINC has a total of 3 million material samples from 23 different material categories, with the wood category having the largest number of samples (564,891 images) and the wallpaper category having the smallest number of samples (14,954 images).
MINC is more diverse, has more sam- ples in each category, and is much larger than previous datasets.
Bell et al.
concluded that a large and well-sampled dataset such as MINC is key for real-world material recognition and segmentation.
Concurrent to the work by Bell et al.
[10], Cimpoi et al.
[30] derived a new dataset from OS to conduct a study of material and describable texture attribute recognition in clutter.
Since not all segments in OS have a complete set of annotations, Cimpoi et al.
[30] selected a subset of segments annotated with material names, removing those material classes containing fewer than 400 seg- cracked, pittedbraided, knitted, woven, interlacedhoneycombed, flecked, meshedFabricBrickFood20 Li Liu et al.
have rotation and gray scale invariances, give perfect accuracies, revealing one of the limitations of CNN based descriptors in be- ing sensitive to image degradations.
Despite the usual advantages of CNN based methods, it is at a cost of very high computational complexity and memory requirements.
We believe that traditional texture descriptors, like the efﬁcient LBP [143] and robust variants such as MRELBP [114, 113], still have merits in cases when real time computation is a priority or when robustness to image degra- dations is needed [114].
Unfortunately, all the commonly-used tex- ture benchmarks in Table 4 are very small compared to large scale image datasets, such as ImageNet [178] and MITPlaces [243] com- monly used in CNN training, thus the texture datasets are not able to provide sufﬁcient training data to fully take advantage of CNN learning.
As can be seen from Table 4, currently the highest classiﬁ- cation scores on Outex TC10, Outex TC12, CUReT, KTHTIPS, UIUC, UMD and ALOT are nearly perfect, in excess of 99.5%, and quite a few texture representation approaches can achieve more than 99.0% accuracy on these datasets.
Since the inﬂuential work by Cimpoi et al.
[28, 29, 30] who reported near perfect classiﬁca- tion accuracies with pretrained CNN features for texture classiﬁ- cation, subsequent representative CNN based approaches have not reported results on these datasets because performance is saturated, and because the datasets are not large enough to allow ﬁnetuning to obtain improved results.
The FMD, DTD and KTHTIPS2b are undoubtedly more challenging than other texture datasets listed in Table 4, as is shown in Fig.
27 comparing texture category sep- aration in UIUC and FMD, and these more challenging datasets appear more frequently in recent works for evaluating texture rep- resentation methods.
However, since the IFV encoding of VG- GVD descriptors [30], the progress on these three datasets has been slow, with incremental improvements in accuracy and efﬁciency obtained by building more complex or deeper CNN architectures.
As can be observed from Table 4, LBP type methods (LBP [143], MRELBP [113] and BIF [33]) which adopt a predeﬁned codebook have a much more efﬁcient feature extraction step than the remaining methods listed in Table 4.
For those BoW based methods which require codebook learning, since the codebook learn- ing, feature encoding and pooling process are similar, the distin- guishing factors are the computation and feature dimensionality of the local texture descriptor.
Among commonly-used local tex- ture descriptors, those approaches ﬁrst detecting local regions of interest followed by local descriptors, such as SIFT, RIFT and SPIN [96, 239], are among the slowest and have relatively high dimensionality.
For the CNN based methods developed in [28, 29, 30], CNN feature extraction is performed on multiple scaled ver- sions of the original texture image, which requires more computa- tional time.
In general, CNN pretraining and ﬁnetuning is efﬁcient, whereas CNN model training is time consuming.
From [114], Scat- Net is computationally expensive at the feature extraction stage, though it has medium feature dimensionality.
Finally, at the fea- ture classiﬁcation stage linear SVM is signiﬁcantly faster than ker- nel SVM.
Recently Liu et al.
[114] proposed an extensive benchmark for measuring the robustness of a number of texture features against different classiﬁcation challenges, including changes in rotation, scale, illumination, viewpoint, number of classes, different types of image degradation, and computational complexity.
Fourteen datasets Fig.
26 Image samples from the MINC database.
The ﬁrst row are images from the food category, while the second row are images from foliage.
ments.
Cimpoi et al.
also annotated their dataset with eleven tex- ture attributes, since their selected OS segments do not trigger with sufﬁcient frequency all 47 attributes (as discussed in Section 5).
Similarly, the Robotics Domain Attributes Database (RDAD) [13] contains 57 categories of everyday indoor object and surface tex- tures labeled with a set of seventeen texture attributes, collected to addresses the target domain of everyday objects and surfaces that a service robot might encounter.
Wang et al.
[217] introduced a new light-ﬁeld dataset of ma- terials, called the Light-Field Material Database (LFMD).
It con- tains twelve material categories, each with 100 images taken with a Lytro Illum camera.
Since light-ﬁelds can capture multiple view- points in a single shot, they implicitly contain reﬂectance infor- mation, which should be helpful in material recognition.
The goal of LFMD is to investigate whether 4D light-ﬁeld information im- proves the performance of material recognition.
Compared with FMD [183], LFMD has two more material classes (fur and sky) which are very common in natural scenes.
LFMD is an important step towards light-ﬁeld based material recognition.
Finally, Xue et al.
[231] built a material database named the Ground Terrain in Outdoor Scenes (GTOS) to study the use of spa- tial and angular reﬂectance information of outdoor ground terrain for material recognition.
It consists of over 30,000 images cover- ing 40 classes of outdoor ground terrain under varying weather and lighting conditions.
The 40 material classes mostly have between 4 and 14 instances and each instance is imaged under 19 viewing directions.
The viewpoints in this dataset are regularly sampled in angle space.
6.2 Performance Table 4 presents a performance summary of representative meth- ods applied to popular benchmark texture datasets.
It is clear that major improvements have come from more powerful local texture descriptors such as MRELBP [114, 113], ScatNet [19] and CNN- based descriptors [30] and from advanced feature encoding meth- ods like IFV [159].
With the advance in CNN architectures, CNN- based texture representations have quickly demonstrated their strengths in texture classiﬁcation, especially for recognizing textures with very large appearance variations, such as the textures in KTHTIPS2b, FMD and DTD.
Off the shelf CNN based descriptors, in combination with IFV feature encoding, have advantages in nearly all of the benchmark datasets, except for Outex TC10 and Outex TC12 where texture descriptors, such as MRELBP [114, 113] and ScatNet [19] that (cid:7072)(cid:6558)(cid:5315)(cid:3374)(cid:1791)(cid:7072)(cid:11550)(cid:12971)(cid:2139)(cid:7072)(cid:11550)(cid:3374)(cid:1791)(cid:4714)(cid:4648)(cid:6208)(cid:1791)(cid:7569)(cid:1318)(cid:7163)(cid:17820)(cid:2568)(cid:2374)(cid:16374)(cid:9961)(cid:2568)(cid:2374)(cid:4714)(cid:5334)(cid:2568)(cid:2374)(cid:9888)(cid:5334)(cid:20172)(cid:14498)(cid:4558)(cid:1467)(cid:12971)(cid:2139)(cid:3374)(cid:1791)(cid:1973)(cid:4585)(cid:5418)(cid:12539)(cid:5284)(cid:5334)(cid:104)(cid:9888)(cid:5334)(cid:4558)(cid:20668)(cid:2591)(cid:6615)(cid:10393)(cid:1411)(cid:16024)(cid:19858)(cid:4558)(cid:1467)(cid:104)(cid:20172)(cid:14498)(cid:6247)(cid:3910)(cid:10393)(cid:1411)(cid:16024)(cid:19858)(cid:4558)(cid:1467)(cid:21)(cid:19)(cid:19)(cid:104)(cid:20172)(cid:14498)(cid:4558)(cid:20668)(cid:2591)(cid:6615)(cid:7552)(cid:7113)(cid:16024)(cid:19858)(cid:4558)(cid:1467)(cid:104)(cid:20172)(cid:14498)(cid:4558)(cid:20668)(cid:2591)(cid:6615)(cid:7552)(cid:17240)(cid:10393)(cid:1411)(cid:4558)(cid:1467)(cid:1085)(cid:3370)(cid:4554)(cid:20172)(cid:14498)(cid:1085)(cid:2591)(cid:6615)(cid:7552)(cid:7113)(cid:16024)(cid:19858)(cid:7538)(cid:8978)(cid:284)(cid:284)(cid:1913)(cid:10135)(cid:2568)(cid:2374)(cid:284)(cid:4671)(cid:284)(cid:284)(cid:284)(cid:284)(cid:284)(cid:284)(cid:284)(cid:1085)(cid:3370)(cid:4554)(cid:20172)(cid:14498)(cid:1085)(cid:2591)(cid:6615)(cid:7552)(cid:7113)(cid:16024)(cid:19858)(cid:7538)(cid:8978)(cid:284)(cid:284)(cid:284)(cid:284)(cid:1085)(cid:3370)(cid:4554)(cid:20172)(cid:14498)(cid:1085)(cid:2591)(cid:6615)(cid:13545)(cid:10806)(cid:4750)(cid:5719)(cid:12971)(cid:2139)(cid:284)(cid:284)(cid:284)(cid:26)(cid:25)(cid:27)(cid:104)(cid:20172)(cid:14498)(cid:4558)(cid:20668)(cid:2591)(cid:6615)(cid:7552)(cid:7113)(cid:16024)(cid:19858)(cid:4558)(cid:1467)(cid:24)(cid:20)(cid:21)(cid:104)(cid:20172)(cid:14498)(cid:1085)(cid:2591)(cid:6615)(cid:7552)(cid:7113)(cid:16024)(cid:19858)(cid:12971)(cid:2139)(cid:284)(cid:284)(cid:284)(cid:284)(cid:284)(cid:284)(cid:284)(cid:21)(cid:19)(cid:19)(cid:104)(cid:21)(cid:19)(cid:19)(cid:20172)(cid:14498)(cid:4558)(cid:20668)(cid:2591)(cid:6615)(cid:7552)(cid:7113)(cid:16024)(cid:19858)(cid:4558)(cid:1467)(cid:284)(cid:284)(cid:284)(cid:20172)(cid:14498)(cid:4558)(cid:20668)(cid:2591)(cid:6615)(cid:7552)(cid:7113)(cid:16024)(cid:19858)(cid:12971)(cid:2139)(cid:20172)(cid:14498)(cid:4558)(cid:20668)(cid:2591)(cid:6615)(cid:7552)(cid:7113)(cid:16024)(cid:19858)(cid:12971)(cid:2139)(cid:104)(cid:104)(cid:284)(cid:4671)(cid:4671)(cid:284)(cid:284)(cid:4671)(cid:4671)(cid:284)(cid:104)(cid:20172)(cid:14498)(cid:4558)(cid:20668)(cid:2591)(cid:6615)(cid:7552)(cid:7113)(cid:16024)(cid:19858)(cid:4558)(cid:1467)(cid:284)(cid:4671)(cid:4671)(cid:284)(cid:104)(cid:9888)(cid:5334)(cid:6247)(cid:3910)(cid:2591)(cid:6615)(cid:10393)(cid:1411)(cid:16024)(cid:19858)(cid:4558)(cid:1467)(cid:284)(cid:284)(cid:284)(cid:104)(cid:9888)(cid:5334)(cid:6247)(cid:3910)(cid:2591)(cid:6615)(cid:7552)(cid:7113)(cid:16024)(cid:19858)(cid:4558)(cid:1467)(cid:284)(cid:284)(cid:284)(cid:284)(cid:104)(cid:20172)(cid:14498)(cid:1085)(cid:2591)(cid:6615)(cid:7552)(cid:7113)(cid:16024)(cid:19858)(cid:7538)(cid:8978)(cid:284)(cid:284)(cid:284)(cid:284)A Survey of Recent Advances in Texture Representation 21 Fig.
27 t-distributed Stochastic Neighbor Embedding (tSNE) [119] of textures from the IFV encoding of the VGGVD features [30] from (a) the UIUC dataset and (b) the FMD dataset.
Clearly the classes in UIUC are more separable than those in FMD.
from eight most commonly used texture sources were used in the benchmark.
An extensive evaluation of the recent most promising LBP variants and some descriptors based on deep convolutional networks was carried out, with the best overall performance ob- tained by the MRELBP feature.
7 Discussion and Conclusion The importance of texture representations is in the fact that they have extended to many different problems beyond that of textures.
As the ﬁrst comprehensive survey on texture representations in the new century, this paper has highlighted the recent achieve- ments, provided some structural categories for the methods accord- ing to their roles in feature representation, analyzed their merits and demerits, summarized existing popular texture datasets, and discussed performance for the most representative approaches.
Al- most any practical application is a compromise between classiﬁca- tion accuracy, robustness to image degradations, compactness and efﬁciency, number of training data available, and cost and power consumption of implementations.
Although signiﬁcant progress has been made in the last two decades, we may be at the beginning of a new era, and there is still ample room for future research, which leads to the following discussions.
Large Scale Texture Dataset Collection.
The constantly increas- ing volume of image and video data creates new opportunities and challenges.
The complex variability of big image data reveals the inadequacies of conventional handcrafted texture descriptors and brings opportunities for representation learning techniques, such as deep learning, which aim at learning good representations au- tomatically from data.
The recent unprecedented success of deep learning in image classiﬁcation and object recognition is insepara- ble from the availability of large-scale annotated image datasets such as ImageNet [178] and MS COCO [105].
However, deep learning based texture analysis has not kept pace with the rapid progress witnessed in other ﬁelds, such as image classiﬁcation, partially due to the unavailability of a large-scale texture database.
As a result there is signiﬁcant motivation for a good, large-scale texture dataset, which will signiﬁcantly advance texture analysis, especially using deep learning.
Efforts need to be devoted to study what kinds of texture datasets are needed for future research.
More Effective and Robust Texture Representations.
Despite signiﬁcant progress in recent years most texture descriptors, ir- respective of whether handcrafted or learned, have not been ca- pable of performing at a level sufﬁcient for real world textures.
The ultimate goal of the community is to develop texture repre- sentations that can accurately and robustly discriminate massive image texture categories in all possible scenes, at a level compa- rable to the human visual system.
In practical applications, factors such as signiﬁcant changes in illumination, rotation, viewpoint and scale, and image degradations such as occlusions, image blur and random noise call for more discriminative and robust texture rep- resentations.
Further input from psychological research of visual perception and the biology of the human visual system would be welcome.
Compact and Efﬁcient Texture Representations.
There is a ten- sion between the demands of big data and desire for highly com- pact and efﬁcient feature representations.
Thus, on the one hand, in the era of big data many existing texture representations are failing to keep pace with the emerging big dimensionality [236], leading to a pressing need for new strategies in dealing with scal- ability, high computational complexity, and storage.
On the other hand, there is a growing need for deploying highly compact and resource-efﬁcient feature representations on platforms like low en- ergy embedded vision sensors and handheld devices.
Many of the existing descriptors would similarly fail in these contexts, and the current general trend of deep CNN architectures has been to de- velop deeper and more complicated networks, advances requiring massive data and power hungry GPUs for training, not suitable to be deployed on mobile platforms that have limited resources.
As a result, there is a growing interest in building compact and efﬁ- cient CNN-based features [73, 171].
While CNNs generally out- perform classical texture descriptors like LBP, it remains to be seen which approaches will be most effective in resource-limited contexts, and whether some degree of LBP / CNN hybridization might be considered, such as recent lightweight CNN architectures in [108, 227].
(a)UIUC(25Classes)(b)FMD(10Classes)22 Li Liu et al.
Reduced Dependence on Large Amounts of Data.
Deep learn- ing features have signiﬁcantly promoted the performance of visual recognition, including texture recognition, however the deep learn- ing feature representations need to be learned on a large amount of training data.
Certainly, there are many applications where tex- ture representations are very useful and only limited amounts of annotated training data can be available or collecting labeled train- ing data is too expensive (such as visual inspection, facial micro- expression recognition, age estimation and medical texture anal- ysis).
Possible research directions could be the development of learnable local feature descriptors with small or medium amounts of training data, as in [47, 117], or to explore effective transfer learning methods.
Semantic Texture Attributes.
Progress in image texture repre- sentation and understanding, while substantial, has so far been mostly focused on low-level feature representation.
However, in order to address advanced human-centric applications such as detailed im- age search and human-robotic interaction, low-level understanding will not be sufﬁcient.
Texture patterns are another type of visual abstraction that we can describe with no less richness and detail than the objects themselves, however there has been only limited research on describing textures with human-like texture attributes.
Future efforts should be devoted along this direction to go beyond texture identiﬁcation and categorization, and to develop semantic and easily describable texture attributes that can be well predicted with low-level texture representations, and to explore even ﬁne- grained and compositional structure analysis of texture patterns.
Effect of Smaller Image Size.
Performance evaluation of texture descriptors is usually done with texture datasets consisting of rel- atively large images.
For a large number of applications an ability to analyze small image sizes at high speed is vital, including facial image analysis, interest region description, segmentation, defect detection, and tracking.
Many existing texture descriptors would fail in this respect, and it would be important to evaluate the per- formance of new descriptors, as was done in [182].
References 1.
Aharon M., Elad M., Bruckstein A.
(2006) ksvd: An algorithm for de- signing overcomplete dictionaries for sparse representation.
IEEE Trans- actions on signal processing 54(11):4311–4322 2.
Ahonen T., Pietik¨ainen M.
(2007) Soft histograms for local binary pat- terns.
In: Proceedings of the Finnish signal processing symposium, vol 5, p.
1 3.
Ahonen T., Hadid A., Pietikainen M.
(2006) Face description with lo- cal binary patterns: Application to face recognition.
IEEE trans pattern analysis and machine intelligence 28(12):2037–2041 (2009) http://aloi.science.uva.nl/public_ 4.
ALOT alot/ 5.
Amadasun M., King R.
(1989) Textural features corresponding to tex- tural properties.
IEEE Transactions on systems, man, and Cybernetics 19(5):1264–1274 6.
Andrearczyk V., Whelan P.
(2016) Using ﬁlter banks in convolutional neural networks for texture classiﬁcation.
Pattern Recognition Letters 84:63–69 7.
Arandjelovic R., Gronat P., Torii A., Pajdla T., Sivic J.
(2016) NetVLAD: CNN architecture for weakly supervised place recognition.
In: Inter- national Conference on Computer Vision and Pattern Recognition, pp.
5297–5307 8.
Baraniuk R., Davenport M., DeVore R., Wakin M.
(2008) A simple proof of the restricted isometry property for random matrices.
Constructive Approximation 28(3):253–263 9.
Bell S., Upchurch P., Snavely N., Bala K.
(2013) Opensurfaces: A richly annotated catalog of surface appearance.
ACM Transactions on Graphics 32(4):111 10.
Bell S., Upchurch P., Snavely N., Bala K.
(2015) Material recognition in the wild with the materials in context database.
In: International Confer- ence on Computer Vision and Pattern Recognition, pp.
3479–3487 11.
Bengio Y., Courville A., Vincent P.
(2013) Representation learning: A review and new perspectives.
IEEE trans pattern analysis and machine intelligence 35(8):1798–1828 12.
Bhushan N., Rao A.
R., Lohse G.
L.
(1997) The texture lexicon: Under- standing the categorization of visual texture terms and their relationship to texture images.
Cognitive Science 21(2):219–246 13.
Bormann R., Esslinger D., Hundsdoerfer D., Haegele M., Vincze M.
(2016) Texture characterization with semantic attributes: Database and algorithm.
In: The 47th International Symposium on Robotics, pp.
1–8 14.
Boureau Y., Ponce J., LeCun Y.
(2010) A theoretical analysis of feature pooling in visual recognition.
In: International Conference on machine learning, pp.
111–118 15.
Bovik A., Clark M., Geisler W.
(1990) Multichannel texture analysis using localized spatial ﬁlters.
IEEE Trans Pattern Analysis and Machine Intelligence 12(1):55–73 16.
Brahnam S., Jain L., Nanni L., Lumini A.
(2014) Local binary patterns: new variants and applications.
Springer 17.
Brodatz (1966) brodatz.html http://www.ux.uis.no/˜tranden/ 18.
Brodatz P.
(1966) Textures: A Photographic Album for Artists and De- signers.
Dover Publications, New York, USA 19.
Bruna J., Mallat S.
(2013) Invariant scattering convolution networks.
IEEE trans Pattern Analysis and Machine Intelligence 35(8):1872–1886 20.
Burghouts G., Geusebroek J.
(2009) Material speciﬁc adaptation of color invariant features.
Pattern Recognition Letters 30(3):306 – 313 21.
Candes E.
J., Tao T.
(2006) Near-optimal signal recovery from random projections: Universal encoding strategies?
IEEE transactions on infor- mation theory 52(12):5406–5425 22.
Caputo B., Hayman E., Mallikarjuna P.
(2005) Class speciﬁc material categorisation.
In: International Conference on Computer Vision, vol 2, pp.
1597–1604 23.
Chan T., Jia K., Gao S., Lu J., Zeng Z., Ma Y.
(2015) PCANet: A sim- ple deep learning baseline for image classiﬁcation?
IEEE Trans Image Processing 24(12):5017–5032 24.
Chatﬁeld K., Lempitsky V., Vedaldi A., Zisserman A.
(2011) The devil is in the details: an evaluation of recent feature encoding methods.
In: BMVC, vol 2, p.
8 25.
Chatﬁeld K., Simonyan K., Vedaldi A., Zisserman A.
(2014) Return of the devil in the details: Delving deep into convolutional nets.
British Machine Vision Conference 26.
Chellappa R., Chatterjee S.
(1985) Classiﬁcation of textures using Gaus- sian Markov Random ﬁelds.
IEEE Trans Acoustics, Speech, and Signal Processing 33(4):959–963 27.
Chen J., Shan S., He C., Zhao G., Pietikainen M., Chen X., Gao W.
(2010) Wld: a robust local image descriptor.
IEEE Trans Pattern Anal Mach Intell 32(9):1705–1720 28.
Cimpoi M., Maji S., Kokkinos I., Mohamed S., Vedaldi A.
(2014) De- scribing textures in the wild.
In: International Conference on Computer Vision and Pattern Recognition, pp.
3606–3613 29.
Cimpoi M., Maji S., Vedaldi A.
(2015) Deep ﬁlter banks for texture recognition and segmentation.
In: International Conference on Computer Vision and Pattern Recognition, pp.
3828–3836 30.
Cimpoi M., Maji S., Kokkinos I., Vedaldi A.
(2016) Deep ﬁlter banks for texture recognition, description, and segmentation.
International Journal of Computer Vision 118(1):65–94 31.
Cinbis R.
G., Verbeek J., Schmid C.
(2016) Approximate ﬁsher kernels of non-iid image models for image categorization.
IEEE transactions on pattern analysis and machine intelligence 38(6):1084–1098 32.
Conners R.
W., Harlow C.
A.
(1980) A theoretical comparison of tex- ture algorithms.
IEEE Trans Pattern Analysis and Machine Intelligence (3):204–222 33.
Crosier M., Grifﬁn L.
D.
(2010) Using basic image features for texture classiﬁcation.
Int J Comput Vision 88(3):447–460 34.
Cross G., Jain A.
(1983) Markov random ﬁeld texture models.
IEEE Trans Pattern Analysis and Machine Intelligence (1):25–39 A Survey of Recent Advances in Texture Representation 23 35.
Csurka G., Dance C., Fan L., Willamowski J., Bray C.
(2004) Visual categorization with bags of keypoints.
In: ECCV Workshop on statistical learning in computer vision 36.
CUReT (1999) http://www.cs.columbia.edu/CAVE/ software/curet/html/about.php 37.
Cusano C., Napoletano P., Schettini R.
(2016) Evaluating color texture descriptors under large variations of controlled lighting conditions.
Jour- nal of the Optical Socienty of America A 33(1):17–30 38.
Dahl A., Larsen R.
(2011) Learning dictionaries of discriminative image patches.
In: British Machine Vision Conference 39.
Dai X., Ng J.
Y.-H., Davis L.
S.
(2017) FASON: First and Second Order information fusion Network for texture recognition.
In: IEEE Confer- ence on Computer Vision and Pattern Recognition, pp.
7352–7360 40.
Dana K., Van Ginneken B., Nayar S., Koenderink J.
(1999) Reﬂectance and texture of real world surfaces.
ACM Transactions On Graphics 18(1):1–34 41.
Depeursinge A., Al-Kadi O., Mitchell J.
(2017) Biomedical Texture Analysis.
Academic Press 42.
Ding C., Choi J., Tao D., Davis L.
S.
(2016) Multi-directional multi- level dual-cross patterns for robust face recognition.
IEEE trans pattern analysis and machine intelligence 38(3):518–531 43.
Donahue J., Jia Y., Vinyals O., Hoffman J., Zhang N., Tzeng E., Darrell T.
(2014) DeCAF: A deep convolutional activation feature for generic visual recognition.
In: International conference on machine learning, pp.
647–655 44.
Donoho D.
L.
(2006) Compressed sensing.
IEEE Transactions on infor- mation theory 52(4):1289–1306 45.
Drexel (2012) texture/ https://www.cs.drexel.edu/˜kon/ 46.
DTD (2014) http://www.robots.ox.ac.uk/˜vgg/ data/ dtd/ 47.
Duan Y., Lu J., Feng J., Zhou J.
(2017) Context aware local binary fea- ture learning for face recognition.
IEEE Trans Pattern Analysis and Ma- chine Intelligence 48.
Efros A.
A., Leung T.
K.
(1999) Texture synthesis by nonparametric sampling.
In: International Conference on Computer Vision, vol 2, pp.
1033–1038 49.
Farhadi A., Endres I., Hoiem D., Forsyth D.
(2009) Describing objects by their attributes.
In: International Conference on Computer Vision and Pattern Recognition, pp.
1778–1785 50.
FMD (2009) http://people.csail.mit.edu/ celiu/ CVPR2010/FMD/ 51.
Forsyth D., Ponce J.
(2012) Computer Vision: A Modern Approach, 2nd Edition 52.
Freeman W., Adelson E.
(1991) The design and use of steerable ﬁlters.
IEEE Trans Pattern analysis and machine intelligence 13(9):891–906 53.
Fritz M., Hayman E., Caputo B., Eklundh J.
(2004) The KTH- TIPS database.
http://www.nada.kth.se/cvap/databases /kth-tips/kth_tips.pdf 54.
Gao Y., Beijbom O., Zhang N., Darrell T.
(2016) Compact bilinear pooling.
In: International Conference on Computer Vision and Pattern Recognition, pp.
317–326 55.
G˚arding J., Lindeberg T.
(1996) Direct computation of shape cues using scale-adapted spatial derivative operators.
International Journal of Com- puter Vision 17(2):163–191 56.
Gatys L., Ecker A., Bethge M.
(2015) Texture synthesis using convolu- tional neural networks.
In: Advances in Neural Information Processing Systems, pp.
262–270 57.
Gatys L., Ecker A., Bethge M.
(2016) Image style transfer using convo- lutional neural networks.
In: CVPR, pp.
2414–2423 58.
Georgescu B., Shimshoni I., Meer P.
(2003) Mean shift based clustering in high dimensions: A texture classiﬁcation example.
In: International Conference on Computer Vision, vol 3, p.
456 59.
Girshick R., Donahue J., Darrell T., Malik J.
(2014) Rich feature hi- erarchies for accurate object detection and semantic segmentation.
In: International Conference on Computer Vision and Pattern Recognition, pp.
580–587 60.
Giryes R., Sapiro G., Bronstein A.
M.
(2016) Deep neural networks with random gaussian weights: a universal classiﬁcation strategy?
IEEE Trans Signal Processing 64(13):3444–3457 61.
Grifﬁn L., Lillholm M., Crosier M., van Sande J.
(2009) Basic image features (bifs) arising from approximate symmetry type.
Scale Space and Variational Methods in Computer Vision pp.
343–355 62.
Grifﬁn L.
D., Lillholm M.
(2010) Symmetry sensitivities of derivative- of-gaussian ﬁlters.
IEEE trans pattern analysis and machine intelligence 32(6):1072–1083 63.
Ground Terrain in Outdoor Scenes (GTOS) (2016) http:// computervision.engr.rutgers.edu/ 64.
Gu J., Wang Z., Kuen J., Ma L., et al.
(2017) Recent advances in convo- lutional neural networks.
Pattern Recognition 65.
Guo Z., Zhang L., Zhang D.
(2010) A completed modeling of local bi- nary pattern operator for texture classiﬁcation.
IEEE Trans Image Pro- cess 9(16):1657–1663 66.
Guo Z., Wang X., Zhou J., You J.
(2016) Robust texture image represen- tation by scale selective local binary patterns.
IEEE Trans Image Pro- cessing 25(2):687–699 67.
Haralick R.
(1979) Statistical and structural approaches to texture.
Pro- ceedings of the IEEE 67(5):786–804 68.
Haralick R., Shanmugam K., Dinstein I.
(1973) Textural features for image classiﬁcation.
IEEE Trans on Systems, Man, and Cybernetics (6):610–621 69.
Hariharan B., Arbel´aez P., Girshick R., Malik J.
(2015) Hypercolumns for object segmentation and ﬁne-grained localization.
In: International Conference on Computer Vision and Pattern Recognition, pp.
447–456 70.
Hayman E., Caputo B., Fritz M., Eklundh J.
(2004) On the signiﬁcance of real world conditions for material classiﬁcation.
European Conference on Computer Vision pp.
253–266 71.
He C., Li S., Liao Z., Liao M.
(2013) Texture classiﬁcation of PolSAR data based on sparse coding of wavelet polarization textons.
IEEE Trans Geoscience and Remote Sensing 51(8):4576–4590 72.
He K., Zhang X., Ren S., Sun J.
(2016) Deep residual learning for im- age recognition.
In: International Conference on Computer Vision and Pattern Recognition, pp.
770–778 73.
Howard A., Zhu M., Chen B., Kalenichenko D., Wang W., Weyand T., Andreetto M., Adam H.
(2017) Mobilenets: Efﬁcient convolutional neu- ral networks for mobile vision applications.
In: CVPR 74.
Huang D., Shan C., Ardabilian M., Wang Y., Chen L.
(2011) Local bi- nary patterns and its application to facial image analysis: a survey.
IEEE Transactions on Systems, Man, and Cybernetics Part C 41(6):765–781 75.
Huang G., Liu Z., Weinberger K.
Q., van der Maaten L.
(2017) Densely connected convolutional networks.
In: International Conference on Computer Vision and Pattern Recognition 76.
Huang Y., Wu Z., Wang L., Tan T.
(2014) Feature coding in image clas- siﬁcation: A comprehensive study.
IEEE trans pattern analysis and ma- chine intelligence 36(3):493–506 77.
Jain A., Farrokhnia F.
(1991) Unsupervised texture segmentation using Gabor ﬁlters.
Pattern Recognition 24(12):1167–1186 78.
Jain A., Duin R., Mao J.
(2000) Statistical pattern recognition: A review.
IEEE Trans pattern analysis and machine intelligence 22(1):4–37 79.
Jegou H., Perronnin F., Douze M., S´anchez J., Perez P., Schmid C.
(2012) Aggregating local image descriptors into compact codes.
IEEE Trans Pattern Analysis and Machine Intelligence 34(9):1704–1716 80.
Julesz B.
(1962) Visual pattern discrimination.
IRE transactions on In- formation Theory 8(2):84–92 81.
Julesz B.
(1981) Textons, the elements of texture perception, and their interactions.
Nature 290(5802):91–97 82.
Julesz B., Bergen J.
(1983) Human factors and behavioral science: Tex- tons, the fundamental elements in preattentive vision and perception of textures.
The Bell System Technical Journal 62(6):1619–1645 83.
Kadir T., Brady J.
(2002) Scale, saliency and scene description.
PhD thesis, Oxford University 84.
Kandaswamy U., Adjeroh D., Lee M.
(2005) Efﬁcient texture analysis of SAR imagery.
IEEE Trans Geoscience and Remote Sensing 43(9):2075– 2083 85.
Keller J., Chen S., Crownover R.
(1989) Texture description and segmen- tation through fractal geometry.
Computer Vision, Graphics, and Image Processing 45(2):150–166 86.
Kim K., Jung K., Park S., Kim H.
(2002) Support vector machines for texture classiﬁcation.
IEEE trans pattern analysis and machine intelli- gence 24(11):1542–1550 87.
Kong S., Fowlkes C.
(2017) Low rank bilinear pooling for ﬁne-grained classiﬁcation.
In: International Conference on Computer Vision and Pat- tern Recognition, pp.
7025–7034 24 Li Liu et al.
88.
Kong S., Wang D.
(2012) Multilevel feature descriptor for robust texture classiﬁcation via locality constrained collaborative strategy.
arXiv:12030488 89.
Krishna R., Zhu Y., Groth O., Johnson J., Hata K., Kravitz J., Chen S., Kalantidis Y., Li L., Shamma D., Bernstein M., FeiFei L.
(2017) Visual genome: Connecting language and vision using crowdsourced dense im- age annotations.
International Journal on Computer Vision 123(1):32– 73 90.
Krizhevsky A., Sutskever I., Hinton G.
(2012) ImageNet classiﬁcation with deep convolutional neural networks.
In: Advances in neural infor- mation processing systems, pp.
1097–1105 91.
KTHTIPS (2004) http://www.nada.kth.se/cvap/ databases/kth-tips/download.html 92.
Kumar N., Berg A., Belhumeur P.
N., Nayar S.
(2011) Describable vi- sual attributes for face veriﬁcation and image search.
IEEE Trans Pattern Analysis and Machine Intelligence 33(10):1962–1977 93.
Lategahn H., Gross S., Stehle T., Aach T.
(2010) Texture classiﬁcation by modeling joint distributions of local patterns with gaussian mixtures.
IEEE Trans Image Processing 19(6):1548–1557 94.
Laws K.
(1980) Rapid texture identiﬁcation.
In: Proc.
SPIE Conf.
Image Processing for Missile Guidance, vol 238, pp.
376–381 95.
Lazebnik S., Schmid C., Ponce J.
(2003) A sparse texture representation using afﬁne-invariant regions.
In: International Conference on Computer Vision and Pattern Recognition, vol 2, pp.
II–II 96.
Lazebnik S., Schmid C., Ponce J.
(2005) A sparse texture representa- tion using local afﬁne regions.
IEEE Trans Pattern Anal Mach Intell 27(8):1265–1278 97.
Lazebnik S., Schmid C., Ponce J.
(2006) Beyond bags of features: Spa- tial pyramid matching for recognizing natural scene categories.
In: Inter- national Conference on Computer vision and pattern recognition, vol 2, pp.
2169–2178 98.
LeCun Y., Bengio Y., Hinton G.
(2015) Deep learning.
Nature 521(7553):436–444 99.
Lee T.
S.
(1996) Image representation using 2D Gabor wavelets.
IEEE Transactions on pattern analysis and machine intelligence 18(10):959– 971 100.
Leung T., Malik J.
(2001) Representing and recognizing the visual appearance of materials using three-dimensional textons.
International journal of computer vision 43(1):29–44 101.
Levi G., Hassner T.
(2015) Emotion recognition in the wild via convolu- tional neural networks and mapped binary patterns.
In: ACM ICMI, pp.
503–510 http://eceweb1.rutgers.edu/˜kdana/ 102.
LFMD (2016) code.html 103.
Li S.
(2009) Markov Random Field Modeling in Image Analysis.
Springer Science and Business Media 104.
Lin T., Maji S.
(2016) Visualizing and understanding deep texture repre- sentations.
In: IEEE Conference on Computer Vision and Pattern Recog- nition, pp.
2791–2799 105.
Lin T., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Doll´ar P., Zitnick L.
(2014) Microsoft COCO: Common objects in context.
In: ECCV, pp.
740–755 106.
Lin T., RoyChowdhury A., Maji S.
(2015) Bilinear cnn models for ﬁne- grained visual recognition.
In: Proceedings of the IEEE International Conference on Computer Vision, pp.
1449–1457 107.
Lin T., RoyChowdhury A., Maji S.
(2017) Bilinear convolutional neural networks for ﬁne-grained visual recognition.
IEEE Trans Pattern Analy- sis and Machine Intelligence 108.
Lin X., Zhao C., Pan W.
(2017) Towards accurate binary convolutional neural network.
In: NIPS, pp.
344–352 109.
Liu L., Fieguth P.
(2012) Texture classiﬁcation from random features.
IEEE Trans Pattern Analysis and Machine Intelligence 34(3):574 –586 110.
Liu L., Fieguth P., Kuang G., Zha H.
(2011) Sorted random projections for robust texture classiﬁcation.
In: International Conference on Com- puter Vision, IEEE, pp.
391–398 111.
Liu L., Fieguth P., Kuang G., Clausi D.
(2012) Sorted random projections for robust rotation invariant texture classiﬁcation.
Pattern Recognition 45(6):2405–2418 112.
Liu L., Fieguth P., Wang X., Pietik¨ainen M., Hu D.
(2016) Evaluation of LBP and deep texture descriptors with a new robustness benchmark.
In: European Conference on Computer Vision 113.
Liu L., Lao S., Fieguth P., Guo Y., Wang X., Pietikainen M.
(2016) Me- dian robust extended local binary pattern for texture classiﬁcation.
IEEE Trans Image Processing 25(3):1368–1381 114.
Liu L., Fieguth P., Guo Y., Wang X., Pietik¨ainen M.
(2017) Local binary features for texture classiﬁcation: Taxonomy and experimental study.
Pattern Recognition 62:135–160 115.
Liu Y., Tsin Y., Lin W.
(2005) The promise and perils of near regular texture.
International Journal of Computer Vision 62(1):145–159 116.
Lowe D.
(2004) Distinctive image features from scale-invariant key- points.
International journal of computer vision 60(2):91–110 117.
Lu J., Liong V.
E., Zhou J.
(2017) Simultaneous local binary feature learning and encoding for homogeneous and heterogeneous face recog- nition.
IEEE trans pattern analysis and machine intelligence 118.
Ma L., Tan T., Wang Y., Zhang D.
(2003) Personal identiﬁcation based on iris texture analysis.
IEEE Trans Pattern Analysis and Machine Intel- ligence 25(12):1519–1533 119.
Maaten L., Hinton G.
(2008) Visualizing data using t-SNE.
Journal of Machine Learning Research 9(Nov):2579–2605 120.
Mairal J., Bach F., Ponce J., Sapiro G., Zisserman A.
(2008) Discrimina- tive learned dictionaries for local image analysis.
In: IEEE Conference on Computer Vision and Pattern Recognition, IEEE, pp.
1–8 121.
Mairal J., Ponce J., Sapiro G., Zisserman A., Bach F.
(2009) Supervised dictionary learning.
In: Advances in neural information processing sys- tems, pp.
1033–1040 122.
Malik J., Perona P.
(1990) Preattentive texture discrimination with early vision mechanisms.
JOSA A 7(5):923–932 123.
Malik J., Belongie S., Shi J., Leung T.
(1999) Textons, contours and regions: Cue integration in image segmentation.
In: International Con- ference on Computer Vision, vol 2, pp.
918–925 124.
Mallat S.
(1989) A theory for multiresolution signal decomposition: the wavelet representation.
IEEE Trans Pattern Analysis and Machine Intel- ligence 11(7):674–693 125.
Mallikarjuna P., Tavakoli A., Fritz M., Hayman E., Caputo B., Eklundh J.
(2006) The KTH-TIPS2 database.
http://www.nada.kth.se/ cvap/databases /kth-tips/kth-tips2.pdf 126.
Mandelbrot B., Pignoni R.
(1983) The fractal geometry of nature.
Free- man, New York 127.
Manjunath B., Chellappa R.
(1991) Unsupervised texture segmentation using markov random ﬁeld models.
IEEE Trans Pattern Analysis and Machine Intelligence 13(5):478–482 128.
Manjunath B.
S., Ma W.-Y.
(1996) Texture features for browsing and retrieval of image data.
IEEE Trans Pattern Analysis and Machine Intel- ligence 18(8):837–842 129.
Mao J., Jain A.
(1992) Texture classiﬁcation and segmentation using multiresolution simultaneous autoregressive models.
Pattern Recogni- tion 25(2):173–188 130.
Marszałek M., Schmid C., Harzallah H., J.
van de W.
(2007) Learn- ing object representations for visual object class recognition.
In: Visual Recognition Challange workshop in conjunction with ICCV 131.
Matthews T., Nixon M.
S., Niranjan M.
(2013) Enriching texture analy- sis with semantic data.
In: International Conference on Computer Vision and Pattern Recognition, pp.
1248–1255 132.
Mellor M., Hong B.-W., Brady M.
(2008) Locally rotation, contrast, and scale invariant descriptors for texture analysis.
IEEE Transactions on Pattern Analysis and Machine Intelligence 30(1):52–61 133.
Mikolajczyk K., Schmid C.
(2002) An afﬁne invariant interest point de- tector.
European Conference on Computer Vision pp.
128–142 134.
Mikolajczyk K., Schmid C.
(2005) A performance evaluation of lo- cal descriptors.
IEEE trans pattern analysis and machine intelligence 27(10):1615–1630 135.
Mikolajczyk K., Tuytelaars T., Schmid C., Zisserman A., Matas J., Schaffalitzky F., Kadir T., Van Gool L.
(2005) A comparison of afﬁne region detectors.
International journal of computer vision 65(1-2):43– 72 136.
MINC (2015) http://opensurfaces.cs.cornell.edu/ publications/minc/ 137.
Mirmehdi M., Xie X., Suri J.
(2008) Handbook of Texture Analysis.
Imperial College Press, London, UK 138.
Nanni L., Lumini A., Brahnam S.
(2010) Local binary patterns vari- ants as texture descriptors for medical image analysis.
Artif Intell Med 49(2):117–125 A Survey of Recent Advances in Texture Representation 25 139.
Napoletano P.
(2017) Hand crafted vs learned descriptors for color texture classiﬁcation.
In: International Workshop Computational Color Imaging, pp.
259–271 140.
Ohanian P., Dubes R.
(1992) Performance evaluation for four classes of textural features.
Pattern recognition 25(8):819–833 141.
Ojala T., Pietik¨ainen M., Harwood D.
(1996) A comparative study of tex- ture measures with classiﬁcation based on featured distributions.
Pattern recognition 29(1):51–59 142.
Ojala T., M¨aenp¨a¨a T., Pietik¨ainen M., Viertola J., Kyll¨onen J., Huovi- nen S.
(2002) Outex-new framework for empirical evaluation of texture analysis algorithms.
In: International Conference on Pattern Recogni- tion, vol 1, pp.
701–706 143.
Ojala T., Pietik¨ainen M., Maenp¨a¨a T.
(2002) Multiresolution gray-scale and rotation invariant texture classiﬁcation with local binary patterns.
IEEE Trans Pattern Anal Mach Intell 24(7):971–987 144.
Ojansivu V., Heikkil¨a J.
(2008) Blur insensitive texture classiﬁcation us- ing local phase quantization.
In: International Conference On Image And Signal Processing, pp.
236–243 145.
Ojansivu V., Rahtu E., Heikkila J.
(2008) Rotation invariant local phase quantization for blur insensitive texture analysis.
In: International Con- ference on Pattern Recognition, pp.
1–4 146.
Okazawa G., Tajima S., Komatsu H.
(2015) Image statistics underlying natural texture selectivity of neurons in macaque v4.
Proceedings of the National Academy of Sciences 112(4):E351–E360 147.
Olshausen B., Field D.
(1996) Emergence of simple cell receptive ﬁeld properties by learning a sparse code for natural images.
Nature 381(6583):607–609 148.
Olshausen B.
A., Field D.
J.
(1997) Sparse coding with an overcomplete basis set: A strategy employed by v1?
Vision research 37(23):3311– 3325 149.
Open Surfaces (2013) http://opensurfaces.cs.cornell.
edu/ 150.
Oquab M., Bottou L., Laptev I., Sivic J.
(2014) Learning and transferring mid-level image representations using convolutional neural networks.
In: International Conference on Computer Vision and Pattern Recognition, pp.
1717–1724 151.
Outex (2002) http://www.outex.oulu.fi/ index.php?
page=outex_home 152.
Oxholm G., Bariya P., Nishino K.
(2012) The scale of geometric texture.
European Conference on Computer Vision pp.
58–71 153.
Oyallon E., Mallat S.
(2015) Deep roto-translation scattering for ob- ject classiﬁcation.
In: IEEE Conference on Computer Vision and Pattern Recognition, pp.
2865–2873 154.
P.
Mallikarjuna, M.
Fritz, A.
Tavakoli Targhi, E.
Hayman, B.
Caputo, and J.
Eklundh, The KTH-TIPS and KTH-TIPS2 databases (2004) http://www.nada.kth.se/cvap/ databases/kth-tips/ documentation.html 155.
Parikh D., Grauman K.
(2011) Relative attributes.
In: International Con- ference on Computer Vision, pp.
503–510 156.
Patterson G., Xu C., Su H., Hays J.
(2014) The sun attribute database: Beyond categories for deeper scene understanding.
International Journal of Computer Vision 108(1-2):59–81 157.
Peikari M., Gangeh M.
J., Zubovits J., Clarke G., Martel A.
L.
(2016) Triaging diagnostically relevant regions from pathology whole slides of breast cancer: A texture based approach.
IEEE trans medical imaging 35(1):307–315 158.
Perronnin F., Dance C.
(2007) Fisher kernels on visual vocabularies for image categorization.
In: International Conference on Computer Vision and Pattern Recognition, pp.
1–8 159.
Perronnin F., Sanchez J., Mensink T.
(2010) Improving the ﬁsher kernel for large scale image classiﬁcation.
In: European Conference on Com- puter Vision, vol 6314, pp.
143–156 160.
Petrou M., Sevilla P.
(2006) Image Processing: Dealing with Texture, 162.
Picard R.
W., Kabir T., Liu F.
(1993) Real-time recognition with the en- tire brodatz texture database.
In: International Conference on Computer Vision and Pattern Recognition, pp.
638–638 163.
Pichler O., Teuner A., Hosticka B.
(1996) A comparison of texture fea- ture extraction using adaptive Gabor ﬁltering, pyramidal and tree struc- tured wavelet transforms.
Pattern Recognition 29(5):733–742 161.
Peyr´e G.
(2009) Sparse modeling of textures.
Journal of Mathematical vol 1.
Wiley Online Library Imaging and Vision 34(1):17–31 164.
Pietik¨ainen M., Ojala T., Xu Z.
(2000) Rotation invariant texture classi- ﬁcation using feature distributions.
Pattern Recognition 33(1):43–52 165.
Pietik¨ainen M., Hadid A., Zhao G., Ahonen T.
(2011) Computer vision using local binary patterns.
Springer, London, UK 166.
Portilla J., Simoncelli E.
P.
(2000) A parametric texture model based on joint statistics of complex wavelet coefﬁcients.
International journal of computer vision 40(1):49–70 167.
Pun C., Lee M.
(2003) Log-polar wavelet energy signatures for rotation and scale invariant texture classiﬁcation.
IEEE trans pattern analysis and machine intelligence 25(5):590–603 168.
Quan Y., Xu Y., Sun Y., Luo Y.
(2014) Lacunarity analysis on image pat- terns for texture classiﬁcation.
In: International Conference on Computer Vision and Pattern Recognition, pp.
160–167 169.
Raad L., Davy A., Desolneux A., Morel J.
(2017) A survey of exemplar based texture synthesis.
arXiv preprint arXiv:170707184 170.
Randen T., Husoy J.
(1999) Filtering for texture classiﬁcation: A com- parative study.
IEEE Trans Pattern Analysis and Machine Intelligence 21(4):291–310 171.
Rastegari M., Ordonez V., Redmon J., Farhadi A.
(2016) XNORNet: ImageNet classiﬁcation using binary convolutional neural networks.
In: ECCV, pp.
525–542 172.
Raw Food Texture (RFT) (2016) http://www.ivl.disco.
unimib.it/minisites/ rawfoot/download.php 173.
Reed T., Wechsler H.
(1990) Segmentation of textured images and gestalt organization using spatial/spatial-frequency representations.
IEEE Trans Pattern Analysis and Machine Intelligence 12(1):1–12 174.
Reed T.
R., Dubuf J.
H.
(1993) A review of recent texture segmen- tation and feature extraction techniques.
CVGIP: Image understanding 57(3):359–372 175.
Ren J., Jiang X., Yuan J.
(2013) Noise resistant local binary pattern with an embedded error-correction mechanism.
IEEE Transactions on Image Processing 22(10):4049–4060 176.
Robotics Domain Attributes Database (RDAD) (2016) http:// wiki.ros.org/ ipa_texture_classification 177.
Rubner Y., Tomasi C., Guibas L.
(2000) The Earth Mover’s Distance as a metric for image retrieval.
International Journal of Computer Vision 40(2):99–121 178.
Russakovsky O., Deng J., Su H., Krause J., Satheesh S., Ma S., Huang Z., Karpathy A., Khosla A., Bernstein M., et al.
(2015) Imagenet large scale visual recognition challenge.
International Journal of Computer Vision 115(3):211–252 179.
Ryu J., Hong S., Yang H.
(2015) Sorted consecutive local binary pat- tern for texture classiﬁcation.
IEEE Trans Image Processing 24(7):2254– 2265 180.
Sanchez J., Perronnin F., Mensink T., Verbeek J.
(2013) Image classiﬁ- cation with the ﬁsher vector: Theory and practice.
International Journal of Computer Vision 105(3):222–245 181.
Schmid C.
(2001) Constructing models for content based image retrieval.
In: International Conference on Computer Vision and Pattern Recogni- tion, vol 2, pp.
39–45 182.
Schwartz G., Nishino K.
(2015) Automatically discovering local visual material attributes.
In: International Conference on Computer Vision and Pattern Recognition, pp.
3565–3573 183.
Sharan L., Rosenholtz R., Adelson E.
(2009) Material perception: What can you see in a brief glance?
Journal of Vision 9(8):784–784 184.
Sharan L., Liu C., Rosenholtz R., Adelson E.
(2013) Recognizing mate- rials using perceptually inspired features.
International journal of com- puter vision 103(3):348–371 185.
Sharan L., Rosenholtz R., Adelson E.
H.
(2014) Accuracy and speed of material categorization in real-world images.
Journal of vision 14(9):12– 12 186.
Sharif Razavian A., Azizpour H., Sullivan J., Carlsson S.
(2014) CNN features off the shelf: an astounding baseline for recognition.
In: Inter- national Conference on Computer Vision and Pattern Recognition work- shops, pp.
806–813 187.
Sharma G., Jurie F.
(2016) Local higher order statistics (LHS) describing images with statistics of local non-binarized pixel patterns.
Computer Vision and Image Understanding 142:13–22 188.
Shotton J., Winn J., Rother C., Criminisi A.
(2009) Textonboost for im- age understanding: Multiclass object recognition and segmentation by jointly modeling texture, layout, and context.
International Journal of Computer Vision 81(1):2–23 26 Li Liu et al.
189.
Sifre L.
(2014) Rigid motion scattering for image classiﬁcation, 2014.
3367 PhD thesis, ´Ecole Polytechnique 190.
Sifre L., Mallat S.
(2012) Combined scattering for rotation invariant tex- ture analysis.
In: Proc.
European Symp.
Artiﬁcial Neural Networks 191.
Sifre L., Mallat S.
(2013) Rotation, scaling and deformation invariant scattering for texture discrimination.
In: International Conference on Computer Vision and Pattern Recognition, pp.
1233–1240 192.
Simonyan K., Zisserman A.
(2015) Very deep convolutional networks for large-scale image recognition 193.
Simonyan K., Parkhi O., Vedaldi A., Zisserman A.
(2013) Fisher vector faces in the wild.
In: BMVC, vol 2, p.
4 194.
Sivic J., Zisserman A.
(2003) Video google: A text retrieval approach to object matching in videos.
In: International Conference on Computer Vision, vol 2, pp.
1470–1477 195.
Skretting K., Husøy J.
(2006) Texture classiﬁcation using sparse frame- based representations.
EURASIP Journal on Advances in Signal Pro- cessing 2006(1):1–11 196.
Song Y., Zhang F., Li Q., Huang H., O’Donnell L., Cai W.
(2017) Locally transferred ﬁsher vectors for texture classiﬁcation.
In: International Con- ference on Computer Vision and Pattern Recognition, pp.
4912–4920 197.
Sulc M., Matas J.
(2014) Fast features invariant to rotation and scale of texture.
In: European Conference on Computer Vision, pp.
47–62 198.
Szegedy C., Liu W., Jia Y., Sermanet P., Reed S., Anguelov D., Erhan D., Vanhoucke V., Rabinovich A.
(2015) Going deeper with convolutions.
In: International Conference on Computer Vision and Pattern Recogni- tion, pp.
1–9 199.
Tamura H., Mori S., Yamawaki T.
(1978) Textural features correspond- ing to visual perception.
IEEE Transactions on Systems, Man, and Cy- bernetics 8(6):460–473 200.
Tan X., Triggs B.
(2007) Enhanced local texture feature sets for face recognition under difﬁcult lighting conditions.
Analysis and modeling of faces and gestures pp.
168–182 201.
Timofte R., Van Gool L.
(2012) A training-free classiﬁcation framework for textures, writers, and materials.
In: BMVC, vol 13, p.
14 202.
Tuceryan M., Jain A.
(1993) Handbook of pattern recognition and com- puter vision.
chap Texture Analysis, pp.
235–276 203.
Turner M.
(1986) Texture discrimination by gabor functions.
Biological Cybernetics 55(2):71–82 204.
Tuytelaars T., Mikolajczyk K., et al.
(2008) Local invariant feature detec- tors: a survey.
Foundations and trends in computer graphics and vision 3(3):177–280 205.
UBO2014 (2016) http://cg.cs.uni-bonn.de/en/ projects/ btfdbb/download/ubo2014/ 206.
UIUC (2005) http://www-cvr.ai.uiuc.edu/ ponce_grp/ data/ 207.
Ulyanov D., Vedaldi A., Lempitsky V.
(2017) Improved texture net- works: Maximizing quality and diversity in feed forward stylization and texture synthesis.
In: International Conference on Computer Vision and Pattern Recognition (2009) http://www.cfar.umd.edu/˜fer/ 208.
UMD website-texture/texture.html 209.
Valkealahti K., Oja E.
(1998) Reduced multidimensional cooccurrence histograms in texture classiﬁcation.
IEEE Trans Pattern Analysis and Machine Intelligence 20(1):90–94 210.
Van Gemert J., Geusebroek J., Veenman C., Smeulders A.
(2008) Kernel codebooks for scene categorization.
In: European conference on com- puter vision, pp.
696–709 211.
Van Gool L., Dewaele P., Oosterlinck A.
(1985) Texture analysis anno 1983.
Computer Vision, Graphics, and Image Processing 29(3):336–357 212.
Varma M., Garg R.
(2007) Locally invariant fractal features for statistical texture classiﬁcation.
In: International Conference on Computer Vision, pp.
1–8 213.
Varma M., Zisserman A.
(2005) A statistical approach to texture classi- ﬁcation from single images.
Int J Comput Vision 62(1-2):61–81 214.
Varma M., Zisserman A.
(2009) A statistical approach to material clas- siﬁcation using image patches.
IEEE Trans Pattern Anal Mach Intell 31(11):2032–2047 215.
VisTex (1995) http://vismod.media.mit.edu/vismod/ imagery/VisionTexture/ 216.
Wang J., Yang J., Yu K., Lv F., Huang T., Gong Y.
(2010) Locality- constrained linear coding for image classiﬁcation.
In: International Con- ference on Computer Vision and Pattern Recognition, IEEE, pp.
3360– 217.
Wang T., Zhu J., Hiroaki E., Chandraker M., Efros A.
A., Ramamoor- thi R.
(2016) A 4d light ﬁeld dataset and cnn architectures for material recognition.
In: European Conference on Computer Vision, pp.
121–138 218.
Webb A., Copsey K.
(2011) Statistical Pattern Recognition (Third Edi- tion).
Wiley 219.
Wei L., Levoy M.
(2000) Fast texture synthesis using tree-structured vec- tor quantization.
In: Internation Conference on Computer graphics and interactive techniques, pp.
479–488 220.
Weinmann M., Gall J., Klein R.
(2014) Material classiﬁcation based on training data synthesized using a btf database.
In: European Conference on Computer Vision, pp.
156–171 221.
Weszka J.
S., Dyer C.
R., Rosenfeld A.
(1976) A comparative study of texture measures for terrain classiﬁcation.
IEEE Trans Systems, Man, and Cybernetics (4):269–285 222.
Winn J., Criminisi A., Minka T.
(2005) Object categorization by learned universal visual dictionary.
In: International Conference on Computer Vision, vol 2, pp.
1800–1807 223.
Wright J., Yang A., Ganesh A., Sastry S., Ma Y.
(2009) Robust face recognition via sparse representation.
IEEE Trans Pattern Analysis Ma- chine Intelligence 31(2):210–227 224.
Wu Y., Zhu S., Liu X.
(2000) Equivalence of julesz ensembles and FRAME models.
International Journal of Computer Vision 38(3):247– 265 225.
Xie J., Hu W., Zhu S., Wu Y.
(2015) Learning sparse FRAME models for natural image patterns.
International Journal of Computer Vision 114(2- 3):91–112 226.
Xie X., Mirmehdi M.
(2007) TEXEMS: texture exemplars for defect detection on random textured surfaces.
IEEE Trans Pattern Analysis and Machine Intelligence 29(8):1454–1464 227.
Xu J., Boddeti V.
N., Savvides M.
(2017) Local binary convolutional neural networks.
In: International Conference on Computer Vision and Pattern Recognition 228.
Xu Y., Huang S., Ji H., Fermuller C.
(2009) Combining powerful local and global statistics for texture description.
In: Computer Vision and Pattern Recognition, 2009.
CVPR 2009.
IEEE Conference on, pp.
573– 580 229.
Xu Y., Ji H., Ferm¨uller C.
(2009) Viewpoint invariant texture descrip- tion using fractal analysis.
International Journal of Computer Vision 83(1):85–100 230.
Xu Y., Yang X., Ling H., Ji H.
(2010) A new texture descriptor us- ing multifractal analysis in multiorientation wavelet pyramid.
In: Inter- national Conference on Computer Vision and Pattern Recognition, pp.
161–168 231.
Xue J., Zhang H., Dana K., Nishino K.
(2017) Differential angular imag- ing for material recognition.
In: International Conference on Computer Vision and Pattern Recognition 232.
Yang J., Yu K., Gong Y., Huang T.
(2009) Linear spatial pyramid match- ing using sparse coding for image classiﬁcation.
In: International Con- ference on Computer Vision and Pattern Recognition, pp.
1794–1801 233.
Yang L., Jin R., Sukthankar R., Jurie F.
(2008) Unifying discriminative visual codebook generation with classiﬁer training for object category recognition.
In: International Conference on Computer Vision and Pat- tern Recognition, pp.
1–8 234.
Ylioinas J., Hong X., Pietik¨ainen M.
(2013) Constructing local binary pattern statistics by soft voting.
In: Scandinavian Conference on Image Analysis, pp.
119–130 235.
Zhai H., Liu C., Dong H., Ji Y., Guo Y., Gong S.
(2015) Face veriﬁcation across aging based on deep convolutional networks and local binary pat- terns.
In: International Conference on Intelligent Science and Big Data Engineering, pp.
341–350 236.
Zhai Y., Ong Y.-S., Tsang I.
(2014) The emerging “big dimensionality”.
IEEE Computational Intelligence Magazine 9(3):14–26 237.
Zhang H., Jia X., Dana K.
(2017) Deep TEN: Texture encoding network.
In: International Conference on Computer Vision and Pattern Recogni- tion 238.
Zhang J., Tan T.
(2002) Brief review of invariant texture analysis meth- ods.
Pattern recognition 35(3):735–747 239.
Zhang J., Marszalek M., Lazebnik S., Schmid C.
(2007) Local features and kernels for classiﬁcation of texture and object categories: a compre- hensive study.
International Journal of Computer Vision 73(2):213–238 A Survey of Recent Advances in Texture Representation 27 240.
Zhang W., Shan S., Gao W., Chen X., Zhang H.
(2005) Local gabor binary pattern histogram sequence (LGBPHS): A novel nonstatistical model for face representation and recognition.
In: International Confer- ence on Computer Vision, vol 1, pp.
786–791 241.
Zhao G., Pietik¨ainen M.
(2007) Dynamic texture recognition using local binary patterns with an application to facial expressions.
IEEE Trans Pattern Anal Mach Intell 29(6):915–928 242.
Zheng L., Yang Y., Tian Q.
(2017) SIFT meets CNN: A decade survey of instance retrieval.
IEEE Trans Pattern Analysis and Machine Intelli- gence 243.
Zhou B., Lapedriza A., Xiao J., Torralba A., Oliva A.
(2014) Learning deep features for scene recognition using places database.
In: Advances in neural information processing systems, pp.
487–495 244.
Zhu S.
(2003) Statistical modeling and conceptualization of visual pat- terns.
IEEE Trans Pattern Analysis and Machine Intelligence 25(6):691– 712 245.
Zhu S., Wu Y., Mumford D.
(1998) Filters, random ﬁelds and maximum entropy (FRAME): Towards a uniﬁed theory for texture modeling.
Inter- national Journal of Computer Vision 27(2):107–126 246.
Zhu S., Liu X., Wu Y.
(2000) Exploring texture ensembles by efﬁcient markov chain monte carlo-toward a “trichromacy” theory of texture.
IEEE Trans Pattern Analysis and Machine Intelligence 22(6):554–569 247.
Zhu S., Guo C., Wang Y., Xu Z.
(2005) What are textons?
International Journal of Computer Vision 62(1):121–143 28 Li Liu et al.
(cid:5) (cid:63) (cid:5) (cid:5) (cid:63) (cid:63) (cid:63) (cid:63) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) TraditionalBoWbasedTextureRepresentation CNNbasedTextureRepresentation
In recent years, Gaussian process regression has become a prime regression technique (Rasmussen & Williams, 2006).
Roughly, a Gaussian process can be viewed as a suitable2 probability distribution on a set of functions, which we can condition on observations using Bayes’ rule.
The resulting mean function is used for regression.
Additionally, one can also easily sample random functions, so-called realizations, from this distribution.
The strength of Gaussian process regression lies in avoiding overﬁtting while still ﬁnding functions complex enough to describe any behavior present in given observations, even in noisy or unstructured data.
Gaussian processes are usually applied when observations are rare or expensive to produce.
Applications range, among many others, from robotics (Deisenroth et al., 2015), biology (Honkela et al., 2015), global optimization (Osborne et al., 2009), astrophysics (Garnett et al., 2015) to engineering (Thewes et al., 2015).
Incorporating justiﬁed assumptions into the prior helps these applications: the full information content of the scarce ob- servations can be utilized to create a more precise regression model.
Examples of such assumptions are smooth or rough behavior, trends, homogeneous or heterogeneous noise, lo- cal or global behavior, and periodicity (cf.
§4 in (Rasmussen Lange-Hegermann 1Correspondence <markus.lange.hegermann@rwth-aachen.de>.
to: Markus 2Gaussian processes are the maximum entropy prior for ﬁnite mean and variance in the unknown behavior (Jaynes, 1968; Jaynes & Bretthorst, 2003).
& Williams, 2006),(Duvenaud, 2014)).
Such assumptions are usually incorporated in the covariance structure of the Gaussian process.
Even certain physical laws, given by certain linear differ- ential equations, could be incorporated into the covariance structures of Gaussian process priors.
Thereby, despite their random nature, all realizations and the mean function of the posterior strictly adhere to these physical laws3.
For example, (Macˆedo & Castro, 2008) constructed covariance structures for divergence-free and curl-free vector ﬁelds, which (Wahlstr¨om et al., 2013; Solin et al., 2015) used to model electromagnetic phenomena.
A ﬁrst step towards systematizing this construction was achieved in (Jidling et al., 2017).
In certain cases, a parametrization of all solutions for physical laws could be found by a computation that does not necessarily terminate.
Having found a parametrization, one could assume a Gaus- sian process prior for the parametrizing functions and push it forward.
This results in a Gaussian process prior for the solutions of the physical laws.
In Section 3, this paper combines these ideas from (Jidling et al., 2017) with an algorithm which computes this parametrization if it exists, or reports failure if it does not exist.
This algorithm is a homological result in algebraic system theory (cf.
§7.(25) in (Oberst, 1990)).
This paper adds information to Gaussian processes in two ways: (i) restricting to solutions of linear operator matrices by constructing a suitable prior and (ii) conditioning on observations using Bayes’ rule.
Since these two constructions are compatible, we can com- bine strict, global information from equations with noisy, local information from observations.
The author views this combination of techniques from homological algebra and machine learning as the main result of this paper, and 3For notational simplicity, we refrain from using the phrases “almost surely” and “up to equivalence” in this paper.
We say that all realizations of a Gaussian process adhere to such laws if there is an element in its equivalence class w.r.t. almost sure equality that has this property, and similarly for continuity and smoothness.
Algorithmic Linearly Constrained Gaussian Processes the construction of covariance functions satisfying physical laws as a proof of concept.
Example 4.2 shows a typical application.
It constructs a Gaussian process such that all of its realizations satisfy the inhomogeneous Maxwell equations of electromagnetism.
Conditioning this Gaussian process on a single observa- tion of electric current yields, as expected, a magnetic ﬁeld circling around this electric current.
Even though Gaussian processes are a highly precise interpo- lation tool, they lack in two regards: missing extrapolation capabilities and high computation time, cubically in the amount of observations.
These problems have, to a cer- tain degree, been addressed: several fast approximations to Gaussian process regression (Titsias, 2009; Hensman et al., 2013; Wilson et al., 2015a; Izmailov et al., 2017; Dong et al., 2017) and more powerfull covariance structures (Lee et al., 2017; Wilson & Adams, 2013; Wilson et al., 2015b; Calandra et al., 2016) have been proposed.
This paper ad- dresses these two problems from a complementary angle.
The linear differential equations allow to extrapolate and reduce the needed amount of observations, which improves computation time.
2.
Differential Equations and Gaussian Processes This section is mostly expository and summarizes Gaus- sian processes and how differential operators act on them.
Subsection 2.1 summarizes Gaussian process regression.
We then introduce differential (Subsection 2.2) and other operators (Subsection 2.4), and sketch their connection to constructing priors (Subsection 2.3).
2.1. Gaussian processes A Gaussian process g = GP(µ, k) is a distribution on the set of functions Rd → R(cid:96) such that the function values g(x1), .
.
.
, g(xn) at x1, .
.
.
, xn ∈ Rd have a joint Gaussian distribution.
It is speciﬁed by a mean function µ : Rd → R(cid:96) : x (cid:55)→ E(g(x)) and a positive semideﬁnite covariance function k : Rd ⊕ Rd → R(cid:96)×(cid:96)(cid:23)0 : (x, x(cid:48)) (cid:55)→ E((g(x) − µ(x))(g(x(cid:48)) − µ(x(cid:48)))T ) .
Assume the regression model yi = g(xi) and condition on n observations {(xi, yi) ∈ R1×d ⊕ R1×(cid:96) | i = 1, .
.
.
, n} .
Denote by k(x, X) ∈ R(cid:96)×(cid:96)n resp.
k(X, X) ∈ R(cid:96)n×(cid:96)n(cid:23)0 the (covariance) matrices obtained by concatenating the matri- ces k(x, xj) resp.
the positive semideﬁnite block partitioned matrix with blocks k(xi, xj).
Write µ(X) ∈ R(cid:96)×n for the matrix obtained by concatenating the vectors µ(xi) and y ∈ R1×(cid:96)n for the row vector obtained by concatenating the rows yi.
The posterior GP(cid:0) x (cid:55)→ µ(X) + (y − µ(X))k(X, X)−1k(x, X)T , (x, x(cid:48)) (cid:55)→ k(x, x(cid:48)) − k(x, X)k(X, X)−1k(x(cid:48), X)T(cid:1) , is a Gaussian process and its mean function x (cid:55)→ µ(X) + (y − µ(X))k(X, X)−1k(x, X)T is used as regression model.
2.2. Differential equations From now on, let R = R[∂x1, .
.
.
, ∂xd ] be the polynomial ring in the partial differential operators.
This ring models lin- ear (partial) differential equations with constant coefﬁcients, as it acts on the vector space F = C∞(Rd, R) of smooth functions, where ∂xi acts by partial derivative w.r.t. xi.
The set of realizations of a Gaussian process with squared ex- ponential covariance function is dense in F (cf.
Thm.
12, Prop.
42 in (Simon-Gabriel & Sch¨olkopf, 2016)).
Roughly speaking, Gaussian processes are the linear objects among stochastic processes.
Hence, it is not surprising to ﬁnd a rich interplay of Gaussian processes and linear differential equations.
The class of Gaussian processes is closed under matrices B ∈ R(cid:96)×(cid:96)(cid:48)(cid:48) of linear differential operators with constant coefﬁcients.
Let g = GP(µ, k) be a Gaussian process with realizations in the space F (cid:96)(cid:48)(cid:48) of vectors with functions in F as entries.
Deﬁne the Gaussian process B∗g as the Gaussian process induces by the pushforward measure under B of the Gaussian measure induced by g.
It holds that B∗g = GP(Bµ(x), Bk(x, x(cid:48))(B(cid:48))T ) , (1) where B(cid:48) denotes the operation of the operators in B on functions with argument x(cid:48) ∈ Rd (cf.
Thm.
9 in (Bertinet & Agnan, 2004)).
The covariance matrices for such Gaussian processes as in (1) are often singular.
This is to be expected, as B∗g is rarely dense in F (cid:96).
For numerical stability, we tacitely assume the model yi = g(xi) + ε for small Gaussian white noise term ε and adopt k by adding var(ε) to k(xi, xi) for observations xi.
Example 2.1. Let g = GP(0, k(x, x(cid:48))) be a scalar univari- ate Gaussian process with differentiable realizations.
Then, the Gaussian process of derivatives of functions is given by (cid:2) ∂ (cid:19) ∂x∂x(cid:48) k(x, x(cid:48)) One can interpret this Gaussian process(cid:2) ∂ (cid:3) (cid:3) ∗ g = GP ∗ g as taking derivatives as measurement data and producing a regression model of derivatives.
∂x ∂x (cid:18) ∂2 0, 2.3. Parametrizations 2.4. Further operator rings Algorithmic Linearly Constrained Gaussian Processes We say that a Gaussian process is in a function space, if its realizations are contained in said space.
For A ∈ R(cid:96)(cid:48)×(cid:96) deﬁne the solution set solF (A) := {f ∈ F (cid:96) | Af = 0} .
Such solutions sets and Gaussian processes are connected.
Lemma 2.2. Let g = GP(µ, k) be a Gaussian process in F (cid:96)×1.
Then g is also a Gaussian process in solF (A) for A ∈ R(cid:96)(cid:48)×(cid:96) if and only if µ ∈ solF (A) and A∗(g − µ) is the constant zero process.
Proof.
Assume that g is a Gaussian process in solF (A).
Then, as the mean function is a realization, µ ∈ solF (A).
Furthermore, for ˜g := (g − µ) = GP(0, k) we have that all realizations are annihilated by A, and hence A∗˜g is the constant zero process.
Conversely, assume that µ ∈ solF (A) and A∗(g − µ) is the constant zero process.
This implies 0 = A∗(g − µ) = A∗g − A∗µ = A∗g, i.e. all realizations of g become zero after a pushforward by A.
In particular, all realizations of g are contained in solF (A).
Our goal is to construct Gaussian processes with realizations dense in the solution set solF (A) of an operator matrix A ∈ R(cid:96)(cid:48)×(cid:96).
The following remark, implicit in (Jidling et al., 2017), is a ﬁrst step towards an answer.
Remark 2.3. Let A ∈ R(cid:96)(cid:48)×(cid:96) and B ∈ R(cid:96)×(cid:96)(cid:48)(cid:48) Let g = GP(0, k) be a Gaussian process in F (cid:96)(cid:48)(cid:48) set of realizations of B∗g is contained in solF (A).
with AB = 0.
.
Then, the This follows easily from Lemma 2.2, as A∗(B∗g) = (AB)∗g = 0∗g = 0.
We call B ∈ R(cid:96)×(cid:96)(cid:48)(cid:48) a parametrization of solF (A) if solF (A) = BF (cid:96)(cid:48)(cid:48) .
Parametrizations yield the denseness of the realizations of a Gaussian process B∗g in solF (A).
Proposition 2.4. Let B ∈ R(cid:96)×(cid:96)(cid:48)(cid:48) be a parametrization of solF (A) for A ∈ R(cid:96)(cid:48)×(cid:96).
Let g = GP(0, k) be a Gaussian process dense in F (cid:96)(cid:48)(cid:48) .
Then, the set of realizations of B∗g is dense in solF (A).
This proposition is a consequence of partial derivatives be- ing bounded, and hence continuous, when F is equipped with the Fr´echet topology generated by the family of semi- norms (cid:107)f(cid:107)a,b := sup i∈Zd≥0 |i|≤a sup z∈[−b,b]d | ∂ ∂zi f (z)| for a, b ∈ Z≥0 (cf.
§10 in (Treves, 1967)).
Now, the con- tinuous surjective map induced by B maps a dense set to a dense set.
The theory presented for differential equations with constant coefﬁcients also holds for other rings R of linear operators and solution spaces F.
The following three operator rings are prominent examples.
The polynomial ring R = R[x1, .
.
.
, xd] models polynomial equations when it acts on the set F of smooth functions deﬁned on a (Zariski-)open set in Rd. To model ordinary linear differential equations with ratio- nal4 coefﬁcients consider the Weyl algebra R = R(t)(cid:104)∂t(cid:105), with the non-commutative relation ∂tt = t∂t + 1 represent- ing the product rule of differentiation.
Here, we consider solutions in the set F of smooth functions deﬁned on a co-ﬁnite set.
The polynomial ring R = R[σx1, .
.
.
, σxd ] models linear shift equations with constant coefﬁcients when it acts on the set F = RZd≥0 of d-dimensional sequences by translation of the arguments.
3.
Computing parametrizations By the last section, constructing a parametrization B of solF (A) yields a Gaussian process dense in the solution set solF (A) of an operator matrix A ∈ R(cid:96)(cid:48)×(cid:96).
Subsection 3.1 gives necessary and sufﬁcient conditions for a parametriza- tion to exist and Subsection 3.2 describes their computation.
It turns out that these computations are purely algebraic over R.
3.1. Existence of parametrizations It turns out that we can decide whether a parametrization exists purely algebraically, with operations over R that do not involve F.
By r-ker(A) we denote the right kernel of A ∈ R(cid:96)(cid:48)×(cid:96), i.e. r-ker(A) = {m ∈ R(cid:96)×1 | Am = 0}.
By l-ker(A) we denote the left kernel of A, i.e. l-ker(A) = {m ∈ R1×(cid:96)(cid:48) | mA = 0}.
Abusing notation, denote any matrix as left resp.
right kernel if its rows resp.
columns generate the kernel as an R-module.
Theorem 3.1. Let A ∈ R(cid:96)(cid:48)×(cid:96).
Deﬁne matrices B = r-ker(A) and A(cid:48) = l-ker(B).
Then solF (A(cid:48)) is the largest subset of solF (A) that is parametrizable and B parametrizes solF (A(cid:48)).
In constrast to vector spaces, the left kernel of the right kernel of A is not necessarily A (up to an equivalence) in case of operator rings.
For example, the solution set solF (A(cid:48)) is the subset of controllable behaviors in solF (A).
4No major changes for polynomial, holonomic, or meromor- phic coefﬁcients.
Algorithmic Linearly Constrained Gaussian Processes Corollary 3.2. In Theorem 3.1, solF (A) is parametrizable if and only if the rows of A and A(cid:48) generate the same row- module.
Since AB = 0, this is the case if all rows of A(cid:48) are contained in the row module generated by the rows of A.
In this case, solF (A) is parametrized by B.
Furthermore, a Gaussian process g with realizations dense in F (cid:96)(cid:48)(cid:48) leads to a Gaussian process B∗g with realizations dense in solF (A).
For a proof of this theorem and its corollary see Thm.
2 in (Zerz et al., 2010), (cf.
also Thm.
3, Alg.
1, and Lemma 1.2.3 in (Zerz, 2000) or §7.(24) in (Oberst, 1990)) and for addi- tional characterizations, generalizations, and proofs using more homological machinery see (Quadrat, 2013; 2010b; Barakat, 2010; Seiler & Zerz, 2010; Chyzak et al., 2005; Robertz, 2015) and references therein.
The approach assigns a prior to the parametrising functions and pushes this prior forward to a prior of the solution set solF (A).
The paramerization is not canonical, and hence different parametrizations might lead to different priors.
This did not lead to practical problems, yet.
3.2. Algorithms Summarizing Theorem 3.1 and Corollary 3.2 algorithmi- cally, we need to compute right kernels (of A), compute left kernels (of B), and decide whether rows (of A(cid:48)) are contained in a row module (generated by the rows of A).
All these computations are an application of Gr¨obner basis algorithms.
In the recent decades, Gr¨obner bases algorithms have be- come one of the core algorithms of computeralgebra, with manifold applications among others in in geometry, system theory, natural sciences, and automatic theorem proving.
Generalizing the Gaussian algorithm, reduced Gr¨obner bases generalize the reduced echolon form to systems of linear operator equations.
In particular, using them, one can compute all solutions in R (not in F) of the homogeneous system and compute, if it exists, a particular solution in R (not in F) for an inhomogeneous system.
Solving homo- geneous systems is the same as computing its right resp.
left kernel or ﬁnding all relations (the generalization of lin- ear dependencies) between columns resp.
rows of a matrix.
Solving inhomogeneous equations decides whether an ele- ment is contained in a module.
Alternatively, the uniqueness of reduced Gr¨obner bases also decides submodule equality.
A formal description of Gr¨obner bases would exceed the scope of this note.
Instead, we refer to the excellent litera- ture (Sturmfels, 2005; Eisenbud, 1995; Adams & Loustau- nau, 1994; Greuel & Pﬁster, 2002; Gerdt, 2005; Buchberger, 2006) and explicitely show in the next section how to use Gr¨obner bases in computer algebra systems.
They are imple- mented in various computer algebra systems, e.g., Singular (Decker et al., 2016) and Macaulay2 (Grayson & Stillman) are two well-known open source implementations.
3.3. Hyperparameters Many covariance functions5 incorporate hyperparameters and advanced methods speciﬁcally add more hyperparam- eters to Gaussian processes, see e.g. (Snelson et al., 2003; Calandra et al., 2016; Wilson & Adams, 2013), for addi- tional ﬂexibility.
The approach in this paper is the opposite.
We restrict the Gaussian process prior, speciﬁcally to solutions of an oper- ator matrix, guarding against overﬁtting.
The prior of the parametrizing functions can, and usually does, still contain hyperparameters.
These can be determined by maximizing the likelihood, as usual.
Many important applications contain unknown parameters in the equations.
Such parameters can also be estimated by the likelihood, when conditioning on data.
For ordinary differential equations, the solution set of an operator matrix is a direct sum of parametrizable functions and a ﬁnite dimensional set of functions, both with con- stant resp.
variable coefﬁcients, due to the Smith form resp.
Jacobson form.
In many cases, in particular the case of con- stant coefﬁcients, the solution set of the ﬁnite dimensional summand can easily be computed.
This paper also allows to compute with the parametrizable summand of the solution set and estimate parameters and hyperparameters of both summands together.
4.
Examples Example 4.1. We reproduce the well-known fact that divergence-free ﬁelds can be parametrized by the curl oper- ator.
This has been used in connection with Gaussian pro- cesses to model electric and magnetic phenomena (Macˆedo & Castro, 2008; Wahlstr¨om et al., 2013; Solin et al., 2015).
The same algebraic computation also constructs a prior for tangent ﬁelds of a sphere.
The computer algebra system Macaulay2 (Grayson & Still- man) performs the Gr¨obner basis computations.
Let R be the polynomial ring in three indeterminates, which we can both interpret as the polynomial ring Q[∂1, ∂2, ∂3] in the differential operators resp.
the polynomial ring Q[x1, x2, x3] in the indeterminates.
i1 : o1 = R o1 : PolynomialRing R=QQ[d1,d2,d3] 5Sometimes even the mean function contains hyperparameters.
These additional hyperparameters are usually not very expressive, compared to the non-parametric Gaussian process model.
Algorithmic Linearly Constrained Gaussian Processes and consider the matrix A = (cid:2)∂1 ∂2 ∂3 (cid:3) representing the divergence resp.
the normals of circles centered around the origin.
i2 : o2 = | d1 d2 d3 | A=matrix{{d1,d2,d3}} o2 : Matrix R <--- R The right kernel of A is given by the operator B representing the curl resp.
tangent spaces of circles centered around the origin.
i3 : o3 = 1 | -d2 0 B = generators kernel A 1 | d1 -d3 0 1 | 0 -d3 | d2 d1 | Matrix R <--- R o3 : Since the right kernel A(cid:48) of B is again the A, the matrix B is really parametrization matrix of the solutions of A.
i4 : transpose B o4 = | d1 d2 d3 | A1 = transpose generators kernel Matrix R <--- R o4 : (As kernel in Macaulay2 yields right kernels, we compute the left kernel by transposition.) We construct a prior for tangent ﬁelds on the sphere by assuming equal covariance functions k for 3 uncorrelated parametrizing functions.
A mean ﬁeld is demonstrated in Figure 1 and the covariance function for the tangent ﬁeld is k(x1, y1, z1, x2, y2, z2)· −y1y2 − z1z2 x1y2 x1z2 y1x2 −x1x2 − z1z2 y1z2 z1x2 z1y2 −x1x2 − y1y2  .
Example 4.2. Maxwell’s equations of electromagnetism uses curl and divergence operators as building blocks.
It is a well-known result that the solutions of the inhomogeneous Maxwell equations are parametrized by the electric and mag- netic potentials.
We verify this and use the parametrization to construct a Gaussian process, such that its realizations adhere to Maxwell’s equations.
We condition this prior on a single observation of ﬂowing electric current, which leads to the magnetic ﬁeld circling around the current.
All compu- tations have been performed in Maple using the Involutive package (Blinkov et al., 2003).
The homogenous Mawell equations are given by the opera- Figure 1.
Taking the squared exponential covariance function for k in Example 4.1 yields the above smooth mean tangent ﬁeld on the sphere after conditioning at 4 evenly distributed points on the equator with two opposite tangent vectors pointing north and south each.
The two visible of these four vectors are displayed signiﬁcantly bigger.
tor matrix Mh :=  ∂z −∂y −∂t ∂x −∂z ∂x −∂t ∂y ∂y ∂t −∂x ∂x ∂z −∂t −∂y ∂z ∂t ∂y −∂z ∂x  ∂t ∂z ∂y −∂x applied to three components of the electric ﬁeld and three components of the magnetic (pseudo) ﬁeld.
We have set all constants to 1.
The right kernel of Mh is zero, in particular no parametrization exists.
The inhomogeneous Maxwell equations with three addi- tional components of the electric current and one additional component of electric ﬂux are given by the 8 × 10 operator .
Using Gr¨obner bases, one matrix Mi := computes the right kernel 04×4−I4 (cid:20) (cid:21) Mh   ∂x ∂y ∂z −∂t∂x −∂t∂y −∂t∂z ∂2 x + ∂2 y + ∂2 ∂t −∂z ∂y z − ∂2 ∂2 y + ∂2 −∂y ∂x −∂z ∂x ∂t∂x ∂t ∂z −∂x −∂y ∂x −∂z ∂y ∂t∂y ∂2 x + ∂2 z − ∂2 ∂t −∂y ∂x −∂z ∂x −∂z ∂y y − ∂2 ∂2 x + ∂2 ∂t∂z of Mi and veriﬁes that it is a parametrization of the set of Algorithmic Linearly Constrained Gaussian Processes > T := E[3]; (cid:20)0 (cid:2)1 (cid:21) t3 Dt 0(cid:3) solutions of the inhomogeneous Maxwell equations.
We assume squared exponential covariance functions k := exp(cid:0) − 1 (cid:0)(x1 − x2)2 + (y1 − y2)2 +(z1 − z2)2 + (t1 − t2)2(cid:1)(cid:1) and a zero mean function for four uncorrelated parametris- ing functions (electric and magnetic potential).
The result- ing covariance matrix can be found in Appendix A and for a demonstration see Figure 2.
Figure 2.
We condition the prior in Example 4.2 on an electric current in z-direction and zero electric ﬂux at the origin x = y = z = t = 0.
The diagram shows the mean posterior magnetic ﬁeld in the (z, t) = (0, 0)-plane.
As expected, it circles around the point with electric current.
This mean ﬁeld has closed form (cid:0)x2 + y2 − 4(cid:1) exp(cid:0)− 1 (cid:0)x2 + y2(cid:1)(cid:1) · (cid:20)−y (cid:21) 10 Example 4.3. We consider the one-dimensional Weyl algebra R = R(t)(cid:104)∂t(cid:105).
It allows a stronger notion of a basis than a Gr¨obner basis: the Jacobson form (Jacobson, 1943).
It is similar to the Smith normal form for PID’s, in that multiplying with invertible matrices on both sides yields a diagonal matrix.
We use the Maple packages Janet and OreModules (Blinkov et al., 2003; Chyzak et al., 2007).
> with( Janet ): with( OreModules ): > Alg := DefineOreAlgebra( diff=[ Dt, t ], polynom=[ t ] ): consider system We dt x(t) = t3u(t) from Example 1.5.7 in (Quadrat, 2010a).
The Jacobson form is just(cid:2)1 0(cid:3).
time-varying control the > E := ElementaryDivisors( [ diff(x(t),t)-tˆ3*u(t) ], [ t ], [ x, u ] ): > T1 := E[1]; (cid:2)− 1 t3 (cid:3) > Mult( T1, [[ Dt, -tˆ3 ]], T, Alg ); In particular, after the base change with the matrix T , the system is free on one generator and parametrizable by the matrix (cid:20)0 (cid:21) The original system is thus parametrizable by (cid:20)0 (cid:21) (cid:20) 1 (cid:21) t3 ∂t B = T · For a parametrizing functions with squared exponential co- 2 (t1 − t2)2) and a variance functions k(t1, t2) = exp(− 1 (cid:19) (cid:18) zero mean function, the covariance function for (x, u) is (cid:34) 1 t1−t2 (cid:35) t2−t1 t3 (t2−t1−1)(t1−t2−1) exp t3 t3 2t3 (t1 − t2)2 − 1 There are no exceptional points in the domain of the trans- formed system, but T removes {0} from the domain of the original system.
We restrict ourselves to the set of smooth functions C∞(R>0, R) deﬁned on positive real numbers.
For a demonstration of these priors see Figures 3 and 4.
x(t) u(t) Figure 3.
The state function x(t) of the system in Example 4.3 can be inﬂuenced by assigning an input function u(t).
For example 10 , .
.
.
, 5} setting x(1) = 0 and u(t) = 1 leads to the above posterior mean.
This model yields x(5) ≈ t4+1 for t ∈ {1, 11 10 , 12 1.436537, close to(cid:82) 5 t3 t4+1 dt ≈ 1.436551.
A.
Covariance matrices The covariance matrix for the 3-dimensional electric ﬁeld E, the 3-dimensional magnetic (pseudo) ﬁeld B, the 3- dimensional electric current C and 1-dimensional electric ﬂux F in Maxwell’s equations as constructed in Example 4.2 is KE,E KE,B KE,C KE,F KB,E KB,B KB,C KB,F KC,E KC,B KC,C KC,F KF,E KF,B KF,C KF,F  K := 4k · Algorithmic Linearly Constrained Gaussian Processes x(t) u(t) -1 -2 Figure 4.
We prescribe a desired behavior for the state x(t) in Example 4.3 and let the Gaussian process construct a suitable input u(t).
Starting with x(1) = 1 we give u(t) one time step to control x(t) to zero, e.g., by setting x(t) = 0 for t ∈ { 20 10 , .
.
.
, 5}.
10 , 21 for matrices KE,E = (2 − (t1 − t2)2)I3 − vT v KB,B = (2 − vvT )I3 − vT v KC,C = (4 + 3(t1 − t2)2 − (vvT ))vT v +((vvT − (t1 − t2)2)2 − 3(t1 − t2)2 − 7vvT + 10)I3 KF,F = (t1 − t2)2(vvT − 3) + (vvT )2 − 11vvT + 18 KB,E = (t1 − t2)(v ∧ v) KC,E = (t1 − t2)((2 + vvT − (t1 − t2)2)I3 − 2vT v) KF,E = ((t1 − t2)2 + vvT − 6)v KC,B = (vvT − (t1 − t2)2 − 4)(v ∧ v) KF,B =(cid:2)0 0 0(cid:3)  using for KF,C = −(t1 − t2)(vvT + (t1 − t2)2 − 8)v the 3 × 3 identity matrix, z1 − z2 −(y1 − y2) := −(z1 − z2) x1 − x2 y2 − y2 y1 − y2 −(x1 − x2) .
(cid:2)x1 − x2 (cid:3), z1 − z2 and v ∧ v I3 := Acknowledgments The authors thanks M.
Barakat, S.
Gutsche, C.
Kaus, D.
Moser, S.
Posur, and O.
Wittich for discussions concern- ing this paper, W.
Plesken, A.
Quadrat, D.
Robertz, and E.
Zerz for introducing him to the algebraic background of this paper, S.
Thewes for introducing him to the stochastic background of this paper, and the authors of (Jidling et al., 2017) for providing the starting point of this work.
References Adams, William W.
and Loustaunau, Philippe.
An introduc- tion to Gr¨obner bases.
Graduate Studies in Mathematics.
American Mathematical Society, 1994.
Barakat, Mohamed.
Purity ﬁltration and the ﬁne structure of autonomy.
In Proceedings of the 19th International Symposium on Mathematical Theory of Networks and Sys- tems - MTNS 2010, pp.
1657–1661, Budapest, Hungary, 2010.
Bertinet, A.
and Agnan, Thomas C.
Reproducing Kernel Hilbert Spaces in Probability and Statistics.
Kluwer Academic Publishers, 2004.
Blinkov, Yuri A., Cid, Carlos F., Gerdt, Vladimir P., Plesken, Wilhelm, and Robertz, Daniel.
The MAPLE Package JANET: I.
Polynomial Systems.
II.
Linear Partial Dif- In Proceedings 6th International ferential Equations.
Workshop on Computer Algebra in Scientiﬁc Computing, pp.
31–40 and 41–54, 2003.
(http://www.mathb.
rwth-aachen.de/Janet).
Buchberger, Bruno.
An algorithm for ﬁnding the basis elements of the residue class ring of a zero dimensional polynomial ideal.
J.
Symbolic Comput., 41(3-4):475– 511, 2006.
Translated from the 1965 German original by Michael P.
Abramson.
Calandra, Roberto, Peters, Jan, Rasmussen, Carl E., and Deisenroth, Marc P.
Manifold Gaussian processes for regression.
In International Joint Conference on Neural Networks, pp.
3338–3345, 2016.
doi: 10.1109/IJCNN.
2016.7727626.
Chyzak, Fr´ed´eric, Quadrat, Alban, and Robertz, Daniel.
Effective algorithms for parametrizing linear control sys- tems over Ore algebras.
Appl.
Algebra Engrg.
Comm.
Comput., 16(5):319–376, 2005.
Chyzak, Fr´ed´eric, Quadrat, Alban, and Robertz, Daniel.
OreModules: a symbolic package for the study of mul- In Applications of time tidimensional linear systems.
delay systems, volume 352 of Lecture Notes in Con- trol and Inform.
Sci., pp.
233–264.
Springer, Berlin, 2007.
(http://www.mathb.rwth-aachen.de/ OreModules).
Decker, Wolfram, Greuel, Gert-Martin, Pﬁster, Gerhard, and Sch¨onemann, Hans.
SINGULAR 4-1-0 — A computer algebra system for polynomial computations.
http: //www.singular.uni-kl.de, 2016.
Deisenroth, Marc Peter, Fox, Dieter, and Rasmussen, Carl Edward.
Gaussian processes for data-efﬁcient learn- ing in robotics and control.
IEEE Trans.
Pattern Anal.
Mach.
Intell., 37(2):408–423, 2015.
Dong, Kun, Eriksson, David, Nickisch, Hannes, Bindel, David, and Wilson, Andrew Gordon.
Scalable log de- terminants for gaussian process kernel learning.
2017.
(arXiv:1711.03481).
Duvenaud, David.
Automatic Model Construction with Gaussian Processes.
PhD thesis, University of Cam- bridge, 2014.
Algorithmic Linearly Constrained Gaussian Processes Eisenbud, David.
Commutative Algebra with a View Toward Algebraic Geometry, volume 150 of Graduate Texts in Mathematics.
Springer-Verlag, 1995.
Garnett, Roman, Ho, Shirley, and Schneider, Jeff G.
Find- ing galaxies in the shadows of quasars with Gaussian processes.
In Bach, Francis R.
and Blei, David M.
(eds.), ICML, volume 37 of JMLR Workshop and Conference Proceedings, pp.
1025–1033.
JMLR.org, 2015.
Gerdt, Vladimir P.
Involutive algorithms for computing Gr¨obner bases.
In Computational commutative and non- commutative algebraic geometry, volume 196 of NATO Sci.
Ser.
III Comput.
Syst.
Sci., pp.
199–225.
2005.
Grayson, Daniel R.
and Stillman, Michael E.
Macaulay2, a software system for research in algebraic geometry.
http://www.math.uiuc.edu/Macaulay2/.
Greuel, G.
and Pﬁster, G.
A Singular introduction to com- mutative algebra.
Springer-Verlag, 2002.
With contri- butions by Olaf Bachmann, Christoph Lossen and Hans Sch¨onemann.
Hensman, James, Fusi, Nicol´o, and Lawrence, Neil D.
Gaus- sian processes for big data.
In Proceedings of the Twenty- Ninth Conference on Uncertainty in Artiﬁcial Intelligence, 2013.
Honkela, Antti, Peltonen, Jaakko, Topa, Hande, Chara- pitsa, Iryna, Matarese, Filomena, Grote, Korbinian, Stun- nenberg, Hendrik G., Reid, George, Lawrence, Neil D., and Rattray, Magnus.
Genome-wide modeling of transcription kinetics reveals patterns of rna production delays.
Proceedings of the National Academy of Sci- ences, 112(42):13115–13120, 2015.
doi: 10.1073/pnas.
1420404112.
Lee, Jaehoon, Bahri, Yasaman, Novak, Roman, Schoen- holz, Samuel S., Pennington, Jeffrey, and Sohl-Dickstein, Jascha.
Deep neural networks as Gaussian processes, 2017.
(arXiv:1711.00165).
Macˆedo, Ives and Castro, Rener.
Learning divergence-free and curl-free vector ﬁelds with matrix-valued kernels.
Instituto Nacional de Matematica Pura e Aplicada, Brasil, Tech.
Rep, 2008.
Oberst, Ulrich.
Multidimensional constant linear systems.
Acta Appl.
Math., 20(1-2):1–175, 1990.
Osborne, Michael A., Garnett, Roman, and Roberts, Stephen J.
Gaussian processes for global optimization.
In 3rd international conference on learning and intelligent optimization (LION3), pp.
1–15, 2009.
Quadrat, Alban.
An introduction to constructive al- In Journ´ees gebraic analysis and its applications.
Nationales de Calcul Formel, volume 1 of Les cours du CIRM, pp.
279–469.
CIRM, Luminy, 2010a.
(http://ccirm.cedram.org/ccirm-bin/ fitem?id=CCIRM_2010__1_2_281_0).
Quadrat, Alban.
Syst`emes et Structures – Une approche de la th´eorie math´ematique des syst`emes par l’analyse alg´ebrique constructive.
April 2010b.
Habilitation thesis.
Quadrat, Alban.
Grade ﬁltration of linear functional systems.
Acta Appl.
Math., 127:27–86, 2013.
doi: 10.1007/s10440-012-9791-2.
Rasmussen, Carl Edward and Williams, Christopher K.
I.
Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning).
The MIT Press, 2006.
Izmailov, Pavel, Novikov, Alexander, and Kropotov, Dmitry.
Scalable Gaussian processes with billions of inducing inputs via tensor train decomposition, 2017.
(arXiv:math/1710.07324).
Robertz, Daniel.
Recent progress in an algebraic anal- ysis approach to linear systems.
Multidimensional Syst.
Signal Process., 26(2):349–388, April 2015.
doi: 10.1007/s11045-014-0280-9.
Jacobson, N.
The Theory of Rings.
Mathematical surveys and monographs.
American Mathematical Society, 1943.
Jaynes, Edwin T.
Prior probabilities.
IEEE Transactions on systems science and cybernetics, 4(3):227–241, 1968.
Jaynes, Edwin T.
and Bretthorst, G.
Larry.
Probability Theory: The Logic of Science.
Cambridge University Press, 2003.
Jidling, Carl, Wahlstr¨om, Niklas, Wills, Adrian, and Sch¨on, Thomas B.
Linearly constrained Gaussian processes.
2017.
(arXiv:1703.00787).
Seiler, Werner M.
and Zerz, Eva.
The inverse syzygy prob- lem in algebraic systems theory.
PAMM, 10(1):633–634, 2010.
Simon-Gabriel, C.-J.
and Sch¨olkopf, B.
Kernel distribution embeddings: Universal kernels, characteristic kernels and kernel metrics on distributions.
Technical report, 2016.
(arXiv:1604.05251).
Snelson, Edward, Rasmussen, Carl Edward, and Ghahra- mani, Zoubin.
Warped gaussian processes.
In Thrun, Sebastian, Saul, Lawrence K., and Schlkopf, Bernhard (eds.), NIPS, pp.
337–344.
MIT Press, 2003.
Algorithmic Linearly Constrained Gaussian Processes Solin, Arno, Kok, Manon, Wahlstr¨om, Niklas, Sch¨on, Thomas B., and S¨arkk¨a, Simo.
Modeling and interpola- tion of the ambient magnetic ﬁeld by Gaussian processes.
2015.
(arXiv:1509.04634).
Sturmfels, Bernd.
What is...
a Gr¨obner basis?
Notices of the AMS, 52(10):2–3, 2005.
Thewes, Silja, Lange-Hegermann, Markus, Reuber, Christoph, and Beck, Ralf.
Advanced Gaussian Process Modeling Techniques.
In Design of Experiments (DoE) in Powertrain Development.
Expert, 2015.
Titsias, Michalis K.
Variational learning of inducing vari- ables in sparse Gaussian processes.
In Artiﬁcial Intelli- gence and Statistics 12, pp.
567–574, 2009.
Treves, F.
Topological Vector Spaces, Distributions and Kernels.
Dover books on mathematics.
Academic Press, 1967.
Wahlstr¨om, Niklas, Kok, Manon, Sch¨on, Thomas B., and Gustafsson, Fredrik.
Modeling magnetic ﬁelds using Gaussian processes.
In in Proceedings of the 38th Inter- national Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2013.
Wilson, Andrew G.
and Adams, Ryan Prescott.
Gaussian process kernels for pattern discovery and extrapolation.
In ICML (3), volume 28 of JMLR Workshop and Conference Proceedings, pp.
1067–1075.
JMLR.org, 2013.
Wilson, Andrew G., Dann, Christoph, and Nickisch, Hannes.
Thoughts on massively scalable Gaussian processes.
2015a.
(arXiv:1511.01870).
Wilson, Andrew G., Hu, Zhiting, Salakhutdinov, Rus- lan, and Xing, Eric P.
Deep kernel learning.
2015b.
arXiv:1511.02222).
Zerz, Eva.
Topics in multidimensional linear systems theory, volume 256 of Lecture Notes in Control and Information Sciences.
London, 2000.
Zerz, Eva, Seiler, Werner M, and Hausdorf, Marcus.
On the inverse syzygy problem.
Communications in Algebra, 38 (6):2037–2047, 2010.

Learning to rank is an important research topic in infor- mation retrieval and data mining, which aims to learn a ranking model to produce a query-specﬁc ranking list.
The ranking model establishes a relationship between each pair of data samples by combining the corresponding features in an optimal way [1].
A score is then assigned to each pair to evaluate its relevance forming a global ranking list across all pairs.
The success of learning to rank solutions has brought a wide spectrum of applications, including online advertising [2], natural language processing [3] and multimedia retrieval [4].
Learning appropriate data representation and a suitable scoring function are two vital steps in the ranking problem.
Traditionally, a feature mapping models the data distribu- tion in a latent space to match the relevance relationship, while the scoring function is used to quantify the relevance measure [1]; however, the ranking problem in the real world emerges from multiple facets and data patterns are mined from diverse domains.
For example, universities are posi- tioned differently based on numerous factors and weights used for quality evaluation by different ranking agencies.
Therefore, a global agreement across sources and domains should be achieved while still maintaining a high ranking performance.
Multi-view learning has received a wide attention with a special focus on subspace learning [5], [6] and co-training [7], and few attempts have been made in ranking problems [8].
It introduces a new paradigm to jointly model and combine information encoded in multiple views to enhance the learning performance.
Speciﬁcally, subspace learning ﬁnds a common space from different input modalities using an optimization criterion.
Canonical Correlation Analysis (CCA) [9], [10] is one of the prevailing unsupervised method used to measure a cross-view correlation.
By contrast, Multi- view Discriminant Analysis (MvDA) [6] is a supervised learning technique seeking the most discriminant features across views by maximizing the between-class scatter while minimizing the within-class scatter in the underlying feature space.
Furthermore, a generalized multi-view embedding method [5] was proposed using a graph embedding frame- work for numerous unsupervised and supervised learning techniques with extension to nonlinear transforms includ- ing (approximate) kernel mappings [11], [12] and neural networks [5], [13].
A nonparametric version of [5] was also proposed in [14].
On the other hand, co-training [7] was introduced to maximize the mutual agreement between two distinct views, and can be easily extended to multiple inputs by subsequently training over all pairs of views.
A solution to the learning to rank problem was provided by minimizing the pairwise ranking difference using the same co-training mechanism [8].
Although there are several applications that could ben- eﬁt from multi-view learning to rank approach, the topic has still been insufﬁciently studied up to date [15].
Ranking of multi-facet objects is generally performed using com- posite indicators.
The usefulness of a composite indicator depends upon the selected functional form and the weights associated with the component facets.
Existing solutions for university ranking are an example of using the subjective weights in the method of composite indicators.
However, the functional form and its assigned weights are difﬁcult to deﬁne.
Consequently, there is a high disparity in the eval- uation metric between agencies, and the produced ranking lists usually cause dissension in academic institutes.
How- ever, one observation is that, the indicators from different agencies may partially overlap and have a high correlation between each other.
We present an example in Fig.
1 to show that, several attributes in the THE dataset [16], including teaching, research, student staff ratio and student number are highly correlated with all of the attributes in the ARWU dataset [17].
Therefore, the motivation of this paper is to ﬁnd a composite ranking by exploiting the correlation between individual rankings.
Earlier success in multi-view subspace learning pro- vides a promising way for composite ranking.
Concatenat- ing multiple views into a single input overlooks possible view discrepancy and does not fully exploit their mutual 2 agreement within a single optimization process.
The rest of the paper is organized as follows.
In Section 2, we describe the related work close to our proposed methods.
The proposed methods are introduced in Section 3.
In Section 4, we present quantitative results to show the effectiveness of the proposed methods.
Finally, Section 5 concludes the paper.
2 RELATED WORK 2.1 Learning to rank Learning to rank aims to optimize the combination of data representation for ranking problems [18].
It has been widely used in a number of applications, including image retrieval and ranking [4], [19], image quality ratings [20], online advertising [2], and text summarization [8].
Solutions to this problem can be decomposed into several key compo- nents, including the input feature, the output vector and the scoring function.
The framework is developed by training the scoring function from the input feature to the output ranking list, and then, scoring the ranking of new data.
Traditional methods also include engineering the feature using the PageRank model [21], for example, to optimally combine them for obtaining the output.
Later, research was focused on discriminatively training the scoring function to improve the ranking outputs.
The ranking methods can be organized in three categories for the scoring function: the pointwise approach, the pairwise apporach, and the listwise approach.
We consider the pairwise approach in this paper and review the related methods as follows.
A preference net- work is developed in [22] to evaluate the pairwise order between two documents.
The network learns the preference function directly to the binary ranking output without using an additional scoring function.
RankNet [23] deﬁnes the cross-entropy loss and learns a neural network to model the ranking.
Assuming the scoring function to be linear [24], the ranking problem can be transformed to a binary classiﬁca- tion problem, and therefore, many classiﬁers are available to be applied for ranking document pairs.
RankBoost [25] adopts Adaboost algorithm [26], which iteratively focuses on the classﬁcation errors between each pair of documents, and subsequently, improves the overall output.
Ranking SVM [27] applies SVM to perform pairwise classiﬁcation.
GBRank is a ranking method based on Gradient Boost Tree [28].
Semi-supervised multi-view ranking (SmVR) [8] follows the co-training scheme to rank pairs of samples.
Moreover, recent efforts focus on using the evaluation metric to guide the gradient with respect to the ranking pair during training.
These studies include AdaRank [29], which optimizes the ranking errors rather than the classiﬁcation error in an adaptive way, and LambdaRank [30].
However, all of these methods above consider the case of single view inputs, while multi-view learning to rank is overlooked.
2.1.1 Bipartite ranking The pairwise approach of the ranking methods serves as the basis of our ranking method, and therefore, reviewed explicitly in this section.
Suppose that the training data Fig.
1: The correlation matrix between the measurements of Times Higher Education (THE) and Academic Ranking of World Universities (ARWU) rankings.
The data is extracted and aligned based on the performance of the common universities in 2015 between the two ranking agencies.
The reddish color indicates high correlation, while the matrix elements with low correlation are represented in bluish colors.
agreement in ranking.
Our goal is to study beyond the direct multi-view subspace learning for ranking.
This paper offers a multi-objective solution to ranking by capturing relevant information of feature mapping from within each view as well as across views.
Moreover, we propose an end-to-end method to optimize the trade-off between view- speciﬁc ranking and a discriminant combination of multi- view ranking.
To this end, we can improve cross-view ranking performance while maintaining individual ranking objectives.
Intermediate feature representation in the neural net- work are exploited in our ranking solutions.
Speciﬁcally, the ﬁrst contribution is to provide two closely related methods by adopting an autoencoder-like network.
We ﬁrst train a network to learn view-speciﬁc feature mappings, and then maximize their correlation with the intermediate represen- tations using either an unsupervised or discriminant pro- jection to a common latent space.
A stochastic optimization method is introduced to ﬁt the correlation criterion.
Both the autoencoding sub-network per view with a reconstruc- tion objective and feedforward sub-networks with a joint correlation-based objective are iteratively optimized in the entire network.
The projected feature representations in the common subspace are then combined and used to learn for the ranking function.
The second contribution (graphically described in Fig.
2) is an end-to-end multi-view learning to rank solution.
A sub-network for each view is trained with its own ranking objective.
Then, features from intermediate layers of the sub- networks are combined after a discriminant mapping to a common space, and training towards the global ranking objective.
As a result, a network assembly is developed to enhance the joint ranking with mimimum view-speciﬁc ranking loss, so that we can achieve the maximum view i , yq i )}, where q ∈ is organized in query-sample pairs {(xq i ∈ Rd is the d-dimensional feature vector {1, 2, .
.
.
, Q}, xq i ∈ {0, 1} is the for the pair of query q, the i-th sample, yq relevance score, and the number of query-speciﬁc samples is Nq. We perform the pairwise transformation before the relevance prediction of each query-sample pair, so that only the samples that belong to the same query are evaluated [24].
The modeled probability between each pair in this paper is deﬁned as pq i (φ) = 1 + exp(φ(xi) − φ(xq)) k, y(cid:48) k) = (xq − xi, yq where φ : x → R is the linear scoring function as φ(x) = a(cid:62)x, which maps the input feature vectors to the scores.
Due to its linearity, we can transform the feature vectors and relevance score into (x(cid:48) i ).
In case of the ordered list (r) as the raw input, each data sample xi paired with its query xq is investigated, and their raw orders (ri, rq) are transformed as yq i = 0, else if ri > rq.
In pairwise ranking, the relevance yq i = 1, if the query and sample are relevant, and yq i = 0, otherwise.
k) becomes the new feature vector as the input data for nonlinear transforms and sub- space learning.
Therefore, the probability can be rewritten as The feature difference (x(cid:48) i = 1, if ri < rq; yq k, y(cid:48) pk(φ) = 1 + exp(−φ(x(cid:48) k)) 1 + exp(−a(cid:62)x(cid:48) k) (1) The objective to make the right order of ranking can then be formulated as the cross entropy loss such that, (cid:0)yq Q(cid:88) K(cid:88) q=1 Nq(cid:88) (cid:0)y(cid:48) i=1 (cid:96)Rank = arg min = arg min i log pq i ) + (1 − yq k log pk) + (1 − y(cid:48) i ) log pq i )(cid:1) k) log pk)(cid:1), (2) k=1 which is proved in [23] that it is an upper bound of the pairwise 0-1 loss function and optimized using gradient de- scent.
The logistic regression or softmax function in neural networks can be used to learn the scoring function.
2.2 Multi-view deep learning Multi-view learning considers enhancing the feature dis- criminability by taking inputs from diverse sources.
One important approach to follow is subspace learning, which is traced back to CCA [31], [32] between two input domains, and its multi-view extension, which has been studied in [33], [34], [35].
This approach can also be generalized using a higher-order correlation [35].
The main idea behind these techniques is to project the data representations in the two domains to a common subspace optimizing their mutual correlation.
Subspace learning with supervision has also been extensively studied.
Multi-view Discriminant Analysis [6] performs the dimensionality reducation of features from multiple views exploiting the class information.
Recently, these methods were generalized in the same framework [5], [36], which accommodates multiple views, supervision and nonlinearity.
Co-training [7] ﬁrst trains two seperate regressors and then, iteratively maximizes their agreements.
Deep learning, which exploits the nonlinear transform of the raw feature space, has also been studied in the multi- view scenario.
The multi-modal deep autoencoder [37] was proposed by taking nonlinear representations of a pair of views to learn their common characteristics.
Deep CCA [13] is another two-view method which maximizes the pair- wise correlation using neural networks.
Thereafter, a two- view correlated autoencoder was developed [38], [39] with objectives to correlate the view pairs but also reconstruct the individual view in the same network.
Multi-view Deep Network [40] was also proposed as an extension of MvDA [6].
It optimizes the ratio trace of the graph embedding [41] to avoid the complexity of solutions without a closed form [42].
In this paper, however, we show that the trace ratio optimization can be solved efﬁciently in the updates of the multi-view networks.
Deep Multi-view Canonical Correla- tion Analysis (DMvCCA) and Deep Multi-view Modular Discriminant Analysis (DMvMDA) [5] are closely related to our work, and hence, they are described in the following sections.
2.2.1 Deep Multi-view Canonical Correlation Analysis (DMvCCA) The idea behind DMvCCA [5] is to ﬁnd a common subspace using a set of linear transforms W to project nonlinearly mapped input samples Zv from the vth view where the correlation is maximized.
Speciﬁcally, it aims to maximize Tr (cid:18) V(cid:80) V(cid:80) Tr(cid:0) V(cid:80) i=1 i=1 j=1 j(cid:54)=i W(cid:62) (cid:19) (cid:1) W(cid:62) i Zi L Z(cid:62) j Wj i Zi L Z(cid:62) i Wi (3) JDMvCCA = arg max Wv,v=1,...,V where the matrix L = I − 1 N ee(cid:62) centralizes the input data matrix of each view v, and e is a vector of ones, .
By deﬁning the cross-view covariance matrix between views i and j ˜Zi ˜Zj, where ˜Zv, v = 1, .
.
.
, V , is the centered as Σij = 1 view, the data projection matrix W, which has the column vector of Wv in the vth view, can be obtained by solving the generalized eigenvalue problem  0 Σ12 ··· Σ1V Σ21 0 ··· Σ2V ...
...
ΣV 1 ΣV 2 ··· 0 .
.
.
...
 W = λ  Σ11 0 ··· 0 0 Σ22 ··· 0 ...
...
...
0 ··· ΣV V .
.
.
 W.
(4) It shows that the solution to this problem is derived with the maximal inter-view covariances and the minimal intra-view covariances.
2.2.2 Deep Multi-view Modular Discriminant Analysis (DMvMDA) DMvMDA [5] is the neural network-based multi-view solu- tion of LDA which maximizes the ratio of the determinant of the between-class scatter matrix of all view pairs to that of the within-class scatter matrix.
Mathematically, it is written as the projection matrix of the DMvMDA and is derived by optimizing the objective function Tr(cid:0) V(cid:80) V(cid:80) Tr(cid:0) V(cid:80) i=1 j=1 i=1 (cid:1) (cid:1) , (5) W(cid:62) i ZiLBZ(cid:62) j Wj W(cid:62) i ZiLW Z(cid:62) i Wi JDMvMDA = arg max Wv,v=1,...,V where the between-class Laplacian matrix is LB = 2 ep e(cid:62) p − 1 NpNq ep e(cid:62) q ).
and the within-class Laplacian matrix is p=1 N 2 q=1 C(cid:88) C(cid:88) LW = I − C(cid:88) c=1 3 MODEL FORMULATION We ﬁrst introduce the formulation of MvCCAE and MvM- DAE, and then the extension as multi-view subspace learn- ing to rank.
Finally, the end-to-end ranking method is de- scribed.
Here, the output of each sub-network Fv is denoted by Zv = Fv(Xv).
Then, we have V(cid:88) V(cid:88) V(cid:88) Wi W(cid:62) ∂f ∂Zi j Zj L, j(cid:54)=i j=1 and (7) i=1 ∂g ∂Zi i=1 Wi W(cid:62) i Zi L.
(8) By using (7) and (8) and following the quotient rule, we derive the stochastic optimization of MvCCAE to be ∂JMvCCAE (cid:19) − f ∂g ∂Zv (cid:18) g2 − ∂ ∂Zv ∂f ∂Zv V(cid:88) (cid:96)AE(Xv;Gv(Fv(·))).
(9) The gradient to compute the autoencoding loss (cid:96)AE is derived from the view-speciﬁc sub-networks Fv and Gv. The sub-network Fv is optimized with ∂Zv to obtain the ∂Fv output Zv, while the gradient of Gv network with respect to its parameters can be obtained using the chain rule from ∂Gv(Xv) ∂Zv (cid:62).
ecec Nc ∂Zv 3.1 Multi-view Canonically Correlated Auto-Encoder (MvCCAE) 3.2 Multi-view Modularly Discriminant Auto-Encoder (MvMDAE) In contrast to DMvCCA and DMvMDA, where the non- linear correlation between multiple views is optimized, we propose a multi-objective solution by maximizing the between-view correlation while minimizing the reconstruc- tion error from each view source.
Given the data matrix X = {X1, X2, .
.
.
, XV } of V views, the encoding network F and the decoding network G, and the projection matrix W, the objective of MvCCAE is formulated as follows, (cid:16) JMvCCAE = arg max J (cid:48) (cid:96)AE where we introduce the new objective J (cid:48) DMvCCA, and the loss function of the vth autoencoder is (cid:96)AE(Xv;Gv(Fv(·))) = l (cid:107)∇XvF l v(Xv)(cid:107)2, with the L2 regularization at the lth intermediate layer of the vth view denoted by Zl v(Xv).
Here, α and ρ are controlling parameters for the trade-off between the terms.
(cid:107)Xv − Gv(Fv(Xv))(cid:107)2 + ρ(cid:80) Xv;Gv(Fv(·)) DMvCCA − α v = F l V(cid:88) , (6) (cid:17) Similar to MvCCAE, the objective of MvMDAE is to opti- mize the view-speciﬁc reconstruction error and the cross- view correlation as follows, JMvMDAE = arg maxJ (cid:48) Xv;Gv(Fv(·)) DMvMDA − α V(cid:88) (cid:16) (cid:17) (cid:96)AE (10) 3.2.1 Optimization The detailed optimization is derived by replacing the lapla- cian matrix in MvCCAE with LB and LW .
We let g = Tr i Wi i Zi LB Z(cid:62) j Wj (cid:33) (cid:33) i=1 j(cid:54)=i j=1 W(cid:62) W(cid:62) (cid:32) V(cid:88) V(cid:88) (cid:32) V(cid:88) i Zi LW Z(cid:62) V(cid:88) V(cid:88) V(cid:88) Wi W(cid:62) j(cid:54)=i j=1 i=1 i=1 Wi W(cid:62) ∂g ∂Zi i=1 i Zi LW .
j Zj LB, (11) (12) f = Tr and 3.1.1 Optimization Then, we have Following the objective of DMvCCA [5], we aim to directly optimize the trace ratio in (3) and let ∂f ∂Zi W(cid:62) i Zi L Z(cid:62) j Wj and f = Tr and (cid:32) V(cid:88) V(cid:88) (cid:32) V(cid:88) j(cid:54)=i j=1 i=1 i=1 (cid:33) (cid:33) g = Tr W(cid:62) i Zi L Z(cid:62) i Wi The stochastic optimization of MvMDAE can be derived by using (11), (12) and applying the quotient rule as follows, ∂JMvMDAE ∂Zv (cid:18) g2 − ∂ ∂Zv (cid:19) − f ∂g ∂Zv (cid:96)AE(Xv,Gv(Fv(·))).
(13) ∂f ∂Zv V(cid:88) The gradient of the objective can be calculated using the chain rule, and the stochastic gradient descent (SGD) is used with mini-batches for optimization.
3.3 Multi-view Subspace Learning to Rank (MvSL2R) Multi-view subspace learning to rank is formulated based on the fact that the projected feature in the common sub- space can be used to train a scoring function for ranking.
We generate the training data from the intersection of ranking samples between views to have the same samples but various representations from different view origins.
The overall ranking agreement is made by calculating the average voting from the intersected ranking orders as V(cid:88) v=1 r = rv.
(14) By performing the pairwise transform in section 2.1.1 over the ranking data, we have the input X = {X1, X2, .
.
.
, XV } of V views and the cross-view relevance scores y obtained from the average ranking orders r.
The proposed ranking method consists of feature mapping into a common sub- space, training a logistic regressor as the scoring function, and predicting the relevance of new sample pairs using the probability function pv(Xv) = 1 + exp(−a(cid:62)W(cid:62) v Fv(Xv)) (15) where Wv is the data projection matrix of the vth view, and a is the weight from the logistic regressor described in (1).
We summarize these steps in the algorithm below.
Algorithm 1: Multi-view Subspace Learning to Rank.
1 Function MvSL2R (X, Y, k); Input : The feature vectors of V views X = {X1, X2, .
.
.
, XV }, the relevance y, and the dimensionality in the subspace k.
Output: The predicted relevance probabilities p = {p1, p2, .
.
.
, pV } of the new data.
2 Perform the nonlinear transformation to obtain the representation Zv in the vth autoencoder.
3 Perform the subspace learning by optimizing (6) or (10) to obtain the projection matrix W = [W1W2 .
.
.
WV ](cid:62).
4 Train a logistic regressor (1) as the scoring function to obtain the weight matrix a.
5 Predict the new sample pairs for their relevance probabilities using (15) with the trained sub-networks F and G, and the obtained weights W and a.
3.4 Deep Multi-view Discriminant Ranking (DMvDR) Multi-view Subspace Learning to Rank provides a promis- ing method with MvCCAE and MvMDAE.
However, it does not have a direct connection to ranking.
Continuing the idea of multi-objective optimization, we propose to optimize the view-speciﬁc and the joint ranking together in the single network as shown in Fig.
2.
Taking university ranking as an example, various ranking lists are generated from different agencies, and each agency uses a different set of attributes to represent the universities.
In training, given the inputs X = {X1, X2, .
.
.
, XV }, the cross entropy loss (2) is optimized with the view-speciﬁc relevance y and the joint view relevance y.
Based on their evaluation metrics, the attributes Xv, where v = 1, .
.
.
, V , are trained through the view-speciﬁc sub-network Fv. The nonlinear representations Zv = Fv(Xv), v = 1, .
.
.
, V , are the inputs of the joint network H as W(cid:62) v Zv, v = 1, .
.
.
, V , after the mappings to generate the joint university ranking list.
Each of them is also the input to the view-speciﬁc network Gv, which minimizes its distance to the original ranking rv.
We similarly exploit the effectiveness of intermediate layers Zv in between the view-speciﬁc sub-networks Fv and Gv, but towards the ranking loss for DMvDR.
The detailed procedure of this method is described below.
The gradient of each view-speciﬁc sub-network Gv is calculated from the output y with respect to its parameters.
Since the loss passes from each view-speciﬁc Fv to Gv sub- network, the gradient can be calculated independently with respect to the output of each view-speciﬁc Fv sub-network } Then, the gradient of ∂yv as ∂y ∂Gv with respect to its network weights can be determined through backpropagation [43].
All sub-networks contain several layers with Sigmoid functions.
The fused sub-network H is updated with the gradient of the ranking loss from the cross-view relevance scores y.
Similar to the generation of training data in MvSL2R, we ﬁnd the intersection of the ranking data with different representations or measurements from various sources, and perform the pairwise transform to have the sample pairs as the input X and y from the cross-view ranking orders r in (14).
As a result, the input S to the fused sub-network H is the concatenation of the nonlinear mapping from the V view-speciﬁc networks Fv as 1 Z1 W(cid:62) 2 Z2 .
.
.
W(cid:62) ∂Z = { ∂y1 S = [W(cid:62) V ZV ](cid:62).
(16) , ∂yv ∂Zv , .
.
.
, ∂yV ∂ZV ∂Z1 During testing, we can distinguish two possible scenarios: (a) If the samples are aligned and all presented from each view, the results from nonlinear mappings are combined in the same manner as the training phase to generate a fused ranking list p at the end of H sub-network; and (b) If there are missing samples or completely unaligned in the test data, S = W(cid:62) v Zv for the vth view.
The resulting view- speciﬁc prediction pv still maintains the cross-view agree- ment which is ranked from the trained joint network.
The gradient of ∂y ∂H can be easily calculated afterwards using the SGD.
∂S and ∂y Joint ranking is achieved using a multi-view subspace embedding layer.
Similar to MvMDAE, we take the map- pings from the outputs from the sub-networks Fv. The 6 Fig.
2: System diagram of the Deep Multi-view Discriminant Ranking (DMvDR).
First, the features X = {X1, X2, .
.
.
, XV } are extracted for data representations in different views and fed through the individual sub-network Fv to obtain the nonlinear representation Zv of the vth view.
The results are then passed through two pipelines of networks.
One line goes to the projection W, which maps all Zv to the common subspace, and their concatenation is trained to optimize the fused ranking loss with the fused sub-network H.
The other line connects Zv to the sub-network Gv,∀v = 1, .
.
.
, V for the optimization of the vth ranking loss.
gradient of multi-view subspace embedding (MvSE) in the trace ratio form is calculated by combining (11) and (12): ∂JMvSE ∂Zv g2 ∂f ∂Zv − f ∂g ∂Zv (17) (cid:18) (cid:19) The embedding layer is important as its gradient is forward passed to the fused sub-network H.
Meanwhile, it is back- ward propagated in the layers of Fv to reach the input Xv. In turn, the parameters in Gv are also affected by the outputs of Fv sub-networks.
The update of the view-speciﬁc Fv depends on the view- speciﬁc ranking output y and the cross-view relevance y as it is a common sub-network in both pipelines of networks.
Through backpropagation, the v-th sub-networks Fv and Gv are optimized consecutively with respect to the gradient ∂y .
Meanwhile, the training error with respect to the ∂Xv fused ranking y is passed through multi-view subspace embedding (MvSE) from S in (16) as the input to the fused sub-network H.
The resulting gradient of each sub-network Fv is given by ∂JDMvDR (cid:96)Rank(Xv, yv;Gv(Fv(·))) − α ∂Zv V(cid:88) (cid:96)Rank(S, y;H(·)), ∂Zv ∂JMvSE ∂Zv − β ∂Zv (18) where α and β are the scaling factors controlling the magnitude of the ranking loss.
Similar to the other sub- networks, the gradients with respect to their parameters can be obtained by following the chain rule.
The update of the entire network of DMvDR can be summarized using the SGD with mini-batches.
The parameters of the sub-network are denoted by θ = {θF1 , θFv , .
.
.
, θFV , θG1, θGv , .
.
.
, θGV , θH}.
A gradient de- ∂θJDMvDR, where η is the learning scent step is ∆θ = −η ∂ rate.
The gradient update step at time t can be written down with the chain rule collectively: , .
.
.
, ∆θtFV , .
.
.
, ∆θtGV , ∆θtH} ∆θt ={∆θtF1 ∆θtG1 = − ∂(cid:96)rank ∆θtGv ∂y ∆θtH = − ∂(cid:96)rank ∂y ∂JMvSE ∂Zv − ∂(cid:96)rank ∂y ∆θtFv , ∆θtF2 , ∆θtG2 · ∂y ∂Gv · ∂y ∂H · ∂Zv ∂Fv · ∂y ∂S · ∂y ∂Zv · ∂Zv ∂Fv − ∂(cid:96)rank ∂y · ∂S ∂Zv · ∂Zv ∂Fv (19) We generate the training data using the pairwise transform presented in Section 2.1.1. During testing, the test samples can also be transformed into pairs to evaluate the relative relevance of each sample to its query.
The raw ranking data can also be fed into the trained model to predict their overall ranking positions.
Fused Ranking lossXRankingloss1Ranking loss 2Ranking loss VFGHABCDEABCDEABCDEABCDEABCDEABCDEABCDEABCDE1XVX2WVWW2W1WZ1Z2ZV4 EXPERIMENTS In this section, we evaluate the performance of the proposed multi-view learning to rank methods in three challeng- ing problems: university ranking, multi-linguistic ranking and image data ranking.
The proposed methods are also compared to the related subspace learning and co-training methods.
The subspace learning methods follow the steps proposed in Section 3.3 for ranking.
We compare the perfor- mance of the following methods in the experiments: • Best Single View: a method which shows the best performance of Ranking SVM [27] over the individ- ual views.
Feature Concat: a method which concatenate the fea- tures of the common samples for training a Ranking SVM [27].
• LMvCCA [5]: a linear multi-view CCA method.
• LMvMDA [5]: a linear supervised method for multi- view subspace learning.
• MvDA [6]: another linear supervised method for multi-view subspace learning.
It differs from the above in that the view difference is not encoded in this method.
• SmVR [8]: a semi-supervised method that seeks a global agreement in ranking.
It belongs to the cat- egory of co-training.
We develop the complete data in the following experiments for training so that its comparison with the subspace learning methods is fair.
Therefore, SmVR becomes a supervised method in this paper.
• DMvCCA [5]: a nonlinear extension of LMvCCA • DMvMDA [5]: a nonlinear extension of LMvMDA using neural networks.
using neural networks.
• MvCCAE: the ﬁrst proposed multi-view subspace learning to rank method proposed in the paper.
• MvMDAE: the supervised multi-view subspace learning to rank method proposed in the paper.
• DMvDR: the end-to-end multi-view learning to rank method proposed in the paper.
We present the quantitative results using several evaluation metrics including the Mean Average Precision (MAP), clas- siﬁcation accuracy and Kendal’s tau.
The Average Precision (AP) measures the relevance of all query and sample pairs with respect to the same query, while the MAP score calcu- lates the mean AP across all queries [44].
After performing pairwise transform on the ranking data, the relevance pre- diction can be considered as a binary classiﬁcation problem, and therefore the classiﬁcation is utilized for evaluation.
Kendal’s tau measures the ordinal association between two lists of samples.
We also present the experimental results graphically, and the following measures are used.
The Mean Average Preci- sion (MAP) score, which is the average precision at the ranks where recall changes, is illustrated on the 11-point interpo- lated precision-recall curves (PR curve) to show the ranking performance.
Also, the ROC curve provides a graphical representation of the binary classiﬁcation performance.
It shows the true positive rates against the false positive rate at different thresholds.
The correlation plots show linear correlation coefﬁcients between two ranking lists.
4.1 University Ranking The university ranking dataset available in Kaggle.com [45] collects the world ranking data from three rating agencies, including the Times Higher Education (THE) World Univer- sity Ranking, the Academic Ranking of World Universities (ARWU), and the Center for World University Rankings (CWUR).
Despite political and controversial inﬂuences, they are widely considered as authorities for university ranking.
The measurements are used as the feature vectors after feature preprocessings, which includes feature standardiza- tion and removal of categorical variables and groundtruth indicators including the ranking orders, university name, location, year and total scores.
The 271 common universities from 2012 to 2014 are considered for training.
After the pair- wise transform in each year, 36542 samples are generated as the training data.
The entire data in 2015 is considered for testing.
The data distribution (after binary transform) of the 196 common universities in 2015 is shown in Fig.
3.
(a) Raw data distribu- tion.
(b) Projected data distri- bution.
Fig.
3: The left plot shows the data distribution by con- catenating the measurements as features of the common universities from 3 different agencies in 2015.
The right plot shows the concatenated and projected features using MvMDAE for the same universities.
We can make several observations from the data distri- bution in Fig.
3.
Firstly, the pairwise transform is applied on the university ranking data, which equally assigns the original ranking data to two classes.
Then, the dimension- ality of the data is reduced to 2-dimensional using PCA in order to display it on the plots of Fig.
3.
The data is then labelled with two colors red and green indicating the relevance between samples.
We can notice a high overlap between the two classes in the case of raw data (left plot of Fig.
3), while the data on the right is clearly better separated after the projection using the proposed MvMDAE.
This shows the discrimination power of the proposed supervised embedding method.
Furthermore, a rank correlation matrix of plots is pre- sented in Fig.
4 with correlations among pairs of ranking lists from the views 1-3 and the predicted list denoted by ’Fused’.
Histograms of the ranking data are shown along the matrix diagonal, while scatter plots of data pairs appear off diagonal.
The slopes of the least-squares reference lines in the scatter plots are equal to the displayed correlation coefﬁcients.
The fused ranking list is produced by the pro- posed DMvDR, and the results are also generated from the common universities in 2015.
We ﬁrst take a closer look at 01018 TABLE 1: Average Prediction Results (%) on 3 University Ranking Datasets in 2015.
Methods Best Single View Feature Concat LMvCCA [5] LMvMDA [5] MvDA [6] SmVR [8] DMvCCA [5] DMvMDA [5] MvCCAE (ours) MvMDAE (ours) DMvDR (ours) Kendal’s tau Accuracy 65.38 35.10 86.04 87.00 85.81 80.75 70.07 70.81 75.94 81.04 89.28 94.49 94.97 94.34 93.20 94.75 94.01 94.85 95.30 Fig.
4: Rank correlation matrix for views 1-3 and the fused view.
the correlations between the views 1-3.
The correlation co- efﬁcients are generally low, with the highest (0.81) between view 1 and 3, while the others are around 0.70.
In contrast, the fused rank has a high correlation to each view.
The scatter plots and the reference lines are well aligned, and the correlation coefﬁcients are all above 0.80, demonstrating that the proposed DMvDR effectively exploits the global agreement with all view.
Finally, the average prediction results over 3 different university datasets of the proposed and competing methods are reported in Table 1.
Due to the misalignment of ranking data in 2015 across datasets, we make the ranking prediction based on each view input, which is further elaborated in the Section 3.4. We observe that Ranking SVM [27] on the single feature or its concatenation performs poorly compared to the other methods.
This shows that when the data is heterogeneous, simply combining the features cannot enhance joint ranking.
Kendal’s tau from the linear subspace learning methods are comparatively higher than their nonlinear counterparts.
This is due to the fact that the nonlinear methods aim to maximize to the correlation in the embedding space, while the scoring function is not optimized for ranking.
In contrast, DMvDR optimizes the entire ranking process, which is conﬁrmed with the highest ranking and classiﬁcation performance.
4.2 Multi-lingual Ranking The Multi-lingual Ranking is performed on Reuters RCV1/RCV2 Multi-lingual, Multi-view Text Categorization Test collection [3].
We use Reuters to indicate this dataset in later paragraphs.
It is a large collection of documents with news ariticles written in ﬁve languages, and grouped into 6 categories by topic.
The bag of words (BOW) based on a TF-IDF weighting method [44] is used to represent the documents.
The vocabulary has a size of approximately 15000 on average and is very sparse.
the 5 views are numbered as follows: • View 1: original English documents; • View 2: English documents translated to French; • View 3: English documents translated to German; • View 4: English documents translated to Italian; • View 5: English documents translated to Spanish.
Due to its high dimensionality, the BOW representation of each document is projected using a sparse SVD to a 50- dimensional compact feature vector.
We randomly select 40 samples from each category in each view as training data.
The training data composed of 28680 samples is generated between pairs of English documents based on the pairwise transform in Section 2.1.1, and the translations to other languages are used for augmenting the views.
We select another 360 samples from 6 categories and create a test dataset of 64620 document pairs.
If considering the ranking function linear as proved in [24], we make document pairs comparable and balance them by assigning some of the data to the other class with the opposite sign of the feature vectors, so that the number of samples is equally distributed in both classes.
We ﬁrst analyze the PR and ROC curves in Fig.
5.
Since we have all translations of the English documents, each sample is well aligned in all views and, therefore we perform joint learning and prediction in all multi-lingual ex- periments.
The experiments start with 2 views with English and its translation to French, and then the views are aug- mented with the documents of other languages.
Subspace ranking methods are trained by embedding with increasing number of views, while SmVR as a co-training takes two views at a time, and the average performance of all pairs is reported.
The proposed methods with two competing ones are included in the plots in Fig.
5.
The proposed DMvDR clearly performs the best across all views as can be seen in the PR and ROC plots in Fig.
5.
SmVR is the second best with a lower precision and less area under curve compared to DMvDR.
Among the remaining three methods, DMvMDA performs favorably in the PR curves but not as well in the ROC plots.
The results are comparatively consistent across all views.
We consider the English documents and their transla- tions to the other 4 languages in our experiment.
Speciﬁcally, We can observe the quantitative MAP and accuracy results in Table 2.
It shows that the linear methods together 9 (a) PR curve on Reuters.
Fig.
5: The PR and ROC curves with 2-5 views applied to Reuters dataset.
(b) ROC curve on Reuters.
TABLE 2: Quantitative Results (%) on the Reuter Dataset.
Methods Feature Concat LMvCCA [5] LMvMDA [5] MvDA [6] SmVR [8] DMvCCA [5] DMvMDA [5] MvCCAE (ours) MvMDAE (ours) DMvDR (ours) 2 views 3 views 4 views 5 views MAP@100 Accuracy MAP@100 Accuracy MAP@100 Accuracy MAP@ 100 Accuracy 58.87 59.10 59.09 55.95 78.37 53.87 60.08 48.75 62.63 80.01 70.41 70.20 70.16 69.03 71.44 67.41 71.40 66.43 74.20 72.68 56.97 62.40 58.81 55.42 78.24 42.68 63.12 49.10 63.02 79.34 70.10 72.01 71.94 67.57 71.15 62.02 70.93 62.90 71.04 72.23 57.59 54.41 61.54 55.64 78.66 54.51 61.55 60.70 60.74 80.32 69.88 66.61 72.45 68.64 71.37 68.03 72.12 71.86 72.60 73.07 58.46 60.41 59.28 58.93 79.36 57.27 62.52 48.80 62.74 81.64 69.97 72.62 72.07 68.46 71.64 65.00 70.78 63.05 71.20 72.39 00.51Recall0.50.550.60.650.70.750.80.850.90.95Precision2 viewsSmVRDMvMDAMvCCAEMvMDAEDMvDR00.51Recall0.550.60.650.70.750.80.850.90.95Precision3 views00.51Recall0.550.60.650.70.750.80.850.90.95Precision4 views00.51Recall0.50.550.60.650.70.750.80.850.90.95Precision5 views00.51FP Rate00.10.20.30.40.50.60.70.80.91TP Rate2 viewsSmVRDMvMDAMvCCAEMvMDAEDMvDR00.51FP Rate00.10.20.30.40.50.60.70.80.91TP Rate3 views00.51FP Rate00.20.40.60.81TP Rate4 views00.51FP Rate00.20.40.60.81TP Rate5 viewswith the feature concatenation have similar results which are generally inferior to the nonlinear methods in classiﬁca- tion.
Note also that nonlinear subspace learning methods cannot provide any superior MAP scores, which can be explained by the fact that the embedding is only intended to construct a discrimative feature space for classifying the pairs of data.
We can also observe the MAP scores and accuracies are stable across views.
This can be interpreted as the global ranking agreement can be reached to a certain level when all languages correspond to each other.
It is again conﬁrmed that the end-to-end solution consistently provides the highest scores, while SvMR is a few percent- ages behind.
When the features from different views follow a similar data distribution, the co-training method performs well and competes with the proposed DMvDR.
4.3 Image Data Ranking Image data ranking is a problem to evaluate the relevance between two images represented by different types of fea- tures.
We adopt the Animal With Attributes (AWA) dataset [46] for this problem due to its diversity of animal appear- ance and large number of classes.
The dataset is composed of 50 animal classes with a total of 30475 images, and 85 animal attributes.
We follow the feature generation in [5] to adopt 3 feature types forming the views: Label Encoding: Image Feature by VGG-16 pre-trained model: a 1000- dimensional feature vector is produced from each image by resizing them to 224 × 224 and taken from the outputs of the f c8 layer with a 16-layer VGGNet [47].
• Class 100-dimensional Word2Vector is extracted from each class label.
Then, we can map the visual feature of each image to the text feature space by using a ridge regressor with a similar setting as in [5] to genenate another set of textual feature, with connection to the visual world.
The text embedding space is constructed by training a skip-gram [48] model on the entire English Wikipedia articles, including 2.9 billion words.
• Attibute Encoding: an 85-dimensional feature ve- tor can be produced with a similar idea as above.
Since each class of animals contains some typical patterns of the attribute, a 50 × 85 lookup table can be constructed to connect the classes and attributes [49], [50].
Then, we map each image feature to the attribute space to produce the mid-level feature.
We generate the image ranking data as follows.
From the 50 classes of animal images, we ﬁnd 400 pairs of images with 200 in-class pairs and 200 out-of-class image pairs from each class.
We then end up with 20000 training data pairs.
Similarly, we will have 20000 test data pairs.
We select 40 images from each class used for training data and a separate set of 40 samples as test data.
Another 10 images are used as queries: 5 of them are associated with the in-class images as positive sample pairs and 5 as negative sample pairs.
For the negative sample pairs, we randomly select 40 classes from 49 remaining animal classes at a time, and one image per class is associated with each query image under study.
10 TABLE 3: Quantitative Results (%) on the AWA Dataset.
Fig.
6: PR and ROC curves on AWA.
Methods Feature Concat LMvCCA [5] LMvMDA [5] MvDA [6] SmVR [8] DMvCCA [5] DMvMDA [5] MvCCAE (ours) MvMDAE (ours) DMvDR (ours) MAP@100 Accuracy 38.08 49.97 49.70 49.20 52.12 51.38 51.52 49.01 48.99 76.83 50.60 51.85 52.35 52.82 50.33 50.83 51.38 53.28 53.30 71.48 We can observe the performance of the methods on the animal dataset graphically in Fig.
6 and quantitatively in Table 3.
DMvDR outperforms the other competing methods by a large margin as shown in the plots of Fig.
6.
Due to the variety of data distribution from different feature types as view inputs, the co-training type of SmVR can no longer compete with the end-to-end solution.
From Table 3, one can observe that the performance of the feature concatenation suffers from the same problem.
On the other hand, our proposed subspace ranking methods produces satisfactory classiﬁcation rates while the precisions remain somewhat low.
This implies again the scoring function is critical to be trained together with the feature mappings.
The other linear and nonlinear subspace ranking methods have comparatively similar performance at a lower position.
5 CONCLUSION Learning to rank has been a popular research topic with numerous applications, while multi-view ranking remains a relatively new research topic.
In this paper, we aimed to associate the multi-view subspace learning methods with the ranking problem and proposed three methods in this direction.
MvCCAE is an unsupervised multi-view embed- ding method, while MvMDAE is its supervised counter- part.
Both of them incorporate multiple objectives, with a 00.51Recall0.50.550.60.650.70.750.80.850.90.95PrecisionSmVRDMvMDAMvCCAEMvMDAEDMvDR00.51FP Rate00.20.40.60.81TP RateSmVRDMvMDAMvCCAEMvMDAEDMvDRcorrelation maximization on one hand, and reconstruction error minimization on the other hand, and have been ex- tended in the multi-view subspace learning to rank scheme.
Finally, DMvDR is proposed to exploit the global agreement while minimizing the individual ranking losses in a single optimization process.
The experimental results validate the superior performance of DMvDR compared to the other subspace and co-training methods on multi-view datasets with both homogeneous and heterogeneous data represen- tations.
In the future, we will explore the scenario when there exists missing data, which is beyond the scope of the current proposed subspace ranking methods during training.
Mul- tiple networks can also be combined by concatenating their outputs, and further optimized in a single sub-network.
This solution may also be applicable for homogeneous represen- tations.
REFERENCES [1] T.-Y.
Liu, “Learning to rank for information retrieval,” Foundations and Trends in Information Retrieval, vol.
3, no.
3, pp.
225–331, 2009.
[2] Y.
Zhu, G.
Wang, J.
Yang, D.
Wang, J.
Yan, J.
Hu, and Z.
Chen, “Optimizing search engine revenue in sponsored search,” in Pro- ceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval.
ACM, 2009, pp.
588–595.
[3] M.
Amini, N.
Usunier, and C.
Goutte, “Learning from multiple partially observed views - an application to multilingual text cat- egorization,” in Advances in Neural Information Processing Systems 22.
Curran Associates, Inc., 2009, pp.
28–36.
J.
Yu, D.
Tao, M.
Wang, and Y.
Rui, “Learning to rank using user clicks and visual features for image retrieval,” IEEE transactions on cybernetics, vol.
45, no.
4, pp.
767–779, 2015.
[4] [5] G.
Cao, A.
Iosiﬁdis, K.
Chen, and M.
Gabbouj, “General- ized multi-view embedding for visual recognition and cross- modal retrieval,” IEEE Transactions on Cybernetics, 2017, doi: 10.1109/TCYB.2017.2742705.
[6] M.
Kan, S.
Shan, H.
Zhang, S.
Lao, and X.
Chen, “Multi-view discriminant analysis,” IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), vol.
38, no.
1, pp.
188–194, Jan 2016.
[7] A.
Blum and T.
Mitchell, “Combining labeled and unlabeled data with co-training,” in Proceedings of the eleventh annual conference on Computational learning theory.
ACM, 1998, pp.
92–100.
[8] N.
Usunier, M.-R.
Amini, and C.
Goutte, “Multiview semi- supervised learning for ranking multilingual documents,” Machine Learning and Knowledge Discovery in Databases, pp.
443–458, 2011.
[9] D.
R.
Hardoon, S.
Szedmak, and J.
Shawe-Taylor, “Canonical correlation analysis: An overview with application to learning methods,” Neural computation, vol.
16, no.
12, pp.
2639–2664, 2004.
[10] J.
Rupnik and J.
Shawe-Taylor, “Multi-view canonical correlation analysis,” in Slovenian KDD Conference on Data Mining and Data Warehouses (SiKDD 2010), 2010, pp.
1–4.
[11] A.
Iosiﬁdis, A.
Tefas, and I.
Pitas, “Kernel reference discriminant analysis,” Pattern Recognition Letters, vol.
49, pp.
85–91, 2014.
[12] A.
Iosiﬁdis and M.
Gabbouj, “Nystr¨om-based approximate kernel subspace learning,” Pattern Recognition, vol.
57, pp.
190–197, 2016.
[13] G.
Andrew, R.
Arora, J.
Bilmes, and K.
Livescu, “Deep canonical correlation analysis,” in Proceedings of the 30th International Confer- ence on Machine Learning, 2013, pp.
1247–1255.
[14] G.
Cao, A.
Iosiﬁdis, and M.
Gabbouj, “Multi-view nonparametric discriminant analysis for image retrieval and recognition,” IEEE Signal Processing Letters, vol.
24, no.
10, pp.
1537–1541, Oct 2017.
[15] F.
Feng, L.
Nie, X.
Wang, R.
Hong, and T.-S.
Chua, “Computational social indicators: A case study of chinese university ranking,” in Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval.
New York, NY, USA: ACM, 2017, pp.
455–464.
[16] “The times higher education world university ranking,” https://www.timeshighereducation.com/world-university- rankings, 2016.
[17] N.
C.
Liu and Y.
Cheng, “The academic ranking of world univer- sities,” Higher education in Europe, vol.
30, no.
2, pp.
127–136, 2005.
11 [18] T.
Liu, J.
Wang, J.
Sun, N.
Zheng, X.
Tang, and H.-Y.
Shum, “Picture collage,” IEEE Transactions on Multimedia (TMM), vol.
11, no.
7, pp.
1225 –1239, 2009.
[19] X.
Li, T.
Pi, Z.
Zhang, X.
Zhao, M.
Wang, X.
Li, and P.
S.
Yu, “Learn- ing bregman distance functions for structural learning to rank,” IEEE Transactions on Knowledge and Data EngineeringS, vol.
29, no.
9, pp.
1916–1927, Sept 2017.
[20] O.
Wu, Q.
You, X.
Mao, F.
Xia, F.
Yuan, and W.
Hu, “Listwise learn- ing to rank by exploring structure of objects,” IEEE Transactions on Knowledge and Data Engineering, vol.
28, no.
7, pp.
1934–1939, 2016.
[21] L.
Page, S.
Brin, R.
Motwani, and T.
Winograd, “The pagerank citation ranking: Bringing order to the web.” Stanford InfoLab, Tech.
Rep., 1999.
[22] W.
W.
Cohen, R.
E.
Schapire, and Y.
Singer, “Learning to order things,” in Advances in Neural Information Processing Systems, 1998, pp.
451–457.
[23] C.
Burges, T.
Shaked, E.
Renshaw, A.
Lazier, M.
Deeds, N.
Hamil- ton, and G.
Hullender, “Learning to rank using gradient descent,” in Proceedings of the 22nd international conference on Machine learning.
ACM, 2005, pp.
89–96.
[24] R.
Herbrich, T.
Graepel, and K.
Obermayer, “Large margin rank boundaries for ordinal regression,” 2000.
[25] Y.
Freund, R.
Iyer, R.
E.
Schapire, and Y.
Singer, “An efﬁcient boosting algorithm for combining preferences,” The Journal of machine learning research, vol.
4, pp.
933–969, 2003.
[26] Y.
Freund and R.
E.
Schapire, “A desicion-theoretic generalization of on-line learning and an application to boosting,” in European conference on computational learning theory.
Springer, 1995, pp.
23– 37.
[27] T.
Joachims, “Optimizing search engines using clickthrough data,” in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining.
ACM, 2002, pp.
133–142.
[28] J.
H.
Friedman, “Greedy function approximation: a gradient boost- ing machine,” Annals of statistics, pp.
1189–1232, 2001.
[29] J.
Xu and H.
Li, “AdaRank: a boosting algorithm for information retrieval,” in Proceedings of the 30th annual international ACM SI- GIR conference on Research and development in information retrieval.
ACM, 2007, pp.
391–398.
[30] C.
J.
Burges, R.
Ragno, and Q.
V.
Le, “Learning to rank with nons- mooth cost functions,” in Advances in neural information processing systems, 2007, pp.
193–200.
[31] H.
Hotelling, “Relations between two sets of variates,” Biometrika, [32] M.
Borga, “Canonical correlation: a tutorial,” http://people.imt.
pp.
321–377, 1936.
liu.se/∼magnus/cca/tutorial/tutorial.pdf, 2001.
[33] A.
A.
Nielsen, “Multiset canonical correlations analysis and mul- tispectral, truly multitemporal remote sensing data,” IEEE Trans- actions on Image Processing (TIP), vol.
11, no.
3, pp.
293–305, 2002.
[34] Y.
Gong, Q.
Ke, M.
Isard, and S.
Lazebnik, “A multi-view embed- ding space for modeling internet images, tags, and their seman- tics,” International Journal of Computer Vision, vol.
106, no.
2, pp.
210–233, 2014.
[35] Y.
Luo, D.
Tao, K.
Ramamohanarao, C.
Xu, and Y.
Wen, “Tensor canonical correlation analysis for multi-view dimension reduc- tion,” IEEE Transactions on Knowledge and Data Engineering, vol.
27, no.
11, pp.
3111–3124, Nov 2015.
[36] A.
Sharma, A.
Kumar, H.
Daume III, and D.
W.
Jacobs, “General- ized multiview analysis: A discriminative latent space,” in Proceed- ings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
IEEE, 2012, pp.
2160–2167.
[37] J.
Ngiam, A.
Khosla, M.
Kim, J.
Nam, H.
Lee, and A.
Y.
Ng, “Multimodal deep learning,” in Proceedings of the 28th international conference on machine learning (ICML-11), 2011, pp.
689–696.
[38] W.
Wang, R.
Arora, K.
Livescu, and J.
Bilmes, “On deep multi-view representation learning,” in Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015, pp.
1083–1092.
[39] S.
Chandar, M.
M.
Khapra, H.
Larochelle, and B.
Ravindran, “Correlational neural networks,” Neural computation, 2016.
[40] M.
Kan, S.
Shan, and X.
Chen, “Multi-view deep network for cross-view classiﬁcation,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp.
4847–4855.
[41] S.
Yan, D.
Xu, B.
Zhang, H.-J.
Zhang, Q.
Yang, and S.
Lin, “Graph embedding and extensions: a general framework for dimension- ality reduction,” IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), vol.
29, no.
1, pp.
40–51, 2007.
[42] Y.
Jia, F.
Nie, and C.
Zhang, “Trace ratio problem revisited,” IEEE Transactions on Neural Networks, vol.
20, no.
4, pp.
729–735, 2009.
12 [43] Y.
LeCun, L.
Bottou, G.
B.
Orr, and K.-R.
M ¨uller, “Efﬁcient back- Springer, 1998, pp.
prop,” in Neural networks: Tricks of the trade.
9–50.
[44] C.
D.
Manning, P.
Raghavan, H.
Sch ¨utze et al., Introduction to Cambridge university press Cambridge, information retrieval.
2008, vol.
1, no.
1.
[45] “World university rankings: kaggle dataset.” https://www.kaggle.com/mylesoneill/world-university- rankings, 2016.
[46] C.
H.
Lampert, H.
Nickisch, and S.
Harmeling, “Attribute-based classiﬁcation for zero-shot visual object categorization,” IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), vol.
36, no.
3, pp.
453–465, 2014.
[47] K.
Simonyan and A.
Zisserman, “Very deep convolutional net- works for large-scale image recognition,” International Conference on Learning Representations (ICLR), 2015.
[48] T.
Mikolov, I.
Sutskever, K.
Chen, G.
S.
Corrado, and J.
Dean, “Distributed representations of words and phrases and their com- positionality,” in Advances in neural information processing systems (NIPS), 2013, pp.
3111–3119.
[49] C.
Kemp, J.
B.
Tenenbaum, T.
L.
Grifﬁths, T.
Yamada, and N.
Ueda, “Learning systems of concepts with an inﬁnite relational model,” in AAAI, vol.
3, 2006, p.
5.
[50] D.
N.
Osherson, J.
Stern, O.
Wilkie, M.
Stob, and E.
E.
Smith, “Default probability,” Cognitive Science, vol.
15, no.
2, pp.
251–269, 1991.

In a variety of application areas, processing systems are dynamically scheduled to maintain stability and to meet various other objectives.
Indeed, the basic prob- lem in scheduling theory has been to ﬁnd and study poli- cies that accomplish this task under diﬀerent modeling assumptions.
In practice however, while human experts may be able to manage real-world processing systems, it is typically non-trivial to precisely quantify the costs and objectives that govern expert schedulers.
For example, in operating room scheduling, ad hoc metrics have been ap- plied in an attempt to model the cost of delays, e.g. [1], but these metrics are largely subjective.
The Delphi Method is commonly used in management science to quantitatively model expert opinions but such methods have no algorith- mic guarantees and are not always reliable [2].
In this paper, we present an online algorithm that allows us to emulate an expert scheduler based on observations of the backlog of the queues in the system and observations of the expert’s scheduling decisions.
We use the term “emu- late” to mean that while the parametric form of the learned policy may not converge to the parametric form of the ex- pert policy in all cases, it will always yield scheduling de- cisions that on average converge to the expert’s decisions.
This oﬀers a data-driven way for designing autonomous scheduling systems.
We speciﬁcally consider a projective cone scheduling (PCS) model which has applications in manufacturing, call/service centers, and in communication networks [3, 4].
Email address: neal.m.master@berkeley.edu (Neal Master) The algorithm in this paper uses the multiplicative weight update (MWU) method [5].
The MWU method has been used in several areas including solving zero-sum games [6], solving linear programs [7], and inverse opti- mization [8].
Because the PCS policy can be written as a maximization, our techniques are most similar to those used in [8].
In [8], the authors apply an MWU algorithm over a ﬁxed horizon to learn the objective of an expert who is solving a sequence of linear programs.
Our results diﬀer from [8] in several ways.
One is that because of the queue- ing dynamics that we consider, our expert’s objective will vary over time whereas in [8] the objective is constant.
A related issue is that in [8], when the expert has a decision variable of dimension n, the dimension of the parameter being learned is also n.
In our case, when the expert has a decision variable of dimension n (i.e. there are n queues in the system), we need to estimate Θ(n2) parameters.
We also note that in this paper we provide an algorithm that can be applied even when the horizon is not known a priori.
The goal of inferring parts of an optimization model from data is a well-studied problem in many other ap- plications.
For example, genetic algorithm heuristics have been applied to estimate the objective and constraints of a linear program in a data envelopment analysis context [9].
The goal of imputing the objective function of a convex optimization problem has also been considered in the opti- mization community, e.g. [10, 11].
These papers rely heav- ily on the convexity of the objective and the feasible set.
This approach does not apply in a PCS context because the set of feasible scheduling actions is discrete and hence non-convex.
Preprint January 29, 2018 This paper is also related to inverse reinforcement learn- ing.
Inverse reinforcement learning is the problem of esti- mating the rewards of a Markov decision process (MDP) given observations of how the MDP evolves under an opti- mal policy [12].
Inverse reinforcement learning can be used to emulate expert decision makers (referred to as “appren- ticeship learning” in the machine learning community) as long as the underlying dynamics are Markovian [13].
In the PCS model, no such assumption is made and so our results naturally do not require Markovian dynamics.
The remainder of this paper is organized as follows.
Sec- tion 2 speciﬁes the PCS model that we consider.
Section 3 presents our algorithms and the relevant guarantees.
Be- cause we take a MWU approach to the problem, our guar- antees are bounds on the average loss.
However, we also provide a concentration bound which gives guarantees on the tail of the loss distribution.
We provide a simple nu- merical demonstration of our algorithms in Section 4.
In Section 5 we discuss some extensions of our results and we conclude in Section 6.
2.
Projective Cone Scheduling Dynamics In this section we summarize the PCS model presented in [4] and comment on the connection to the model pre- sented in [3].
The PCS model has n queues each with inﬁnite waiting room following an arbitrary queueing dis- cipline.
Time is discretized into slots t ∈ Z+ 1.
The backlog in queue i at the beginning of time slot t is xt(i).
The back- log across all queues can be written as a vector xt ∈ Zn +.
The number of customers that arrive at queue i at the end of time slot t is at(i).
The arrivals across all queues can be written as a vector at ∈ Zn +.
Scheduling conﬁg- urations are chosen from a ﬁnite set S (cid:40) Zn If con- +.
ﬁguration st ∈ S is chosen in time slot t then for each queue i, min{st(i), xt(i)} customers are served at the be- ginning of the time slot.
We take the departure vector as dt = min{st, xt} ∈ Zn + where the minimum is taken component-wise.
This gives us the following dynamics xt+1 = xt − dt + at (1) where x0 ∈ Zn + is arbitrary.
Note that the arrival vector is allowed to depend on previous scheduling conﬁgurations, previous arrivals, and previous backlogs in an arbitrary way.
The scheduling conﬁgurations are dynamically chosen by solving the maximization s∈S (cid:104)s, Bxt(cid:105) = max s∈S max (cid:88) i,j s(i)B(i, j)xt(j) (2) where B ∈ Rn×n is symmetric and positive-deﬁnite with non-positive oﬀ-diagonal elements.
We assume that S is endowed with some arbitrary ordering used for breaking ties.
This PCS policy deﬁnes a broad class of scheduling policies and in particular we note that by taking B as the identity matrix, we return to the typical maximum weight matching scheduling algorithm.
Although B is a matrix, because B is symmetric, there are only p = n(n + 1)/2 rather than n2 free param- eters that need to be learned.
Consequently, we will represent the projective cone scheduler with an upper- triangular array rather than a matrix.
In particular, take b(i, i) ∝ B(i, i) for i ∈ [n] and b(i, j) ∝ −B(i, j) for (i, j) ∈ {(i, j) ∈ [n] × [n] : i < j}2.
We can also assume i,j b(i, j) = 1.
Then we can write the projective cone scheduling decision as fol- lows: without loss of generality that (cid:80) B(i, j)s(i)xt(j) B(i, i)s(i)xt(i) B(i, j)(s(i)xt(j) + s(j)xt(i)) (cid:111) (cid:111) st = arg max = arg max s∈S = arg max s∈S i=1 j=1 s∈S (cid:104)s, Bxt(cid:105) n(cid:88) n(cid:88) (cid:110)(cid:88) (cid:88) (cid:110)(cid:88) −(cid:88) i<j = arg max s∈S b(i, i)s(i)xt(i) i<j (cid:44) µ(xt; b) For convenience, let us deﬁne b(i, j)(s(i)xt(j) + s(j)xt(i)) (3) (4) , i = j , i (cid:54)= j (cid:26) 1 −1 σ(i, j) = (cid:88) i≤j (cid:26) so that we we can write µ more compactly as follows: µ(xt; b) = arg max s∈S σ(i, j)b(i, j)(s(i)xt(j) + s(j)xt(i)) (5) Note that if we deﬁne yt, a normalized version of xt, as follows, yt = xt xt/(cid:107)xt(cid:107)1 , (cid:107)xt(cid:107)1 = 0 , otherwise (6) then we have that st = µ(xt; b) = µ(yt; b).
Modeling each customer as having a uniform determin- istic service time is motivated largely by applications in computer systems and in particular, packet switch schedul- ing.
However, PCS models with non-deterministic service times have also been considered in the literature [3].
How- ever, the results in [3] only apply to the case where B is diagonal.
We have opted to present our algorithms in the 1We use the notation Z+ = {0, 1, 2, .
.
.}.
2We use the notation [k] = {1, 2, .
.
.
, k}.
context of the non-diagonal case because we feel that hav- ing Θ(n2) parameters is more interesting than having only Θ(n) parameters.
Our algorithms can still be applied in the case of stochastic service times; this is discussed along with other extensions in Section 5.
Finally, we note that previous literature on PCS [3, 4] has required a variety of additional assumptions.
For ex- ample, in [4] it is assumed that the arrival process is mean ergodic.
We do not require such an assumption and more- over, while the results in [3] and [4] are primarily stability guarantees, we make no assumptions on the stability of the system.
3.
The Learning Algorithm In this section we present our algorithm.
We ﬁrst present a ﬁnite horizon algorithm and then leverage this to present an inﬁnite horizon algorithm.
For both algorithms, we show that the average error is O(ln(T )(cid:112)ln(p)/T ).
We also provide a bound on the fraction of observations for which the error exceeds our average case bound.
These algorithms are applied causally in an online fash- ion.
Although we do not focus on computational issues, we note that computing µ(yt; b) is generally a diﬃcult prob- lem.
However, there are local search heuristics that allow eﬃcient computation of µ(yt+1; b) based on the solution to µ(yt; b) [14].
Our algorithms require the computation of µ(yt; ˆbt) where ˆbt is the estimate of b at time t and so an online algorithm is appropriate if we want to use the previous solution as a warm start.
Before presenting the algorithms, we consider the loss function of interest.
Since the expert we are trying to emulate is speciﬁed by the array b, it may seem reasonable to want our estimates ˆbt to converge to b.
However, this goal is not as reasonable as it may seem.
Because S is discrete, it is possible that two diﬀerent values of b can render the same scheduling decisions.
Consequently, the goal of exactly recovering b may be ill-posed.
We aim to emulate the expert scheduler so we want ˆst = µ(yt; ˆbt), the scheduling decision induced by the estimate ˆbt, to be the same as st, the expert’s scheduling decision.
Hence, the loss should directly penalize discrepancies between st and ˆst.
This leads us to jointly consider ˆbt and ˆst so that the loss at time t is (cid:88) i≤j where δt = ˆst − st.
When b = ˆbt, we have that st = ˆst and (cid:96)t = 0.
In addition, when st = ˆst we have that (cid:96)t = 0 even if b (cid:54)= ˆbt.
The deﬁnition of µ will allow us to show below that (cid:96)t ≥ 0.
Another advantage to this loss function is that it allows us to give guarantees that are independent of the statistics of the arrival process.
For example, suppose that there are no arrivals at some subset of the queues.
In this case, it would be unreasonable to expect to be able to estimate the rows and columns of b relevant to those queues.
More generally, the arrival process may not suﬃciently excite all modes of the system.
By considering ˆst and ˆbt simul- taneously, we can provide bounds that apply even in the presence of pathological arrival processes.
3.1. A Finite Horizon Algorithm We ﬁrst present Algorithm 1, a ﬁnite horizon algorithm that requires knowledge of the horizon.
Algorithm 1 is a multiplicate weights update algorithm and this time hori- zon is used to set the learning rate.
Algorithm 1: Online Parameter Learning with a Fixed Horizon Input : ((y1, s1), .
.
.
, (yT , sT )) # observations Output: (ˆb1, .
.
.
, ˆbT ) # parameter estimates Output: (ˆs1, .
.
.
, ˆsT ) # scheduling estimates 1 η ←(cid:112)ln(p)/T ˆbt ← wt/(cid:80) 2 w1 ← upper triangular array of 1s 3 for t = 1, .
.
.
T do i≤j wt(i, j) ˆst ← µ(yt; ˆbt) mt ← upper triangular array of 0s δt = ˆst − st if ˆst (cid:54)= st then zt = δt/(cid:107)δt(cid:107)∞ for (i, j) ∈ [n]2 : i ≤ j do mt(i, j) ← σ(i, j)(zt(i)yt(j) + zt(j)yt(i)) end end wt+1 ← wt(1 − ηmt) # component-wise 10 11 12 13 14 15 end Theorem 1.
Let D = max (u,v)∈S 2 (cid:107)u − v(cid:107)∞ and p = n(n + 1)/2.
Algorithm 1 satiﬁes the following inequality: If T > 4 ln(p) then the output of (cid:114) T(cid:88) t=1 Proof.
Note that because mt ∈ [−1, 1]p and η < 1 directly apply [5, Corollary 2.2.]: 2 we can (cid:96)t = σ(i, j)(ˆbt(i, j)−b(i, j))(δt(i)yt(j)+δt(j)yt(i)) (7) 0 ≤ 1 (cid:96)t ≤ 2D ln(p) (8) T(cid:88) t=1 (cid:88) ≤ T(cid:88) i≤j (cid:88) mt(i, j)ˆbt(i, j) t=1 i,j (mt(i, j) + η |mt(i, j)|)b(i, j) + ln(p) (9) i≤j ˆbt(i, j) = 1 we have the Since |mt(i, j)| ≤ 1 and (cid:80) following: T(cid:88) t=1 (cid:88) ≤ T(cid:88) i≤j (cid:88) mt(i, j)ˆbt(i, j) mt(i, j)b(i, j) + ηT + (10) ln(p) i,j t=1 is minimized when η = (cid:112)ln(p)/T .
Rearranging the in- A straightforward calculation shows that this upper bound equality and applying this fact give us the following: T(cid:88) (cid:88) t=1 (cid:114) i≤j ≤ 2 ln(p) mt(i, j)ˆbt(i, j) − 1 T(cid:88) (cid:88) t=1 i≤j mt(i, j)b(i, j) (11) Now we apply the speciﬁcs of mt.
By deﬁnition of D, (cid:107)δt(cid:107)∞ ≤ D.
This gives us the following: σ(i, j)(δt(i)yt(j) + δt(j)yt(i))ˆbt(i, j) σ(i, j)(−δt(i)yt(j) − δt(j)yt(i))b(i, j) Algorithm 2: Online Parameter Learning with an Unknown Horizon Input : ((y1, s1), (y2, s2), .
.
.) # observations Output: (ˆb1, ˆb2, .
.
.) # parameter estimates Output: (ˆs1, ˆs2, .
.
.) # scheduling estimates 1 T−1 (cid:44) 0 2 Tk (cid:44) 2k(4 ln(p)) for k ∈ {0, 1, 2, .
.
.}.
3 for t = 1, 2, .
.
.
do if Tk < t ≤ Tk+1 then Apply Algorithm 1 with T ≡ Tk and without re-initializing wt end 8 end Theorem 2.
Suppose T ≥ T0.
Deﬁne lg(·) as (cid:100)log2(·)(cid:101).
Then the output of Algorithm 2 satiﬁes the following in- equality: T(cid:88) t=1 0 ≤ 1 (cid:96)t ≤ 2 2D lg (cid:19)(cid:114) (cid:18) 2T T0 ln(p) (13) Note that these are the same bounds as in Theorem 1 but with an additional factor of 2 lg(2T /T0).
Proof.
First note that the proof of [5, Corollary 2.2.] does not require the initial weights to be uniform so Theorem 1 still applies even without the initialization on line 2 of Algorithm 1.
For convenience, let Uk = 2D(cid:112)ln(p)/Tk and (12) take K = lg(T /T0).
Applying Theorem 1 to each stage of Algorithm 2 gives us the following: (cid:96)t t=1 0 ≤ T(cid:88) ≤ K(cid:88) Tk(cid:88) (cid:96)t ≤ K(cid:88) TkUk ≤ K(cid:88) (cid:112) ≤ 2D(K + 1)(cid:112)ln(p) 2D(K + 1)(cid:112)ln(p) ≤ 2 TK k=0 k=0 k=0 t=1 2D(cid:112)ln(p) (cid:112) Tk (14) The ﬁrst inequality follows from the fact that (cid:96)t ≥ 0; the second inequality follows by extending the sum from T up to TK; the third and fourth inequalities follow from The- orem 1.
The penultimate inequality follows from the fact that {Tk} is an increasing sequence and the ﬁnal inequality follows because TK can be no more that 2T .
Dividing by T gives the desired result.
3.3. A Concentration Bound Our previous results provided bounds on the average loss of our algorithms.
In this section, we provide bounds for the tail of the distribution of the loss.
This gives us the guarantee that the fraction of observations for which the loss exceeds our average case bound tends to zero.
T(cid:88) T(cid:88) t=1 t=1 i≤j (cid:88) (cid:88) (cid:114) i≤j ≤ 2D ln(p) (cid:88) i≤j ≥(cid:88) i≤j Note that ˆst = µ(yt; ˆbt) and µ is deﬁned in terms of a maximization.
Therefore, σ(i, j)(ˆst(i)yt(j) + ˆst(j)yt(i))ˆbt(i, j) σ(i, j)(s(i)yt(j) + s(j)yt(i))ˆbt(i, j) for any s ∈ S.
This shows that each term in the ﬁrst Ces`aro sum in (12) is non-negative.
Similarly, each term in the second Ces`aro sum in (12) is non-negative.
This gives us a lower bound of zero.
Rearranging the terms leaves us with the desired results.
3.2. An Inﬁnite Horizon Algorithm We now present Algorithm 2, an inﬁnite horizon algo- rithm that dynamically changes the learning rate.
Al- gorithm 2 applies the “doubling trick” to Algorithm 1.
The idea is that we deﬁne epochs [Tk, Tk+1] where Tk = 2k(4 ln(p)) for k ≥ 0 with T−1 = 0.
The duration of the kth epoch is Tk and in this epoch we apply Algo- rithm 1.
Up to poly-logarithmic factors of T , this gives us the same convergence rate that we had for Algorithm 1.
Theorem 3.
Let t ≤ T : (cid:96)t > 2 2D lg (cid:12)(cid:12)(cid:12)(cid:12)(cid:26) (cid:16) 2T T0 (cid:17)(cid:113) ln(p) T +  (cid:27)(cid:12)(cid:12)(cid:12)(cid:12) fT () = (15) be the fraction of observations up to time T ≥ T0 for which the loss exceeds the average-case bound by at least .
Then for any  > 0 we have that (cid:17)(cid:113) ln(p) fT () ≤ 1 − (cid:16) 2T 2D lg (16) T +  T0 and hence, lim T→∞ fT () = 0.
(17) Proof.
The observed loss sequence {(cid:96)t}T t=1 deﬁnes a point measure on R+ where each point has mass 1/T .
Applying Markov’s Inequality to this measure gives us that fT () ≤ 2D lg 2D lg T +  T +  (cid:16) 2T (cid:16) 2T T0 T0 (cid:17)(cid:113) ln(p) (cid:17)(cid:113) ln(p) (cid:18) 2T 2D lg T0 (cid:19)(cid:114) lim T→∞ 2 Rearranging the upper bound gives the ﬁrst result.
For the second result we simply take the limit and note that (a) Evolution of ˆbt (cid:19)(cid:114) (cid:18) 2T T0 ln(p) T(cid:88) t=1 (cid:96)t 2D lg · 1 · 2 ln(p) = 0.
4.
A Numerical Demonstration We now demonstrate Algorithm 2 on a small example of n = 2 queues.
In each time slot, the number of arriving customers is geometrically distributed on Z+.
For queue 1 the mean number of arriving customers is 1 and for queue 2 the mean number of arriving customers is 2.
The arrivals are independent across time slots as well as across queues.
We take b = (cid:20) 0.5 0.3 ,(cid:2) 2 0.2 ,(cid:2) 1 0 (cid:3)(cid:48) (cid:21) 1 (cid:3)(cid:48) ,(cid:2) 0 2 (cid:3)(cid:48)(cid:111) (cid:110)(cid:2) 0 0 (cid:3)(cid:48) and S = This choice of b shows that the expert scheduler prioritizes queue 1 over queue 2 and the expert also has a preference to not serve both queues simultaneously.
We simulate the system and run Algorithm 2 for T = 106 time slots with x0 = (cid:2) 0 0 (cid:3)(cid:48) .
The results are shown in Figure 1.
First note that Figure 1a shows that the ˆbt does not converge to b.
We see that (to 4 decimal places) (cid:20) 0.4998 ˆbT = (cid:21) 0.3018 0.1984 (b) Error in learned scheduling decisions (c) Average realized loss Figure 1: Output of Algorithm 2 for the example in Section 4 0.450.500.55ˆbt(0,0)b(0,0)0.270.300.33ˆbt(0,1)b(0,1)0.00.20.40.60.81.0Time1e60.180.200.22ˆbt(1,1)b(1,1)0.00.20.40.60.81.0Time1e60.00.51.01.52.0kˆst−stk∞0.00.20.40.60.81.0Time1e610-310-210-1100101Average LossRealizedUpper Boundand for the majority of the simulation these parameter estimates do not change.
The reason is that (as shown in in Figure 1b) ˆbT yields the same scheduling decisions as b.
The algorithm learns to emulate the expert scheduler so the loss becomes zero and the weights stop updating.
This possibility was discussed at the beginning of Section 3.
Figure 1c which shows that while the average loss does indeed tend to zero, the upper bound proved in Theorem 2 is quite loose in this situation.
This is expected due to the generality of the theorem.
This also means that the concentration bound in Theorem 3 is quite conservative.
Indeed, for this simulation we see that ft() = 0 for all t and for any  > 0.
In other words, no observed loss ever exceeds the average case bound.
5.
Extensions We now discuss some extensions to our algorithms.
We ﬁrst note that we could replace line 14 in Algorithm 1 with same goals as a human manager.
We have provided several theoretical guarantees and have numerically demonstrated the eﬃcacy of the algorithm on a simple example.
This paper opens the door for a few area of future work.
One idea is to provide tighter bounds that depend on the statistical properties of the arrival process.
A beneﬁt of the current approach is that it does not require any as- sumptions on the arrival process but the clear downside is that the resulting bounds are quite loose.
An algorithm that uses information about the arrival process could have faster convergence rates and tighter bounds.
Another idea is to investigate the impact of an approx- imate computation of µ.
As mentioned in Section 3, in large-scale problems, exactly computing µ(y; b) is gener- ally a diﬃcult problem and heuristic approaches are typ- ically taken in practice.
An area of future work would be to consider how such approximation “noise” aﬀects our ability to emulate the expert scheduler.
wt+1 ← wt · exp(−ηwt).
References The new algorithm would be a Hedge-style algorithm and we would be able to use apply other results (e.g. [5, The- orem 2.4]) to obtain similar upper bounds on the average loss.
We also note that we could modify our algorithms and obtain tighter upper bounds if we impose additional as- sumptions on the expert.
For example, the expert may have a fairly simple objective that leads to prioritization of some queues over others.
In this case, we would have b(i, j) = 0 for i (cid:54)= j.
Rather than having a triangular ar- ray of p parameter estimates, we could instead keep track of just n estimates.
Since Θ(ln(p)) = Θ(ln(n)), the con- vergence rate would not change but we would have smaller constant factors.
Other sparsity patterns could be handled in a similar fashion.
The diagonal case is slightly simpler because there would be no need use σ(i, j) to keep track of the appropriate signs.
As noted in Section 2, a continuous-time PCS model with heterogenous and stochastic service times was con- sidered in [3].
Our algorithms could be applied in this setting as well by updating the algorithm immediately af- ter customer arrivals and departures rather than in dis- crete time slots.
In [3], B is diagonal and so we could apply the simpliﬁcations mentioned above.
Our theorems would still hold because they not require that the state up- date happen at regularly spaced intervals – the algorithms merely require a stream of observed backlogs and observed scheduling actions.
6.
Conclusions and Future Work In this paper we have proposed an algorithm that learns a scheduling policy that emulates the behavior of an expert projective cone scheduler.
This oﬀers a data-driven way of designing automated scheduling policies that achieve the [1] N.
Master, Z.
Zhou, D.
Miller, D.
Scheinker, N.
Bambos, P.
Glynn, Improving predictions of pediatric surgical durations with supervised learning, International Journal of Data Science and Analytics (2017) 1–18.
[2] C.
Okoli, S.
D.
Pawlowski, The Delphi method as a research tool: an example, design considerations and applications, Infor- mation & management 42 (1) (2004) 15–29.
[3] M.
Armony, N.
Bambos, Queueing dynamics and maximal throughput scheduling in switched processing systems, Queue- ing systems 44 (3) (2003) 209–252.
[4] K.
Ross, N.
Bambos, Projective cone scheduling (PCS) algo- rithms for packet switches of maximal throughput, IEEE/ACM Transactions on Networking 17 (3) (2009) 976–989.
[5] S.
Arora, E.
Hazan, S.
Kale, The Multiplicative Weights Up- date Method: a Meta-Algorithm and Applications., Theory of Computing 8 (1) (2012) 121–164.
[6] Y.
Freund, R.
E.
Schapire, Adaptive game playing using mul- tiplicative weights, Games and Economic Behavior 29 (1-2) (1999) 79–103.
[7] S.
A.
Plotkin, D.
B.
Shmoys, ´E.
Tardos, Fast approximation algorithms for fractional packing and covering problems, Math- ematics of Operations Research 20 (2) (1995) 257–301.
[8] A.
B¨armann, S.
Pokutta, O.
Schneider, Emulating the Expert: Interna- Inverse Optimization through Online Learning, tional Conference on Machine Learning, 2017, pp.
400–410.
in: [9] M.
D.
Troutt, A.
A.
Brandyberry, C.
Sohn, S.
K.
Tadisina, Lin- ear programming system identiﬁcation: The general nonnega- tive parameters case, European Journal of Operational Research 185 (1) (2008) 63–75.
[10] A.
Keshavarz, Y.
Wang, S.
Boyd, Imputing a convex objec- tive function, in: IEEE International Symposium on Intelligent Control, IEEE, 2011, pp.
613–619.
[11] J.
Thai, A.
M.
Bayen, Imputing a variational inequality function or a convex objective function: A robust approach, Journal of Mathematical Analysis and Applications.
[12] A.
Y.
Ng, S.
J.
Russell, Algorithms for inverse reinforcement learning., in: International Conference on Machine Learning, 2000, pp.
663–670.
[13] P.
Abbeel, A.
Y.
Ng, Apprenticeship learning via inverse re- inforcement learning, in: International Conference on Machine learning, ACM, 2004, pp.
1–8.
[14] K.
Ross, N.
Bambos, Local search scheduling algorithms for maximal throughput in packet switches, in: Annual Joint Con- ference of the IEEE Computer and Communications Societies (INFOCOM), Vol.
2, IEEE, 2004, pp.
1158–1169.

Computational models for generating audio signals are a means of exploring and understanding our perception of sound.
Natural sounds, deﬁned here as everyday non-music, non-speech sounds, are an appealing medium with which to study perception since they exclude cognitive factors such as language and musical interpretation.
McDermott [1] used synthesis as a means to demonstrate that the human auditory system utilises time-averaged statistics of subband amplitudes to classify sound textures.
In a similar vein, Turner [2] constructed a synthesis model based on probabilistic latent variable analysis of those same subband amplitudes.
One main advantage of a latent variable approach is the possibility that the uncovered latent behaviour may represent either i) the primitive source that generated the signal, or ii) the latent information that the human auditory system encodes when it calculates time-averaged statistics.
2 Latent Force Modelling of Natural Sounds Latent variable analysis captures correlations across multiple dimensions by modelling the data’s shared dependence on some unobserved (latent) variable or function.
It is, by its very nature, ill-posed; we typically aim to simultaneously predict both the latent functions and the mapping from this latent space to the observation data.
As such, inﬁnitely many potential solutions exist and we cannot guarantee that our prediction will encode the true sound source or our true perceptual representation.
The ill-posed nature of the problem necessitates the use of prior information.
It is commonly suggested that nonnegativity, smoothness and sparsity form a suitable set of prior assumptions about real life signals.
We argue that, even after imposing such constraints, a simple scalar mapping between the latent space and observation space is insuﬃcient to capture all the complex behaviour that we observe in the subband amplitude envelopes of an audio signal.
We construct a latent force model (LFM) [3] to incorporate prior knowledge about how amplitude envelopes behave via a discrete diﬀerential equation that models exponential decay [4].
Utilising the state space formulation [5], we augment the standard LFM by explicitly including in the current state information from many discrete time steps.
This allows us to capture phenomena such as feedback, damping and to some extent reverberation.
In this probabilistic approach the latent functions are modelled with Gaussian processes, which provide uncertainty information about our predictions whilst also guaranteeing that the latent functions are smooth.
Nonnegativity is imposed via a nonlinear transformation.
Evaluating latent representations is not straightforward.
Objective measures of our ability to reconstruct the observation data don’t inform us about the interpretability of our predictions.
We hypothesise that if the latent functions capture physically or perceptually meaningful information, then a generative model based on synthesising latent functions that are statistically similar should generate realistic data when projected back to the observation space.
In this paper we introduce a generative model, applicable to a wide range of natural sounds, based on an extended LFM1 (Section 3).
Comparative models based on variants of nonnegative matrix factorisation (NMF) are implemented to perform evaluation-by-synthesis, which shows how listeners often perceive the LFM approach to generate more realistic sounds even in cases where NMF is more eﬃcient from a reconstruction error perspective (Section 4).
2 Background The perceptual similarity of two sounds is not determined by direct comparison of their waveforms, but rather by comparison of their statistics [1].
Hence it is argued that prior information for natural sounds should take a statistical form [2].
We argue in Section 3 that these statistical representations can be improved through the inclusion of assumptions about the physical behaviour of sound, resulting in a hybrid statistical-physical prior.
1 Matlab source code and example stimuli can be found at c4dm.eecs.qmul.ac.uk/ audioengineering/natural_sound_generation Latent Force Modelling of Natural Sounds In order to analyse sound statistics, both McDermott [1] and Turner [2] utilise the subband ﬁltering approach to time-frequency analysis, in which the signal is split into diﬀerent frequency channels by a bank of band-pass ﬁlters.
The time- frequency representation is then formed by tracking the amplitude envelopes of each subband.
McDermott generates sound textures by designing an objective function which allows the statistics of a synthetic signal to be matched to that of a target signal.
Turner utilises probabilistic time-frequency analysis combined with probabilistic latent variable analysis to represent similar features.
Turner’s approach has the advantage that once the parameters have been optimised, new amplitude envelopes can be generated by drawing samples from the latent distribution.
It should be noted that samples drawn from the model will not exhibit the fast attack and slow decay we observe in audio amplitude envelopes, since the model is temporally symmetric.
NMF is a ubiquitous technique for decomposing time-frequency audio data [6,7,8], however a common criticism is its inability to take into account temporal information.
The most common approach to dealing with this issue is to impose smoothness on the latent functions, the idea being that smoothness is a proxy for local correlation across neighbouring time steps.
Temporal NMF (tNMF) imposes smoothness by penalising latent functions which change abruptly [8] or by placing a Gaussian process prior over them [9].
An alternative approach is to use a hidden Markov model to capture the changes in an audio signal’s spectral make up over time [10].
High resolution NMF (HR-NMF) models the temporal evolution of a sound by utilising the assumption that natural signals are a sum of exponentially modulated sinusoids, with each frequency channel being assigned its own decay parameter estimated using expectation-maximisation [11].
2.1 Latent Force Models To incorporate our prior assumptions into data-driven analysis we use latent force models (LFMs) [3], a probabilistic modelling approach which assumes M observed output functions xm are produced by some R < M unobserved (latent) functions ur being passed through a set of diﬀerential equations.
If the chosen set of diﬀerential equations represents some physical behaviour present in the system we are modelling, even if only in a simplistic manner, then such a technique can improve our ability to learn from data [12].
This is achieved by placing a Gaussian process (GP) prior [13] over the R latent functions, calculating the cross-covariances (which involves solving the ODEs), and performing regression.
It was shown by Hartikeinen and S¨arkka [5] that, under certain conditions, an equivalent regression task can be performed by reformulating the model (i.e. the ODE representing our physical knowledge of the system) into state space (SS) form, reformulating the GP as a stochastic diﬀerential equation (SDE), and then combining them into a joint SS model: = f (x(t)) + Lw(t) .
(1) Here x(t) represents the state vector containing {xm(t)}M the SDE {ur(t), ˙ur(t), ...}R m=1 and the states of r=1, w(t) is a white noise process, f is the transition dx(t) dt 4 Latent Force Modelling of Natural Sounds function which is dependent on θ, the set of all ODE parameters and GP / SDE hyperparameters, and L is a vector determining which states are driven by the white noise.
The model’s discrete form is x[tk] = ˆf (x[tk−1], ∆tk) + q[tk−1] , (2) where ∆t is the time step size, ˆf is the discretised transition function and q[tk−1] ∼ N (0, Q[∆tk]) is the noise term with process noise matrix Q.
The corresponding output measurement model is y[tk] = Hx[tk] + [tk], [tk] ∼ N (0, σ2) , (3) where measurement matrix H simply selects the outputs from the joint model.
The posterior process x[tk], i.e. the solution to (2), is a GP in the linear case such that the ﬁltering distribution p(x[tk] | y[t1], ..., y[tk]) is Gaussian.
Hence state estimation can be performed via Kalman ﬁltering and smoothing [14].
However, if f is a nonlinear function, as is the case if we wish to impose nonnegativity on the latent functions, then calculation of the predictive and ﬁltering distributions involves integrating equations which are a combination of Gaussian processes and nonlinear functions.
We may approximate the solutions to these integrals numerically using Gaussian cubature rules.
This approach is known as the cubature Kalman ﬁlter (CKF) [15].
The Kalman update steps provide us with the means to calculate the marginal data likelihood p(y[t1:T ] | θ).
Model parameters θ can therefore be estimated from the data by maximising this likelihood using gradient-based methods.
3 Latent Force Models for Audio Signals To obtain amplitude data in the desired form we pass an audio signal through an equivalent rectangular bandwidth (ERB) ﬁlter bank.
We then use Gaussian process probabilistic amplitude demodulation (GPPAD) [16] to calculate the subband envelopes and their corresponding carrier signals.
GPPAD allows for control over demodulation time-scales via GP lengthscale hyperparameters.
We are concerned with slowly varying behaviour correlated across the frequency spectrum, in accordance with the observation that the human auditory system summarises sound statistics over time [1].
Fast-varying behaviour is relegated to the carrier signal and will be modelled as independent ﬁltered noise.
The number of channels in the ﬁlter bank and the demodulation lengthscales must be set manually during this ﬁrst analysis stage.
Keeping the number of total model parameters small is a priority (see Section 3.1), so we typically set the number of ﬁlters to 16, and the lengthscales such that we capture amplitude behaviour occurring over durations of 10ms and slower.
3.1 Augmented Latent Force Models for Amplitude Envelopes We use a ﬁrst order diﬀerential equation to model the exponential decay that occurs in audio amplitude envelopes [4].
However this overly simplistic model Latent Force Modelling of Natural Sounds does not take into account varying decay behaviour due to internal damping, or feedback and other nonstationary eﬀects which occur as a sound is generated and propagates towards a listener.
Since we require nonnegativity of our latent functions, which is imposed via nonlinear transformation, we use the nonlinear LFM whose general from is (2) with nonlinear ˆf .
For a ﬁrst order ODE its discrete form is ˙xm[tk] = −Dmxm[tk] + Smrg(ur[tk]) , (4) r=1 for m = 1, ..., M where M is the number of frequency channels.
Dm and Smr are the damping and sensitivity parameters respectively and g(u) = log(1+eu) is the positivity-enforcing nonlinear transformation.
The model progresses forwards in time with step size ∆t using Euler’s method: xm[tk+1] = xm[tk] + ∆t ˙xm[tk].
To account for the complex behaviour mentioned above that occurs in real audio signals, we extend this discrete model such that predictions at the current time step tk can be inﬂuenced explicitly by predictions from multiple time steps in the past.
As in [4] we augment the model by adding a parameter γm which controls the “linearity” of decay.
Our ﬁnal model becomes R(cid:88) P(cid:88) p=1 P(cid:88) R(cid:88) q=0 r=1 ˙xm[tk] = −Dmxγm m [tk] + Bmpxm[tk−p] + Smrqg(ur[tk−q]) .
(5) We restrict γm ∈ [0.5, 1], and for sounding objects with strong internal damping we expect γm to be small, representing an almost linear decay.
Parameters Bmp are feedback coeﬃcients which determine how the current output is aﬀected by output behaviour from p time steps in the past.
Smrq are lag parameters which determine how sensitive the current output is to input r from q time steps ago.
The lag term is important since modes of vibration in a sounding object tend to be activated at slightly diﬀerent times due to deformations in the object as it vibrates, and due to the interaction of multiple modes of vibration.
It can also capture eﬀects due to reverberation.
The feedback terms allow for long and varied decay behaviour that can’t be described by simple exponential decay.
The challenge is to incorporate (5) into our ﬁltering procedure.
We do this by augmenting our state vector x[tk] and transition model ˆf (x[tk−1], ∆tk) = x[tk] + ∆t ˙x[tk] with new rows corresponding to the delayed terms.
Fig.
1 shows how after each time step the current states X[tk] = {xm[tk]}M m=1, U [tk] = {ur[tk]}R r=1 are “passed down” such that at the next time step they are in the locations corresponding to feedback and lag terms.
When performing the Kalman ﬁlter prediction step, augmented states are included since they inﬂuence predictions for the current state, however the predictions for these augmented entries are simply exact copies from the previous time step.
(6) Fig.
2 shows the latent prediction for a metal impact sound with one latent force, R = 1.
The mean of the distribution is the minimum least squares error 6 Latent Force Modelling of Natural Sounds Fig.
1.
The augmented LFM stores terms from previous time steps in the state vector.
Blue represents output predictions X (amplitudes), green represents latent predictions U .
Each step, predictions pass down to feedback and lag state locations.
The entire state is used to predict the next step’s outputs and latents via Kalman ﬁltering.
estimate, so we pass it through discrete model (5) to reconstruct the amplitude envelopes.
Despite the single latent force, we observe that some of the complex behaviour has been learnt.
Additionally, the latent force is both smooth and sparse, and the reconstructed envelopes have a slow decay despite this sparsity.
3.2 Generating Novel Instances of Natural Sounds A signiﬁcant beneﬁt of probabilistic approaches such as LFM or tNMF is that, as well as providing us with uncertainty information about our predictions, they provide the means to sample new latent functions from the learnt distribution.
By passing these new functions through the model we can generate amplitude envelopes.
These envelopes modulate carrier signals produced using a sinusoids- plus-noise approach based on analysis of the original carriers.
The subbands are then summed to create a new synthetic audio signal distinct from the original but with similar characteristics.
Sampling from the prior of the learnt distribution generates functions with appropriate smoothness and magnitude, however the desired energy sparsity is not guaranteed.
Latent functions are modelled independently, but in practice they tend to co-occur and are activated in similar regions of the signal.
We use GPPAD again to demodulate our latent functions with a slowly varying envelope, then ﬁt a GP with a squared exponential covariance function to this envelope [13].
We sample from this high-level envelope and use it to modulate our newly generated latent functions; the results of this product is latent behaviour with sparse energy, as demonstrated in Fig.
3(d).
3.3 Optimisation Settings The set of model parameters {Dm, Bmp, Smrq, γm, λr}, with GP lengthscales λr, becomes large as R, P increase.
To alleviate issues that occur when our parameter space becomes large we sparsify the feedback and sensitivity parameters.
For tktk 1tk+1KalmanﬁlterpredictionKalmanﬁlterpredictionfeedbacklagX[tk]X[tk 1]U[tk 1]U[tk 2]X[tk 2]U[tk]Latent Force Modelling of Natural Sounds Fig.
2.
LFM applied to a metal impact sound, with mean and 95% conﬁdence of the latent distribution shown.
The mean is passed through the model (5) to reconstruct the envelopes.
Complex behaviour is maintained despite using a single force.
example, if P = 10, we may manually ﬁx Bmp to zero for p ∈ [3, 4, 6, 7, 9] such that only half the parameters are included in the optimisation procedure.
Reliability of the optimisation procedure suﬀers as the number of parameters increases, so in practice all M frequency channels are not optimised together.
We select the 6 envelopes contributing the most energy and train the model on the observations from only these channels.
The remaining channels are then appended on and optimised whilst keeping the already-trained parameters ﬁxed.
This improves reliability but prioritises envelopes of high energy.
We also skip prediction steps for periods of the signal that are of very low amplitude, which speeds up the ﬁltering step.
Despite these adjustments, optimisation still takes up to 3 days for a 2 second sound sample.
4 Evaluation To evaluate our method we collated a set of 20 audio recordings, selected as being representative of everyday natural sounds2.
Music and speech sounds were not included, nor were sounds with signiﬁcant frequency modulation, since our model doesn’t capture this behaviour.
4.1 Reconstruction Error of Original Sound We analyse our ability to reconstruct the original data by projecting the latent representation back to the output space.
For the LFM this means passing the mean of the learnt distribution through model (5).
Fig.
4 shows reconstruction RMS error and cosine distance of LFM and tNMF relative to NMF for the 20 recordings.
The smoothness constraint enforced by placing a GP prior over the latent functions negatively impacts the reconstruction.
This is demonstrated by the fact that tNMF performs poorly from an RMS error perspective.
Despite this, the LFM has much descriptive power, and is sometimes capable of achieving a lower RMS error than the unconstrained NMF.
Interestingly however, tNMF consistently outperforms the other two models based on cosine distance.
2 From freesound.org and from the Natural Sound Stimulus set: mcdermottlab.mit.
edu/svnh/Natural-Sound/Stimuli.html 00.050.10.150.2Time(seconds)00.050.10.15AmplitudeAmplitudeenvelopes00.050.10.150.2Time(seconds)PredictedlatentforceUncertaintyMean00.050.10.150.2Time(seconds)Reconstructedamplitudeenvelopes8 Latent Force Modelling of Natural Sounds Fig.
3.
LFM generative model with 3 latent forces applied to an applause sound.
The high-level modulator (black line in (b)) is calculated by demodulating the latent forces.
4.2 Listening Test for Novel Sounds Objective results suggest that smoothness constraints harm reconstruction of the original signal.
However, our aim is to learn realistic latent representations that will be the foundation of a generative model.
To test their suitability, we designed an experiment to compare generative models based on LFM, NMF and tNMF.
The approach outlined in Section 3.2 was used for all model types.
Since NMF is non-probabilistic, it does not provide an immediate way in which to sample new data, therefore GPs were ﬁt to the latent functions after analysis.
Our experiment followed a multi-stimulus subjective quality rating paradigm3: 24 participants were shown 20 pages (order randomised), one per sound example, and asked to listen to the reference recording and then rate 7 generated sounds (2 from each model plus an anchor) based on their credibility as a new sound of the same type as the reference.
Ratings were on a scale of 0 to 1, with a score of 1 representing a very realistic sound.
Fig.
5 shows the mean realism ratings.
Whilst variation was large between sound examples, LFM was generally rated as more realistic than the other methods.
To test for signiﬁcance we applied a generalised linear mixed eﬀects model (GLMM), with beta regression, in which sound example and participant were treated as random eﬀects.
Table 1 shows that the mean realism rating was highest for LFM regardless of number of latent functions.
The diﬀerence was signiﬁcant at a 5% level except for LFM vs.
NMF with 3 latent functions.
This suggests that for sounds requiring many latent functions to capture their behaviour, such 3 The test was run online and implemented with the Web Audio Evaluation Tool: github.com/BrechtDeMan/WebAudioEvaluationTool Latent Force Modelling of Natural Sounds Fig.
4.
Reconstruction error of LFM and tNMF plotted relative to NMF.
Crosses represent the median, error bars range from ﬁrst to third quartile.
as textural sounds, LFM may not oﬀer a signiﬁcant gain over purely statistical approaches.
For example, the wind recording in Fig.
5, a textural sound whose envelopes do not exhibit clear exponential decay, was captured best with tNMF.
All sounds 1 latent fn.
2 latent fns.
3 latent fns.
LFM vs.
NMF 0.2052 LFM vs.
tNMF 0.4987 <1e-04 0.7976 <1e-05 0.5134 <0.001 0.3243 0.1191 NMF vs.
tNMF 0.1148 Estimate p value Estimate p value Estimate p value Estimate p value 0.2867 0.0285 0.7154 -0.0272 0.3839 <1e-04 0.8248 <1e-05 0.3140 0.0448 0.3750 0.9980 0.1994 0.3218 Table 1.
GLMM with three-way comparison applied to listening test results.
LFM received higher mean ratings, but conﬁdence decreases with number of latent forces, indicated by increasing p values.
Estimate can be interpreted as the ratio increase in realism rating when choosing model A over model B.
5 Conclusion Our results show that in order to extend existing synthesis techniques to a larger class of sounds, it is important to utilise prior knowledge about how natural sound behaves.
We achieved this by using latent force modelling to capture exponential decay, and augmented the standard approach to include feedback and delay across many discrete time steps.
Doing so allowed us to make smooth, sparse latent predictions that we argue are more representative of the real source that generated a given sound.
This claim is supported by the fact that a generative model based on LFM was consistently rated as more realistic by listeners than alternatives based on variants of NMF, even in cases where it was not superior in reconstruction of the original signal.
Resonance, decay and modulations in the subband amplitudes were captured well by our model, which is ﬂexible enough to be applicable to sounds ranging from glass breaking to dogs barking.
The nonlinear ODE representing our physical knowledge contains a large number of parameters, making our approach impractical in some cases, so a more compact model would be of huge beneﬁt.
Eﬃcient nonlinear ﬁltering methods or numerical ODE solvers would make the computation time more acceptable.
Future work includes amplitude behaviour occurring on multiple time scales at once, and models for frequency modulation and other nonstationary eﬀects would further expand the class of sounds to which such techniques can be applied.
123Numberoflatentfunctions024RMSerrorrelativetoNMF123Numberoflatentfunctions-101CosinedistancerelativetoNMFNMFLFMtNMF10 Latent Force Modelling of Natural Sounds Fig.
5.
Mean realism ratings obtained from the listening test.
References 1.
McDermott, J.H., Simoncelli, E.P.: Sound texture perception via statistics of the auditory periphery: Evidence from sound synthesis.
Neuron 71(5) (2011) 926–940 2.
Turner, R.E.: Statistical Models for Natural Sounds.
PhD thesis, UCL (2010) 3.
Alvarez, M.A., Luengo, D., Lawrence, N.D.: Latent force models.
In: Int.
Conf.
on Artiﬁcial Intelligence and Statistics (AISTATS).
Volume 12.
(2009) 9–16 4.
Wilkinson, W.J., Reiss, J.D., Stowell, D.: Latent force models for sound, Int.
Conf.
on Digital Audio Eﬀects (2017) 5.
Hartikainen, J., S¨arkk¨a, S.: Sequential inference for latent force models.
In: Int.
Conf.
on Uncertainty in Artiﬁcial Intelligence (UAI-11).
(2011) 311–318 6.
F´evotte, C., Bertin, N., Durrieu, J.L.: Nonnegative matrix factorization with the itakura-saito divergence: With application to music analysis.
Neural Computation 21(3) (2009) 793–830 7.
Smaragdis, P., Brown, J.C.: Non-negative matrix factorization for polyphonic mu- sic transcription.
In: WASPAA.
(2003) 177–180 8.
Virtanen, T.: Monaural sound source separation by nonnegative matrix factoriza- tion with temporal continuity and sparseness criteria.
IEEE transactions on audio, speech, and language processing 15(3) (2007) 1066–1074 9.
Turner, R.E., Sahani, M.: Time-frequency analysis as probabilistic inference.
IEEE Transactions on Signal Processing 62(23) (2014) 6171–6183 10.
Mysore, G.J., Smaragdis, P., Raj, B.: Non-negative hidden markov modeling of audio with application to source separation.
In: LVA/ICA, Springer (2010) 140–148 11.
Badeau, R.: Gaussian modeling of mixtures of non-stationary signals in the time- frequency domain (HR-NMF).
In: WASPAA.
(2011) 253–256 12.
Alvarez, M.A., Luengo, D., Lawrence, N.D.: Linear latent force models using Gaus- sian processes.
IEEE Transactions on pattern analysis and machine intelligence 35(11) (2013) 2693–2705 13.
Rasmussen, C.E., Williams, C.K.I.: Gaussian Processes for Machine Learning.
MIT Press (2006) 14.
S¨arkk¨a, S.: Bayesian Filtering and Smoothing.
Cambridge University Press (2013) 15.
Hartikainen, J., Seppanen, M., Sarkka, S.: State-space inference for non-linear latent force models with application to satellite orbit prediction.
In: 29th Interna- tional Conference on Machine Learning (ICML).
(2012) 903–910 16.
Turner, R.E., Sahani, M.: Demodulation as probabilistic inference.
IEEE Trans- actions on Audio, Speech, and Language Processing 19(8) (2011) 2398–2411 LFMNMFtNMF00.20.40.60.81Realismrating1latentfunctionCymbalMetalHammeringGunshotBasketballLFMNMFtNMF2latentfunctionsBreathingBellGlassDogFrogKeysStairsLFMNMFtNMF3latentfunctionsGravelWindChimesApplauseCameraStairsDishesBikeHorn
relaxation).
QRS  complex  Electrocardiogram  (ECG)  represents  electrical  activity  of human heart.
It is composite from 5 waves: P, Q, R, S  and  T.
P  wave  is  the  Atria  depolarization  (Atrial  Contraction),  T  wave  is  the  depolarization  of  ventricles  (Ventricular  represents  ventricles  depolarization  (Ventricular  contraction).
The  QRS complex, ST segment, PR interval, RR interval, PR  segment,  QT  interval  are  the  most  important  sections  in  an  ECG  signal  for  the  diagnosis  of  different  cardiac  diseases,  especially  arrhythmia.
An  arrhythmia  is  an  alteration  of  the  regular  rate  or  rhythm  of  the  heartbeat.
feature  extraction  pre-processing,   Automatic ECG classification is an emerging tool for  the  cardiologists  in  medical  diagnosis  for  effective  treatments.
Traditional  methods  to  classify  ECG  signal  include  and  classification  steps.
Various  kinds  of  noise  and  artefacts  are  first  removed,  and  then  the  signal  is  segmented  to  calculate  features  vectors  over  time,  possibly  using  a  feature  reduction  algorithm  to  reduce  dimensionality.
Finally,  for classification  the  features  form the input to a  machine  learning  algorithm  such  as  Support  Vector  Machines  (SVMs)  [2],  Neural  Networks  (NNs)  [3]  or  ensemble learning [4].
Deep learning (DL) based neural network models have  achieved  great  success  in  multiple  fields  such  as  natural  language  processing,  computer  vision,  biomedical  signal  processing  and  others.
DLs  can  overcome  the  challenge  of  the  often  tedious  feature  engineering  task  and  helps  with parameterizing traditional NN with many layers [5].
Convolution  Neural  Networks  (CNNs)  and  Recurrent  Neural  Networks  (RNNs)  especially  Long  Short-term  Memory  Network  two  widely  used  architectures  learn  appropriate  filters  that  reduce  the  input  dimensionality,  while LSTMs are useful to model system dynamics [5].
these  models.
CNNs  (LSTM)  are  among  Many  approaches  have  been  performed  to  classify  various  cardiac  arrhythmias  that  used  DL  models.
Authors  in  [6]  developed  a  new  model  that  predicted  12  arrhythmias  from  single-lead  ECG.
It  consisted  of  34- layer  CNN  that  maps  a  sequence  of  ECG  samples  to  a  sequence  of  rhythm  classes.
A  new  neural  network  architecture was proposed in [7] for anomaly detection in  ECG  time  signals.
It  consisted  by  stacking  multiple  recurrent  LSTM  layers.
Another  approach  based  on  DL  for  classification  of  ECG  signals  was  proposed  in  [8].
It  consisted  of  two  layers:  feature  representation  layer  and  softmax regression layer.
The feature representation layer  was  trained  from  the  raw  ECG  data  in  an  unsupervised  way  employing  stacked  denoising  autoencoders  with  sparsity constraint.
In this paper, we propose a novel deep neural network  longer  that  combines  CNN  and  LSTM  to  effectively  learn  sequence  data  containing  term  patterns  of  unknown  length  extracted  from  ECG  signals.
The  model  does  not  require  explicit  pre-processing,  but  can  adaptively  discover  hidden  structures  of  different  ECG  entities and automatically learn their dependencies.
.
The  output  of  a  one-layer  CNN  is  fed  into  a  stack  of  three  recurrent LSTM layers.
The CNN is constructed on top of  the  signal  vectors  from  a  large  corpus  of  ECG  data  to  learn  higher-level  representations  of  PQRST  regions.
In  our  approach,  the  task  is  formulated  as  a  temporal  sequence  predicting  problem  that  can  be  solved  under  a  sequence-to-sequence  learning  framework.
The  new  model  classifies  ECG  signals  into  normal  sinus  rhythm  (N),  atrial  fibrillation  (A),  an  alternative  rhythm  (O),  or  noisy (~).
We used the PhysioNet Challenge 2017 dataset  [9]  which  consists  of  8528  ECG  signals  sampled  at  300  Hz and mostly lasting between 30 and 60 s although there  were  some  records  as  short  as  6  s.
We  will  refer  to  our  proposed deep architecture by  CL3 (One  CNN and  three  LSTMs).
The rest of the paper is organized as follows.
Detailed  descriptions of our proposed deep model are presented in  sections  2.
Section  3  reports  experimental  settings  and  evaluation  metrics,  and  discusses  experimental  results.
Finally,  conclusions  and  future  directions  are  outlined  in  section 4.
2.
CL3 Model   The  architecture  of  the  CL3  model  is  illustrated  in  Figure 1, which consists of two main components: one is  representation  learning,  CNN,  and  the  other  one  is  the  sequence  learning,  a  stack  of  three  LSTMs.  After  applying  one-layer  CNN  on  the  raw  input  sequence  to  extract  local  and  discriminative  features,  three  layers  of  LSTMs are put up on top of the previous CNN to encode  the sequential patterns.
Then, one dense layer is added to  process the output of the third LSTM.
Finally, a softmax  function is adopted to predict the class.
2.1. Model architecture  2.1.1. Representation learning  rectified  This component consists of CNN and one max-pooling  layer.
The  convolution  layer  performs  three  operations  sequentially:  1D-convolution  with  its  filters,  batch  normalization,  with  (ReLU)  activation.
Pooling  layer  downsamples  the  inputs  using  the  max  operation.
There  are  two  reasons  to  use  a  max- pooling layer here.
First, it reduces computation for upper  layers.
Second, it can extract local dependencies and keep  the  most  salient  information.
The  obtained  vectors  are  then  fed  to  the  second  component  which  is  sequence  linear  unit  learning.
We  used  10  filters  to  be  comparative  to  other  filtering  schemes  (e.g.,  the  use  of  up  to  10  PCA  or  wavelet  components  is  typical  for  biomedical  signals).
All system parameters including the number of filters and  pooling size can be found in section 3.
Dropout and Batch  Normalization  blocks  are  also  explained  later  in  section  2.2.  Each  ECG  recording  is  considered  a  univariate  time  series  and  it  is  denoted  by  X={x1,  x2,…,xN},  where  N  represents the length of the ECG signal.
CNN extracts the  i-th feature ai from the i-th ECG sample xi as follows:   𝑎𝑖 = 𝐶𝑁𝑁𝜃(𝑥𝑖)  where  CNNθ(xi)  is  a  function  that  transforms  an  ECG  signal  into  a  feature  vector  ai  using  a  CNN  with  θ  parameter  to  represent  the  number  of  filters.
These  features  vectors {a1, a2,…, aN} are then  forwarded to the  sequence learning component.
(1)  Figure 1.
CL3 architecture.
2.1.2. Sequence learning  This component is created by stacking multiple LSTM  hidden  layers  on  top  of  each  other,  with  the  output  sequence of one layer forming the input sequence for the  next.
Here  three  LSTM  layers  are  used.
Specifically,  input  of  the  upper  LSTM  layer  (u)  takes  hm(t)  from  the  middle  LSTM  layer  (m),  and  again  the  middle  LSTM  layer  takes  hl(t)  from  the  lower  LSTM  layer  (l).
The  outputs of the second layer and the third layer are as same  operation as LSTMl and LSTMm.   Formally,  suppose  there  are  N  features  obtained  from  the  CNN  {a1,  a2,…,aN}  organized  sequentially  and  t=1…N  denotes  the  time  index  of  ECG  samples,  the  sequence learning component is defines as follows:  𝑙 , 𝑐𝑡 ℎ𝑡 𝑚, 𝑐𝑡   ℎ𝑡 𝑢, 𝑐𝑡 ℎ𝑡 1 = 𝐿𝑆𝑇𝑀𝜃𝑙 𝑚 = 𝐿𝑆𝑇𝑀𝜃𝑚 𝑢 = 𝐿𝑆𝑇𝑀𝜃𝑢 𝑙 (ℎ𝑡−1 𝑚 (ℎ𝑡−1 𝑢 (ℎ𝑡−1 , 𝑐𝑡−1 𝑚 , 𝑐𝑡−1 𝑢 , 𝑐𝑡−1 , 𝑎𝑡)    𝑚 , ℎ𝑡 𝑙 )  𝑢 , ℎ𝑡 𝑚)  (2) (3)  (4)  where  LSTM  denotes  a  function  that  processes  sequences of  features at  using a  stacked of three  LSTMs  parametrized by θl, θm and θu for lower, middle and upper  layers;  h  and  c  are  vectors  of  hidden  and  cell  states  of  LSTMs;  ℎ0  are set to zero vectors.
𝑢 𝑎𝑛𝑑 𝑐𝑁+1 𝑙 , ℎ𝑁+1 𝑙 , 𝑐0 A  dense  layer  forms  the  final  layer  and  its  output  is  passed  to  a  softmax  function  whose  output  is  the  probability distribution over labels [5].
2.2. Learning CL3 for ECG Classification  The  entire  model  is  trained  by  minimizing  the  cross- entropy  error.
Given  a  training  sample  x  and  its  corresponding label y∈{1,2,…,K} where K is the number  of  possible  labels  (ECG  classes),  and  the  estimated  probabilities 𝑦̃𝑗 ∈ [0,1]  for each label j ∈{1, 2, …, K}, the  error is defined by equation 5, where 1{condition} could  be  1  or  0.
We  used  the  Root  Mean  Square  Propagation  (RMSprop)  optimization  method  that  calculates  the  magnitude  of  recent  gradients  to  normalize  the  gradients  to  optimize  model  parameters  over  following  objective function:  the  𝑗=1     1{𝑦 = 𝑗}log (𝑦̃𝑗) 𝐿(𝑥, 𝑦) = ∑ (5)  Dropout  was  employed  to  prevent  overfitting  during  training: it was applied to the input of each LSTM, while  Recurrent  Dropout  was  employed  to  drop  neurons  directly  in  recurrent  connections  of  each  LSTM  Batch  Normalization  was  used  to  keep  values  in-bounds  and  avoid  saturation  at  the  various  processing  steps.
It  was  performed  on  the  outputs  of  the  CNN  and  all  LSTM  layers.
therefore  we  defined  a  maximum  Efficient  batch-oriented  training  requires  fixed-length  input;  length  (max_length)  for  the  ECG  signals  in  the  dataset.
Each  ECG  signal  that  has  a  length  less  than  max_length  was  padded  with  zeros.
Zero  label  is  also  added  to  the  sequence  of  labels  to  be  referred  as  PAD  class.
On  the  other  hand,  those  ECG  signals  that  are  longer  than  max_length  were  split  into  multiple  sequences  in  such  that  each  new  sequence  length  was  less  than  or  equal  to  max_length.
Because a large majority of signals had 9000  samples  (30  s),  and  a  significant  number  were  twice  this  length (18000), we chose a max_length of 9000.
The  class  vector  now  contains  five  different  labels  [PAD, N, AF, O, ~] which are one-hot encoded.
The final  target sequence for each input sequence is constructed by  repeating  the  binary  vector  rep_length  times,  defined  by  equation 6.
𝑟𝑒𝑝_𝑙𝑒𝑛𝑔𝑡ℎ = 𝑚𝑎𝑥_𝑙𝑒𝑛𝑔𝑡ℎ/𝑡𝑎𝑟𝑔𝑒𝑡_𝑓𝑎𝑐𝑡𝑜𝑟   where target_factor is determined experimentally.
The  final  train  input  shape  is  (Number  of  sequences  to  be  trained*  max_length*1),  where  1  refers  the  single  input  signal dimension, while the final output shape is given by  (Number  of  sequences  to  be  trained  *  rep_length  *  number of classes).
(6)  3.
Experimental settings and results  Our deep model was implemented in Python using the  Keras library with a TensorFlow backend, which provides  efficient  functionality  on  CPUs  and  GPUs.  The  hyper- parameters  were  chosen  according  to  experiment  results  and they are as follows:  a)  Batch  size  is  500,  Max_length  of  input  sequence  is  9000,  and  the  weight  matrix  is  initialized  to  1.
b)  For  CNN layer, both the number of filters and kernel size are  equal to 10.
c) For Max pooling layer, the pool size is 18.
d) For the three LSTM layers, the number of cells is equal  to 100 and both dropout and recurrent dropout are 0.1. e)  For  BN  layers,  0.99,  0.001,  zeros,  ones,  zeros,  ones  are  used  for  momentum,  epsilon,  beta  initializer,  gamma  initializer,  moving  mean  initializer  and  moving  variance  initializer,  respectively  [5].
f)  For  the  dense  layer,  a  normal  distribution  centered  on  zero  is  used  for  kernel  initializer  and  output  shape  is  (9000,  500,  5).
500  is  the  result  of  the  target_factor=18  sample  decimation  effect  of  the  max  pooling  layer,  while  5  represents  number  of  classes.
The  classifier  performance  was  evaluated  in  terms  of  F1-measure and it is calculated by  F1=2*(P*R)/(P+R).
It  represents the harmonic mean of Precision (P) and Recall  (R).
P  is  the  ratio  of  true  positives  to  all  predicted  positives  and  given  by  P=TP/(TP+FP).
For  more  information, refer to [9].
Table 1.
Results of different experiments and entries.
σ is  the cross-validation F1 standard deviation.
10-folds CV  #  Model  Entry  Network  variant(s)  Class  σ (%)  F1   (%)  F1  (%)  1  CL3  Batch  size=500  I  2  CL3  II  3  CL3  Overall  Batch  size=500 &  Class  weight  Overall  Dilated  CNN=2  N  AF  O  ~  N  AF  O  ~  N  AF  O  ~  92  85  75  90.10  0.30  76.00  6.20  75.20  3.50  47.10  10.90  -  83.10  1.50  90.20  1.00  75.90  6.50  75.50  3.70  48.50  1.34  83.30  2.20  89.70  0.80  75.70  5.50  74.10  3.40  47.10  9.00  82.60  1.70  84  91  82  74  -  82  92  83  75  -  83  Overall  III  Three  experiments  were  conducted  to  evaluate  the  effectiveness  of  the  proposed  deep  architecture  for  ECG  signal  classification.
Table  1  displays  10-fold  cross  validation  scores  of  those  experiments  along  with  the  3  best  entries  submitted  to  the  challenge  server.
It  can  be  noted  that  Experiment  2  exhibited  the  best  F1  (83.30%),  but  its  standard  deviation  (σ)  (1.80)  is  higher  than  Experiment  1  (1.50).
In  Experiment  2,  imbalanced  class  issue  was  dealt  by  applying  weights  to  misclassification  sequences.
F1  was  better  than  Experiment  1,  but  its  σ  is  higher and as result its Entry II performed poorer than the  first  one  with  F1  of  82%.
Dilated  convolution  layer  was  employed  in  experiment  3  and  its  results  showed  that  it  performed  also  worse  than  Experiment  1;  yielding  lower  F1s  and  higher  σ  with  an  entry  score  that  reached  up  to  83%.
We  can  conclude  that  Entry  I  yielded  our  best  overall  score  of  the  submitted  entries  of  83.10±1.50  in  training  and  82  using  the  hidden  dataset.
Our  final  score  after version 3 relabelling was 80.
showed  that  CL3  is  suitable  for  diagnosis  of  different  cardiac  diseases  with  good  accuracy.
Future  work  will  focus  on  refining  our  model  by  applying  an  ensemble  deep learning framework to decrease information loss and  overfitting  problems,  and  the  class  imbalance problem.
to  overcome  Figure 2.
Class prediction vs original class.
Figure  2  depicts  6  sample  ECG  examples  A00001,  A00002,  A00003,  A00004,  A00005  and  A00006  that  were labelled manually by cardiologists as N, N, N, A, A  and N, and  automatically by our CL3 in Entry I.
It can be  seen  that  for  A00001,  A00002,  A00003  and  A00006  recordings,  CL3  could  predict  perfectly  almost  all  ECG  samples with the same label assigned by the expert, while  for  A00004  and  A0005,  there  are  some  ECG  samples  could  not  be  learnt  completely  by  our  new  deep  architecture.
In addition to that, Figure 3 shows F1 results  over time per class.
It can be noticed from this figure that  the  model  performance  begun  to  degrade  for  classes  ~  and  AF,  from  the  middle  of  training.
This  is  due  to  the  fact  the  minority  classes,  while  performances  of  majority  classes  keep  significantly  increasing  throughout  the  training  (and  supporting  our  decision to use the final prediction for classification).
that  both  are     4.
Conclusions  Figure  3.
F1-measure  curves  per  class  vs  training  prediction for Entry I.
References  [1] Davey P.
ECG at a glance.
John Wiley & Sons, 2013.
[2] Zhao Q, Zhang L.
ECG Feature extraction and classification  using  wavelet  transform  and  support  vector  machines.
ICNN&B'05.
2005; 2:1089 -1092.
[3] Prasad GK, Sahambi JS.
Classification of ECG arrhythmias  using  multi-resolution  analysis  and  neural  networks.
TENCON2003.
2003; 1:  227-231.
[4] Zeng XD, Chao S, Wong F.
Ensemble learning on heartbeat  type classification.
ICSSE2011.
2011; 320-325.
[5]  Buduma  N,  Locascio  N,  Fundamentals  of  Deep  Learning:  Designing Next-Generation Machine, O’Reilly, 2017.
[6] Rajpurkar P, Hannun AY, Haghpanahi M, Bourn C, Ng AY.
Cardiologist-Level  with  Convolutional Neural Networks.
2017; arXiv:1707.01836.
[7] Chauhan S, Vig L.
Anomaly  detection in ECG time signals  Arrhythmia  Detection  via  deep  long  short-term  memory  networks.
DSAA,  2015;    1-7.
[8] Al Rahhal MM, Bazi Y, AlHichri H, Alajlan N, Melgani F,  Yager RR.
Deep learning approach for active classification  of  electrocardiogram  signals.
Information  Sciences.
2016;   1:340-54.
[9]  Gari  Clifford,  Chengyu  Liu,  Benjamin  Moody,  Ikaro  Silva,  Qiao  Li,  Alistair  Johnson,  Roger  Mark.
AF  Classification  from  a  Short  Single  Lead  ECG  Recording:  the  PhysioNet  Computing  in  Cardiology  Challenge  2017.
Computing  in  Cardiology (Rennes: IEEE).
2017;44 (In Press)  A  new  deep  learning  model,  named  CL3,  for  automatic  classification of cardiac arrhythmias based on raw single- lead ECGs is proposed.
CL3 uses CNN to extract features  which  are  introduced  to  a  stack  of  LSTMs  to  learn  automatically  hidden  patterns  from  ECG  epochs  with  very  little  manual  parameter  tuning  required.
Our  results     Address for correspondence.
Philip Warrick, PeriGen.
Inc.
Montreal, Canada.
philip.warrick@gmail.com  Masun Nabhan Homsi, Universidad Simón Bolívar.
mnabhan@usb.ve
Creating a model of a computer system that can be used for tasks such as predicting future resource usage and detecting anomalies is a challenging problem.
Most current systems rely on heuristics and overly simplistic assumptions about the workloads and system statistics.
(cid:140)ese heuristics are typically a one-size-(cid:128)ts-all solution so as to be applicable in a wide range of applications and systems environments.
(cid:140)is limitation is all the more striking considering the wide range of problems that could be approached with a more sophisticated model of a computing system: for example, resource allocators, both process schedulers in OSs and orchestrators in clusters[5, 7, 9, 15], use simple heuristics, and still o(cid:137)en struggle to get performance right[11, 12]; and monitoring systems whose objec- tive is the detection of anomalies have used some machine-learning approaches in network-based scenarios[2, 14, 16], but much less so in the more systems-heavy domain.
Tailoring the prediction models to speci(cid:128)c situations, however, can be extremely complex: they have to take into account the interplay of systems components and concurrently running heterogeneous applications, while being able to adapt to a dynamically and o(cid:137)en abruptly changing state of the system.
Considering developing generic heuristics is already an extremely time-consuming task, creating tailor-made solutions by hand is rarely worth the e(cid:130)ort.
However, there are several recent developments that bring us closer to developing systems models that could perform much be(cid:138)er than existing generic methods based on simple heuristics.
Machine learning is becoming more e(cid:130)ective and e(cid:129)cient at learning from large amounts of data.
Moreover, we have a much be(cid:138)er under- standing of ways to embed heterogeneous feature types (categor- ical, numerical, structured, temporal) into a joint representation amenable to downstream tasks.
If we extract and collect the right input data, we may be able to automatically create tailor-made models that outperform generic heuristics.
With this paper, we present our ongoing work of integrating systems telemetry ranging from standard resource usage statistics to kernel and library calls of applications into a machine learning model.
Intuitively, such a ML model approximates, at any point in time, the state of a system and allows us to solve tasks such as resource usage prediction and anomaly detection.
To achieve this goal, we leverage readily-available information that does not require any changes to the applications run on the system.
We train recurrent neural networks such as Long Short-Term Memory (LSTM) neural networks [8] to learn a model of the system under consideration.
As a proof of concept, we train models speci(cid:128)cally to predict future resource usage of running applications.
2 DATA COLLECTION To learn a model of a system that is good at predicting future resource usage, we need to collect data about the present.
One obvious approach is to collect data about the resources that we want to predict such as CPU and memory usage statistics.
(cid:140)is follows the idea that, in many cases, the previous values of resource usage will have at least some in(cid:131)uence on current resource usage.
For example, memory consumption will o(cid:137)en increase or decrease gradually over time.
CPU usage is o(cid:137)en more spiky, but even here, for many processes, phases of low activity and high activity will be apparent.
Such resource usage information is easily available on Unix-like systems; however, it is generally accounted for on a per-process basis.
(cid:140)is is useful for process scheduling but for more coarse- grained scheduling of jobs or services, which comprise several processes run in sequence or in parallel, we will need aggregate measurements.
For this, we monitor the process group, that is, all processes spawned by one initial process (that do not speci(cid:128)cally request to leave the group).
Aggregation is slightly cumbersome because there is no easy way to look up all processes belonging to a group given the group ID; it instead requires traversing all processes and asking them for the process group they belong to.
(cid:140)is, and the resource information, can be collected from by reading it from /proc/<pid>/stat.
Alternatively, a new cgroup can be created and the initial process spawned into it.
(cid:140)e resource requirements of the process group are then the resources used by the cgroup.
However, these high-level usage statistics alone provide no deeper insights into the state of a process.
It would be useful to have at least a rough understanding of what a process “is doing” at runtime.
Unfortunately, the possibilities here are limited if we want to stay generic and not require ancillary or internal information that is speci(cid:128)c to a certain problem domain (such as information about input data), or requiring speci(cid:128)c compiling or linking steps.
Using a pro(cid:128)ler to measure which functions are being run for how long, for instance, requires a symbol table which is not always available (stripped).
(cid:140)ere are options, however, to inspect program behavior without requiring such additional information.
By analyzing the system calls that a program performs, we can get a rough under- standing of what a process is doing and this information is always available, because it does not rely on code annotation or additional symbols.
Some system calls also have an obvious relationship with certain kinds of resources.
For example, the write, read and similar system calls work on (cid:128)les or sockets, which translates into disk or network I/O.
If we want to predict I/O, the relationship is obvious; but even for CPU usage, there is a strong relationship: for example, disk I/O o(cid:137)en correlates with low CPU usage, since the process is waiting on I/O accesses to the (cid:128)nished.
System calls are also easily traced: strace[10] is a standard tool available on Unix-like systems, and these days, its overhead is low – a few percent when a moderately high number of system calls occurs, to virtually none when there are no system calls happening.
In case of workloads with extremely high numbers of system calls, perf[1] can be used to sample system calls instead of tracing every single one, further reducing the overhead.
Conversely, if a higher level of detail is required, ltrace[3] can be used instead to catch SysML’18, February 2018, Stanford, CA, USA Florian Schmidt, Mathias Niepert, and Felipe Huici Figure 1: To predict resource usage, we collect typical telemetry data (top le(cid:133)), as well as the application’s system calls (bottom le(cid:133)), over a time period t.
(cid:135)e variable number of calls is transformed into a (cid:128)xed-size vector via a word embedding.
(cid:135)e two vectors are combined and used as input for an LSTM that then predicts the resource usage at some future time period t + i.
all library interactions.
As a proof of concept, we developed a ML model that integrates usage statistics and sequences of system calls into a joint representations.
3 DATA PREPROCESSING In order to prepare the data such as usage statistics as input for the ML models, we need to discretize it into time intervals for several reasons.
First, some data only makes sense as values over a time period: what was the CPU utilization in the last second?
How many bytes were wri(cid:138)en to disk?
Second, for the eventual goal of resource allocation, we will also have to predict resource usage over a a time period that the scheduler uses as time slice.
Finally, calculating each resource usage over a certain time period provides us a (cid:128)xed-size value: each information can be interpreted as a single value which then can all be combined into an input vector of (cid:128)xed size.
For system calls, however, such a (cid:128)xed-size representation is not straightforward to generate.
System calls occur at (seemingly) random times and are discrete events as opposed to continuous numerical values.
Within a time period of a second thousands of system calls, or none, can occur.
Fortunately, to transform se- quences of system calls into a (cid:128)xed-sized vector representation, we can use representation learning approaches for sequence data such as the word2vec skip-gram model [13].
Instead of applying these representation learning approaches to sequences of words to learn meaningful vector representations, we can apply these methods to sequences of system calls (and also other types of event sequences occurring a system) to learn representations of these events.
To collect a corpus of system call sequences, we ran strace[10] on the various types of applications run on the system under consideration.
We then used that data to learn system call embeddings through a model similar to the skip-gram model [13].
As a result, we can take all system calls occurring within a time period, interpret them as a “sentence,” and use the event embeddings to create a (cid:128)xed-size vector representation.
Since we now have a (cid:128)xed-sized representa- tion of the resource usage statistics and a (cid:128)xed-size representation for system calls, we can directly use this data to train a ML systems model, as shown in Figure 1 that depicts the overall architecture.
4 NEURAL NETWORKS FOR SYSTEMS MODELING (cid:140)e objective of this work is to learn and maintain a model of a computing system on a particular level of abstraction.
In the end, all systems are state-based and, given a current state, we want Figure 2: Looking farther into the future tends to increase the prediction error, but taking more history into account mitigates this e(cid:130)ect.
to use the model of the system to make predictions about its and its applications future behavior.
Several recent neural network based machine learning architectures maintain some sort of in- ternal state.
Examples are recurrent networks such as LSTMs [8] and variants [4], memory networks [17, 18], and neural Turing machines [6], to name but a few.
We are taking advantage of these methods by developing a system model that maintains a vector (hidden) representation of the current state of the system and is trained so as to minimize the expected error (here: the root-mean- square error (RMSE)) of predicting future resource usage.
To keep the model simple and for the use case of resource usage prediction, we train an LSTM with the collected and preprocessed data con- sisting of past usage statistics and system calls.
(cid:140)e learning of system calls embeddings can be performed as a preprocessing step or within an end-to-end architecture.
We conducted some preliminary experiments by collecting sys- tem calls from various applications to create the system call corpus.
We then collected the resource usage and system calls of a scienti(cid:128)c computing toolchain that executed a number of bash and python scripts, which interleaved I/O- and CPU-heavy phases.
Finally, we embedded the system calls and trained an LSTM with the data.
(cid:140)e model is trained to minimize the RMSE of the CPU usage (as a value between 0 and 1) i seconds into the future.
Figure 2 shows the results, varying both how far to predict into the future, and how much history to take into account for the prediction.
Acknowledgments—(cid:140)is project has received funding from the European Union’s Horizon 2020 research and innovation pro- gramme under grant agreement No 761592.
𝐶𝑃𝑈𝑅𝑆𝑆⋮𝐷𝑖𝑠𝑘𝑤𝑟𝑖𝑡𝑒 stat() open() read() stat() stat() open() read() mmap() mmap() write() write() close() Mem CPU 𝐸𝑚𝑏1𝐸𝑚𝑏2⋮𝐸𝑚𝑏𝑛 𝐶𝑃𝑈𝑅𝑆𝑆⋮𝐷𝑖𝑠𝑘𝑤𝑟𝑖𝑡𝑒𝐸𝑚𝑏1𝐸𝑚𝑏2⋮𝐸𝑚𝑏𝑛 𝐶𝑃𝑈𝑡+𝑖𝑅𝑆𝑆𝑡+𝑖⋮𝐷𝑖𝑠𝑘𝑤𝑟𝑖𝑡𝑒,𝑡+𝑖 ⋮ ⋮  0 0.025 0.05 0.075 0.1 0.125 0.15 0.175 0.2 1 2 3 4 5 6 7 8 10 15Prediction error [RMSE]Prediction into the future [seconds]History taken into account1 second10 secondsRepresentation Learning for Resource Usage Prediction SysML’18, February 2018, Stanford, CA, USA REFERENCES [1] [3] [n.
d.].
perf: Linux pro(cid:128)ling with performance counters.
h(cid:138)ps://perf.wiki.kernel.
org.
([n.
d.]).
[2] A.
L.
Buczak and E.
Guven.
2016.
A Survey of Data Mining and Machine Learning Methods for Cyber Security Intrusion Detection.
IEEE Communications Surveys Tutorials 18, 2 (Secondquarter 2016), 1153–1176.
h(cid:138)ps://doi.org/10.1109/COMST.
2015.2494502 Juan Cespedes et al.
[n.
d.].
ltrace: A library call tracer.
h(cid:138)p://www.ltrace.org/.
([n.
d.]).
Ivo Danihelka, Greg Wayne, Benigno Uria, Nal Kalchbrenner, and Alex Graves.
2016.
Associative Long Short-Term Memory.
In Proceedings of (cid:138)e 33rd Interna- tional Conference on Machine Learning.
1986–1994.
[5] Ali Ghodsi, Matei Zaharia, Benjamin Hindman, Andy Konwinski, Sco(cid:138) Shenker, and Ion Stoica.
2011.
Dominant Resource Fairness: Fair Allocation of Multiple Resource Types.
In Proceedings of the 8th USENIX Conference on Networked Systems Design and Implementation (NSDI’11).
USENIX Association, Berkeley, CA, USA, 323–336.
h(cid:138)p://dl.acm.org/citation.cfm?id=1972457.1972490 [6] Alex Graves, Greg Wayne, and Ivo Danihelka.
2014.
Neural Turing Machines.
[4] CoRR abs/1410.5401 (2014).
[7] Ryan Hnarakis.
2013.
In Perfect Xen: A Performance Study of the Emerging Xen Scheduler.
Master’s thesis.
California Polytechnic State University.
[8] Sepp Hochreiter and J¨urgen Schmidhuber.
1997.
Long Short-Term Memory.
Neural Comput.
9, 8 (Nov.
1997), 1735–1780.
h(cid:138)ps://doi.org/10.1162/neco.1997.9. 8.1735 [9] Michael Isard, Vijayan Prabhakaran, Jon Currey, Udi Wieder, Kunal Talwar, and Andrew Goldberg.
2009.
(cid:139)incy: Fair Scheduling for Distributed Computing Clusters.
In Proceedings of the ACM SIGOPS 22Nd Symposium on Operating Systems [11] Principles (SOSP ’09).
ACM, New York, NY, USA, 261–276.
h(cid:138)ps://doi.org/10.
1145/1629575.1629601 [10] Paul Kranenburg, Dmitry Levin, et al.
[n.
d.].
strace: Linux syscall tracer.
h(cid:138)ps: //strace.io/.
([n.
d.]).
Jean-Pierre Lozi, Baptiste Lepers, Justin Funston, Fabien Gaud, Vivien (cid:139)´ema, and Alexandra Fedorova.
2016.
(cid:140)e Linux Scheduler: A Decade of Wasted Cores.
In Proceedings of the Eleventh European Conference on Computer Systems (EuroSys ’16).
ACM, New York, NY, USA, Article 1, 16 pages.
h(cid:138)ps://doi.org/10.1145/ 2901318.2901326 [12] Anshul Makkar.
2016.
Scope and Performance of Credit-2 Scheduler.
In Xen Project Developer Summit.
[13] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Je(cid:130) Dean.
2013.
Distributed representations of words and phrases and their compositionality.
In Advances in neural information processing systems.
3111–3119.
[14] Taeshik Shon and Jongsub Moon.
2007.
A Hybrid Machine Learning Approach to Network Anomaly Detection.
Inf.
Sci.
177, 18 (Sept.
2007), 3799–3821.
h(cid:138)ps: //doi.org/10.1016/j.ins.2007.03.025 [15] Chandandeep Singh Pabla.
2009.
Completely Fair Scheduler.
Linux Journal 184 (Aug.
2009).
[16] Robin Sommer and Vern Paxson.
2010.
Outside the Closed World: On Using Ma- chine Learning for Network Intrusion Detection.
In Proceedings of the 2010 IEEE Symposium on Security and Privacy (SP ’10).
IEEE Computer Society, Washington, DC, USA, 305–316.
h(cid:138)ps://doi.org/10.1109/SP.2010.25 [17] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus.
2015.
End- To-End Memory Networks.
In Advances in Neural Information Processing Systems.
2440–2448.
Jason Weston, Sumit Chopra, and Antoine Bordes.
2014.
Memory Networks.
CoRR abs/1410.3916 (2014).
[18]
In recent years, neural networks with many hidden lay- ers, which are usually called deep neural networks, have been used in a lot of machine learning and visual comput- ing tasks, delivering unprecedented good results [15, 3, 11].
Unlike other machine learning approaches relying on linear models (possibly with kernel extensions), neural networks are inherently nonlinear models because of the nonlinear activation functions used in the neurons.
Properties and ap- plications of neural networks have been studied [1].
Some earlier, theoretical results, such as the Universal Approx- ∗Indicates equal contributions.
imation Theorem, which states that, any continuous func- tion on a compact subset of Rn can be approximated by a feed-forward network with a single hidden layer containing a ﬁnite number of neurons [5, 13], have alluded to the po- tential of deep neural networks, although much more and better understanding has yet to be developed.
Recent years have witnessed various efforts to this end.
In particular, there are studies aiming at investigating the reason why deeper networks generally perform better.
It was shown that [6], there exist families of functions which can be approximated much more efﬁciently by a deep neu- ral network than a shallow one, when the number of hid- den units in the networks is kept the same.
In [22, 19], researchers investigated the relationship between the the depth of the neural networks and the complexity of func- tions that are computable by the networks.
It was also shown that, when using Rectiﬁed Linear Unit (ReLU) [7] as the activation function, the number of response regions of the neural network grows exponentially in the number of hidden layers but in polynomial in the number of neurons in one layer.
This also appears to be intuitive, as the more number of hidden layers a network has, the more number of hierarchical nonlinear mappings the inputs will go through, hence enabling a more ﬂexible representation by the net- work.
One thread of research towards increasing the ﬂexi- bility of the networks is by using a potentially better activa- tion function, which is the source of the nonlinearity of the model.
Despite the widespread usage of ReLU, the research on how the activation functions may affect the ﬂexibility of a network has not been well discussed.
In this paper, we propose a novel activation function named Power Linear Unit (PoLU).
We prove that networks using PoLU are able to increase the maximal number of re- sponse regions.
We further compare PoLU to other state- of-the-art activation functions in different networks and on different datasets.
Experimental results demonstrate that PoLU outperforms almost all leading activation functions that have been widely used.
Figure 1.
(a): the plot of PoLU (n = 2), ReLU and ELU.
(b): the plot of PoLU for n = {1, 1.5, 2}.
(c): the plot of the derivatives of PoLU for n = {1, 1.5, 2} 2.
Related Work Rectiﬁed Linear Unit (ReLU), which is called ramp function sometimes, was ﬁrst applied to restricted Boltz- mann machine (RBM) in [20], and to neural networks [7] later.
It is among the most popular activation function nowadays.
The ReLU activation function can be deﬁned as f : R → R, where f (x) = max(0, x), and x is the input of the neuron.
Comparing to traditional activation functions like the logistic sigmoid units or hyperbolic tangent units, which are anti-symmetric, ReLU is one-sided.
This prop- erty encourages the network to be sparse (i.e. the outputs of the hidden units are sparse), and thus more biologically plausible.
In the experiments stated in [7], the sparsity of the network can be from 50% to 80%.
Using ReLUs as activation functions also decreases the computational cost, as they can be implemented via IF statements.
Most im- portantly, ReLU alleviates the problem of vanishing gradi- ent [12], as the derivative of its positive part is always 1.
This vanishing problem occurs in sigmoid and hyperbolic tangent units, where the gradients vanish to 0 after some epochs of training (due to the horizontal asymptotes) and stop the learning in the corresponding parts of the network.
Although ReLU has several advantages, there are also some potential problems.
For instance, ReLUs can ”die” sometimes: once a neuron outputs 0, the corresponding weights may not be updated again, since the gradients are also 0.
Another issue is that, since ReLU is non-negative, the mean of the outputs of ReLUs in a layer will be positive, which leads to the bias shift effect [4], and may decrease the speed of learning.
To overcome these problems, researchers proposed some variants of ReLU.
Leaky Rectiﬁed Linear Unit (LReLU) [18] sets the output to be directly proportional to the input with a very small proportional constant α (i.e. α = 0.01).
This is equivalent to f (x) = max(x, αx).
Under this def- inition of activation functions, the neurons won’t ”die” as the gradient is small but still non-zero.
Parametric Recti- ﬁed Linear Unit (PReLU) [9] makes α to be a learnable parameter instead of a ﬁxed constant.
Since PReLU needs extra space and time complexity to learn αs, Randomized Leaky Rectiﬁed Linear Unit (RReLU) [25] was proposed, where α is a random number sampled from a uniform dis- tribution.
Shifted Rectiﬁed Linear Unit (SReLU) and Ex- ponential Linear Unit (ELU) [4] both try to make the mean activation towards 0 such that the gradients are closer to the natural gradient [16], and hence speeding up the learning process.
ELU also makes the gradient of its negative part to be non-zero to avoid dead neuron.
However, considering the function of ELU in the negative part, α(ex − 1), we can see that α represents the slope of the function at x → 0−, and the saturation value (i.e. the value of y when x → −∞) will be changed simultaneously if we vary the value of α.
So, it is impossible to change the slope of the curve around x = 0 while keeping the asymptote y = −1.
In the lat- ter section, we demonstrate that under different saturation value, the performance of y = −1 is a better choice for ELU (see Fig.3).
Therefore, we would like to develop an activation function where the slope at x → 0− is indepen- dent of the asymptote of saturation.
For our proposed Power Linear Units (PoLUs), we adopt the advantages of the activation functions mentioned above.
First, the output of PoLUs for positive input is designed to be identity to avoid gradient vanishing problem.
Second, PoLUs have non-zero output for negative inputs, such that the output mean of the units is close to zero, and thus reduce the bias shift effect.
Thirdly, there is a saturation on the neg- ative part of PoLU, which makes it more noise-robust for negative inputs.
Last but not least, PoLUs are able to map more portions of every layer’s input space to the same range by using power function and thus increase the response re- gions of the neural network.
More details are to presented in the next section.
3.
Power Linear Unit In this section we propose PoLU and analyze the rela- tion between the PoLU and the number of response regions.
Based on [19], we redeﬁne some terms such that the idea can be extended to a more general case.
We ﬁrst start by giving the deﬁnitions.
Deﬁnition 1.
Let fn : R → R be a function, the Power Linear Unit (PoLU) can be deﬁned as follows: (cid:40) (cid:40) fn(x) = (1 − x)−n − 1 and its derivative can be expressed as: f(cid:48) n(x) = n(1 − x)−n−1 if x ≥ 0 if x < 0 if x ≥ 0 if x < 0 (1) (2) The parameter n controls the rate of change of PoLUs at the negative part.
Fig.1(b) and (c) show the plots of PoLU and the derivative of PoLU under different power values n.
Similar to many previously proposed models of activation functions, keeping the identity as the positive section helps PoLUs against the gradient vanishing problem.
PoLUs also have non-zero outputs and a saturation plateau for negative inputs.
These not only increase their ability to learn a stable representation, but also make the mean of the output of the units closer to zero, which reduces the bias shift effect [4].
In contrast to the previously proposed activation functions, PoLU has an intersection to y = x at its negative regime when n > 1, which is proven to be good for increasing the number of response regions (see the proof of Theorem 2).
Although ELU also has the same property when α > 1, using such value of α may also push the mean activation away from zero (as α is a scaling factor), which typically leads to worse performance.
Deﬁnition 2.
[19] Let F : RM → RN be a map, S ⊆ RM and T ⊆ RM .
F identiﬁes S and T if F (S) = F (T ).
Deﬁnition 3.
A response region of a function F is a maxi- mal connected subset of the domain on which F is differen- tiable and monotonic.
As our proposed activation function is nonlinear for neg- ative input, we deﬁne response region in a more general way (i.e. requiring F to be differentialbe and monotonic instead of being just linear).
Under this deﬁnition, Lemma 1 in [22] leads to Theorem 1, and the proofs in [22, 19] still hold un- der deﬁnition 3.
Theorem 1.
[22] The maximal number of response regions of the functions computed by a neural network, which has n0 input units, one hidden layer with n1 PoLUs, is bounded below by(cid:80)n0 j=0 (cid:0)n1 (cid:1).
Figure 2.
The plots of different curves for n = 2.
(a) blue: y = fn(x); red: y = fn(−x); (b) y = ˆϕn(x); (c) y = ϕn(x, 0.5).
The distance from the troughts to the origin is 0.5; (d) blue: y = S2(x); red(upper): y = b; red(lower): y = a.
The intervals which are subsets of {x | a < S2(x) < b} are mapped to the same set.
Theorem 2.
The maximal number of response regions of the functions computed by a neural network, which has n0 input units, L hidden layers with ni ≥ n0 PoLUs at the i-th layer, is bounded below by (cid:32)L−1(cid:89) (cid:22) ni (cid:23)n0(cid:33) n0(cid:88) (cid:18)nL (cid:19) n0 i=1 j=0 2n0(L−1) (3) Proof.
We ﬁrst start with two PoLUs. Let n > 1 and ˆϕn : R → R be a function deﬁned as: ˆϕn(x) = fn(x) + fn(−x) (4) It is easy to prove that there is an intersection to y = x at the negative region of PoLU for n > 1, which causes two local minima exist in y = ˆϕn(x).
While a function formed by two ReLUs, following the construction in [19], can only identify two regions, ˆϕn, which is formed by two PoLUs, can identify four regions.
As the inputs always go through afﬁne maps before they reach the activation functions, we choose the weights and bias of these afﬁne transformations.
Consider the modiﬁed version of ˆϕn, ϕn : R × [0, 1) → R, which is deﬁned as follows: ϕn(x, d) = fn(an(d)x + bn(d)) + fn(−an(d)x + bn(d)) (5) We can always choose some suitable a > 0 and b > 0 to (i) rescale ϕ such that ϕ(−1, d) = ϕ(0, d) = ϕ(1, d).
(ii) separate the two troughs by a horizontal line with its length equals to 2d < 1, Fig.
2(c) is the plot of ϕn(x, 0.5), the distance from the troughs to the origin is equal to 0.5. Consider a layer of n1 PoLUs with n0 input, where n1 ≥ n0.
The PoLUs are separated into n0 disjoint subsets of cardinality p, which is the largest even number not greater than n1/n0 (i.e. p = 2k ≤ (cid:98)n1/n0(cid:99) for some k ∈ N), the remaining PoLUs are ignored.
Without loss of generality, we consider the j-th input of the layer, where j ∈ {1, ..., n0}.
As we can choose the input weights and biases, the functions can be constructed in the following way: h1(x) = ϕn(x, d1) h2(x) = ϕn(x, d2) ...
hk(x) = ϕn(x, dk) (6) and let (−ci, hi(−ci)) and (ci, hi(ci)) be the coordinates of the local minima of hi(x) for i ∈ {1, 2, ..., k}.
We use a pair of fn to construct one function.
Fig.2(a) and (b) show the plots of h1 and h2 respectively.
We can construct a function deﬁned as follows: SN (x) = aihi(x) ∀N ∈ {1, ..., k} N(cid:88) i=1 ˜h(x) = Sk(x) (7) (8) For h1, we set d1 = 0 such that the two troughs are stick together.
For h2, we set d2 > c1 to ensure that there are four local minima for the function y = S2(x) = a1h1(x) + a2h2(x) for some constant a1, a2.
We keep constructing Si with i > 2 in a similar way.
In general, we have d1 = 0 < c1 < d2 < c2 < d3 < c3 < ...
< dk < ck < 1.
Under this setting, we have 2k troughs for y = Sk(x).
By choosing suitable coefﬁcients {ai}, we can construct a function with p = 2k local minima, where they have a same value at y-coordinates.
Fig.2(d) shows the plot of y = S2(x), for the 8 intervals [−1,−c2], [−c2,−d2], [−d2,−c1], [−c1, 0], [0, c1], [c1, d2], [d2, c2], [c2, 1], there exists at least one subset of each interval such that these subsets are mapped onto the same interval by S2.
Now, for the following intervals: [−1,−ck], [−ck,−dk], [−dk,−ck−1], ..., [−c1, 0], [0, c1], [c1, d2], ..., [ck−1, dk], [dk, ck], [ck, 1] (9) there is a subset of each intervals that is mapped onto the same interval.
Therefore, ˜h identiﬁes 2p = 4k regions in the input domain.
Since ˜h is the linear combination of h1, ..., hk, we can treat it as an output of the current layer of the neural net- work.
By considering all the n0 subset, we can conclude that, ˜h identiﬁes 2n0(cid:98)n1/n0(cid:99)n0 regions in total.
Using the same strategy as [22, 19], we can conclude that, the maximal number of response regions computed by the neural network using PoLUs is bounded below by (cid:4) 2n0(L−1)(cid:16)(cid:81)L−1 (cid:107)n0(cid:17)(cid:80)n0 (cid:106) ni (cid:0)nL (cid:1).
i=1 n0 j=0 for This proof holds the neural networks with PoLUs(n > 1) or ELUs(α > 1), as they have intersection with y = x for negative input.
While for the networks with PoLUs(n ≤ 1), ELUs(α ≤ 1) or ReLUs, the lower bounds remain the same as in [19].
As the number of response re- gions of the functions computed by a neural network, is a measure of ﬂexibility of that network, the networks with PoLUs(n > 1) and ELUs(α > 1) are considered to be more ﬂexible.
Note that, even though we prove that the PoLU has more response regions than ELU does, we cannot simply deter- mine that any new activation functions which may increase the number of response regions can have a better perfor- mance.
Because there are many factors that may affect the performance of a network, like the bias shift effect, value of the saturation for negative input, etc.
Hence, ”the larger the number of response regions, the better the network is” is not the ﬁnal conclusion and not the only advantage that PoLU has.
4.
Experiments In this section, we ﬁrst evaluate the impact of α on ELUs by setting α to 0.5, 1 and 2 with the ELU-Network on CIFAR-100 dataset.
We then evaluate PoLUs with different convolutional neural networks using different power values n ∈ {1, 1.5, 2} on ﬁve benchmark datasets: MNIST [17], CIFAR-10 [14], CIFAR-100 [14], Street View House Num- bers [21], and ImageNet [23].
Compared with other state- of-the-art activation functions, including Exponential Lin- ear Units (ELUs), and the most widely used activation func- tion, Rectiﬁed Linear Units (ReLUs), convolutional neural networks with PoLUs present the best performance on all four datasets.
Several deep neural networks with a different number of layers are implemented to demonstrate that Po- LUs are more compatible than ELUs and ReLUs for convo- lutional neural networks.
The experiments are implemented using deep learning toolbox Keras [2] with tensorﬂow back- end.
Note that, we run experiments with activation func- tions SReLU and Leaky ReLU as well, obtained similar re- sults comparable to those reported in the ELU paper (i.e. the accuracy difference is within 1%).
Therefore, to make the Figure 3.
The plots show the results of using simple-ELU-Net on the CIFAR-100 dataset, by using ELU with α ∈ {0.5, 1, 2}.
(a-b) The testing error (c) The plots of ELU under different α.
ELU with α = 1 has the best performance among them.
Table 1.
This table shows the result for MNIST dataset and SVHN dataset.
For SVHN, Simple-ELU-Net and a 4CNN+2NN network are used for testing.
For MNIST, only a 2CNN+2NN is used for testing.
We use ReLU, ELU and PoLU(n = 1, 1.5, 2) for both datasets.
The best result for each network is bold.
SVHN MNIST 4 CNN + 2 NN simple-ELU-Net 0.69(±0.03)% 4.85(±0.07)% 5.18(±0.09)% 5.16(±0.08)% 4.87(±0.05)% 0.99(±0.02)% 4.84(±0.06)% 0.83(±0.03)% 5.02(±0.08)% 4.96(±0.07)% 0.83(±0.02)% 4.71(±0.05)% 4.63(±0.06)% 4.90(±0.07)% 0.87(±0.02)% ReLUs ELUs PoLUs (n=1) PoLUs (n=1.5) PoLUs (n=2) plots more clear and easier to demonstrate the differences between ELU, PoLU and the baseline activation function ReLU, we do not contain the performance of SReLu and Leaky ReLU in the plots.
4.1. MNIST dataset The MNIST dataset [17] contains 60,000 training and 10,000 testing samples with 28 × 28 pixel size.
Each im- age is drawn from greyscale handwritten digits 0-9.
Since the MNIST dataset has been well studied in evaluating dif- ferent neural networks, we utilize this dataset to assess the performance of PoLUs with a relatively shallow neural net- work.
We train a network with two convolutional layers and two densely connected layers followed by a softmax layer which is arranged in stack of ([1× 32× 3], [1× 64× 3], [1× 128×F C], [1×10×Sof tmax]).
A 2×2 max-pooling layer with stride of 2 is applied to the end of the second stack.
We leverage dropout with a ratio 0.5 to regularize the network.
The results are provided in Table 1,and ReLUs achieved the best results with the testing error equaling to 0.69%.
4.2. CIFAR-10 and CIFAR-100 dataset The CIFAR-10 and CIFAR-100 [14] datasets are similar.
Both of these two datasets contain 60,000 color images with 32×32 size, which are split into 50,000 training and 10,000 testing samples.
The only difference between them is that CIFAR-10 is drawn from 10 classes while CIFAR-100 con- tains 100 classes.
Therefore, the same neural network struc- tures followed by different softmax layer are implemented for both datasets.
The neural networks we implemented are (i) the ELU-Network with 11 convolutional layers from [4](named simple-ELU-Net), (ii) the ELU-Network with 18 convolutional layers from [4](named ELU-Net), (iii) A VGG16-structure-like neural network [24], and (iv) Deep Residual Network (ResNet) with 50-layer structure from [10].
As mentioned before, we also implement the ELU- Network with 11 convolutional layers with ELUs assigned with different α values to evaluate the effect of α and the relationship between slope and saturation.
The assessment of PoLUs assigned with different power values n are mainly based on the comparison with ELUs and ReLUs by utilizing the ELU-Network containing 11 convolutional layers on CIFAR-100 dataset.
The relatively simple ELU-Network is arranged in stacks of ([1 × 192 × 5], [1× 192× 1, 1× 240× 3], [1× 240× 1, 1× 260× 2], [1× 260× 1, 1× 280× 2], [1× 280× 1, 1× 300× 2], [1× 300× 1], [1× 100× 1]) (in each stack, the ﬁrst number is the num- ber of layers, the second number represents the number of ﬁlters and the third number is the size of each ﬁlter).
After each stack (except the last two stacks), a 2× 2 max-pooling layer with stride of 2 is applied.
Followed by the instruction in [4], the dropout ratio of (0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.0) Figure 4.
These plots are the results of using Simple-ELU-Net and different activation functions (PoLU(n ∈ {1, 1.5, 2}), ELU, ReLU) on CIFAR-100 dataset.
(a)-(c): The training loss; (d)-(f): The testing error.
Each curve represents the average of 5 runs.
PoLU with n = 2 achieves the best performance.
is applied at the last layer of each stack and the L2-weight decay regularization is set to 0.0005.
Learning rate is initial- ized at 0.01 and after 70 epochs drop to 0.005.
Then learn- ing rate decays by a factor of 10 every 70 epochs.
The total number of epochs is 300.
Stochastic Gradient Descent is used with momentum set to 0.9. Global contrast normaliza- tion and ZCA whitening which are mentioned in [8] are also applied to the dataset.
Moreover, the training samples are randomly cropped into 32 × 32 size with random horizon- tal ﬂipping images padded with four pixels at all borders.
Each network with different activation function is trained 5 times, and the mean and standard deviation of the testing errors on CIFAR-10 and CIFAR-100 are shown in Table 2 and 3.
Fig.4 presents the comparison of different activa- tion functions based on the testing error and training loss on CIFAR-100.
The results demonstrate that the PoLUs with power value n > 1 achieves the best results among the oth- ers, which also satisfy our prediction.
Additionally, training loss of PoLUs with different power values drops faster than ReLU and with a comparative speed with ELU.
To evaluate deeper and more complex neural networks which contain more parameters, we implemented both layers.
a VGG16-structure-like neural network and the ELU- Network with 18 convolutional The VGG16- structure-like neural work is derived from [24], with the number of units in the last two densely connected layers changed to 512 since the size of the input feature maps of them are 1 × 1 × 512.
The structure of the neural network is arranged in stacks of ([2 × 64 × 3], [2 × 128 × 3], [3 × 256× 3], [3× 512× 3], [3× 512× 3], [2× 512× F C], [1× 100 × F C]).
Max-pooling with size 2 × 2 and stride 2 is applied after each stack except the last three densely con- nected layers.
The dropout ratio is set to 0.5 for the last two densely connected layers and the L2-regularization term for each convolutional layer is set to 0.0005.
The learning rate is initialized to 0.01 and decays by a factor of 10 every 70 epochs.
The optimizer for the neural network is SGD with momentum equals to 0.9. The more sophisticated ELU- Network is arranged in stacks of ([1 × 384 × 3], [1 × 384 × 1, 1× 384× 2, 1× 640× 2], [1× 640× 1, 3× 768× 2], [1× 768×1, 2×896×2], [1×896×3, 2×1024×2], [1×1024× 1, 1 × 1152 × 2], [1 × 1152 × 1], [1 × 100 × 1]).
Similar to the simpler ELU-Network mentioned above, there is a 2× 2 max-pooling layer with stride 2 after each stack except the Table 2.
This table shows the result for CIFAR-10 dataset.
Different Networks (simple-ELU-Net, ELU-Net, VGG16, ResNet-50) and activation function (ReLU, ELU, PoLU(n = 1, 1.5, 2)) are used.
The best result for each network is bold.
ReLUs ELUs PoLUs (n=1) PoLUs (n=1.5) PoLUs (n=2) ResNet-50 VGG16 simple-ELU-Net ELU-Net 9.44(±0.20)% 8.16(±0.11)% 11.73(±0.20)% 8.37(±0.19)% 8.76(±0.17)% 7.46(±0.13)% 9.92(±0.13)% 6.67(±0.11)% 8.64(±0.13)% 7.57(±0.10)% 9.87(±0.11)% 6.68(±0.10)% 7.03(±0.14)% 8.14(±0.13)% 9.03(±0.12)% 5.85(±0.15)% 8.74(±0.11)% 5.45(±0.10)% 7.35(±0.12)% 8.42(±0.14)% last two stacks.
Zero padding is applied before each convo- lutional layer to keep the dimension unchanged.
The initial dropout ratio, L2-Regularization, momentum value are the same as the simpler ELU-Network.
For both the VGG16- structure-like network and the sophisticated ELU-Network, the dataset is preprocessed as described in [8] with global contrast normalization and ZCA whitening.
Thanks to the success of much deeper neural networks on image classiﬁcation, the experiments with different ac- tivation functions should also take deeper neural network such as Deep Residual Network (ResNet) into consider- ation.
Comparing with the neural networks mentioned above, ResNet is much deeper and the structure is kind of different since there is a shortcut connection from each stack input to the next stack input.
As described in [10], this skip connection structure can efﬁciently solve the vanishing gradient problem which is described in a previous section.
Another difference is, there is always a batch normalization layer before each activation function.
Batch normalization can provide the input batch of each stack with zero mean and unit variance, which will compensate one of the disad- vantages of ReLUs. Therefore, the difference of the perfor- mance among ReLUs, ELUs, and PoLUs in ResNet is not as signiﬁcant, compared with other networks.
Such phe- nomenon is also observed in Table 2 and 3.
However, we implemented ResNet-50 with the last two residual stages ending up by utilizing a 1×1 convolutional layer with stride of 2 to downsample the feature maps, thus it will result in a worse performance than what is described in [10].
The difference of the test errors of ResNet-50 with ReLUs on CIFAR-10 dataset is about 3%.
The evaluation of different α of ELUs on CIFAR-100 dataset using ELU-Network with 11 convolutional layers is presented in Fig.3. The results show that ELUs with α = 1 could achieve a better performance compared with α = 0.5 or α = 2, since the saturation value is too small or too large, which may push the mean activation away from 0.
This is also the evidence of the disadvantages of ELUs, which we mentioned in a previous section.
4.3. Street View House Number (SVHN) dataset The Street View House Number (SVHN) [21] dataset is collected by Google Street View, focusing on the color images of house numbers.
There are two formats of this dataset, and the second one was what used in our experi- ments.
In the second format, each image is in the ﬁxed size with 32 × 32 pixels and most center part of the image is a digit.
There are 73,257 training and 26,032 testing sam- ples.
Additionally, there are also 531,131 extra samples that can be used as additional training samples.
SVHN can be viewed as similar to MNIST since both datasets are mainly focused on digits.
However, SVHN is harder than MNIST since images from SVHN are cropped from real world color images.
Moreover, the size of each image is 32a × 32, and thus neural networks which are designed for CIFAR-10 and CIFAR-100 can also be implemented for SVHN.
We im- plemented the ELU-Network with 11 convolutional layers and a relatively shallower neural network which contains four convolutional layers and one densely connected layer followed by a softmax layer.
The relatively shallow neural network is arranged in stack of ([1×32×3], [1×32×3], [1× 64× 3], [1× 64× 3].[1× 512× F C], [1× 10× Sof tmax]).
The dataset is preprocessed followed by [26] with local con- trast normalization.
Only training and testing samples are used, the set of extra samples is not considered.
The ﬁnal performance is shown in Table 1.
4.4. ImageNet We evaluate PoLU on the ImageNet dataset which con- tains more than 1.2 million training images belonging to 1000 classes.
There are also 50k and 100k images for val- idation and testing respectively.
The network we imple- mented has the same structure as the one proposed in [4], in which the 15 layers are now arranged as ([1 × 96 × 6], [3 × 512× 3], [5× 768× 3], [3× 1024× 3].[2× 4096× F C], [1× 1000 × Sof tmax]).
After each stack, a 2 × 2 max-pooling layer with stride of 2 is applied and spatial pyramid pool- ing (SPP) with [1,2,4] levels is employed before the ﬁrst FC layer [9].
In Fig.5, even though the network with ELU has sim- ilar performance with network with PoLU in the (a) and (b) plots , we can still observe that, comparing with the net- work with ELUs, the one with PoLUs not only has a slightly lower loss at the last several epochs, but also has a better ﬁ- nal accuracy in plots (c) and (d).
Table 3.
This table shows the result for CIFAR-100 dataset.
Different Networks (simple-ELU-Net, ELU-Net, VGG16, ResNet-50) and activation function (ReLU, ELU, PoLU(n = 1, 1.5, 2)) are used.
The best result for each network is bold.
ReLUs ELUs PoLUs (n=1) PoLUs (n=1.5) PoLUs (n=2) ResNet-50 VGG16 simple-ELU-Net 30.27(±0.31)% 32.87(±0.21)% 33.53(±0.41)% 28.21(±0.29)% 29.29(±0.17)% 29.06(±0.25)% 28.46(±0.27)% 28.90(±0.22)% 29.02(±0.34)% 27.48(±0.23)% 28.01(±0.29)% 27.89(±0.26)% 27.01(±0.21)% 23.07(±0.20)% 28.64(±0.21)% 28.13(±0.30)% ELU-Net 29.47(±0.31)% 24.88(±0.21)% 24.97(±0.22)% 24.05(±0.23)% and ReLUs are not so signiﬁcant.
In our experiments, we also implemented power value n > 2.
However, due to too-large a slope at x → 0−, the activation functions are over sensitive to the input values.
PoLUs will map inputs around 0− with a little difference into larger area and Po- LUs also reach to saturation faster as the rate of change to- wards saturation is increased.
Moreover, together with the experiments we implemented with ELUs by setting differ- ent values of α, we can draw the conclusion that both slope and saturation cannot be set too large.
For the time complexity of our experiments, as demon- strated in both Fig.
4 and Fig.
5, similar to ELU, PoLU can achieve the same training loss or training accuracy with less epochs compared with ReLU.
Therefore, even though PoLU is more complex than ReLU and it takes about 5% more training time for each training epoch, the time of reaching the best accuracy that ReLU can get is still less.
Note that the extra time spent on the calculation of acti- vation function is only a small portion, comparing to the total computational time of the network.
While training with same number of epochs, the total training time for ELU/PoLU network and ReLU network on ImageNet are 12 hours and 11.5 hours respectively.
5.
Conclusions We proposed a new activation function - Power Linear Unit (PoLU), which uses identity function and power func- tion to construct its positive and negative sections respec- tively.
Even though the PoLU is along the direction of the efforts like ELU, which are variants of ReLU, in PoLU more attention is on the negative part of the activation func- tion.
This was motivated by the observation that not only the saturation value but also the slope in the negative part of the activation function can signiﬁcantly affect the perfor- mance of the network.
In contrast, ELU focuses on better saturation values via using exponential computation.
Note that the saturation value will change if we change the slope.
We have shown that ELU with saturation value 1 performs best, which implies that the slope cannot be changed if we want to use this optimal value.
The proposed PoLU can avoid this dilemma: we can achieve the desired saturation while being able to adjust the slope based on changing the power parameter n, which means that the negative region of Figure 5.
The plots of the training loss and testing error(%) on ImageNet.
(a) The training loss on the 1 - 250kth iterations; (b) The testing error(%) on the 1 - 250kth iterations; (c) The training loss on the 200k -250kth iterations; (d) The testing error(%) on the 200k - 250kth iterations.
4.5. Discussion on the result The performance of different neural networks with dif- ferent activation functions illustrates that the PoLUs and ELUs are much better than ReLUs in deep neural network if there are no batch normalization layers, which indicates that PoLUs and ELUs overcome the bias shift problem and push the input’s mean to zero from the other side.
However, the result on MNIST dataset shows that ReLU work better than ELU and PoLU with n = 1, 1.5, 2, which suggests that the bias shift effect may have less inﬂuence in shallow neu- ral networks.
Another observation from the comparison of the ELU-Network and the VGG16-structure-like network on CIFAR-100 dataset is that, if the network consists of all convolutional layers, like the ELU-Network, the PoLUs and ELUs, which contain negative part, it will perform much better than ReLUs, while if there are densely connected lay- ers, the difference in performance between PoLUs, ELUs Advances in neural information processing systems, pages 1097–1105, 2012.
1 [16] Y.
Le Cun, I.
Kanter, and S.
A.
Solla.
Eigenvalues of co- variance matrices: Application to neural-network learning.
Physical Review Letters, 66(18):2396, 1991.
2 [17] Y.
LeCun, L.
Bottou, Y.
Bengio, and P.
Haffner.
Gradient- based learning applied to document recognition.
Proceed- ings of the IEEE, 86(11):2278–2324, 1998.
4, 5 [18] A.
L.
Maas, A.
Y.
Hannun, and A.
Y.
Ng. Rectiﬁer nonlin- In Proc.
earities improve neural network acoustic models.
ICML, volume 30, 2013.
2 [19] G.
F.
Montufar, R.
Pascanu, K.
Cho, and Y.
Bengio.
On the number of linear regions of deep neural networks.
In Advances in neural information processing systems, pages 2924–2932, 2014.
1, 3, 4 [20] V.
Nair and G.
E.
Hinton.
Rectiﬁed linear units improve restricted boltzmann machines.
In Proceedings of the 27th international conference on machine learning (ICML-10), pages 807–814, 2010.
2 [21] Y.
Netzer, T.
Wang, A.
Coates, A.
Bissacco, B.
Wu, and A.
Y.
Ng. Reading digits in natural images with unsupervised fea- ture learning.
In NIPS workshop on deep learning and un- supervised feature learning, volume 2011, page 5, 2011.
4, [22] R.
Pascanu, G.
Mont´ufar, and Y.
Bengio.
On the number of inference regions of deep feed forward networks with piece- wise linear activations.
CoRR, abs/1312.6098, 2013.
1, 3, [23] O.
Russakovsky, J.
Deng, H.
Su, J.
Krause, S.
Satheesh, S.
Ma, Z.
Huang, A.
Karpathy, A.
Khosla, M.
Bernstein, A.
C.
Berg, and L.
Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge.
International Journal of Computer Vision (IJCV), 115(3):211–252, 2015.
4 [24] K.
Simonyan and A.
Zisserman.
Very deep convolu- tional networks for large-scale image recognition.
CoRR, abs/1409.1556, 2014.
5, 6 [25] B.
Xu, N.
Wang, T.
Chen, and M.
Li. Empirical evaluation of rectiﬁed activations in convolutional network.
arXiv preprint arXiv:1505.00853, 2015.
2 [26] M.
D.
Zeiler and R.
Fergus.
Stochastic pooling for regular- ization of deep convolutional neural networks.
arXiv preprint arXiv:1301.3557, 2013.
7 PoLU can intersect y = x while keeping the same asymp- tote for the saturation by means of setting n > 1.
We also demonstrated the advantage of having a proper slope in the negative part of the activation function, that networks using PoLU may have a larger number of response regions, which helps to improve the nonlinearity capacity of the neural net- work.
Experimental results showed that PoLU outperforms other state-of-the-art on most networks.
References [1] M.
Anthony and P.
L.
Bartlett.
Neural network learning: Theoretical foundations.
cambridge university press, 2009.
[2] F.
Chollet et al.
Keras.
fchollet/keras, 2015.
4 https://github.com/ [3] D.
Ciregan, U.
Meier, and J.
Schmidhuber.
Multi-column deep neural networks for image classiﬁcation.
In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Confer- ence on, pages 3642–3649.
IEEE, 2012.
1 [4] D.-A.
Clevert, T.
Unterthiner, and S.
Hochreiter.
Fast and accurate deep network learning by exponential linear units (elus).
arXiv preprint arXiv:1511.07289, 2015.
2, 3, 5, 6, 7 [5] G.
Cybenko.
Approximation by superpositions of a sig- moidal function.
Mathematics of Control, Signals, and Sys- tems (MCSS), 2(4):303–314, 1989.
1 [6] O.
Delalleau and Y.
Bengio.
Shallow vs.
deep sum-product In Advances in Neural Information Processing networks.
Systems, pages 666–674, 2011.
1 [7] X.
Glorot, A.
Bordes, and Y.
Bengio.
Deep sparse rectiﬁer neural networks.
In Aistats, volume 15, page 275, 2011.
1, 2 [8] I.
J.
Goodfellow, D.
Warde-Farley, M.
Mirza, A.
Courville, arXiv preprint and Y.
Bengio.
Maxout networks.
arXiv:1302.4389, 2013.
6, 7 [9] K.
He, X.
Zhang, S.
Ren, and J.
Sun.
Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation.
CoRR, abs/1502.01852, 2015.
2, 7 [10] K.
He, X.
Zhang, S.
Ren, and J.
Sun.
Deep residual learn- ing for image recognition.
In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, pages 770–778, 2016.
5, 7 [11] G.
Hinton, L.
Deng, D.
Yu, G.
E.
Dahl, A.-r.
Mohamed, N.
Jaitly, A.
Senior, V.
Vanhoucke, P.
Nguyen, T.
N.
Sainath, et al.
Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups.
IEEE Signal Processing Magazine, 29(6):82–97, 2012.
1 [12] S.
Hochreiter.
The vanishing gradient problem during learn- Interna- ing recurrent neural nets and problem solutions.
tional Journal of Uncertainty, Fuzziness and Knowledge- Based Systems, 6(02):107–116, 1998.
2 [13] K.
Hornik, M.
Stinchcombe, and H.
White.
Multilayer feed- forward networks are universal approximators.
Neural net- works, 2(5):359–366, 1989.
1 [14] A.
Krizhevsky and G.
Hinton.
Learning multiple layers of features from tiny images.
2009.
4, 5 [15] A.
Krizhevsky, I.
Sutskever, and G.
E.
Hinton.
classiﬁcation with deep convolutional neural networks.
Imagenet In
The world is dynamic, in a constant state of ﬂux.
Traditional learning systems that learn static models from historical data are unable to adjust to concept drift — changes in the distributions from which data are drawn.
A growing body of experimental machine learning research investigates incremental learn- ers that seek to adjust models as appropriate when confronted with concept drift (Gaber et al, 2005; Gama and Rodrigues, 2009; Aggarwal, 2009; ˇZliobaite, 2010; Bifet et al, 2011; Nguyen et al, 2014; Brzezinski and Stefanowski, 2014; Krempl et al, 2014; Gama et al, 2014; Ditzler et al, 2015).
This paper seeks to inform this line of research by identifying relationships between types of Nayyar A.
Zaidi, Geoﬀrey I.
Webb, Francois Petitjean, Germain Forestier Faculty of Information Technology, Monash University, Clayton, VIC 3800, Australia E-mail: {ﬁrstname.lastname}@monash.edu 2 Nayyar A.
Zaidi et al.
Fig.
1: An illustration of the hypothesized sweet path, a four way interaction among drift rate, forgetting rate, learner’s bias and variance and expected error.
concept drift and the properties of the learners that will best handle those forms of drift.
Speciﬁcally, we propose and investigate two hypotheses — 1.
The drift-rate/forgetting-rate nexus.
As the rate of concept drift increases, model accuracy will in general be maximized by increasing forgetting rates, and conversely, as the drift rate decreases, model accuracy will in general be maximized by decreasing forgetting rates.
Here increasing forgetting rates means focusing on more recent evidence by reducing window sizes or increasing decay rates.
Decreasing forgetting rates means focusing on longer term evidence by increasing window sizes or decreasing decay rates.
2.
The forgetting-rate/bias-variance-proﬁle nexus.
As forgetting rates increase, model accuracy will in general be maximized by altering the bias/variance proﬁle of a learner to decrease variance, and conversely, as forgetting rates increase, model accuracy will in general be maximized by decreasing bias.
The ﬁrst of these hypotheses is intuitive.
The faster the world is changing, the less relevance older information will have.
In consequence, more aggressive forgetting mechanisms, speciﬁcally smaller windows or higher decay rates, will be required to exclude older examples for which the trade-oﬀ between provid- ing additional information that is relevant to the current state-of-the-world and that which is misleading is weighted too heavily to the latter.
The second hypothesis derives from the hypothesis that when learning from smaller quantities of data, lower variance learners will maximize accuracy due to their ability to avoid overﬁtting, whereas when learning from larger datasets lower bias learners will maximize accuracy, due to their ability to model the details present in large data (Brain and Webb, 1999).
The more past data we forget, the smaller the eﬀective data quantity from which we learn and, therefore, with high-forgetting rate, low-variance models are more desirable and low-bias models with low-forgetting rate.
Drift RateSlowFastModel  Forgetting  RateSlowFastModel Bias- VarianceLow BiasHigh BiasLow VarianceHigh VarianceThe Sweet PathTitle Suppressed Due to Excessive Length Put together these hypothesized eﬀects imply the sweet path for concept drift illustrated in Figure 1, whereby the lowest error for a low drift rate will be achieved by a low bias learner with a low forgetting rate and the lowest error for a high drift rate will be achieved by a low variance learner with a high forgetting rate.
The bulk of this paper (Section 2 to Section 5) comprises detailed experi- ments that investigate the two hypotheses and the hypothesized sweet path.
We discuss implications and directions for future research in Section 6.
2 Background In supervised machine learning we seek to learn a model M that can predict the value (or probability of each value) y of a target variable Y for an example x = (cid:104)x1, .
.
.
, xa(cid:105) of an input variable X = (cid:104)X1, .
.
.
, Xa(cid:105).
We learn M from a training set T = {(cid:104)x1, y1(cid:105), .
.
.
,(cid:104)xs, ys(cid:105)}.
In incremental learning, the training set is presented to the learner as a sequence over a period of time and the learner updates M in light of each new example or set of examples as it is encountered.
Concept drift occurs when the distribution Pt(X, Y ) from which the data are drawn at time t diﬀers from that at subsequent time u, Pu(X, Y ).1 We can measure the magnitude of drift from time t to u, D(t, u), by a mea- sure of distance between the probability distributions Pt(X, Y ) and Pu(X, Y ) and the rate of drift at time t by: Ratet = lim n→∞ nD (t−0.5/n, t+0.5/n) (1) (Webb et al, 2016).
The observations in this paper hold for any distance mea- sure that is a metric, such as the Total Variation Distance (Levin et al, 2008).
Forgetting mechanisms are a standard strategy for dealing with concept drift.
The two main forgetting mechanisms are windowing, in which a sliding window is maintained containing only the W most recent examples; and decay or weighting, in which greater weight is placed on more recent examples and lesser weight on older ones (Gama et al, 2014).
3 Experimental setup To explore the drift-rate/forgetting-rate/bias-variance-proﬁle nexus, we re- quire an incremental learner that can learn from sliding windows or with decay.
Of course, we also require means of varying the learner’s bias/variance proﬁle.
For our experiments, we use the semi-naive Bayesian method AnDE (Webb et al, 2012), as it satisﬁes these requirements.
First, the model has a tuneable 1 Note that some papers (Gama et al, 2014) distinguish real concept drift in which P (Y | X) changes, from virtual concept drift in which P (X) changes.
For the purposes of this paper we do not distinguish between these, as the distinction does not appear pertinent.
4 Nayyar A.
Zaidi et al.
parameter n that controls the representation bias and variance.
When n = 0 (in AnDE), one gets a naive Bayes classiﬁer which is highly biased but has low variance.
Higher values of n decrease bias at the cost of an increase in variance and lower values decrease variance at a cost of increased bias.
Second, the AnDE model can be represented using counts of observed marginal frequencies, the dimensionality of each of which is controlled by n.
As described below, these can readily be incrementally updated to reﬂect a sliding window or incremental decay without need for relearning the entire model.
The goal of Bayesian methods is to factorize the joint distribution: P(y, x).
The AnDE model factorizes the joint distribution as: (cid:88) a(cid:89) i=1 δ(xs)ˆP(y, xs)  (cid:1) indicates the set of all size-n subsets of {1, .
.
.
a} and δ(xα) is a s∈(A n) ˆPA(n-1)DE(y, x) ˆP(xi | y, xs)/ (cid:88) s∈(A n) (cid:88) s∈(A n) : otherwise (2) δ(xs) : δ(xs) > 0 ˆPAnDE(y, x) = where (cid:0)A function that is 1 if the training data contains an object with the value xα, otherwise 0.
3.1 Window-based Adaptation A sliding window that supports learning only from the last W data points can be achieved simply with a queue-based data structure.
At each time step t, when a new data point (cid:104)x, y(cid:105) arrives: – Increment relevant count statistics based on (cid:104)x, y(cid:105) – Push (cid:104)x, y(cid:105) onto the queue – If queue length exceeds W – (cid:104)˜x, ˜y(cid:105) = De-queue.
– Decrement relevant count statistics based on (cid:104)˜x, ˜y(cid:105) It can be seen that parameter W controls the forgetting rate.
Large W means large windows, hence slow forgetting and small W means small windows, hence fast forgetting.
3.2 Decay-based Adaptation To support incremental exponential decay, before adding the count statistics of the data point x at step t, all that is required is that the counts in the count table are decayed.
For example if Nxi,y denotes the stored count of the number of times attribute i takes value xi and class attribute takes the value y, it is decayed as: Nxi,y = Nxi,y ∗ exp(−D), Title Suppressed Due to Excessive Length Fig.
2: Simple illustration of the structure of superparent k-DB classiﬁers.
(Left) super- parent 1-DB, each variable takes one more parent other than the class, (Right) superparent 2-DB, each variable takes two more parents other than the class.
where D is the decay parameter.
Like W in window-based adaptation, it can be seen that parameter D controls the model adaptation rate.
Large D means large decay, and hence fast model adaptation-rate, and, small D means small decay, and hence a slow model adaptation-rate.
4 Data Generation To test our hypotheses, we require data streams with varying drift rates.
To this end we create a framework where we can generate synthetic data for which we can systematically manipulate the rate of drift.
We represent the probability distribution using the most common formal- ism for doing so, a Bayesian network.
Because changing the probability dis- tribution at a node may change all of the probability distributions of all its children and their descendant nodes, in order to allow systematic manipula- tion of the drift rate we minimize the number of parent nodes.
Speciﬁcally, we sample from superparent k-DB (Keogh and Pazzani, 1999) distributions.
We show the structure of superparent 1 and 2-DB in Figure 2.
In a superparent k- DB model, each attribute Xi other than the ﬁrst k attributes takes X1, .
.
.
Xk and Y as its parents.
In a superparent 1-DB structure every attribute Xi other than X1 takes Y and X1 as its parents and in a superparent 2-DB structure every attribute other than X1 and X2 takes Y , X1 and X2 as its parents.
These structures are shown in Figure 2.
Specifying such networks is simple – the standard process is: 1.
Specify the (possibly empty) set πXi of parents for each node Xi. 0-DB1-DB2-DBP(xi|y)P(xi|y,xj)P(xi|y,xi,xk)yx1x2x3xayx1x2x3xayx1x2x3xa6 Nayyar A.
Zaidi et al.
2.
Fill the Conditional-Probability-Table (CPT) for each node.
This speciﬁes a probability distribution over the values of the attribute for each combi- nation of values of the parents.
Once the network is speciﬁed, one can use Ancestral Sampling (Bishop, 2006), to generate data therefrom.
We use the following simple (heuristic) procedure to generate the network: – All attributes including the class are binary.
– Set the initial number of attributes to 100.
– We aim for half of these attributes to have three parents, and the remaining half to have two parents in order that the resulting distributions do not too closely ﬁt the biases of a single AnDE learner.
Note, however, that as we are creating superparent k-DB structures, X1 can only have Y as a parent and that X2 must have X1 and Y as parents.
Therefore, in practice, we have 1 attribute with 1 parent, 49 attributes with 2 parents and 50 attributes with 3 parents.
– To allow direct control over the rate of drift, we do not allow parent at- tributes to drift.
To maximize diversity, we thus wish to minimize the number of parent attributes, which is why we use superparent 1-DB and 2-DB structures instead of more general k-DB structures.
To this end, for X3 to X50, we uniformly at random select either X1 or X2, together with Y as parents.
For X51 to X100, all of Y , X1 and X2 are assigned as parents.
– Once the structure is speciﬁed, we randomly initialize the CPTs. Note, we have to fulﬁll the sum-to-one constraint that P(Xi = 0 | πXi) + P(Xi = 1 | πXi) = 1.
To this end, for each combination of values for the parent attributes, we randomly select a value between 0 and 1 for P(Xi = 0 | πXi), and set P(Xi = 1 | πXi) = 1 − P(Xi = 0 | πXi).
– Next, we add 100 more binary attributes which have no parents and hence represent noise.
The CPTs for these nodes are also initialized randomly as above.
In total, we now have 200 attributes.
To sample from the distribution deﬁned by this network, we: – Choose the class y by uniformly at random selecting either 0 or 1.
– For i = 1 to 100 sample xi from the distribution deﬁned by P (Xi y, x1, .
.
.
, xi−1).
Introducing Drift To introduce drift we want to change the CPTs in a controlled manner.
We need to strictly control the change because we need to systematically increase and decrease the rate of drift.
To this end we ensure that — – Y , X1 and X2 (the three nodes that are parents to the other nodes) do not drift, therefore, their CPTs will not be changed through-out the data generation process.
– Drift occurs only after every T steps.
– Drift only inﬂuences X% of the attributes.
Title Suppressed Due to Excessive Length The ﬁrst constraint is necessary because changing parent probabilities will indirectly change all the child probabilities in a complex manner that is diﬃcult to manage.
The second and third constraints ensure that there is short term directionality in the drift.
Simply randomly drifting every attribute 1/10th of the drift rate every step results in half the steps simply canceling out the previous step for the attribute.
However, if only some attributes are drifted at each step and drift occurs only every T steps it ensures that non-trivial drift lasts for a non-trivial period of time.
Throughout the experiments, we will set T to 10 and X is set to 50.
That is, half of the attributes are randomly selected after every 10-th step and are drifted.
The drifting process is controlled by a single parameter ∆.
The method is designed to ensure that a higher value of ∆ leads to a fast drift, and a smaller value of ∆ will lead to a slow drift.
When a node is drifted, ∆ is either added or subtracted from each of its CPT values in a manner to ensure that the values sum to 1.0 and no value exceeds 1.0 or falls below 0.0, hence maintaining a valid probability distribution.
5 Experimental Analysis We have seen how parameter n controls the bias/variance proﬁle of the model, how parameters W and D control the forgetting rate, and how parameter ∆ controls the rate of the drift.
In this section, we present experiments that systematically study the interaction among these factors.
We use ∆ = 0.05 to represent fast drift, ∆ = 0.01 for medium drift and ∆ = 0.0005 for slow drift.
We select these from a wider range of values explored as exemplars that demonstrate clear diﬀerences in outcomes.
As an example of the rates not presented, for ∆ = 0.02 it is less clear which out of our fast and medium forgetting rates provides lower error, as our ﬁrst hypothesis predicts.
We generate data streams of 5,000 successive time steps, at each time step drawing one example randomly from the probability distribution for that step and drifting the distribution every 10 steps.
We use prequential evaluation, whereby at each time step the current model is applied to classify the next example in the data stream and then the example is used to update the model.
We plot the resulting error rates, where each point in the plot is the average error over 50 successive time steps.
We run each experiment 150 times for NB and A1DE and 100 times for A2DE (due to there being insuﬃcient time to complete more runs).
We present averages over all runs.
We ﬁrst present results for fast drift.
Figure 3 presents results using win- dows for forgetting and Figure 4 presents results using decay for forgetting.
The average prequential error for the window size or decay rate that achieves the lowest such error is listed for each of the three classiﬁers (Figures 3d and 4d).
Here we see that for NB, as predicted, the lowest error is achieved with fast forgetting (window size 20; decay rate 0.15).
8 Nayyar A.
Zaidi et al.
(a) (b) (c) (d) Fig.
3: Windowing with Fast Drift (∆ = 0.05) – Variation in prequential loss of NB (Figure 3a), A1DE (Figure 3b) and A2DE (Figure 3c) with window sizes of 20, 50 and 500.
Figure 3d: Comparison of NB (error = 0.253), A1DE (error = 0.337) and A2DE (error = 0.361) with best window size.
However, contrary to our expectations, A1DE and A2DE achieve their lowest error with slower forgetting.
This is because these models eﬀectively fail in the face of such rapid drift.
Recall that A1DE must estimate for every attribute Xi and attribute Xj both P(Y, Xi) and P(Xj | Y, Xi).
A2DE must estimate for every attribute Xi, attribute Xj and attribute Xk – P(Y, Xi, Xj) and P(Xk | Y, Xi, Xj).
The distributions for Y , X1 and X2 are not drifting, but all others are drifting at a rapid rate.
Larger window sizes allow these classiﬁers to produce more accurate estimates of the unvarying probabilities, P(Y, X1), P(Y, X2), P(X1 | Y, X2), P(X2 | Y, X1) and P(Y, X1, X2), whereas no window size provides accurate estimates of the remaining probabilities because either they are too small to provide accurate estimates or the distributions change too much over the duration of the window for the estimate to be accurate.
Thus, the relative error is dominated by the ability to accurately estimate the invariant probabilities and the performance approximates learning from a stationary distribution when the majority of attributes are noise attributes.
At an intermediate drift rate (∆ < 0.25), the intermediate bias learner A1DE starts to outperform NB.
Figures 5 and 6 show prequential 0-1 Loss 010002000300040005000Time Step (t)0.20.30.40.50.6ErrorNB (0.05)W = 20W = 50W = 500010002000300040005000Time Step (t)0.20.30.40.50.6ErrorA1DE (0.05)W = 20W = 50W = 500010002000300040005000Time Step (t)0.20.30.40.50.6ErrorA2DE (0.05)W = 20W = 50W = 500010002000300040005000Time Step (t)0.20.30.40.50.6ErrorNB vs.
A1DE vs.
A2DENBA1DEA2DETitle Suppressed Due to Excessive Length (a) (b) (c) (d) Fig.
4: Decay with Fast Drift (∆ = 0.05) – Variation in prequential loss of NB (Figure 4a), A1DE (Figure 4b) and A2DE (Figure 4c) with decay rates of 0.15, 0.05 and 0.005.
Figure 4d: Comparison of NB (error = 0.186), A1DE (error = 0.283) and A2DE (error = 0.408) with best decay rate.
with diﬀerent window sizes and decay rates with a drift delta of size 0.01.
It is apparent that for this intermediate drift rate the intermediate window size (50) and intermediate decay rate (0.05) attain the lowest error and that the learner with intermediate bias (A1DE) minimizes overall error.
Figures 5d and 6d, shows the comparison of NB, A1DE and A2DE with their best respective window size and decay rate, and it can be seen that A1DE results in best performance.
Figures 7 and 8 show prequential error with a slow drift rate, ∆ = 0.0005, with varying window sizes and decay rates.
Figure 7d, compares the perfor- mance of the three models with their respective best window size and Figure 8d with their best decay rate.
In both cases it is apparent that A2DE achieves the lowest error.
Thus, in all three scenarios of fast, intermediate and slow drift we ﬁnd the relationship predicted by the sweet path between drift rate, forgetting rate and bias/variance proﬁle.
010002000300040005000Time Step (t)0.10.20.30.40.50.6ErrorNB (0.05)D = 0.15D = 0.05D = 0.005010002000300040005000Time Step (t)0.20.30.40.50.6ErrorA1DE (0.05)D = 0.15D = 0.05D = 0.005010002000300040005000Time Step (t)0.20.30.40.50.6ErrorA2DE (0.05)D = 0.15D = 0.05D = 0.005010002000300040005000Time Step (t)00.10.20.30.40.50.6ErrorNB vs.
A1DE vs.
A2DENBA1DEA2DE10 Nayyar A.
Zaidi et al.
(a) (b) (c) (d) Fig.
5: Windowing with Medium Drift (∆ = 0.01) – Variation in prequential loss of NB (Figure 5a), A1DE (Figure 5b) and A2DE (Figure 5c) with window sizes of 20, 50 and 500.
Figure 5d: Comparison of NB (error = 0.0531), A1DE (error = 0.047) and A2DE (error = 0.083) with best window size.
5.1 Exploiting the insights of the sweet path for designing practical learners We have demonstrated that a generalizable and falsiﬁable hypothesis is consis- tent with experimental outcomes.
This raises the question of how the resulting insights might be used to create new eﬀective learners that can respond to con- cept drift.
Unfortunately, doing so appears to be far from trivial.
If we allow that drift rates may vary over time, the sweet path suggests that we need learners that can adapt to changes in drift rates with corresponding adapta- tion in their forgetting rates and bias/variance proﬁles.
We are yet to devise a practical algorithmic solution to the complex problem.
To assess the practical implications of our hypotheses for designing prac- tical learning algorithms, we compare the performance on real-world drifting data of our AnDE classiﬁers with diﬀering forgetting rates and bias/variance proﬁles against a range of state of the art concept drift classiﬁers.
We use 4 standard benchmark drift datasets, whose details are given in Table 1.
Nu- 010002000300040005000Time Step (t)00.10.20.30.40.50.6ErrorNB (0.01)W = 20W = 50W = 500010002000300040005000Time Step (t)00.10.20.30.40.50.6ErrorA1DE (0.01)W = 20W = 50W = 500010002000300040005000Time Step (t)00.10.20.30.40.50.6ErrorA2DE (0.01)W = 20W = 50W = 500010002000300040005000Time Step (t)0.020.040.060.080.10.120.14ErrorNB vs.
A1DE vs.
A2DENBA1DEA2DETitle Suppressed Due to Excessive Length 11 (a) (b) (c) (d) Fig.
6: Decay with Medium Drift (∆ = 0.01) – Variation in prequential loss of NB (Fig- ure 6a), A1DE (Figure 6b) and A2DE (Figure 6c) with varying decay rates of 0.15, 0.05 and 0.005.
Figure 6d: Comparison of NB (error = 0.0599), A1DE (error = 0.0498) and A2DE (error = 0.0629) with best decay rates.
#Instances #Attributes #Classes PowerSupply Airlines ElectricNorm Sensor 29928 539383 45312 2219803 58 Table 1: Details of datasets.
meric attributes are discretized using IDAW discretization (Webb, 2014) with 5 intervals.
We compare the performance with 12 standard learning techniques: 1.
AccuracyUpdatedEnsemble (Brzezinski and Stefanowski, 2014), 2.
AccuracyWeightedEnsemble (Wang et al, 2003), 3.
DriftDetectionMethodClassiﬁer (Gama et al, 2004), 4.
DriftDetectionMethodClassiﬁerEDDM (Baena-Garcıa et al, 2006), 5.
HoeﬀdingAdaptiveTree (Bifet and Gavald`a, 2009), 6.
HoeﬀdingOptionTree (Pfahringer et al, 2007), 010002000300040005000Time Step (t)00.10.20.30.40.50.6ErrorNB (0.01)D = 0.15D = 0.05D = 0.005010002000300040005000Time Step (t)00.10.20.30.40.50.6ErrorA1DE (0.01)D = 0.15D = 0.05D = 0.005010002000300040005000Time Step (t)00.10.20.30.40.50.6ErrorA2DE (0.01)D = 0.15D = 0.05D = 0.005010002000300040005000Time Step (t)00.020.040.060.080.1ErrorNB vs.
A1DE vs.
A2DENBA1DEA2DE12 Nayyar A.
Zaidi et al.
(a) (b) (c) (d) Fig.
7: Windowing with Slow Drift (∆ = 0.0005) – Variation in prequential loss of NB (Figure 7a), A1DE (Figure 7b) and A2DE (Figure 7c) with window sizes of 20, 50 and 500.
Figure 7d: Comparison of NB (error = 0.0289), A1DE (error = 0.0156) and A2DE (error = 0.0151) with best window size.
7.
HoeﬀdingTree (Hulten et al, 2001), 8.
LeveragingBag (Bifet et al, 2010), 9.
OzaBag (Oza and Russell, 2001), 10.
OzaBagAdwin (Bifet et al, 2009), 11.
OzaBoost (Oza and Russell, 2001), 12.
OzaBoostAdwin (Oza and Russell, 2001; Babcock et al, 2002).
It can be seen from Table 2 that AccWeightedEns (AccuracyWeightedEnsem- ble) achieved the lowest error on PowerSupply, AccUpdatedEn (AccuracyUp- dateEnsemble) the lowest on Airlines, whereas LeveregingBag (Levereg- ingBagging) achieved the lowest error on ElectricNorm and Sensor.
In the following, we will use these (best) results as benchmarks and see how adaptive AnDE with decay and window based adaptation can perform relative to these results.
We compare the performance of adaptive AnDE with window-based adap- tation in Figure 9, depicting results with various window sizes.
The low- est error obtained on the dataset by one of 12 standard techniques, is also 010002000300040005000Time Step (t)00.10.20.30.4ErrorNB (0.0005)W = 20W = 50W = 500010002000300040005000Time Step (t)00.10.20.30.4ErrorA1DE (0.0005)W = 20W = 50W = 500010002000300040005000Time Step (t)00.10.20.30.4ErrorA2DE (0.0005)W = 20W = 50W = 500010002000300040005000Time Step (t)00.010.020.030.04ErrorNB vs.
A1DE vs.
A2DENBA1DEA2DETitle Suppressed Due to Excessive Length 13 (a) (b) (c) (d) Fig.
8: Decay with Slow Drift (∆ = 0.0005) – Variation in 0-1 Loss of NB (Figure 8a), A1DE (Figure 8b) and A2DE (Figure 8c) with varying decay rates of 0.15, 0.05 and 0.005.
Figure 8d: Comparison of NB (error = 0.0290), A1DE (error = 0.0153) and A2DE (error = 0.0148) with best decay size.
plotted as a horizontal blue line for comparison.
It can be seen that except for PowerSupply, adaptive AnDE can achieve lower error than the lowest of any of the twelve state-of-the-art techniques.
For Airlines, A1DE achieves the lowest error of 0.3309 and for ElectricNorm and Sensors, A2DE achieves errors of: 0.1124 and 0.2250.
A comparison of adaptive AnDE with decay-based adaptation is given in Figure 10.
Similar to window-based results, adaptive AnDE achieved lower er- ror than the lowest achieved by any of the state-of-the-art techniques on all but the PowerSupply dataset.
On Airlines and ElectricNorm, A1DE achieved error of 0.3267 and 0.1061 respectively, and on Sensor, A2DE achieved error of 0.2268.
Note that we are not suggesting that our learners are currently suited for practical use.
There is still need to ﬁnd eﬀective mechanisms to dynami- cally select forgetting rates and bias/variance proﬁles.
Also, while our learners outperformed the best of the state-of-the-art, many of those learners could 010002000300040005000Time Step (t)00.10.20.30.4ErrorNB (0.0005)D = 0.15D = 0.05D = 0.005010002000300040005000Time Step (t)00.10.20.30.4ErrorA1DE (0.0005)D = 0.15D = 0.05D = 0.005010002000300040005000Time Step (t)00.10.20.30.4ErrorA2DE (0.0005)D = 0.15D = 0.05D = 0.005010002000300040005000Time Step (t)00.010.020.030.04ErrorNB vs.
A1DE vs.
A2DENBA1DEA2DE14 Nayyar A.
Zaidi et al.
AccUpdatedEns OzaBagAdwin DriftDetClassiﬁer DriftDetClassiﬁerEDDM (1) 0.8599 0.3335 0.2219 0.3102 (2) 0.8692 0.3448 0.167 0.2874 ASHoeﬀdingTree HoeﬀdingTree (5) 0.864 0.3552 0.2007 0.7153 (6) 0.864 0.3552 0.2007 0.7153 (3) 0.8634 0.3534 0.1984 0.3206 OzaBag (7) 0.8655 0.3575 0.1982 0.7067 (4) 0.8615 0.3511 0.149 0.3159 HoeﬀdingAdaptiveTree (8) 0.8661 0.3632 0.1759 0.3718 OzaBoost AccWeightedEns LeveragingBag OzaBoostAdwin (9) 0.9583 0.3719 0.1781 0.9514 (10) 0.8579 0.3751 0.2471 0.3596 (11) 0.8717 0.3769 0.1303 0.2395 (12) 0.9584 0.3888 0.143 0.407 PowerSupply Airlines ElectricNorm Sensor PowerSupply Airlines ElectricNorm Sensor PowerSupply Airlines ElectricNorm Sensor Table 2: Comparison of 0-1 Loss performance of 12 standard concept drift techniques on four real-world datasets: PowerSupply, Airlines, ElectricNorm, Sensor.
The best results are highlighted in bold font.
also have performed better if a sweep had been performed over their meta- parameters.
6 Conclusions We have proposed novel, generalizable and falsiﬁable hypotheses about the inter-relationships between drift rates, forgetting mechanisms, bias/variance proﬁles and error.
Our experiments with the AnDE learners have been con- sistent with the predictions of the hypotheses, that there will be a sweet path whereby as the drift rate increases the optimal forgetting rates will also in- crease; and as forgetting rates increase the optimal model variance proﬁle will decrease.
The AnDE classiﬁers are well suited to this study due to their eﬃcient support of incremental learning with both windowing and decay and the range of bias/variance proﬁles that they provide.
Our studies with real-world data show that this framework can result in prequential accuracies that are highly competitive with the best of the state- of-the-art.
Development of practical techniques for dynamically selecting and adjusting forgetting rates and bias/variance proﬁles as drift rates vary remains a promising avenue for future research.
Another intriguing insight that invites further investigation arises from the observation that for fast drift the lowest bias learner achieved lowest error with the slowest forgetting rate, in apparent disagreement with our hypotheses.
As we discuss in Section 5, this is due to its failure to learn from the fast drift- ing attributes and hence its performance being dominated by the attribute Title Suppressed Due to Excessive Length 15 Fig.
9: Comparison of adaptive NB, A1DE and A2DE based on window-based adaptation on four real world datasets: PowerSupply, Airlines, ElectricNorm, Sensor.
Horizontal blue line depicts the best performance out of 12 standard techniques.
subspace that is stationary (Y , X1 and X2).
This gives support to our argu- ment elsewhere (Webb et al, in press) that it is important to analyze drift in marginal distributions as well as at the course global level.
It invites the development of learners that bring diﬀerent forgetting rates and bias/variance proﬁles to bear on diﬀerent attribute sub-spaces at diﬀerent times as their rates of drift vary.
We hope that our hypotheses will provide insights into how to optimize a wide range of mechanisms for handling concept drift and will stimulate future research.
7 Acknowledgments This material is based upon work supported by the Air Force Oﬃce of Scientiﬁc Research, Asian Oﬃce of Aerospace Research and Development (AOARD) under award number FA2386-17-1-4033.
100101102103104105Window Size (W)0.90.951ErrorPowerSupplyNBA1DEA2DEBest Method100101102103104105Window Size (W)0.340.360.380.40.42ErrorAirlinesNBA1DEA2DEBest Method100101102103104105Window Size (W)0.120.140.160.180.20.22ErrorElectricNormNBA1DEA2DEBest Method100101102103104105Window Size (W)0.30.40.50.60.70.80.9ErrorSensorNBA1DEA2DEBest Method16 Nayyar A.
Zaidi et al.
Fig.
10: Comparison of adaptive NB, A1DE and A2DE based on decay-based adaptation on four real world datasets: PowerSupply, Airlines, ElectricNorm, Sensor.
Horizontal blue line depicts the best performance out of 12 standard techniques.
A Code The code used in this work can be downloaded from repository: https://github.com/ nayyarzaidi/SweetPathVoyager.
References Aggarwal C (2009) Data Streams: An Overview and Scientiﬁc Applications, Springer Berlin Heidelberg, pp 377–397.
DOI 10.1007/978-3-642-02788-8 14 Babcock B, Datar M, Motwani R (2002) Sampling from a moving window over streaming data.
In: Proceedings of the Thirteenth Annual ACM-SIAM Symposium on Discrete Algorithms, Society for Industrial and Applied Mathematics, pp 633–634 Baena-Garcıa M, del Campo- ´Avila J, Fidalgo R, Bifet A, Gavalda R, Morales-Bueno R (2006) Early drift detection method.
In: Fourth International Workshop on Knowledge Discovery from Data Streams, vol 6, pp 77–86 Bifet A, Gavald`a R (2009) Adaptive learning from evolving data streams.
In: Advances in Intelligent Data Analysis VIII, Springer, pp 249–260 Bifet A, Holmes G, Pfahringer B, Kirkby R, Gavald`a R (2009) New ensemble methods for evolving data streams.
In: Proceedings of the 15th ACM SIGKDD international confer- ence on Knowledge discovery and data mining, ACM, pp 139–148 Bifet A, Holmes G, Pfahringer B (2010) Leveraging bagging for evolving data streams.
In: Machine Learning and Knowledge Discovery in Databases, Springer, pp 135–150 10-610-410-2100Decay Rate (D)0.90.951ErrorPowerSupplyNBA1DEA2DEBest Method10-610-410-2100Decay Rate (D)0.340.360.380.40.42ErrorAirlinesNBA1DEA2DEBest Method10-610-410-2100Decay Rate (D)0.120.140.160.180.20.22ErrorElectricNormNBA1DEA2DEBest Method10-610-410-2100Decay Rate (D)0.30.40.50.60.70.80.9ErrorSensorNBA1DEA2DEBest MethodTitle Suppressed Due to Excessive Length 17 Bifet A, Gama J, Pechenizkiy M, Zliobaite I (2011) Handling concept drift: Importance, challenges and solutions.
PAKDD-2011 Tutorial, Shenzhen, China Bishop C (2006) Pattern Recognition and Machine Learning (Information Science and Statis- tics).
Springer-Verlag New York, Inc., Secaucus, NJ, USA Brain D, Webb G (1999) On the eﬀect of data set size on bias and variance in classiﬁcation learning.
In: Richards D, Beydoun G, Hoﬀmann A, Compton P (eds) Proceedings of the Fourth Australian Knowledge Acquisition Workshop (AKAW-99), The University of New South Wales, Sydney, pp 117–128 Brzezinski D, Stefanowski J (2014) Reacting to diﬀerent types of concept drift: The accuracy updated ensemble algorithm.
Neural Networks and Learning Systems, IEEE Transactions on 25(1):81–94 Ditzler G, Roveri M, Alippi C, Polikar R (2015) Learning in nonstationary environments: A survey.
IEEE Computational Intelligence Magazine 10(4):12–25 Gaber MM, Zaslavsky A, Krishnaswamy S (2005) Mining data streams: a review.
ACM Sigmod Record 34(2):18–26 Gama J, Rodrigues P (2009) An Overview on Mining Data Streams, Studies in Compu- tational Intelligence, vol 206, Springer Berlin / Heidelberg, pp 29–45.
DOI 10.1007/ 978-3-642-01091-0 2 Gama J, Medas P, Castillo G, Rodrigues P (2004) Learning with drift detection.
In: SBIA 2004, Lecture Bazzan A, Labidi S (eds) Advances in Artiﬁcial Intelligence Notes in Computer Science, vol 3171, Springer Berlin Heidelberg, pp 286–295, DOI 10.1007/978-3-540-28645-5 29 Gama J, ˇZliobaite I, Bifet A, Pechenizkiy M, Bouchachia A (2014) A survey on concept drift adaptation.
ACM Computing Surveys 46(4):44:1–44:37, DOI 10.1145/2523813 Hulten G, Spencer L, Domingos P (2001) Mining time-changing data streams.
In: Proceed- ings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD-01, ACM, pp 97–106 Keogh E, Pazzani M (1999) Learning augmented Bayesian classiﬁers: A comparison of distribution-based and classiﬁcation-based approaches.
In: Proceedings of the Interna- tional Workshop on Artiﬁcial Intelligence and Statistics, pp 225–230 Krempl G, Zliobaite I, Brzezinski D, Hullermeier E, Last M, Lemaire V, Noack T, Shaker A, Sievi S, Spiliopoulou M, Stefanowski J (2014) Open challenges for data stream mining research.
In: ACM SIGKDD Explorations Newsletter, Springer, vol 16–1, pp 1–10 Levin D, Peres Y, Wilmer E (2008) Markov Chains and Mixing Times.
American Mathe- matical Soc.
Nguyen H, Woon Y, Ng W (2014) A survey on data stream clustering and classiﬁcation.
Knowledge and Information Systems pp 1–35 Oza N, Russell S (2001) Online bagging and boosting.
In: Artiﬁcial Intelligence and Statistics 2001, Morgan Kaufmann, pp 105–112 Pfahringer B, Holmes G, Kirkby R (2007) New options for Hoeﬀding trees.
In: Orgun M, Thornton J (eds) AI 2007: Advances in Artiﬁcial Intelligence, Lecture Notes in Computer Science, vol 4830, Springer, pp 90–99, DOI 10.1007/978-3-540-76928-6 11 Wang H, Fan W, Yu PS, Han J (2003) Mining concept-drifting data streams using ensem- ble classiﬁers.
In: Proceedings of the Ninth ACM SIGKDD International conference on Knowledge Discovery and Data Mining, KDD-03, ACM, pp 226–235 Webb G (2014) Contrary to popular belief incremental discretization can be sound, compu- tationally eﬃcient and extremely useful for streaming data.
In: Proceedings of the 14th IEEE International Conference on Data Mining, pp 1031–1036, DOI 10.1109/ICDM.2014.
123 Webb G, Boughton J, Zheng F, Ting K, Salem H (2012) Learning by extrapolation from marginal to full-multivariate probability distributions: Decreasingly naive Bayesian clas- siﬁcation.
Machine Learning 86(2):233–272, DOI 10.1007/s10994-011-5263-6 Webb G, Hyde R, Cao H, Nguyen H, Petitjean F (2016) Characterizing concept drift.
Data Mining and Knowledge Discovery 30(4):964–994, DOI 10.1007/s10618-015-0448-4 Webb G, Lee L, Goethals B, Petitjean F (in press) Analyzing concept drift and shift from sample data.
Data Mining and Knowledge Discovery ˇZliobaite I (2010) Learning under concept drift: an overview.
CoRR abs/1010.4784, URL http://arxiv.org/abs/1010.4784, 1010.4784
In the reinforcement learning problem, an agent interacts with an environment, receiving rewards along the way that indicate the quality of its decisions.
The agent’s task is to learn to behave in a way that maximizes reward.
Model- based reinforcement learning (MBRL) techniques approach this problem by learning a predictive model of the envi- ronment and applying a planning algorithm to the model to make decisions.
Intuitively and theoretically (Szita & Szepesv´ari, 2010) there are many advantages to learning a model of the environment, but MBRL is challenging in practice, since even seemingly minor ﬂaws in the model or the planner can result in catastrophic failure.
As a result model-based methods have generally not been successful in large-scale problems, with only a few notable exceptions (e.g. Abbeel et al., 2007).
This paper addresses an important but understudied problem in MBRL: learning a reward function.
It is common for 1Department of Computer Science, Franklin & Marshall Col- lege, Lancaster, Pennsylvania, USA.
Correspondence to: Erik Talvitie <erik.talvitie@fandm.edu>.
Preliminary work, currently under review.
Figure 1.
The Shooter domain.
work in model learning to ignore the reward function (e.g. Bellemare et al., 2014; Oh et al., 2015; Chiappa et al., 2017) or, if the model will be used for planning, to assume the reward function is given (e.g. Ross & Bagnell, 2012; Talvitie, 2017; Ebert et al., 2017).
Indeed, it is true that if an accurate model of the environment’s dynamics can be learned, reward learning is relatively straightforward – the two problems can be productively decoupled.
However, in this paper we will see that when the model class is mispeciﬁed (i.e. that the representation does not admit a perfectly accurate model), as is inevitable in problems of genuine interest, learning a reward function becomes more complicated.
1.1. An Example To better understand how the limitations of the dynamics model impact reward learning, consider the simpliﬁed video game Shooter (Talvitie, 2015), pictured in Figure 1.
At the bottom of the screen is a spaceship which can move left and right and ﬁre bullets, which ﬂy upward.
When the ship ﬁres a bullet the agent receives -1 reward.
Near the top of the screen are three targets.
When a bullet hits a target in the middle (bullseye), the target explodes and the agent receives 20 reward; otherwise a hit is worth 10 reward.
Figure 1 shows the explosions that indicate how much reward the agent receives.
It is typical to treat predicting the next state and the predict- ing the reward as separate learning problems.
In the former the agent must learn to map an input state and action to the next state.
In the latter the agent must learn to map a state and action to reward.
In this example the agent might learn to associate the presence of explosions with reward.
How- ever, this decomposed approach can fail when the dynamics model is imperfect.
For instance, say the dynamics model in this case is a fac- Learning the Reward Function for a Misspeciﬁed Model tored MDP, which predicts the value of each pixel in the next image based on the 7 × 5 neighborhood centered on the pixel.
Figure 2b shows a short sample rollout from such a model, sampling each state based on the previous sampled state.
The second image in the rollout illustrates the model’s ﬂaw: when predicting the pixel marked with a question mark the model cannot account for the presence of the bullet under the target.
Hence, errors appear in the subsequent image (marked with red outlines).
What reward should be associated with this erroneous im- age?
The value the learned model assigns will have a dra- matic impact on the extent to which the model is useful for planning and yet it is clear that no amount of traditional data associating environment states with rewards can answer this question.
Even a provided, “perfect” reward function would not answer this question; a reward function could assign any value to this state and still be perfectly accurate in states that are reachable in the environment.
Intuitively it seems that the best case for the sake of planning would be for the reward model to predict 20 reward for the ﬂawed state, thus preserving the semantics that a target has been hit in the bullseye.
In order to achieve this, the reward model must be trained in states the environment would never generate.
The remainder of this paper formalizes this intuition.
Sec- tion 3 presents a novel error bound on value estimates in terms of reward error, taking into accout the rewards in ﬂawed states generated by the model.
In Section 4.1 the prac- tical implications of this theoretical insight are discussed, leading to an extension of the existing Hallucinated DAgger- MC algorithm, which provides theoretical guarantees in deterministic MDPs, even when the model class is misspec- iﬁed.
Section 5 demonstrates empirically that the approach suggested by the theoretical results can produce good plan- ning performance with a ﬂawed model, while reward models learned in the typical manner (or even “perfect” reward func- tions) can lead to catastrophic planning failure.
2.
Background We focus on Markov decision processes (MDP).
The en- vironment’s initial state s1 is drawn from a distribution µ.
At each step t the environment is in a state st.
The agent selects an action at which causes the environment to transi- tion to a new state sampled from the transition distribution: st+1 ∼ P at st .
The environment also emits a reward, Rat st .
We assume that rewards are bounded within [0, M ].
A policy π speciﬁes a way to behave in the MDP.
Let π(a | s) be the probability that π chooses action a in state s.
For a sequence of actions a1:t let P (s(cid:48) | s, a1:t) = P a1:t (s(cid:48)) be the probability of reaching s(cid:48) by starting in s and taking the actions in the sequence.
For any state s, action a, and policy π, let Dt s,a,π be the state-action distribution obtained Figure 2.
A ﬂawed model may generate states for which the reward function is undeﬁned.
T (s, a) = (cid:80)T s,a,π ξ,π = E(s,a)∼ξ Dt t=1 γt−1 E(s(cid:48),a(cid:48))∼Dt T (s) = Ea∼πs [Qπ after t steps, starting with state s and action a and thereafter following policy π.
For a state action distribution ξ, let s,a,π.
We let S be the set of states reach- Dt able in ﬁnite time by some policy with non-zero probability.
One may only observe the behavior of P and R in states contained in S.
The T -step state-action value of a policy, Qπ T (s, a) repre- sents the expected discounted sum of rewards obtained by taking action a in state s and executing π for an additional T − 1 steps: Qπ Ra(cid:48) s(cid:48) .
Let the T -step state value V π T (s, a)].
Let Qπ = Qπ∞, and V π = V π∞.
The agent’s goal will be to learn a policy π that maximizes Es∼µ[V π(s)].
In MBRL we seek to learn a dynamics model ˆP , approx- imating P , and a reward model ˆR, approximating R, and then to use the combined model ( ˆP , ˆR) to produce a policy via a planning algorithm.
We let ˆD, ˆQ, and ˆV represent the corresponding quantities using the learned model.
We assume that ˆP and ˆR are deﬁned over ˆS ⊇ S; there may be states in ˆS for which P and R are effectively undeﬁned, and it may not be known a priori which states these are.
Let P represent the dynamics model class, the set of models the learning algorithm could possibly produce and corre- spondingly let R be the reward model class.
In this work we are most interested in the common case that the dynamics model is misspeciﬁed: there is no ˆP ∈ P that matches P in every s ∈ S.
In this case it is impossible to learn a perfectly accurate model; the agent must make good decisions despite ﬂaws in the learned model.
The results in this paper also permit the reward model to be similarly misspeciﬁed.
2.1. Bounding Planning Performance For ease of analysis we focus our attention on the simple one- ply Monte Carlo planning algorithm (one-ply MC), similar to the “rollout algorithm” (Tesauro & Galperin, 1996).
For every state-action pair (s, a), the planner executes N T -step Learning the Reward Function for a Misspeciﬁed Model sample rollouts using ˆP , starting at s, taking action a, and then following a rollout policy ρ.
At each step of the rollout, ˆR gives the reward.
Let ¯Q(s, a) be the average discounted return of the rollouts starting with state s and action a.
For large N, ¯Q will closely approximate ˆQρ T (Kakade, 2003).
The execution policy ˆπ will be greedy with respect to ¯Q.
Talvitie (2015) bounds the quality of ˆπ.
For a policy π and state-action distribution ξ, let ξ,π,T be the error in the T -step state-action values the model assigns to the policy: ξ,π,T Then the following can be straightforwardly adapted from an existing bound (Talvitie, 2015).
Lemma 1.
Let ¯Q be the value function returned by applying depth T one-ply Monte Carlo to the model ˆP with rollout policy ρ.
Let ˆπ be greedy w.r.t. ¯Q.
For any policy π and state-distribution µ, T (s, a)|(cid:3).
T (s, a) − ˆQπ val = E(s,a)∼ξ (cid:2)|Qπ val (cid:2)V π(s) − V ˆπ(s)(cid:3) ≤ 4 s∼µ 1 − γ ξ,ρ,T val + mc, (cid:16) ξ(s, a) (1 − γ)µ(s)ˆπs(a) + γ(cid:80) where 1−γ(cid:107) ¯Q − ˆQρ and mc = 4 B is the Bellman operator).
2 Dµ,ˆπ(s, a) + 1 z,b Dµ,π(z, b)P b T(cid:107)∞ + 2 1−γ(cid:107)BV ρ 4 Dµ,π(s, a) + z (s)ˆπs(a) (cid:17) T (cid:107)∞ (here T − V ρ The mc term captures error due to properties of the one-ply MC algorithm: error in the sample average ¯Q and the sub- optimality of the T -step value function with respect to ρ.
The ξ,ρ,T term captures error due to the model.
We see that the model’s usefulness for planning is tied to the accuracy of the value it assigns to the rollout policy.
Thus, in order to obtain a good plan ˆπ, we aim to learn a model for which ξ,ρ,T val is small.
val 2.2. Error in the Dynamics Model Assuming the reward function is known, a bound on ξ,ρ,T can be straightforwardly adapted from Ross & Bagnell (2012) in terms of the one-step prediction error of the dy- namics model.
Lemma 2.
For any policy π and state-action distribution ξ, val T−1(cid:88) t=1 val ≤ M ξ,π,T 1 − γ (γt − γT ) E (s,a)∼Dt ξ,π s − ˆP a s (cid:107)1 (cid:2)(cid:107)P a (cid:3).
Combining Lemmas 1 and 2 yields an overall bound on con- trol performance in terms of model error.
However, recent work (Talvitie, 2017) offers a tighter bound in a special case.
Let the true dynamics P be deterministic, and let the rollout policy ρ be blind (Bowling et al., 2006); the action selected by ρ is conditionally independent of the current state, given the history of actions.
Then for any state-action distribu- tion ξ, let H t ξ,ρ be the joint distribution over environment (cid:2)(cid:80) state, model state, and action if a single action sequence is sampled from ρ and then executed in both the model and ξ,ρ(s1, z1, a1) = ξ(s1, a1) when the environment.
So, H 1 z1 = s1 (0 otherwise) and for all t ≥ 2, H t ξ,ρ(st, zt, at) = | a1)P a1:t−1 (st) ˆP a1:t−1 E(s1,a1)∼ξ Since P is deterministic, let σa1:t be the unique state that re- sults from starting in state s and taking the action sequence a1:t.
Then Talvitie (2017) offers the following result: Theorem 3.
If P is deterministic, then for any blind policy ρ and any state-action distribution ξ, (zt)(cid:3).
ρ(a2:t a2:t−1 s1 s1 T(cid:88) t=1 val ≤ M ξ,ρ,T T−1(cid:88) T−1(cid:88) ≤ 2M t=1 ≤ 2M 1 − γ γt t=1 (cid:2)(cid:107)Dt (cid:2)1 − ˆP a γt−1 E (s,a)∼ξ (s,z,a)∼H t ξ,ρ (γt − γT ) E (s,a)∼Dt ξ,ρ s,a,ρ − ˆDt s,a,ρ(cid:107)1 z (σa s )(cid:3) (cid:2)1 − ˆP a s )(cid:3).
s (σa (1) (2) (3) (cid:3) Inequality 3 is Lemma 2 specialized to the deterministic case, expressing the bound in terms of the one-step pre- diction error of ˆP .
Inequality 1 gives the bound in terms of the error in the discounted distribution of states along T -step rollouts.
Though this is the tightest bound of the three, in practice it is difﬁcult to optimize this objective di- rectly.
Inequality 2 gives the bound in terms of hallucinated one-step error, so called because it considers the accuracy of the model’s predictions based on states generated from its own sample rollouts (z), rather than states generated by the environment (s).
To optimize hallucinated error, the model can be rolled out in parallel with the environment, and trained to predict the next environment state from each “hallucinated” state in the model rollout.
Talvitie (2017) shows that this approach can dramatically improve planning performance when the model class is mispeciﬁed.
Similar approaches have also had em- pirical success in MBRL tasks (Talvitie, 2014; Venkatraman et al., 2016) and sequence prediction tasks (Venkatraman et al., 2015; Oh et al., 2015; Bengio et al., 2015).
Talvitie (2017) shows that the relative tightness of the hal- lucinated error bound does not hold for general stochastic dynamics or for arbitrary rollout policies.
However, note that these assumptions are not as limiting as they ﬁrst appear.
By far the most common rollout policy chooses actions uni- formly randomly, and is thus blind.
Furthermore, though P is assumed to be deterministic, it is also assumed to be too complex to be practically captured by ˆP .
From the agent’s perspective, un-modeled complexity will manifest as ap- parent stochasticity.
For example Oh et al.
(2015) learned dynamics models of Atari 2600 games, which are fully de- terministic (Hausknecht et al., 2014); human players often Learning the Reward Function for a Misspeciﬁed Model perceive them to be stochastic due to their complexity.
For the remainder of the paper we focus on the special case of deterministic dynamics and blind rollout policies.
3.
Incorporating Reward Error As suggested by Talvitie (2017), there is a straightforward extension of Theorem 3 to account for reward error.
Theorem 4.
If P is deterministic, then for any blind policy ρ and any state-action distribution ξ, val ≤ T(cid:88) ξ,ρ,T s,a,ρ(cid:107)1 s(cid:48) − ˆRa(cid:48) s(cid:48) − ˆRa(cid:48) s(cid:48)(cid:12)(cid:12)(cid:3) s,a,ρ − ˆDt (cid:2)(cid:12)(cid:12)Ra(cid:48) (cid:2)(cid:107)Dt (cid:2)(cid:12)(cid:12)Ra(cid:48) s(cid:48)(cid:12)(cid:12)(cid:3) (cid:2)1 − ˆP a s(cid:48)(cid:12)(cid:12)(cid:3) (cid:2)(cid:12)(cid:12)Ra(cid:48) (cid:2)1 − ˆP a s(cid:48) − ˆRa(cid:48) s )(cid:3) z (σa ξ,ρ t=1 + M ≤ T(cid:88) t=1 (s(cid:48),a(cid:48))∼Dt ξ,ρ γt−1 E (s,a)∼ξ (s(cid:48),a(cid:48))∼Dt ξ,ρ γt−1 T(cid:88) t=1 γt−1 T−1(cid:88) γt (s,z,a)∼H t + 2M ≤ T(cid:88) t=1 γt−1 t=1 2M 1 − γ T−1(cid:88) t=1 (s(cid:48),a(cid:48))∼Dt ξ,ρ (γt − γT ) E (s,a)∼Dt ξ,ρ (cid:3) (4) (5) (6) s )(cid:3).
s (σa Proof.
The derivation of inequality 4 is below.
The rest follow immediately from Theorem 3.
T (s, a)|(cid:3) s,a,ρ(s(cid:48), a(cid:48))Ra(cid:48) Dt s(cid:48) (cid:16) − ˆDt s,a,ρ(s(cid:48), a(cid:48)) ˆRa(cid:48) s(cid:48) s,a,ρ(s(cid:48), a(cid:48))Ra(cid:48) Dt s(cid:48) s(cid:48) + Dt − ˆDt s,a,ρ(s(cid:48), a(cid:48)) ˆRa(cid:48) s(cid:48) s,a,ρ(s(cid:48), a(cid:48)) ˆRa(cid:48) s(cid:48) ξ,ρ,T val = E (s,a)∼ξ T (s, a) − ˆQρ (cid:16) (cid:2)|Qρ (cid:34)(cid:12)(cid:12)(cid:12)(cid:12) T(cid:88) γt−1 (cid:88) (cid:34)(cid:12)(cid:12)(cid:12)(cid:12) T(cid:88) γt−1 (cid:88) t=1 t=1 (s(cid:48),a(cid:48)) (s(cid:48),a(cid:48)) = E (s,a)∼ξ = E (s,a)∼ξ − Dt s,a,ρ(s(cid:48), a(cid:48)) ˆRa(cid:48) (cid:16) γt−1 (cid:88) (cid:34)(cid:12)(cid:12)(cid:12)(cid:12) T(cid:88) t=1 = E (s,a)∼ξ s,a,ρ(s(cid:48), a(cid:48))(Ra(cid:48) Dt s(cid:48) − ˆRa(cid:48) s(cid:48) ) (s(cid:48),a(cid:48)) s,a,ρ(s(cid:48), a(cid:48)) − ˆDt + (Dt s,a,ρ(s(cid:48), a(cid:48))) ˆRa(cid:48) s(cid:48) (cid:35) (cid:17)(cid:12)(cid:12)(cid:12)(cid:12) (cid:35) (cid:17)(cid:12)(cid:12)(cid:12)(cid:12) (cid:17)(cid:12)(cid:12)(cid:12)(cid:12) ≤ T(cid:88) t=1 γt−1 (cid:12)(cid:12)Ra(cid:48) s(cid:48) − ˆRa(cid:48) s(cid:48)(cid:12)(cid:12) (cid:2)(cid:13)(cid:13)Dt (s(cid:48),a(cid:48))∼Dt ξ,ρ T(cid:88) + M γt−1 E (s,a)∼ξ s,a,ρ − ˆDt s,a,ρ (cid:13)(cid:13)(cid:3), t=1 which gives the result.
As is typical, these bounds break the value error into two parts: reward error and dynamics error.
The reward error measures the accuracy of the reward model in environment states encountered by policy ρ.
The dynamics error mea- sures the probability that the model will generate the correct states in rollouts, effectively assigning maximum reward error (M) when the dynamics model generates incorrect states.
This view corresponds to common MBRL practice: separately train the dynamics model to assign high proba- bility to correct states and the reward model to accurately map environment states to rewards.
However, as discussed in Section 1.1, these bounds are overly conservative (and thus loose): generating an erroneous state need not be catas- trophic if the associated reward is still reasonable.
We can derive a bound that accounts for this.
Theorem 5.
If P is deterministic, then for any blind policy ρ and any state-action distribution ξ, ξ,ρ,T γt−1 E(s,z,a)∼H t ξ,ρ Proof.
ξ,ρ,T val = E(s1,a1)∼ξ ρ (s1, a1) − ˆQT s − ˆRa (cid:2)(cid:12)(cid:12)Ra (cid:12)(cid:12)(cid:3).
ρ (s1, a1)(cid:12)(cid:12)(cid:3) = E (s1,a1)∼ξ = E (s1,a1)∼ξ st,at t=1 t=1 val ≤ T(cid:88) (cid:2)(cid:12)(cid:12)QT (cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) T(cid:88) γt−1(cid:88) (cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:0)Ra1 (cid:18)(cid:88) −(cid:88) (cid:88) −(cid:88) zt (st)Rat st − ˆRa1 (st)Rat st P a1:t−1 P a1:t−1 s1 s1 s1 s1 s1 zt st zt (st) ˆP a1:t−1 s1 P a1:t−1 s1 (cid:88) st (cid:88) st,zt (cid:35) Now note that for t ≥ 2, P a1:t−1 s1 ˆP a1:t−1 zt (zt) ˆRat zt Dt s1,a1,ρ(st, at)Rat st zt,at ˆDt −(cid:88) γt−1(cid:88) T(cid:88) (cid:1) + −(cid:88) a2:t t=2 s1,a1,ρ(zt, at) ˆRat zt ρ(a2:t | a1) (cid:35) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) (cid:35) (cid:19)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) (st)Rat st ˆP a1:t−1 s1 (zt) ˆRat zt ˆP a1:t−1 s1 (zt) s1 ˆP a1:t−1 (zt)(cid:0)Rat st (zt) ˆRat zt − ˆRat zt (cid:88) (cid:1) st P a1:t−1 s1 (st) Learning the Reward Function for a Misspeciﬁed Model (cid:12)(cid:12) + T(cid:88) t=2 − ˆRa1 s1 (cid:34)(cid:12)(cid:12)Ra1 (cid:88) s1 st,zt Thus (s1,a1)∼ξ val ≤ E ξ,ρ,T (cid:88) T(cid:88) ρ(a2:t | a1) a2:t γt−1 t=1 P a1:t−1 s1 (st) ˆP a1:t−1 s1 (cid:2)(cid:12)(cid:12)Ra s − ˆRa (cid:12)(cid:12)(cid:3).
(s,z,a)∼H t ξ,ρ γt−1 (zt)(cid:12)(cid:12)Rat st (cid:12)(cid:12)(cid:35) − ˆRat zt Similar to the hallucinated one-step error for the dynamics model (inequality 2), Theorem 5 imagines that the model and the environment are rolled out in parallel.
It measures the error between the rewards generated in the model rollout and the rewards in the corresponding steps of the environ- ment rollout.
We call this the hallucinated reward error.
However, unlike the bound in Theorem 4, which is focused on the model placing high probability on “correct” states, the hallucinated reward error may be small even if the state sequence sampled from the dynamics model is “incorrect”, as long as the sequence of rewards is similar.
As such, we can show that this bound is tighter than inequality 5.
Theorem 6.
If P is deterministic, then for any blind policy ρ and any state-action distribution ξ, γt−1 E(s,z,a)∼H t ξ,ρ T(cid:88) t=1 ≤ T(cid:88) t=1 (cid:12)(cid:12)(cid:3) s − ˆRa (cid:2)(cid:12)(cid:12)Ra (cid:2)(cid:12)(cid:12)Ra(cid:48) s(cid:48) − ˆRa(cid:48) γt−1 (s(cid:48),a(cid:48))∼Dt ξ,ρ T−1(cid:88) t=1 + 2M γt (s,z,a)∼H t ξ,ρ s(cid:48)(cid:12)(cid:12)(cid:3) (cid:2)1 − ˆP a s )(cid:3).
z (σa Proof.
T(cid:88) γt−1 t=1 H t (cid:2)(cid:12)(cid:12)R(s, a) − ˆR(z, a)(cid:12)(cid:12)(cid:3) ξ,ρ(s, z, a)(cid:12)(cid:12)R(s, a) − ˆR(z, a)(cid:12)(cid:12) ξ,ρ(s, s, a)(cid:12)(cid:12)R(s, a) − ˆR(s, a)(cid:12)(cid:12) ξ,ρ(s, z, a)(cid:12)(cid:12)R(s, a) − ˆR(z, a)(cid:12)(cid:12).
H t ξ,ρ t=1 s,z,a (s,z,a)∼H t γt−1(cid:88) T(cid:88) γt−1(cid:88) T(cid:88) T(cid:88) γt−1 (cid:88) t=1 s,a H t t=1 s,z(cid:54)=s,a This breaks the expression into two terms.
Now consider the ﬁrst term: s,a t=1 Dt H t γt−1(cid:88) T(cid:88) ξ,ρ(s, s, a)(cid:12)(cid:12)R(s, a) − ˆR(s, a)(cid:12)(cid:12) γt−1(cid:88) ≤ T(cid:88) γt−1 (cid:88) T(cid:88) T(cid:88) ξ,ρ(s, a)(cid:12)(cid:12)R(s, a) − ˆR(s, a)(cid:12)(cid:12).
ξ,ρ(s, z, a)(cid:12)(cid:12)R(s, a) − ˆR(z, a)(cid:12)(cid:12) γt−1 (cid:88) ≤ M s,z(cid:54)=s,a ξ,ρ(s, z, a).
H t H t t=1 t=1 s,a Now consider the second term: t=1 s,z(cid:54)=s,a (7) Recall that H 1 ξ,ρ(s, z, a) = 0 if s (cid:54)= z.
Thus, t=1 t=1 s,z(cid:54)=s,a s,z(cid:54)=s,a T(cid:88) γt−1 (cid:88) γt (cid:88) T−1(cid:88) (cid:32) (cid:88) (cid:88) (cid:88) (cid:0)1 − ˆP a(cid:48) T−1(cid:88) s(cid:48),z(cid:48),a(cid:48) s(cid:48),z(cid:48),a(cid:48) s,z(cid:54)=s = M = M = M = M γt (s,z,a)∼H t ξ,ρ t=1 H t ξ,ρ(s, z, a) (cid:33) T−1(cid:88) H t+1 ξ,ρ (s, z, a) P a(cid:48) s(cid:48) (s) ˆP a(cid:48) z(cid:48) (z) s(cid:48) )(cid:1) T−1(cid:88) z(cid:48) (σa(cid:48) t=1 γtH t ξ,ρ(s(cid:48), z(cid:48), a(cid:48)) γtH t ξ,ρ(s(cid:48), z(cid:48), a(cid:48)) t=1 [1 − ˆP a z (σa s )].
(8) Combining lines 7 and 8 yields the result.
The next section discusses the practical implications of this result for MBRL algorithms and extends an existing MBRL algorithm to incorporate this insight.
4.
Implications for MBRL This is not the ﬁrst observation of the difﬁculties inherent in reward learning.
Sorg et al.
(2010b) argued as we have that when the model or planner are limited in some way, reward functions other than the true reward may lead to better planning performance.
Accordingly, policy gradient approaches have been employed to learn reward functions for use with online planning algorithms, providing a beneﬁt even when the reward function is known (Sorg et al., 2010a; 2011; Bratman et al., 2012; Guo et al., 2016).
Tamar et al.
(2016) take this idea to its logical extreme, treating the en- tire model and even the planning algorithm itself as a policy parameterization, adapting them to directly improve control Learning the Reward Function for a Misspeciﬁed Model performance rather than to minimize any measure of predic- tion error.
Though appealing in its directness, this approach offers little theoretical insight into what makes a model useful for planning.
Furthermore, there are advantages to optimizing quantities other than planning performance; this allows the model to exploit incoming data even when it is unclear how to improve the agent’s policy (for instance if the agent has seen little reward).
Theorem 5 provides more speciﬁc guidance about how to choose amongst a set of ﬂawed models.
Rather than attempting to directly opti- mize control performance, this result suggests that we can take advantage of model error signals while still offering guarantees in terms of control performance.
It is notable that, unlike Theorem 4, Theorem 5 does not contain a term measuring dynamics error.
Certainly the dynamics model is implicitly important; for some choices of ˆP the hallucinated reward error can be made very small while for others it may be irreducibly high (for instance if ˆP simply loops on a single state).
Nevertheless, low halluci- nated reward error does not require that the dynamics model place high probability on “correct” states.
In fact, it may be that dynamics entirely unrelated to the environment yield the best reward predictions.
This intriguingly suggests that the dynamics model and reward model parameters could be adapted together to optimize hallucinated reward error.
Arguably, the recently introduced Predictrons (Silver et al., 2017) and Value Prediction Networks (Oh et al., 2017) are attempts to do just this – they adapt the model’s dynamics solely to improve reward prediction.
We can see Theorem 5 as theoretical support for these approaches and encour- agement of more study in this direction.
Still, in practice it may be much harder to learn to predict reward sequences than state sequences, especially when the reward signal is sparse.
It may also be difﬁcult to tie reward prediction er- ror to dynamics model parameters in a way that allows for theoretical performance guarantees.
Another possible interpretation of Theorem 5 is that the reward model should be customized to the dynamics model.
That is, if we hold the dynamics model ﬁxed, then the result gives a clear objective for the reward model.
Theorem 6 suggests an algorithmic structure where the dynamics model is trained via its own objective, and the reward model is then trained to minimize hallucinated error with respect to the learned dynamics model.
The clear downside of this ap- proach is that it will not in general ﬁnd the best combination of dynamics model and reward model; it could be that a less accurate dynamics model results in lower hallucinated reward error.
The advantage is that it allows us to effectively exploit the prediction error signal for the dynamics model and removes the circular dependence between the dynamics model and the reward model.
In this paper we explore this third option by extending the ex- isting Hallucinated DAgger-MC algorithm (Talvitie, 2017).
Because the resulting algorithm is very similar to the orig- inal, we leave a detailed description and analysis to the appendix and here focus on key, high-level points.
Section 5 presents empirical results illustrating the impact of training the reward model to minimize hallucinated error.
4.1. Hallucinated DAgger-MC with Reward Learning The “Data Aggregator” (DAgger) algorithm (Ross & Bag- nell, 2012) was the ﬁrst practically implementable MBRL al- gorithm with performance guarantees agnostic to the model class.
It did, however, require that the planner be near opti- mal.
DAgger-MC (Talvitie, 2015) relaxed this assumption, accounting for the limitations of a particular suboptimal planner (one-ply MC).
Hallucinated DAgger-MC (or H- DAgger-MC) (Talvitie, 2017) altered DAgger-MC to opti- mize the hallucinated error, rather than the one-step error.
All of these algorithms were presented under the assump- tion that the reward function was known a priori.
As we will see in Section 5, the reward function cannot be ignored.
Even when the reward function is given, these algorithms can fail catastrophically due to issues like the one described in Section 1.1. At a high level, H-DAgger-MC proceeds in iterations.
In each iteration a batch of data is gathered by sampling state- action pairs using a mixture of the current plan and an “exploration distribution” (to ensure that important states are visited, even if the plan would not visit them).
The rollout policy is used to generate parallel rollouts in the environment and model from these sampled state-action pairs, which form the training examples.
The collected data is used to update the dynamics model, which is then used to produce a new plan to be used in the next iteration.
We augment H-DAgger-MC, adding a reward learning step to each iteration (rather than assuming the reward is given).
In each rollout, training examples mapping “hallucinated” model states to the real environment rewards are collected and used to update the reward model.
The extended H- DAgger-MC algorithm offers theoretical guarantees similar to those of the original algorithm.
Essentially, if • the exploration distribution is similar to the state visita- tion distribution of a good policy, • mc is small, • the learning algorithms for the dynamics model and reward model are both no-regret, and • the reward model class R contains a low hallucinated reward error model with respect to the lowest halluci- nated prediction error model in P, then in the limit H-DAgger-MC will produce a good policy.
Learning the Reward Function for a Misspeciﬁed Model As discussed in Section 4, this does not guarantee that H- DAgger-MC will ﬁnd the best performing combination of dynamics model and reward model, since the training of the dynamics model does not take hallucinated reward error into account.
It is, however, an improvement over the original H-DAgger-MC result in that good performance can still be assured even if there is no low error dynamics model in P, as long as there is a low error reward model in R.
For completeness’ sake, a more detailed description of anal- ysis of the algorithm can be found in the appendix.
Here we turn to an empirical evaluation of the algorithm.
5.
Experiments In this section we illustrate the impact of optimizing hallu- cinated reward error in the Shooter example described in Section 1 using both DAgger-MC and H-DAgger-MC1.
The one-ply MC planner used 50 uniformly random rollouts of depth 20 per action at every step.
The exploration distri- bution was generated by following the optimal policy with (1− γ) probability of termination at each step.
The discount factor was γ = 0.9. In each iteration 500 training rollouts were generated and the resulting policy was evaluated in an episode of length 30.
The discounted return obtained by the policy in each iteration is reported, averaged over 50 trials.
The dynamics model for each pixel was learned using Con- text Tree Switching (Veness et al., 2012), similar to the FAC-CTW algorithm (Veness et al., 2011).
At each position the model takes as input the values of the pixels in a w × h neighborhood around the position in the previous timestep.
Data was shared across all positions.
The reward was ap- proximated with a linear function for each action, learned via stochastic weighted gradient descent.
The feature repre- sentation contained a binary feature for each possible 3 × 3 conﬁguration of pixels at each position.
This representation admits a perfectly accurate reward model.
The qualitative observations presented in this section were robust to a wide range of choices of step size for gradient descent.
Here, in each experiment the best performing step size for each approach is selected from 0.005, 0.01, 0.05, 0.1, and 0.5. In the experiments a practical alteration has been made to the H-DAgger-MC algorithm.
H-DAgger-MC requires an “unrolled” dynamics model (with a separate model for each step of the rollout, each making predictions based on the output of the previous model).
While this is important for H-DAgger-MC’s theoretical guarantees, Talvitie (2017) found empirically that a single dynamics model for all steps could be learned, provided that the training rollouts had limited depth.
Following Talvitie (2017), in the ﬁrst 10 iterations only the ﬁrst example from each training rollout 1Source code for these experiments will be available upon publication.
is added to the dynamics model dataset; thereafter only the ﬁrst two examples are added.
The entire rollout was used to train the reward model.
DAgger-MC does not require an unrolled dynamics model or truncated training rollouts and was implemented as originally presented (Talvitie, 2015), with a single dynamics model and full training rollouts.
5.1. Results We consider both DAgger-MC and H-DAgger-MC with a perfect reward model, a reward model trained only on environment states during rollouts, and a reward model trained on “hallucinated” states as in Algorithm 1.
The perfect reward model is one that someone familiar with the rules of the game would likely specify; it simply checks for the presence of explosions in the three target positions and gives the appropriate value if an explosion is present or 0 otherwise (subtracting 1 if the action is “shoot”).
Results are presented in three variations on the Shooter problem.
5.1.1. NO MODEL LIMITATIONS In the ﬁrst experiment we apply these algorithms to Shooter, as described in Section 1.
Here, the dynamics model uses a 7 × 7 neighborhood, which is sufﬁcient to make perfectly accurate predictions.
Figure 3a shows the discounted return of the policies generated by DAgger-MC and H-DAgger- MC, averaged over 50 independent trials.
The shaded region surrounding each curve represents a 95% conﬁdence inter- val.
The gray line marked “Random” shows the average discounted return of the uniform random policy (with a 95% conﬁdence interval).
The gray line marked “Perfect Model” shows the average discounted return of the one-ply MC planner using a perfect model.
Unsurprisingly, the performance DAgger-MC is compara- ble with that of planning with the perfect model.
As ob- served by Talvitie (2017), with the perfect reward model H-DAgger-MC performs slightly worse than DAgger-MC; the dynamics model in H-DAgger-MC receives noisier data and is thus less accurate.
Interestingly, we can now see that the learned reward model yields better performance than the perfect reward model, even without hallucinated training! The perfect reward model relies on speciﬁc screen conﬁguations that are less likely to appear in ﬂawed sample rollouts, but the learned reward model generalizes to screens not seen during training.
Of course, it is coincidental that this generalization is beneﬁcial; under standard training the reward model is only trained in environment states, giving no guidance in erroneous model states.
Hallucinated train- ing speciﬁcally trains the reward model to make reasonable predictions during model rollouts, so it yields better per- formance, comparable with that of DAgger-MC.
Thus we see that learning the reward function in this way mitigates a shortcoming of H-DAgger-MC, making it more effective in Learning the Reward Function for a Misspeciﬁed Model (a) No model limitations (b) Moving bullseyes (2nd-order Markov) (c) Pixel models use 5 × 7 neighborhood Figure 3.
Performance of DAgger-MC and H-DAgger-MC in three variations on the Shooter domain.
practice when a perfectly accurate model can be learned.
5.1.2. FAILURE OF THE MARKOV ASSUMPTION Next we consider a version of shooter presented by Talvitie (2017) in which the bullseye in each target moves from side to side, making the environment second-order Markov.
Because the model is Markov, it cannot accurately predict the movement of the bullseyes, though the representation is sufﬁcient to accurately predict every other pixel.
In Figure 3b shows the results.
As Talvitie (2017) observed, DAgger-MC fails catastrophically in this case.
Though the model’s limitation only prevents it from accurately predict- ing the bullseyes, the resulting errors compound during rollouts, quickly rendering them useless.
As previously ob- served, H-DAgger-MC performs much better, as it trains the model to produce more stable rollouts.
In both cases we see again that the learned reward models outperform the perfect reward model, and hallucinated reward training yields the best performance, even helping to mitigate impact of the ﬂaws in DAgger-MC’s model.
5.1.3. FLAWED FACTORED STRUCTURE We can see the importance of hallucinated reward training even more clearly when we consider the original Shooter domain (with static bullseyes), but limit the size of the neigh- borhood used to predict each pixel, as described in Section 1.1. Figure 3c shows the results.
Once again DAgger-MC fails.
Again we see that the learned reward models yield bet- ter performance than the perfect reward function, and that hallucinated training guides the reward model to be useful for planning, despite the ﬂaws in the dynamics model.
In this case, we can see that H-DAgger-MC also fails when combined with the perfect reward model, and performs poorly with the reward model trained only on environment states.
Hallucinated training helps the dynamics model produce stable sample rollouts, but does not correct the fun- damental limitation: the dynamics model cannot accurately predict the shape of the explosion when a target is hit.
As a result, a reward model that bases its predictions only the explosions that occur in the environment will consistently fail to predict reward when the agent hits a target in sample rollouts.
Hallucinated training, in contrast, specializes the reward model to the ﬂawed dynamics model, allowing for performance comparable to planning with a perfect model.
6.
Conclusion This paper has introduced hallucinated reward error, which measures the extent to which the rewards in a sample rollout from the model match the rewards in a parallel rollout from the environment.
Under some conditions, this quantity is more tightly related to control performance than the more traditional measure of model quality (reward error in envi- ronment states plus error in state transition).
Empirically we have seen that when the dynamics model is ﬂawed, reward functions learned in the typical manner and even “perfect” reward functions given a priori can lead to catastrophic planning failure.
When the reward function is trained to minimize hallucinated reward error, it speciﬁcally accounts for the model’s ﬂaws, signiﬁcantly improving performance.
050100150200Iteration024681012Avg.DiscountedReturnPerfectModelRandomPerfectRewardStandardTrainingHallucinatedDAgger-MC050100150200Iteration024681012Avg.DiscountedReturnPerfectModelRandomPerfectRewardStandardTrainingHallucinatedH-DAgger-MC050100150200Iteration024681012Avg.DiscountedReturnPerfectModelRandomPerfectRewardStandardTrainingHallucinatedDAgger-MC050100150200Iteration024681012Avg.DiscountedReturnPerfectModelRandomPerfectRewardStandardTrainingHallucinatedH-DAgger-MC050100150200Iteration024681012Avg.DiscountedReturnPerfectModelRandomPerfectRewardStandardTrainingHallucinatedDAgger-MC050100150200Iteration024681012Avg.DiscountedReturnPerfectModelRandomPerfectRewardStandardTrainingHallucinatedH-DAgger-MCLearning the Reward Function for a Misspeciﬁed Model References Abbeel, Pieter, Coates, Adam, Quigley, Morgan, and Ng, Andrew Y.
An application of reinforcement learning to aerobatic helicopter ﬂight.
In Advances in Neural Infor- mation Processing Systems 20 (NIPS), pp.
1–8, 2007.
Bellemare, Marc G., Veness, Joel, and Talvitie, Erik.
Skip context tree switching.
In Proceedings of the 31st Inter- national Conference on Machine Learning (ICML), pp.
1458–1466, 2014.
Bengio, Samy, Vinyals, Oriol, Jaitly, Navdeep, and Shazeer, Noam.
Scheduled sampling for sequence prediction with recurrent neural networks.
In Advances in Neural Infor- mation Processing Systems 28 (NIPS), pp.
1171–1179, 2015.
Bowling, Michael, McCracken, Peter, James, Michael, Neufeld, James, and Wilkinson, Dana.
Learning pre- dictive state representations using non-blind policies.
In Proceedings of the 23rd International Conference on Ma- chine Learning (ICML), pp.
129–136, 2006.
Bratman, Jeshua, Singh, Satinder, Sorg, Jonathan, and Lewis, Richard.
Strong mitigation: Nesting search for good policies within search for good reward.
In Proceed- ings of the 11th International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp.
407–414, 2012.
Chiappa, Silvia, Racani`ere, S´ebastien, Wierstra, Daan, and Mohamed, Shakir.
Recurrent environment simulators.
In Proceedigns of the International Conference on Learning Representations (ICLR), 2017.
Ebert, Frederik, Finn, Chelsea, Lee, Alex X., and Levine, Sergey.
Self-supervised visual planning with temporal skip connections.
In Proceedings of the 1st Annual Con- ference on Robot Learning (CoRL), volume 78 of Pro- ceedings of Machine Learning Research (PMLR), pp.
344–356, 2017.
Guo, Xiaoxiao, Singh, Satinder P., Lewis, Richard L., and Lee, Honglak.
Deep learning for reward design to im- prove monte carlo tree search in ATARI games.
In Pro- ceedings of the Twenty-Fifth International Joint Confer- ence on Artiﬁcial Intelligence (IJCAI), pp.
1519–1525, 2016.
Hausknecht, Matthew, Lehman, Joel, Miikkulainen, Risto, and Stone, Peter.
A neuroevolution approach to general atari game playing.
IEEE Transactions on Computational Intelligence and AI in Games, 6(4):355–366, 2014.
Kakade, Sham Machandranath.
On the sample complexity of reinforcement learning.
PhD thesis, University of London, 2003.
Oh, Junhyuk, Guo, Xiaoxiao, Lee, Honglak, Lewis, Richard L, and Singh, Satinder.
Action-conditional video prediction using deep networks in atari games.
In Ad- vances in Neural Information Processing Systems 28 (NIPS), pp.
2845–2853, 2015.
Oh, Junhyuk, Singh, Satinder, and Lee, Honglak.
Value prediction network.
In Advances in Neural Information Processing Systems 30, pp.
6120–6130, 2017.
Ross, Stephane and Bagnell, Drew.
Agnostic system iden- tiﬁcation for model-based reinforcement learning.
In Proceedings of the 29th International Conference on Ma- chine Learning (ICML), pp.
1703–1710, 2012.
Silver, David, van Hasselt, Hado, Hessel, Matteo, Schaul, Tom, Guez, Arthur, Harley, Tim, Dulac-Arnold, Gabriel, Reichert, David P., Rabinowitz, Neil, Barreto, Andr´e, and Degris, Thomas.
The predictron: End-to-end learning and planning.
In Proceedings of the 34th International Con- ference on Machine Learning (ICML), pp.
3191–3199, 2017.
Sorg, Jonathan, Lewis, Richard L, and Singh, Satinder.
Re- ward design via online gradient ascent.
In Advances in Neural Information Processing Systems 23 (NIPS), pp.
2190–2198, 2010a.
Sorg, Jonathan, Singh, Satinder P, and Lewis, Richard L.
Internal rewards mitigate agent boundedness.
In Proceed- ings of the 27th International Conference on Machine Learning (ICML), pp.
1007–1014, 2010b.
Sorg, Jonathan, Singh, Satinder P, and Lewis, Richard L.
Optimal rewards versus leaf-evaluation heuristics in plan- ning agents.
In Proceedings of the Twenty-Fifth AAAI Conference on Artiﬁcial Intelligence (AAAI), pp.
465–470, 2011.
Szita, Istv´an and Szepesv´ari, Csaba.
Model-based reinforce- ment learning with nearly tight exploration complexity bounds.
In Proceedings of the 27th International Con- ference on Machine Learning (ICML), pp.
1031–1038, 2010.
Talvitie, Erik.
Model regularization for stable sample roll- outs.
In Proceedings of the 30th Conference on Uncer- tainty in Artiﬁcial Intelligence (UAI), pp.
780–789, 2014.
Talvitie, Erik.
Agnostic system identiﬁcation for monte carlo planning.
In Proceedings of the 29th AAAI Con- ference on Artiﬁcial Intelligence (AAAI), pp.
2986–2992, 2015.
Talvitie, Erik.
Self-correcting models for model-based re- inforcement learning.
In Proceedings of the Thirty-First AAAI Conference on Artiﬁcial Intelligence (AAAI), pp.
2597–2603, 2017.
Learning the Reward Function for a Misspeciﬁed Model Tamar, Aviv, Wu, Yi, Thomas, Garrett, Levine, Sergey, and Abbeel, Pieter.
Value iteration networks.
In Advances in Neural Information Processing Systems 29 (NIPS), pp.
2154–2162, 2016.
Tesauro, Gerald and Galperin, Gregory R.
On-line policy In Advances improvement using monte-carlo search.
in Neural Information Processing Systems 9 (NIPS), pp.
1068–1074, 1996.
Veness, Joel, Ng, Kee Siong, Hutter, Marcus, Uther, William T.
B., and Silver, David.
A Monte-Carlo AIXI Approxi- mation.
Journal of Artiﬁcial Intelligence Research (JAIR), 40:95–142, 2011.
Veness, Joel, Ng, Kee Siong, Hutter, Marcus, and Bowling, Michael.
Context tree switching.
In Proceedings of the 2012 Data Compression Conference (DCC), pp.
327–336, 2012.
Venkatraman, Arun, Hebert, Martial, and Bagnell, J.
An- drew.
Improving multi-step prediction of learned time series models.
In Proceedings of the 29th AAAI Confer- ence on Artiﬁcial Intelligence (AAAI), pp.
3024–3030, 2015.
Venkatraman, Arun, Capobianco, Roberto, Pinto, Lerrel, Hebert, Martial, Nardi, Daniele, and Bagnell, J Andrew.
Improved learning of dynamics models for control.
In 2016 International Symposium on Experimental Robotics, pp.
703–713.
Springer, 2016.
A.
Hallucinated DAgger-MC Details Hallucinated DAgger-MC, like earlier variations on DAgger, requires the ability to reset to the initial state distribution µ and also the ability to reset to an “exploration distribution” ν.
The exploration distribution ideally ensures that the agent will encounter states that would be visited by a good policy.
The performance bound for H-DAgger-MC depends in part on the quality of the selected ν.
In addition to assuming a particular form for the planner (one-ply MC with a blind rollout policy), H-DAgger-MC requires the dynamics model to be “unrolled”.
Rather than learning a single ˆP , H-DAgger-MC learns a set { ˆP 1, .
.
.
, ˆP T−1} ⊆ P, where model ˆP i is responsible for predicting the outcome of step i of a rollout, given the state sampled from ˆP i−1.
While this impractical assumption is important theoretically, Talvitie (2017) showed that in prac- tice a single P can be used for all steps; the experiments in Section 5 make use of this practical alteration.
Algorithm 1 augments H-DAgger-MC to learn a reward model as well as a dynamics model.
In particular, H- DAgger-MC proceeds in iterations, each iteration producing , ˆR1).
for k ← 1 .
.
.
K do With probability...
(cid:46) First sample from ξ Algorithm 1 Hallucinated DAgger-MC (+ reward learning) Require: LEARN-DYNAMICS, LEARN-REWARD, explo- ration distr.
ν, MC-PLANNER(blind rollout policy ρ, depth T ), # iterations N, # rollouts per iteration K.
and E1 (maybe using ν) ← LEARN-DYNAMICS(D1:T−1 ).
1: Get initial datasets D1:T−1 2: Initialize ˆP 1:T−1 3: Initialize ˆR1 ← LEARN-REWARD(E1).
4: Initialize ˆπ1 ← MC-PLANNER( ˆP 1:T−1 5: for n ← 2 .
.
.
N do 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 1/2: Sample (x, b) ∼ D ˆπn 1/4: Reset to (x, b) ∼ ν.
(1−γ)/4: Sample x ∼ µ, b ∼ ˆπn(· | x).
γ/4: Reset to (y, c) ∼ ν Sample x ∼ P (· | y, c), b ∼ ˆπn(· | x) Let s ← x, z ← x, a ← b.
for t ← 1 .
.
.
T − 1 do (cid:46) Parallel rollouts...
Sample s(cid:48) ∼ P (· | s, a).
Add (cid:104)z, a, s(cid:48)(cid:105) to Dt n.
(cid:46) (DAgger-MC adds (cid:104)s, a, s(cid:48)(cid:105)) Add (cid:104)z, a, Ra s , γt−1(cid:105)) (cid:46) (Standard approach adds (cid:104)s, a, Ra Sample z(cid:48) ∼ ˆP t Let s ← s(cid:48), z ← z(cid:48), and sample a ∼ ρ.
s , γt−1(cid:105) to En. n−1(· | z, a).
17: 18: 19: 20: 21: end for Add (cid:104)z, a, Ra s , γT−1(cid:105) to En. (cid:46) (Standard approach adds (cid:104)s, a, Ra end for ← LEARN-DYNAMICS( ˆP 1:T−1 ˆP 1:T−1 n−1 ˆRn ← LEARN-REWARD( ˆRn−1, En) ˆπn ← MC-PLANNER( ˆP 1:T−1 , ˆRn).
22: 23: 24: 25: 26: end for 27: return the sequence ˆπ1:N s , γT−1(cid:105)) , D1:T−1 a new plan, which is turn used to collect data to train a new model.
In each iteration state-action pairs are sampled using the current plan and the exploration distribution (lines 7-13), and then the world and model are rolled out in parallel to generate hallucinated training examples (lines 14-21).
The resulting data is used to update the model.
We simply add a reward model learning process, and collect training ex- amples along with the state transition examples during the rollout.
After both parts of the model have been updated, a new plan is generated for the subsequent iteration.
Note that while the dynamics model is “unrolled”, there is only a single reward model that is responsible for predicting the reward at every step of the rollout.
We assume that the re- ward learning algorithm is performing a weighted regression (where each training example is weighted by γt−1 for the rollout step t in which it occurred).
Learning the Reward Function for a Misspeciﬁed Model A.1. Analysis of H-DAgger-MC We now derive theoretical guarantees for this new version of H-DAgger-MC.
The analysis is similar to that of existing DAgger variants (Ross & Bagnell, 2012; Talvitie, 2015; 2017), but the proof is included for completeness.
Let H t be the distribution from which H-DAgger-MC samples a training example at depth t (lines 7-13 to pick an initial state-action pair, lines 14-21 to roll out).
Deﬁne the average error of the dynamics model at depth t to be where ξπ,ˆπn (s, a) = (cid:16) Dµ,ˆπn(s, a) + Dµ,π(s, a) (1 − γ)µ(s)ˆπn(a | s) + γ Dµ,π(z, b)P b (cid:17) z (s)ˆπn(a | s) (cid:88) z,b Then, combining the above with Theorem 5, N(cid:88) N(cid:88) n=1 1 − γ ≤ 1 1 − γ n=1 T(cid:88) t=1 (s,a)∼ξπ,ˆπn [| ˆQρ T,n(s, a) − Qρ T (s, a)|] + ¯mc γt−1 (s, z, a) ∼ H t,n π,ˆπn [ ˆRn (s, z, a)] + ¯mc ,ρ Now note that for any t and any n, (cid:2) ˆRn (s, z, a)(cid:3) (s,z,a)∼H t,n π,ˆπn ,ρ (cid:88) (cid:88) (cid:88) s(cid:48),a(cid:48) s(cid:48),a(cid:48) (cid:88) s(cid:48),a(cid:48) s(cid:48)(cid:48),a(cid:48)(cid:48) Dµ,ˆπn (s(cid:48), a(cid:48)) (s,z,a)∼H t,n s(cid:48) ,a(cid:48) ,ρ Dµ,π(s(cid:48), a(cid:48)) (s,z,a)∼H t,n Dµ,π(s(cid:48)(cid:48), a(cid:48)(cid:48))P a(cid:48)(cid:48) s(cid:48) ,a(cid:48),ρ s(cid:48)(cid:48) (s(cid:48))ˆπn(a(cid:48) | s(cid:48)) (cid:88) s(cid:48),a(cid:48) 1 − γ (s,z,a)∼H t,n s(cid:48) ,a(cid:48),ρ µ(s(cid:48))ˆπn(a(cid:48) | s(cid:48)) Dµ,ˆπn (s(cid:48), a(cid:48)) (s,z,a)∼H t,n s(cid:48) ,a(cid:48) ,ρ (s,z,a)∼H t,n s(cid:48) ,a(cid:48) ,ρ (cid:88) s(cid:48),a(cid:48) cπ cπ ≤ 1 (cid:88) (cid:88) s(cid:48),a(cid:48) s(cid:48),a(cid:48) s(cid:48)(cid:48),a(cid:48)(cid:48) (cid:88) s(cid:48),a(cid:48) 1 − γ (s,z,a)∼H t,n s(cid:48) ,a(cid:48) ,ρ µ(s(cid:48))ˆπn(a(cid:48) | s(cid:48)) (s,z,a)∼H t,n s(cid:48) ,a(cid:48) ,ρ ν(s(cid:48), a(cid:48)) (cid:88) (s,z,a)∼H t,n ν(s(cid:48)(cid:48), a(cid:48)(cid:48))P a(cid:48)(cid:48) s(cid:48) ,a(cid:48) ,ρ s(cid:48)(cid:48) (s(cid:48))ˆπn(a(cid:48) | s(cid:48)) (s, z, a)(cid:3) (cid:2) ˆRn (s, z, a)(cid:3) (cid:2) ˆRn (s, z, a)(cid:3) (cid:2) ˆRn (s, z, a)(cid:3) (cid:2) ˆRn (cid:2) ˆRn (s, z, a)(cid:3) (cid:2) ˆRn (s, z, a)(cid:3) (cid:2) ˆRn (s, z, a)(cid:3) (cid:2) ˆRn (s, z, a)(cid:3) N(cid:88) n=1 N(cid:88) T(cid:88) n=1 t=1 ¯t prd = E(s,z,a)∼H t [1 − ˆP t s | z, a)].
n(σa Let  ˆRn (s, z, a) = |R(s, a) − ˆRn(z, a)| and let ¯hrwd = γt−1 E(s,z,a)∼H t [ ˆRn (s, z, a)|] be the average reward model error.
Finally, let Dt n be the distribution from which H-DAgger-MC samples s and a during the rollout in lines 14-21.
The error of the reward model with respect to these environment states is γt−1 E(s,a)∼Dt [|R(s, a) − ˆR(s, a)|].
N(cid:88) T(cid:88) n=1 t=1 ¯erwd = Dµ,π(s,a) ν(s,a) ν = sups,a For a policy π, let cπ represent the mis- match between the discounted state-action distribution under π and the exploration distribution ν.
Now, consider the se- quence of policies ˆπ1:N generated by H-DAgger-MC.
Let ¯π be the uniform mixture over all policies in the sequence.
Let T (cid:107)∞ ¯mc = 1 be the error induced by the choice of planning algorithm, averaged over all iterations.
Lemma 7.
In H-DAgger-MC, the policies ˆπ1:N are such that for any policy π, (cid:80)N n=1 (cid:107) ¯Qn− ˆQρ T,n(cid:107)∞ + 2 1−γ(cid:107)BV ρ T −V ρ 1−γ (cid:2)V π(s) − V ¯π(s)(cid:3) ≤ 4 s∼µ (cid:16) cπ ν ¯hrwd + ¯mc 1 − γ T−1(cid:88) (cid:17) ≤ 4 1 − γ cπ ¯erwd + 2M γt−1¯t prd + ¯mc.
t=1 s∼µ (cid:2)V π(s) − V ˆπn(s)(cid:3).
N(cid:88) Proof.
Recall that (cid:2)V π(s) − V ¯π(s)(cid:3) = (cid:2)V π(s) − V ˆπn (s)(cid:3) ≤ s∼µ and by Lemma 1 for any n ≥ 1, s∼µ n=1 1 − γ (s,a)∼ξπ,ˆπn [| ˆQρ T,n(s, a) − Qρ T (s, a)|] + ¯mc, (cid:18) 1 (cid:88) s(cid:48),a(cid:48) ≤ cπ Dµ,ˆπn (s(cid:48), a(cid:48)) Learning the Reward Function for a Misspeciﬁed Model (s, z, a)(cid:3) (cid:2) ˆRn (s, z, a)(cid:3) (cid:2) ˆRn (s, z, a)(cid:3) (cid:2) ˆRn (s, z, a)(cid:3) (cid:2) ˆRn (s,z,a)∼H t,n s(cid:48),a(cid:48) ,ρ ν(s(cid:48), a(cid:48)) (cid:88) (s,z,a)∼H t,n ν(s(cid:48)(cid:48), a(cid:48)(cid:48))P a(cid:48)(cid:48) s(cid:48) ,a(cid:48) ,ρ s(cid:48)(cid:48) (s(cid:48))ˆπn(a(cid:48) | s(cid:48)) (cid:88) (cid:88) s(cid:48),a(cid:48) s(cid:48),a(cid:48) s(cid:48)(cid:48),a(cid:48)(cid:48) (cid:88) s(cid:48),a(cid:48) 1 − γ (s,z,a)∼H t,n s(cid:48),a(cid:48) ,ρ µ(s(cid:48))ˆπn(a(cid:48) | s(cid:48)) (cid:2) ˆRn (s,z,a)∼H t,n (s, z, a)(cid:3).
s(cid:48),a(cid:48) ,ρ (cid:2) ˆRn (s, z, a)(cid:3) = (cid:2) ˆRn (s, z, a)(cid:3).
(s,a)∼ξn(s,a) = cπ (s,z,a)∼H t,n ξn ,ρ When t = 1, (s,z,a)∼H t,n ξn,ρ When t > 1, (s,z,a)∼H t,n ξn ,ρ (cid:88) (cid:2) ˆRn (s, z, a)(cid:3) (cid:20) (cid:88) Theorem 3, ¯hrwd = n=1 N(cid:88) T(cid:88) (cid:18) T(cid:88) T−1(cid:88) t=1 t=1 t=1 N(cid:88) n=1 N(cid:88) n=1 + 2M T(cid:88) T−1(cid:88) t=1 t=1 + 2M ≤ 1 (s,z,a)∼H t γt−1 E (s,a)∼Dt γt−1 (s,z,a)∼H t (cid:2) ˆRn(s, z, a)(cid:3) (cid:2)|R(s, a) − ˆRn(s, a)|(cid:3) s | z, a)|(cid:3)(cid:19) (cid:2)1 − ˆP t n(σa (cid:2)|R(s, a) − ˆRn(s, a)|(cid:3) (cid:2)1 − ˆP t s | z, a)|(cid:3) n(σa (s,z,a)∼H t γt−1 E (s,a)∼Dt N(cid:88) n=1 γt−1 1 T−1(cid:88) ≤ ¯erwd + 2M γt−1¯t prd.
t=1 This gives the second inequality.
Note that this result holds for any comparison policy π.
Thus, if ¯mc is small and the learned models have low error, then if ν is similar to the state-action distribution under some good policy, ¯π will compare favorably to it.
That said, Lemma 7 shares the limitations of the comparable results for the other DAgger algorithms.
It focuses on the L1 loss, which is not always a practical learning objective.
It also assumes that the expected loss at each iteration can be computed exactly (i.e. that there are inﬁnitely many samples per iteration).
It also applies to the average policy ¯π, rather than ˆπN .
Ross & Bagnell (2012) discuss extensions that address more practical loss functions, ﬁnite sample bounds, and results for ˆπN .
Lemma 7 effectively says that if the models have low train- ing error, the resulting policy will be good.
It does not promise that the models will have low training error.
Fol- lowing Ross & Bagnell (2012) note that ¯t prd and ¯hrwd can each be interpreted as the average loss of an online learner on the problem deﬁned by the aggregated datasets.
Then for each horizon depth t let ¯t pmdl be the error of the best dynamics model in P under the training distribution at that depth, in retrospect.
Speciﬁcally, ¯tP = inf P (cid:48)∈P (s,z,a)∼H t [1 − P (cid:48)(σa s | z, a)].
N(cid:88) n=1 (s1,a1)∼ξn st,zt,at a1:t−1 ρ(a2:t | a1) P a0:t−1 s1 (st | s1, a0:t−1) ˆP 1:t−1 (cid:2) ˆRn (s, z, a)(cid:3).
(s,z,a)∼H t (zt | s1, a0:t−1) (cid:21)  ˆRn (st, zt, at) Thus, putting it all together, we have shown that (cid:2)V π(s) − V ¯π(s)(cid:3) N(cid:88) T(cid:88) s∼µ ≤ 4 1 − γ cπ n=1 t=1 1 − γ cπ ν ¯hrwd + ¯mc.
(cid:2) ˆRn (s, z, a)(cid:3) γt−1 (s,z,a)∼H t + ¯mc Similarly, let ¯R = inf R(cid:48)∈R N(cid:88) T(cid:88) n=1 t=1 γt−1 (s,z,a)∼ ˜H t [R(cid:48)(s, z, a)] Thus we have proven the ﬁrst inequality.
Furthermore, by be the error of the best reward model in R in retrospect.
Learning the Reward Function for a Misspeciﬁed Model The average regret for the dynamics model at depth t is prd − ¯tP.
For the reward model it is ¯rrgt = prgt = ¯t ¯t ¯hrwd − ¯R.
For a no-regret online learning algorithm, average regret approaches 0 as N → ∞.
This gives the following bound on H-DAgger-MC’s performance in terms of model regret.
Theorem 8.
In H-DAgger-MC, the policies ˆπ1:N are such that for any policy π, (cid:2)V π(s) − V ¯π(s)(cid:3) s∼µ ≤ 4 1 − γ ≤ 4 1 − γ (cid:16) cπ ν (¯R + ¯rrgt) + ¯mc T−1(cid:88) t=1 (cid:17) cπ ¯erwd + 2M γt−1(¯tP + ¯t prgt) + ¯mc prgt → 0 for each 1 ≤ t ≤ T − 1.
and if the learning algorithms are no-regret then as N → ∞, ¯rrgt → 0 and ¯t Theorem 8 says that if R contains a low-error reward model relative to the learned dynamics models then, as discussed above, if ¯mc is small and ν visits important states, the resulting policy will yield good performance.
If P and R contain perfect models, ¯π will be comparable to the plan generated by the perfect model.
As noted by Talvitie (2017), this result does not promise that H-DAgger-MC will eventually achieve the performance of the best available set of dynamics models.
The model at each rollout depth is trained to minimize prediction error given the input distribution provided by the shallower models without regard for the effect on deeper models.
It is possible that better overall error could be achieved by increasing the prediction error at one depth in exchange for a favorable state distribution for deeper models.
Similarly, as discussed in Section 4, H-DAgger-MC will not necessarily achieve the performance of the best available combination of dynamics and reward models.
The dynamics model is trained without regard for the impact on the reward model.
It could be that a dynamics model with higher prediction error would allow for lower hallucinated reward error.
H-DAgger-MC does not take this possibility into account.

long history in deep learning (Hinton & Salakhutdinov, 2006; Autoencoders have Salakhutdinov & Hinton, 2009a; Vincent et al., 2010; Kingma & Welling, 2013).
In most cases, autoencoders operate on continuous representations, either by simply making a bottleneck (Hinton & Salakhutdinov, 2006), denoising (Vincent et al., 2010), or adding a variational compo- nent (Kingma & Welling, 2013).
In many cases though, a discrete latent representation is potentially a better ﬁt.
Language is inherently discrete, and autoregressive models based on sequences of discrete symbols yield impressive results.
A discrete representation can be fed into a reasoning or planning system or act as a bridge towards any other part of a larger system.
Even in reinforcement learning where action spaces are naturally continuous, Metz et al.
(2017) show that discretizing them and using autoregressive models can yield improvements.
Unluckily, using discrete latent variables is challenging in deep learning.
And even with continuous autoencoders, the interactions with an autoregressive component cause difﬁculties.
Despite some success (Bowman et al., 2016; Yang et al., 2017), the task of meaningfully autoencoding text in the presence of an autoregressive decoder has remained a challenge.
In this work we present an architecture that autoencodes a sequence s of N discrete symbols from any vocabulary (e.g., a tokenized sentence), into a K-fold (we test K = 8 and K = 32) compressed sequence c(s) of ⌈ N K ⌉ latent symbols from a new vocabulary which is learned.
The compressed sequence is generated to minimize perplexity in a (possibly conditional) language model trained to predict the next token on c(s) ◦ s: the concatenation of c(s) with the original sequence s.
Since gradient signals can vanish when propagating over discrete variables, the compression func- tion c(s) can be hard to train.
To solve this problem, we draw from the old technique of seman- tic hashing (Salakhutdinov & Hinton, 2009b).
There, to discretize a dense vector v one computes σ(v + n) where σ is the sigmoid function and n represents annealed Gaussian noise that pushes the network to not use middle values in v.
We enhance this method by using a saturating sigmoid and a straight-through pass with only bits passed forward.
These techniques, described in detail below, allow to forgo the annealing of the noise and provide a stable discretization mechanism that requires neither annealing nor additional loss factors.
We test our discretization technique by amending language models over s with the autoencoded sequence c(s).
We compare the perplexity achieved on s with and without the c(s) component, and contrast this value with the number of bits used in c(s).
We argue that this number is a proper measure for the performance of a discrete autoencoder.
It is easy to compute and captures the performance of the autoencoding part of the model.
This quantitative measure allows us to compare the technique we introduce with other methods, and we show that it performs better than a Gumbel- Softmax (Jang et al., 2016; Maddison et al., 2016) in this context.
Finally, we discuss the use of adding the autoencoded part c(s) to a sequence model.
We present samples from a character-level language model and show that the latent symbols correspond to words and phrases when the architecture of c(s) is local.
ehen, we introduce a decoding method in which c(s) is sampled and then s is decoded using beam search.
This method alleviates a number of problems observed with beam search or pure sampling.
We show how our decoding method can be used to obtain diverse translations of a sentence from a neural machine translation model.
To summarize, the main contributions of this paper are: (1) a discretization technique that works well without any extra losses or parameters to tune, (2) a way to measure performance of autoencoders for sequence models with baselines, (3) an improved way to sample from sequence models trained with an autoencoder part.
2 TECHNIQUES Below, we introduce our discretization method, the autoencoding function c(s) and ﬁnally the com- plete model that we use for our experiments.
All code and hyperparameter settings needed to repli- cate our experiments are available as open-source1.
2.1 DISCRETIZATION BY IMPROVED SEMANTIC HASHING As already mentioned above, our discretization method stems from semantic hashing (Salakhutdinov & Hinton, 2009b).
To discretize a b-dimensional vector v, we ﬁrst add noise, so vn = v + n.
The noise n is drawn from a b-dimensional Gaussian distribution with mean 0 and standard deviation 1 (deviations between 0 and 1.5 all work ﬁne, see ablations below).
The sum is component-wise, as are all operations below.
Note that noise is used only for training, during evalua- tion and inference n = 0.
From vn we compute two vectors: v1 = σ′(vn) and v2 = (vn < 0), where σ′ is the saturating sigmoid function from (Kaiser & Sutskever, 2016; Kaiser & Bengio, 2016): σ′(x) = max(0, min(1, 1.2σ(x) − 0.1)).
The vector v2 represents the discretized value of v and is used for evaluation and inference.
During training, in the forward pass we use v1 half of the time and v2 the other half.
In the backward pass, we let gradients always ﬂow to v1, even if we used v2 in the forward computation2.
We will denote the vector v discretized in the above way by vd.
Note that if v is b-dimensional then vd will have b bits.
Since in other parts of the system we will predict vd with a softmax, we want the number of bits to not be too large.
In our experiments we stick with b = 16, so vd is a vector of 16 bits, and so can be interpreted as an integer between 0 and 216 − 1 = 65535.
The dense vectors representing activations in our sequence models have much larger dimen- sionality than 16 (often 512, see the details in the experimental section below).
To discretize such a high-dimensional vector w we ﬁrst have a simple fully-connected layer converting it into v = dense(w, 16).
In our notation, dense(x, n) denotes a fully-connected layer applied to x and mapping it into n dimensions, i.e., dense(x, n) = xW + B where W is a learned matrix of shape d × n, where d is the dimensionality of x, and B is a learned bias vector of size n.
The discretized vector vd is converted back into a high-dimensional vector using a 3-layer feed-forward network: h1a = dense(vd, filter_size) h1b = dense(1.0 - vd, filter_size) 1See transformer vae.py in https://github.com/tensorflow/tensor2tensor 2This can be done in TensorFlow using: v2 += v1 - tf.stop gradient(v1).
h2 = dense(relu(h1a + h1b), filter_size) result = dense(relu(h2), hidden_size) Above, every time we apply dense we create a new weight matrix an bias to be learned.
The relu function is deﬁned in the standard way: relu(x) = max(x, 0).
In the network above, we usually use a large filter size; in our experiments we set it to 4096 while hidden size was usually 512.
We suspect that this allows the above network to recover from the discretization bottleneck by simulating the distribution of w encountered during training.
Given a dense, high-dimensional vector w we will denote the corresponding result returned from the network above by bottleneck(w) and the corresponding discrete vector v2 by discrete(w).
2.2 GUMBEL-SOFTMAX FOR DISCRETIZATION As an alternative discretization method, we consider the recently studied Gumbel-Softmax (Jang et al., 2016; Maddison et al., 2016).
In that case, given a vector w we compute discreteg(w) by applying a linear layer mapping into 216 elements, resulting in the logits l.
During evaluation and inference we simply pick the index of l with maximum value for discreteg(w) and the vector bottleneckg(w) is computed by an embedding.
During training we ﬁrst draw samples g from the Gumbel distribution: g ∼ − log(− log(u)), where u ∼ U(0, 1) are uniform samples.
Then, as in (Jang et al., 2016), we compute x, the log-softmax of l, and set: yi = exp((xi + gi)/τ ) Pi exp((xi + gi)/τ With low temperature τ this vector is close to the 1-hot vector representing the maximum index of l.
But with higher temperature, it is an approximation (see Figure 1 in Jang et al.
(2016)).
We multiply this vector y by the embedding matrix to compute bottleneckg(w) during training.
2.3 AUTOENCODING FUNCTION Having the functions bottleneck(w) and discrete(w) (respectively their Gumbel-Softmax versions), we can now describe the architecture of the autoencoding function c(s).
We assume that s is already a sequence of dense vectors, e.g., coming from embedding vectors from a tokenized sentence.
To halve the size of s, we ﬁrst apply to it 3 layers of 1-dimensional convolutions with kernel size 3 and padding with 0s on both sides (SAME-padding).
We use ReLU non-linearities between the layers and layer-normalization (Ba et al., 2016).
Then, we add the input to the result, forming a residual block.
Finally, we process the result with a convolution with kernel size 2 and stride 2, effectively halving the size of s.
In the local version of this function we only do the ﬁnal strided convolution, without the residual block.
To autoencode a sequence s and shorten it K-fold, with K = 2k, we ﬁrst apply the above step k times obtaining a sequence s′ that is K times shorter.
Then we put it through the discretization bottleneck described above.
The ﬁnal compression function is given by c(s) = bottleneck(s′) and the architecture described above is depicted in Figure 1.
Note that, since we perform 3 convolutions with kernel 3 in each step, the network has access to a large context: 3 · 2k−1 just from the receptive ﬁelds of convolutions in the last step.
That’s why we also consider the local version.
With only strided convolutions, the i-th symbol in the local c(s) has only access to a ﬁxed 2k symbols from the sequence s and can only compress them.
Training with c(s) deﬁned above from scratch is hard, since at the beginning of training s′ is generated by many layers of untrained convolutions that are only getting gradients through the discretization bottleneck.
To help training, we add a side-path for c(s) without discretization: we just use c(s) = s′ for the ﬁrst 10000 training steps.
In this pretraining stage the network reaches loss of almost 0 as everything needed to reconstruct s is encoded in s′.
After switching to c(s) = bottleneck(s′) the loss is high again and improves during further training.
2.4 AUTOENCODING SEQUENCE MODEL To test the autoencoding function c(s) we will use it to preﬁx the sequence s in a sequence model.
Normally, a sequence model would generate the i-th element of s conditioning on all elements of Single step Autoencoding function c(s) length×hidden size length×hidden size relu convk=3 s=1 layer-norm convk=2 s=2 ×3 single step ×k bottleneck length/2×hidden size length/K×hidden size Figure 1: Architecture of the autoencoding function c(s).
We write convk=a volutional layer with kernel size a and stride b.
See text for more details.
s=b to denote a 1D con- si+1 si si+1 si reverse c(s) Standard language model.
Autoencoder-augmented language model.
Figure 2: Comparison of a standard language model and our autoencoder-augmented model.
The architecture for c(s) is presented in Figure 1 and the arrows from si to s<i depict dependence.
s before that, s<i, and possibly on some other inputs.
For example, a language model would just condition on s<i while a neural machine translation model would condition on the input sentence (in the other language) and s<i.
We do not change the sequence models in any way other than adding the sequence c(s) as the preﬁx of s.
Actually, for reasons analogous to those in (Sutskever et al., 2014), we ﬁrst reverse the sequence c(s), then add a separator symbol (#), and only then concatenate it with s, as depicted in Figure 2.
We also use a separate set of parameters for the model predicting c(s) so as to make sure that the models predicting s with and without c(s) have the same capacity.
As the architecture for the sequence model we use the Transformer (Vaswani et al., 2017).
Trans- former is based on multiple attention layers and was originally introduced in the context of neural machine translation.
We focused on the autoencoding function c(s) and did not tune the sequence model in this work: we used all the defaults from the baseline provided by the Transformer authors (6 layers, hidden size of 512 and ﬁlter size of 4096) and only varied parameters relevant to c(s).
3 EXPERIMENTS We experimented with autoencoding on 3 different sequence tasks: (1) on a character-level language model, (2) on a word-level language model, and (3) on a word-level translation model.
The goal for (1) was to check if our technique works at all, since character sequences are naturally amenable to compression into shorter sequences of objects from a larger vocabulary.
For (2), we wanted to check if the good results obtained in (1) will still hold if the input is from a larger vocabulary and inherently more compressed space.
Finally, in (3) we want to check if this method is applicable to conditional models and how it can be used to improve decoding.
We use the LM1B corpus (Chelba et al., 2013) for language modelling and we tokenize it using a subword (wordpiece) tokenizer (Sennrich et al., 2016) into a vocabulary of 32000 words and word- pieces.
For translation, we use the WMT English-German corpus, similarly tokenized into a vocab- ulary of 32000 words and word-pieces3.
3We used https://github.com/tensorflow/tensor2tensor for data preparation.
Problem LM-en (characters) LM-en (word) NMT-en-de (word) LM-en (word, Gumbel-Softmax) NMT-en-de (word, Gumbel-Softmax) ln(p) 1.027 3.586 1.449 3.586 1.449 ln(p’) K DSAE 59% 0.822 55% 2.823 19% 1.191 3.417 12% 0% 1.512 32 Table 1: Log-perplexities per word of sequence models with and without autoencoders, and their autoencoding efﬁciency.
Results for Gumbel-Softmax heavily depend on tuning; see text for details.
Below we report both qualitative and quantitative results.
First, we focus on measuring the perfor- mance of our autoencoder quantitatively.
To do that, we introduce a measure of discrete autoen- coder performance on sequence tasks and compare our semantic hashing based method to Gumbel- Softmax on this scale.
3.1 DISCRETE SEQUENCE AUTOENCODING EFFICIENCY Sequence models trained for next-symbol prediction are usually trained (and often also evaluated) based on the perplexity per token that they reach.
Perplexity is deﬁned as 2H , where H is the entropy (in bits) of a distribution.
Therefore, a language model that reaches a per-word perplexity of p, say p = 32, on a sentence s can be said to compress each word from s into log(p) = 5 bits of information.
Let us now assume that this model is allowed to access some additional bits of information about s before decoding.
In our autoencoding case, we let it peek at c(s) before decoding s, and c(s) has K = 8 times less symbols and b = 16 bits in each symbol.
So c(s) has the information capacity of 2 bits per word.
If our autoencoder was perfectly aligned with the needs of the language model, then allowing it to peek into c(s) would lower its information needs by these 2 bits per word.
The perplexity p′ of the model with access to c(s) would thus satisfy log2(p′) = 5 − 2 = 3, so its perplexity would be p′ = 8.
Getting the autoencoder c(s) perfectly aligned with the language model is hard, so in practice the perplexity p′ is always higher.
But since we measure it (and optimize for it during training), we can calculate how many bits has the c(s) part actually contributed to lowering the perplexity.
We calculate log2(p) − log2(p′) and then, if c(s) is K-times shorter than s and uses b bits, we deﬁne the discrete sequence autoencoding efﬁciency as: DSAE = K(log2(p) − log2(p′)) K(ln(p) − ln(p′)) b ln(2) The second formulation is useful when the raw numbers are given as natural logarithms, as is often the case during neural networks training.
Deﬁned in this way, DSAE measures how many of the available bits in c(s) are actually used well by the model that peeks into the autoencoded part.
Note that some models may have autoencod- ing capacity higher than the number of bits per word that log(p) indicates.
In that case achieving DSAE=1 is impossible even if log(p′) = 0 and the autoencoding is perfect.
One should be careful when reporting DSAE for such over-capacitated models.
So how does our method perform on DSAE and how does it compare with Gumbel-Softmax?
In Ta- ble 1 we list log-perplexties of baseline and autoencoder models.
We report numbers for the global version of c(s) on our 3 problems and compare it to Gumbel-Softmax on word-level problems.
We did not manage to run the Gumbel-Softmax on character-level data in our baseline conﬁguration because it requires too much memory (as it needs to learn the embeddings for each latent discrete symbol).
Also, we found that the results for Gumbel-Softmax heavily depend on how the tempera- ture parameter τ is annealed during training.
We tuned this on 5 runs of a smaller model and chose the best conﬁguration.
This was still not enough, as in many runs the Gumbel-Softmax would only utilize a small portion of the discrete symbols.
We added an extra loss term to increase the variance of the Gumbel-Softmax and ran another 5 tuning runs to optimize this loss term.
We used the best Noise standard deviation 1.5 1.0 0.5 0.0 ln(p) 3.912 3.912 3.912 3.912 ln(p’) K DSAE 43.2% 3.313 48.5% 3.239 48.5% 3.236 3.288 45.0% Table 2: Autoencoder-augmented language models with different noise deviations.
All values from no noise (0.0) upto a deviation of 1.5 yield DSAE between 40% and 50%.
conﬁguration for the experiments above.
Still, we did not manage to get any information autoen- coded in the translation model, and got only 12% efﬁciency in the language model (see Table 1).
Our method, on the other hand, was most efﬁcient on character-level language modeling, where we reach almost 60% efﬁciency, and it retained high 55% efﬁciency on the word-level language modeling task.
On the translation task, our efﬁciency goes down to 19%, possibly because the c(s) function does not take inputs into account, and so may not be able to compress the right parts to align with the conditional model that outputs s depending on the inputs.
But even with 19% efﬁciency it is still useful for sampling from the model, as shown below.
3.2 SENSITIVITY TO NOISE To make sure that our autoencoding method is stable, we experiment with different standard devi- ations for the noise n in the semantic hashing part.
We perform these experiments on word-level language modelling with a smaller model conﬁguration (3 layers, hidden size of 384 and ﬁlter size of 2048).
The results, presented in Table 2, show that our method is robust to the amount of noise.
Interestingly, we see that our method works even without any noise (standard deviation 0.0).
We suspect that this is due to the fact that half of the time in the forward computation we use the discrete values anyway and pass gradients through to the dense part.
Also, note that a standard deviation of 1.5 still works, despite the fact that our saturating sigmoid is saturated for values above 2.4 as 1.2 · σ(2.4) − 0.1 = 1.0002.
Finally, with deviation 1.0 the small model achieves DSAE of 48.5%, not much worse than the 55% achieved by the large baseline model and better than the larger baseline model with Gumbel-Softmax.
3.3 DECIPHERING THE LATENT CODE Having trained the models, we try to ﬁnd out whether the discrete latent symbols have any inter- pretable meaning.
We start by asking a simpler question: do the latent symbols correspond to some ﬁxed phrases or topics?
We ﬁrst investigate this in a 32-fold compressed character-level language model.
We set c(s) to 4 random latent symbols [l1, l2, l3, l4] and decode s with beam search, obtaining: All goods are subject to the Member States’ environmental and security aspects of the common agricultural policy.
Now, to ﬁnd out whether the second symbol in c(s) stands for anything ﬁxed, we replace the third symbol by the second one, hoping for some phrase to be repeated.
Indeed, decoding s from the new c(s) = [l1, l2, l2, l4] with beam search we obtain: All goods are charged EUR 50.00 per night and EUR 50.00 per night stay per night.
Note that the beginning of the sentence remained the same, as we did not change the ﬁrst symbol, and we see a repetition of EUR 50.00 per night.
Could it be that this is what that second latent symbol stands for?
But there were no EUR in the ﬁrst sentence.
Let us try again, now changing the ﬁrst symbol to a different one.
With c(s) = [l5, l2, l2, l4] the decoded s is: All bedrooms suited to the large suite of the large living room suites are available.
We see a repetition again, but of a different phrase.
So we are forced to conclude that the latent code is structured, the meaning of the latent symbols can depend on other symbols before them.
Failing to decipher the code from this model, we try again with an 8-fold compressed character-level language model that uses the local version of the function c(s).
Recall (see Section 2.3) that a local function c(s) with 8-fold compression generates every latent symbol from the exact 8 symbols that correspond to it in s, without any context.
With this simpler c(s) the model has lower DSAE, 35%, but we expect the latent symbols to be more context-independent.
And indeed: if we pick the ﬁrst 2 latent symbols at random but ﬁx the third, fourth and ﬁfth to be the same, we obtain the following: It’s studio, rather after a gallery gallery ...
When prices or health after a gallery gallery ...
I still offer hotels at least gallery gallery ...
So the ﬁxed latent symbol corresponds to the word gallery in various contexts.
Let us now ignore context-dependence, ﬁx the ﬁrst three symbols, and randomly choose another one that we repeat after them.
Here are a few sample decodes: Come to earth and culturalized climate climate ...
Come together that contribution itself, itself, ...
Come to learn that countless threat this gas threat...
In the ﬁrst two samples we see that the latent symbol corresponds to climate or itself, respectively.
Note that all these words or phrases are 7-characters long (and one character for space), most proba- bly due to the architecture of c(s).
But in the last sample we see a different phenomenon: the latent symbol seems to correspond to X threat, where X depends on the context, showing that this latent code also has an interesting structure.
3.4 MIXED SAMPLE-BEAM DECODING From the results above we know that our discretization method works quantitatively and we see interesting patterns in the latent code.
But how can we use the autoencoder models in practice?
One well-known problem with autoregressive sequence models is decoding.
In settings where the possible outputs are fairly restricted, such as translation, one can obtain good results with beam search.
But results obtained by beam search lack diversity (Vijayakumar et al., 2016).
Sampling can improve diversity, but it can introduce artifacts or even change semantics in translation.
We present an example of this problem in Figure 3.
We pick an English sentence from the validation set of our English-German dataset and translate it using beam search and sampling (left and middle columns).
In the left column, we show top 3 results from beam search using our baseline model (without autoencoder).
It is not necessary to speak German to see that they are all very similar; the only difference between the ﬁrst and the last one are the spaces before ”%”.
Further beams are also like this, providing no real diversity.
In the middle column we show 3 results sampled from the baseline model.
There is more diversity in them, but they still share most of the ﬁrst half and unluckily all of them actually changed the semantics of the sentence in the second half.
The part African-Americans, who accounted however for only 13% of voters in the State becomes The american voters were only 13% of voters in the state in the ﬁrst case, African-Americans, who accounted however for only 13% of all people in the State in the second one, and African-Americans, who elected only 13% of people in the State in the third case.
This illustrates the dangers of just sampling different words during decoding.
Using a model with access to the autoencoded part c(s) presents us with another option: sample c(s) and then run beam search for the sequence s appropriate for that c(s).
In this way we do not introduce low-level artifacts from sampling, but still preserve high-level diversity.
To sample c(s) we train a language model on c(s) with the same architecture as the model for s (and also conditioned on the input), but with a different set of weights.
We then use the standard multinomial sampling from this model to obtain c(s) and run a beam search on the model for s with the sampled c(s).
English sentence: For example, during the 2008 general election in Florida, 33% of early voters were African- Americans, who accounted however for only 13% of voters in the State.
Base model, beam decoding.
Base model, sampling.
Mixed decoding.
der W¨ahrend Parla- mentswahlen 2008 in Florida beispielsweise waren 33 % der fr¨uhen W¨ahler Afroamerikaner, die jedoch nur 13 % der W¨ahler im Staat ausmachten.
der W¨ahrend Parla- mentswahlen 2008 in Florida beispielsweise waren 33 % der fr¨uhen W¨ahler Afroamerikaner, die jedoch nur 13 % der W¨ahler im Staat stellten.
der W¨ahrend Parla- mentswahlen 2008 in Florida beispielsweise waren 33% der fr¨uhen W¨ahler Afroamerikaner, die jedoch nur 13% der W¨ahler im Staat ausmachten.
So waren zum Beispiel bei den Parlamentswahlen 2008 in Florida 33 % der fr¨uhen W¨ahler Afroamerikaner.
Die amerikanischen W¨ahler waren aber nur 13 % der W¨ahler im Staat.
So waren w¨ahrend der Parla- mentswahlen 2008 in Florida 33 % der fr¨uhen W¨ahler Afroamerikaner, die aber nur 13 % der Bev¨olkerung im Staat ausmachten.
So waren w¨ahrend der Parla- mentswahlen 2008 in Florida 33% der fr¨uhen W¨ahler Afroamerikaner, die jedoch nur 13% der Bev¨olkerung im Staat w¨ahlten.
sich stellte Es beispiel- sweise im Verlauf der Parla- mentswahlen in Florida heraus, dass 33% der fr¨uhen W¨ahler zu den afrikanischen Amerikan- ern z¨ahlten, die allerdings nur 13% der W¨ahler des Staates betrafen.
ist zum Beispiel Dabei im Laufe der Parlamentswahlen 2008 in Florida 33% in den fr¨uhen Wahlen der Afro- Amerikaner die allerdings nur 13% der W¨ahler des Staates betrafen.
vertreten, Hauptwahlen fr¨uhen W¨ahler 33% der beispielsweise waren w¨ahrend 2008 der in afrikanische Amerikaner, die f¨ur einen An- teil von nur 13% der W¨ahler im Staat verantwortlich waren.
Florida Figure 3: Decoding from baseline and autoencoder-enhanced sequence-to-sequence models.
In the right column in Figure 3 we show 3 samples obtained in this way.
As you can see, these samples are much more diverse and they still preserve the semantics of the original sentence, even if with sometimes strange syntax.
One would back-translate the ﬁrst example as: In turned out, for example, in the course of the parliamentary elections in Florida, that 33% of the early voters are African-Americans, which were, however, only 13% of the voters of the state.
Note the addition of It turned out and restructuring of the sentence.
In the third sample the whole order is reversed, as it starts with 33% of the voters ...
instead of the election phrase.
Obtaining such samples that differ in phrase order and other aspects but preserve semantics has been a challenge in neural translation.
4 CONCLUSION In this work, the study of text autoencoders (Bowman et al., 2016; Yang et al., 2017) is combined with the research on discrete autoencoders (Jang et al., 2016; Maddison et al., 2016).
It turns out that the semantic hashing technique (Salakhutdinov & Hinton, 2009b) can be improved and then yields good results in this context.
We introduce a measure of efﬁciency of discrete autoencoders in sequence models and show that improved semantic hashing has over 50% efﬁciency.
In some cases, we can decipher the latent code, showing that latent symbols correspond to words and phrases.
On the practical side, sampling from the latent code and then running beam search allows to get valid but highly diverse samples, an important problem with beam search (Vijayakumar et al., 2016).
We leave a number of questions open for future work.
How does the architecture of the function c(s) affect the latent code?
How can we further improve discrete sequence autoencoding efﬁciency?
Despite remaining questions, we can already see potential applications of discrete sequence autoen- coders.
One is the training of multi-scale generative models end-to-end, opening a way to generating truly realistic images, audio and video.
Another application is in reinforcement learning.
Using la- tent code may allow the agents to plan in larger time scales and explore more efﬁciently by sampling from high-level latent actions instead of just atomic moves.
REFERENCES Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.
Layer normalization.
arXiv preprint arXiv:1607.06450, 2016.
Samuel R.
Bowman, Luke Vilnis, Oriol Vinyals, Andrew M.
Dai, Rafal J´ozefowicz, and Samy Bengio.
Generating sentences from a continuous space.
In Proceedings of the SIGNLL’16, pp.
10–21, 2016.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson.
One billion word benchmark for measuring progress in statistical language modeling.
CoRR, abs/1312.3005, 2013.
URL http://arxiv.org/abs/1312.3005.
Geoffrey E.
Hinton and Ruslan Salakhutdinov.
Reducing the dimensionality of data with neural networks.
Science, 313(5786):504–507, 2006.
Eric Jang, Shixiang Gu, and Ben Poole.
Categorical reparameterization with gumbel-softmax.
CoRR, abs/1611.01144, 2016.
URL http://arxiv.org/abs/1611.01144.
Łukasz Kaiser and Samy Bengio.
Can active memory replace attention?
In Advances in Neural Information Processing Systems, (NIPS), 2016.
Łukasz Kaiser and Ilya Sutskever.
Neural GPUs learn algorithms.
In International Conference on Learning Representations (ICLR), 2016.
Diederik P.
Kingma and Max Welling.
Auto-encoding variational bayes.
CoRR, abs/1312.6114, 2013.
Chris J.
Maddison, Andriy Mnih, and Yee Whye Teh.
tinuous relaxation of discrete random variables.
http://arxiv.org/abs/1611.00712.
The concrete distribution: A con- URL CoRR, abs/1611.00712, 2016.
Luke Metz, Julian Ibarz, Navdeep Jaitly, and James Davidson.
Discrete sequential prediction of con- tinuous actions for deep rl.
arXiv, 2017.
URL https://arxiv.org/abs/1705.05035.
Ruslan Salakhutdinov and Geoffrey E.
Hinton.
Deep Boltzmann machines.
In Proceedings of AISTATS’09, pp.
448–455, 2009a.
Ruslan Salakhutdinov and Geoffrey E.
Hinton.
Semantic hashing.
Int.
J.
Approx.
Reasoning, 50(7): 969–978, 2009b.
Rico Sennrich, Barry Haddow, and Alexandra Birch.
Neural machine translation of rare words with subword units.
In Proceedings of ACL’16, 2016.
Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural net- In Advances in Neural Information Processing Systems, pp.
3104–3112, 2014.
URL works.
http://arxiv.org/abs/1409.3215.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.
Gomez, CoRR, 2017.
URL Lukasz Kaiser, and Illia Polosukhin.
Attention is all you need.
http://arxiv.org/abs/1706.03762.
Ashwin K.
Vijayakumar, Michael Cogswell, Ramprasath R.
Selvaraju, Qing Sun, Ste- Diverse beam search: Decoding di- URL fan Lee, David J.
Crandall, and Dhruv Batra.
verse solutions from neural sequence models.
http://arxiv.org/abs/1610.02424.
CoRR, abs/1610.02424, 2016.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol.
Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.
Journal of Machine Learning Research, 11:3371–3408, 2010.
Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and Taylor Berg-Kirkpatrick.
Improved variational autoencoders for text modeling using dilated convolutions.
In Proceedings of ICML’17, pp.
3881– 3890, 2017.
10
Healthcareassociatedinfectionsrepresentamajorcauseofmorbidityandmortal-ityintheUnitedStatesandothercountries[1].Althoughmanycanbetreated,theseinfectionsaddgreatlytohealthcarecosts[2].Furthermore,theemergenceofmultidrugresistantbacteriahavegreatlycomplicatedtreatmentofhealthcareas-sociatedinfections[3],makingthepreventionoftheseinfectionsevenmoreimpor-tant.Oneofthemosteﬀectiveinterventionsforpreventinghealthcareassociatedinfectionsishandhygiene[4].Yet,despiteinternationalprogramsaimedatincreas-inghandhygiene[4,5,6],ratesremainlow,lessthan50%inmostcases[4,6,7].Becauseoftheimportanceofhandhygieneinpreventinghealthcareassociatedinfections,infectioncontrolprogramsareencouragedtomonitorratestoencour-ageprocessimprovement[6,8,9].Inmostcases,handhygienemonitoringisdoneexclusivelybyhumanobservers,whicharestillconsideredthegoldstandardformonitoring[7].Yet,humanobservationsaresubjecttoanumberoflimitations.Forexample,humanobserversincurhighcostsandtherearediﬃcultiesinstan-dardizingtheelicitedobservations.Also,thetimingandlocationofobserverscangreatlyaﬀectthediversityandthequantityofobservations[10,11].Furthermore,thedistanceofobserverstohealthcareworkersunderobservationandtherelativebusynessofclinicalunitscanadverselyaﬀecttheaccuracyofhumanobservers[11].Thepresenceofhumanobserversmayartiﬁciallyincreasehandhygieneratestem-porarilyjustasthepresenceofotherhealthcareworkerscaninducepeereﬀectstoincreaserates[12,13].Finally,thenumberofhumanobservationspossibleisquitesmallincomparisontothenumberofopportunities[7,12].Asaconsequence,severalautomatedapproachestomonitoringhavebeenpro-posed[8,14,15,16].Manyofthesemeasurehandhygieneuponenteringandleavingapatient’sroom.Thesubsequentactivationofanearbyhandhygienedispenserisrecordedasahandhygieneopportunityfulﬁlledwhereas,ifnosuchactivationisobserved,theopportunityisnotsatisﬁed.Suchapproaches,whilenotcapturingallﬁvemomentsofhandhygiene,doprovideaneasyandconvenientmeasureofhandhygienecompliance.Withautomatedapproachesbecomingmorecommon,amoreongoingandcomprehensivepictureofhandhygieneadherenceshouldemerge,providingnewinsightsintowhyhealthcareworkersabstainfrompracticinghandhygiene.
21MillionOpportunities3Inthiswork(anextensionof[17]),weprovideanin-depthexplorationoffactorsaﬀectinghandhygienecomplianceacrossmultiplehospitalfacilitiesusinglinearpredictivemodels.2DataandMethods2.1HandHygieneEventDataOurhandhygieneeventdataisaproprietarydatasetprovidedbyGojoIndustries.Thedatawereobtainedfromanumberofinstallationsconsistingofdoorcountersensors1,whichincrementacounteranytimeanindividualgoesinoroutofaroom,andhandhygienesensors,whichincrementacounterwhensoaporalcoholrubaredispensed.Additionalsupportingtechnologywasalsoinstalledtocollectandrecordtimestampedsensor-reportedcounts.WeprovideasimpleillustrationofhowthesetechnologiesareusedinFigure1andapictureofaninstrumentedroomentranceinFigure2.Inthispaper,wewillusethetermdispensereventtodesignatetriggeringanduseofaninstrumentedhandhygienedispenseranddooreventtodesignatethetriggeringofacountersensorlocatedononeoftheinstrumenteddoors.(a)(b)(c)Fig.1:Asimpleillustrationofthesensorsandcorrespondinginfrastructure.In(a),healthcareworkersenterandexitpatientroomsthatareﬁttedwithsensors,interactingwithinstrumenteddispensersastheydo;notethatthesensoronthehandhygienedispenserisinternal,andnotvisible.In(b),thesedooranddispensercountsareintermittentlysenttoawirelesstransmitter.In(c),thesecountsarerelayedviatransmitterandstoredinadatabase,alongwithotherinformation,suchastheroomthecountscamefromandthetimeanddateinwhichtheyweresent.1Practicallyspeaking,thesesensorscanbeﬁttoanysortofpatiententrance/exitarea,asdepictedinFigure2.
4MichaelT.LashMSetal.Fig.2:Anurseapplyinghandhygienerubuponleavinganinstrumentedpatientarea.Notethedoorsensorhighlightedbytheredbox.Atotalof19facilitiesin10stateswereoutﬁttedwithsensors;becauseofprivacyconcerns,wereportonlythestateandCDCDivisionforeach.Thefacilitiescompriseawiderangeofgeographies,spanningbothcoasts,themidwest,andthesouth.Atotalof1851doorsensorsand639dispensersensorsreportedatotalof24,525,806dooreventsand6,140,067dispensereventsacrossthese19facilitiesbetweenOctober21,2013andJuly7,2014.Eachfacilitycontributedanaverageof172.3reportingdays,makingthisstudythelargestinvestigationofhandhygienecompliancetodate(i.e.,largerthanthe13.1millionopportunitiesreportedin[18]).Assumingeachdooreventcorrespondstoahandhygieneopportunity,weestimateanaveragefacilitycompliancerateof25.03%,inlinewithifnotjustbelowthereportedlow-endratefoundin[19].Theoriginaldata,consistingoftimestampedcountsreportedfromindividualsensorsovershortintervals,werere-factoredtosupportouranalysis.First,datafromeachsensorwerebinnedbytimestamp,t,into12hourintervals,correspondingtotraditionaldayandnightshifts,asindicatedbyanadditionalvariable,night,deﬁnedasfollows:nightShift=(1t[7pm,7am)0t[7am,7pm)Second,dooranddispensercountswereaggregatedbasedondayandnightshiftsoastoproduceaseriesofshift-levelrecords.Foreachsuchrecordwecomputehandhygienecompliance,orjustcompliance,bydividingthenumberofreported21MillionOpportunities5dispensedeventsbythenumberofdoorevents:compliance=#dispenser#doorSuchadeﬁnitionofcomplianceassumesthateachdooreventcorrespondstoasinglehand-hygieneopportunityandeachdispensereventcorrespondstoasinglehand-hygieneeventwhereas,inreality,ahealthcareworkermightwellbeexpectedtoperformhandhygienemorethanonceperentry,resultinginratesthatexceedone,ifonlyslightly.Thisestimatoralsoignorestheplacementofdoorswithrespecttodispensers:multipledispensersmaywellbeassociatedwithasingledoorway,andsomedispensersmaybeinroomshavingmultipledoors.Thus,simplyaddingnewdispenserswillraiseapparentcomplianceratescomputedinthisfashion,whileaddingnewdoorsensorswillappeartoreducecompliance.Evenso,whenappliedconsistentlyandifsystemlayoutsareﬁxed,thisestimatorisareasonableapproximationoftruehandhygienecompliance,andsupportssoundcomparisonswithinafacility(butnotacrossfacilities).Becausemalfunctioningsensorsordeadbatteriescanproduceoutliers(i.e.,veryloworveryhighvalues),shiftswithfewerthan10doorordispensereventsreportedperday(possiblyindicatinganinstallationundergoingmaintenance),zerocompliance,orcompliancevaluesgreaterthan1wereremovedpriortoanalysis(atthecostofpossiblyexcludingsomelegalrecords).Theremainingdataconsistof5308shiftsfromtheoriginal5647records,having21,273,980handhygieneopportunitiesand5,296,749handhygieneevents(seeTable1).FacilityStateCDCDivTotDispTotDoorDaysRep91OHENC234292518772252101OHENC3509012021665260105TXWSC2388991940024260119MNWNC123877242939156123TXWSC3256181112198243127NMMnt13068554546171260135OHENC125731264331258144CAPac3989611744642260145CAPac5670962073566260147CAPac5009792462900260149CAPac5907082306392260153CTNewE169564603482208155NYM-At171275619507117156NCS-At43813820015157OHENC39455313396101163OHENC344102335168PAM-At304218690920170ILENC11260435363147173OHENC47881512232Total1085296749212739803274Table1:Descriptivestatisticsforallreportingfacilitiesintermsofstate,CDCdivision,handhygieneevents,peopleevents,andreportingdays.
6MichaelT.LashMSetal.2.2FeatureDeﬁnitionsInthissubsectionwedeﬁnethefeatures(factors)thatwillbeexamined,andhoweachisderived.Fig.3:Assigning(redbox)NOAAweatherdata,reportedintermsofageographicgrid,tohealthcarefacilities(reddots),wherethebluecolorgradientmightrep-resenttemperature.2.2.1LocalWeatherDataBecausehealthcareworkersfrequentlyciteskindrynessandirritationasafactorindecreasedcompliance(particularlyincoldweathermonthswhereenvironmen-talhumidityisreduced),weassociatedailyairtemperature(denotedtemp)andrelativehumidity(denotedhumid)toeachtimestampedrecordbasedoneachfa-cility’sreportedzipcode.Spatiallyassimilatedweathervalues(σ=0.995)fortheentireglobewereobtainedfromtheNationalOceanicandAtmosphericAdminis-tration(NOAA)[20].Givenintermsofgridelements(atessilationofboundingboxescovering2.5°latitudeby2.5°longitude),theworldisthusdeﬁnedasa144by73gridhaving10512distinctgridelements.Weatherdataareavailableataﬁneleveloftemporalgranularity(ontheorderof4timesdailyforeachgridunit)fortheentireperiodofinterest.Thegeographicalassignmentofweatherdatawasob-tainedbyﬁrstmappingeachfacility’snumericalzipcodetothezipcode’scentroid(2010USCensusdata),andthensubsequentlymappingzipcodecentroid(lat,lon)tothecorrespondingNOAAgridelement.AnexampleofthisassignmentcanbeobservedinFigure3.Weassociateweatherinformationfromtheobservationtemporallyclosesttothestartofeachshift.
21MillionOpportunities72.2.2InﬂuenzaSeverityWeconjecturethatthelocalseverityofcommonseasonalinfectiousdiseasessuchasinﬂuenzamayalsoaﬀecthandhygienecompliancerates.Wedeﬁneinﬂuenzaseverity(denotedflu)asthenumberofinﬂuenza-relateddeathsrelativetoalldeathsoveraspeciﬁedtimeinterval.InﬂuenzaseveritydatawereobtainedfromtheCDC’sMorbidityandMortalityWeeklyReport(MMWR),whichalsoreportsdataatweeklytemporalgranularity.RatherthanreportingdatabyCDCregion,however,dataareprovidedbyreportingcity(oneof122participatingcities,mostlylargemetropolitanareas).Wemapeachfacilityinourdatasettotheclosestreportingcityinordertoassociatetheappropriateseverityvaluetoeachrecord.InotherwordsrepCity=argmin{dist(facility,cityi):i=1,...,122}wheredist(fac,city),k(faclat,faclon),(citylat,citylon)k2,theEuclideandis-tancebetweentwoentitiesgivenintermsof(lat,lon)coordinates.Eightof19facilitieswerelocatedinareportingcity(i.e.,dist(fac,city)=0).Theremaining11facilitiesweremappedtoareportingcitythatwas,onaverage,66.2milesaway(only3of19facilitiesweremappedtoareportingcityfurtherthanthisaverage,withthelargestdistancebeing142miles).2.2.3TemporalFactorsWealsoconjecturethatexternalfactorsassociatedwithspeciﬁcholidaysoreventsmayaﬀecthandhygienecompliancerates.Holidaysmaychangestaﬃngratesoraﬀecthealthcareworkerbehaviors.Thenumberofvisitors(aﬀectingdoorcounterrates)mayalsobegreaterthanduringregularweekdays.Holidayssuchasthe4thofJulyareoftenassociatedwithalcohol-relatedaccidents,andmayincreasehealthcarefacilityworkloads(similarfactorsmayalsoapplyonweekends).Wedeﬁneanewvariableholidaythatreﬂectswhetheragivenshiftoccursononeofthe10federalholidays(NewYear’sEve,MartinLutherKingDay,Presi-dent’sDay,MemorialDay,the4thofJuly,LaborDay,ColumbusDay,Veteran’sDay,ThanksgivingorChristmas)where,ifanypartoftheshift(day/night)fallsontheholidayinquestion,theindicatorissetto1.Moreformally:holiday=(0t/∈{holidays}1t∈{holidays}Similarly,inordertoascertaintheimpactofweekendsoncompliance,wedeﬁneanewvariableweekdayasfollows:weekday=(0t∈{Sat,Sun}1t∈{Mon,Tues,Weds,Thurs,Fri}Noteherethatifashiftspanstheweekdayintoaweekend(orviceversa),itisencodedasaweekend.Arelatedconceptisthepresenceofnewresidentphysicians,whotraditionallystartworktheﬁrstofJuly.Wedeﬁneanewvariablethatcorrespondswiththis8MichaelT.LashMSetal.timeperiodinordertoseeifthedatarevealthepresenceofaJulyeﬀect(denotedJuly):July=(0t/∈July1−71t∈July1−72.3ExploringFactorsAﬀectingHandHygiene2.3.1M5RidgeRegressionforFeatureExaminationWithcovariatesdeﬁnedandassociatedwiththecollectedsensordata,wewishtobuildalinearhypothesishthat(a)accuratelyestimateshandhygieneand(b)reportsthedirectionanddegreeofeﬀectofourdeﬁnedfeatures.Inaccomplishing(b)webearinmindtwothings:(1)Theremaybemulti-collinearityamongfeatures,whichmayadverselyaﬀecttheoutput.(2)That(a)and(b)maybeatoddswithoneanother;i.e.,obtaininggoodpre-dictionsmayentaildiscardingsomeprediction-inhibitingfeaturesforwhichwewouldliketoobtaineﬀectestimates(inpractice,weﬁndthatthisisnotactuallythecase).Therefore,weproposeanM5RidgeRegressionforFeatureExaminationmethoddesignedtoaccomplish(a)and(b),whilebearing(1)and(2)inmind.Thismethodisgivenbyh∗=argminh∈HlkΛ(X)h−yk22+λkhk22s.t.ρ(hj)≤.05∀j(1)whereX∈Rn×pisadesignmatrix,histhehypothesis,yisthetargetvectorconsistingofcomplianceratesinwhichaparticularyi∈[0,1],λisaregularizationterm,k·k2isthe‘2-norm,andρ(·)isafunctionthatreportsthep-valueofahypothesisterm(thisconstraintisensuredviasequentialbackwardselimination[21]).ThefunctionΛ(X)canbedeﬁnedasΛ(X),argmin{t∈THl}(2)wheretishypothesisselectedfromatreeofhypothesesconstructedusingtheM5method[22].Eﬀectively,(2)onlyreducesthepdimension,actingasafeatureselectionmethod,andhavingnobearingonthendimension.Thereareafewbeneﬁtsoftheabovemethodworthnoting.First,thehypothe-sisclassHlislinearandcommontoboth(1)and(2).Suchtwo-stageoptimizationapproaches,wheretheﬁrstobjectiveisoptimizedtakingintoaccountthehypoth-esisclassbeforethehypothesisitselfisoptimizedforpredictiveaccuracy(orsomeothersuchmeasure),havebeenshowntoworkwellinothercontexts[23].Sec-ondly,suchamethodisspeciﬁcallygearedtowardproducingahypothesisthatmakesuseoffeaturesthathaveanimmediatebearingupontheproblem,while21MillionOpportunities9eliminatinginterpretabilityobscuringeﬀects,suchasmulti-collinearity.Moreover,thesedesirablesareobtainedwhileattemptingtoproducethemostaccuratehy-pothesis:anhthatelicitsfeatureindicativeness,producesaccurateresults,andcontrolsforconfoundingeﬀectsisthegoalofthistwo-stepoptimizationprocedure.Ultimately,weconductouranalysisbyobservingthesignandmagnitudeofthevaluesinthehypothesisvectorinordertodeterminethefactorsthatinﬂuencehandhygienecompliance,andwhethersuchfactorsaﬀectcomplianceinapositiveornegativemanner.WealsoobservecorrelationandRMSEvaluestodeterminehowwellourpredictivemodelworks,andwhetherthecorrespondingresultscanbetrusted.Allresultsandareobtainedviak-foldcross-validation(k=10).2.3.2SupportingMethodologyWealsousetwoestablished/standardtechniques–RReliefFfeaturerankingandmarginaleﬀectsmodeling–thatwillserveasapointofcomparisonbetweenourmethod,andalsohelpinformthediscussionoftheresultsobtained2.Featureranking:First,weproposetheuseoftheRReliefFalgorithm[26],amodiﬁcationoftheoriginalReliefalgorithmofKiraandRendell[27].RReliefFﬁndsafeaturej’sweightbyrandomlyselectingaseedinstancexifromdesignma-trixXandthenusingthatinstance’sknearestneighborstoupdatetheattribute.Thisdescriptionconsistsofthreeterms:theprobabilityofobservingadiﬀerentrateofhandhygienecompliancethanthatofthecurrentvaluegiventhatofthenearestneighbors,givenbyA=p(rate6=ratexi|kNN(xi)),(3)theprobabilityofobservingthecurrentattributevaluegiventhenearestneighbors,givenbyB=p(xi,j|kNN(xi)),(4)andtheprobabilityofobservingadiﬀerenthandhygieneratethanthecurrentvaluegivenadiﬀerentfeaturevaluevandthenearestneighbors,givenbyC=p(rate6=ratexi|kNN(xi)∧j=v).(5)Attributedistanceweightingisusedinordertoplacegreateremphasisoninstancesthatareclosertotheseedinstancewhenupdatingeachterm;ﬁnalweightsareobtainedbyapplyingBayes’ruletothethreetermsmaintainedforeachattribute,whichcanbeexpressedC∗BA−(1−C)∗B1−A.(6)Byusingthismethodwecouldthenrankattributesintermsoftheirimportance.Weagainreportrankingsusingk-fold(k=10)crossvalidation.MarginalEﬀectsModeling:Toprovideadditionalinsightintothefeaturesthatarerelevanttohandhygieneweanalyzedtheirmarginaleﬀects[28].Marginaleﬀects,alsoreferredtoasinstantaneousratesofchange,arecomputedbyﬁrst2NotethatboththeLASSO[24]andElasticNet[25]wouldhavealsomadeappropriatesupportingmethods.
10MichaelT.LashMSetal.trainingahypothesish,then,usingthetestingdata,theeﬀectofeachcovariatecanbeestimatedbyholdingallothersconstantandobservingthepredictions.Suchamethodcanbeexpressedbyˆratei,j=h>[xi,j,¯x6=j](7)where,withaslightabuseofnotation,xi,j,thevalueofinstancei’sjthfeature,isaddedtothevector¯x6=j,whichconsistsoftheaverageofeachnon-jfeature,attheappropriatelocation(namely,thejthposition).Here,thenotation6=jisusedtoreinforcethefactthatthevectorofaverages¯xhasit’sjthelementreplacedbyxi,j.Othernon-jentriesaregivenby¯xk=µ(Xk),foranarbitraryindexpositionk.3Results3.1GlobalModelInthissectionweexaminetheresultsobtainedbycross-validatingglobalmodels,whereallfacilityrecordsareused,andafacility-identifyingfeatureisincluded.3.1.1PredictivePower:M5RidgeRegressionWelearnedahypothesisusingallavailablefeatures,includinganominalizedfa-cilityidentiﬁer.OurpredictiveresultscanbeobservedinTable2.WenotethattheRMSEisnotlargeandthecorrelationismoderate,implyingrelativelygoodpredictiveperformance.MeasureValueCorrelation0.3441RMSE0.1702Table2:CorrelationcoeﬃcientandRMSEofcross-validatedmodelpredictions.3.1.2ExaminingHypothesish∗Wenextexaminethetermsofthelearnedhypothesish∗(seeTable3).Themodelincludesuniqueidentiﬁersforall19facilities,12ofwhichhadpositivecorre-spondingvalues,indicatingrelativelyhigherratesofcompliance.Theremainingfacilities’h∗termshadrelativelysmallnegativevalues,indicatinglowerratesofcompliance.Amongotherfeatures,holidaysareassociatedwithlowercomplianceratesandinﬂuenzaseveritywithhighercompliance.Weekdaysareassociatedwithhighercompliancerates,asarehighertemperaturesandhumidity.Interestingly,theM5RidgeRegressionmodelappearstohaveeliminatedsomeholidays(MartinLutherKingday,Memorialday,Laborday,Columbusday,andThanksgiving),aswellasFacility163(thefacilitywiththelowestamountofhand-hygienedata).Thismeansthatthesefeaturesdonotcontributetohand-hygienecomplianceratesinanymeaningfulway.
21MillionOpportunities11Featurehjfacility−={1,105,147,156,157,170}hj∈Fac−∈[−0.103,−0.016]facility+={91,119,123,127,135,144,hj∈Fac+∈145,149,153,155,168,173}[0.008,0.261]temp0.022humid0.0079weekday0.0069night−0.0218holiday={IndepDay,Pres.Day,hj∈HolVetDay,NewYear’s,Christmas}[−0.017,−.006]flu0.014July−0.0106Table3:Featurespeciﬁchjterms,whereredhighlightsfeatureswithanegativeassociationandbluehighlightsthosewithapositiveassociation.3.1.3RReliefFUsingRReliefFwecanrankfeaturesintermsoftheirimportanceinordertosupportandsupplementtheresultobtainedusingM5RidgeRegression.TheseresultsarereportedinTable4,whererankingsshownareaveragesfor10-foldcross-validation.Notethatherefacilitywasrepresentedasasinglediscretely-valuedfeatureinordertodeterminetheimportanceoffacilityasawhole(insteadoftreatingeachfacilityasitsownfeature),aswasholiday.AttributeAvgValAvgRankfacility0.029(±.001)1flu0.0072temp0.0053.3(±0.46)weekday0.0025humid.0016.3(±0.64)July≈0.07.2(±0.4)holiday≈0.07.8(±1.08)night≈0.08.7(±0.46)Table4:RReliefFattributeweights.3.1.4MarginalEﬀectsTheresultsobtainedfrommodelingthemarginaleﬀectscanbeobservedinFigure4.Figures4aand4bshowthemarginaleﬀectsoftworandomlyselectedfacilities;oneidentiﬁedasbeingassociatedwithlowerratesofcomplianceandoneidentiﬁedashavinghigherratesofcompliance(fromTable3).Notethat,becausethesearebinaryfeatures(takingonvaluesofeitherzeroorone),thekerneldensityoftheunderlyingdataisnotreadilyvisible(unliketheotherﬁgures,whichshowresultsfornon-binaryfeatures).AswecanseethemarginaleﬀectssupporttheresultobtainedusingbothM5RidgeRegressionandRReliefF,andalsoseemtosuggest12MichaelT.LashMSetal.(a)Facility91.(b)Facility101.(c)flu.(d)humid.(e)temp.Fig.4:Themarginaleﬀectsofseveralselectcovariates,whereblueshowsthekerneldensityoftheoriginaldataandtheredlinesshowtheestimation.Rate(y-axis)vs.feature(x-axis).Notethatin4aand4bnokerneldensityestimateisprovided,astheseplotsareforbinaryfeatures.
21MillionOpportunities13anevengreaterassociationbetweenfacilitiesandratesofcompliancethanwasoriginallyapparent(atleastforthesetwofacilities).Figure4cshowsthemarginaleﬀectsofinﬂuenzaseverity.Thefluresultshowsaslightlypositiverelationshipbetweentheseverityofﬂu,measuredintermsofmortality,andhand-hygienecompliancerates.ThisisfurthersupportedbytheresultobtainedfromM5RidgeRegressionandtheRReliefFranking.Figures4dand4eshowthemarginaleﬀectsofhumidityandtemperature.TheresultobtainedforbothisconsistentwiththatfromM5RidgeRegression.ThelessereﬀectofhumidityandgreatereﬀectoftemperaturearealsoreﬂectedintheRReliefFranking.3.1.5WeatherandTemperature:StatisticalSigniﬁcanceTofurtherexploretherelationshipbetweenhand-hygieneandweathereﬀects,weconductedasimplestatisticalanalysis.Foreachfacility,weselectedthetem-peratureandhumidityvaluescorrespondingtothebottom10%andtop10%ofhand-hygienecompliancerates.Wethenperformedapairedt-testoneachsetofsamples;temperatureandhumidityvalueswerescaledto[0,1].TheresultsofthisanalysisarereportedinTable5.FacilityStatetemphumidµtop−µbot(p-val)µtop−µbot(p-val)91OH-0.004(0.750)-0.007(0.489)101OH0.001(0.909)0.004(0.457)105TX0.041(<0.000)-0.028(0.001)119MN-0.008(0.699)-0.013(0.337)123TX0.017(0.002)0.029(<0.000)127NM0.032(<0.000)-0.063(<0.000)135OH-0.045(0.010)0.017(0.278)144CA0.009(<0.000)-0.018(0.002)145CA-0.001(0.675)0.004(0.549)147CA0.011(<0.000)-0.013(0.017)149CA-0.007(0.025)0.008(0.214)153CT0.043(<0.000)-0.003(0.746)155NY0.093(<0.000)0.012(0.341)156NC0.040(0.007)-0.041(0.445)157OH-0.132(<0.000)-0.020(0.638)163OH0.180(0.010)0.179(0.021)168PA0.012(0.122)0.071(0.006)170IL-0.001(0.772)-0.007(0.642)173OH0.037(0.003)-0.033(0.440)Table5:Thediﬀerenceinmeansandpairedt-testp-valueresults,obtainedbycomparingtemperature/humidityvaluesamongthebottom10%andtop10%ofhand-hygienecompliancerates,byfacility(boldenedblueindicatesthateithertemperature,humidity,orbothhaveapositivediﬀerenceinmeansandap-value≤.05).Table5showsthatmostfacilitieshavestatisticallysigniﬁcantdiﬀerencesbe-tweenthetwosamplesandthatµtop10>µbottom10.Suchresultsindicatesthat14MichaelT.LashMSetal.highertemperaturesandlevelsofhumidity(particularlytemperature)arestatis-ticallyassociatedwithhigherratesofhandhygiene.However,weﬁndthatsomefacilitiesco-locatedinthesamegeographicregionhaveconﬂictingstatisticalre-sults(e.g.,Facs.91,173).Weconjecturethatsucharesultmayattributabletodiﬀerencesinsensordeploymentlocation,butweleavesuchaninvestigationasfuturework.3.2Facility-SpeciﬁcModelingThefullM5RidgeRegressionmodels’relianceonfacilityidentities,coupledwiththeRReliefFfeaturerankingresult,suggeststhatcompliancedepends,atleastinpart,uponfacility-speciﬁchealthcareworkerattitudes,administrativeculture,orevensimplythedispositionofsensorsandthearchitectureofthefacility.Therefore,weproposetoconstructandanalyzefacility-speciﬁcmodelsinthesamemannerasourglobalmodel.Therefore,inthissection,wepresentacomprehensivesetoffacility-speciﬁcresultsobtainedusingfacility-speciﬁcmodels:weexplore10ofthe19facilitiesdisclosedinTable1,whichcomprehensivelyrepresentalargegeographicdispersion(whichmayproducegeographic-speciﬁcsimilaritiesanddiﬀerencesintheobtainedresults),whichwillfurtherillustratethefacility(andlocation)-speciﬁcfactorsaﬀectinghandhygienecompliance.3.2.1PredictivePower:M5RidgeRegressionThefacility-speciﬁcM5RidgeRegressionmodelingresultsarereportedinTable6.Fac#CorrelationRMSE1550.59070.06581530.20890.09911490.1168.04891230.61930.111270.7133.0313910.53840.09391010.37510.04421700.06450.06071680.3620.0794Table6:Facility-speciﬁcM5RidgeRegressioncross-validationresults.ComparingTable2,showingtheperformanceoftheglobalmodel,withTable6,wecanseethatthereisuniformlylowerRMSEamongthefacility-speciﬁcmodels(Table6)ascomparedtothatoftheglobalmodel.Thisresultisnotunexpected.Ontheotherhand,weobservearangeofcorrelationvalues,someofwhicharebetterthantheglobalmodel(theﬁrsteightfacilitiesinTable6),andsomeofwhichthatareworse.Wenotethatthelasttwofacilities,whichhadworsecorrelationresultsthantheglobalmodel,arealsothefacilitiesthathavecomparablylittledata.
21MillionOpportunities15Wenowturntoexaminingthetermsofeachfacility-speciﬁchypothesisvector,whichcanbeobservedinTable7.Notethat,forthesakeofsimplicityinanalyzingthesefeatures,wehavecreatedasingle,binaryholidayfeature(asopposedtohavingafeatureforeachholiday,asinourglobalmodel).facility#temphumidweekdayfluholidaynightJuly1470.42370.0594NA−0.937NA−0.0176NA1550.2721NA0.04910.1847NA-0.178NA153NA0.0480.0168-0.0638NA-0.0514-0.0779149NANA0.0184-0.0543NA0.0093NA1230.419NA-0.0572-0.2392NA0.0787NA1270.0672NA0.03150.0383-0.0232-0.0499NA91-0.05460.13290.0683NA-0.1207-0.1012NA101-0.04370.02190.0219NA-0.0234-0.0169-0.0617170NANANA-0.1518NANANA168NA0.1414NA0.207NA-0.0742-0.0729Table7:Hypothesisvectortermsforeachfacility-speciﬁcmodel.InexaminingTable7,wewishtoﬁrstpointoutthat,relativetotheglobalmodelresultreportedinTable3,thatallfacility-speciﬁcmodelshadatleastonetermthatwasremovedviasequentialbackwardselimination.Moreover,theseeliminatedtermsdiﬀerbyfacility,demonstratingthatlocalmodelsaresensitivetodiﬀerentfeaturesindiﬀerentways.Inexaminingthehypothesisterms,someinterestingﬁndingsemerge.Withrespecttoourweather-basedfeatures–temperatureandhumidity–wecanseethat,forthemostpart,thesefactorswerepositivelyassociatedwithhigherratesofhandhygienecomplianceand,forcertainfacilities(147,155,123),thesefeaturesappeartobefairlyimportant(basedonthemagnitudeofthecoeﬃcients).Twofacilities,however,haveanegativeassociationwithtemperatureandcompliance.Thesecoeﬃcients,however,arerelativelysmallandareoﬀsetbypositiveassoci-ationsamonghumidity:inotherwords,theeﬀectsoftemperatureoncomplianceratesatthesefacilitiesappeartobesomewhatnegligible.Inexaminingweekdayandholiday,wecanseethatinallbutonefacility,weekdayhasapositiveinﬂuenceonhandhygienerates.Thissuggeststhatem-ployeesthatworkduringweekendsatthesefacilitiesmaybewashingtheirhandsless;thismaybeattributabletoanumberoffactors(increasedworkload,etc.).Theholidayfeature,ontheotherhand,tendstobeindicativeoflowerratesofcomplianceamongthethreefacilitiesreportinganon-zerotermintheirhypothesisvector(i.e.,facilities91,101,127).ThenightandJulyfeaturesalsotendtobenegativelyassociatedwithhandhygienecompliance,withJulyEffectbeinguniversallyassociatedwithnegativeratesofcompliance(amongthethreefacilitiesforwhichthistermwasnotelimi-nated).night,bycontrast,hadtwofacilitieswhichwerefoundtohaveapositivetermforthisfeature.Thesemaybehospitalswherethereisrelativelylessactiv-ityatnight(lessbusy);however,furtherinvestigationisneededtoteaseoutthereasonsindividualfacilitiesexperiencethesediﬀeringrates.Finally,fluappearstohaveamixofpositiveandnegativeassociationsamongfacilities.Inthosefacilitiesthathavenegativeassociations,acampaignfocusing16MichaelT.LashMSetal.onﬂuawarenessmaybebeneﬁcial;however,lowerratesmaybeattributabletoincreasedactivityduringpeakﬂuseason,whichmayalsosuggesttheneedforhigherstaﬃnglevels–furtherinvestigationisneededtouncoverthereasonsbehindtheseassociations.3.2.2RReliefFInthissubsectionwediscusstheresultsofRReliefFfeaturerankingobtainedforeachofthe10facilitiesbeinginvestigated;theresultsarepresentedinTable8.Fac#temphumidweekdayfluholidaynightJuly14725.73.815.246.3155124.84.85.23.271532.25.84.514.73.16.71494.35.11.22.166.62.712314.15.434.373.21272.83.56.54.413.36.5913.935.52.115.571013.175.3434.611701.94.43.91.24.36.95.41681.53.82.91.56.36.61.8Table8:Facility-speciﬁcRReliefFfeaturerankings.Theﬁrstobservationwewishtomakeisthatthereisnosinglefeaturethatcompletelydominatesthefeaturerankingsamongthediﬀerentfacilities.Thissug-geststhatfacilities’complianceratesareaﬀecteddiﬀerentlybyourselectedfea-tures.However,wecanalsothatsomefeaturesareoftenrankedasbeingmoreimportant,whileothersaslessimportant.Forinstance,tempisfrequentlyoneofthetopthreefeatures,whileJulymoreoftenappearstowardthebottomoftheranking.Itisimportanttonotehere,however,thatwhileJuly,weekday,night,andholidayappeartowardtheendofthefeaturerankingforsomefacilities,theyappeartowardsthetopforothers.Theflufeaturealsofrequentlyappearsinthetopthreefeaturerankingsamongfacilities,whilehumidoftenappearssomewherenearthemiddleoftherankings.3.2.3MarginalEﬀectsThefacility-speciﬁcmarginaleﬀectsmodelingresultsarepresentedinFigure5.Notethatwearereportingonlyasubsetofresults,whichincludetemp,humid,weekday,andflu.Cumulatively,theseresultsfurthersupportwhatwehavealreadydiscussed,withafewobservationalcaveats.First,temperatureisfoundtobeuniversallyindicativeofhigherratesofcompliance,whichwasfoundtonotbeentirelytrueforfacilities91and101;thesecoeﬃcientsarelikelyobscuredbysomedegreeofmulticollinearitywithotherfeatures–thesameistrueofhumid.weekdayandflu,asintheotherresults,arefoundtobemostlyindicativeofhigherratesofcompliance,withtheexceptionofafewfacilities.
21MillionOpportunities17Facilitytemptempweekdayflu14715515314912312791101170168Fig.5:Facility-speciﬁcmarginaleﬀectsmodelingresults.
18MichaelT.LashMSetal.4DiscussionandFutureWorkInthissectionwediscussthebroaderimplicationsofourﬁndings,aswellasdirectionsforfuturework.Theglobalresults,includingthefullM5RidgeRegressionmodel,marginaleﬀectsmodels,andRReliefFfeatureranking,provideseveralinsights.First,wefoundthatfacilityidentitiesarestronglyrelatedtocompliance,suggestingthatfacility-wideattitudestowardshandhygieneexist,persistintime,andarepre-dictiveofcompliancerates.Ontheotherhand,thisobservationmayalsoreﬂectdiﬀerencesinsensorinstallation,wherediﬀerentfacilitiesmayhavesensorsinstru-mentedindiﬀerentdepartments,thusaﬀectingreportedrates.Second,increasesininﬂuenzaseveritywereassociatedwithanincreaseincompliance,whichisen-couragingbecauseitimpliesthathealthcareworkersarerespondingpositively(i.e,morehandhygiene)toanincreasedpresenceofinfectiousdisease.Thisartifactalsosurfacedinourfacility-speciﬁcmodels,whichalsorevealedthatdiﬀerentfa-cilitieshavediﬀerentmagnitudesintheeﬀectofﬂuseverityonhandhygiene.Third,ourconjectureregardinglowerweekendandholidaycomplianceappearstohavesomemerit,althoughthespeciﬁcholidaysassociatedwithnegativecom-plianceweresomewhatsurprising.Weagainacknowledgethatthisresultmaybeaﬀectedbyincreasedvisitorsduringthesetimes,dilutingtheperceivedcompliancerate.Furthermore,ourfacility-speciﬁcmodelingshowedthat,forsomehospitals,bothweekdayandholidayhadalargebearingonhandhygienecompliancepre-dictions(i.e.,thesefactorswereimportantpredictorsofcompliance).Fourth,ourconjecturesthathigherhumidityandtemperatureareindicativeofhigherratesofcompliancewereconﬁrmedbythefullmodel,marginaleﬀectsmodel,andsta-tisticalanalysis.Thisﬁndingisimportantashealthcareworkersoftenciteskinirritationordryskinasreasonsforreducedfrequencyofhandhygiene.Thesesamefactorswerealsostronglysuggestedbyourfacility-speciﬁcmodeling.Fifth,wefoundthatcomplianceduringtheﬁrstweekofresidents’attendancerancon-trarytoouroriginalconjecture:theJulywasessentiallyunobservable.Howeverwedidﬁndthatselectfacilities(153,101,and168)hadthisasaninﬂuencingfac-tor(particularly101and168).Finally,wefoundthatnightwasassociatedwithslightlylowercompliancerates.However,asourfacility-speciﬁcmodelingexposed,somefacilities(149,123)appeartohaveslightlyhigherratesofcomplianceduringtheevening;although,itisworthnotingthat,forthesefacilities,nightwasatthebottomoftheRReﬂiefFfeatureranking(indicatingrelativelylowimportance).Diﬀerentfacilitieshavediﬀerentfactorsthataﬀectcomplianceratesdiﬀerently:notwofacilitiesarealike.Whilemanyofthefacilitieshavefactorsthatinﬂuencecomplianceratesinsimilarways–positiveornegative(e.g.,temperature)–theydiﬀerindegree(howmuchthesecommonfactorsinﬂuencecompliance)andcom-position(thespeciﬁcsetofnon-zerotermsinthehypothesisvectorh∗).Cumula-tively,wecanseethatfactorsaﬀectinghandhygienecomplianceamongfacilitiesisacomplicatedtopicrequiringfurtherinvestigation.Thisworkhasseverallimitations.First,therearediﬀerencesamonginstalla-tions:notalldoorsanddispensersmaybeinstrumentedand,therefore,wecannottrack,forexample,theuseofpersonalalcoholdispensers(wecanonlyassumestablepracticeswithinfacilities).Thusourcomplianceestimatesmaybebasedonpartialinformationandarecertainlynotcomparableacrossfacilities.Second,ourcomplianceestimatesarefacilitywide,meaningthatwedonotexploittheco-21MillionOpportunities19locationofdispensersanddooreventsensors,butonlythetemporalcorrelationoftheindividualevents.Thus,ourassumptionthateachdooreventcorrespondstoahand-hygieneopportunitymaybefundamentallyﬂawed,evenasitallowsforconsistentintra-facilitycomparisons.Third,weacknowledgethepossibilityoflocationandsamplingbiaswithregardtoboththesensorsandfacilities.IfsensorsweretobeplacedinonlytheICUofonefacilityandintheemergencyroomofanother,wemayobservediﬀerentrates,whichmaybeentirelyreasonableandexpectedinclinicalpractice.Additionally,thoughfacilitiesaredistributedacrosstheUnitedStates,theyarebynomeansmeanttobearepresentativesampleoffacilitytypesorclimaticconditions.Inourfutureendeavorswewouldﬁrstliketoconsideralternativedeﬁnitionsofcomplianceandexaminecomplianceatﬁner-grainedtemporallevels,perhapsex-ploringtime-seriesanalyses.Weintendtoalsoexploreframingtheproblemasoneofclassiﬁcation,ratherthanonlyregression,whichmayhelpteaseoutadditionalartifacts.Finally,datapertainingtocomplianceratesundercertaininterventionswouldgivewaytoexplorationofinterventioneﬃcacybothingeneralandus-ingprediction-basedmethodology,suchasinverseclassiﬁcation,torecommendfacility-speciﬁcinterventionpolicies[29,30].Handhygienecomplianceisasimpleyeteﬀectivemethodofpreventingthetransmissionofdisease,bothamongthepopulationatlarge,andwithinhealthcarefacilities,yettherehavebeenfewattemptstostudythefactorsthatcanaﬀectcompliance.Thisstudypresentsaﬁrstlookatfactorsthatunderliehealthcareworkerhand-hygienecompliancerates,includingweatherconditions,holidaysandweekends,andinfectiousdiseaseprevalenceandseverity,andservesasamodelforfuturestudiesthatwillexploittheavailabilityoftemporallyandspatiallyrichcompliancedatacollectedbythesophisticatedsensorsystemsnowbeingputintopractice.5ConﬂictsofInterestPhilipM.PolgreenhasreceivedresearchfundingfromCompanyGOJOIndustries,Inc.AuthorJasonSlaterisanemployeeofGOJOIndustries,Inc.References1.R.Klevens,J.Edwards,C.Richards,andT.Horan,“Estimatinghealthcare-associatedinfectionsanddeathsinushospitals,”PublicHealth,no.122,pp.160–166,2007.2.R.Roberts,R.Scott,B.Hota,L.Kampe,F.Abbasi,S.Schabowski,I.Ahmad,G.Ciavarella,R.Cordell,S.Solomon,R.Hagtvedt,andR.Weinstein,“Costsattributabletohealthcare-acquiredinfectioninhospitalizedadultsandacomparisonofeconomicmeth-ods,”MedicalCare,vol.48,no.11,pp.1026–1035,November2010.3.R.Roberts,B.Hota,I.Ahmad,R.Scott,S.Foster,F.Abbasi,S.Schabowski,L.Kampe,G.Ciavarella,M.Supino,J.Naples,R.Cordell,S.Levy,andR.Weinstein,“Hospitalandsocietalcostsofantimicrobial-resistantinfectioninachiagoteachinghospital:implicationsforantibioticstewardship,”ClinicalInfectiousDiseases,vol.49,no.8,pp.1175–1184,October2009.4.J.M.BoyceandD.Pittet,“Guidelinesforhandhygieneinhealth-caresettings:rec-ommendationsofthehealthcareinfectioncontrolpracticesadvisorycommitteeandthehicpac/shea/apic/idsahandhygienetaskforce,”InfectionControlandHospitalEpidemi-ology,no.23,pp.S3–S41,2002.
20MichaelT.LashMSetal.5.B.Allegranzi,H.Sax,L.Bengaly,H.Richet,D.Minta,M.Chraiti,F.Sokona,A.Gayet-Ageron,P.Bonnabry,andD.Pittet,“Worldhealthorganization”pointg”projectmanage-mentcommittee.successfulimplementationoftheworldhealthorganizationhandhygieneimprovementstrategyinareferralhospitalinmali,africa,”InfectionControlandHospitalEpidemiology,vol.31,no.2,pp.133–141,February2010.6.D.Pittet,B.Allegranzi,andJ.Boyce,“Worldhealthorganizationworldallianceforpa-tientsafetyﬁrstglobalpatientsafetychallengecoregroupofexperts.theworldhealthorganizationguidelinesonhandhygieneinhealthcareandtheirconsensusrecommen-dations,”InfectionControlandHospitalEpidemiology,vol.30,no.7,pp.611–622,July2009.7.J.P.HassandL.E.L.,“Measurementofcompliancewithhandhygiene,”JournalofHospitalInfection,no.66,pp.6–14,2007.8.J.BoyceandM.Cooper,TandaDolan,“Evaluationofanelectronicdeviceforreal-timemeasurementofalcohol-basedhandrubuse,”InfectionControlandHospitalEpidemiol-ogy,vol.30,no.11,pp.1090–1095,November2009.9.JointCommissionofAccreditationofHealthcareOrganizations,“Patientsafetygoals,”JointCommissionofAccreditationofHealthcareOrganizations,Tech.Rep.,2017.[Online].Available:http://www.jcaho.org/accredited+organizations/patient+safety/npsg.htm10.J.Fries,A.Segre,G.Thomas,T.Herman,K.Ellingson,andP.Polgreen,“Monitoringhandhygieneviahumanobservers:Howshouldwebesampling?”InfectionControlandHospitalEpidemiology,vol.33,no.7,pp.689–695,Jul.2012,[PMID:22669230].11.D.Sharma,G.Thomas,E.Foster,J.Iacovelli,K.Lea,J.Streit,andP.Polgreen,“Theprecisionofhuman-generatedhand-hygieneobservations:acomparisonofhumanobserva-tionwithanautomatedmonitoringsystem,”InfectionControlandHospitalEpidemiology,vol.33,no.12,pp.1259–1261,December2012.12.T.Eckmanns,J.Bessert,M.Behnke,andH.Gastmeier,PandaRuden,“Compliancewithantiseptichandrubuseinintensivecareunits:Thehawthorneeﬀect,”InfectionControlandHospitalEpidemiology,no.27,pp.931–934,2006.13.M.Monsalve,S.Pemmaraju,G.Thomas,T.Herman,andP.Segre,AMandaPolgreen,“Dopeereﬀectsimprovehandhygieneadherenceamonghealthcareworkers?”InfectionControlandHospitalEpidemiology,vol.35,no.10,pp.1277–1285,October2014.14.V.Boscart,K.McGilton,A.Levchenko,G.Hufton,P.Holliday,andG.Fernie,“Ac-ceptabilityofawearablehandhygienedevicewithmonitoringcapabilities,”JournalofHospitalInfection,vol.70,no.3,pp.216–222,November2008.15.A.Venkatesh,M.Lankford,D.Rooney,T.Blachford,C.Watts,andG.Noskin,“Useofelectronicalertstoenhancehandhygienecomplianceanddecreasetransmissionofvancomycin-resistantenterococcusinahematologyunit,”AmericanJournalofInfectionControl,vol.36,no.3,pp.199–205,April2008.16.P.M.Polgreen,C.S.Hlady,M.a.Severson,A.M.Segre,andT.Herman,“Methodforautomatedmonitoringofhandhygieneadherencewithoutradio-frequencyidentiﬁcation.”Infectioncontrolandhospitalepidemiology:theoﬃcialjournaloftheSocietyofHospitalEpidemiologistsofAmerica,vol.31,no.12,pp.1294–1297,2010.17.M.T.Lash,J.Slater,P.M.Polgreen,andA.M.Segre,“Alarge-scaleexplorationoffactorsaﬀectinghandhygienecomplianceusinglinearpredictivemodels,”inHealthcareInformatics,2017IEEEInternationalConferenceon(ICHI),2017,pp.66–73.[Online].Available:http://ieeexplore.ieee.org/document/8031133/18.H.Dai,K.L.Milkman,D.A.Hofmann,andB.R.Staats,“TheImpactofTimeatWorkandTimeOﬀfromWorkonRuleCompliance:TheCaseofHandHygieneinHealthcare,”JournalofAppliedPsychology,vol.100,no.3,pp.846–862,2014.[Online].Available:http://papers.ssrn.com/sol3/papers.cfm?abstractid=242300919.C.JarrinTejadaandG.Bearman,“HandHygieneComplianceMonitoring:theStateoftheArt,”CurrentInfectiousDiseaseReports,vol.17,no.4,2015.[Online].Available:http://link.springer.com/10.1007/s11908-015-0470-020.E.Kalnay,M.Kanamitsu,R.Kistler,W.Collins,D.Deaven,L.Gandin,S.Iredell,S.Saha,G.White,Y.Zhu,a.Leetmaa,R.Reynolds,M.Chelliah,W.Ebisuzaki,W.Higgins,J.Janowiak,K.Mo,C.Ropelewski,J.Wang,R.Jenne,andD.Joseph,“TheNCEP/NCAR40-YearReanalysisProject,”pp.437–471,1996.[Online].Available:21.N.R.Draper,H.Smith,andE.Pownell,AppliedRegressionAnalysis.WileyNewYork,1966,vol.3.22.J.R.Quinlan,“Learningwithcontinuousclasses,”in5thAustralianJointConferenceonArtiﬁcialIntelligence,vol.92,1992,pp.343–348.
21MillionOpportunities2123.F.D.Johansson,U.Shalit,andD.Sontag,“Learningrepresentationsforcounterfactualinference,”in33rdInternationalConferenceonMachineLearning(ICML),2016.24.R.Tibshirani,“Regressionshrinkageandselectionviathelasso,”JournaloftheRoyalStatisticalSociety.SeriesB(Methodological),pp.267–288,1996.25.H.ZouandT.Hastie,“Regularizationandvariableselectionviatheelasticnet,”JournaloftheRoyalStatisticalSociety:SeriesB(StatisticalMethodology),vol.67,no.2,pp.301–320,2005.26.M.Robnik-ˇSikonjaandI.Kononenko,“Anadaptationofreliefforattributeestimationinregression,”inMachineLearning:ProceedingsoftheFourteenthInternationalConference(ICML97),1997,pp.296–304.27.K.KiraandL.A.Rendell,“Apracticalapproachtofeatureselection,”inProceedingsoftheninthinternationalworkshoponMachinelearning,1992,pp.249–256.28.R.Williamsetal.,“Usingthemarginscommandtoestimateandinterpretadjustedpre-dictionsandmarginaleﬀects,”TheStataJournal,vol.12,no.2,p.308,2012.29.M.T.Lash,Q.Lin,W.N.Street,J.G.Robinson,andJ.Ohlmann,“Gen-eralizedinverseclassiﬁcation,”inProceedingsofthe2017SIAMInternationalConferenceonDataMining(SDM’17),2017,pp.162–170.[Online].Available:https://doi.org/10.1137/1.9781611974973.1930.M.T.Lash,Q.Lin,W.N.Street,andJ.Robinson,“Abudgetconstrainedinverseclas-siﬁcationframeworkforsmoothclassiﬁers,”inDataMiningWorkshops(ICDMW),2017IEEEInternationalConferenceon,2017,pp.1184–1193.

Cluster analysis is essential to many applications in com- puter vision and pattern recognition.
Given this fact and the recent advent of deep learning, there is an increasing interest in learning deep unsupervised representations for clustering analysis [1], [2], [3], [4].
Most of the methods that perform clustering on deep representations, make use of auto-encoder representations (output of the encoder part) and deﬁne clus- tering losses on them.
The focus of previous works have been on the choice of the auto-encoder type and architecture and the clustering loss.
In DEC [1], ﬁrst a dense auto-encoder is trained with minimizing reconstruction error.
Then, as a clustering optimization stage, the method iterates between computing an auxiliary target distribution from auto-encoder representations and minimizing the Kullback-Leibler diver- gence to it.
In IDEC [2], it is argued that the clustering loss of DEC corrupts the feature space, therefore IDEC proposes to jointly optimize the clustering loss and reconstruction loss of the auto-encoder.
DCEC [4] argues the inefﬁciency of using dense auto-encoders for image clustering, therefore adopts a convolutional auto-encoder and shows that it improves the clustering accuracy of DEC and IDEC.
GMVAE [3] adopts a variational auto-encoder in order to learn unsupervised representations and simply applies K-means clustering on representations.
In this manuscript, we show that regardless of the auto- encoder type (dense or convolutional), constraining the auto- encoder representations to be on the unit-ball, i.e. to be l2 normalized, during auto-encoder training, greatly improves the clustering accuracy.
We show that a simple k-means clustering on the auto-encoder representations trained with our constraint already gives improved accuracy with a large margin compared to baselines with or without additional clustering losses.
Motivated by the high performance of our clustering method on deep representations, we propose an unsupervised anomaly detection method based on this clustering.
We show that our anomaly detection method greatly improves on other deep anomaly detection strategies such as reconstruction error based ones.
We also investigate the effect of l2 normalization constraint during training on the anomaly detection accuracy and show that it leads to superior results compared to not applying the constraint.
II.
RELATED WORK A.
Deep Unsupervised Anomaly Detection Unsupervised anomaly detection tries to ﬁnd anomalies in the data without using any annotation [7].
Recently, deep learning methods have also been used for this task [5], [6].
These works train auto-encoders on the entire data and use reconstruction loss as an indicator of anomaly.
DRAE [5] trains auto-encoders and uses reconstruction error as an anomaly indicator.
Moreover, DRAE proposes a method to make the reconstruction error distributions of the normal and abnormal classes even more separable so that is easier to detect anomalies.
AVAE [6] trains both conventional and variational auto-encoders and use reconstruction error as an anomaly indicator.
it The general assumption of the above works is that since the anomaly data is smaller in ratio than the normal data, the auto-encoder would not learn to reconstruct it accurately.
The above assumption seems to work in a speciﬁc deﬁnition of anomaly where the normal samples are drawn from a single class only and anomaly classes have been selected from many other classes [5].
However, the assumption fails in another anomaly type where the normal samples are drawn from multiple classes and anomaly class is sampled from a speciﬁc class [6].
In this paper we propose an unsupervised anomaly detection method based on clustering on deep auto-encoder represen- tations and show that it gives a superior performance than reconstruction error based anomaly.
(a) No normalization (b) l2 normalization after training (c) l2 normalization during training Fig.
1: Illustration of t-SNE encoding of auto-encoder representations for MNIST dataset to two dimensions.
Best viewed in color.
B.
Regularization and Normalization in Neural Networks III.
PROPOSED METHOD Due to the high number of parameters, neural networks have a risk of over-ﬁtting to the training data.
This sometimes reduces the generalization ability of the learned network.
In order to deal with over-ﬁtting, mostly regularization methods are employed.
One of the most widely used regularization technique is weight norm regularization.
Here the aim is to add an additional regularization loss to the neural network error, which gives high penalty to weights that have high norms.
Both l1 and l2 norm can be exploited.
Recently some normalization techniques for neural networks emerged such as [8], [9].
Batch normalization [8], aims to ﬁnd a statistical mean and variance for the activations which are calculated and updated according to batch statistics.
The activations are normalized according to these statistics.
In layer normalization [9], the mean and variance are computed from all of the summed inputs to the neurons on a layer on a single training sample.
This overcomes the batch-size dependency drawback of batch-normalization.
Although these methods were mainly proposed as tricks to make the neural network training faster by conditioning each layer’s input, it is argued that they may also have a regularization effect due to their varying estimations of parameters for standardization at each epoch.
In our proposed method, the unit ball constraint that we put on activations is a normalization technique.
However, unlike layer or batch normalization, the unit ball constraint is parameter-free as it simply sets the norm of each activation vector to 1.
Therefore, it is free from the parameter estimation stochasticity.
Yet, it may still act as a regularization method due to its hard constraint on some activations to be of ﬁxed norm.
This slightly resembles the l2 norm regularization.
A key difference is that in l2 norm regularization, the norm is of the weights, but in our case it is applied on the activations.
Another key difference is that we ﬁx the activation norms to 1, whereas l2 norm regularization penalizes to large weight norms and does not ﬁx the norms to any value.
A.
Clustering on l2 Normalized Deep Auto-Encoder Repre- sentations We represent the auto-encoder representations for the input I as E(I) and the reconstructed input as the D(E(I)).
The representations are generally obtained via several dense or convolutional layers applied on the input I.
In each layer, usually there are ﬁltering and an activation operations, and optionally pooling operations.
Let fi be the computations applied to the input at layer i, then the encoded representations for an n-layer encoder are obtained as in Eq. 1.
E(I) = fn(fn−1(...f1(I))) (1) The reconstruction part of the auto-encoder applies on E(I) and is obtained via several dense or deconvolutional layers.
In each layer, usually there are ﬁltering and activation operations and optionally un-pooling or up-sampling operations.
Let gi be the computations applied to the auto-encoder representa- tions, then the reconstructed signal for an m-layer decoder is obtained as as in Eq. 2.
D(E(I)) = gm(gm−1(...g1(E(I)))) (2) The auto-encoder training is conducted in order to reduce the reconstruction error (loss) given in Eq. 3.
(cid:88) j∈J L = |J| (Ij − D(E(Ij)))2 (3) Here, we propose an additional step conducted on the auto-encoder representations E(I).
In particular, we apply l2 normalization on E(I).
This corresponds to adding a hard constraint on the representations to be on the unit ball.
The loss function with our introduced constraint can then be written as in Eq. 4, where Lc and Ec are loss and encoded representations with our introduced constraint.
(a) Clustering Method (b) Normality Score Method Fig.
2: Illustration of proposed methods in inference phase.
(cid:88) j∈J Lc = |J| (Ij − D(Ec(Ij)))2, Ec(I) = E(I) (cid:107)E(I)(cid:107)2 (4) We believe that l2 normalized features (representations) are more suitable for clustering purposes, especially for the methods that use Euclidean distance such as conventional k-means.
This is because the distances between the vectors would be independent of their length, and would instead depend on the angle between the vectors.
As a positive side effect, enforcing the unit norm on representations would act as a regularization for the entire auto-encoder.
In Fig.
1, we illustrate t-SNE [10] encoding (to 2 dimen- sions) of the auto-encoder representations of networks with same architecture.
All auto-encoders were trained on MNIST [11] dataset training split.
The representations corresponding to Fig.
1a are from the auto-encoder that was trained by the loss in Eq. 3.
The same representations with l2 normalization applied after training are illustrated in Fig.
1b.
Finally, the representations with l2 constraint during training , i.e. training with loss Eq. 4, are illustrated in Fig.
1c.
It is observed from the ﬁgures that the l2 normalization during training (Fig.
1c) results into more separable clusters.
One example is the distributions of digit 7 in MNIST dataset.
Note that the numbers are indicated with color codes where the color bar is available in Fig.
1.
It is clearly observed that with no normalization during training (Fig.
1a), digit 7 is divided into 2 parts where the small part is surrounded by 8,9,6 and 2 digits.
With normalization applied after training (Fig.
1b) this effect becomes even more evident.
So, we clearly observe here that applying normalization after training does not help at all.
But, with l2 normalization constraint during training (Fig.
1c), we see a clear separation of digit 7 as a single cluster from the rest of the numbers.
Moreover, it can be observed that the clusters are more compact in Fig.
1c compared to others.
After training the auto-encoder with the loss function in Eq. 4, the clustering is simply performed by k-means algorithm.
No more clustering loss is applied.
Our clustering method is illustrated in Fig.
2a.
B.
Unsupervised Anomaly Detection using l2 Normalized Deep Auto-Encoder Representations Here, we propose a clustering based unsupervised anomaly detection.
We train an auto-encoder on the entire dataset including normal and abnormal samples and no annotation or supervision is used.
The auto-encoder is simply trained with the loss in Eq. 4.
After training, the l2 normalized auto- encoder representations are clustered with k-means algorithm.
We assume the anomaly cases to be considerably smaller in number than any normal clusters.
Note that this assumption does not put any constraint on the dataset, but it simply follows the general deﬁnition of anomaly.
Therefore, the centroids obtained by the k-means method can be considered to be representations of normal clusters by some errors that are caused by anomaly samples in the dataset.
To each sample i, we assign the normality score vi in Eq. 5.
(Ec(Ii) · Cj(cid:107)Cj(cid:107)2 (5) In Eq. 5, Cj is a cluster centroid and · is the dot product oper- ator.
Notice that we l2 normalize the cluster centroids.
Since representations Ec(Ii) are already l2 normalized, vi ∈ [0, 1] holds.
The measure in Eq. 5 is intuitive considering that we expect high similarities of normal samples to normal classes.
Our normality scoring method is illustrated in Fig.
2b.
vi = max The normality score can be used for anomaly detection in a straightforward manner.
Simply, the abnormal samples can be detected as the ones having vi < τ , where τ ∈ [0, 1] is a threshold.
IV.
EXPERIMENTAL RESULTS A.
Clustering Evaluation Metrics: We use the widely used evaluation for unsupervised clustering accuracy [1] given in Eq. 6.
(cid:80)n i=1 111{li = m(ci)} acc = max (6) In Eq. 6, li is the ground truth labeling of sample i, ci is the cluster assignment according to the one-to-one mapping deﬁned by m, where m ranges over all possible one-to-one mappings between clusters generated by the algorithm and the ground truth labels.
The maximization in Eq. 6 can be performed by Hungarian algorithm [12].
We compare the clustering accuracy of auto-encoder rep- resentations with and without l2 normalization constraint.
We make this comparison in dense and convolutional auto- encoders.
For dense auto-encoder, we use MNIST [11] and for convolutional auto-encoders, we use MNIST [11] and USPS [15] datasets.
This is due to the availability of results in the works that use dense and convolutional auto-encoders.
For dense auto-encoder, we use the network structure which is used both in DEC [1] and IDEC [2].
In encoding part there are 4 layers with 500 − 500 − 2000 − 10 hidden neurons and in decoding part there are 2000 − 500 − 500 − d neurons, where d is the dimension of the input.
We re-implement the auto-encoder training and with leaky relu [13] activations after each hidden layer except for the last one and trained the auto- encoder end to end for 100 epochs.
We select the best model with lowest reconstruction error.
As it can be observed from Table I, we obtain a very similar clustering accuracy when we apply k-means on auto-encoder representations compared to the original paper of DEC [1].
Note here that results indicated with * corresponds to our own implementation.
Other results for baselines are borrowed from original papers.
Table I shows that when we train the auto-encoder with our l2 normalization constraint on the representations, we achieve a much better clustering accuracy when we apply k-means on the representations.
We denote our method as AE-l2 which stands for auto-encoder with l2 normalization.
Moreover, our clustering accuracy is even better than the methods that deﬁne a separately designed clustering loss on the representations (DEC and IDEC).
Next, we make experiments for the convolutional auto- encoder.
For this, we make use of the model structure in- troduced in DCEC [4].
This model consists of 5x5, 5x5 and 3x3 convolutional ﬁlters in the encoding layers respectively.
There are 32,64 and 128 ﬁlters in encoding layers respectively.
Convolutions are applied with 2x2 strides and with relu [14] activations.
After the convolutional layers, the activations are ﬂattened and there is a dense layer of dimension 10.
This is followed by another dense layer and reshaping.
Decoder part consists of 64,32 and 1 deconvolutional ﬁlters of size 3x3, 5x5 and 5x5 respectively.
Relu activations were applied after each convolution, except for the last one.
The network was trained for 200 epochs as in the original paper of DCEC [4].
In Table II, we show clustering accuracy of k-means applied on convolutional autoencoder representations.
We were able to obtain similar results as in the original paper (DCEC).
Note here that results indicated with * corresponds to our own implementation.
Other results for baselines are borrowed from original papers.
It can be observed from Table II that when we train the convolutional autoencoder with our l2 normaliza- tion constraint on representations, we achieve a much better performance.
We denote our method as CAE-l2 which stands for convolutional auto-encoder with l2 normalization.
Our performance is superior to DCEC which introduces additional clustering loss.
TABLE I: Clustering on Dense Auto-Encoder Representations AE* k-means 81.43 AE k-means 81.82 DEC IDEC 86.55 88.06 AE-l2 k-means 90.20 MNIST TABLE II: Clustering on Convolutional Auto-Encoder Repre- sentations CAE* k-means 84.83 73.521 MNIST USPS CAE k-means 84.90 74.15 DCEC 88.97 79.00 CAE-l2 k-means 95.11 91.35 TABLE III: Comparison of Normalization Methods MNIST USPS batch-norm layer-norm l2-norm 95.11 91.35 70.83 75.263 70.67 74.95 l2 versus Batch and Layer Normalization: Due to l2 nor- malization step in our clustering method, we compare it with applying other normalization techniques training.
In particular we train two separate networks by using batch [8] and layer [9] normalization, instead of l2 normalization.
All other setup for the experiments are the same.
Batch size of 256 is used for all methods in order to have a large enough batch for batch normalization.
Our method performs superior to both baselines by a large margin, as the accuracies in Table III indicate.
More importantly it is noticed that neither batch nor layer normalization provides a noticeable accuracy increase over the baseline (CAE+k-means).
Moreover in MNIST dataset, layer and batch normalization results into a signiﬁcant accuracy decrease.
This is an important the performance upgrade of our method is not a result of a input conditioning, but it is a result of the speciﬁc normalization type that is more ﬁt for clustering in Euclidean space.
indicator showing that B.
Anomaly Detection Evaluation Metrics: An anomaly detection method often generates an anomaly score, not a hard classiﬁcation result.
Therefore, a common evaluation strategy in anomaly detec- tion is to threshold this anomaly score and form a receiver operating curve where each point is the true positive and false positive rate of the anomaly detection result corresponding to a threshold.
Then, the area under the curve (AUC) of RoC curve is used as an evaluation of the anomaly detection method [7].
Here, we evaluate our method introduced in Section III-B.
The evaluation setup and implementation of our method are as follows.
In MNIST training dataset, we select a digit class as anomaly class and keep a random 10% of that class in the dataset while the remaining 90% is ignored.
We leave the rest of the classes as is.
Then, we use the convolutional autoencoder structure in DCEC [4] and train it with our l2 normalization constraint on representations.
Finally, we apply k-means clustering on the representations and keep the centroids.
In our experiments we use k=9 for k- means, since we assume that we know the number of normal classes in the data.
For MNIST test dataset, we calculate the auto-encoder representations.
As a normality measure for each sample, we calculate the corresponding representation’s maximum similarity to pre-calculated cluster centroids as in 5.
It should be noted that we repeat the above procedure by choosing a different class to be anomaly class, for all possible classes.
We also evaluate two baselines.
In the ﬁrst baseline, we ex- actly repeat the above procedure, but without l2 normalization constraint on representations.
In the second baseline, again we train the auto-encoder with l2 normalization constraint on representations.
Then, on the test set, we calculate the reconstruction error per sample and deﬁne that as anomaly score.
Using reconstruction error based anomaly detection follows the works in AVAE [6] and DRAE [5].
The setups in AVAE and DRAE are different than ours.
In AVAE, the training is only conducted on normal data, so the method is not entirely unsupervised in that sense.
In DRAE, the anomaly deﬁnition is different: only a single class is kept as normal and samples from other classes are treated as anomaly.
That setup presents a much easier case and therefore reconstruction error based anomaly detection produces acceptable results.
Next, we show that in our setup this is not the case.
For each method we plot a RoC curve via thresholding the normality (or anomaly) score with multiple thresholds.
Then, we evaluate the area under the RoC curve (AUC) for measuring the anomaly detection performance.
The training and test datasets for all methods are the same.
Due to the random selection of 10% of the anomaly class to be kept, performance can change according to the partition that is randomly selected.
Therefore, we run the method 10 times for different random partitions and report the mean AUC.
It can be observed from Table IV that our clustering based anomaly detection method drastically outperforms the reconstruction error based anomaly detection for CAE neural network structure.
It is worth noting here an interesting observation from Table IV: for digits 1, 7 and 9, reconstruction error based anomaly detection gets a very inferior performance.
This is most evident in digit 1.
The reason for this is that the digit 1 is very easy to reconstruct (only 1 stroke) and even though an auto- encoder is trained on much less examples of this digit, it can reconstruct it quite well.
This shows a clear drawback of the reconstruction error based anomaly detection.
However, in our clustering based method, we achieve a very high accuracy in all the digits.
The effect of our proposed l2 normalization constraint on representations during training can also be observed from Table IV.
In 9/10 cases, i.e. digits selected as anomaly, anomaly detection with the network trained with l2 normal- ization constraint on representations performs much better than the one without.
Only in digit 9, we observe an inferior accuracy of our method.
Compared to other digits, we observe less performance for digits 4 and 9.
We argue that this might be happening due to very similar appearance of these digits in some handwritings.
Therefore, the method may confuse these TABLE IV: Anomaly Detection with Auto-Encoder Represen- tations Anom.
Digit CAE.
(recons) 0.7025 0.0782 0.879 0.8324 0.7149 0.8359 0.6925 0.5767 0.8912 0.514 CAE (cluster) 0.7998 0.8871 0.7512 0.8449 0.4988 0.7635 0.7896 0.7421 0.9200 0.8944 CAE-l2 (cluster) 0.9615 0.9673 0.9790 0.9382 0.7825 0.9136 0.9497 0.9100 0.9237 0.7495 TABLE V: Anomaly Detection with Auto-Encoder Represen- tations Anom.
(Digit) AE.
(recons.) 0.825 0.135 0.874 0.761 0.727 0.792 0.812 0.508 0.869 0.548 VAE.
(recons.) 0.917 0.136 0.921 0.781 0.808 0.862 0.848 0.596 0.895 0.545 CAE-l2 cluster 0.9615 0.9673 0.9790 0.9382 0.7825 0.9136 0.9497 0.9100 0.9237 0.7495 numbers with each other during clustering.
In Table V, we compare our method to another method [6] that performs reconstruction error based anomaly detection, but using dense auto-encoders.
There is also a variational auto-encoder based version of the method.
It should be noted that this method trains auto-encoders only on normal data.
This presents a much easier task compared to our case where we also include anomalous samples during training.
Thus our case is entirely unsupervised.
Still, 9/10 cases, our method outperforms both variants of the method with a large margin.
Only in digit 4, we observe an inferior performance of our method compared to VAE method.
V.
CONCLUSION In this paper, we have applied a l2 normalization constraint to deep autoencoder representations during autoencoder train- ing and observed that the representations obtained in this way clusters well in Euclidean space.
Therefore, applying a simple k-means clustering on these representations gives high clustering accuracies and works better than methods deﬁning additional clustering losses.
We have also shown that the high performance is not due to any conditioning applied on the representations but it is due to selection of a particular nor- malization that leads to more separable clusters in Euclidean space.
We have proposed an unsupervised anomaly detection method on l2 normalized deep auto-encoder representations.
We have shown that the proposed l2 normalization constraint drastically increases the anomaly detection method’s perfor- mance.
Finally, we have shown that the commonly adopted deep anomaly detection method based on the reconstruction error performs weak in a deﬁnition of anomaly, whereas our method performs superior.
REFERENCES [1] J.
Xie, R.
Girshick and A.
Farhadi, Unsupervised deep embedding for clustering analysis, International Conference on Machine Learning, pp.
478-487, June, 2016.
[2] X.
Guo, L.
Gao, X.
Liu and J.
Yin, Improved deep embedded clustering with local structure preservation, International Joint Conference on Artiﬁcial Intelligence, pp.
1753-1759, June, 2017.
[3] N.
Dilokthanakul, P.
A.
Mediano, M.
Garnelo, M- C- Lee, H.
Sal- imbeni, K- Arulkumaran and M.
Shanahan, Deep unsupervised clus- tering with gaussian mixture variational autoencoders, arXiv preprint arXiv:1611.02648, 2016.
[4] X.
Huo, X.
Liu, E.
Zheand J.
Yin, Deep Clustering with Convolutional Autoencoders, International Conference on Neural Information Process- ing, pp.
373-382, 2017.
[5] Y.
Xia, X.
Cao, F.
Wen, G.
Hua and J.
Sun, Learning discriminative reconstructions for unsupervised outlier removal, Proceedings of the IEEE International Conference on Computer Vision, pp.
1511-1519, 2015.
[6] J.
An and S.
Cho, Variational autoencoder based anomaly detection using reconstruction probability, SNU Data Mining Center, Tech.
Rep., 2015.
[7] M.
Goldstein and S.
Uchida, A comparative evaluation of unsupervised anomaly detection algorithms for multivariate data, PloS one, vol 11, no.
4, 2016.
[8] S.
Ioffe and C.
Szegedy, Batch normalization: Accelerating deep network training by reducing internal covariate shift, International conference on machine learning, pp.
448-456, 2015.
[9] J.
L.
Ba, J.
R.
Kiros and G.
E.
Hinton, Layer normalization, arXiv preprint arXiv:1607.06450, 2016.
[10] L.
V.
D.
Maaten and G.
Hinton, Visualizing data using t-SNE, Journal of machine learning research, pp.
2579-2605, 2008.
[11] Y.
LeCun, L.
Bottou, Y.
Bengio and P.
Haffner, Gradient-based learning applied to document recognition, Proceedings of the IEEE, vol.
86, no.
11, pp.
2278-2324, 1998.
[12] H.
W.
Kuhn, The Hungarian method for the assignment problem, Naval Research Logistics, 2(1-2), pp.
83-97, 1955.
[13] V.
Nair and G.
E.
Hinton, Empirical evaluation of rectiﬁed activations in convolutional network, arXiv preprint arXiv:1505.00853, 2015.
[14] B.
Xu, N.
Wang, T.
Chen and M.
Li, Rectiﬁed linear units improve restricted boltzmann machines, International Conference on Machine Learning, pp.
807-814, 2010.
[15] Y.
LeCun, O.
Matan, B.
Boser, J.
D.
Denker, D.
Henderson, R.
E.
Howard, W.
Hubbard, L.
D.
Jacket and H.
S.
Baird, Handwritten zip code recognition with multilayer networks.
International Conference on Pattern Recognition, vol.
2, pp.
35-40, 1990.

Over the last few decades, many initialization, optimiza- tion, regularization, and many other techniques have been invented (Bengio et al., 2007; Hinton et al., 2006) to make deep neural networks (DNNs) easily applicable to solv- ing challenging artiﬁcial intelligence tasks (Lecun et al., 2015).
Nevertheless, classical DNNs like VGG networks (Simonyan & Zisserman, 2014) have the problem of degra- dation, i.e., when the network goes deeper both training and testing errors increase even with sufﬁcient training data (He et al., 2016a).
Deep residual networks (ResNets), especially the pre-activated ones (He et al., 2016a;b), proposed by He et al.
employ shortcut connections to learn residuals only and keep a clean information path which efﬁciently solve the aforementioned degradation problem.
Furthermore, deep 1Department of Mathematics, UCLA, Los Angeles, California, USA 2Department of Mathematical Sciences, Yau Mathematical Sciences Center, Tsinghua University, Beijing, China 3Department of Mathematics, Duke University, Durham, North Carolina, USA.
Correspondence to: Bao Wang <wangbaonj@gmail.com>.
ResNets enable generalization accuracy improvement for networks up to 1000 layers.
Many advances have been made after the emergence of the deep ResNets.
These include both theoretical analysis and algorithmic development.
In the original work of He et al (He et al., 2016b), the pre-activated ResNets are formulated as discrete dynamical systems.
This dynamical system point of view leads to an elegant analysis on the optimality of the identity map in the ResNets.
Hardt and Ma (Hardt & Ma, 2017) use matrix factorization techniques to analyze the landscape of the linear ResNets and its representation power.
E considers the deep ResNets as a control problem of a class of continuous dynamical systems (E, 2017).
New network structures development is as thriving as theoretical analysis.
Instead of using a single shortcut to connect two consecutive residual blocks, densely connect convolutional networks employ shortcut connections to connect all the distinct blocks (Huang et al., 2017).
The wide residual networks (Zagoruyko & Komodakis, 2016) increase the width of the layers in the original ResNets.
Both dense nets and wider ResNets have certain amount of improvement compared to the ResNets.
Dual path networks is another family of interesting improvement over both ResNets and dense nets (Chen et al., 2017).
The accuracy of the DNN externally depends on massive amounts of training data.
The lack of sufﬁcient training data typically leads to another degradation problem.
Sig- niﬁcant accuracy reduction tends to occur as the network goes deeper, which will be further demonstrated in this pa- per.
Many regularization techniques explored to attempt to tackle this challenge(Zhu et al., 2017; Srivastava et al., 2014; Wen et al., 2016), but satisfactory results are rare.
Most existing strategies can be classiﬁed to either loss func- tion regularization or network structure regularization.
None of these considered the speciﬁcity of the data which is of critical importance in data analysis(Bishop, 2006).
In this paper, we try to solve the issue of lacking enough training data by using the information of data as a prior to train the DNNs. We will ﬁrst build the connection be- tween the deep ResNets and the partial differential equation (PDE) control problems.
Well-posedness theories of PDE control problems suggest us to take data dependent acti- vations in DNNs. To make the data dependent activation Deep Learning with Data Dependent Implicit Activation Function control problems, let us consider the terminal value problem of the linear transport equation in Rd: (cid:40) ∂u ∂t + v(x, t) · ∇u = 0 x ∈ Rd, t ≥ 0 u(x, 1) = f (x) x ∈ Rd, (2) where v(x, t) is a given velocity ﬁeld, d is the dimension of the ﬂattened input tensor, f is the composition of the activation function and the fully connected layer.
If we use the softmax activation function, f (x) = softmax(WF C · x), (3) where WF C is the weight in the fully connected layer, and the softmax function is given by softmax(x)i = (cid:80) exp(xi) j exp(xj) which models the posterior probability of the instance be- longing to each class.
It is well-known (Evans, 2010) that the solution at t = 0 can be solved along the characteristics: dX(t; x) dt = v (X(t; x), t) , X(0; x) = x.
(4) We know that along the characteristics, u is a constant (Set T = 1 below): u(x, 0) = u (X(1; x), T ) = f (X(1; x)).
Let {tk}L k=0 with t0 = 0 and tL = 1 be a partition of [0, 1].
The characteristics of the transport equation Eq.(4) can be approximately solved by using the simple forward Euler discretization from X0(x) = x: Xk+1(x) = Xk(x) + ∆tv(Xk(x), tk), (5) where ∆t is the time step.
If we choose the velocity ﬁeld such that ∆t v(x, t) = W(2)(t) · σ W(1)(t) · σ(x) (6) (cid:16) (cid:17) where W(1)(t) and W(2)(t) corresponds to the “weight” layers in the residual block, σ = ReLU ◦ BN, one step in the forward Euler discretization Eq.(5) is equivalent to advancing one layer in the deep ResNets, (see Fig.
1).
The numerical solution of the transport equation Eq.(2) at t = 0 is given by u(x, 0) = f (XL(x)), (7) which is exactly the output we get from the ResNets.
Let x be a point from the training data with its label g(x).
Training the ResNet is equivalent to ﬁnding the parameters in the velocity ﬁeld Eq.(6) and the terminal value so that the output in Eq.(7) matches the label g(x) Figure 1.
The building block of the pre-activated ResNets.
trainable, we propose surgeries to the existing DNNs to construct new data dependent implicitly activated DNNs. Efﬁcient algorithms to train and test the model are investi- gated.
Numerical results on the CIFAR10 dataset with quite limited and randomly selected instances show great suc- cess of our paradigm in solving the challenge of insufﬁcient data.
Another successful achievement of our framework is regarding the generalization error reduction.
On the CI- FAR10 and CIFAR100 datasets we receive 30% and 20% error reduction, respectively, compared to the base DNNs which includes VGG, ResNet, and pre-activated ResNet families.
Our method provides an alternative towards model compression which is important for applications on mobile devices.
This paper is structured as follows: In section 2, we present the connection of the PDE control problems with the deep ResNets, and some improvements of deep ResNets moti- vated by the PDE theory.
Data interpolation on a general manifold in a harmonic extension manner is reviewed in sec- tion 3.
Our DNN surgeries, along with their training/testing procedures, will be presented in section 4.
To validate our algorithms, a large variety of numerical results are demon- strated in section 5.
The summary of this work and future directions are discussed in section 6.
2.
Deep Residual Networks and PDE Control Problem Deep ResNets, especially the pre-activated ResNets (Pre- ActResNets), are realized by adding shortcut connections to connect the consecutive residual blocks in the classical convolutional neural networks (CNN).
This can also be re- garded as a cascade of the residual block, as shown in Fig.1, followed by the ﬁnal ﬂatten and activation layers.
Mathe- matically, a residual block is formulated as: y = F(x,{Wi}) + x, (1) where x and y are the input and output tensors of the block, the function F(x,{Wi}) represents the residual mapping to be parametrized.
To build the connection between the deep ResNets and PDE BNxlxl+1ReLUConv.BNReLUConv.+Deep Learning with Data Dependent Implicit Activation Function In summary, the training process of the deep ResNets can be regarded as solving the following control problem of a transport equation in Rd: ∂t + v(x, t) · ∇u(x, t) = 0 x ∈ Rd, t ≥ 0 u(x, 1) = f (x) u(xi, 0) = g(xi) x ∈ Rd xi ∈ T, (8)  ∂u where T denotes the training set, g(xi) is the label of in- stance xi.
This control problem is uniquely determined by the terminal value u(x, 1) = f (x) and the velocity ﬁeld v(x, t).
Conventionally, the corresponding terminal value of the transport equation is selected to be a softmax activation function as shown in (3).
From the control problem point of view, the softmax function may not be a good terminal condition, since it is pre-determined and maybe far from the real value .
The ideal terminal function should be a smooth function and close to the labeled value in the training set .
Based on this observation, the weighted nonlocal Laplacian (Shi et al., 2017) seems to provide a good choice for the terminal function.
3.
Manifold Interpolation-A Harmonic Extension Approach In this section, we will brieﬂy discuss smooth interpo- lation on a general smooth manifold, and give a sufﬁ- cient condition on the number of samples needed to make sure this interpolation has enough representation diver- sity.
Consider the following interpolation problem: Let P = {p1, p2,··· , pn} be a set of points in a manifold M ⊂ Rd and S = {s1, s2,··· , sm} be a subset of P .
Sup- pose we have the labels for the data in S, and we want to extend the label function u to the entire dataset P .
Har- monic extension is a natural approach, which minimizes the following Dirichlet energy functional: (cid:88) x,y∈P E(u) = with the boundary condition: u(x) = g(x), x ∈ S, (cid:40)(cid:80) where w(x, y) is a weight function, typically chosen to be the Gaussian weight w(x, y) = exp(−||x−y||2 ), and σ is a scaling parameter.
The Euler-Lagrange equation for Eq.(9) is: σ2 u(x) = g(x) y∈P (w(x, y) + w(y, x)) (u(x) − u(y)) = 0 x ∈ P/S x ∈ S, (10) which indeed is a boundary value problem for the graph Laplacian (GL).
It is observed in the work (Shi et al., 2017), (cid:17)(cid:80) that adding a scale on the GL effectively tames the issue of highly unbalanced data, which leads to the following weighted nonlocal Laplacian (WNLL):  (cid:80) (cid:16)|P| y∈P (w(x, y) + w(y, x)) (u(x) − u(y)) + |S| − 1 u(x) = g(x) y∈S w(y, x) (u(x) − u(y)) = 0 x ∈ P/S x ∈ S, (11) In order to guarantee the representability of the WNLL, the labeled data should cover all types of instances in the data pool.
For this, we give a necessary condition in Theorem 1.
Theorem 1.
(Representability) Suppose we have a data pool formed by N classes of data uniformly, with the number of instances of each class to be sufﬁciently large.
If we want all classes of data to be sampled at least once, on average (cid:1) data need to be sampled at least N(cid:0)1 + 1 2 + 1 3 + ··· + 1 from the data pool.
In this case, the number of data sampled for each class is 1 + 1 Proof.
Let Xi, i = 1, 2,··· , N, be the number of addi- tional data needed to obtain the i-type after (i − 1) distinct types have been sampled.
The total number of instances needed is: N , in expectation.
3 + ··· + 1 2 + 1 X = X1 + X2 + ··· + XN = Xi. N(cid:88) i=1 For any i, i− 1 distinct types of instances have already been sampled.
It follows that the probability of a new instance being of a different type is 1− i−1 N = N−i+1 N .
Essentially, to obtain the i-th distinct type, the random variable X follows a geometric distribution with p = N−i+1 and E[Xi] = N−i+1.
Thus, we have N(cid:88) N(cid:88) N − i + 1 Asymptotically, E[X] ≈ N ln N for sufﬁciently large N.
4.
Network Structure & Training Algorithms Deep ResNets enable hierarchical representation learning which leads to fabulous performance on many artiﬁcial intel- ligence tasks.
WNLL is a harmonic extension approach for manifold extension analytically and adaptable even when the labeled instances are extremely scarce.
As illustrated in the connection between deep ResNets and PDE control problems, a data dependent terminal value in the control problem, i.e., data dependent activation function in the deep ResNets, should be better than the ad hoc activation func- tions, e.g., softmax or linear activations.
In this section, we w(x, y) (u(x) − u(y))2 , (9) E[X] = E[Xi] = i=1 i=1 Deep Learning with Data Dependent Implicit Activation Function will discuss how to efﬁciently perform label extension in a harmonic extension manner and put the WNLL into the deep ResNets.
The new framework has the following ben- eﬁts: the deep ResNet to learn optimal representations for the WNLL interpolation; simultaneously with the WNLL as the activation layer, the learned deep representations can be better utilized than the classical activation functions.
We will show that this on-the-ﬂy coupling via information feed- forward and error back propagation solves the lack of data issue and achieves great accuracy improvement compared to the existing DNNs. Let us ﬁrst discuss the numerical approach to solve the WNLL interpolation given in Eq.(11).
The numerical ap- proach is straight forward with two computational burdens involved: ﬁnding the weights w(x, y) for any x, y ∈ P , and solving the resulting linear system.
To ﬁnd the pairwise weights, we need to perform the nearest neighbor searching.
A brute-force approach is of quadratic scaling, however, there are many fast algorithms with sub-linear scaling for this purpose, e.g., KD-Tree, Ball-Tree, etc.
Here we adopt the approximate nearest neighbor (ANN) searching algo- rithm which is scalable to extremely large scale and high dimensional data(Muja & Lowe, 2014).
The resulted linear system is sparse and positive deﬁnite which is efﬁciently solved by the conjugate gradient method in this work.
It is worth emphasizing that in order to guarantee the WNLL interpolation is suitable to represent all classes of instances, the labeled instances should be at least around N ln N with N be the number of classes.
The most important component of our algorithm is to put the WNLL activation layer into the DNNs, and design ef- ﬁcient algorithms for information feed-forward propaga- tion and error back-propagation.
Our information and error propagation paths are demonstrated in Fig.2. The standard DNN, e.g., VGG, ResNet, is plotted in Fig.2 (a), the ’DNN’ block represents all the layers except the last softmax ac- tivation function.
A naive approach to place the WNLL into the DNN is to simply replace the softmax function by WNLL.
However, in this case though the information can be feed-forwarded, the error cannot be back-propagated, since WNLL implicitly deﬁnes an activation function on the learned representation, and the gradient is not explic- itly available.
To efﬁciently train the network with WNLL activation, we introduce a new structure inherited from the standard DNN as depicted in chart (c) of Fig.2. Our structure is quite ﬂexible which can be inherited from any existing DNN.
We equip two new blocks to the standard DNN, a buffer block and a WNLL activation function, where the buffer block is simply chosen to be a composition of a fully connected layer which preserves the dimension of the input tensor followed by a ReLU function(Nair & Hinton, 2010).
The buffer block can be made more complicated.
After the buffer block, the tensor is passed into two activations, a linear activation and a WNLL function, in parallel.
Our training algorithm for the network in Fig.2(c) is an iterative procedure among the following three steps: • Step 1.
Train the network with only the linear activa- tion functions to steady state.
For this purpose, we do not feed the data to the WNLL activation.
• Step 2.
Run a few training epochs on the network which we freeze the “DNN” and ”Linear Activation” blocks, and only ﬁne tune the ’Buffer Block’.
In order to back-propagate the error between the ground-truth and the WNLL interpolated results, we feed the data into the pre-trained linear activation function, and use the corresponding computational graph to perform er- ror back-propagation.
• Step 3.
Unfreeze the entire network, and train the network with data only feeding to the linear activation to the steady state again.
With the trained network, during the generalization step, we use the WNLL activation to get the ﬁnal inference.
Our algorithm is designed in a greedy fashion.
The following numerical results validate the efﬁciency of our training al- gorithm and the superiority of our network structure.
The training and testing procedures of our proposed network are summarized in Algorithms 1 and 2, respectively.
Remark 1.
During the back-propagation, we use the com- putational graph of the linear function to approximate that of the WNLL function since the linear functions are the simplest nontrivial harmonic functions.
Mixed Gaussian seems to be a more appealing approximation, since it is com- patible with the WNLL.
We will continue to explore better approximations in our subsequent work.
5.
Numerical Results To validate the accuracy, efﬁciency, and robustness of the proposed model, we present the numerical results of dif- ferent tests on the CIFAR10 and CIFAR100 (Krizhevsky, 2009), MNIST(LeCun, 1998) and SVHN dataset(Netzer et al., 2011).
It is generally believe that the difﬁculty of these datasets are ranked as CIFAR100 followed by CI- FAR10, then SVHN, and the easiest one is MNIST.
In all of our numerical experiments, we take the standard data aug- mentation that is widely used for both CIFAR datasets (He et al., 2016a; Huang et al., 2017; Zagoruyko & Komodakis, 2016).
For MNIST and SVHN, we use the raw data without any data augmentation.
In order to use the computational graph of the linear function to approximate that of WNLL, we need the dynamical computational graph.
For this pur- pose, we implement our algorithm on the PyTorch platform (Paszke et al., 2017), where automatic differentiation is used Deep Learning with Data Dependent Implicit Activation Function (a) (b) (c) Figure 2.
Illustration of the DNN architectures.
Panel (a) depicts the standard DNN, where the “DNN” block represents the layers except the last activation layer in the network.
Panel (b) plots the standard network with the last layer replaced by the WNLL layer.
Panel (c) shows the network structure used in our work, detailed explanation is presented in the paper.
instead of the symbolic computation used in TensorFlow (Abadi et al., 2016) and many other systems.
All compu- tations are done on a machine using only a single Nvidia Titan Xp graphics card.
Before diving into the performance of the DNNs, we ﬁrst compare the performance of WNLL interpolation and other shallow classiﬁers on different datasets.
Table 1 lists the performance of k-nearest neighbors (KNN) (the optimal k is listed in the table), support vector machine (SVM) with RBF kernel, softmax regression and WNLL interpolation function.
For the WNLL interpolation, in order to speed up the computation, we only keep 15 nearest neighbors, and the 8th neighbor’s distance is used to normalize the weight matrix.
Both KNN and WNLL can be regarded as nonpara- metric approaches.
WNLL outperforms the other methods except SVM.
KNN, in general, is better than softmax regres- sion which demonstrates the importance of the manifold structure of the data.
These results show the potential of using WNLL instead of softmax as the activation function in the DNNs. Table 1.
Accuracy of simple classiﬁers over different datasets SVM Softmax WNLL 57.14% 39.91% 40.73% 97.79% 92.65% 97.74% 70.45% 24.66% 56.17% 32.77% (k=5) 96.40% (k=1) 41.47% (k=1) KNN Dataset Cifar10 MNIST SVHN We run 400 epochs when training the vanilla DNN, i.e., standard DNN, with the initial learning rate being 0.05 and halved after every 50 epochs for Cifar10 and Cifar100 datasets.
We also train 5 epochs of the WNLL DNN, i.e., WNLL activated DNN.
Since at this stage, the DNN is al- ready well trained, we use a smaller learning rate 0.0005.
For the Cifar datasets, these hyper-parameters are chosen based on cross validation.
We keep alternating the above three steps with the learning rate being one ﬁfth of that in the previous stages.
The batch size in training the vanilla DNN is set to 128 for all experiments.
In the SVHN experiments, we use the same hyperparameters as reported in (Huang et al., 2016).
The batch size is set to 2000 when training the WNLL DNN.
By Theorem 1, this number is big enough to sample all types of instances for even CIFAR100 with 100 classes of images.
All the optimizations are carried out by a simple SGD solver with the default Nesterov momentum acceleration.
5.1. Resolving the Challenge of the Lack of Training Data When we do not have sufﬁcient training data, the generaliza- tion accuracy typically decays as the network goes deeper.
This phenomenon is illustrated in Fig.3. The left and right panels plot the cases when the ﬁrst 1000 and 10000 data in the training set of CIFAR10 are involved in training the vanilla and WNLL DNNs. We believe the increase of the generalization error is due to the data not being sufﬁcient to parametrize the deep networks.
With suitable regularization techniques, the deep networks can be better parametrized by a small amount of training data.
WNLL activation which involves the information of the data’s geometric structures is one of such regularizers.
With the WNLL activation, the generalization error rate decays persistently as the net- work goes deeper.
The generalization accuracy between the vanilla and WNLL DNN can differ up to 10 percent within our testing regime.
Even though we have only built the connection between the deep ResNets and the PDE control problems, we also test the performance of our surgeries and algorithms on other base DNNs, e.g., VGG networks.
In table 2 we list the generalization error rates of 15 different DNNs from VGG, ResNet, Pre-activated ResNet families on the entire, ﬁrst 10000, 5000, and 1000 instances in the CIFAR10 training set.
It is easy to see that WNLL activated DNNs typically has much more accuracy improvement for ResNets or pre- DNNX(x;Θ)OutputSoftmaxDNNX(x;Θ)WNLLOutputDNNX(x;Θ)BuﬀerBlockFC(linearapprox.)WNLLOutputDeep Learning with Data Dependent Implicit Activation Function Algorithm 1 DNN with Data Dependent Activation: Training Procedure.
Input: Training set (T, lT ), where T is the set of features, lT is the label set.
Output: An optimized DNN with our surgeries, denoted as DNNs. for iter = 1,.
.
.
, N (where we let N = 2 for the entire data, N = 5 for the selected subset of the training set.
do Train the DNN with linear activation starting from the previous iteration.
In the ﬁrst step use the default initialized one.
Denote the temporary model as DNN1.
Split T into training and validation parts, T the model.
Partition T T and T V into nbatch1 and nbatch2 mini-batches, denoted as {T T nbatch1 and nbatch2 are given integers.
for i = 1, 2,··· , nbatch2 do = T T(cid:83) T V .
We will treat instances in T V as unlabeled data when training i }nbatch2 i }nbatch1 and {T V , where i=1 i=1 for j = 1, 2,··· , nbatch1 do Apply the model DNN1 to T T Apply the WNLL interpolation to ˆT T (cid:83) T V (cid:83) ˆT V to get ˆT T i = DNN1(T T i by solving the linear system (cid:83) T V i ).
(cid:83) ˆT V (cid:83) T V i | |T V i | |T T (cid:88) xn∈T V (cid:88) (cid:83) T V xn∈T T (wmn + wnm)(um − un) + wnm(um − ln) = 0, ∀xm ∈ T T (cid:91) T V i .
to obtain the inferred label uj T V T V j=1 }nbatch1 end for Voting on {uj Back-propagate the loss(lT V ) by using the computational graph of the linear activation function, and update DNN1.
For generally classiﬁcation tasks, the loss is selected to be cross entropy between the exact and predicted labels.
end for to get the inferred label uT V , uT V end for Table 2.
Generalization error rate over the test set of the vanilla DNNs and the WNLL activated ones trained over the entire, the ﬁrst 10000, 5000, and 1000 instances of the training set of CIFAR10.
(Median of 5 independent trials) Network Whole 10000 5000 1000 VGG11 VGG13 VGG16 VGG19 ResNet20 ResNet32 ResNet44 ResNet56 ResNet110 ResNet18 ResNet34 ResNet50 PreActResNet18 PreActResNet34 PreActResNet50 9.12% 9.01% 9.62% Vanilla WNLL Vanilla WNLL Vanilla WNLL Vanilla WNLL 9.23% 7.35% 10.37% 8.88% 12.36% 10.49% 26.75% 24.10% 7.64% 10.89% 9.02% 24.85% 22.56% 6.66% 5.58% 7.54% 11.25% 9.13% 25.41% 22.23% 6.72% 5.69% 6.95% 5.92% 8.09% 11.76% 9.22% 25.70% 22.87% 9.06% 7.09% 12.83% 9.96% 14.30% 11.24% 34.90% 29.91% 7.99% 5.95% 11.18% 8.15% 12.75% 10.63% 33.41% 28.78% 7.31% 5.70% 10.66% 7.96% 11.84% 10.14% 34.58% 27.94% 7.61% 12.39% 10.17% 37.83% 28.18% 7.24% 5.61% 7.13% 13.45% 10.05% 42.94% 28.29% 6.41% 4.98% 6.16% 4.65% 6.29% 10.38% 8.53% 27.02% 22.48% 6.11% 10.75% 8.65% 26.47% 20.27% 5.93% 4.26% 6.49% 12.96% 8.76% 29.69% 20.19% 6.24% 4.17% 6.61% 10.64% 8.18% 27.36% 21.88% 6.21% 4.74% 6.08% 4.40% 6.34% 10.85% 8.44% 23.56% 19.02% 6.05% 10.64% 8.35% 25.05% 18.61% 6.05% 4.27% 9.83% 8.91% 8.26% 8.31% 9.64% 8.20% 8.52% 9.18% activated ResNets.
Signiﬁcant accuracy improvement can still be observed when our surgeries and training algorithms are applied to the VGG networks.
Except for VGG networks, we can achieve relatively 20% to 30% testing error rate reduction.
All the results presented here and in the rest of this paper are the median of 5 independent trials to reduce Deep Learning with Data Dependent Implicit Activation Function Algorithm 2 DNN with Data Dependent Activation: Testing Procedure.
Input: Testing set W and training set (T, lT ), where W and T are features, lT are the labels of the instances in T .
And the trained DNN with our surgeries, denoted as DNNs. Output: The predicted labels for the test set W .
Partition T and W into nbatch1 and nbatch2 number of mini-batches, denoted as {Ti}nbatch1 nbatch1 and nbatch2 are given integers.
for i = 1, 2,··· , nbatch2 do and {Wi}nbatch2 , where i=1 i=1 for j = 1, 2,··· , nbatch1 do Apply the model DNNs to Tj Apply the WNLL interpolation to ˆTj (cid:88) (cid:83) Wi xn∈Tj (cid:83) Wi to get ˆTj (cid:83) Wi) (cid:83) ˆWi by solving the linear system (cid:83) ˆWi = DNNs(Tj (cid:88) (cid:83) Wi| |Tj |Wi| xn∈Wi (wmn + wnm)(um − un) + wnm(um − ln) = 0 ∀xm ∈ Tj (cid:91) Wi. to obtain the inferred label uj Wi end for Voting on {uj Output the predicted label uW =(cid:83)nbatch2 }nbatch1 j=1 Wi i=1 to get the inferred label uWi. uWi. end for ter around 300 epochs, the accuracies of the vanilla DNNs plateaued and cannot improve any more.
However, in stage 2, once we use the WNLL activation, there is a jump in the generalization accuracy; during stage 3, even though initially there is an accuracy reduction, with the training con- tinuing, the accuracy keeps climbing for a while.
The gener- alization accuracy increases if ﬁnally we use the WNLL as the activation function.
(a) (c) (b) (d) Figure 4.
The evolution of the generation accuracy over the training procedure.
Charts (a) and (b) are the accuracy plots for ResNet50 with 1000 training data, where (a) and (b) are plots for the epoch v.s. accuracy of the vanilla and the WNLL activated DNN.
Pan- els (c) and (d) correspond to the case of 10000 training data for PreActResNet50.
All tests are done on the Cifar10 dataset.
(a) (b) Figure 3.
Taming of the degeneration problem of vanilla DNN by WNLL activated DNN.
Panels (a) and (b) plot the generation error when 1000 and 10000 training data are used to train the vanilla and the WNLL activated DNN, respectively.
In each plot, we test three different networks: PreActResNet18, PreActResNet34, and PreActResNet50.
It is easy to see that when the vanilla network becomes deeper, the generation error does not decayed, while WNLL activation resolves this degeneracy.
All tests are done on the Cifar10 dataset.
the inﬂuence of stochasticity.
5.2. Error Rate Reduction in Base DNNs We next present the superiority of our deep network in terms of the generalization accuracy when compared to its base network.
Figure.
4 plots the generalization accuracy evolu- tion during the training procedure.
Panels (a) and (b) plot the cases for the ResNet50 and WNLL activated ResNet50 when only the ﬁrst 1000 CIFAR10 training data are utilized.
Charts (c) and (d) are for the cases when the ﬁrst 10000 CIFAR10 training instances are used to train the vanilla pre-activated ResNet50 and WNLL activated version.
Af- Deep Learning with Data Dependent Implicit Activation Function For the street view house number recognition (SVHN) task, we simply test the performance when the full training data are used.
Here we only test the performance of the ResNets and pre-activated ResNets.
There is a relatively 7%-10% error rate reduction for all these DNNs. There is rela- tively more improvement on pre-activated ResNets than on ResNets, this is consistent with our basic PDE control problem ansatz.
Table 3.
Error rate of the vanilla DNN v.s. the WNLL activated DNN over the whole SVHN dataset.
(Median of 5 independent trials) Network ResNet20 ResNet32 ResNet44 ResNet56 ResNet110 ResNet18 ResNet34 PreActResNet18 PreActResNet34 Vanilla DNN WNLL DNN 3.76% 3.28% 2.84% 2.64% 2.55% 3.96% 3.81% 4.03% 3.66% 3.44% 2.96% 2.56% 2.32% 2.26% 3.65% 3.54% 3.70% 3.32% Tables 2 and 4 list the error rate of 15 different vanilla net- works and the WNLL activated networks.
On CIFAR10, the WNLL activated DNNs outperformed the vanilla ones with around 1.5% to 2.0% absolute, or 20% to 30% relative error rate reduction.
We reproduced the results of the vanilla DNNs on both datasets.
Our results are consistent with the original reports and other researchers’ reproductions (He et al., 2016a;b; Huang et al., 2017).
Interestingly, when the task become harder, our improvement becomes more signiﬁcant.
This builds our conﬁdent in trying harder tasks in the future.
Reducing the sizes of DNN models is an important direction to make the DNN applicable for gen- eralize purpose, e.g., auto-drive, mobile intelligence, etc.
So far the most successful attempt is DNN weights quan- tization(Bengio & David, 2015).
Our approach is a new direction for reducing the size of the model: to achieve the same level of accuracy, compared to the vanilla networks, our model’s size can be tens of times smaller.
6.
Concluding Remarks Motivated by the connection between deep ResNets and PDE control problems, we propose a novel DNN struc- ture which can be inherited from any existing DNN.
An end-to-end greedy styled, multi-staged training algorithm is proposed to train the novel networks.
In order to efﬁciently back propagate the errors, we utilized the computational graph of a linear function dynamically to approximate that Table 4.
Error rate of the vanilla DNN v.s. the WNLL activated DNN over the whole CIFAR100 dataset.
(Median of 5 independent trials) Network VGG11 VGG13 VGG16 VGG19 ResNet20 ResNet32 ResNet44 ResNet56 ResNet110 ResNet18 ResNet34 ResNet50 PreActResNet18 PreActResNet34 PreActResNet50 Vanilla DNN WNLL DNN 32.68% 29.03% 28.59% 28.55% 35.79% 32.01% 31.07% 30.03% 28.86% 27.57% 25.55% 25.09% 28.62% 26.84% 25.95% 28.80% 25.21% 25.72% 25.07% 31.53% 28.04% 26.32% 25.36% 23.74% 22.89% 20.78% 20.45% 23.45% 21.97% 21.51% for the manifold interpolation function.
On one hand, our new framework resolves the issue of the lack of big training data, on the other hand, it provides great accuracy improve- ment compared to the base DNNs. This improvement is consistent for networks with different depths.
Utilizing our structure, it is very easy to get near state-of-the-art results with very small model, which has great potential for the mobile device applications.
Nevertheless, there are many di- rections for improvement: the current manifold interpolation is still one of the computational bottlenecks, according to the representability theorem for the data with many classes.
For instance, the batch size need to be very large for the ImageNet dataset (J.
et al., 2009), which poses memory challenges.
Another important issue is the approximation of the gradient of the WNLL activation function.
Linear function is an option but it is far from optimal.
We believe a better harmonic function approximation can further lift the model’s performance.
7.
Acknowledgement This material is based, in part, upon work supported by the U.S. Department of Energy, Ofﬁce of Science and by National Science Foundation, and National Science Foun- dation of China, under Grant Numbers doe-sc0013838 and DMS-1554564, (STROBE), NSFC 11671005.
Deep Learning with Data Dependent Implicit Activation Function LeCun, Yann.
The mnist database of handwritten digits.
1998.
Lecun, Yann, Bengio, Yoshua, and Hinton, Geoffrey.
Deep learning.
Nature, 521:436–444, 2015.
Muja, Marius and Lowe, David G.
Scalable nearest neighbor algorithms for high dimensional data.
Pattern Analysis and Machine Intelligence (PAMI), 36, 2014.
Nair, Vinod and Hinton, Geoffrey.
Rectiﬁed linear units improve restricted boltzmann machines.
ICML, 2010.
Netzer, Yuval, Wang, Tao, Coates, Adam, Bissacco, Alessan- dro, Wu, Bo, and Ng, Andrew Y.
Reading digits in nat- ural images with unsupervised features learning.
NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011.
Paszke, Adam, Gross, Sam, Chintala, Soumith, Chanan, Gregory, Yang, Edward, DeVito, Zachary, Lin, Zeming, Desmaison, Alban, Antiga, Luca, and Lerer, Adam.
Au- tomatic differentiation in pytorch.
2017.
Shi, Z., Osher, S., and Zhu, W.
Weighted nonlocal Laplacian on interpolation from sparse data.
Journal of Scientiﬁc Computing, pp.
to appear, 2017.
Simonyan, Karen and Zisserman, Andrew.
Very deep con- volutional networks for large-scale image recognition.
Arxiv:1409.1556, 2014.
Srivastava, N., Hinton, G.
E., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R.
Dropout: a simple way to prevent neural networks from overﬁtting.
Journal of Machine Learning Research, 15(1):1929–1958, 2014.
Wen, Yandong, Zhang, Kaipeng, Li, Zhifeng, and Qian, Yu. A discriminative feature learning approach for deep face recognition.
ECCV, 2016.
Zagoruyko, S.
and Komodakis, N.
Wide residual networks.
BMVC, 2016.
Zhu, Wei, Qiu, Qiang, Huang, Jiaji, Carderbank, Robert, Sapiro, Guillermo, and Daubechies, Ingrid.
Low dimen- sional manifold regularized neural networks.
UCLA CAM Report: 17-66, 2017.
References Abadi, M., Agarwal, A., and et al.
Tensorﬂow: Large-scale machine learning on heterogeneous distributed systems.
ArXiv:1603.04467, 2016.
Bengio, M.
Courbariaux Y.
and David, J.
Binaryconnet: Training deep neural networks with binary weights.
NIPS, 2015.
Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H.
Greedy layer-wise training of deep networks.
NIPS, 2007.
Bishop, C.
M.
Pattern recognition and machine learning.
Springer, 2006.
Chen, Yunpeng, Li, Jianan, Xiao, Huaxin, Jin, Xiaojie, Yan, Shuicheng, and Feng, Jiashi.
Dual path networks.
NIPS, 2017.
E, Weinan.
A proposal on machine learning via dynamical systems.
Communications in Mathematics and Statistics, 5(1):1–11, 2017.
Evans, L.C. Partial differential equations.
American Mathe- matical Soc, 2010.
Hardt, Moritz and Ma, Teng Yu. Identity matters in deep learning.
ICLR, 2017.
He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian.
Deep residual learning for image recognition.
CVPR, 2016a.
He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian.
Identity mappings in deep residual networks.
ECCV, 2016b.
Hinton, G.
E., Osindero, S., and Teh, T.
W.
A fast learning algorithm for deep belief nets.
Neural Computation, 18 (7):1527–1554, 2006.
Huang, Gao, Sun, Yu, Liu, Zhuang, Sedra, Daniel, and WeinBerger, Kilian.
Deep networks with stochastic depth.
ECCV, 2016.
Huang, Gao, Liu, Zhuang, Weinberger, K.
Q., and van der Maaten, Laurens.
Densely connected convolutional net- works.
CVPR, 2017.
J., Deng, W., Dong, R., Socher, L.-J., Li, K., Li, and L., Fei Fei.
ImageNet: A Large-Scale Hierarchical Image Database.
In CVPR09, 2009.
Krizhevsky, Alex.
Learning multiple layers of features from tiny images.
2009.
Langley, P.
Crafting papers on machine learning.
In Langley, Pat (ed.), Proceedings of the 17th International Confer- ence on Machine Learning (ICML 2000), pp.
1207–1216, Stanford, CA, 2000.
Morgan Kaufmann.

A wide variety of problems can be reduced to computing the sum of (many) non-negative numbers.
These include calculat- ing the partition function of a graphical model, propositional model counting (#SAT), and calculating the permanent of a non-negative matrix.
Equivalently, each can be viewed as computing the discrete integral of a non-negative weight func- tion.
Exact summation, however, is generally intractable due to the curse of dimensionality (Bellman 1961).
As alternatives to exact computation, variational methods (Jordan et al.
1998; Wainwright, Jordan, and others 2008) and sampling (Jerrum and Sinclair 1996; Madras 2002) are popular approaches for approximate summation.
However, they generally do not guarantee the estimate’s quality.
An emerging line of work estimates and formally bounds propositional model counts or, more generally, discrete in- tegrals (Ermon et al.
2013a; Chakraborty, Meel, and Vardi 2013; Ermon et al.
2014; Zhao et al.
2016).
These approaches reduce the problem of integration to solving a small number of optimization problems involving the same weight function but subject to additional random constraints introduced by a random hash function.
This results in approximating the Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org).
All rights reserved.
#P-hard problem of exact summation (Valiant 1979) using the solutions of NP-hard optimization problems.
Optimization can be performed efﬁciently for certain classes of weight functions, such as those involved in the computation of the permanent of a non-negative matrix.
If instead of summing (permanent computation) we max- imize the same weight function, we obtain a maximum weight matching problem, which is in fact solvable in poly- nomial time (Kuhn 1955).
However, adding hash-based con- straints makes the maximum matching optimization prob- lem intractable, which limits the application of random- ized hashing approaches (Ermon et al.
2013c).
On the other hand, there do exist fully polynomial-time random- ized approximation schemes (FPRAS) for non-negative per- manent computation (Jerrum, Sinclair, and Vigoda 2004; Bezáková et al.
2006).
This gives hope that approximation schemes may exist for other counting problems even when optimization with hash-based constraints is intractable.
We present a new method for approximating and bound- ing the size of a general weighted set (i.e., the sum of the weights of its elements) using geometric arguments based on the set’s shape.
Our approach, rather than relying on hash-based techniques, establishes a novel connection with Rademacher complexity (Shalev-Shwartz and Ben-David 2014).
This generalizes geometric approaches developed for the unweighted case to the weighted setting, such as the work of Barvinok (1997) who uses similar reasoning but with- out connecting it with Rademacher complexity.
In particular, we ﬁrst generalize Rademacher complexity to weighted sets.
While Rademacher complexity is deﬁned as the maximum of the sum of Rademacher variables over a set, weighted Rademacher complexity also accounts for the weight of each element in the set.
Just like Rademacher complexity is related to the size of the set, we show that weighted Rademacher complexity is related to the total weight of the set.
Further, it can be estimated by solving multiple instances of a maximum weight optimization problem, subject to random Rademacher perturbations.
Notably, the resulting optimization problem turns out to be computationally much simpler than that re- quired by the aforementioned randomized hashing schemes.
In particular, if the weight function is log-supermodular, the corresponding weighted Rademacher complexity can be estimated efﬁciently, as our perturbation does not change the original optimization problem’s complexity (Orlin 2009; Bach and others 2013).
Our approach most closely resembles a recent line of work involving the Gumbel distribution (Hazan and Jaakkola 2012; Hazan, Maji, and Jaakkola 2013; Hazan et al.
2016; Balog et al.
2017; Mussmann and Ermon 2016; Mussmann, Levy, and Ermon 2017).
There, the Gumbel-max idea is used to bound the partition function by performing MAP inference on a model where the unnormalized probability of each state is perturbed by random noise variables sampled from a Gumbel distribution.
While very powerful, exact application of the Gumbel method is impractical, as it requires exponentially many independent random perturbations.
One instead uses local approximations of the technique.
Empirically, on spin glass models we show that our tech- nique yields tighter upper bounds and similar lower bounds compared with the Gumbel method, given similar computa- tional resources.
On a suite of #SAT model counting instances our approach generally produces comparable or tighter upper and lower bounds given limited computation.
Background Rademacher complexity is an important tool used in learn- ing theory to bound the generalization error of a hypothesis class (Shalev-Shwartz and Ben-David 2014).
Deﬁnition 1.
The Rademacher complexity of a set A ⊆ Rn is deﬁned as: R(A) := Ec sup a∈A ciai (1) where Ec denotes expectation over c, and c is sampled uni- formly from {−1, 1}n.
As the name suggests, it is a measure of the complexity of set A (which, in learning theory, is usually a hypothesis class).
It measures how “expressive” A is by evaluating how well we can “ﬁt” to a random noise vector c by choosing the closest vector (or hypothesis) from A.
Intuitively, Rademacher com- plexity is related to |A|, the number of vectors in A, another crude notion of complexity of A.
However, it also depends on how vectors in A are arranged in the ambient space Rn. A central focus of this paper will be establishing quantitative relationships between R(A) and |A|.
A key property of Rademacher complexity that makes it extremely useful in learning theory is that it can be estimated using a small number of random noise samples c under mild conditions (Shalev-Shwartz and Ben-David 2014).
The result follows from McDiarmid’s inequality: Proposition 1 (McDiarmid, 1989).
Let X1, ..., Xm ∈ X be independent random variables.
Let f : X m (cid:55)→ R be a function that satisﬁes the bounded differences condition that ∀i ∈ {1, ..., m} and ∀x1, ..., xm, x(cid:48) i ∈ X : |f (x1, ..., xi, ..., xm) − f (x1, ..., x(cid:48) (cid:35) (cid:34) n(cid:88) i=1 Then for all  > 0 (cid:104)(cid:12)(cid:12)f (X1, ..., Xm) − E(cid:2)f (X1, ..., Xm)(cid:3)(cid:12)(cid:12) ≥  Pr i, ..., xm)| ≤ di.
(cid:32) −22(cid:80) (cid:105) ≤ exp (cid:33) j d2 McDiarmid’s inequality says we can bound, with high probability, how far a function f of random variables may deviate from its expected value, given that the function does not change much when the value of a single random variable is changed.
Because the function in Eq. (1) satisﬁes this property (Shalev-Shwartz and Ben-David 2014), we can use Eq. (1) to bound R(A) with high probability by computing the supremum for only a small number of noise samples c.
Problem Setup In this section we formally deﬁne our problem and intro- duce the optimization oracle central to our solution.
Let w : {−1, 1}n → [0,∞) be a non-negative weight function.
We consider the problem of computing the sum (cid:88) Z(w) = w(x).
x∈{−1,1}n Many problems, including computing the partition func- tion of an undirected graphical model, where w(x) is the unnormalized probability of state x (see Koller and Fried- man (2009)), propositional model counting (#SAT), and com- puting the permanent of a non-negative matrix can be reduced to calculating this sum.
The problem is challenging because explicit calculation requires summing over 2n states, which is computationally intractable in cases of interest.
Due to the general intractability of exactly calculating Z(w), we focus on an efﬁcient approach for estimating Z(w) which additionally provides upper and lower bounds that hold with high probability.
Our method depends on the following assumption: Assumption 1.
We assume existence of an optimization ora- cle that can output the value {(cid:104)c, x(cid:105) + log w(x)} x∈{−1,1}n δ(c, w) = max (2) for any vector c ∈ {−1, 1}n and weight function w : {−1, 1}n → [0,∞).
Note that throughout the paper we simply denote log2 as log, loge as ln, and assume log 0 = −∞.
Assump- tion 1 is reasonable, as there are many classes of models where such an oracle exists.
For instance, polynomial time algorithms exist for ﬁnding the maximum weight match- ing in a weighted bipartite graph (Hopcroft and Karp 1971; Jonker and Volgenant 1987).
Graph cut algorithms can be ap- plied to efﬁciently maximize a class of energy functions (Kol- mogorov and Zabin 2004).
More generally, MAP inference can be performed efﬁciently for any log-supermodular weight function (Orlin 2009; Chakrabarty, Jain, and Kothari 2014; Fujishige 1980).
Our perturbation preserves the submodular- ity of − log w(x), as (cid:104)c, x(cid:105) can be viewed as n independent single variable perturbations, so we have an efﬁcient opti- mization oracle whenever the original weight function is log-supermodular.
Further, notice that this is a much weaker assumption compared with the optimization oracle required by randomized hashing methods (Chakraborty, Meel, and Vardi 2013; Ermon et al.
2014; Zhao et al.
2016).
If an approximate optimization oracle exists that can ﬁnd a value within some known bound of the maximum, we can modify our bounds to use the approximate oracle.
This may improve the efﬁciency of our algorithm or extend its use to additional problem classes.
For the class of log-supermodular distributions, approximate MAP inference is equivalent to performing approximate submodular minimization (Jegelka, Lin, and Bilmes 2011).
We note that even when an efﬁcient optimization oracle exists, the problem of exactly calculating Z(w) is gener- ally still hard.
For example, polynomial time algorithms ex- ist for ﬁnding the maximum weight perfect matching in a weighted bipartite graph.
However, computing the permanent of a bipartite graph’s adjacency matrix, which equals the sum of weights for all perfect matchings or Z(w), is still #P- complete(Jerrum, Sinclair, and Vigoda 2004).
A fully poly- nomial randomized approximation scheme (FPRAS) exists (Jerrum, Sinclair, and Vigoda 2004; Bezáková et al.
2006), based on Markov chain Monte Carlo to sample over all per- fect matchings.
However, the polynomial time complexity of this algorithm suffers from a large degree, limiting its practical use.
Our approach for estimating the sum Z(w) = (cid:80) Weighted Rademacher Bounds on Z(w) x w(x) is based on the idea that the Rademacher complexity of a set is related to the set’s size.
In particular, Rademacher complexity is monotonic in the sense that R(A) ≤ R(B) whenever A ⊆ B.
Note that monotonicity does not hold for |A| ≤ |B|, that is, R(A) is monotonic in the contents of A but not necessarily in its size.
We estimate the sum of arbi- trary non-negative elements by generalizing the Rademacher complexity in deﬁnition 2.
Deﬁnition 2.
We deﬁne the weighted Rademacher complexity of a weight function w : {−1, 1}n → [0,∞) as R(w) := Ec max x∈{−1,1}n {(cid:104)c, x(cid:105) + log w(x)} (3) for c sampled uniformly from {−1, 1}n.
In the notation of Eq. (2), the weighted Rademacher com- plexity is simply R(w) = Ec[δ(c, w)].
For a set A ⊆ {−1, 1}n, let IA : {−1, 1}n → {0, 1} denote the indicator weight function for A, deﬁned as IA(x) = 1 ⇐⇒ x ∈ A.
Then R(IA) = R(A), that is, the weighted Rademacher com- plexity is identical to the standard Rademacher complexity for indicator weight functions.
For a general weight function, the weighted Rademacher complexity extends the standard Rademacher complexity by giving each element (hypothesis) its own weight.
Algorithmic Strategy The key idea of this paper is to use the weighted Rademacher complexity R(w) to provide probabilistic estimates of Z(w), the total weight of w.
This is a reasonable strategy because as we have seen before, for an indicator weight function IA : {−1, 1}n → {0, 1}, R(IA) reduces to the standard Rademacher complex- ity R(A), and Z(IA) = |A| is simply the cardinality of the set.
Therefore we can use known quantitative relation- ships between R(A) and |A| from learning theory to estimate (cid:20) (cid:21) k(cid:88) i=1 |A| = Z(IA) in terms of R(A) = R(IA).
Although not for- mulated in the framework of Rademacher complexity, this is the strategy used by Barvinok (1997).
Here, we generalize these results to general weight func- tions w and show that it is, in fact, possible to use R(w) to obtain estimates of Z(w).
This observation can be turned into an algorithm by observing that R(w) is the expectation of a random variable concentrated around its mean.
Therefore, as we will show in Proposition 2, a small number of samples suf- ﬁces to reliably estimate R(w) (and hence, Z(w)) with high probability.
Whenever w is ‘sufﬁciently nice’ and we have access to an optimization oracle, the estimation algorithm is efﬁcient.
Algorithm 1 Rademacher Estimate of log Z(w) Inputs: A positive integer k and weight function w : {−1, 1}n → [0,∞).
Output: A number ¯δk(w) which approximates log Z(w) = log (cid:16)(cid:80) (cid:17) x∈{−1,1}n w(x) 1.
Sample k vectors c1, c2, .
.
.
, ck independently and uni- formly from {−1, 1}n.
2.
Apply the optimization oracle of assumption 1 to each vector c and compute the mean ¯δk(w) = max x∈{−1,1}n {(cid:104)ci, x(cid:105) + log w(x)}.
3.
Output ¯δk(w) as an estimator of R(w) and thus log Z(w).
Bounding Weighted Rademacher Complexity The weighted Rademacher complexity is an expectation over optimization problems.
The optimization problem is deﬁned by sampling a vector, or direction since all have length n, uniformly from {−1, 1}n and ﬁnding the vector x that is most aligned (largest dot product) after adding log w(x).
Our ﬁrst objective is to derive bounds on the weighted Rademacher complexity in terms of the sum Z(w).
We begin with the observation that it is impossible to de- rive bounds on the Rademacher complexity in terms of set size that are tight for sets of all shapes.
To gain intuition, note that in high dimensional spaces the dot product of any par- ticular vector and another chosen uniformly at random from {−1, 1}n is close to 0 with high probability.
The distribution of weight vectors throughout the space may take any geomet- ric form.
One extreme conﬁguration is that all vectors with large weights are packed tightly together, forming a Ham- ming ball.
At the other extreme, all vectors with large weights could be distributed uniformly through the space.
As Figure 1 illustrates, a large set of tightly packed vectors and a small set of well-distributed vectors will both have similar Rademacher complexity.
Thus, bounds on Rademacher complexity that are based on the underlying set’s size fundamentally cannot always be tight for all distributions.
Nevertheless, the lower and upper bounds we derive next are tight enough to be useful in practice.
Lemma 2.
For any λ > 0, γ > 0, and weight functions w, wγ : {−1, 1}n → [0,∞) with wγ(x) = w(x)γ, the weighted Rademacher complexity of wγ is upper bounded by R(wγ) ≤ 1 (cid:40) with log w∗(λ, γ) + λ λγ − 1 log Z(w) + (4) wmax = maxx w(x), wmin = minx{w(x) : w(x) > 0}, (λ, γ) = Note that for an indicator weight function we recover the if λγ ≥ 1 if λγ ≤ 1 (cid:113) 2 log Z(w) bound from Massart’s Lemma by setting λ = and γ = 1.
Corollary 2.1. For sufﬁciently large γ and (cid:115) λ = 2 log Z(w) wmax we recover the bound wmax ≤ Z(w) from Lemma 2.
Lemma 2 holds for any λ > 0 and γ > 0.
In general we set γ = 1 and optimize over λ to make the bound as tight as possible, comparing the result with the trivial bound given by Corollary 2.1. More sophisticated optimization strategies over λ and γ could result in a tighter bound.
Please see the appendix for further details and proofs.
Bounding the Weighted Sum Z(w) With our bounds on the weighted Rademacher complexity from the previous section, we now present our method for efﬁciently bounding the sum Z(w).
Proposition 2 states that we can estimate the weighted Rademacher complexity using the optimization oracle of assumption 1.
Proposition 2.
For c ∈ {−1, 1}n sampled uniformly at ran- dom, the bound R(w) − 6n ≤ δ(c, w) ≤ R(w) + 6n (5) holds with probability greater than .95.
Proof.
By applying Proposition 1 to the function fw(c) = δ(c, w), and noting the constant di = 2, we have (cid:104)|δ(c, w) − R(w)| ≥ (cid:105) ≤ e−3 ≤ .05.
6n This ﬁnishes the proof.
To bound Z(w) we use our optimization oracle to solve a perturbed optimization problem, giving an estimate of the weighted Rademacher complexity, R(w).
Next we invert the bounds on R(w) (Lemmas 1 and 2) to obtain bounds on Z(w).
We optimize the parameters λ and β (from equations 1 and 2) to make the bounds as tight as possible.
By applying our optimization oracle repeatedly, we can reduce the slack introduced in our ﬁnal bound when estimating R(w) (by Lemma 2) and arrive at our bounds on the sum Z(w), stated (cid:80) in the following theorem.
Theorem 1.
With probability at least 0.95, the sum Z(w) = x∈{−1,1}n w(x) of any weight function w : {−1, 1}n → [0,∞) is bounded by the outputs of algorithms 2 and 3 as ψLB < log Z(w) < ψU B.
Figure 1: Illustration mapping a set of vectors in high di- mensional space {−1, 1}n to the unit circle.
Red regions correspond to regions of space that have a large dot prod- uct with some vector in the set.
Left: when the size of a set is small, very few regions have a large dot product with any vector in the set, so the Rademacher complexity will be small.
Right: when a large set of vectors is tightly packed in a small region of space, the Rademacher complexity will remain relatively small.
In both left and right ﬁgures we have similar (small) Rademacher complexities, yet different set sizes.
This illustrates why tight bounds on the set size based on Rademacher complexity are difﬁcult to achieve.
Lower bound.
To lower bound the weighted Rademacher complexity we adapt the technique of (Barvinok 1997) for lower bounding the standard Rademacher complexity.
The high level idea is that the space {−1, 1}n can be mapped to the leaves of a binary tree.
By following a path from the root to a leaf, we are dividing the space in half n times, until we arrive at a leaf which corresponds to a single element (with some ﬁxed weight).
By judiciously choosing which half of the space (branch of the tree) to recurse into at each step we derive the bound in Lemma 1, whose proof is given in the appendix.
Lemma 1.
For any β ∈ (0, 1/2), the weighted Rademacher complexity of a weight function w : {−1, 1}n → [0,∞) is lower bounded by R(w) ≥ log w n log (1 − β) + log Z(w) − log w∗(β) (β) + (cid:16) 1−β (cid:17) log (cid:40) with (β) = wmax = maxx w(x), wmin = minx{w(x) : w(x) > 0}, if β ≥ 1/3 if β ≤ 1/3 Upper bound.
In the unweighted setting, a standard up- per bound on the Rademacher complexity is used in learn- ing theory to show that the Rademacher complexity of a small hypothesis class is also small, often to prove PAC- learnability.
Massart’s Lemma (see (Shalev-Shwartz and Ben-David 2014), lemma 26.8) formally upper bounds the Rademacher complexity in terms of the size of the set.
This result is intuitive since, as we have noted, the dot product between any one vector x ∈ {−1, 1}n is small with most other vectors c ∈ {−1, 1}n.
Therefore, if the set is small the Rademacher complexity must also be small.
Adapting the proof technique of Massart’s Lemma to the weighted setting we arrive at the following bound: 2.
If log wmin was provided as input and λ ≤ 1, 2.
If wmax was provided as input, calculate ln Z(w) = Eγ max x∈{−1,1}n {ln w(x) + γ(x)} where w∗ = (cid:19) (cid:18) 1 − βopt (cid:26)wmin, βopt wmax, if βopt < 1 if βopt > 1 Algorithm 2 Rademacher Lower Bound for log Z(w) Inputs: The estimator ¯δk(w) output by algorithm 1, k used to compute ¯δk(w), and optionally wmin and wmax.
Output: A number ψLB which lower bounds log Z(w).
1.
If log wmin was provided as input, calculate k − log wmin λ = ¯δk(w) −(cid:113) 6n (¯δk(w) −(cid:113) 6n ψLB = 3.
Otherwise, + log wmin.
k − log wmin)2 2n ψLB = ¯δk(w) − (cid:114) 6n − n 4.
Output the lower bound max{ψLB, log wmax}.
Experiments The closest line of work to this paper showed that the partition function can be bounded by solving an optimization problem perturbed by Gumbel random variables (Hazan and Jaakkola 2012; Hazan, Maji, and Jaakkola 2013; Hazan et al.
2016; Kim, Sabharwal, and Ermon 2016; Balog et al.
2017).
This approach is based on the fact that (cid:21) (cid:40) (cid:40) where all 2n random variables γ(x) are sampled from the Gumbel distribution with scale 1 and shifted by the Euler- Mascheroni constant to have mean 0.
Perturbing all 2n states with IID Gumbel random variables is intractable, leading the authors to bound ln Z(w) by perturbing states with a combi- nation of low dimensional Gumbel perturbations.
Speciﬁcally the upper bound ln Z(w) ≤ ΘU B = Eγ max x∈{−1,1}n ln w(x) + γi(xi) (Hazan et al.
2016) and lower bound (cid:41)(cid:35) (cid:41)(cid:35) n(cid:88) n(cid:88) i=1 i=1 ln Z(w) ≥ ΘLB = Eγ max x∈{−1,1}n ln w(x) + γi(xi) (Balog et al.
2017, p.
6) hold in expectation, where γi(x) for i = 1, .
.
.
, n are sampled from the Gumbel distribution with scale 1 and shifted by the Euler-Mascheroni constant to have mean 0.
To obtain bounds that hold with high probability using Gumbel perturbations we calculate the slack term (Hazan et al.
2016, p.
32) (cid:33)2 (cid:114) 1 ln 2k (cid:40) n max (cid:114) 32 (cid:41)(cid:41) ln ln (cid:32) (cid:40) g = min 1 + (cid:20) (cid:34) (cid:34) Algorithm 3 Rademacher Upper Bound for log Z(w) Inputs: The estimator ¯δk(w), k used to compute ¯δk(w), and optionally wmin and wmax.
Output: A number ψU B which upper bounds log Z(w).
1.
If wmin was provided as input, calculate (cid:113) 6n k − log wmin (cid:113) 6n k − log wmax ¯δk(w) + βmin = ¯δk(w) + βmax = 3.
Set the value βopt =  βmin, βmax, 2 , 3 , if 0 < βmin < 1 if 1 3 < βmax < 1 if 1 2 < βmax otherwise 4.
Output the upper bound ψU B: (a) If βopt = 1 (b) If βopt = 1 (c) Otherwise, 3, ψU B = ¯δk(w) + 2, ψU B = n + log wmax.
(cid:113) 6n k + n log(cid:0) 3 (cid:1).
ψU B = nβopt log −n log (1 − βopt)+log w giving upper and lower bounds θU B = ΘU B + g and θLB = n that hold with probability 1 − α where k samples ΘLB − g are used to estimate the expectation bounds.
We note the Gumbel expectation upper bound takes nearly the same form as the weighted Rademacher complexity, with two differences.
The perturbation is sampled from a Gum- bel distribution instead of a dot product with a vector of Rademacher random variables and, without scaling, the two bounds are naturally written in different log bases.
We experimentally compare our bounds with those ob- tained by Gumbel perturbations on two models.
First we bound the partition function of the spin glass model from (Hazan et al.
2016).
For this problem the weight function is given by the unnormalized probability distribution of the spin glass model.
Second we bound the propositional model counts (#SAT) for a variety of SAT problems.
This problem falls into the unweighted category where every weight is either 0 or 1, speciﬁcally every satisfying assignment has weight 1 and we bound the total number of satisfying assign- ments.
the question of how many assignments x to the underlying boolean variables result in F evaluating to true.
Our weight function is given by w(x) = 1 if F (x) evaluates to true, and 0 otherwise.
We performed MAP inference on the perturbed problem using the weighted partial MaxSAT solver MaxHS (Davies 2013).
Ground truth was obtained for a variety of models1 us- ing three exact propositional model counters (Thurley 2006; Sang et al.
2004; Oztok and Darwiche 2015)2.
Table 1 shows bounds that hold with probability .95 and k = 1.
While the Gumbel lower bounds are always trivial, we produce non- trivial lower bounds for several model instances.
Our upper bounds are generally comparable to or tighter than Gumbel upper bounds.
Analysis Our bounds are much looser than those computed by random- ized hashing schemes (Chakraborty, Meel, and Vardi 2013; Ermon et al.
2013d; Ermon et al.
2013b; Zhao et al.
2016), but also require much less computation (Ermon et al.
2013c; Achim, Sabharwal, and Ermon 2016).
While our approach provides polynomial runtime guarantees for MAP inference in the spin glass model after random perturbations have been applied, randomized hashing approaches do not.
For proposi- tional model counting, we found that our method is computa- tionally cheaper by over 2 orders of magnitude than results reported in Zhao et al.
(2016).
Additionally, we tried reducing the runtime and accuracy of randomized hashing schemes by running code from Zhao et al.
(2016) with f values of 0, .01, .02, .03, .04, and .05.
We set the maximum time limit to 1 hour (while our method required .01 to 6 seconds of computation for reported results).
Throughout experiments on models reported in Table 1 our approach still generally re- quired orders of magnitude less computation and also found tighter bounds in some instances.
Empirically, our lower bounds were comparable to or tighter than those obtained by Gumbel perturbations on both models.
The weighted Rademacher complexity is generally at least as good an estimator of log Z as the Gumbel up- per bound, however it is only an estimator and not an upper bound.
Our upper bound using the weighted Rademacher complexity, which holds in expectation, is empirically weaker than the corresponding Gumbel expectation upper bound.
However, the slack term needed to transform our expecta- tion bound into a high probability bound is tighter than the corresponding Gumbel slack term.
Since both slack terms approach 0 in the limit of inﬁnite computation (k = ∞, the number of samples used to estimate the expectation bound), this can result in a trade-off where we produce a tighter upper bound up to some value of k, after which the Gumbel bound becomes tighter.
1The models used in our experiments can be downloaded from http://reasoning.cs.ucla.edu/c2d/results.html counts were 2Precomputed model downloaded from https://sites.google.com/site/marcthurley/sharpsat/benchmarks/ collected-model-counts Figure 2: Bounds for a 7x7 spin glass model with k = 5 (for both methods), that hold with probability .95.
Our bounds and estimator are scaled to match Gumbel log base e bounds.
Spin Glass Model Following (Hazan et al.
2016), we bound the partition func- tion of a spin glass model with variables xi ∈ {−1, 1} for i = 1, 2, .
.
.
, n, where each variable represents a spin.
Each spin has a local ﬁeld parameter θi which corresponds to its local potential function θi(xi) = θixi.
We performed experi- ments on grid shaped models where each spin variable has 4 neighbors, unless it occupies a grid edge.
Neighboring spins interact with coupling parameters θi,j(xi, xj) = θi,jxixj.
The potential function of the spin glass model is θ(x1, x2, .
.
.
, xn) = θixi + θi,jxixj, (cid:88) i∈V (cid:88) (i,j)∈E (cid:88) i∈V (cid:88) (i,j)∈E  .
with corresponding weight function w(x) = exp θixi + θi,jxixj We compare our bounds on a 7x7 spin glass model.
We sam- pled the local ﬁeld parameters θi uniformly at random from [−1, 1] and the coupling parameters uniformly at random from [0, c) with c varying.
Non-negative coupling parameters make it possible to perform MAP inference efﬁciently us- ing the graph-cuts algorithm (Kolmogorov and Zabin 2004; Greig, Porteous, and Seheult 1989).
We used the python maxﬂow module wrapping the implementation from Boykov and Kolmogorov (2004).
Figure 2 shows bounds that hold with probability .95, where all bounds are computed with k = 5.
For this value of k, our approach produces tighter upper bounds than using Gumbel perturbations.
The crossover to a tighter Gumbel per- turbation upper bound occurs around k ≈ 15.
Lower bounds are equivalent, although we note it is trivial to recover this bound by simply calculating the largest weight over all states.
Propositional Model Counting Next we evaluate our method on the problem of proposi- tional model counting.
Given a boolean formula F , this poses Model Name log-1 log-2 log-3 log-4 tire-1 tire-2 tire-3 tire-4 ra rb sat-grid-pbl-0010 sat-grid-pbl-0015 sat-grid-pbl-0020 sat-grid-pbl-0025 sat-grid-pbl-0030 c432 c499 c880 c1355 c1908 c2670 #Variables #Clauses 939 1337 1413 2303 352 550 577 812 1236 1854 110 240 420 650 930 196 243 417 555 751 1230 3785 24777 29487 20963 1038 2001 2004 3222 11416 11324 191 436 781 1226 1771 514 714 1060 1546 2053 2876 ln(Z) 47.8 24.2 26.4 65.3 20.4 27.3 26.1 32.3 659.2 855.9 54.7 125.4 220.4 348.3 502.4 25.0 28.4 41.6 28.4 22.9 161.5 ¯δ1(w) 64.5 (20.8) 48.6 (20.7) 49.9 (22.3) 106.0 (26.6) 30.7 (11.2) 42.1 (14.2) 36.9 (17.1) 55.0 (17.4) 621.1 (15.5) 857.2 (12.6) 51.6 (4.9) 120.2 (6.6) 215.5 (9.0) 338.8 (9.4) 482.6 (13.0) 42.7 (5.8) 58.5 (6.2) 83.1 (8.4) 79.8 (12.2) 87.8 (12.2) 260.0 (14.6) ψU B θU B ψLB 438.0 (46.2) 485.7 (60.3) 503.9 (65.3) 830.2 (77.7) 198.5 (17.6) 283.9 (27.7) 280.5 (36.1) 384.7 (38.9) 856.7 (0.0) 1285.1 (0.0) 76.2 (0.0) 166.4 (0.0) 291.1 (0.0) 450.5 (0.0) 644.6 (0.0) 135.3 (1.0) 168.2 (0.4) 281.1 (4.7) 342.9 (14.2) 427.7 (19.3) 812.8 (10.8) 426.5 (43.0) 464.0 (45.1) 478.2 (42.3) 676.9 (58.8) 249.6 (23.7) 310.2 (29.6) 316.5 (29.1) 383.3 (35.3) 1100.9 (45.7) 1387.5 (43.9) 176.3 (13.6) 310.3 (18.8) 472.3 (26.5) 667.8 (33.1) 893.3 (36.7) 212.6 (18.2) 243.8 (16.6) 332.9 (21.3) 368.6 (28.7) 419.1 (32.8) 701.4 (39.6) 0.5 (0.6) 0.3 (0.4) 0.4 (0.4) 0.4 (0.5) 0.3 (0.4) 0.3 (0.4) 0.4 (0.6) 0.3 (0.3) 184.1 (10.1) 239.3 (7.7) 7.6 (2.2) 26.6 (3.8) 56.2 (5.6) 97.0 (6.2) 144.1 (8.7) 1.4 (0.8) 3.2 (1.2) 4.2 (1.4) 2.3 (1.3) 1.8 (0.9) 23.7 (3.4) θLB -0.3 (0.0) -0.3 (0.0) -0.3 (0.0) -0.2 (0.0) -0.5 (0.1) -0.4 (0.1) -0.4 (0.1) -0.3 (0.0) 0.3 (0.0) 0.2 (0.0) -0.5 (0.1) -0.1 (0.1) 0.0 (0.1) 0.2 (0.1) 0.2 (0.0) -0.5 (0.1) -0.4 (0.1) -0.3 (0.1) -0.3 (0.1) -0.3 (0.0) -0.1 (0.0) Table 1: Empirical comparison of our estimate of (¯δ1(w)) and bounds on (ψ) propositional model counts against bounds based on Gumbel perturbations (θ).
The mean over 100 runs is shown with the standard deviation in parentheses.
Bounds hold with probability .95 and k = 1 for both methods.
Tighter bounds are in bold.
Meta column descriptions, left to right: model name and information, natural logarithm of ground truth model counts and our estimator, upper bounds, and lower bounds.
Conclusion We introduced the weighted Rademacher complexity, a novel generalization of Rademacher complexity.
We showed that this quantity can be used as an estimator of the size of a weighted set, and gave bounds on the weighted Rademacher complexity in terms of the weighted set size.
This allowed us to bound the sum of any non-negative weight function, such as the partition function, in terms of the weighted Rademacher complexity.
We showed how the weighted Rademacher complexity can be efﬁciently approximated whenever an efﬁcient optimization oracle exists, as is the case for a variety of practical problems including calculating the partition function of certain graphical models and the per- manent of non-negative matrices.
Experimental evaluation demonstrated that our approach provides tighter bounds than competing methods under certain conditions.
In future work our estimator R(w) and bounds on Z(w) may be generalized to other forms of randomness.
Rather than sampling c uniformly from {−1, 1}n, we could conceiv- ably sample each element ci from some other distribution, such as the uniform distribution over [−1, 1], a Gaussian, or Gumbel.
Our bounds should readily adapt to continuous uniform or gaussian distributions, although derivations may be more complex in general.
As another line of future work, the weighted Rademacher complexity may be useful beyond approximate inference to learning theory.
Acknowledgments We gratefully acknowledge funding from Ford, FLI and NSF grants #1651565, #1522054, #1733686.
We also thank Tri Dao, Aditya Grover, Rachel Luo, and anonymous reviewers.
References [2016] Achim, T.; Sabharwal, A.; and Ermon, S.
2016.
Be- yond parity constraints: Fourier analysis of hash functions for inference.
In International Conference on Machine Learning, 2254–2262.
[2013] Bach, F., et al.
2013.
Learning with submodular functions: A convex optimization perspective.
Foundations and Trends R(cid:13) in Machine Learning 6(2-3):145–373.
[2017] Balog, M.; Tripuraneni, N.; Ghahramani, Z.; and Weller, A.
2017.
Lost relatives of the Gumbel trick.
In 34th International Conference on Machine Learning, 371– 379.
[1997] Barvinok, A.
I.
1997.
Approximate counting via random optimization.
Random Structures and Algorithms 11(2):187–198.
[1961] Bellman, R.
E.
1961.
Adaptive control processes: a guided tour.
Princeton university press.
[2006] Bezáková, I.; Štefankoviˇc, D.; Vazirani, V.
V.; and Vigoda, E.
2006.
Accelerating simulated annealing for the permanent and combinatorial counting problems.
In Proceed- ings of the seventeenth annual ACM-SIAM symposium on Discrete algorithm, 900–907.
[2004] Boykov, Y., and Kolmogorov, V.
2004.
An experimen- tal comparison of min-cut/max-ﬂow algorithms for energy minimization in vision.
IEEE transactions on pattern analy- sis and machine intelligence 26(9):1124–1137.
[2014] Chakrabarty, D.; Jain, P.; and Kothari, P.
2014.
Prov- able submodular minimization using wolfe’s algorithm.
In Advances in Neural Information Processing Systems, 802– 809.
[2013] Chakraborty, S.; Meel, K.
S.; and Vardi, M.
Y.
2013.
A scalable approximate model counter.
In International Confer- ence on Principles and Practice of Constraint Programming, 200–216.
Springer.
[2013] Davies, J.
2013.
Solving MAXSAT by Decoupling Optimization and Satisfaction.
Ph.D. Dissertation, University of Toronto.
[2013a] Ermon, S.; Gomes, C.; Sabharwal, A.; and Selman, B.
2013a.
Taming the curse of dimensionality: Discrete In Proceedings integration by hashing and optimization.
of the 30th International Conference on Machine Learning (ICML-13), 334–342.
[2013b] Ermon, S.; Gomes, C.
P.; Sabharwal, A.; and Selman, B.
2013b.
Embed and project: Discrete sampling with univer- sal hashing.
In Advances in Neural Information Processing Systems (NIPS), 2085–2093.
[2013c] Ermon, S.; Gomes, C.
P.; Sabharwal, A.; and Selman, B.
2013c.
Optimization with parity constraints: From binary codes to discrete integration.
In Proc.
of the 29th Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
[2013d] Ermon, S.; Gomes, C.
P.; Sabharwal, A.; and Selman, B.
2013d.
Taming the curse of dimensionality: Discrete integration by hashing and optimization.
In Proc.
of the 30th International Conference on Machine Learning (ICML).
[2014] Ermon, S.; Gomes, C.; Sabharwal, A.; and Selman, B.
2014.
Low-density parity constraints for hashing-based discrete integration.
In International Conference on Machine Learning, 271–279.
[1980] Fujishige, S.
1980.
Lexicographically optimal base of a polymatroid with respect to a weight vector.
Mathematics of Operations Research 5(2):186–196.
[1989] Greig, D.
M.; Porteous, B.
T.; and Seheult, A.
H.
1989.
Exact maximum a posteriori estimation for binary images.
Journal of the Royal Statistical Society.
Series B (Method- ological) 271–279.
[2012] Hazan, T., and Jaakkola, T.
S.
2012.
On the partition function and random maximum a-posteriori perturbations.
In Langford, J., and Pineau, J., eds., Proceedings of the 29th International Conference on Machine Learning (ICML-12), 991–998.
New York, NY, USA: ACM.
[2016] Hazan, T.; Orabona, F.; Sarwate, A.
D.; Maji, S.; and Jaakkola, T.
2016.
High dimensional inference with ran- dom maximum a-posteriori perturbations.
arXiv preprint arXiv:1602.03571.
[2013] Hazan, T.; Maji, S.; and Jaakkola, T.
2013.
On sam- pling from the Gibbs distribution with random maximum a-posteriori perturbations.
In Advances in Neural Informa- tion Processing Systems, 1268–1276.
[1971] Hopcroft, J.
E., and Karp, R.
M.
1971.
A n5/2 algo- rithm for maximum matchings in bipartite graphs.
In Switch- ing and Automata Theory, 1971., 12th Annual Symposium on, 122–125.
IEEE.
[2011] Jegelka, S.; Lin, H.; and Bilmes, J.
A.
2011.
On fast approximate submodular minimization.
In Advances in Neural Information Processing Systems, 460–468.
[1996] Jerrum, M., and Sinclair, A.
1996.
The markov chain monte carlo method: an approach to approximate counting and integration.
Approximation algorithms for NP-hard prob- lems 482–520.
[2004] Jerrum, M.; Sinclair, A.; and Vigoda, E.
2004.
A polynomial-time approximation algorithm for the permanent of a matrix with nonnegative entries.
Journal of the ACM (JACM) 51(4):671–697.
[1987] Jonker, R., and Volgenant, A.
1987.
A shortest aug- menting path algorithm for dense and sparse linear assign- ment problems.
Computing 38(4):325–340.
[1998] Jordan, M.
I.; Ghahramani, Z.; Jaakkola, T.
S.; and Saul, L.
K.
1998.
An introduction to variational methods for graphical models.
NATO ASI SERIES D BEHAVIOURAL AND SOCIAL SCIENCES 89:105–162.
[2016] Kim, C.; Sabharwal, A.; and Ermon, S.
2016.
Exact sampling with integer linear programs and random perturba- tions.
In Proc.
30th AAAI Conference on Artiﬁcial Intelli- gence.
[2009] Koller, D., and Friedman, N.
2009.
Probabilistic graphical models: principles and techniques.
MIT press.
[2004] Kolmogorov, V., and Zabin, R.
2004.
What energy functions can be minimized via graph cuts?
IEEE transac- tions on pattern analysis and machine intelligence 26(2):147– 159.
[1955] Kuhn, H.
W.
1955.
The hungarian method for the assignment problem.
Naval Research Logistics (NRL) 2(1- 2):83–97.
[2002] Madras, N.
N.
2002.
Lectures on monte carlo methods, volume 16.
American Mathematical Soc.
[1989] McDiarmid, C.
1989.
On the method of bounded differences.
Surveys in combinatorics 141(1):148–188.
[2016] Mussmann, S., and Ermon, S.
2016.
Learning and inference via maximum inner product search.
In International Conference on Machine Learning, 2587–2596.
[2017] Mussmann, S.; Levy, D.; and Ermon, S.
2017.
Fast amortized inference and learning in log-linear models with randomly perturbed nearest neighbor search.
UAI.
[2009] Orlin, J.
B.
2009.
A faster strongly polynomial time algorithm for submodular function minimization.
Mathemat- ical Programming 118(2):237–251.
[2015] Oztok, U., and Darwiche, A.
2015.
A top-down compiler for sentential decision diagrams.
In IJCAI, 3141– 3148.
[2004] Sang, T.; Bacchus, F.; Beame, P.; Kautz, H.
A.; and Pitassi, T.
2004.
Combining component caching and clause learning for effective model counting.
In SAT.
[2014] Shalev-Shwartz, S., and Ben-David, S.
2014.
Un- derstanding machine learning: From theory to algorithms.
Cambridge university press.
[2006] Thurley, M.
2006.
sharpsat-counting models with advanced component caching and implicit bcp.
In SAT, 424– 429.
[1979] Valiant, L.
G.
1979.
The complexity of enumera- tion and reliability problems.
SIAM Journal on Computing 8(3):410–421.
[2008] Wainwright, M.
J.; Jordan, M.
I.; et al.
2008.
Graphi- cal models, exponential families, and variational inference.
Foundations and Trends R(cid:13) in Machine Learning 1(1–2):1– 305.
[2016] Zhao, S.; Chaturapruek, S.; Sabharwal, A.; and Ermon, S.
2016.
Closing the gap between short and long xors for model counting.
In AAAI, 3322–3329.
Appendix We present formal proofs of our bounds on the sum Z(w) of any non-negative weight function w : {−1, 1}n → [0,∞).
For readability we occasionally restate results from the main paper.
The format of our proof is as follows.
First we bound the weighted Rademacher complexity, R(w), by the output of our optimization oracle (δ(c, w), described in Assumption 1), which we refer to as the slack bound.
Next we lower bound the sum Z(w) by R(w) and apply our slack bound to obtain a lower bound on Z(w) in terms of δ(c, w).
Similarly, we upper bound the sum Z(w) by R(w) and apply our slack bound to obtain an upper bound on Z(w) in terms of δ(c, w).
Finally we tighten the bounds by repeatedly applying our optimization oracle.
Slack Bound We use McDiarmid’s bound (Proposition 1) to bound the difference between the output of our optimization oracle (δ(c, w), described in Assumption 1) and its expectation, which is the weighted Rademacher complexity R(w).
For the function the constant di = 2 in McDiarmid’s bound, giving fw(c) = δ(c, w) = max x∈{−1,1}n P [|δ(c, w) − R(w)| ≥ ] ≤ exp P [|δ(c, w) − R(w)| ≥ 6n] ≤ exp {(cid:104)c, x(cid:105) + log2 w(x)}, (cid:33) (cid:32) −22(cid:80) (cid:33) j c2 ≤ .05.
(cid:32)−2( 4n 6n)2 By choosing c ∈ {−1, 1}n uniformly at random we can say with probability greater than .95 that R(w) − 6n ≤ δ(c, w) ≤ R(w) + 6n.
(6) Lower Bound In this section we lower bound the sum Z(w) by R(w) and apply our slack bound to obtain a lower bound on Z(w) in terms of δ(c, w).
We extend the Massart lemma (Shalev-Shwartz and Ben-David 2014, lemma 26.8) to the weighted setting by accounting for the log2 w(x) weight term in the weighted Rademacher complexity.
Our lower bound on Z(w) is given by the following Lemma: Lemma 3.
For c ∈ {−1, 1}n sampled uniformly at random, the following bound holds with probability greater than .95: Proof.
We begin by upper bounding R(w) in terms of Z(w).
Deﬁne c ∈ {−1, 1}n generated uniformly at random and x ∈ {−1, 1}n.
For any λ > 0, γ > 0, and weight functions w, wγ : {−1, 1}n → [0,∞) with wγ(x) = w(x)γ we have (cid:40) (δ(c,w)−√ δ(c, w) − √ 6n−log2 wmin)2 2n 6n − n 2 , log2 Z(w) ≥ (cid:104) Ec λ(cid:104)c, x(cid:105) + λγ log2 w(x) max (cid:105) + log2 wmin, if δ(c,w)−√ otherwise 6n−log2 wmin ≤ 1 (cid:104) R(wγ) = Ec Ec (cid:34) Ec log2 log2 max max (cid:104) (cid:105) 2λ((cid:104)c,x(cid:105)+γ log2 w(x))(cid:105) ≤ (cid:104)c, x(cid:105) + γ log2 w(x) (cid:35) (cid:88) (cid:35) (cid:34)(cid:88) 2λ((cid:104)c,x(cid:105)+γ log2 w(x)) 2λ((cid:104)c,x(cid:105)+γ log2 w(x)) Jensen≤ log2 Ec where we have used Jensen’s inequality.
By the linearity of expectation and independence between elements ci in a random vector c, (cid:88) (cid:16) (cid:104) 2λ(cid:104)c,x(cid:105)(cid:105)(cid:17) R(wγ) ≤ 1 log2 2λγ log2 w(x)Ec log2 2λγ log2 w(x) i=1 n(cid:89) Eci (cid:2)2λcixi(cid:3)(cid:33) (cid:32) (cid:88) Using Lemma A.6 from (Shalev-Shwartz and Ben-David 2014), (cid:32) (cid:88) λ2||x||2 (cid:19) R(wγ) ≤ 1 log2 (cid:18) (cid:88) 2λγ log2 w(x)2 log2 2λγ log2 w(x) 2λxi + 2−λxi log2 λ2 n (cid:33) log2 ≤ 1 (cid:33) (cid:32) (cid:88) log2 2λγ log2 w(x) (cid:32)(cid:88) n(cid:89) i=1 (λxi)2 (cid:33) (cid:33) λn 2λγ log2 w(x) 2λγ log2 w(x) (cid:32)(cid:88) λn log2 (cid:33) λn w(x)w(x)λγ−1 (cid:88) (cid:33) n(cid:89) (cid:32) i=1 (cid:32)(cid:88) (cid:33) log2 w(x)λγ log2 max w(x) λn log2 Z(w) + log2 max (cid:32) Next, where R(wγ) ≤ 1 (cid:8)w(x)λγ−1(cid:9)(cid:88) w∗(λ, γ) = log2 Z(w) + (cid:26)wmax = maxx w(x), wmin = minx{w(x) : w(x) > 0}, if λγ ≥ 1 if λγ ≤ 1 {wλγ−1 max , wλγ−1 min } + λ log2 w∗(λ, γ) + λ λγ − 1 (7) Note that for λγ = 1 we have two valid inequalities that hold for either choice of w∗(λ, γ).
Having bounded the weighted Rademacher complexity in terms of Z(w), we now apply the slack bound from equation 6 and have that with probability greater than .95 (8) This upper bound on δ(w) holds for any λ > 0, so we could jointly optimize over λ and γ to make the bound as tight as possible.
However, this is non-trivial because changing γ changes the weight function we supply to our optimization oracle.
Instead we generally set γ = 1 and optimize over only λ.
At the end of this section we derive another bound with a different choice of γ.
This bound is trivial to derive, but illustrates that other choices of γ could result in meaningful bounds.
log2 Z(w) + 6n.
Rewriting the bound in Equation 8 with γ = 1 and w∗(λ) = w∗(λ, 1) we have log2 w∗(λ, γ) + λ δ(c, w) ≤ 1 λγ − 1 (9) so the optimal value of λ that makes our bound as tight as possible occurs at the maximum of the quadratic function − (λ − 1) log2 w∗(λ) − λ2 n + λ(δ(c, w) − h(λ) = −λ2 n − λ(log2 w∗(λ) − (δ(c, w) − h(cid:48)(λ) = −λn − log2 w∗(λ) + 6n) ≤ log2 Z(w), (cid:17) (cid:16) 6n)) + log2 w∗(λ) δ(c, w) − 6n h(cid:48)(cid:48)(λ) = −n, Where the stated derivatives are valid for λ (cid:54)= 1, as w∗(λ) is piecewise constant with a discontinuity at λ = 1.
The maximum of h(λ) must occur at λ = −∞, +∞, 1, or the value of λ that makes h(cid:48)(λ) = 0.
By inspection the maximum does not occur at λ = ±∞, so the maximum will occur at h(cid:48)(λ) = 0 or else λ = 1 if the derivative is never zero.
We have h(cid:48)(λ) = 0 at λ = (δ(c, w) − √ (cid:17) 6n − log2 w∗(λ))/n.
Rearranging equation 9 we have + log2 w∗(λ) ≤ log2 Z(w).
Depending on the value of δ(c, w) we have 3 separate regimes for the optimal lower bound on Z(w).
6n − log2 w∗(λ) δ(c, w) − −λ2 n (cid:16) + λ 6n + log2 wmin: In this case h(cid:48)(λ) = 0 at λ = (δ(c, w) − √ 6n − log2 wmin)/n (note λ < 1 so that 1.
δ(c, w) < n + w∗(λ) = wmin) and the optimal lower bound is (δ(c, w) − √ 6n − log2 wmin)2 2n + log2 wmin ≤ log2 Z(w).
We require that λ > 0, but note that we can discard our bound and recompute with a new c if we ﬁnd that λ < 0 for our computed value of δ(c, w), as this can only happen with low probability when our slack bound is violated and we have estimated R(w) poorly with δ(c, w).
Note that R(w) = Ec Ec (cid:2)maxx∈{−1,1}n{(cid:104)c, x(cid:105) + log2 wmin}(cid:3) = n + log2 wmin, so R(w) − log2 wmin = n > 0.
(cid:2)maxx∈{−1,1}n{(cid:104)c, x(cid:105) + log2 w(x)}(cid:3) ≥ 2.
n + 6n + log2 wmin < δ(c, w) < n + lower bound of 3.
δ(c, w) > n + 6n + log2 wmax: In this case h(cid:48)(λ) is never zero, so at λ = 1 we have the optimal δ(c, w) − ≤ log2 Z(w), 6n − n 6n + log2 wmax: This case cannot occur because δ(c, w) ≤ n + log2 wmax by deﬁnition.
We now illustrate how alternative choices of γ could result in meaningful bounds.
From Equation 7 we have (cid:20) max x∈{−1,1}n Ec (cid:21) (cid:20) {(cid:104)c, x(cid:105) + γ log2 w(x)} R(wγ) ≤ 1 ≤ 1 (cid:104)c, x(cid:105) + γ log2 (cid:26) Ec max x∈{−1,1}n log2 Z(w) + log2 Z(w) + w(x) w∗(λ, γ) λγ − 1 λγ − 1 ≤ 1 (cid:27)(cid:21) log2 w∗(λ, γ) + λ log2 w∗(λ, γ) + λ log2 Z(w) w∗(λ, γ) + λ For sufﬁciently large γ (γ ≥ 1 large γ, λ) we have w∗(λ, γ) = wmax, which makes log2 w(x) w∗(λ,γ) ≤ 0 for all x.
Further, for sufﬁciently (for all c) and when a single element x ∈ {−1, 1}n has the unique largest weight (w(x) > w(y)∀x (cid:54)= y) arg max x∈{−1,1}n (cid:104)c, x(cid:105) + γ log2 w(x) w∗(λ, γ) = arg max x∈{−1,1}n {w(x)} (cid:27) (cid:27)(cid:21) w(x) w∗(λ, γ) = 0.
(cid:26) Ec max x∈{−1,1}n (cid:26) (cid:20) (cid:114) (cid:26) Therefore, for sufﬁciently large γ and λ = 2 log2 Z(w) wmax (cid:20) Ec max x∈{−1,1}n (cid:104)c, x(cid:105) + γ log2 (cid:104)c, x(cid:105) + γ log2 (cid:27)(cid:21) , we have w(x) w∗(λ, γ) ≤ 1 log2 Z(w) w∗(λ, γ) + λ (cid:115) 0 ≤ Z(w) 2n log2 wmax wmax ≤ Z(w).
(cid:114) λ or γ ≤ 1 λ.
Note that we have used λ = λ requires a bound on Z(w).
We leave joint optimization over λ and γ for future work.
λ or alternatively we would make γ as big as possible subject to γ ≤ 1 This bound is trivial, however we picked γ poorly.
To tighten this bound we would make γ as small as possible subject to γ ≥ 1 gauranteeing γ ≥ 1 Upper Bound In this section we upper bound the sum Z(w) by R(w) and apply our slack bound to obtain an upper bound on Z(w) in terms of δ(c, w).
Our proof technique is inspired by (Barvinok 1997), who developed the method for bounding the sum Z(w) of a weight function with values of either 0 or 1.
We generalize the proof to the weighted setting for any weight function w : {−1, 1}n → [0,∞).
The principle underlying the method is that by dividing the space {−1, 1}n in half n times we arrive at a single weight.
By judiciously choosing which half of the space to recurse into at each step we can bound upper bound Z(w).
Deﬁne the j dimensional space Ij = {x : x ∈ {−1, 1}j} for j ≥ 1 and I0 = {(0)}.
For any vector x ∈ Ij−1 with j ≥ 2, deﬁne x+, x− ∈ Ij as For the single element (0) ∈ I0, deﬁne (0)+ = (1) and (0)− = (−1).
x+ = (x1, x2, .
.
.
, xj−1, 1), x− = (x1, x2, .
.
.
, xj−1,−1).
Given the weight function wj : Ij → [0,∞) (in this section we explicitly write wj to denote that the weight function has a j Z(w) wmax , so 2 log2 dimensional domain while w implicitly denotes wn), deﬁne the weight functions j−1 : {−1, 1}j−1 → [0,∞), with w+ w+ j−1(x) = wj(x+) and We have split the weights of our original weight function between two new weight functions, each with j−1 dimensional domains (two disjoint half spaces of our original j dimensional domain).
Now we relate the expectation R(wj) to the expectations R(w+ Lemma 4.
For j ≥ 1 one has j−1 : {−1, 1}j−1 → [0,∞), with w− w− j−1) in Lemmas 4 and 5.
j−1(x) = wj(x−) j−1) and R(w− R(w+ j−1),R(w− j−1) ≤ R(wj) Proof.
Given x, c ∈ Ij−1 we have (cid:104)c, x(cid:105) + log2 w+ j−1(x) = ((cid:104)c+, x+(cid:105) + log2 wj(x+)) + ((cid:104)c−, x+(cid:105) + log2 wj(x+)) ≤ maxy∈Ij {(cid:104)c+, y(cid:105) + log2 wj(y)} + maxy∈Ij {(cid:104)c−, y(cid:105) + log2 wj(y)} δ(c+, wj) + δ(c−, wj) (10) This inequality holds for any x, so we can maximize the left hand side of Equation 10 over x and get δ(c, w+ j−1) = max x∈Ij−1 Now we average over c ∈ Ij−1 and get j−1(x)(cid:9) ≤ δ(c+, wj) + δ(c−, wj) (cid:8)(cid:104)c, x(cid:105) + log2 w+ (cid:88) (cid:88) 2j−1 c∈Ij−1 ≤ 1 2j−1 R(w+ j−1) = δ(c, w+ j−1) δ(c+, wj) + δ(c−, wj) (cid:88) j−1) ≤ R(wj) follows the same structure.
c∈Ij 2j c∈Ij−1 δ(c, wj) = R(wj) The proof for R(w− Lemma 5.
For j ≥ 1 one has Proof.
Let c, x ∈ Ij−1.
Then R(w− j−1) + R(w+ j−1) ≤ R(wj) − 1 (cid:104)c, x(cid:105) + log2 w+ j−1(x) = (cid:104)c+, x+(cid:105) − 1 + log2 w+ j−1(x) = (cid:104)c+, x+(cid:105) + log2 wj(x+) − 1 ≤ δ(c+, wj) − 1 The inequality (cid:104)c, x(cid:105) + log2 w+ j−1(x) ≤ δ(c+, wj) − 1 holds for any x.
Maximizing over x we get δ(c, w+ j−1) ≤ δ(c+, wj) − 1.
Similarly, and maximizing over x we get Therefore (cid:104)c, x(cid:105) + log2 w− j−1(x) = (cid:104)c−, x−(cid:105) − 1 + log2 w− j−1(x) = (cid:104)c−, x−(cid:105) + log2 wj(x−) − 1 ≤ δ(c−, wj) − 1, δ(c, w− j−1) ≤ δ(c−, wj) − 1.
δ(c, w− j−1) + δ(c, w+ j−1) ≤ δ(c−, wj) + δ(c+, wj) − 1 and averaging over c ∈ Ij−1 we get R(w− j−1) + R(w+ j−1) ≤ 1 2j−1 2j−1 j−1) c∈Ij−1 δ(c, w− j−1) + δ(c, w+ (cid:88) (cid:19) (cid:18) δ(c−, wj) + δ(c+, wj) (cid:88)  − 1 = R(wj) − 1.
(cid:88) δ(c, wj) − 1 c∈Ij−1 2j c∈Ij j−1) and R(w− Equipped with our relations between the expectation R(wj) and the expectations R(w+ j−1) from Lemmas 4 and 5, we are prepared to upper bound Z(wn) by R(wn).
To understand our strategy it is helpful to view the weight function wj : {−1, 1}j → [0,∞) as a binary tree with 2j leaf nodes, each corresponding to a weight.
The weight functions w+ j−1 and w− j−1 correspond to subtrees whose root nodes are the two children of the root node in the complete tree representing wj.
Our strategy is to recursively divide the original weight function wn in half, picking one of the two subtrees based on their relative sizes (as measured by the sum of each subtree’s weights and their weighted Rademacher complexities).
Eventually we arrive at a leaf, corresponding to a single weight.
This allows us to relate the weighted Rademacher complexity of the original weight function (or the entire tree) to Z(wn) and the weight of this single leaf.
The problem of choosing which subtree to pick at each step based on their relative sizes is computationally intractable.
To avoid this difﬁculty we do not explicitly ﬁnd the leaf, but instead conservatively pick either the largest or smallest weight in the entire tree, which gives us an upper bound on Z(wn) in terms of R(wn).
After upper bounding Z(wn) by R(wn) we apply our slack bound to obtain an upper bound on Z(wn) in terms of δ(c, wn).
Lemma 6.
Assuming Z(w) > 0, for c ∈ {−1, 1}n sampled uniformly at random, the following bound holds with probability greater than .95: 6n − log2 wmin 6n − log2 wmax (cid:1) ≈ δ(c, w) + (cid:1) − n log2 (1 − βopt) + log2 wmin, (cid:1) − n log2 (1 − βopt) + log2 wmax, 6n + .58n, if 0 < βopt < 1 if 1 3 < βopt < 1 if βopt = 1 if βopt = 1 log2 Z(w) ≤ where  (cid:16) 1−βopt (cid:16) 1−βopt βopt log2 βopt log2 δ(c, w) + log2 wmax + n, 6n + n log2 (cid:17)(cid:0)δ(c, w) + (cid:17)(cid:0)δ(c, w) + (cid:0) 3  δ(c,w)+ δ(c,w)+ 2 , 3 , βopt = 6n−log2 wmin 6n−log2 wmax 6n−log2 wmin if 0 < δ(c,w)+ 6n−log2 wmax 3 < δ(c,w)+ if 1 6n−log2 wmax 2 < δ(c,w)+ if 1 otherwise < 1 < 1 Proof.
Let β be a parameter, to be set later, such that 0 < β ≤ 1/2.
We construct a sequence of weight functions wn, wn−1, .
.
.
, w1, w0 where wj : {−1, 1}j → [0,∞).
Starting with our original weight function wn, we use two rules to decide whether wj−1 = w− j−1 or wj−1 = w+ Rule 1: Given wj, if min{(cid:80) Rule 2: Given wj, if min{(cid:80) y w+ y w+ j−1(y),(cid:80) j−1(y),(cid:80) j−1.
y w− y w− j−1(y)} < β(cid:80) (cid:40)(cid:88) j−1(y)} ≥ β(cid:80) (cid:8)R(w+ w+ wj−1 = arg max j−1 j−1,w w+ wj−1 = arg min j−1 j−1,w w+ (cid:88) y wj(y), we let j−1(y), w− j−1(y) y wj(y), we let j−1),R(w− j−1)(cid:9) (cid:41) Note that R(w0) = log2 wn(x) for some x ∈ In. That is, after dividing our original space with 2n states in half n times, we are left with a single state.
Now, given that Z(w) > 0, rule 1 guarantees wn(x) > 0.
As proof by contradiction, assume that wn(x) = 0.
This requires that for some for some integer i (with 0 < i ≤ n) we have 0 <(cid:80) following rule 1 makes this impossible.
y wi−1(y), but Our ﬁrst step is to relate the weighted Rademacher complexity R(wn) to the ﬁnal leaf weight wn(x) based on the number of y wi(y) and 0 =(cid:80) times we use rule 2 when dividing the original tree.
Every time we use rule 2 R(wj−1) = min w+ j−1,w j−1 {R(w+ j−1),R(w− j−1)}.
By Lemma 5, R(wj) is at least as large as the average of R(w− minimum of R(w− R(wj) ≥ R(wj−1) regardless of whether we use rule 1 or 2.
Let m be the number of times we have used Rule 2, then j−1) plus one, making it at least as large as the j−1) plus one.
Therefore R(wj) ≥ R(wj−1) + 1 whenever we use rule 2.
By lemma 4 we have j−1) and R(w+ j−1) and R(w+ z∈Ij R(w) = R(wn) ≥ R(w0) + m = log2 wn(x) + m.
Now we relate the number of times we use rule 2 to the sum Z(wn).
Observe that(cid:80) (cid:80) wj(z); therefore if we have used rule 1 we must have(cid:80) y wj−1(y) ≥ (1 − β)(cid:80) of the two subtrees.
If we have used rule 2 then(cid:80) y wj−1(y) ≥ β(cid:80) (cid:18) β w0(y) ≥ (1 − β)n−m βm (cid:88) subtrees both carry at least the fraction β of the total weight.
Therefore wn(y) = (1 − β)n (cid:88) wn(x) = j−1(y) +(cid:80) (cid:19)m w− j−1(y) = z wj(z) because we picked the largest z wj(z), because by deﬁnition we use rule 2 when the two y∈Ij−1 y∈Ij−1 w+ 1 − β y∈I0 y∈In Taking logarithms (cid:16) 1−β (cid:17) Note that log2 (cid:19) (cid:18) β 1 − β log2 wn(x) ≥ n log2 (1 − β) + m log2 + log2 Z(wn) −m log2 ≥ n log2 (1 − β) + log2 Z(wn) − log2 wn(x) (cid:18) β (cid:18) 1 − β 1 − β (cid:19) (cid:19) ≥ n log2 (1 − β) + log2 Z(wn) − log2 wn(x).
m log2 > 0 for 0 < β < 1/2 so m ≥ n log2 (1 − β) + log2 Z(wn) − log2 wn(x) (cid:16) 1−β (cid:17) log2 n log2 (1 − β) + log2 Z(wn) − log2 wn(x) Z(wn).
(11) (12) (13) (14) and by combining Equations 11 and 13 we have R(w) ≥ log2 wn(x) + Applying the slack bound from Equation 6 we have3 log2 (cid:17) (cid:16) 1−β (cid:16) 1−β (cid:17) log2 δ(c, w) ≥ log2 wn(x) + n log2 (1 − β) + log2 Z(wn) − log2 wn(x) 6n with probability greater than .95.
Now we can choose β to optimize this bound.
Note that the bound in Equation 14 contains the term wn(x), however it is computationally intractable to explicitly ﬁnd this leaf following the procedure outlined above.
Instead we conservatively use either the smallest or largest weight depending on the value of β.
If these weights cannot be computed we may alternatively pick β = 1/3 to eliminate wn(x) from the bound.
Depending on the value of β we have 4 cases outlined below: 1.
When β = 1/3, the quantity log2 (cid:16) 1−β (cid:17) δ(c, w) ≥ log2 Z(wn) − n log2(3/2) − = 1 and 6n.
3This bound is scale invariant; if we scale the weight function by a constant a so that w(cid:48)(x) = aw(x) then R(w(cid:48)) = log2 a + R(w), log2 Z(w(cid:48) n) = log2 a + log2 Z(wn), and log2 w(cid:48) n(x) = log2 a + log2 wn(x) so the bound remains unchanged.
2.
When 0 < β < 1 3, the quantity 1 − log2( 1−β β ) > 0 and δ(c, w) ≥ n log2 (1 − β) + log2 Z(wn) 3.
When 1 3 < β < .5, the quantity 1 − log2( 1−β β ) < 0 and δ(c, w) ≥ n log2 (1 − β) + log2 Z(wn) (cid:16) 1−β (cid:17) (cid:16) 1−β (cid:17) log2 log2 (cid:32) 1 − + log2 wmin (cid:32) 1 − + log2 wmax (cid:17)(cid:33) (cid:17)(cid:33) (cid:16) 1−β (cid:16) 1−β log2 log2 6n 6n.
4.
When β = .5, the quantity 1−β β = 0 and we recover the trivial bound log2 Z(wn) ≤ log2 wmax + n from equation 12.
To choose the best value for β we minimize the upper bound on log2 Z(wn) with respect to β.
The upper bound on log2 Z(wn) is (cid:18) 1 − β (cid:19)(cid:16) δ(c, w) + (cid:17) − n log2 (1 − β) + log2 w∗(β) (cid:19) 6n − log2 w∗(β) (cid:18) 1 − β − n log2 (1 − β) + log2 w∗(β), = a log2 log2 Z(wn) ≤ L(β) = log2 with 3.
Differentiating we a = δ(c, w) + where w∗(β) = wmin = minx{w(x) : w(x) > 0} for β < 1 get (cid:16)− 1−β L(cid:48)(β) = (cid:17) β2 − 1 (1 − β)2 The ﬁrst derivative has a root at β = a L(cid:48)(cid:48)(β) = aβ (cid:16)− 1−β (cid:16)− 1−β 6n − log2 w∗(β), (cid:17) 3 and w∗(β) = wmax = maxx w(x) for β > 1 β2 − 1 (cid:17) 1 − β β2 − 1 1 − β n ) = n3 (cid:16) 2(1−β) β3 + 2 β2 1 − β (1 − β)2 1 − β (cid:17) n, with L(cid:48)(cid:48)( a a(n−a).
By deﬁnition the only meaningful values of β are 0 < β ≤ .5, so either 0 < a < n/2 and the second derivative is positive making this root a minimum or a n is outside our valid range for β and the minimum occurs at an endpoint of the range.
Note that for a > 0 we have limβ→0 L(β) = ∞, so the minimum never occurs at the endpoint β = 0 when a > 0.
If a ≤ 0 then our slack bound has been violated and we have estimated R(w) poorly with δ(c, w), so it is appropriate to sample a new c and recompute δ(c, w).
(To see this, note that R(w) = Ec R(w) − log2 wmin = n > 0.) Also, lim→0 L( 1 3 − ) (at β = 1/3 the term w∗(β) doesn’t appear in the bound so it doesn’t matter whether w∗(β) = wmin or w∗(β) = wmax).
Assuming we have sampled c such that a > 0, this means the optimal value of β that minimizes our upper bound is: (cid:2)maxx∈{−1,1}n{(cid:104)c, x(cid:105) + log2 wmin}(cid:3) = n + log2 wmin, so (cid:2)maxx∈{−1,1}n{(cid:104)c, x(cid:105) + log2 w(x)}(cid:3) ≥ Ec 3 + ) = lim→0 L( 1 δ(c,w)+ δ(c,w)+  (cid:17)(cid:0)δ(c, w) + (cid:17)(cid:0)δ(c, w) + (cid:0) 3 6n + n log2 6n−log2 wmin 6n−log2 wmax 6n−log2 wmin if 0 < δ(c,w)+ 6n−log2 wmax 3 < δ(c,w)+ if 1 6n−log2 wmax 2 < δ(c,w)+ if 1 otherwise < 1 < 1 6n − log2 wmin 6n − log2 wmax (cid:1) ≈ δ(c, w) + (cid:1) − n log2 (1 − βopt) + log2 wmin, (cid:1) − n log2 (1 − βopt) + log2 wmax, 6n + .58n, βopt = 2 , 3 , Our upper bound on Z(wn) is given by  (cid:16) 1−βopt (cid:16) 1−βopt βopt log2 βopt log2 δ(c, w) + log2 wmax + n, log2 Z(wn) ≤ if 0 < βopt < 1 if 1 3 < βopt < 1 if βopt = 1 if βopt = 1 Tightening the Slack Bound We can improve our high probability bounds on δ(c, w) in terms of Z(w) by generating k independent vectors c1, c2, .
.
.
, ck ∈ {−1, 1}n, applying the optimization oracle from Assumption 1 to each, and taking the mean ¯δk(w) = (δ(c1, w) + ··· + δ(ck, w))/k.
This gives the bounds log2 w∗(β)+ n log2 (1 − β) + log2 Z(w) − log2 w∗(β) log2 w∗(λ, γ)+λ (cid:114) 6n (cid:114) 6n log2 Z(w)+ λγ − 1 ≤ ¯δk(w) ≤ 1 In the last two sections we inverted these bounds and optimized over β and λ to obtain high probability bounds on Z(w) in terms of δ(c, w).
This process is unchanged, with the exceptions that we replace δ(w, c) (computed from a single c ∈ {−1, 1}n) with ¯δk(w) and the term w(cid:48)(c1, ..., ck) = w(c1) × ··· × w(ck) for c1, .
.
.
, ck ∈ {−1, 1}n.
The sum of this new function’s weights is Proof: recall the weight function w : {−1, 1}n → [0,∞).
Let’s deﬁne a new weight function w(cid:48) : {−1, 1}nk → [0,∞) as 6n with k .
log2 (cid:17) (cid:16) 1−β (cid:113) 6n (cid:88) w(cid:48)(x) (cid:88) (cid:88) x1∈{−1,1}n Z(w(cid:48)) = ··· (cid:88) w(x1) × ··· × (cid:88) xk∈{−1,1}n x∈{−1,1}kn w(x1) × ··· × w(xk) w(xk) xk∈{−1,1}n x1∈{−1,1}n Also note that the largest and smallest non-zero weights in w(cid:48) are w(cid:48) c(cid:48) = (c1, .., ck).
The value δ(w(cid:48), c(cid:48)) for our new weight function is now max = wk = Z(w)k.
max and w(cid:48) min = wk min.
Deﬁne c(cid:48) ∈ {−1, 1}nk as δ(w(cid:48), c(cid:48)) = max {log2 w(x1) + (cid:104)c1, x1(cid:105)} + ··· + max x(cid:48)∈{−1,1}nk xk∈{−1,1}n = max x1∈{−1,1}n {log2 w(cid:48)(x(cid:48)) + (cid:104)c(cid:48), x(cid:48)(cid:105)} {log2 w(xk) + (cid:104)ck, xk(cid:105)} = k¯δk(w).
We lower bound ¯δk(w) by applying the bound from Equation 14 to w(cid:48) (recalling that either w∗(β) = wmin or w∗(β) = wmax) and ﬁnd that ¯δk(w) = δ(w(cid:48), c(cid:48)) ≥ log2 w∗(β)k nk log2 (1 − β) + log2 Z(w)k − log2 w∗(β)k = log2 w∗(β) + n log2 (1 − β) + log2 Z(w) − log2 w∗(β) k log2 (cid:17) (cid:16) 1−β (cid:17) (cid:16) 1−β log2 Similarly, for the upper bound on ¯δk(w) we apply the bound from Equation 8 to w(cid:48) and ﬁnd that nk log2 w∗(λ, γ) + λ log2 w∗(λ, γ)k + λ λγ − 1 log2 Z(w)k + log2 Z(w) + δ(w(cid:48), c(cid:48)) λγ − 1 ¯δk(w) = ≤ 1 (cid:18) 1 6nk (cid:114) 6n (cid:19) (cid:114) 6n 6nk
Convolutional networks are able to detect local patterns regardless of their position in the image.
Like patterns in a planar image, patterns on the sphere can move around, but in this case the “move” is a 3D rotation instead of a translation.
In analogy to the planar CNN, we would like to build a network that can detect patterns regardless of how they are rotated over the sphere.
As shown in Figure 1, there is no good way to use translational convolution or cross-correlation1 to analyze spherical signals.
The most obvious approach, then, is to change the deﬁnition of cross- correlation by replacing ﬁlter translations by rotations.
Doing so, we run into a subtle but important difference between the plane and the sphere: whereas the space of moves for the plane (2D translations) is itself isomorphic to the plane, the space of moves for the sphere (3D rotations) is a different, three-dimensional manifold called SO(3)2.
It follows that the result of a spherical correlation (the output feature map) is to be considered a signal on SO(3), not a signal on the sphere, S2.
For this reason, we deploy SO(3) group correlation in the higher layers of a spherical CNN (Cohen and Welling, 2016).
Figure 1: Any planar projec- tion of a spherical signal will re- sult in distortions.
Rotation of a spherical signal cannot be emu- lated by translation of its planar projection.
∗Equal contribution 1Despite the name, CNNs typically use cross-correlation instead of convolution in the forward pass.
In this paper we will generally use the term cross-correlation, or correlation for short.
2To be more precise: although the symmetry group of the plane contains more than just translations, the translations form a subgroup that acts on the plane.
In the case of the sphere there is no coherent way to deﬁne a composition for points on the sphere, and so the sphere cannot act on itself (it is not a group).
For this reason, we must consider the whole of SO(3).
Published as a conference paper at ICLR 2018 The implementation of a spherical CNN (S2-CNN) involves two major challenges.
Whereas a square grid of pixels has discrete translation symmetries, no perfectly symmetrical grids for the sphere exist.
This means that there is no simple way to deﬁne the rotation of a spherical ﬁlter by one pixel.
Instead, in order to rotate a ﬁlter we would need to perform some kind of interpolation.
The other challenge is computational efﬁciency; SO(3) is a three-dimensional manifold, so a naive implementation of SO(3) correlation is O(n6).
We address both of these problems using techniques from non-commutative harmonic analysis (Chirikjian and Kyatkin, 2001; Folland, 1995).
This ﬁeld presents us with a far-reaching generalization of the Fourier transform, which is applicable to signals on the sphere as well as the rotation group.
It is known that the SO(3) correlation satisﬁes a Fourier theorem with respect to the SO(3) Fourier transform, and the same is true for our deﬁnition of S2 correlation.
Hence, the S2 and SO(3) correlation can be implemented efﬁciently using generalized FFT algorithms.
Because we are the ﬁrst to use cross-correlation on a continuous group inside a multi-layer neural network, we rigorously evaluate the degree to which the mathematical properties predicted by the continuous theory hold in practice for our discretized implementation.
Furthermore, we demonstrate the utility of spherical CNNs for rotation invariant classiﬁcation and regression problems by experiments on three datasets.
First, we show that spherical CNNs are much better at rotation invariant classiﬁcation of Spherical MNIST images than planar CNNs. Second, we use the CNN for classifying 3D shapes.
In a third experiment we use the model for molecular energy regression, an important problem in computational chemistry.
CONTRIBUTIONS The main contributions of this work are the following: 1.
The theory of spherical CNNs. 2.
The ﬁrst automatically differentiable implementation of the generalized Fourier transform for S2 and SO(3).
Our PyTorch code is easy to use, fast, and memory efﬁcient.
3.
The ﬁrst empirical support for the utility of spherical CNNs for rotation-invariant learning problems.
2 RELATED WORK It is well understood that the power of CNNs stems in large part from their ability to exploit (translational) symmetries though a combination of weight sharing and translation equivariance.
It thus becomes natural to consider generalizations that exploit larger groups of symmetries, and indeed this has been the subject of several recent papers by Gens and Domingos (2014); Olah (2014); Dieleman et al.
(2015; 2016); Cohen and Welling (2016); Ravanbakhsh et al.
(2017); Zaheer et al.
(2017b); Guttenberg et al.
(2016); Cohen and Welling (2017).
With the exception of SO(2)-steerable networks (Worrall et al., 2017; Weiler et al., 2017), these networks are all limited to discrete groups, such as discrete rotations acting on planar images or permutations acting on point clouds.
Other very recent work is concerned with the analysis of spherical images, but does not deﬁne an equivariant architecture (Su and Grauman, 2017; Boomsma and Frellsen, 2017).
Our work is the ﬁrst to achieve equivariance to a continuous, non-commutative group (SO(3)), and the ﬁrst to use the generalized Fourier transform for fast group correlation.
A preliminary version of this work appeared as Cohen et al.
(2017).
To efﬁciently perform cross-correlations on the sphere and rotation group, we use generalized FFT algorithms.
Generalized Fourier analysis, sometimes called abstract- or noncommutative harmonic analysis, has a long history in mathematics and many books have been written on the subject (Sugiura, 1990; Taylor, 1986; Folland, 1995).
For a good engineering-oriented treatment which covers generalized FFT algorithms, see (Chirikjian and Kyatkin, 2001).
Other important works include (Driscoll and Healy, 1994; Healy et al., 2003; Potts et al., 1998; Kunis and Potts, 2003; Drake et al., 2008; Maslen, 1998; Rockmore, 2004; Kostelec and Rockmore, 2007; 2008; Potts et al., 2009; Makadia et al., 2007; Gutman et al., 2008).
Published as a conference paper at ICLR 2018 3 CORRELATION ON THE SPHERE AND ROTATION GROUP We will explain the S2 and SO(3) correlation by analogy to the classical planar Z2 correlation.
The planar correlation can be understood as follows: The value of the output feature map at translation x ∈ Z2 is computed as an inner product between the input feature map and a ﬁlter, shifted by x.
Similarly, the spherical correlation can be understood as follows: The value of the output feature map evaluated at rotation R ∈ SO(3) is computed as an inner product between the input feature map and a ﬁlter, rotated by R.
Because the output feature map is indexed by a rotation, it is modelled as a function on SO(3).
We will discuss this issue in more detail shortly.
The above deﬁnition refers to various concepts that we have not yet deﬁned mathematically.
In what follows, we will go through the required concepts one by one and provide a precise deﬁnition.
Our goal for this section is only to present a mathematical model of spherical CNNs. Generalized Fourier theory and implementation details will be treated later.
The Unit Sphere S2 can be deﬁned as the set of points x ∈ R3 with norm 1.
It is a two-dimensional manifold, which can be parameterized by spherical coordinates α ∈ [0, 2π] and β ∈ [0, π].
Spherical Signals We model spherical images and ﬁlters as continuous functions f : S2 → RK, where K is the number of channels.
Rotations The set of rotations in three dimensions is called SO(3), the “special orthogonal group”.
Rotations can be represented by 3 × 3 matrices that preserve distance (i.e. ||Rx|| = ||x||) and orientation (det(R) = +1).
If we represent points on the sphere as 3D unit vectors x, we can perform a rotation using the matrix-vector product Rx. The rotation group SO(3) is a three-dimensional manifold, and can be parameterized by ZYZ-Euler angles α ∈ [0, 2π], β ∈ [0, π], and γ ∈ [0, 2π].
Rotation of Spherical Signals In order to deﬁne the spherical correlation, we need to know not only how to rotate points x ∈ S2 but also how to rotate ﬁlters (i.e. functions) on the sphere.
To this end, we introduce the rotation operator LR that takes a function f and produces a rotated function LRf by composing f with the rotation R−1: [LRf ](x) = f (R−1x).
Due to the inverse on R, we have LRR(cid:48) = LRLR(cid:48).
Inner products The inner product on the vector space of spherical signals is deﬁned as: (1) (2) K(cid:88) S2 (cid:104)ψ, f(cid:105) = (cid:90) S2 f (Rx)dx =(cid:82) (cid:90) (cid:90) (cid:104)LRψ, f(cid:105) = ψk(x)fk(x)dx, k=1 of the measure ensures that(cid:82) The integration measure dx denotes the standard rotation invariant integration measure on the sphere, which can be expressed as dα sin(β)dβ/4π in spherical coordinates (see Appendix A).
The invariance S2 f (x)dx, for any rotation R ∈ SO(3).
That is, the volume under a spherical heightmap does not change when rotated.
Using this fact, we can show that K(cid:88) LR−1 is adjoint to LR, which implies that LR is unitary: K(cid:88) ψk(R−1x)fk(x)dx (3) k=1 S2 ψk(x)fk(Rx)dx S2 = (cid:104)ψ, LR−1 f(cid:105).
k=1 Spherical Correlation With these ingredients in place, we are now ready to state mathematically what was stated in words before.
For spherical signals f and ψ, we deﬁne the correlation as: [ψ (cid:63) f ](R) = (cid:104)LRψ, f(cid:105) = ψk(R−1x)fk(x)dx.
(4) (cid:90) K(cid:88) S2 k=1 Published as a conference paper at ICLR 2018 As mentioned before, the output of the spherical correlation is a function on SO(3).
This is perhaps somewhat counterintuitive, and indeed the conventional deﬁnition of spherical convolution gives as output a function on the sphere.
However, as shown in Appendix B, the conventional deﬁnition effectively restricts the ﬁlter to be circularly symmetric about the Z axis, which would greatly limit the expressive capacity of the network.
Rotation of SO(3) Signals We deﬁned the rotation operator LR for spherical signals (eq.
1), and used it to deﬁne spherical cross-correlation (eq.
4).
To deﬁne the SO(3) correlation, we need to generalize the rotation operator so that it can act on signals deﬁned on SO(3).
As we will show, naively reusing eq.
1 is the way to go.
That is, for f : SO(3) → RK, and R, Q ∈ SO(3): [LRf ](Q) = f (R−1Q).
(5) Note that while the argument R−1x in Eq. 1 denotes the rotation of x ∈ S2 by R−1 ∈ SO(3), the analogous term R−1Q in Eq. 5 denotes to the composition of rotations (i.e. matrix multiplication).
Rotation Group Correlation Using the same analogy as before, we can deﬁne the correlation of two signals on the rotation group, f, ψ : SO(3) → RK, as follows: (cid:90) K(cid:88) [ψ (cid:63) f ](R) = (cid:104)LRψ, f(cid:105) = ψk(R−1Q)fk(Q)dQ.
(6) SO(3) k=1 The integration measure dQ is the invariant measure on SO(3), which may be expressed in ZYZ-Euler angles as dα sin(β)dβdγ/(8π2) (see Appendix A).
Equivariance As we have seen, correlation is deﬁned in terms of the rotation operator LR.
This operator acts naturally on the input space of the network, but what justiﬁcation do we have for using it in the second layer and beyond?
The justiﬁcation is provided by an important property, shared by all kinds of convolution and correlation, called equivariance.
A layer Φ is equivariant if Φ ◦ LR = TR ◦ Φ, for some operator TR.
Using the deﬁnition of correlation and the unitarity of LR, showing equivariance is a one liner: [ψ (cid:63) [LQf ]](R) = (cid:104)LRψ, LQf(cid:105) = (cid:104)LQ−1Rψ, f(cid:105) = [ψ (cid:63) f ](Q−1R) = [LQ[ψ (cid:63) f ]](R).
(7) The derivation is valid for spherical correlation as well as rotation group correlation.
4 FAST SPHERICAL CORRELATION WITH G-FFT It is well known that correlations and convolutions can be computed efﬁciently using the Fast Fourier Transform (FFT).
This is a result of the Fourier theorem, which states that (cid:91)f ∗ ψ = ˆf · ˆψ.
Since the FFT can be computed in O(n log n) time and the product · has linear complexity, implementing the correlation using FFTs is asymptotically faster than the naive O(n2) spatial implementation.
For functions on the sphere and rotation group, there is an analogous transform, which we will refer to as the generalized Fourier transform (GFT) and a corresponding fast algorithm (GFFT).
This transform ﬁnds it roots in the representation theory of groups, but due to space constraints we will not go into details here and instead refer the interested reader to Sugiura (1990) and Folland (1995).
Conceptually, the GFT is nothing more than the linear projection of a function onto a set of orthogonal basis functions called “matrix element of irreducible unitary representations”.
For the circle (S1) or line (R), these are the familiar complex exponentials exp(inθ).
For SO(3), we have the Wigner D- mn(R) indexed by l ≥ 0 and −l ≤ m, n ≤ l.
For S2, these are the spherical harmonics3 functions Dl m(x) indexed by l ≥ 0 and −l ≤ m ≤ l.
Y l Denoting the manifold (S2 or SO(3)) by X and the corresponding basis functions by U l (which is either vector-valued (Y l) or matrix-valued (Dl)), we can write the GFT of a function f : X → R as ˆf l = f (x)U l(x)dx.
(8) (cid:90) 3Technically, S2 is not a group and therefore does not have irreducible representations, but it is a quotient of groups SO(3)/ SO(2) and we have the relation Y l m = Dl m0|S2 Published as a conference paper at ICLR 2018 b(cid:88) l=0 l(cid:88) l(cid:88) m=−l n=−l This integral can be computed efﬁciently using a GFFT algorithm (see Section 4.1).
The inverse SO(3) Fourier transform is deﬁned as: f (R) = (2l + 1) ˆf l mnU l mn(R), (9) and similarly for S2.
The maximum frequency b is known as the bandwidth, and is related to the resolution of the spatial grid (Kostelec and Rockmore, 2007).
Using the well-known (in fact, deﬁning) property of the Wigner D-functions that Dl(R)Dl(R(cid:48)) = Dl(RR(cid:48)) and Dl(R−1) = Dl(R)†, it can be shown (see Appendix D) that the SO(3) correlation satisﬁes a Fourier theorem4: (cid:91)ψ (cid:63) f = ˆf · ˆψ†, where · denotes matrix multiplication of the two block matrices ˆf and ˆψ†.
m0|S2, one can derive an analogous S2 convolu- Similarly, using Y (Rx) = D(R)Y (x) and Y l tion theorem: (cid:91)ψ (cid:63) f = ˆf l · ˆψl†, where ˆf l and ˆψl are now vectors.
This says that the SO(3)-FT of the S2 correlation of two spherical signals can be computed by taking the outer product of the S2-FTs of the signals.
This is shown in ﬁgure 2.
m = Dl Figure 2: Spherical correlation in the spectrum.
The signal f and the locally-supported ﬁlter ψ are Fourier transformed, block-wise tensored, summed over input channels, and ﬁnally inverse transformed.
Note that because the ﬁlter is locally supported, it is faster to use a matrix multiplication (DFT) than an FFT algorithm for it.
We parameterize the sphere using spherical coordinates α, β, and SO(3) with ZYZ-Euler angles α, β, γ.
4.1 IMPLEMENTATION OF G-FFT AND SPECTRAL G-CONV Here we sketch the implementation of GFFTs. For details, see (Kostelec and Rockmore, 2007).
The input of the SO(3) FFT is a spatial signal f on SO(3), sampled on a discrete grid and stored as a 3D array.
The axes correspond to the ZYZ-Euler angles α, β, γ.
The ﬁrst step of the SO(3)-FFT is to perform a standard 2D translational FFT over the α and γ axes.
The FFT’ed axes correspond to the m, n axes of the result.
The second and last step is a linear contraction of the β axis of the FFT’ed array with a precomputed array of samples from the Wigner-d (small-d) functions dl mn(β).
Because the shape of dl depends on l (it is (2l + 1)× (2l + 1)), this linear contraction is implemented mn for l ≥ n, m ≥ −l and as a custom GPU kernel.
The output is a set of Fourier coefﬁcients ˆf l l = 0, .
.
.
, Lmax.
The algorithm for the S2-FFTs is very similar, only in this case we FFT over the α axis only, and do a linear contraction with precomputed Legendre functions over the β axis.
Our code is available at https://github.com/jonas-koehler/s2cnn.
5 EXPERIMENTS In a ﬁrst sequence of experiments, we evaluate the numerical stability and accuracy of our algorithm.
In a second sequence of experiments, we showcase that the new cross-correlation layers we have 4This result is valid for real functions.
For complex functions, conjugate ψ on the left hand side.
SO(3) IFFTS² FFTS² DFTPublished as a conference paper at ICLR 2018 introduced are indeed useful building blocks for several real problems involving spherical signals.
Our examples for this are recognition of 3D shapes and predicting the atomization energy of molecules.
5.1 EQUIVARIANCE ERROR ReLU & 1 layer 1.5 0.5 no act.
& res.
of 30 10−6 no act.
& 1 layer In this paper we have presented the ﬁrst in- stance of a group equivariant CNN for a con- tinuous, non-commutative group.
In the dis- crete case, one can prove that the network is exactly equivariant, but although we can prove [LRf ]∗ ψ = LR[f ∗ ψ] for continuous functions f and ψ on the sphere or rotation group, this is not exactly true for the discretized version that we actually compute.
Hence, it is reasonable to ask if there are any signiﬁcant discretization ar- tifacts and whether they affect the equivariance properties of the network.
If equivariance can not be maintained for many layers, one may ex- pect the weight sharing scheme to become much less effective.
We ﬁrst tested the equivariance of the SO(3) correlation at various resolutions b.
We do this by ﬁrst sampling n = 500 ran- (cid:80)n dom rotations Ri as well as n feature maps fi with K = 10 channels.
Then we i=1 std(LRi Φ(fi) − compute ∆ = 1 Φ(LRifi))/ std(Φ(fi)), where Φ is a composi- tion of SO(3) correlation layers with randomly initialized ﬁlters.
In case of perfect equivariance, we expect this quantity to be zero.
The results (ﬁgure 3 (top)), show that although the approximation error ∆ grows with the resolution and the number of layers, it stays manageable for the range of resolutions of interest.
We repeat the experiment with ReLU activation function after each correlation operation.
As shown in ﬁgure 3 (bottom), the error is higher but stays ﬂat.
This indicates that the error is not due to the network layers, but due to the feature map rotation, which is exact only for bandlimited functions.
Figure 3: ∆ as a function of the resolution and the number of layers.
20 10 resolution 0.5 0.4 0.3 0.2 0.1 ReLU & res.
of 30 layers 30 10 5.2 ROTATED MNIST ON THE SPHERE In this experiment we evaluate the generalization performance with respect to rotations of the input.
For testing we propose a version MNIST dataset projected on the sphere (see ﬁg.
4).
We created two instances of this dataset: one in which each digit is projected on the northern hemisphere and one in which each projected digit is additionally randomly rotated.
Architecture and Hyperparameters As a baseline model, we use a simple CNN with lay- ers conv-ReLU-conv-ReLU-FC-softmax, with ﬁlters of size 5 × 5, k = 57, 114, 10 chan- nels, and stride 3 in both layers.
We compare to a spherical CNN with layers S2conv-ReLU- SO(3)conv-ReLU-FC-softmax, bandwidth b = 30, 10, 5 and k = 100, 200, 10 channels.
Both models have about 165K parameters.
Figure 4: Two MNIST digits projected onto the sphere using stereographic projection.
Mapping back to the plane results in non-linear distortions.
Results We trained each model on the non- rotated (NR) and the rotated (R) training set and evaluated it on the non-rotated and rotated test set.
See table 1.
While the planar CNN achieves high accuracy in the NR / NR regime, its performance in the R / R regime is much worse, while the spherical CNN is unaffected.
When trained on the Published as a conference paper at ICLR 2018 Figure 5: The ray line is cast from the surface of the sphere in direction of its center.
The ﬁrst intersection with the model gives the values of the signal on the sphere.
The two images of the right represent two spherical signals in (α, β) representation.
They contain respectively the distance from the sphere and the cosine of the ray with the normal of the model.
The red dot corresponds to the pixel set by the red line.
non-rotated dataset and evaluated on the rotated dataset (NR / R), the planar CNN does no better than random chance.
The spherical CNN shows a slight decrease in performance compared to R/R, but still performs quite well.
planar spherical NR / NR R / R NR / R 0.09 0.85 0.99 0.91 0.45 0.91 Table 1: Test accuracy for the networks evaluated on the spherical MNIST dataset.
Here R = rotated, NR = non-rotated and X / Y denotes, that the network was trained on X and evaluated on Y.
5.3 RECOGNITION OF 3D SHAPES Next, we applied S2CNN to 3D shape classiﬁcation.
The SHREC17 task (Savva et al., 2017) contains 51300 3D models taken from the ShapeNet dataset (Chang et al., 2015) which have to be classiﬁed into 55 common categories (tables, airplanes, persons, etc.).
There is a consistently aligned regular dataset and a version in which all models are randomly perturbed by rotations.
We concentrate on the latter to test the quality of our rotation equivariant representations learned by S2CNN.
Representation We project the 3D meshes onto an enclosing sphere using a straightforward ray casting scheme (see Fig.
5).
For each point on the sphere we send a ray towards the origin and collect 3 types of information from the intersection: ray length and cos / sin of the surface angle.
We further augment this information with ray casting information for the convex hull of the model, which in total gives us 6 channels for the signal.
This signal is discretized using a Driscoll-Healy grid (Driscoll and Healy, 1994) with bandwidth b = 128.
Ignoring non-convexity of surfaces we assume this projection captures enough information of the shape to be useful for the recognition task.
Architecture and Hyperparameters Our network consists of an initial S2conv-BN-ReLU block followed by two SO(3)conv-BN-ReLU blocks.
The resulting ﬁlters are pooled using a max pooling layer followed by a last batch normalization and then fed into a linear layer for the ﬁnal classiﬁcation.
It is important to note that the the max pooling happens over the group SO(3): if fk is the k-th ﬁlter in the ﬁnal layer (a function on SO(3)) the result of the pooling is maxx∈SO(3) fk(x).
We used 50, 70, and 350 features for the S2 and the two SO(3) layers, respectively.
Further, in each layer we reduce the resolution b, from 128, 32, 22 to 7 in the ﬁnal layer.
Each ﬁlter kernel ψ on SO(3) has non-local support, where ψ(α, β, γ) (cid:54)= 0 iff β = π 2 and γ = 0 and the number of points of the discretization is proportional to the bandwidth in each layer.
The ﬁnal network contains ≈ 1.4M parameters, takes 8GB of memory at batch size 16, and takes 50 hours to train.
ray castingfrom the sphere to the origindistance sphere-impactnormal at impactPublished as a conference paper at ICLR 2018 P@N Method 0.705 Tatsuma_ReVGG 0.814 Furuya_DLAN SHREC16-Bai_GIFT 0.678 Deng_CM-VGG5-6DB 0.412 Ours 0.701 (3rd) R@N 0.769 0.683 0.667 0.706 0.711 (2nd) F1@N 0.719 0.706 0.661 0.472 0.699 (3rd) mAP 0.696 0.656 0.607 0.524 0.676 (2nd) NDCG 0.783 0.754 0.735 0.624 0.756 (2nd) Table 2: Results and best competing methods for the SHREC17 competition.
Results We evaluated our trained model using the ofﬁcial metrics and compared to the top three competitors in each category (see table 2 for results).
Except for precision and F1@N, in which our model ranks third, it is the runner up on each other metric.
The main competitors, Tatsuma_ReVGG and Furuya_DLAN use input representations and network architectures that are highly specialized to the SHREC17 task.
Given the rather task agnostic architecture of our model and the lossy input representation we use, we interpret our models performance as strong empirical support for the effectiveness of Spherical CNNs. 5.4 PREDICTION OF ATOMIZATION ENERGIES FROM MOLECULAR GEOMETRY Finally, we apply S2CNN on molecular energy regression.
In the QM7 task (Blum and Reymond, 2009; Rupp et al., 2012) the atomization energy of molecules has to be predicted from geometry and charges.
Molecules contain up to N = 23 atoms of T = 5 types (H, C, N, O, S).
They are given as a list of positions pi and charges zi for each atom i.
Representation by Coulomb matrices Rupp et al.
(2012) propose a rotation and translation invariant representation of molecules by deﬁning the Coulomb matrix C ∈ RN×N (CM).
For each pair of atoms i (cid:54)= j they set Cij = (zizj)/(|pi − pj|) and Cii = 0.5z2.4 .
Diagonal elements encode the atomic energy by nuclear charge, while other elements encode Coulomb repulsion between atoms.
This representation is not permutation invariant.
To this end Rupp et al.
(2012) propose a distance measure between Coulomb matrices used within Gaussian kernels whereas Montavon et al.
(2012) propose sorting C or random sampling index permutations.
functions Uz(x) =(cid:80) Representation as a spherical signal We utilize spherical symmetries in the geometry by deﬁning a sphere Si around around pi for each atom i.
The radius is kept uniform across atoms and molecules and chosen minimal such that no intersections among spheres in the training set happen.
Generalizing the Coulomb matrix approach we deﬁne for each possible z and for each point x on Si potential zi·z |x−pi| producing a T channel spherical signal for each atom in the molecule (see ﬁgure 6).
This representation is invariant with respect to translations and equivariant with respect to rotations.
However, it is still not permutation invariant.
The signal is discretized using a Driscoll-Healy (Driscoll and Healy, 1994) grid with bandwidth b = 10 representing the molecule as a sparse N × T × 2b × 2b tensor.
j(cid:54)=i,zj =z Architecture and Hyperparameters We use a deep ResNet style S2CNN.
Each ResNet block is made of S2/SO(3)conv-BN-ReLU-SO(3)conv-BN after which the input is added to the result.
We share weights among atoms making ﬁlters permutation invariant, by pushing the atom dimension into the batch dimension.
In each layer we downsample the bandwidth, while increasing the number of features F .
After integrating the signal over SO(3) each molecule becomes a N × F tensor.
For permutation invariance over atoms we follow Zaheer et al.
(2017a) and embed each resulting feature vector of an atom into a latent space using a MLP φ.
Then we sum these latent representations over the atom dimension and get our ﬁnal regression value for the molecule by mapping with another MLP ψ.
Both φ and ψ are jointly optimized.
Training a simple MLP only on the 5 frequencies of atom types in a molecule already gives a RMSE of ∼ 19.
Thus, we train the S2CNN on the residual only, which improved convergence speed and stability over direct training.
The ﬁnal architecture is sketched in table 3.
It has about 1.4M parameters, consumes 7GB of memory at batch size 20, and takes 3 hours to train.
Published as a conference paper at ICLR 2018 Figure 6: The ﬁve potential channels Uz with z ∈ {1, 6, 7, 8, 16} for a molecule containing atoms H (red), C (green), N (orange), O (brown), S (gray).
Method Author RMSE MLP / random CM (a) LGIKA(RF) (b) RBF kernels / random CM (a) RBF kernels / sorted CM (a) MLP / sorted CM (a) Ours 5.96 10.82 11.40 12.59 16.06 8.47 S2CNN Layer Input ResBlock ResBlock ResBlock ResBlock ResBlock DeepSet Layer φ (MLP) ψ (MLP) Features 20 40 60 80 160 Bandwidth 10 Input/Hidden 160/150 100/50 Table 3: Left: Experiment results for the QM7 task: (a) Montavon et al.
(2012) (b) Raj et al.
(2016).
Right: ResNet architecture for the molecule task.
Results We evaluate by RMSE and compare our results to Montavon et al.
(2012) and Raj et al.
(2016) (see table 3).
Our learned representation outperforms all kernel-based approaches and a MLP trained on sorted Coulomb matrices.
Superior performance could only be achieved for an MLP trained on randomly permuted Coulomb matrices.
However, sufﬁcient sampling of random permutations grows exponentially with N, so this method is unlikely to scale to large molecules.
6 DISCUSSION & CONCLUSION In this paper we have presented the theory of Spherical CNNs and evaluated them on two important learning problems.
We have deﬁned S2 and SO(3) cross-correlations, analyzed their properties, and implemented a Generalized FFT-based correlation algorithm.
Our numerical results conﬁrm the stability and accuracy of this algorithm, even for deep networks.
Furthermore, we have shown that Spherical CNNs can effectively generalize across rotations, and achieve near state-of-the-art results on competitive 3D Model Recognition and Molecular Energy Regression challenges, without excessive feature engineering and task-tuning.
For intrinsically volumetric tasks like 3D model recognition, we believe that further improvements can be attained by generalizing further beyond SO(3) to the roto-translation group SE(3).
The development of Spherical CNNs is an important ﬁrst step in this direction.
Another interesting generalization is the development of a Steerable CNN for the sphere (Cohen and Welling, 2017), which would make it possible to analyze vector ﬁelds such as global wind directions, as well as other sections of vector bundles over the sphere.
Perhaps the most exciting future application of the Spherical CNN is in omnidirectional vision.
Although very little omnidirectional image data is currently available in public repositories, the increasing prevalence of omnidirectional sensors in drones, robots, and autonomous cars makes this a very compelling application of our work.
Published as a conference paper at ICLR 2018 REFERENCES L.
C.
Blum and J.-L.
Reymond.
970 million druglike small molecules for virtual screening in the chemical universe database GDB-13.
J.
Am. Chem.
Soc., 131:8732, 2009.
W.
Boomsma and J.
Frellsen.
Spherical convolutions and their application in molecular modelling.
In I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, and R Garnett, editors, Advances in Neural Information Processing Systems 30, pages 3436–3446.
Curran Associates, Inc., 2017.
A.X. Chang, T.
Funkhouser, L.
Guibas, P.
Hanrahan, Q.
Huang, Z.
Li, S.
Savarese, M.
Savva, S.
Song, H.
Su, et al.
Shapenet: An information-rich 3d model repository.
arXiv preprint arXiv:1512.03012, 2015.
G.S. Chirikjian and A.B. Kyatkin.
Engineering Applications of Noncommutative Harmonic Analysis.
CRC Press, 1 edition, may 2001.
ISBN 9781420041767.
T.S. Cohen and M.
Welling.
Group equivariant convolutional networks.
In Proceedings of The 33rd International Conference on Machine Learning (ICML), volume 48, pages 2990–2999, 2016.
T.S. Cohen and M.
Welling.
Steerable CNNs. In ICLR, 2017.
T.S. Cohen, M.
Geiger, J.
Koehler, and M.
Welling.
Convolutional networks for spherical signals.
In ICML Workshop on Principled Approaches to Deep Learning, 2017.
S.
Dieleman, K.
W.
Willett, and J.
Dambre.
Rotation-invariant convolutional neural networks for galaxy morphology prediction.
Monthly Notices of the Royal Astronomical Society, 450(2), 2015.
S.
Dieleman, J.
De Fauw, and K.
Kavukcuoglu.
Exploiting Cyclic Symmetry in Convolutional Neural Networks.
In International Conference on Machine Learning (ICML), 2016.
J.B. Drake, P.H. Worley, and E.F. D’Azevedo.
Algorithm 888: Spherical harmonic transform algorithms.
ACM Trans.
Math.
Softw., 35(3):23:1–23:23, 2008.
doi: 10.1145/1391989.1404581.
J.R. Driscoll and D.M. Healy.
Computing Fourier transforms and convolutions on the 2-sphere.
Advances in applied mathematics, 1994.
G.B. Folland.
A Course in Abstract Harmonic Analysis.
CRC Press, 1995.
R.
Gens and P.
Domingos.
Deep Symmetry Networks.
In Advances in Neural Information Processing Systems (NIPS), 2014.
B.
Gutman, Y.
Wang, T.
Chan, P.M. Thompson, and others.
Shape registration with spherical cross correlation.
2nd MICCAI workshop, 2008.
N.
Guttenberg, N.
Virgo, O.
Witkowski, H.
Aoki, and R.
Kanai.
Permutation-equivariant neural networks applied to dynamics prediction.
2016.
D.
Healy, D.
Rockmore, P.
Kostelec, and S.
Moore.
FFTs for the 2-Sphere – Improvements and Variations.
The journal of Fourier analysis and applications, 9(4):340–385, 2003.
P.J. Kostelec and D.N. Rockmore.
SOFT: SO(3) Fourier Transforms.
2007.
URL http://www.
cs.dartmouth.edu/~geelong/soft/soft20_fx.pdf.
P.J. Kostelec and D.N. Rockmore.
FFTs on the rotation group.
Journal of Fourier Analysis and Applications, 14(2):145–179, 2008.
S.
Kunis and D.
Potts.
Fast spherical Fourier algorithms.
Journal of Computational and Applied Mathematics, 161:75–98, 2003.
A.
Makadia, C.
Geyer, and K.
Daniilidis.
Correspondence-free structure from motion.
Int.
J.
Comput.
Vis., 75(3):311–327, December 2007.
D.K. Maslen.
Efﬁcient Computation of Fourier Transforms on Compact Groups.
Journal of Fourier Analysis and Applications, 4(1), 1998.
10 Published as a conference paper at ICLR 2018 G.
Montavon, K.
Hansen, S.
Fazli, M.
Rupp, F.
Biegler, A.
Ziehe, A.
Tkatchenko, O.A. von Lilienfeld, and K.
Müller.
Learning invariant representations of molecules for atomization energy prediction.
In P.
Bartlett, F.C.N. Pereira, C.J.C. Burges, L.
Bottou, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 449–457.
2012.
L.
Nachbin.
The Haar Integral.
1965.
C.
Olah.
Groups and Group Convolutions, 2014.
URL https://colah.github.io/posts/ 2014-12-Groups-Convolution/.
D.
Potts, G.
Steidl, and M.
Tasche.
Fast and stable algorithms for discrete spherical Fourier transforms.
Linear Algebra and its Applications, 275:433–450, 1998.
D.
Potts, J.
Prestin, and A.
Vollrath.
A fast algorithm for nonequispaced Fourier transforms on the rotation group.
Numerical Algorithms, pages 1–28, 2009.
A.
Raj, A.
Kumar, Y.
Mroueh, P.T. Fletcher, et al.
Local group invariant representations via orbit embeddings.
arXiv preprint arXiv:1612.01988, 2016.
S.
Ravanbakhsh, J.
Schneider, and B.
Poczos.
Deep learning with sets and point clouds.
International Conference on Learning Representations (ICLR) – workshop track, 2017.
In D.N. Rockmore.
Recent Progress and Applications in Group FFTS.
NATO Science Series II: Mathematics, Physics and Chemistry, 136:227–254, 2004.
M.
Rupp, A.
Tkatchenko, K.-R.
Müller, and O.
A.
von Lilienfeld.
Fast and accurate modeling of molecular atomization energies with machine learning.
Physical Review Letters, 108:058301, 2012.
M.
Savva, F.
Yu, H.
Su, A.
Kanezaki, T.
Furuya, R.
Ohbuchi, Z.
Zhou, R.
Yu, S.
Bai, X.
Bai, M.
Aono, A.
Tatsuma, S.
Thermos, A.
Axenopoulos, G.
Th. Papadopoulos, P.
Daras, X.
Deng, Z.
Lian, B.
Li, H.
Johan, Y.
Lu, and S.
Mk. Large-Scale 3D Shape Retrieval from ShapeNet Core55.
In Ioannis Pratikakis, Florent Dupont, and Maks Ovsjanikov, editors, Eurographics Workshop on 3D Object Retrieval.
The Eurographics Association, 2017.
ISBN 978-3-03868-030-7.
doi: 10.2312/3dor.20171050.
Y.C. Su and K.
Grauman.
Learning spherical convolution for fast features from 360 imagery.
Adv.
Neural Inf.
Process.
Syst., 2017.
M.
Sugiura.
Unitary Representations and Harmonic Analysis.
John Wiley & Sons, New York, London, Sydney, Toronto, 2nd edition, 1990.
M.E. Taylor.
Noncommutative Harmonic Analysis.
American Mathematical Society, 1986.
ISBN 0821815237.
M.
Weiler, F.A. Hamprecht, and M.
Storath.
Learning steerable ﬁlters for rotation equivariant CNNs. 2017.
D.E. Worrall, S.J. Garbin, D.
Turmukhambetov, and G.J. Brostow.
Harmonic networks: Deep translation and rotation equivariance.
In CVPR, 2017.
M.
Zaheer, S.
Kottur, S.
Ravanbakhsh, B.
Poczos, R.
Salakhutdinov, and A.
Smola.
Deep sets.
arXiv preprint arXiv:1703.06114, 2017a.
M.
Zaheer, S.
Kottur, S.
Ravanbakhsh, B.
Poczos, R.R. Salakhutdinov, and A.J. Smola.
Deep sets.
In Advances in Neural Information Processing Systems 30, pages 3393–3403, 2017b.
11 Published as a conference paper at ICLR 2018 APPENDIX A: PARAMETERIZATION OF AND INTEGRATION ON S2 AND SO(3) We use the ZYZ Euler parameterization for SO(3).
An element R ∈ SO(3) is written as R = R(α, β, γ) = Z(α)Y (β)Z(γ), (10) where α ∈ [0, 2π], β ∈ [0, π] and γ ∈ [0, 2π], and Z resp.
Y are rotations around the Z and Y axes.
Using this parameterization, the normalized Haar measure is dγ 2π dβ sin(β) dR = We have (cid:82) is sometimes called the invariant measure because it has the property that(cid:82) (cid:82) SO(3) f (R)dR (this is analogous to the more familiar property(cid:82) SO(3) dR = 1.
The Haar measure (Nachbin, 1965; Chirikjian and Kyatkin, 2001) SO(3) f (R(cid:48)R)dR = R f (x)dx for R f (x + y)dx = (cid:82) dα 2π (11) functions on the line).
This invariance property allows us to do many useful substitutions.
We have a related parameterization for the sphere.
An element x ∈ S2 is written x(α, β) = Z(α)Y (β)n (12) where n is the north pole.
This parameterization makes explicit the fact that the sphere is a quotient S2 = SO(3)/ SO(2), where H = SO(2) is the subgroup of rotations around the Z axis.
Elements of this subgroup H leave the north pole invariant, and have the form Z(γ).
The point x(α, β) ∈ S2 is associated with the coset representative ¯x = R(α, β, 0) ∈ SO(3).
This element represents the coset ¯xH = {R(α, β, γ)|γ ∈ [0, 2π]}.
The normalized Haar measure for the sphere is dx = dα 2π dβ sin β The normalized Haar measure for SO(2) is dh = dγ 2π (13) (14) So we have dR = dx dh, again reﬂecting the quotient structure.
We can think of a function on S2 as a γ-invariant function on SO(3).
Given a function f : S2 → C we associate the function ¯f (α, β, γ) = f (α, β).
When using normalized Haar measures, we have: (cid:90) 2π (cid:90) SO(3) (cid:90) 2π (cid:90) 2π (cid:90) 2π (cid:90) π (cid:90) π (cid:90) π dα f (x)dx ¯f (R)dR = 8π2 8π2 4π (cid:90) dα sin βdβ dγ ¯f (α, β, γ) sin βdβf (α, β) (cid:90) 2π dα sin βdβf (α, β) dγ (15) This will allow us to deﬁne the Fourier transform on S2 from the Fourier transform on SO(3), by viewing a function on S2 as a γ-invariant function on SO(3) and taking its SO(3)-Fourier transform.
S2 APPENDIX B: CORRELATION & EQUIVARIANCE We have deﬁned the S2 correlation as [ψ (cid:63) f ](R) = (cid:104)LRψ, f(cid:105) = (cid:90) K(cid:88) S2 k=1 12 ψk(R−1x)fk(x)dx.
(16) Published as a conference paper at ICLR 2018 Without loss of generality, we will analyze here the single-channel case K = 1.
This operation is equivariant: (cid:90) (cid:90) (cid:90) S2 S2 (cid:90) [ψ (cid:63) [LQf ]](R) = ψ(R−1x)f (Q−1x)dx ψ(R−1Qx)f (x)dx ψ((Q−1R)−1x)f (x)dx S2 = [ψ (cid:63) f ](Q−1R) = [LQ[ψ (cid:63) f ]](R) (17) A similar derivation can be made for the SO(3) correlation.
The spherical convolution deﬁned by Driscoll and Healy (1994) is: [f ∗ ψ](x) = f (Rn)ψ(R−1x)dR (18) SO(3) where n is the north pole.
Note that in this deﬁnition, the output of the spherical convolution is a function on the sphere, not a function on SO(3) as in our deﬁnition of cross-correlation.
Note further that unlike our deﬁnition, this deﬁnition involves an integral over SO(3).
If we write out the integral in terms of Euler angles, noting that the north-pole n is invariant to Z-axis rotations by γ, i.e. R(α, β, γ)n = Z(α)Y (β)Z(γ)n = Z(α)Y (β)n, we see that this deﬁnition implicitly integrates over γ in only one of the factors (namely ψ), making it invariant wrt γ rotation.
In other words, the ﬁlter is ﬁrst “averaged” (making it circularly symmetric) before it is combined with f (This was observed before by Makadia et al.
(2007)).
We consider this to be much too limited for the purpose of pattern matching in spherical CNNs. APPENDIX C: GENERALIZED FOURIER TRANSFORM With each compact topological group (like SO(3)) is associated a discrete set of orthogonal functions that arise as matrix elements of irreducible unitary representations of these groups.
For the circle (the group SO(2)) these are the complex exponentials (in the complex case) or sinusoids (for real functions).
For SO(3), these functions are known as the Wigner D-functions.
As discussed in the paper, the Wigner D-functions are parameterized by a degree parameter l ≥ 0 and order parameters m, n ∈ [−l, .
.
.
, l].
In other words, we have a set of matrix-valued functions Dl : SO(3) → C(2l+1)×(2l+1).
The Wigner D-functions are orthogonal: (cid:104)Dl mn, Dl(cid:48) m(cid:48)n(cid:48)(cid:105) = (cid:90) 2π (cid:90) π dα 2π dβ sin β (cid:90) 2π dγ 2π mn(α, β, γ)Dl(cid:48) Dl m(cid:48)n(cid:48)(α, β, γ) = δll(cid:48)δmm(cid:48)δnn(cid:48) 2l + 1 (19) Furthermore, they are complete, meaning that any well behaved function f : SO(3) → C can be written as a linear combination of Wigner D-functions.
This is the idea of the Generalized Fourier Transform F on SO(3): f (R) = [F−1 ˆf ](R) = (2l + 1) ˆf l mnDl mn(R) (20) l(cid:88) l(cid:88) m=−l n=−l ∞(cid:88) l=0 13 Published as a conference paper at ICLR 2018 where ˆf l mn are called the Fourier coefﬁcients of f.
Using the orthogonality property of the Wigner D-functions, one can see that the Fourier coefﬁcients can be retrieved by computing the inner product with the Wigner D-functions: [Ff ]l mn = (cid:90) (cid:90) ∞(cid:88) SO(3) SO(3) l(cid:48)=0 = ˆf l mn  ∞(cid:88) l(cid:48)=0 f (R)Dl mn(R)dR l(cid:48)(cid:88) l(cid:48)(cid:88) (2l(cid:48) + 1) l(cid:48)(cid:88) l(cid:48)(cid:88) (cid:90) m(cid:48)=−l(cid:48) n(cid:48)=−l(cid:48)  Dl mn(R)dR ˆf l(cid:48) m(cid:48)n(cid:48)Dl(cid:48) m(cid:48)n(cid:48)(R) (21) (2l(cid:48) + 1) ˆf l(cid:48) m(cid:48)n(cid:48) Dl(cid:48) m(cid:48)n(cid:48)(R)Dl mndR m(cid:48)=−l(cid:48) n(cid:48)=−l(cid:48) SO(3) APPENDIX D: FOURIER THEOREMS Fourier convolution theorems for SO(3) and §2 can be found in Kostelec and Rockmore (2008); Makadia et al.
(2007); Gutman et al.
(2008).
We derive them here for completeness.
To derive the convolution theorems, we will use the deﬁning property of the Wigner D-matrices: that they are (irreducible, unitary) representations of SO(3).
This means that they satisfy: Dl(R)Dl(R(cid:48)) = Dl(RR(cid:48)), (22) for any R, R(cid:48) ∈ SO(3).
Notice that the complex exponentials satisfy an analogous criterion for the circle group S1 ∼= SO(2).
That is, einxeiny = ein(x+y), where x + y is the group operation for SO(2).
Unitarity means that Dl(R)Dl†(R) = I.
Irreducibility means, essentially, that the set of matrices {Dl(R)| R ∈ SO(3)} cannot be simultaneously block-diagonalized.
To derive the Fourier theorem for SO(3), we use the invariance of the integration measure dR: (cid:82) SO(3) f (R(cid:48)R)dR =(cid:82) SO(3) f (R)dR.
With these facts understood, we can proceed to derive: (cid:91)ψ (cid:63) f (cid:90) (cid:90) (cid:90) (cid:90) (cid:90) = ˆf l ˆψl† SO(3) ψ(R−1R(cid:48))f (R(cid:48))dR(cid:48)Dl(R)dR ψ(R−1)f (R(cid:48))Dl(R(cid:48)R)dR(cid:48)dR SO(3) SO(3) SO(3) SO(3) SO(3) (ψ (cid:63) f )(R)Dl(R)dR (cid:90) (cid:90) f (R(cid:48))Dl(R(cid:48))dR(cid:48) (cid:90) f (R(cid:48))Dl(R(cid:48))dR(cid:48) (cid:90) SO(3) ψ(R−1)Dl(R)dR ψ(R)Dl(R) dR SO(3) SO(3) (23) So the SO(3)-Fourier transform of the SO(3) convolution of ψ and f is equal to the matrix product of the SO(3)-Fourier transforms ˆf and ˆψ.
For the sphere, we can derive an analogous transform that is sometimes called the spherical harmonics m : S2 → C are a complete orthogonal family of functions.
transform.
The spherical harmonics Y l The spherical harmonics are related to the Wigner D functions by the relation Dl mn(α, β, γ) = m0(α, β, 0).
m(α, β)einγ, so that Y l Y l m(α, β) = Dl 14 Published as a conference paper at ICLR 2018 The S2 convolution of f1 and f2 is equivalent to the SO(3) convolution of the associated right- invariant functions ¯f1, ¯f2 (see Appendix A): (cid:90) (cid:90) (cid:90) (cid:90) S2 SO(2) f1(R−1x)f2(x)dx f1(R−1x)f2(x)dxdh S2 ¯f1(R−1R(cid:48)) ¯f2(R(cid:48))dR(cid:48) SO(3) = [ ¯f1 (cid:63) ¯f2](R) (cid:90) 2π dγ 2π (cid:90) 2π dβ sin β dβ sin β (cid:90) π dβ sin β ¯f (α, β, γ)Dl mn(α, β, γ) f (α, β) dγ 2π Dl mn(α, β, γ) f (α, β)Dl m0(α, β, 0) [f1 (cid:63) f2](R) = dα 2π (cid:90) 2π (cid:90) 2π (cid:90) 2π (cid:90) dα 2π (cid:90) π (cid:90) π dα 2π = δn0 [F ¯f ]l mn = The Fourier transform of a right invariant function on SO(3) equals (24) (25) = δn0 f (x)Y l m(x)dx S2 So we can think of the S2 Fourier transform of a function on S2 as the n = 0 column of the SO(3) Fourier transform of the associated right-invariant function.
This is a beautiful result that we have not been able to ﬁnd a reference for, though it seems likely that it has been observed before.
15
In a wide range of ﬁelds, from music and advertising recommendations to healthcare and a wide range of other consumer applications, learning users’ personal tendencies and judgements is essential.
Many current approaches demand centralized data storage and computation to aggregate and learn globally.
Such central models, along with features known about a given user, make predictions appear personal to that user.
While such global models have proven to be widely eﬀective, they bring with them inherent conﬂicts with privacy.
User data must leave the device, and training a central model requires regular communication between a given user and the remote model.
Further, if users are in some way truly unique, and exhibit diﬀerence preferences than seemingly similar users, large centralized models may have trouble quickly adapting to this behavior.
With these disadvantages in mind, we present a deﬁnition of personalization that allows for no direct sharing or centralization of user data.
We see personalization as the balance between generalization to global information and specialization to a given user’s quirks and biases.
To make this deﬁnition concrete, we show how a simple baseline model’s performance changes on a sentiment analysis task as a function of user bias, and the way information is shared across models.
We hope this work can contribute to framing the discussion around personalization and provide a metric for evaluating in what ways a model is truly providing a user personal recommendations.
We also discuss related areas such as diﬀerential privacy, and federated learning, which have been motivated by similar considerations.
Our work could easily ﬁt into the frameworks of federated learning or diﬀerential privacy.
2.1. Personalized Models.
There has been a long history of research into personalization within machine learning.
There is a wealth of work on using Bayesian hierarchical models 2.
Related Work Date: February 1, 2018.
2 REUBEN BRASHER, NAT ROTH, JUSTIN WAGLE to learn mixes of user and global parameters from data.
These works have achieved success in areas from health care [8], to recommendation systems [29], to generally dealing with a mix of implicit and explicit feedback [30].
There has also been increasing work on helping practitioners to integrate these Bayesian techniques with deep learning models [24].
Many approaches to personalization within deep learning have relied on combining personal features, hand written or learned, with some more global features to make pre- dictions.
For example, in deep recommender systems, a feature might whether a user is a certain gender, or has seen a certain movie.
A deep model may learn to embed these features, and combine them with some linear model as in [5] in order to make recommenda- tions for a speciﬁc user.
It is also common to learn some vector describing the user end to end for a task, rather than doing this featurization by hand.
In such scenarios your input might be a sentence and a user id and the prediction would be the next sentence as in [2], in which the user is featurized via some learned vector.
Similarly, Park et al [20], learn a vector representation of a user’s context to generate image captions that are personal to the user, and [4] learn a user vector alongside a language model to determine if a set of answers to a question will satisfy a user.
These approaches have the beneﬁt of not requiring any manual description of the important traits of a user.
Here, when we discuss personalization, we focus more on personalization work within deep learning.
In general, deep learning models are large, complicated, and very non-linear.
This makes it hard to reason about how incorporating a new user, or set of training examples will aﬀect the state of the model at large, a phenomenon known as “catastrophic forgetting” [9], a topic which itself has seen a large amount of research [12], [10], [3].
In general, this means that if we add a new user whose behavior is very diﬀerent from our previous training examples, we need to take extra steps to preserve our performance on previous users.
This makes online personalization of models to outlier users an open problem within deep learning.
2.2. Federated Learning.
Our other key personalization constraint is privacy related; to get users to trust a model with extremely personal data, it is our believe that it is becoming increasingly necessary, and even legally mandated, to guarantee them a degree of privacy [21], [19].
Research on federated learning has demonstrated that intelligence from users can be aggregated and centralized models trained without ever directly storing user data in a central location, alleviating part of these privacy concerns.
This research focuses on training models when data is distributed on a very large number of devices, and further assumes each device does not have access to a representative sample of the global data [17], [13].
We therefore believe federating learning is a key part of any personalization strategy.
Federated learning is concerned with training a central model that does well on users globally.
However, the contribution from an individual user tends to be washed out after each update to the global model.
Konecny et al., [13] admit as much, explicitly saying that the issues of personalization are separate from federated learning.
Instead, much of the current research focus on improving communication speed [14] and how to maintain stability between models when communication completely drops or lags greatly [25].
[16] comes closest to our concerns, as it hypothesizes a system in which each user has a personal set of knowledge and some more global mechanism aggregating knowledge from similar users.
They do not propose an exact mechanism for how to do this aggregating and how to determine which users are similar.
We hope to contribute to the conversation on how to best minimally compromise the privacy and decentralization of learning, while not enforcing all models to globally cohere and synchronize.
Finally, it is important to note that federation itself does not guarantee privacy.
While in practice this aggregation of gradients, in the place of storing of raw data, will often obscure some user behavior, it may still leak information about users.
For example, if an attacker observes a non-zero gradient for a feature representing a location, it may be trivial to infer that some of the users in the group live in that location.
Making strong guarantees SOMETIMES YOU WANT TO GO WHERE EVERYBODY KNOWS YOUR NAME about the extent to which data gives us information about individual users is the domain of diﬀerential privacy [7], [1].
In future work, we hope to incorporate these stronger notions of privacy into our discussion as well, but believe that federated learning is a good ﬁrst step towards greater user privacy.
3.
Personalization Definition With these problems in mind, we deﬁne personalization as the relative weighting between performance of a model on a large, multi-user, global dataset, and the performance of that model on data from a single user.
This deﬁnition implies several things.
In particular, the extent to which a model can be personalized depends both on the model itself, and the spread of user behavior.
On a task in which users always behave the same, there is little room for personalization, as a global model trained on all user data will likely be optimal globally and locally.
However, on any task where user behavior varies signiﬁcantly between individuals, it is possible a model trained on all users may perform poorly on any speciﬁc user.
Nonetheless, a speciﬁc user may beneﬁt from some global data; for example, a user with less training data may see better performance if they use a model trained with some global data.
Therefore, the best personalization strategy will have some ability to incorporate global knowledge, will minimally distorting the predictions for a given user.
In addition, we add the constraint that user speciﬁc data be private to a user, and cannot be explicitly shared between models.
In particular, this means that even if all user data is drawn from the same distribution, we cannot simply train on all the data.
Instead we must determine other ways to share this knowledge, such as federating or ensembling user models.
In this paper, we establish some simple benchmarks for evaluating how well a model respects this deﬁnition of personalization.
Formally, suppose we have number of users, N , and for each user we have some user speciﬁc data, {Xi : i = 1 .
.
.
N }, and user speciﬁc models, {Mi : i = 1 .
.
.
N }.
Let the global data be D = S Xi, and we suppose we have a loss function, L , which is a function of both Xi and Mi, Li = L (Xi, Mi).
We deﬁne our success at personalization as: αL (Xi, Mi) + (1 − α) L (D, Mi) , (1) where α is between 0 and 1, and determines how much we weight local user and global data performance.
In the case where Xi follows the same distribution as all D, this deﬁnition trivially collapses to approximately optimizing L (D, Mi), the familiar, non personal objec- tive function on a dataset.
However, as α increases and Xi diverges from D, we introduce a tension between optimizing for the speciﬁc user, while still not ignoring the whole dataset.
Finally, to enforce our deﬁnition of privacy, each model, Mi, has access only to Xi, and the weights of all the other models, Mj for j = 1 .
.
.
N , but does not have access to the other datasets, Mj, j = 1 .
.
.
N , j 6= i.
4.
Personalization Motivation And Implications One question might be why we bother at all with adding global data to the equation, since it is more intuitive to think about personalization as just using the model that does best on a single user’s data, and that data alone.
However, that intuition ignores the fact that we may have only observed a small amount of behavior from any given user.
If we only ﬁt optimally to a speciﬁc user’s data, we risk overﬁtting and performing poorly on new data even from that same user.
A Bayesian interpretation of our deﬁnition is to view the global data term as repre- senting our prior belief about user behavior.
Another interpretation is to view α as how much “catastrophic forgetting” we will allow our model to do in order to personalize to a user.
4 REUBEN BRASHER, NAT ROTH, JUSTIN WAGLE From the Bayesian perspective, the global data serves as a type of regularization that penalizes the local model from moving too far away from prior user data in order to ﬁt to a new user.
We can think about α as a hyperparameter representing the strength of our prior belief.
The smaller α is, the less we allow the model to deviate from the global state.
There may be no perfect rule for choosing α, as it may depend on task, and rate at which we want to adapt to the user.
One strategy could be to slowly increase α for a given user as we observe more data from them.
With this strategy, data rich users will have large α and data poor users will have small α.
Thus data rich users will be penalized less for moving further away from the global state.
This is close to treating our loss as the maximum a posteriori estimate of the users data distribution, as we observe more data.
The rate of changing α could be chosen so as to minimize the loss of our approach on some held out user data, following the normal cross validation strategy for choosing hyperparameters.
Alternatively, we may have domain speciﬁc intuition on how much personalization matters, and α provides an easy way to express this.
From the catastrophic forgetting perspective, our deﬁnition is similar to the work in [3], which penalizes weights from moving away from what they were on a previous task.
That work upweights the penalty for weights that have a high average gradient on the previous task, reasoning that such weights are likely to be most important.
We directly penalize the loss of accuracy on other users, rather than indirectly penalizing that change, as the gradient based approach does.
The indirect approach of [3] has the beneﬁts of being scalable, as it may be expensive to recalculate total global loss and potentially adapting to unlabeled data.
Still, we see a common motivation, as in both cases, we have some weighting for how much we want to allow our model to change in response to new examples.
To calculate L (D, Mi) we do not need to gather the data in a central location (which would violate our privacy constraint).
It is enough to share Mi with each other user, or some sampling of other users, and gather summary statistics of how well the model performs.
We could then aggregate these summary statistics to evaluate how well Mi does on D.
However, sharing a user’s model with other users still compromises the original user’s privacy, since model weights potentially oﬀer insight into user behavior.
In practice, we often have a subset of user data that we can centralize from users who have opted in, or a large public curated dataset that is relevant to our task of interest.
We treat such a dataset as a stand in for how users will generally behave.
This approach does not compromise user privacy.
Alternately, since our L (D, Mi) is meant to regularize and stabilize our local models, there may be other approaches that achieve this global objective without directly measuring performance on global data.
In future work, we will more deeply explore how best to measure this global loss without violating user privacy.
5.
Experiments We run an experiment with a simple model to demonstrate the trade-oﬀs between personal and global performance and how the choice of α might aﬀect the way we make future user predictions.
5.1. Setup and Data.
We use the Stanford Sentiment Treebank (SSTB) [26] dataset and evaluate how well we can learn models for sentiment.
As a ﬁrst step, we take the 200 most positive and 200 most negative words in the dataset, which we ﬁnd by training a simple logistic regression model on the train set.
We then run experiments simulating the existence of 2, 5, or 8 users.
In each experiment, these words are randomly partitioned amongst users, and users are assigned sentences containing those words for their validation, train, and test sets.
Sentences that contain no words in this top 400 are randomly assigned, and sentences that contain multiple of these are randomly assigned to one of the relevant users.
This results in a split of the dataset in which each model has a subset of words that are signiﬁcantly enriched for them, but are very underrepresented for all other models.
SOMETIMES YOU WANT TO GO WHERE EVERYBODY KNOWS YOUR NAME This split is meant to simulate a pathological case of user style; we try to simulate users in our train set that are very biased and almost non-overlapping in terms of the word choice they use to express sentiment.
While this may not be the case for this speciﬁc review dataset, in general there will be natural language tasks in which users have speciﬁc slang, inside jokes, or acronyms that they use that may not be used by others.
For such users, an ideal setup would adapt to their personal slang, while still leveraging global models to help understand the more common language they use.
5.2. Architecture.
For each user we train completely separate models with the same archi- tecture.
Roughly following the baseline from the original SSTB paper, we classify sentences using a simple two-layer neural network, and use an average of word embeddings as input and a tanh non-linearity.
We use 35 dimensional word embeddings, dropout of 0.5 [27], and use ADAM to optimize [11].
We start with an initial learning rate of 0.001, which we slowly decay, by multiplying by 0.95, if the validation accuracy has not decreased after a ﬁxed number of batches, which is the same across all experiments.
Finally, we use early stopping in the case validation accuracy does not decrease after a ﬁx number of batches, equivalent to 5 epochs on the full train set.
Once trained we evaluate two ways of combining our ﬁxed models, averaging model predictions, and simply taking the most conﬁdent models, where conﬁdence is deﬁned as the absolute diﬀerence between 0.5 and the models prediction.
5.3. Evaluation Metrics.
To evaluate we use the train, validation, test splits as provided with the dataset, and use pytreebank [22] to parse the data.
We only evaluate on the sentence level data for test and validation sets.
This model is not state of the art.
However, we have experience putting models of similar size on low powered and memory constrained devices, and believe this model could realistically be deployed.
Nevertheless, the model is suﬃciently complicated to give us a sense of what happens as we try to combine separately trained models.
In all tables, we report accuracy, and test accuracy for user speciﬁc data is evaluated solely on sentences that contain only their words and none of the other users’ speciﬁc words.
Global data scores represents the whole test set.
We report all results averaged over 15 independent trials.
6.
Results and Analysis 6.1. Single User Performance on User-Speciﬁc Data Vs. Single User Perfor- mance on Global Data.
Unsurprisingly, as the second and third columns of Table 1 show, single user models perform much better on their own heavily biased user-speciﬁc test set than on the global data.
This makes sense as each model has purposely been trained on more words from their biased test set.
Those words were also speciﬁcally selected to be polarizing, but the gap makes concrete the extent to which varying word usages can hurt model performance on this task.
Num.
Users Single user model (global dataset) 0.797 0.783 0.739 0.697 Average aggregation (global dataset) 0.816 0.813 0.794 0.772 Single user model (user-speciﬁc dataset) 0.826 0.824 0.806 0.795 Conﬁdence aggregation (global dataset) 0.803 0.789 0.746 0.704 Table 1.
Accuracy by number of users.
The second column reports accu- racy of the single user model on the user-speciﬁc datasets.
The last three columns report performance on the global dataset for the single user model and the two ensemble models.
6 REUBEN BRASHER, NAT ROTH, JUSTIN WAGLE 6.2. Single User Performance on User-Speciﬁc Data Vs. Ensembled Models on User-Speciﬁc Data.
As the number of users increases, the single user model outperforms both aggregation methods on user-speciﬁc data (Table 2).
This is particularly pronounced for the conﬁdence aggregation method: ensembling hurts performance across all experi- ments, with this eﬀect increasing as we add users.
As the number of users increases, for any given prediction we are less likely to choose the speciﬁc user’s model, which performs best on their own dataset.
The averaging aggregation method outperforms the conﬁdence aggre- gation method and is competitive with the single user model for up to ﬁve users.
However, for more than ﬁve users, the averaging approach starts to perform worse on the user’s own data, again suggesting that we start to drown out much of the personal judgment and rely on global knowledge.
Num.
Users Diﬀerence (Average Aggregation) Diﬀerence (Conﬁdence Aggregation) -0.001 -0.001 0.005 0.022 0.013 0.026 0.059 0.096 Table 2.
Comparison of ensemble model performance to single user model performance on user-speciﬁc data.
“Diﬀerence” columns denote the single user model accuracy minus the ensemble model accuracy.
As the number of users increases, user-speciﬁc models outperform ensemble models by in- creasingly wide margins.
6.3. User Performance on Global Data Vs. Ensembled Models on Global Data.
While it might be easy to conclude that we should just use a single user model, Table 3 demonstrates that the average-aggregated ensembled models outperform the single user model on global data, particularly as the number of users increases.
Again, this is what we would expect, since the aggregated models have collectively been trained on more words in more examples, and ought to generalize better to unbiased and unseen data.
This global knowledge is still important, as it may contain insights about phrases a user has only used a few times.
This may be especially true for a user who has little data.
Recall we divide the whole dataset amongst all users, so as the number of users increases, each user-speciﬁc model is trained on less data.
In this case the lack of a word in the training set may not indicate that a user will never use that word.
It may be that the user has not interacted with the system enough for their individual model to have fully learned their language.
Num.
Users Diﬀerence (Average Aggregation) Diﬀerence (Conﬁdence Aggregation) -0.019 -0.029 -0.055 -0.075 -0.006 -0.005 -0.007 -0.006 Table 3.
Comparison of ensemble model performance to single user model performance on global data.
“Diﬀerence” columns denote single user model accuracy minus ensemble model accuracy.
As the number of users increases, the average-aggregated ensemble model increasingly outperforms the single user model.
6.4. Choosing an Approach Based on α.
These experiments demonstrate the tensions between performing well on global and user data, the two terms in our loss in Equation 1.
We can apply Equation 1, vary α, and see at what point we should prefer diﬀerent strategies.
SOMETIMES YOU WANT TO GO WHERE EVERYBODY KNOWS YOUR NAME Speciﬁcally, suppose we have two approaches we can choose from, with personalized If L0 − L1 < 0 the ﬁrst losses of p0 and p1, and global losses of g0 and g1 respectively.
approach is superior.
We can solve for the α such that L0 = L1, where our loss term again comes from Equation 1.
Plugging our deﬁnition in, we see that αp0 + (1 − α)g0 = αp1 + (1 − α)g1, αp0 + (1 − α)g0 − αp1 − (1 − α)g1 = 0.
or equivalently, Rearranging this yields α = g1 − g0 (p0 − p1) − (g0 − g1) (2) as our break even personalization point.
For this value of α, we ought to see our two models as equally valid solutions to the problem of personalization.
L0−L1 is linear with respect to α, so if L0−L1 ≥ 0 for any α above our cutoﬀ, it will be greater everywhere above the cutoﬀ, and vice versa.
This yields a rule for how to approach making a decision between multiple types of models.
It only requires choosing a single hyperparameter α between 0 and 1, representing one’s belief on how much personalization matters to the task at hand.
We illustrate how to apply these ideas to our experimental results.
We see from Tables 2 and 3 that when we compare using a single model to averaging predictions from 5 models, we have: gaverage − gsingle = −0.054 psingle − paverage = −0.00523 So, our cutoﬀ value of α, where the single model and averaged models yield equivalent losses and we are indiﬀerent between them, is −0.0545/(−0.0545 − 0.00523) = 0.9124.
We can also compute the ranges of α where we should prefer each model.
Because, as explained above, Lsingle − Laverage is linear in α, evaluating a single point above the cutoﬀ suﬃces: we choose α = 1 for computational convenience, and have Lsingle − Laverage = psingle − paverage = −0.00523 ≤ 0.0. Consequently, we prefer the single model for values of α above the cutoﬀ, and the averaged model for values of α below the cutoﬀ.
7.
Conclusion Our deﬁnition of personalization allows for a complete decoupling of models at train time, while only requiring aggregate knowledge of other models inference in order to po- tentially beneﬁt from this global knowledge.
In addition, it gives a practitioner a simple, one parameter way, of deciding how to choose amongst models that may have diﬀerent strengths and weaknesses.
Further, we have shown how this approach might look on a sim- pliﬁed dataset and model, and why the na¨ıve approach of using a single model, or always aggregating all models, may sometimes not be optimal.
In the future, we will work to develop better methods for combining this aggregate global knowledge, while not hurting user performance.
To better protect user privacy, we will also consider alternate methods for regularizing our models outside of the global loss term, L (D, Mi).
We hope that this work will provide a useful framing for future work on personalization, learning in decentralized architectures, such as Ethereum and Bitcoin [18], [28], and serve as a guideline for situations in which the normal single loss and centralized server training paradigm cannot be used.
8 REUBEN BRASHER, NAT ROTH, JUSTIN WAGLE References 1.
Mart´ın Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang, Deep learning with diﬀerential privacy, Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, ACM, 2016, pp.
308–318.
2.
Rami Al-Rfou, Marc Pickett, Javier Snaider, Yun-hsuan Sung, Brian Strope, and Ray Kurzweil, Con- versational contextual cues: The case of personalization and history for response ranking, arXiv preprint arXiv:1606.00372 (2016).
3.
Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars, Mem- ory aware synapses: Learning what (not) to forget, arXiv preprint arXiv:1711.09601 (2017).
4.
Zheqian Chen, Ben Gao, Huimin Zhang, Zhou Zhao, Haifeng Liu, and Deng Cai, User personalized satisfaction prediction via multiple instance deep learning, Proceedings of the 26th International Con- ference on World Wide Web, International World Wide Web Conferences Steering Committee, 2017, pp.
907–915.
5.
Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al., Wide & deep learning for recommender systems, Proceedings of the 1st Workshop on Deep Learning for Recommender Systems, ACM, 2016, pp.
7–10.
6.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei, Imagenet: A large-scale hierar- chical image database, Computer Vision and Pattern Recognition, 2009.
CVPR 2009.
IEEE Conference on, IEEE, 2009, pp.
248–255.
7.
Cynthia Dwork, Diﬀerential privacy: A survey of results, International Conference on Theory and Applications of Models of Computation, Springer, 2008, pp.
1–19.
8.
Kai Fan, Allison E Aiello, and Katherine A Heller, Bayesian models for heterogeneous personalized health data, arXiv preprint arXiv:1509.00110 (2015).
9.
Robert M French, Catastrophic forgetting in connectionist networks, Trends in cognitive sciences 3 (1999), no.
4, 128–135.
10.
Ronald Kemker, Angelina Abitino, Marc McClure, and Christopher Kanan, Measuring catastrophic forgetting in neural networks, arXiv preprint arXiv:1708.02072 (2017).
11.
Diederik P Kingma and Jimmy Ba Adam, A method for stochastic optimization.
2014, arXiv preprint arXiv:1412.6980.
12.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al., Overcoming catastrophic forgetting in neural networks, Proceedings of the National Academy of Sciences (2017), 201611835.
13.
Jakub Koneˇcn`y, H Brendan McMahan, Daniel Ramage, and Peter Richt´arik, Federated optimization: distributed machine learning for on-device intelligence, arXiv preprint arXiv:1610.02527 (2016).
14.
Jakub Koneˇcn`y, H Brendan McMahan, Felix X Yu, Peter Richt´arik, Ananda Theertha Suresh, and Dave Bacon, Federated learning: Strategies for improving communication eﬃciency, arXiv preprint arXiv:1610.05492 (2016).
15.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence Zitnick, Microsoft coco: Common objects in context, European conference on computer vision, Springer, 2014, pp.
740–755.
16.
Bernd Malle, Nicola Giuliani, Peter Kieseberg, and Andreas Holzinger, The more the merrier-federated learning from local sphere recommendations, International Cross-Domain Conference for Machine Learn- ing and Knowledge Extraction, Springer, 2017, pp.
367–373.
17.
H Brendan McMahan, Eider Moore, Daniel Ramage, and Blaise Aguera y Arcas, Federated learning of deep networks using model averaging, (2016).
18.
Satoshi Nakamoto, Bitcoin: A peer-to-peer electronic cash system, 2008.
19.
State of California Department of Justice Oﬃce of the Attorney General, Privacy laws — state of califor- nia - department of justice - oﬃce of the attorney general, https://oag.ca.gov/privacy/privacy-laws, Accessed:2018-01-22.
20.
Cesc Chunseong Park, Byeongchang Kim, and Gunhee Kim, Attend to you: Personalized image cap- 21.
The European Parliament tioning with context sequence memory networks, arXiv preprint arXiv:1704.06485 (2017).
the Council Of The European Union, parliament (eu) http://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32016R0679&from=en, Accessed:2018-01-23.
and european 2016/679 council of and of the of the 27 april Regulation 2016, 2016, 22.
Jonathan Raiman, Stanford sentiment treebank loader in python, https://github.com/JonathanRaiman/pytreebank, Accessed:2018-01-05.
23.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al., Imagenet large scale visual recognition chal- lenge, International Journal of Computer Vision 115 (2015), no.
3, 211–252.
SOMETIMES YOU WANT TO GO WHERE EVERYBODY KNOWS YOUR NAME 24.
Jiaxin Shi, Jianfei Chen, Jun Zhu, Shengyang Sun, Yucen Luo, Yihong Gu, and Yuhao Zhou, Zhusuan: A library for bayesian deep learning, arXiv preprint arXiv:1709.05870 (2017).
25.
Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet Talwalkar, Federated multi-task learning, arXiv preprint arXiv:1705.10467 (2017).
26.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts, Recursive deep models for semantic compositionality over a sentiment treebank, Pro- ceedings of the 2013 conference on empirical methods in natural language processing, 2013, pp.
1631– 1642.
27.
Nitish Srivastava, Geoﬀrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov, Dropout: a simple way to prevent neural networks from overﬁtting., Journal of machine learning re- search 15 (2014), no.
1, 1929–1958.
28.
Gavin Wood, Ethereum: A secure decentralised generalised transaction ledger, Ethereum Project Yellow Paper 151 (2014).
29.
Yi Zhang and Jonathan Koren, Eﬃcient bayesian hierarchical user modeling for recommendation system, Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, ACM, 2007, pp.
47–54.
30.
Philip Zigoris and Yi Zhang, Bayesian adaptive user proﬁling with explicit & implicit feedback, Proceed- ings of the 15th ACM international conference on Information and knowledge management, ACM, 2006, pp.
397–404.

This paper studies the Restricted Isometry Property (RIP) of random projections for sub- spaces.
It reveals that the distance between two low-dimensional subspaces remain almost unchanged after being projected by a Gaussian random matrix with overwhelming proba- bility, when the ambient dimension after projection is suﬃciently large in comparison with the dimension of subspaces.
1.1 Motivation In the era of data deluge, labeling huge amount of large-scale data can be time-consuming, costly, and even intractable, so unsupervised learning has attracted increasing attention in ∗The authors are with the Department of Electronic Engineering, Tsinghua University, Beijing 100084, China.
The corresponding author of this paper is Yuantao Gu (gyt@tsinghua.edu.cn).
recent years.
One of such methods emerging recently, subspace clustering (SC) [1, 2, 3, 4], which depicts the latent structure of a variety of data as a union of subspaces, has been shown to be powerful in a wide range of applications, including motion segmentation, face clustering, and anomaly detection.
It also shows great potential to some previously less explored datasets, such as network data, gene series, and medical images.
Traditional subspace clustering methods, however, suﬀer from the deﬁciency in simi- larity representation, so it can be computationally expensive to adapt them to large-scale datasets.
In order to alleviate the high computational burden, a variety of works have been done to address the crucial problem of how to eﬃciently handle large-scale datasets.
Compressed Subspace Clustering (CSC) [5] also known as Dimensionality-reduced Subspace Clustering [6] is a method that performs SC on randomly compressed data points.
Because the random compression reduces the dimension of the ambient space, the computational cost of ﬁnding the self-representation in SC can be eﬃciently reduced.
Based on the con- cept of subspace aﬃnity, which characterizes the similarity between two subspaces, and the mathematical tools introduced in [2], the conditions under which several popular al- gorithms can successfully cluster the compressed data have been theoretically studied and numerically veriﬁed [7, 8].
Because the data points are randomly projected from a high-dimensional ambient space RN to a new medium-dimensional ambient space Rn, a worry is that the similarity between any two low-dimensional subspaces increases and the SC algorithms are less likely to per- form well.
Inspired by the well-known Johnson-Lindenstrauss (JL) Lemma [9, 10] and the Restricted Isometry Property (RIP) [11, 12, 13], which allows the use of random projection to reduce the space dimension while keeping the Euclidean distance between any two data points and leads to the boom of sparse signal processing including Compressed Sensing (CS) [14, 15, 16, 17, 18, 19, 20], one may speculate whether the similarity (or distance) between any two given subspaces can remain almost unchanged, if the dimension of the latent sub- space that the data lie in is small compared with that of the ambient space after projection n.
It should be highlighted that this conjecture is not conﬁned to the SC problem, so we believe that it may beneﬁt future studies on other subspace related topics.
Motivated by the conjecture about whether the similarity between any two given sub- spaces can remain almost unchanged after random projection, we study the RIP of Gaussian random projections for a ﬁnite set of subspaces.
In order to give more solid guarantees and more precise insight into the law of magnitude of the dimensions for CSC and other sub- space related problems, we derive an optimum probability bound of the RIP of Gaussian random compressions for subspaces in this paper.
Compared with our previous work [21], the probability bound has been improve from 1 − O(1/n) to 1 − e−O(n), which is optimum when we consider the state-of-the-art statistical probability theories for Gaussian random matrix.
1.2 Main Results The projection Frobenius norm (F-norm for short) distance is adopted in this work to measure the distance between two subspaces.
It should be noted that we slightly generalize the deﬁnition in [22] to the situation where the dimensions of the two subspaces are diﬀerent.
Deﬁnition 1 ([21]) (Projection Frobenius norm distance between subspaces) The generalized projection F-norm distance between two subspaces X1 and X2 is deﬁned as D(X1,X2) := √2kU1UT 1 − U2UT 2 kF, where Ui denotes an arbitrary orthonormal basis matrix for subspace Xi, i = 1, 2.
We will focus on the change of the distance between any two low-dimensional sub- spaces after being randomly projected from RN to Rn (n < N ).
The projection of a low-dimensional subspace by using a Gaussian random matrix is deﬁned as below.
Deﬁnition 2 (Gaussian random projection for subspace) The Gaussian random pro- jection of a d-dimensional subspace X ⊂ RN onto Rn (d < n < N ) is deﬁned as below, −→ Y = {y|y = Φx,∀x ∈ X}, where the projection matrix Φ ∈ Rn×N is composed of entries independently drawn from Gaussian distribution N (0, 1/n).
One may notice that the dimensions of subspaces remain unchanged after random pro- jection with probability one.
Based on the deﬁnitions above, the main theoretical result of this work is stated as follows.
Theorem 1 Suppose X1, .
.
.
,XL ⊂ RN are L subspaces with dimension less than d.
After −→ Yi ⊂ Rn, i = random projection by using a Gaussian random matrix Φ ∈ Rn×N , Xi 1,··· , L, n < N .
There exist constants c1(ε), c2(ε) > 0 depending only on ε such that for any two subspaces Xi and Xj, for any n > c1(ε) max{d, ln L}, (1 − ε) D2(Xi,Xj) < D2(Yi,Yj) < (1 + ε) D2(Xi,Xj) (1) holds with probability at least 1 − e−c2(ε)n.
Theorem 1 reveals that the distance between two subspaces remains almost unchanged after random projection with overwhelming probability, when the ambient dimension after projection n is suﬃciently large.
1.3 Our Contribution In this paper, we study the RIP of Gaussian random matrices for projecting a ﬁnite set of subspaces.
The problem is challenging as random projections neither preserve orthogonality nor normalize the vectors deﬁning orthonormal bases of the subspaces.
In order to measure the change in subspace distance induced by random projections, both eﬀects have to be carefully quantiﬁed.
Based on building a metric space of subspaces with the projection F-norm distance, which is closely connected with subspace aﬃnity, we start from verifying that the aﬃnity between two subspaces concentrates on its estimate with overwhelming probability after Gaussian random projection.
Then we successfully reach the RIP of two subspaces and generalize it to the situation of a ﬁnite set of subspaces, as stated in Theorem 1.
The main contribution of this work is to provide a mathematical tool, which can shed light on many problems including CSC.
As a direct result of Theorem 1, when solving the SC problem at a large scale, one may conduct SC on randomly compressed samples to alleviate the high computational burden and still have theoretical performance guarantee.
Because the distance between subspaces almost remains unchanged after projection, the clustering error rate of any SC algorithm may keep as small as that conducting in the original space.
Considering that our theory is independent of SC algorithms, this may beneﬁt future studies on other subspace related topics.
Except our previous work [21] that will be compared with in Section 6, as far as we know, there is no relevant work that study the distance preserving property between subspaces after random projection.
1.3.1 Comparison with JL Lemma and RIP for Sparse Signals The famous Johnson-Lindenstrauss Lemma illustrates that there exists a map from a higher- dimensional space into a lower-dimensional space such that the distance between a ﬁnite set of data points will change little after being mapped.
Lemma 1 (JL Lemma) [9, 10] For any set V of L points in RN , there exists a map f : RN → Rn, n < N , such that for all x1, x2 ∈ V, (1 − ε)kx1 − x2k2 2 ≤ kf (x1) − f (x2)k2 2 ≤ (1 + ε)kx1 − x2k2 if n is a positive integer satisfying n ≥ 4lnL/(ε2/2 − ε3/3), where 0 < ε < 1 is a constant.
The RIP of random matrix illustrates that the distance between two sparse vectors will change little with high probability after random projection.
Deﬁnition 3 [11, 12, 13] The projection matrix Φ ∈ Rn×N , n < N satisﬁes RIP of order k if there exists a δk ∈ (0, 1) such that (1−δk)kx1−x2k2 2 ≤ kΦx1−Φx2k2 2 ≤ (1+δk)kx1−x2k2 Table 1: Comparison with other dimension-reduction theories including JL Lemma, RIP for sparse signals, and our previous results [21] JL Lemma RIP for sparse signals RIP for low-dimensional subspaces [21] this work object any set of L points in RN all k-sparse signals in RN any set of L d-dimensional subspaces in RN metric compression method error bound condition success probability Euclidean distance projection F-norm distance kxi − xjk2 1√2kPi − PjkF some map f Gaussian random matrix (1 − ε, 1 + ε) n ≥ 4lnL ε2/2−ε3/3 (1 − δk, 1 + δk) n ≥ c1kln(cid:0) N k(cid:1) 1 − e−c2n (1 − ε, 1 + ε) n large enough n > c1max{d, ln L} 1− 2dL(L−1) (ε−d/n)2n 1 − e−c2n holds for any two k-sparse vectors x1, x2 ∈ RN .
Theorem 2 [13] A Gaussian random matrix Φ ∈ Rn×N , n < N has the RIP of order k for n ≥ c1kln(cid:0) N k(cid:1) with probability 1 − e−c2n, where c1, c2 > 0 are constants depending only on δk, the smallest nonnegative constant satisfying Deﬁnition 3.
In summary, the above works focus on the change of the distance between points after determinate mapping or random projection.
In comparison, our work views a subspace as a whole and studies the distance between subspaces, which to the best of our knowledge has never been studied before.
Moreover, the above works study the points in Euclidean space with l2-norm, while our work study the subspaces on the Grassmannian manifold with F-norm metric, which is highly nonlinear and more complex.
A detailed comparison to explain the diﬀerences between our work and related works is presented in Table 1.
1.3.2 Comparison with RIP for Signals in UoS There are literatures studying the distance preserving properties of compressed data points, which may be sparse on speciﬁc basis or lie in a couple of subspaces or surfaces [23, 24, 25, 26].
The authors of [23] extended the RIP to signals that are sparse or compressible with respect to a certain basis Ψ, i.e., x = Ψα, where Ψ is represented as a unitary N×N matrix and α is a k-sparse vector.
The work of [24] proves that with high probability the random projection matrix Φ can preserve the distance between two signals belonging to a Union of Subspaces (UoS).
In [25], it is shown that random projection preserves the structure of surfaces.
Given a collection of L surfaces of linearization dimension d, if they are embedded into a space of O(dδ2 log(Ld/δ)) dimension, the surfaces are preserved in the sense that for any pair of points on these surfaces the distance between them are preserved.
The main contribution of [26] is stated as follows.
If S is an n point subset of RN , 0 < δ < 1 3 and n = 256d log n(max{d, 1/δ})2, there is a mapping of RN into Rn under which volumes of sets of size at most d do not change by more than a factor of 1 + δ, and the distance of points from aﬃne hulls of sets of size at most k − 1 is preserved within a relative error of δ.
According to above survey, those works study embedding of Euclidean distances be- tween points in subspaces, while we discuss embedding of a ﬁnite set of subspaces in terms of the projection F-norm distance.
In both the related works and this paper, the same mathematical tool of concentration inequalities and random matrix theory are adopted to derive the RIP for two diﬀerent objects, i.e., data points in Euclidean space and subspaces in Euclidean space (or points on Grassmann manifold), respectively.
In comparison, both Euclidean space and random projection are linear, but Grassmannian is not linear, let along the projection on it, so the new problem is much more diﬃcult than the existing one, and a core contribution of this work is dealing with the above challenges with a brand-new geometric proof, the technique in which has hardly been used previously to derive the RIP for data points.
1.4 Organization The rest of this paper is organized as follows.
Based on the introduction of principal angles, aﬃnity, and its connection with the projection F-norm distance, we study the RIP for subspaces in the top level in Section 2.
The main result of Theorem 1 is proved by using two core propositions of Lemma 4 and Theorem 3.
In Section 3, we focus on the probability and concentration inequalities of Gaussian random matrix to prepare necessary mathematical tools that will be used through this work.
In Section 4, we prove the ﬁrst core proposition of Lemma 4, which states that the aﬃnity between a line and a subspace will concentrate on its estimate with overwhelming probability after random projection.
In Section 5, we prove the second core proposition of Theorem 3, which provides a general theory that the aﬃnity between two subspaces with arbitrary dimensions demonstrates concentration after random projection.
In Section 6, we compare those theories with our previous results and highlight the novelty.
We conclude this work in Section 7.
Most proofs of lemmas and remarks are included in the Appendix 8.
1.5 Notations Vectors and matrices are denoted by lower-case and upper-case letter, respectively, both in boldface.
AT denotes matrix transposition.
kak and kAkF denote ℓ2 norm of vector a and Frobenius norm of matrix A.
smax(A) and smin(A) denote the largest and smallest singular value of matrix A, respectively.
Subspaces are denoted by X ,Y, and S.
C(A) denotes the column space of matrix A.
We use S⊥ to denote the orthonormal complement space of S.
PS (v) denotes the projection of vector v onto subspace S.
2 RIP of Gaussian Random Projection for Subspaces 2.1 Preliminary Before starting the theoretical analysis, we ﬁrst introduce the deﬁnition of principal angles and aﬃnity.
These two concepts have been widely adopted to describe the relative position and to measure the similarity between two subspaces.
Our theoretical analysis will ﬁrst focus on the estimation of these quantities before and after random projection.
Then using the connection between aﬃnity and projection F-norm distance derived in [21], we can readily derive the result in Theorem 1.
The principal angles (or canonical angles) between two subspaces provide a robust way to characterize the relative subspace positions [27, 28].
Deﬁnition 4 The principal angles θ1,··· , θd1 between two subspaces X1 and X2 of dimen- sions d1 ≤ d2, are recursively deﬁned as cos θk = max x1∈X1 with the orthogonality constraints xT xT 1kx2k xT 1 x2 =: kx1kkx2k max kx1kkkx2kk x2∈X2 i xil = 0, l = 1,··· , k − 1, i = 1, 2.
Beside deﬁnition, an alternative way of computing principal angles is to use the singular value decomposition [29].
Lemma 2 Let the columns of Ui be orthonormal bases for subspace Xi of dimension di, i = 1, 2 and suppose d1 ≤ d2.
Let λ1 ≥ λ2 ≥ ··· ≥ λd1 ≥ 0 be the singular values of UT 1 U2, then cos θk = λk, k = 1,··· , d1.
Based on principle angles, aﬃnity is deﬁned to measure the similarity between subspaces [2].
Deﬁnition 5 The aﬃnity between two subspaces X1 and X2 of dimension d1 ≤ d2 is deﬁned as aﬀ (X1,X2) :=(cid:18) d1 Xk=1 cos2 θk(cid:19)1/2 = kUT 1 U2kF, where the columns of Ui are orthonormal bases of Xi, i = 1, 2.
The relationship between distance and aﬃnity is revealed in Lemma 3.
Because of the concise deﬁnition and easy computation of aﬃnity, we will start the theoretical analysis with aﬃnity, and then present the results with distance by using Lemma 3.
Lemma 3 [21] The distance and aﬃnity between two subspaces X1 and X2 of dimension d1, d2, are connected by D2(X1,X2) = d1 + d2 − aﬀ 2(X1,X2).
2.2 Theoretical Results In this section, we will present the main theoretical results about the aﬃnity and distance between subspaces.
Before that, let us introduce some basic notations to be used.
We denote the random projection of subspaces of X1 and X2 as Y1 and Y2, respectively.
We denote DX = D(X1,X2) and DY = D(Y1,Y2) as the distances before and after random projection.
Similarly, we use aﬀX = aﬀ(X1,X2) and aﬀY = aﬀ(Y1,Y2) to denote the aﬃnities before and after projection.
Without loss of generality, we always suppose that d1 ≤ d2.
For simplicity, we refer the aﬃnity (distance) after random projection as projected aﬃnity (projected distance).
To begin with, we focus on a special case that one subspace is degenerated to a line (one-dimensional subspace).
The following lemma provides an estimation of the aﬃnity between a line and a subspace after Gaussian random projection.
When the dimensionality of the new ambient space is large enough, the real projected aﬃnity will highly concentrate around this estimation with overwhelming probability.
Lemma 4 Suppose X1,X2 ⊂ RN are a line and a d-dimension subspace, d ≥ 1, respectively.
Let λ = aﬀX denote their aﬃnity.
If they are projected onto Rn, n < N, by a Gaussian random matrix Φ ∈ Rn×N , Xi −→ Yi, i = 1, 2, then the projected aﬃnity, aﬀY , can be estimated by and there exist constants c1(ε), c2(ε) > 0 depending only on ε such that for any n > c1(ε)d, Y − aﬀ holds with probability at least 1 − e−c2(ε)n.
aﬀ 2 (cid:12)(cid:12)(cid:12) Y(cid:12)(cid:12)(cid:12) aﬀ Y = λ2 + n(cid:0)1 − λ2(cid:1) , < (1 − λ2)ε (2) (3) Then, we study the general case of projecting two subspaces of arbitrary dimensions.
As mentioned in the last subsection, we will begin with the estimation of aﬃnity and then restate the result in terms of distance.
The following theorem reveals the concentration of aﬃnity between two arbitrary sub- spaces after random projection.
Theorem 3 Suppose X1,X2 ⊂ RN are two subspaces with dimension d1 ≤ d2, respectively.
Take aﬀ Y = aﬀ 2 X + d2 (d1 − aﬀ 2 X ) (4) −→ Yi, i = as an estimate of the aﬃnity between two subspaces after random projection, Xi 1, 2.
Then there exist constants c1(ε), c2(ε) > 0 depending only on ε such that for any n > c1(ε)d2, < (d1 − aﬀ 2 X )ε (5) Y − aﬀ holds with probability at least 1 − e−c2(ε)n.
aﬀ 2 (cid:12)(cid:12)(cid:12) Y(cid:12)(cid:12)(cid:12) Because of its concision when evaluating the relative position in Deﬁnition 5, we present the concentration by using aﬃnity in Lemma 4 and Theorem 3, which play essential role in RIP for subspaces.
Their proofs, which unfurl main text of this work, are postponed to Section 4 and Section 5, respectively.
Using Lemma 3 and Theorem 3, we derive an estimation of the projected distance.
Similarly we prove that the true projected distance will highly concentrate around this estimate with overwhelming probability.
Corollary 1 Suppose X1,X2 ⊂ RN are two subspaces with dimension d1 ≤ d2, respectively.
We use Y = D2 X − d2 n (cid:18)D2 X − 2 (cid:19) d2 − d1 −→ as an estimation of the distance between two subspaces after random projection, Xi Yi, i = 1, 2.
Then there exist constants c1(ε), c2(ε) > 0 depending only on ε such that for any n > c1(ε)d2, D2 Y − D <(cid:18)D2 X − 2 (cid:19) ε d2 − d1 (cid:12)(cid:12)(cid:12) Y(cid:12)(cid:12)(cid:12) holds with probability at least 1 − e−c2(ε)n.
Proof Combining (4) and (6) by using Lemma 3, we readily get that |D2 aﬀ to (5).
Y|.
Using Lemma 3 again, we have D2 2 = d1 − aﬀ 2 X − d2−d1 Y − X .
Therefore, (7) is identical Y| = |aﬀ 2 Y − D (6) (7) 2.3 Proof of Theorem 1 Now we are ready to prove the RIP of Gaussian random matrix for projecting a ﬁnite set of subspaces using the results above.
Without loss of generality, we assume that di ≤ dj ≤ d.
According to Corollary 1, there exist constants c1,1, c2,1 > 0 depending only on ε such that for any n > c1,1dj, D2(Xi,Xi) −(cid:18) dj 2(cid:19)(cid:18)D2(Xi,Xi) − 2 (cid:19) < D2(Yi,Yi) dj − di < D2(Xi,Xi) +(cid:18)− 2(cid:19)(cid:18)D2(Xi,Xi) − dj 2 (cid:19) .
dj − di holds with probability at least 1 − e−c2,1n.
When n > 2d/ε, we have dj/n ≤ d/n < ε/2.
In this case, we have both D2(Yi,Yi) > D2(Xi,Xi) −(cid:18) dj 2(cid:19) D2(Xi,Xi) =(cid:18)1 − dj n − 2(cid:19) D2(Xi,Xi) > (1 − ε) D2(Xi,Xi), D2(Yi,Yi) < D2(Xi,Xi) +(cid:18)− dj 2(cid:19) D2(Xi,Xi) =(cid:18)1 − dj 2(cid:19) D2(Xi,Xi) < (1 + ε) D2(Xi,Xi), (8) (9) hold with probability at least 1− e−c2,1n.
Note that (8) and (9) hold for any 1 ≤ i < j ≤ L.
Then the probability is at least 1− L(L−1) , there exists constant c2 depending only on ε, such that L(L−1) c2,1}, then when n > c1 max{d, ln L}, conditions n > c1,1d, n > 2d/ε, and n > 1 that are required above are all satisﬁed, the probability is at least 1 − e−c2n with c2 > 0.
Then we reach the ﬁnal conclusion.
e−c2,1n.
If n > 1 c2,1 e−c2,1n < e−c2n.
Take c1 := max{c1,1, 2 ε , 2 ln L(L−1) ln L(L−1) c2,1 3 Concentration Inequalities for Gaussian Distribution Before proving the main results, we ﬁrst introduce some useful concentration inequalities for Gaussian distribution.
Most of them are proved using the following lemma, which provides a strict estimation of the singular values of Gaussian random matrix.
Lemma 5 [30] Let A be an N × n matrix whose elements aij are independent Gaussian random variables.
Then for every t ≥ 0, one has and P(cid:16)smax(A) ≥ P(cid:16)smin(A) ≤ √N + √n + t(cid:17) ≤ e− t √N − √n − t(cid:17) ≤ e− t 2 , 2 .
10 (10) (11) Based on Lemma 5, we are ready to prove some useful lemmas that will be directly used to prove our theories on RIP of random projection for subspaces.
Before doing that, we ﬁrst deﬁne standard Gaussian random matrix and verify that the function satisfying certain condition can be written as a single exponential function.
Deﬁnition 6 A Gaussian random matrix (or vector) has i.i.d. zero-mean Gaussian random entries.
A standard Gaussian random matrix A ∈ Rn×N has i.i.d. zero-mean Gaussian random entries with variance 1/n.
Each column of A is a standard Gaussian random vector.
Lemma 6 Given if for all k, it holds that f (ε, n, τ ) = Xk=1 ak(ε, n)e−gk(ε,n,τ ), hk(ε) := lim τ→0 bk(ε) := lim n→∞ lim n→∞ ln ak(ε, n) gk(ε, n, τ ) > 0, < hk(ε), (12) (13) (14) then there exist universal constants n0, c1 > 0, and c2 > 0 depending only on ε, such that when n > n0, τ < c1, it satisﬁes that f (ε, n, τ ) < e−c2n.
Proof The proof is postponed to Appendix 8.1. Remark 1 Lemma 6 illustrates that the summation of ﬁnite multiple exponential decay functions can always be bounded by a single exponential function.
The following lemma illustrates that the norm of standard Gaussian random vector concentrates around 1 with high probability, especially when the dimensionality is high.
Lemma 7 Assume that a ∈ Rn is a standard Gaussian random vector.
For any ε > 0, we have hold for n > n0, where n0 and c are constants dependent on ε.
P(cid:0)(cid:12)(cid:12)kak2 − 1(cid:12)(cid:12) > ε(cid:1) < e−c(ε)n (15) Proof The proof is postponed to Appendix 8.2. Furthermore, Corollary 2 generalizes Lemma 7 to case when we project standard Gaus- sian random vector by orthonormal matrix.
11 Corollary 2 Let a ∈ Rn be a standard Gaussian random vector.
For any given orthonormal matrix V = [v1,··· , vd] ∈ Rn×d and ε > 0, we have hold for n > c1d, where c1, c2 are constants dependent on ε.
kVTak2 − > ε(cid:19) < e−c2(ε)n (16) P(cid:18)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:12)(cid:12)(cid:12)(cid:12) Proof The proof is postponed to Appendix 8.3. Corollary 3 extends Lemma 5 to column-normalized standard Gaussian random matrix.
It reveals that a column-normalized Gaussian random matrix is a high-quality approxima- tion to an orthonormal matrix, because all its singular values are very close to 1.
Corollary 3 Let A = [a1,··· , ak] ∈ Rn×k be a standard Gaussian random matrix.
Each column of ¯A is normalized from the corresponding column of A, that is ¯A =(cid:20) a1 ka1k ,··· , ak kakk(cid:21) .
Then we can get the bound of the minimum and maximum of the singular value of ¯A as below P(cid:0)s2 P(cid:0)s2 min(cid:0) ¯A(cid:1) < 1 − ε(cid:1) < e−c2,1(ε)n, max(cid:0) ¯A(cid:1) > 1 + ε(cid:1) < e−c2,2(ε)n, ∀n > c1,1k, ∀n > c1,2k, (17) (18) where c1,1 and c2,1, c1,2 and c2,2 are constants dependent on ε in (17) and (18), respectively.
Proof The proof is postponed to Appendix 8.4. Remark 2 For A ∈ R(n−d0)×k be a standard Gaussian random matrix, we have (18) hold for n > c1,2 max{k, d0}, where c1,2 and c2,2 are constants dependent on ε.
The following lemma studies a property of Gaussian random projection.
Intuitively, it illustrates that if a line and a subspace are perpendicular to each other, they will still be almost perpendicular after Gaussian random projection.
Lemma 8 Assume u1 ∈ RN is a unit vector, U2 ∈ RN×d is an orthonormal matrix, and u1 is perpendicular to U2.
Let Φ ∈ Rn×N be a standard Gaussian random matrix.
We use a1 = Φu1 and A2 = ΦU2 to denote the projection of u1 and U2 by using Φ.
If V2 is an arbitrary orthonormal basis of C(A2), then for ε > 0, we have > ε(cid:17) ≤ e−c2(ε)n (19) hold for n > c1(ε)d, where c1, c2 are constants determined by ε.
P(cid:16)(cid:13)(cid:13)VT 2 a1(cid:13)(cid:13) Proof The proof is postponed to Appendix 8.5. 12 Corollary 4 In Lemma 8, if we further deﬁne ¯a1 = a1/ka1k as the normalized projection of u1, then for ε > 0, we have hold for n > c1(ε)d, where c1, c2 are constants determined by ε.
P(cid:16)(cid:13)(cid:13)VT 2 ¯a1(cid:13)(cid:13) > ε(cid:17) ≤ e−c2(ε)n (20) Proof The proof is postponed to Appendix 8.6. Remark 3 Using the same notations in Corollary 4, if we take Φ ∈ R(n−d0)×N , ¯a1 ∈ Rn−d0 and V2 ∈ R(n−d0)×d, then we have still have P(cid:16)(cid:13)(cid:13)VT 2 ¯a1(cid:13)(cid:13) for n > c1(ε) max{d, d0}, where c1 and c2 are constants dependent on ε.
This can by readily veriﬁed by replacing n with n − d0 in (20) and then applying Lemma 6.
The detailed proof is postponed to Appendix 8.7. > ε(cid:17) ≤ e−c2(ε)n Finally we state the independence of random vectors, matrices, and their column- spanned subspaces.
Deﬁnition 7 Two random vectors are independent, if and only if the distribution of any one of them does not have inﬂuence on that of the other.
Two random matrices are indepen- dent, if and only if any two columns of them are independent.
Furthermore, we introduce the independence between a random matrix and a subspace, which holds true if and only if the subspace is spanned by the columns of another random matrix that is independent of the ﬁrst one.
Finally, two subspaces are independent, if and only if they are spanned by the columns of two independent random matrices, respectively.
Lemma 9 Assume U and V are two matrices satisfying UTV = 0, and Φ is a Gaussian random matrix.
Then ΦU and ΦV are independent.
This can be readily veriﬁed by calcu- lating the correlation between any two entries in ΦU and ΦV, respectively.
They are all zero.
4 Proof of Lemma 4 The proof of Lemma 4 is made up of two steps.
At ﬁrst, we derive an accurate expression of the error about estimating aﬀY .
Then, the estimate error is bounded by utilizing the concentration inequalities for Gaussian distribution that we derived in the previous section.
Step 1) Let us begin from choosing the bases for the line X1 and the subspace X2 and then calculating the aﬃnity after projection.
According to the deﬁnition of aﬃnity, λ = cos θ, where θ is the only principal angle between X1 and X2.
We use u and u1 to denote, respectively, the basis of X1 and a unit 13 vector in X2, which constructs the principal angle with u.
Therefore, u can be rewritten into the following form u = λu1 +p1 − λ2u0, where u0 denotes some unit vector orthogonal to X2.
Based on the above deﬁnition, we can choose U = [u1, ..., ud] as the basis of X2.
Notice that {u2,··· , ud} could be freely chosen as long as the orthonormality is satisﬁed.
After projecting X1 by random Gaussian matrix, we get subspace Y1, whose basis vector is (21) (22) a = Φu = λΦu1 +p1 − λ2Φu0 = λa1 +p1 − λ2a0, where a1 := Φu1 and a0 := Φu0 are not orthogonal to each other.
As for Y2, considering that ΦU is not a set of orthonormal basis, we do orthogonalization by using Gram-Schmidt process.
Denote the orthonormalized matrix as V = [v1,··· , vd].
By the deﬁnition of Gram-Schmidt process, the ﬁrst column of V should be which does not change its direction after the orthogonalization.
v1 = a1 ka1k (23) Remark 4 Consider the aﬃnity between two subspaces S1 and S2, with dimension 1 and d ≥ 1.
Let v1 and V2 be the orthonormal basis for S1 and S2, respectively.
Then the aﬃnity equals the norm of the projection of v1 onto S2, i.e., λ = kPS2(v1)k =(cid:13)(cid:13)VT By the deﬁnition of aﬃnity and (22), we can calculate the aﬃnity between Y1 and Y2 2 v1(cid:13)(cid:13).
as aﬀ 2 VT a kak(cid:13)(cid:13)(cid:13)(cid:13) Y =(cid:13)(cid:13)(cid:13)(cid:13) λVTa1 +p1 − λ2VTa0(cid:13)(cid:13)(cid:13) kak2 (cid:13)(cid:13)(cid:13) kak2 (cid:16)λ2kVTa1k2 + (1 − λ2)kVTa0k2 + λp1 − λ2kaT 1 a0k(cid:17) .
Because a1 lies in Y2, we have kVTa1k = ka1k.
By taking the norm on both sides of (22), we write kak2 = λ2ka1k2 + (1 − λ2)ka0k2 + 2λp1 − λ2ka0kka1k.
14 (24) (25) (26) Recalling (2) and inserting the estimation into (27), the estimate error is deduced as (cid:12)(cid:12)(cid:12) aﬀ 2 aﬀ 2 Y(cid:12)(cid:12)(cid:12) aﬀ 2 Y − aﬀ (cid:18)1 − the RHS of (28) into three parts using triangle inequality.
kak2 (cid:19)−(cid:18)1−(1−λ2)(cid:18)1− kak2 − kVTa0k2 kak2 (cid:19)(cid:12)(cid:12)(cid:12)(cid:12) Eliminating kVTa1k and ka1k by inserting (25) and (26) into (24), we get kak2(cid:0)kak2−(1−λ2)ka0k2 +(1−λ2)kVTa0k2(cid:1) Y = kak2 − kVTa0k2 = 1 − (1 − λ2)(cid:18)ka0k2 kak2 (cid:19) .
=(cid:12)(cid:12)(cid:12)(cid:12) (1 − λ2)(cid:19)(cid:12)(cid:12)(cid:12)(cid:12) Y −(cid:18)λ2 + 1−(1−λ2)(cid:18)ka0k2 kak2 − kVTa0k2 =(cid:12)(cid:12)(cid:12)(cid:12) n(cid:19) −(cid:18)ka0k2 =(1 − λ2)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:19) −(cid:18)ka0k2 kak2 − kVTa0k2 kak2 (cid:19)(cid:12)(cid:12)(cid:12)(cid:12) (cid:18)1 − kVTa0k2 ka0k2 n(cid:12)(cid:12)(cid:12)(cid:12) kak2 − 1(cid:12)(cid:12)(cid:12)(cid:12) +(cid:12)(cid:12)(cid:12)(cid:12) kak2 − kak2 (cid:12)(cid:12)(cid:12)(cid:12) n(cid:12)(cid:12)(cid:12)(cid:12) kak2 − 1(cid:12)(cid:12)(cid:12)(cid:12) with probability at least 1 − 2e−c2,1(ε1)n, we have (cid:12)(cid:12)kak2 − 1(cid:12)(cid:12) < ε1 (cid:12)(cid:12)ka0k2 − 1(cid:12)(cid:12) < ε1.
n(cid:12)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:12)(cid:13)(cid:13)VTa0(cid:13)(cid:13) kak2 − 1(cid:12)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:12) ≤(cid:12)(cid:12)(cid:12)(cid:12) ≤(cid:12)(cid:12)(cid:12)(cid:12) kVTa0k2 − n(cid:12)(cid:12)(cid:12)(cid:12) ka0k2 < ε2.
and Using (30), (31), and (32) in (29), for n > max{n0, c1,2}d, with probability at least 1 − 2e−c2,1(ε1)n − e−c2,2(ε2)n, we have (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) (cid:18)1 − n(cid:19) − ka0k2 kak2  1 − Xi=1 cos2 θi!(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 15 2ε1 1 − ε1 ≤(cid:18) 3d ε2 1 − ε1 + 3(cid:19) ε1 + 2n ε1 1 − ε1 3ε2 Since u0 is orthogonal to ui, i = 1,··· , d, by Corollary 2, for n > c1,2(ε2)d, with probability at least 1 − e−c2,2(ε2)n, we have Step 2) Before bounding the estimate error by concentration inequalities, we ﬁrst split n(cid:19)(cid:19)(cid:12)(cid:12)(cid:12)(cid:12) (28) Since both a and a0 are standard Gaussian random vectors, by Lemma 7, for n > n0, (29) (27) (30) (31) (32) (33) (34) where the last inequality holds for ε1 < 1/3.
To complete the proof, we need to formulate (34) and (33) into the shape of (3) and a single exponential function, respectively.
Letting ε1 = ε2 =: ε/6 and inserting (34) into (28), we have (cid:12)(cid:12)(cid:12) aﬀ 2 Y − aﬀ Y(cid:12)(cid:12)(cid:12) ≤(cid:0)1 − λ2(cid:1)(cid:18)(cid:18) 3d ≤(cid:0)1 − λ2(cid:1)(cid:18)(cid:18)3 =(cid:0)1 − λ2(cid:1) ε.
By Remark 1, we know that there exist constants c1, c2, such that (33) is greater than 1 − e−c2(ε)n for any n > c1(ε)d and close the proof.
+ 3(cid:19) ε1 + + 3(cid:19) ε1 + 3ε2 2 (cid:19) 2 (cid:19) 3ε2 2n 5 Proof of Theorem 3 The proof of Theorem 3 is divided into two parts.
In subsection 5.1, we will complete the main body of the proof by using an important lemma, which will be proved in subsection 5.2. Before we start, let us introduce some auxiliary variables.
Remark 5 Assume there are two subspaces S1 and S2, with dimension d1 ≤ d2.
Let ˜Ui = [˜ui,1,··· , ˜ui,di] denote any orthonormal matrix for subspace Si, i = 1, 2.
One may do singular value decomposition as ˜UT 1 , where the singular values λk = cos θk, 1 ≤ k ≤ d1 are located on the diagonal of Λ, and θ1 ≤ θ2 ≤ ··· ≤ θd1 denote the principal angles between S1 and S2.
After reshaping, we have ˜U1 = Q2ΛQT UT 2 U1 :=(cid:16) ˜U2Q2(cid:17)T ˜U1Q1 = Λ = λ1  .
.
.
λd1  where Ui := ˜UiQi = [ui,1,··· , ui,di] , i = 1, 2 are the orthonormal basis, which have the closest connection with the aﬃnity between these two subspaces.
Deﬁnition 8 (principal orthonormal bases) We refer U1 and U2 as principal orthonor- mal bases for S1 and S2, if they are derived by using the method in Remark 5.
According to Remark 5, for subspaces X1 and X2, we can get their principal orthonormal bases U1 and U2, respectively.
After projection by multiplying a standard Gaussian random 16 matrix Φ, the original basis matrix changes to Ai = ΦUi = [ai,1,··· , ai,di] , whose columns are no longer unitary and orthogonal to each other.
Then we normalize each columns as kai,diki , whose columns are now unitary but still not ¯Ai = [¯ai,1,··· , ¯ai,di] = h ai,1 orthogonal to each other.
However, by Corollary 3, we know that ¯Ai can be used as a good approximation for the orthonormal basis of Yi. We will see that ¯Ai plays an important role in estimating the aﬃnity after projection.
,··· , kai,1k ai,di We also need to deﬁne an accurate orthonormal basis for the projected subspace.
One eﬃcient way is to process ¯Ai by using Gram-Schmidt orthogonalization, whose result is deﬁned as Vi = [vi,1,··· , vi,di] , i = 1, 2.
5.1 Main Body In order to prove Theorem 3, we need to calculate aﬀ 2 Because ¯Ai is very close to an orthonormal matrix, we may use kVT aﬃnity after projection.
By using triangle inequality, we have Y and estimate its bias from aﬀ Y .
¯A1k to estimate the (35) aﬀ 2 Y − aﬀ (cid:12)(cid:12)(cid:12) Y(cid:12)(cid:12)(cid:12) ≤(cid:12)(cid:12)(cid:12) aﬀ 2 Y −(cid:13)(cid:13)VT ¯A1(cid:13)(cid:13) F(cid:12)(cid:12)(cid:12) +(cid:12)(cid:12)(cid:12)(cid:13)(cid:13)VT ¯A1(cid:13)(cid:13) F − aﬀ Y(cid:12)(cid:12)(cid:12) Therefore, the following proof can be divided into three steps.
The ﬁrst step is to bound the error caused by using ¯A1 as an approximation of V1 to compute the aﬃnity.
To do that, we will introduce an important lemma, which is the essence of the proof.
The second step is to bound the diﬀerence between the approximated aﬃnity and our estimate, which can be derived by using Lemma 4.
Finally, we combine these two bounds and complete the proof.
Step 1) For the ﬁrst item in the RHS of (35), according to the deﬁnition of aﬃnity, we have (cid:12)(cid:12)(cid:12) aﬀ 2 Y −(cid:13)(cid:13)VT ¯A1(cid:13)(cid:13) F(cid:12)(cid:12)(cid:12) d1 =(cid:12)(cid:12)(cid:12)kVT 2 V1k2 =(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) Xk=1(cid:16)kVT Xk=1(cid:12)(cid:12)(cid:12)kVT F(cid:12)(cid:12)(cid:12) ¯A1(cid:13)(cid:13) F −(cid:13)(cid:13)VT 2(cid:17)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 2 v1,kk2 −(cid:13)(cid:13)VT 2 ¯a1,k(cid:13)(cid:13) 2(cid:12)(cid:12)(cid:12) 2 v1,kk2 −(cid:13)(cid:13)VT 2 ¯a1,k(cid:13)(cid:13) d1 (36) Lemma 10 There exist constants c1,1(ε1) and c2,1(ε1) > 0 depending only on ε1, such that for any n > c1,1(ε1)d2, we have (cid:12)(cid:12)(cid:12)kVT 2 v1,kk2 −(cid:13)(cid:13)VT 2 ¯a1,k(cid:13)(cid:13) hold with probability at least 1 − e−c2,1(ε1)n.
Proof The proof is postponed to Section 5.2. 2(cid:12)(cid:12)(cid:12) ≤(cid:0)1 − λ2 k(cid:1) ε1, ∀k = 1,··· , d1 (37) 17 Plugging (37) into (36), we have (cid:12)(cid:12)(cid:12) aﬀ 2 Y −(cid:13)(cid:13)VT ¯A1(cid:13)(cid:13) F(cid:12)(cid:12)(cid:12) ≤ ε1 d1 Xk=1(cid:0)1 − λ2 k(cid:1) (38) hold with probability at least 1 − d1e−c2,1(ε1)n for any n > c1,1(ε1)d2.
Step 2) For the second estimation error in the RHS of (35), we can convert this problem about one subspace Y1 with dimension d1 into d1 subproblems, each of which is about 1- dimensional subspace, and then use Lemma 4 to estimate the error.
Denote X1,k := C{u1,k}, Y1,k := C{a1,k}, 1 ≤ k ≤ d1.
According to the deﬁnition of aﬃnity and Remark 5, we have that the aﬃnity between X1,k and X2 is kUT 2 u1,kk = λk, and the aﬃnity between Y1,k and Y2 is equal to (cid:13)(cid:13)VT and Y1,k and Y2 are the projected subspaces, we have Yk(cid:12)(cid:12)(cid:12) ≤(cid:0)1 − λ2 k(cid:1) ε2, 2 ¯a1,k(cid:13)(cid:13).
Using Lemma 4, where X1,k and X2 are the original subspaces (cid:12)(cid:12)(cid:12)(cid:13)(cid:13)VT 2 ¯a1,k(cid:13)(cid:13) hold with probability at least 1 − d1e−c2,2(ε2)n for any n > c1,2(ε2)d2, where k = 1,··· , d1 − aﬀ (39) Plugging the deﬁnition of aﬀ we have F − aﬀ (cid:12)(cid:12)(cid:12)(cid:13)(cid:13)VT ¯A1(cid:13)(cid:13) Y(cid:12)(cid:12)(cid:12) aﬀ Yk := λ2 k + d2 n (cid:0)1 − λ2 k(cid:1) .
Y in (4), (39), and (40) into the second estimation error, d2 k + (d1 − aﬀ 2 d2 (1 − λ2 X )(cid:19)(cid:12)(cid:12)(cid:12)(cid:12) k)(cid:19)(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) d1 =(cid:12)(cid:12)(cid:12)(cid:12)(cid:13)(cid:13)VT F −(cid:18)aﬀ 2 ¯A1(cid:13)(cid:13) X + =(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) −(cid:18)λ2 Xk=1(cid:18)(cid:13)(cid:13)VT 2 ¯a1,k(cid:13)(cid:13) Xk=1(cid:12)(cid:12)(cid:12)(cid:13)(cid:13)VT Yk(cid:12)(cid:12)(cid:12) 2 ¯a1,k(cid:13)(cid:13) − aﬀ Xk=1 (1 − λ2 k).
≤ ε2 d1 d1 (41) (40) (42) Step 3) Combining (38) and (41) together into (35), we have that for any n > max{c1,1(ε1), c1,2(ε2)}d2, aﬀ 2 Y − aﬀ (cid:12)(cid:12)(cid:12) d1 Y(cid:12)(cid:12)(cid:12) ≤ (ε1 + ε2) Xk=1(cid:0)1 − λ2 k(cid:1) = (ε1 + ε2)(cid:0)d1 − aﬀ 2 X(cid:1) hold with probability at least 1 − d1e−c2,1(ε1)n − d1e−c2,2(ε2)n.
Let ε1 = ε2 =: ε/2, according to Remark 1, one can easily verify that there exist constants c1, c2 depending only on ε, when n > c1(ε)d2, holds with probability at least 1 − e−c2(ε)n.
Then we complete the proof.
aﬀ 2 Y − aﬀ (cid:12)(cid:12)(cid:12) Y(cid:12)(cid:12)(cid:12) ≤(cid:0)d1 − aﬀ 2 X(cid:1) ε 18 5.2 Proof of Lemma 10 In order to improve the readability of the proof, we deﬁne intensively all the variables required in advance.
Not that some variables deﬁned before are also summarized here to make this part self-contained.
We use X1 and X2 to denote the subspaces before projection, with dimensions d1 ≤ d2.
The principal orthonormal bases for X1 and X2 are denoted as U1 and U2, respectively.
The kth column of Ui is denoted as ui,k, which spans a 1-dimensional subspace denoted as Xi,k, k = 1,··· , di, i = 1, 2.
In addition, we deﬁne Ui,1:k as the matrix composed of the ﬁrst k columns of Ui. That is Ui,1:k = [ui,1,··· , ui,k] , 1 ≤ k ≤ di, i = 1, 2.
The subspace spanned by the columns of Ui,1:k is denoted as Xi,1:k = C(Ui,1:k).
We use Y1 and Y2 to denote the subspaces after projection, respectively, from X1 and X2 by using a standard Gaussian random matrix Φ.
The dimensions of Y1 and Y2 stay to be d1 and d2 with probability 1.
Ai = ΦUi is a basis for Yi and its kth column is denoted as ai,k, which spans a 1-dimensional subspace denoted as Yi,k = C(ai,k).
We deﬁne Ai,1:k = [ai,1,··· , ai,k] as the composition of the ﬁrst k columns of Ai. The subspace spanned by the columns in Ai,1:k is denoted as Yi,1:k = C(Ai,1:k).
We use ¯A1 and ¯A2 to denote the column-normalized result of A1 and A2, respectively.
V1 and V2 are deﬁned as the orthonormalized result of ¯A1 and ¯A2, respectively, by using Gram-Schmidt orthogonalization.
As a consequence, Vi provides an orthonormal basis for Yi. Similarly, Vi,1:k denotes the matrix composed of the ﬁrst k columns of Vi. Let’s start the proof of Lemma 10 from the LHS of (37).
According to the deﬁnition of V2, v1,k, ¯a1,k, and Remark 4, we have kVT kVT 2 v1,kk2 = kPY2(v1,k)k2 = 1 −(cid:13)(cid:13)(cid:13) 2 ¯a1,kk2 = kPY2(¯a1,k)k2 = 1 −(cid:13)(cid:13)(cid:13) PY ⊥ PY ⊥ (v1,k)(cid:13)(cid:13)(cid:13) (¯a1,k)(cid:13)(cid:13)(cid:13) (43) (44) As a consequence, the LHS of (37) is derived as the diﬀerence of squared norm of the projection of v1,k and ¯a1,k onto to the orthogonal complement of Y2, i.e. (45) 2 v1,kk2 − kVT (cid:12)(cid:12)kVT 2 ¯a1,kk2(cid:12)(cid:12) =(cid:12)(cid:12)(cid:12)(cid:12)(cid:13)(cid:13)(cid:13) PY ⊥ (v1,k)(cid:13)(cid:13)(cid:13) −(cid:13)(cid:13)(cid:13) PY ⊥ (¯a1,k)(cid:13)(cid:13)(cid:13) 2(cid:12)(cid:12)(cid:12)(cid:12) In order to analyze v1,k, we take a close look at the Gram-Schmidt orthogonalization process.
We introduce αk := kPY1,1:k−1(¯a1,k)k = kVT 1,1:k−1¯a1,kk as the cosine of the only principal angle between Y1,k and Y1,1:k−1, and bk := αk PY1,1:k−1(¯a1,k) 19 (46) (47) as a unit vector along the direction of the projection of a1,k onto Y1,1:k−1.
As a consequence, the Gram-Schmidt orthogonalization process is represented by Then we introduce ¯a1,k = PY1,1:k−1(¯a1,k) + PY ⊥ kv1,k.
= αkbk +q1 − α2 1,1:k−1 (¯a1,k) ˆλk := kPY2(¯a1,k)k = kVT βk := kPY2(bk)k = kVT 2 ¯a1,kk, 2 bkk (48) (49) (50) to denote, respectively, the cosine of the only principal angle between Y1,k and Y2 and that between C{bk} and Y2.
Now projecting both side of (48) on the orthogonal complement of Y2, we have (51) (bk).
q1 − ˆλ2 k ¯a⊥1,k = αkq1 − β2 kb⊥k +q1 − α2 kPY ⊥ (v1,k), where ¯a⊥1,k and b⊥k denotes, respectively, the unit vectors along PY ⊥ (¯a1,k) and PY ⊥ Moving the ﬁrst item in the RHS of (51) to the LHS and then taking norm on both sides, we get (1 − α2 k)kPY ⊥ (v1,k)k2 = 1 − ˆλ2 k + α2 k(cid:0)1 − β2 k(cid:1) − 2αkq1 − ˆλ2 kq1 − β2 kh¯a⊥1,k, b⊥k i.
(52) In addition, the norm of the projection of ¯a1,k onto to Y⊥2 could be directly represented by using ˆλk as Inserting both (52) and (53) into (45), we write kPY ⊥ (¯a1,k)k2 = 1 − ˆλ2 k.
(53) k + α2 2 v1,kk2 − kVT (cid:12)(cid:12)kVT =(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1 − ˆλ2 k(cid:0)1 − β2 k + 1 − β2 k(cid:16)1 − ˆλ2 α2 2 ¯a1,kk2(cid:12)(cid:12) k(cid:1) − 2αkq1 − ˆλ2 1 − α2 k(cid:17) + 2(cid:12)(cid:12)(cid:12)(cid:12) αkq1 − ˆλ2 1 − α2 kq1 − β2 kh¯a⊥1,k, b⊥k i − (1 − ˆλ2 kq1 − β2 kh¯a⊥1,k, b⊥k i(cid:12)(cid:12)(cid:12)(cid:12) k)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) (54) Using the fact that geometric mean is no more than arithmetic mean and α2 will be veriﬁed soon that α2 k is a small quantity, we further reshape (54) as k < 1/3, which k +(cid:12)(cid:12)(cid:12) k(cid:17)(cid:16)α2 In the following, we will estimate the four quantities of 1− ˆλ2 (cid:12)(cid:12)(cid:12)kVT 2 v1,kk2 −(cid:13)(cid:13)VT 2(cid:12)(cid:12)(cid:12) ≤ 2 ¯a1,k(cid:13)(cid:13) 2(cid:16)1 − ˆλ2 k + 1 − β2 αkh¯a⊥1,k, b⊥k i(cid:12)(cid:12)(cid:12)(cid:17) .
k, 1− β2 k, α2 k, and h¯a⊥1,k, b⊥k i (55) separately.
20 Using basic algebra, (56) is reshaped to λ2 k + d2 n (cid:0)1 − λ2 1 − ˆλ2 k ≤1 − λ2 k − =(cid:0)1 − λ2 <(cid:0)1 − λ2 d2 k(cid:1) − ˆλ2 n (cid:0)1 − λ2 k(cid:1)(cid:18)1 − k(cid:1) (1 + ε1) , k ≤(cid:0)1 − λ2 k(cid:1) ε1, k(cid:1) +(cid:0)1 − λ2 + ε1(cid:19) k = 1,··· , d1.
k(cid:1) ε1 d2 (56) (57) Let us ﬁrst consider 1 − ˆλ2 k.
Recalling its deﬁnition in (49), we have already estimate ˆλ2 k in (39).
Inserting (40) into (39), with probability at least 1− e−c2,1(ε1)n, we have for any n > c1,1(ε1)d2 The bound of (57) looks direct because ˆλk denotes the aﬃnity compressed from λk.
According to Lemma 4, the former can be estimated by the latter.
Second let us check αk = kVT 1,1:k−1¯a1,kk.
Intuitively, because X1,1:k−1 is orthogonal to X1,k, the new subspaces Y1,1:k−1 (projected from X1,1:k−1) and Y1,k (projected from X1,k) are approximately orthogonal to each other.
Actually, V1,1:k−1 and ¯a1,k satisfy all conditions in Corollary 4.
As a consequence, there exist constants c1,2(ε2), c2,2(ε2), such that for any ε2 < 1 k < ε2 hold with probability at least 1 − e−c2,2(ε2)n.
3 and n > c1,2(ε2)d1 > c1,2(ε2)(k − 1), we have α2 Next we consider 1 − β2 k, which is also bounded by 1 − λ2 k, Notice that bk lies in Y1,1:k−1 ⊂ Y1,1:k and βk is the norm of the projection of bk onto Y2.
Because the minimum of the norm of the projection of a unit vector in Y1,1:k onto Y2 approximates λk, 1 − β2 should be very close to 1−λ2 k.
The diﬀerence between them can be bounded by the following lemma.
Lemma 11 For any n > c1,3(ε3)d2, we have 1 − β2 k ≤(cid:0)1 − λ2 k(cid:1) (1 + ε3) holds with probability at least 1 − e−c2,3(ε3)n.
Proof The proof is postponed to Appendix 8.8. (58) Finally, as for the last term to be estimated, h¯a⊥1,k, b⊥k i is proved to be a small quantity in Lemma 12.
Intuitively, ¯a⊥1,k and b⊥k is unit projections of ¯a1,k ∈ Y1,k and bk ∈ Y1,1:k−1, respectively, onto Y⊥2 .
Consequently, the inner product between ¯a⊥1,k and b⊥k should be very small if Y1,k and Y1,1:k−1, which are independent with each other, are both independent with Y⊥2 .
Lemma 12 There exist constants c1,4(ε4), c2,4(ε4), such that for any n > c1,4(ε4)d2, we have (cid:12)(cid:12)(cid:12)h¯a⊥1,k, b⊥k i(cid:12)(cid:12)(cid:12) Proof The proof is postponed to Appendix 8.9. < ε4 holds with probability at least 1 − e−c2,4(ε4)n.
21 Now, we are ready to complete the proof by using the concentration properties derived < ε4 into (55), we have for any above.
Plugging (57), (58), α2 n > max{c1,l(εl)}d2, l = 1, 2, 3, 4, k < ε2, and (cid:12)(cid:12)(cid:12)h¯a⊥1,k, b⊥k i(cid:12)(cid:12)(cid:12) 2(cid:12)(cid:12)(cid:12) ≤ 2 ¯a1,k(cid:13)(cid:13) hold with probability at least 1 −P4 (cid:12)(cid:12)(cid:12)kVT 2 v1,kk2 −(cid:13)(cid:13)VT 2(cid:0)1 − λ2 then we have k(cid:1) (2 + ε1 + ε3) (ε2 + √ε2ε4) l=1 e−c2,l(εl)n.
Let ε1 < 1 and ε3 < 1, ε2 = ε4 =: ε/12, (cid:12)(cid:12)(cid:12)kVT 2 v1,kk2 −(cid:13)(cid:13)VT 2 ¯a1,k(cid:13)(cid:13) 2(cid:12)(cid:12)(cid:12) ≤(cid:0)1 − λ2 k(cid:1) ε.
According to Remark 1, we claim that there exist constants c1, c2, such that for any n > c1(ε)d2, (59) holds with probability at least 1 − e−c2(ε)n.
(59) 6 Related Works Our earlier results on the RIP of subspaces in [21] are cited below.
Theorem 4 Suppose X1,X2 ⊂ RN are two subspaces with dimension d1 ≤ d2, respectively.
If X1 and X2 are projected into Rn by a Gaussian random matrix Φ ∈ Rn×N , Xk −→ Yk, k = 1, 2, then we have (1 − ε)D2 X ≤ D2 Y ≤ (1 + ε)D2 X , with probability at least when n is large enough.
1 − 4d1 (ε − d2/n)2n Theorem 5 For any set composed by L subspaces X1,··· ,XL ∈ RN of dimension no more than d, if they are projected into Rn by a Gaussian random matrix Φ ∈ Rn×N , Xk −→ Yk, k = 1,··· , L, and d ≪ n < N , then we have (1 − ε)D2(Xi,Xj) ≤ D2(Yi,Yj) ≤ (1 + ε)D2(Xi,Xj), ∀i, j with probability at least when n is large enough.
1 − 2dL(L − 1) (ε − d/n)2n Compared with the our previous results, this paper has the following two main improve- ments.
Firstly, because we use more advanced random matrix theories and deal with the error more skillfully, the probability bound 1− e−O(n) derived in this paper is much tighter than the 1 − O(1/n) in the previous work, where we used Chebyshev inequality.
Such 22 improvement provides a more accurate law of magnitude of the dimensions in this random projection problem, and the improved probability bound is optimum, if one compares it with the analogical conclusions in the theory of Compressed Sensing.
Secondly, Theorem 5 requires n ≫ d, but it does not specify how large n should be or the connection between ε, d, L, and the lower bound of n.
In comparison, Theorem 1 in this paper rigorously clariﬁes that the conclusion will hold as long as n is larger than c1(ε) max{d, ln L}.
7 Conclusion In this paper, we utilize the random matrix theory to rigorously prove the RIP of Gaussian random compressions for low-dimensional subspaces.
Mathematically, we demonstrate that as long as the dimension after compression n is larger than c1(ε) max{d, ln L}, with proba- bility no less than 1 − e−c2(ε)n, the distance between any two subspaces after compression remains almost unchanged.
The probability bound 1− e−O(n) is optimum in the asymptotic sense, in comparison with the analogical optimum theoretical result of RIP in Compressed Sensing.
Our work can provide a solid theoretical foundation for Compressed Subspace Clustering and other low-dimensional subspace related problems.
8 Appendix 8.1 Proof of Lemma 6 We ﬁrst prove the special case that K = 1, i.e., f = ae−g for short.
According to (13), (14), and the deﬁnition of limitation, there exist constants n0 and c1 > 0 depending only on ε.
When n > n0, τ < c1, we have g 3 > 0 depending only on ε, we can have 3 .
Let c2 := h−b n > h − h−b 3 , and ln a n < b + h−b f = ae−g = e−(g−ln a) = exp(cid:18)−n(cid:18) g n − ≤ exp(cid:18)−n(cid:18)h − 3 (cid:19)(cid:19) h − b = exp(cid:18)− h − b h − b 3 − b − n(cid:19) = e−c2n.
ln a n (cid:19)(cid:19) Now we consider the general case of arbitrary K.
According to the above analysis, we have that, for each term of f , there exist constants n0,k, c1,k > 0, and c2,k > 0 depending only on ε.
When n > n0,k, τ < c1,k, it satisﬁes that ake−gk < e−c2,kn.
Let n0 := maxk n0,k, c1 := mink c1,k > 0, c2 := mink c2,k > 0.
Then when n > n0, τ < c1, we have that f = Xk=1 ake−gk < Xk=1 e−nc2,k ≤ e−c2n, and complete the proof.
23 8.2 Proof of Lemma 7 Regarding √na as a matrix belonging to Rn×1, and using Lemma 5, we have that with probability at least 1 − 2e− t 2 , √n − 1 − t ≤ smin(√na) = √nkak = smax(√na) ≤ √n + 1 + t.
Taking square and subtracting n from both sides, we have −(cid:16)2√n(1 + t) + (1 + t)2(cid:17) ≤ −(cid:16)2√n(1 + t) − (1 + t)2(cid:17) ≤ nkak2−n ≤ 2√n(1+t)+(1 + t)2 , with probability at least 1 − 2e− t By choosing ε satisfying nε = 2√n(1 + t) + (1 + t)2, we can get 2 .
t = √n(cid:0)√1 + ε − 1 − 1/√n(cid:1) .
=: n0,1, we have t > 0.
Substituting this equation into the expression (60) (61) When n >(cid:16) of probability, we have 1√ε+1−1(cid:17)2 P(cid:16)(cid:12)(cid:12)(cid:12)kak2 − 1(cid:12)(cid:12)(cid:12) > ε(cid:17) < 2 exp(cid:16)−n(cid:0)√1 + ε − 1 − 1/√n(cid:1)2 /2(cid:17) .
According to Lemma 6, there exist constants n0,2 and c dependent on ε, such that the RHS of (61) is smaller than e−cn.
Taking n0 = max{n0,1, n0,2}, we complete the proof.
8.3 Proof of Corollary 2 nε d , we have according to (61) in the proof of Lemma 7, by replacing ε with nε d VTa ∈ Rd is a standard Gaussian random vector.
As a consequence, Proof Notice thatp n P (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) /2! , r n VTa(cid:13)(cid:13)(cid:13)(cid:13) (cid:13)(cid:13)(cid:13)(cid:13) d ε+1−1(cid:19)2 d ε+1−1(cid:19)2 where d >(cid:18) 1√ n 1√ n 1 ≤ d, we need n > 3d ε =: c1,1d.
According to Lemma 6, there exist constants c1,2, c2 de- pendent on ε, such that the RHS of (62) is smaller than e−c2n.
Taking c1 := max{c1,1, c1,2} and dividing both sides of the expression in P(·) in (62) by n/d, we complete the proof.
ε − 1 − 1/√d(cid:19)2 − 1(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) is required.
In order to satisfy this requirement, i.e.,(cid:18) d ! < 2 exp −d(cid:18)r1 + (62) 8.4 Proof of Corollary 3 In order to bound s2 min(cid:0) ¯A(cid:1), noticing that min(cid:0) ¯A(cid:1) ≥ s2 s2 min (A) max i kaik2 , (63) we may turn to estimate s2 min (A) and maxi kaik2 separately.
24 We begin from estimating s2 min (A).
According to Lemma 5, with probability at least 1 − e−t2/2, we have s2 min (A) ≥ Let 1 − ε1 be the RHS of (64) then we have =(cid:16)1 −pk/n − t/√n(cid:17)2 √k − t(cid:17)2 n(cid:16)√n − t = √n(cid:16)1 − √1 − ε1 −pk/n(cid:17) .
(64) (65) When n > (1−√1−ε1)2 =: ˆc0,1k, we have t > 0.
Plugging (65) into e−t2/2, we can get /2(cid:19) .
According to the probability that (65) violates as exp(cid:18)−n(cid:16)1 −pk/n − √1 − ε1(cid:17)2 Lemma 6, the above probability can be bounded by e−ˆc2,1(ε1)n for n > ˆc1,1k.
Then we have s2 min (A) ≥ 1 − ε1 (66) hold for n > max{ˆc1,1, ˆc0,1}k =: ˆc3k with probability at least 1 − e−ˆc2,1(ε1)n.
Next we estimate maxi kaik2.
According to Lemma 7, with probability at least 1 − ke−ˆc2,2(ε2)n, for n > n0,1, we have (67) (68) Plugging (66) and (67) into (63), we have i kaik2 ≤ 1 + ε2.
max s2 min(cid:0) ¯A(cid:1) ≥ 1 − ε1 1 + ε2 = 1 − ε1 + ε2 1 + ε2 Let ε1 = ε2 =: ε/2, with probability at least 1 − e−ˆc2,1(ε/2)n − ke−ˆc2,2(ε/2)n, we have s2 min(cid:0) ¯A(cid:1) ≥ 1 − (ε/2 + ε/2) = 1 − ε.
Take c1,1 := max{ˆc3, n0,1} and we prove the ﬁrst part of this corollary.
In order to bound s2 the counterparts of (63), (66), and (67), respectively, as max(cid:0) ¯A(cid:1), following the same approach, we could derive step by step s2 s2 max (A) min i kaik2 , max(cid:0) ¯A(cid:1) ≤ s2 max (A) ≤ 1 + ε1 for n > ˆc4k with probability at least 1 − exp(cid:18)−n(cid:16)1 +pk/n − √1 + ε1(cid:17)2 /2(cid:19) > 1 − e−ˆc2,3n, and i kaik2 ≥ 1 − ε2 min 25 (69) (70) (71) for n > n0,2 with probability at least 1 − ke−ˆc2,2(ε2)n − e−ˆc2,3(ε1)n.
Then we have s2 max(cid:0) ¯A(cid:1) ≤ 1 + ε1 1 − ε2 = 1 + ε1 + ε2 1 − ε2 (72) Similarly reshaping (72) and letting ε2 ≤ 1/2, ε1 = ε2 =: ε/4, taking c1,2 := max{ˆc4, n0,2}, we prove the second part of the corollary.
8.5 Proof of Lemma 8 Using the orthogonality between u1 and U2, C(A2) is independent with a1.
As an or- thonormal basis of such subspace, V2 is also independent with a1.
Then, according to Deﬁnition 6, a1 conditioned on V2 is still a standard Gaussian random vector.
As a conse- quence, √nVT 2 a1 ∈ Rd×1, the entries of which are independent standard Gaussian random variables, satisﬁes the condition in Lemma 5.
With probability no more than e− t 2 , we have (73) (cid:13)(cid:13) Let ε :=(cid:16)√d + 1 + t(cid:17)2 √nVT = s2 2 a1(cid:13)(cid:13) /n, we can get max(cid:0)√nVT t = √nε − 2 a1(cid:1) ≥(cid:16)√d + 1 + t(cid:17)2 √d − 1.
When n > 4d ε =: c1,1d, we have t > √d − 1 ≥ 0.
Plugging (74) into e− t (73) holding is at least 2 exp(cid:18)−(cid:16)√nε − √d − 1(cid:17)2 /2(cid:19) .
According to Lemma 6, there exist constants c1,2, c2, such that when n > c1,2d, this probability is smaller than e−c2n.
Taking c1 := max{c1,1, c1,2} and dividing both sides of (73) by n, we conclude the lemma.
(74) 2 , the probability of 8.6 Proof of Corollary 4 According to the deﬁnition of ¯a1 and basic probability, we have P(cid:16)(cid:13)(cid:13)VT 2 ¯a1(cid:13)(cid:13) > ε(cid:17) =P (cid:13)(cid:13)VT ka1k2 > ε! 2 a1(cid:13)(cid:13) ka1k2 < ε! =1 − P (cid:13)(cid:13)VT 2 a1(cid:13)(cid:13) ≤1 − P(cid:16)ka1k2 > 1 − ε and (cid:13)(cid:13)VT < ε (1 − ε)(cid:17) 2 a1(cid:13)(cid:13) =P(cid:16)ka1k2 < 1 − ε or (cid:13)(cid:13)VT > ε (1 − ε)(cid:17) 2 a1(cid:13)(cid:13) ≤P(cid:0)ka1k2 < 1 − ε(cid:1) + P(cid:16)(cid:13)(cid:13)VT > ε (1 − ε)(cid:17) .
2 a1(cid:13)(cid:13) (75) Now we may estimate the two items in the RHS of (75), separately.
By using Lemma 7, for n > n0, we have P(ka1k2 < 1 − ε) < e−c2,1(ε)n.
(76) 26 By using Lemma 8, for ε < 1 3 and n > c1,2d, we have Plugging (76) and (77) into (75), we readily get P(cid:16)(cid:13)(cid:13)VT 2 a1(cid:13)(cid:13) P(cid:16)(cid:13)(cid:13)VT 2 ¯a1(cid:13)(cid:13) n > c1(ε)d, we have (cid:13)(cid:13)VT 2 ¯a1(cid:13)(cid:13) 8.7 Proof of Remark 3 > ε(1 − ε)(cid:17) < e−c2,2(2ε/3)n.
> ε(cid:17) < e−c2,1(ε)n + e−c2,2(2ε/3)n.
According to Remark 1, we claim that there exist constants c1, c2, such that for any > ε with probability no more than e−c2(ε)n.
(77) (78) Replacing n with n − d0 in (20), we can get P(cid:16)(cid:13)(cid:13)VT 2 ¯a1(cid:13)(cid:13) > ε(cid:17) ≤ e−ˆc2(ε)(n−d0).
We only need to prove that there exist constants c1, c2, when n > c1 max{d, d0}, we have n , according to Lemma 6, we have P(cid:16)(cid:13)(cid:13)VT 2 ¯a1(cid:13)(cid:13) > ε(cid:17) ≤ e−ˆc2(ε)(n−d0) ≤ e−c2n.
Let τ := d0 ˆc2(ε) (n − d0) = lim τ→0 h : = lim τ→0 b : = lim n→∞ Then when n > n0, τ = d0 By choosing c1 := max{n0, 1 condition of (78) holding, that is n > ˆc1d, is also satisﬁed.
lim n→∞ ln 1 n ≤ τ0, there exists constant c2, such that e−ˆc2(ε)(n−d0) ≤ e−c2n.
n ≤ τ0.
The , ˆc1}, when n > c1d0, we have n > n0, τ = d0 ˆc2 (1 − τ ) = ˆc2 > 0, lim n→∞ = 0 < h.
τ0 8.8 Proof of Lemma 11 Using the deﬁnition of βk in (50) and the fact that Y1,1:k−1 ⊂ Y1,1:k, we have β2 k =(cid:13)(cid:13)VT 2 bk(cid:13)(cid:13) = min kbk=1 b∈Y1,1:k−1 kVT 2 bk2 ≥ min kbk=1 b∈Y1,1:k kVT 2 bk2.
Removing both side of (79) from one, we write 1 − β2 k ≤ 1 − min kbk=1 b∈Y1,1:k kVT 2 bk2 = 1 − min kbk=1 b∈Y1,1:k kPY2(b)k2 = max kbk=1 b∈Y1,1:k kPY ⊥ (b)k2.
(79) (80) Then we will loose the condition and rewrite the expression of this maximization problem step by step, and ﬁnally convert it to a problem about the extreme singular value of random matrix.
For any vector b in Y1,1:k, it can be spanned by the columns of ¯A1,1:k as b = ¯A1,1:kx = xj ¯a1,j, Xj=1 27 (81) where x = [x1,··· , xk]T denotes the weight vector.
Consequently, the condition of kbk = 1 can be loosen to the condition on x, i.e., kxk2 ≤ (cid:13)(cid:13) s2 ¯A1,1:kx(cid:13)(cid:13) min(cid:0) ¯A1,1:k(cid:1) Inserting (81) and (82) in (80), we have 1 − β2 k ≤ max s2 s2 kbk2 min(cid:0) ¯A1,1:k(cid:1) kxk2≤xu(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2  Xj=1 PY ⊥ kxk2≤xu(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) Xj=1 xjPY ⊥ kxk2≤xu(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) Xj=1 min(cid:0) ¯A1,1:k(cid:1) (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) xj ¯a1,j (¯a1,j)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) j ¯a⊥1,j(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) xjq1 − ˆλ2 = max = max =: xu.
(82) (83) where ˆλj is deﬁned in (49).
According to (57) and the decreasing order of λ1 ≥ ··· ≥ λd1, we have 1 − ˆλ2 j ≤ (1 − λ2 j )(1 + ε) ≤ (1 − λ2 k)(1 + ε), ∀j = 1,··· , k < d1, (84) hold with probability at least 1 − e−c2,1(ε1)n for any n > c1,1(ε1)d2.
Inserting (84) in (83), we have 1 − β2 k ≤ (1 − λ2 k)(1 + ε1) max ≤ (1 − λ2 k)(1 + ε1)s2 = (1 − λ2 k)(1 + ε1) Xj=1 xj ¯a⊥1,j(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) kxk2≤xu(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) max(cid:16) ¯A⊥1,1:k(cid:17) xu max(cid:16) ¯A⊥1,1:k(cid:17) min(cid:0) ¯A1,1:k(cid:1) s2 s2 (85) where ¯A⊥1,1:k =h¯a⊥1,1,··· , ¯a⊥1,ki .
Now we need to bound the denominator and numerator in the RHS of (85), separately.
As to the denominator, according to Corollary 3, we have for n > c1,2(ε2)d1, P(cid:0)s2 min(cid:0) ¯A1,1:k(cid:1) > 1 − ε2(cid:1) > 1 − e−c2,2(ε2)n.
(86) As for estimating the numerator, since that A1,1:k is correlated with Y2, we can not directly apply the available lemmas about the concentration inequalities of independent Gaussian random matrix.
However, by using the following techniques, we could manage to convert the problem of estimating s2 normalized random matrix satisfying the independence condition.
max(cid:16) ¯A⊥1,1:k(cid:17) to a problem about the singular value of a 28 Remark 6 Recalling Remark 5 about the characteristics of principal orthonormal bases U1, U2 for subspaces X1,X2, and following the decomposition way in the proof of Lemma 4, we can decompose each column of U1 as the projections onto X2 and its orthogonal complement and get U1 = U2Λ + U0Λ⊥, (87) where Λ⊥ = p1 − λ2 .
.
.
  d1 q1 − λ2 2 U0 = 0.
and C(U0) ∈ RN×d1 is a subspace of X ⊥2 , that is UT After random projection, the decomposition in Remark 6 changes to A1 = A2Λ + A0Λ⊥, (88) where A0 = ΦU0.
Projecting both side of (88) onto the orthogonal complement of Y2, we have (A1) = PY ⊥ which means that the normalized column of PY ⊥ identical to the normalized column of PY ⊥ That is PY ⊥ (A0)Λ⊥, (A1), i.e., ¯a⊥1,k deﬁned in (51) are exactly (A0), which is denoted as ¯a⊥0,k, k = 1,··· , d1.
¯A⊥1:1:k =h¯a⊥0,1,··· , ¯a⊥0,ki =: ¯A⊥0,1:k.
(89) Considering its property of isotropy, a Gaussian random vector remains Gaussian distri- bution when it is projected to an independent subspace.
This is demonstrated in Remark 7.
Remark 7 Let A1 ∈ Rn×d1 and A2 ∈ Rn×d2, d1 ≤ d2 be two Gaussian random matrices.
We denote V2 as an orthonormal basis of C(A2).
The projection of A1 = [a1,1,··· , a1,d1] onto C(A2) is denoted by B1 = [b1,1,··· , b1,d1], i.e., b1,k = PC(A2)(a1,k).
If A1 and A2 are independent, we have B1 = V2Ω, where Ω ∈ Rd2×d1 is a Gaussian random matrix.
This can be readily veriﬁed by using the fact that B1 = V2VT 2 A1 =: V2Ω, where Ω = (ωi,j) := VT 2 A1.
Because A1 is independent with C(A2), as well as its orthonormal basis V2, the distribution of A1 is not inﬂuenced, if we ﬁrst condition V2 and regard it as a given matrix.
Consequently, we can readily check that ωi,j are i.i.d. zero mean Gaussian random variables.
29 Recalling that UT 0 U2 = 0, which means uT 0,iu2,j = 0, 1 ≤ i ≤ d1, 1 ≤ j ≤ d2.
According to Lemma 9, we have that a0,i and a2,j are independent.
Moreover, a0,i is independent with Y2 and thus independent with its orthogonal complement, Y⊥2 .
Then according to Remark 7, the projection of A0,1:k ∈ Rn×k onto Y⊥2 can be written as A⊥0,1:k :=ha⊥0,1,··· , a⊥0,ki = V⊥2 Ω1:k, (a0,j), V⊥2 ∈ Rn×(n−d2) is an arbitrary orthonormal basis of Y⊥2 , and where a⊥0,j := PY ⊥ Ω1:k ∈ R(n−d2)×k is a Gaussian random matrix.
According to the orthonormality of V⊥2 , we normalize both sides of (90) as (90) ¯A⊥0,1:k = V⊥2 ¯Ω1:k, (91) where ¯Ω1:k denotes the column-normalized Ω1:k.
Because left multiplying an orthonormal matrix does not change its singular value, we have Combining (89) and (92), and using Remark 2, we have smax(cid:16) ¯A⊥0,1:k(cid:17) = smax(cid:0) ¯Ω1:k(cid:1) .
P(cid:16)s2 max(cid:16) ¯A⊥1,1:k(cid:17) < 1 + ε3(cid:17) = P(cid:0)s2 max(cid:0) ¯Ω1:k(cid:1) < 1 + ε3(cid:1) > 1 − e−c2,3(ε3)n (92) (93) hold when n > c1,3(ε3)d2.
Plugging both bounds of denominator and numerator, i.e., (86) and (93), into (85), we can get 1 − β2 k(cid:1) (1 + ε1) k(cid:1)(cid:18)1 + with probability at least 1 −P3 ε3 ≤ 1/2, ε1 = ε2 = ε3 =: ε/7, we have ε2+ε3 1−ε2 Remark 1 to reshape the probability, we readily complete the proof.
k ≤(cid:0)1 − λ2 =(cid:0)1 − λ2 l=1 e−c2,l(εl)n for any n > max{c1,l(εl)}d2.
Let ε2 ≤ 1/2, 1−ε2 ≤ 2 (ε2 + ε3) + 3ε1 = ε.
By using 1 − ε2(cid:19) , 1 + ε3 1 − ε2 ε2 + ε3 1 − ε2 1 + ε3 + ε1 + ε1 (94) 1+ε3 8.9 Proof of Lemma 12 We will calculate the inner product between ¯a⊥1,k and b⊥k .
Recalling that bk is the projection of a1,k onto C(A1,1:k−1), it is not obvious whether ¯a⊥1,k and b⊥k are independent, and this aggravate the problem to estimate their inner product directly.
In order to solve this, therefore, we have to ﬁnd the relationship between product and projection and then convert the problem to the situation described in Remark 3.
30 Recalling the previous result that ¯a⊥1,k = ¯a⊥0,k in (89) and using the fact of b⊥k ∈ C( ¯A⊥1,1:k−1), we write (cid:12)(cid:12)(cid:12)h¯a⊥1,k, b⊥k i(cid:12)(cid:12)(cid:12) =(cid:12)(cid:12)(cid:12)h¯a⊥0,k, b⊥k i(cid:12)(cid:12)(cid:12) ≤(cid:13)(cid:13)(cid:13) =(cid:13)(cid:13)(cid:13) PC( ¯A⊥ PC( ¯A⊥ 1,1:k−1)(¯a⊥0,k)(cid:13)(cid:13)(cid:13) 0,1:k−1)(¯a⊥0,k)(cid:13)(cid:13)(cid:13) (95) Now we need to construct an orthonormal basis for C( ¯A⊥0,1:k−1) and build its connection with ¯a⊥0,k.
Recalling Remark 7 and the deduction in the proof of Lemma 11, we reshape (91) as h ¯A⊥0,1:k−1, ¯a0,ki = V⊥2 (cid:2) ¯Ω1:k−1, ¯ωk(cid:3) , where ¯ωk denotes the last column of ¯Ω1:k ∈ R(n−d2)×k.
We next apply Gram-Schimidt orthogonalization to Ω1:k−1 and get W1:k−1, which is an orthonormal basis for C( ¯Ω1:k−1).
Because of the orthonormality of V⊥2 , V⊥2 W1:k−1 is an orthonormal basis for C( ¯A⊥0,1:k−1).
As a consequence, we are able to calculate the RHS of (95) as (cid:13)(cid:13)(cid:13) PC( ¯A⊥ 0,1:k−1)(¯a⊥0,k)(cid:13)(cid:13)(cid:13) =(cid:13)(cid:13)(cid:13)(cid:13)(cid:16)V⊥2 W1:k−1(cid:17)T =(cid:13)(cid:13)(cid:13)(cid:13)(cid:16)V⊥2 W1:k−1(cid:17)T =(cid:13)(cid:13)WT 1:k−1 ¯ωk(cid:13)(cid:13) .
¯a⊥0,k(cid:13)(cid:13)(cid:13)(cid:13) V⊥2 ¯ωk(cid:13)(cid:13)(cid:13)(cid:13) (96) Recalling that ¯Ω1:k is a column-normalized Gaussian random matrix, ¯ωk should be inde- pendent with each column of ¯Ω1:k−1, and thus independent with C( ¯Ω1:k−1) = C(W1:k−1).
Combining (95) and (96), and using Remark 3, we have P(cid:18)(cid:12)(cid:12)(cid:12)h¯a⊥1,k, b⊥k i(cid:12)(cid:12)(cid:12) > ε4(cid:19) ≤ P(cid:16)(cid:13)(cid:13)WT 1:k−1 ¯ωk(cid:13)(cid:13) for all n ≥ c1,4d2.
The proof is completed.
> ε4(cid:17) < e−c2,4(ε4)n (97) References [1] E.
Elhamifar and R.
Vidal, “Sparse subspace clustering,” in IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), 2009, pp.
2790–2797.
[2] M.
Soltanolkotabi and E.
J.
Candes, “A geometric analysis of subspace clustering with outliers,” The Annals of Statistics, vol.
40, no.
4, pp.
2195–2238, 2012.
[3] E.
Elhamifar and R.
Vidal, “Sparse subspace clustering: Algorithm, theory, and ap- plications,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol.
35, no.
11, pp.
2765–2781, 2013.
31 [4] R.
Heckel and H.
B¨olcskei, “Robust subspace clustering via thresholding,” IEEE Trans- actions on Information Theory, vol.
61, no.
11, pp.
6320–6342, 2015.
[5] X.
Mao and Y.
Gu, “Compressed subspace clustering: A case study,” in IEEE Global Conference on Signal and Information Processing, 2014, pp.
453 – 457.
[6] R.
Heckel, M.
Tschannen, and H.
Bolcskei, “Subspace clustering of dimensionality- reduced data,” in IEEE International Symposium on Information Theory, 2014, pp.
2997–3001.
[7] R.
Heckel, M.
Tschannen, and H.
B¨olcskei, “Dimensionality-reduced subspace cluster- ing,” arXiv preprint arXiv:1507.07105, 2015.
[8] Y.
Wang, Y.-X.
Wang, and A.
Singh, “A theoretical analysis of noisy sparse subspace clustering on dimensionality-reduced data,” arXiv preprint arXiv:1610.07650, 2016.
[9] W.
B.
Johnson and J.
Lindenstrauss, “Extensions of lipschitz maps into a hilbert space,” Contemporary mathematics, vol.
26, pp.
189–206, 1984.
[10] S.
Dasgupta and A.
Gupta, “An elementary proof of the johnson-lindenstrauss lemma,” International Computer Science Institute, Technical Report, pp.
99–006, 1999.
[11] E.
J.
Candes and T.
Tao, “Decoding by linear programming,” IEEE Transactions on Information Theory, vol.
51, no.
12, pp.
4203–4215, 2005.
[12] E.
J.
Cand`es, “The restricted isometry property and its implications for compressed sensing,” Comptes Rendus Mathematique, vol.
346, no.
9–10, pp.
589–592, 2008.
[13] R.
Baraniuk, M.
Davenport, R.
Devore, and M.
Wakin, “A simple proof of the restricted isometry property for random matrices,” Constructive Approximation, vol.
28, no.
28, pp.
253–263, 2015.
[14] D.
L.
Donoho, “Compressed sensing,” IEEE Transactions on Information Theory, vol.
52, no.
4, pp.
1289–1306, 2006.
[15] E.
J.
Candes, J.
Romberg, and T.
Tao, “Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information,” IEEE Transactions on Information Theory, vol.
52, no.
2, pp.
489–509, 2006.
[16] S.
Aeron, V.
Saligrama, and M.
Zhao, “Information theoretic bounds for compressed sensing,” IEEE Transactions on Information Theory, vol.
56, no.
10, pp.
5111–5130, 2010.
[17] E.
Candes and J.
Romberg, “Sparsity and incoherence in compressive sampling,” In- verse problems, vol.
23, no.
3, p.
969, 2007.
32 [18] Y.
C.
Eldar and G.
Kutyniok, Compressed sensing: theory and applications.
Cam- bridge University Press, 2012.
[19] A.
Eftekhari and M.
B.
Wakin, “New analysis of manifold embeddings and signal recov- ery from compressive measurements,” Applied and Computational Harmonic Analysis, vol.
39, no.
1, pp.
67–109, 2015.
[20] G.
Kutyniok, A.
Pezeshki, R.
Calderbank, and T.
Liu, “Robust dimension reduction, fu- sion frames, and grassmannian packings,” Applied and Computational Harmonic Anal- ysis, vol.
26, no.
1, pp.
64–76, 2009.
[21] G.
Li and Y.
Gu, “Restricted isometry property of gaussian random projection for ﬁnite set of subspaces,” IEEE Transactions on Signal Processing (to appear), arXiv preprint arXiv:1704.02109, 2017.
[22] A.
Edelman, T.
A.
Arias, and S.
T.
Smith, “The geometry of algorithms with orthogo- nality constraints,” SIAM Journal on Matrix Analysis and Applications, vol.
20, no.
2, pp.
303–353, 1998.
[23] M.
A.
Davenport, P.
T.
Boufounos, M.
B.
Wakin, and R.
G.
Baraniuk, “Signal pro- cessing with compressive measurements,” IEEE Journal of Selected Topics in Signal Processing, vol.
4, no.
2, pp.
445–460, 2010.
[24] T.
Blumensath and M.
E.
Davies, “Sampling theorems for signals from the union of ﬁnite-dimensional linear subspaces,” IEEE Transactions on Information Theory, vol.
55, no.
4, pp.
1872–1882, 2009.
[25] P.
K.
Agarwal, S.
Har-Peled, and H.
Yu, “Embeddings of surfaces, curves, and moving points in euclidean space,” in Proceedings of the twenty-third annual symposium on Computational geometry.
ACM, 2007, pp.
381–389.
[26] A.
Magen, “Dimensionality reductions that preserve volumes and distance to aﬃne spaces, and their algorithmic applications,” Randomization and approximation tech- niques in computer science, pp.
953–953, 2002.
[27] C.
Jordan, “Essai sur la g´eom´etrie `a n dimensions,” Bulletin de la Soci´et´e math´ematique de France, vol.
3, pp.
103–174, 1875.
[28] A.
Gal´antai and H.
C.
J., “Jordan’s principal angles in complex vector spaces,” Nuner- ical Linear Algebra with Applications, vol.
13, pp.
589–598, 2006.
[29] A.
Bj¨orck and G.
H.
Golub, “Numerical methods for computing the angles between linear subspaces,” Mathematics of Computation, vol.
27, pp.
579–594, 1973.
[30] K.
R.
Davidson and S.
J.
Szarek, “Local operator theory, random matrices and banach spaces,” Handbook in Banach Spaces, pp.
317–366, 2001.
33
The successful identification of drug-target interactions (DTI) is a critical step in drug discovery.
As the field of drug discovery expands with the discovery of new drugs, repurposing of existing drugs and identification of novel interacting partners for approved drugs is also gaining interest [40].
Until recently, DTI pre- diction was approached as a binary classification problem [4,7,8,14,23,41,50,55], 2 neglecting an important piece of information about protein-ligand interactions, namely the binding affinity values.
Binding affinity provides information on the strength of the interaction between a drug-target (DT) pair and it is usually ex- pressed in measures such as dissociation constant (Kd), inhibition constant (Ki), or the half maximal inhibitory concentration (IC50).
IC50 depends on the con- centration of the target and ligand [9] and low IC50 values signal strong binding.
Similarly, low Ki/Kd values indicate high binding affinity (i.e. good inhibitors have around Ki 1nM or lower).
Ki/Kd values are usually represented in terms of pKd or pKi, the negative logarithm of the binding or inhibition constants.
In binary classification based DTI prediction studies, construction of the data sets constitutes a major problem, since negative (not-binding) information is generally not provided.
In most cases, the DT pairs for which binding infor- mation is not known are treated as negative (not-binding) samples.
The lack of true-negative samples and how the study deals with the generation of synthetic negative samples usually affects the performance of the prediction algorithms.
On the other hand, formulating the DT prediction problem as binding affinity prediction, enables the creation of more realistic data sets, where the binding affinity scores are directly used, obviating the need for the generation of syn- thetic negative samples.
Prediction of protein-ligand interaction binding affinities has been the focus of protein-ligand scoring, which is frequently used after virtual screening and docking campaigns in order to predict the putative strengths of the proposed ligands to the target for identifying the active and inactive compounds [43].
Non-parametric machine learning methods such as the Random Forest (RF) algo- rithm have been used as a successful alternative to parametric scoring functions as of the last decade in order to prevent dependency on the parameters [3,36,45].
Later, Gabel et al.
showed that RF-score failed in virtual screening and docking tests, speculating that using features such as co-occurrence of atom-pairs over- simplified the description of the protein-ligand complex and led to the loss of information that the raw interaction complex could provide [20].
Around the same time this study was published, deep learning started to become a popu- lar architecture powered by the increase in data and high capacity computing machines challenging machine learning methods.
Inspired by the remarkable success rate in image processing [13, 17, 46], and speech recognition [15,25,30], deep learning methods are now being exhaustively used in many other research fields, including bioinformatics such as in genomics studies [35, 54] and quantitative-structure activity relationship (QSAR) studies in drug discovery [37].
The major advantage of deep learning architectures is that they enable better representations of the raw data by non-linear transformations in each layer [34] and thus, learning hidden patterns from the data.
A few studies employing Deep Neural Networks (DNN) have already been proposed for DTI binary class prediction using different input models for proteins and drugs [10, 26, 49] as well as the ones that employ stacked auto-encoders [52] and deep-belief networks [53].
Similarly, stacked auto-encoder based models with Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) were applied to represent chemical and genomic structures in real-valued vector 3 forms [22,31].
Deep learning approaches have also been applied to protein-ligand interaction scoring in which a common application has been the use of CNNs that learn from the 3D structures of the protein-ligand complexes [21, 43, 51].
However, this approach is limited to known protein-ligand complex structures, with only 25000 ligands reported in the PDB [44].
Recently, the SimBoost method was proposed to predict binding affinity scores with a gradient boosting machine by using feature engineering to repre- sent drug-target interactions [28].
They utilized similarity-based information of DT pairs as well as features that were extracted from network-based interactions between the pairs.
Pahikkala et al., on the other hand, employed Kronecker Reg- ularized Least Squares (KronRLS) algorithm that utilized only similarity-based representations of the drugs and targets using a 2D-based compound similar- ity method and the Smith-Waterman algorithm, respectively [42].
Both studies used traditional machine learning algorithms and utilized 2D-representations for compounds in order to provide similarity information.
In this study, we propose an approach to predict the binding affinities of protein-ligand interactions with deep learning models using only sequences (1D representations) of proteins and ligands.
To this end, the sequences of the pro- teins and SMILES (Simplified Molecular Input Line Entry System) representa- tions of the compounds are used rather than external features or 3D-structures of the binding complexes that might limit the data set.
We employ CNN blocks to learn better representations from the raw protein sequences and SMILES strings and combine these representations to feed into a fully-connected layer block that we termed as DeepDTA.
We used the Davis Kinase binding affinity data set [16] to evaluate the performance of our model and compared our results with the KronRLS algorithm [42].
Our results showed that the model that uses two separate CNN-based blocks to represent proteins and drugs performed as well as the KronRLS algorithm.
The model that uses a CNN-block to learn from SMILES and S-W similarity based protein representation, achieved the highest performance with a Concor- dance Index (CI) of 0.894, significantly outperforming the KronRLS algorithm (0.871) on the task of predicting binding affinities of DT pairs.
It also performed significantly better than KronRLS in the task of binding affinity prediction of novel drugs for known proteins.
Materials and Methods Data set We evaluated our proposed model on the Kinase data set by [16] as suggested by [42] to be used as benchmark data set for binding affinity prediction evaluation.
The Davis data set contains selectivity assays of the kinase protein family and the relevant inhibitors with their respective disassociation constant (Kd) values.
The data set comprises interactions of 442 proteins and 68 ligands, as reported in Table 1.
The final aim of the model is to predict binding affinity values.
While Table 1.
Data set Davis (Kd) Proteins Compounds 442 68 Interactions 30056 Pahikkala et al.
used the Kd values of the Davis data set directly, we used the values transformed into log space pKd similarly to [28] as explained in Equation 1.
Kd 1e9 pKd = −log10( (1) Figure 1 illustrates the distribution of the binding affinity values in pKd form.
We can clearly observe the peak at pKd value 5 (10000nM ) which constitutes more than half of the data set (20931 out of 30056).
These values correspond to the negative pairs that either have very weak binding affinities (Kd > 10000nM ) or are not observed in the primary screen [42].
Figure 1.
Distribution of the binding affinity (pKd) values of the Davis data set The compound SMILES strings were extracted from the Pubchem compound database based on their Pubchem CIDs [5].
Figure 2A illustrates the distribution of the lengths of the SMILES strings of the compounds in the Davis data set.
The maximum length of a SMILES is 103, while the average length is equal to 64.
The protein sequences of the Davis data set were extracted from the UniProt protein database based on gene names/RefSeq accession numbers [2].
Figure 2B shows the lengths of the sequences of the proteins in the Davis data set.
The maximum length of a protein sequence is 2549 and the average length is 788 characters.
Input Representation We experimented with two input representation approaches that have been com- monly used by deep-learning based studies: one-hot encoding and integer/label encoding.
One-hot encoding is a way of representing categorical variables in a binary vector form.
For a given set of categories, the entry in a binary vector is set to 1 for the corresponding label and it is set 0 otherwise.
We scanned through 5 Figure 2.
Summary of the Davis data set.
A) Distribution of the lengths of the SMILES strings B) Distribution of the lengths of the protein sequences approximately 2M SMILES sequences that we collected from Pubchem and com- piled 64 labels (unique letters).
For protein sequences, we scanned 550K protein sequences from UniProt and 25 categories (unique letters) were extracted.
The example below illustrates the one-hot representation of an example SMILES that belongs to methyl isocyanate, “CN=C=O”.
For each character in the SMILES, the corresponding position is set to 1.
   C H N 1 O .
.
.
n c = + .
.
.
0 0 .
.
.
0 0 .
.
.
0 0 .
.
.
0 0 .
.
.
0 0 .
.
.
0 0  Another popular form input representation is to use integers for the cate- gories (label/integer encoding).
Here we simply represent each label with a corresponding integer (e.g. “C”:1, “H”:2, ‘N”:3 etc.).
Label encoding for the example SMILES, “CN=C=O”, is given below.
(cid:2)C N = C = O(cid:3) = (cid:2)1 3 63 1 63 5(cid:3) 6 Similar to the SMILES, protein sequences are encoded in the same fashion using both one-hot and label encodings.
Both SMILES and protein sequences have varying lengths.
Hence, in order to create an effective representation form, we decided on fixed maximum lengths of 85 for SMILES and 1200 for protein sequences.
We chose these maximum lengths based on the distributions illus- trated in Figure 2 so that the maximum lengths cover most of the data set.
The sequences that are longer than the maximum length are truncated, whereas shorter sequences are 0-padded.
Proposed Model In this study we treated protein-ligand interaction prediction as a regression problem by aiming to predict the binding affinity scores.
As a prediction model, we adopted a popular deep learning architecture, Convolutional Neural Network (CNN).
CNN is an architecture that contains one or more convolutional layers often followed by a pooling layer.
A pooling layer down-samples the output of the previous layer and provides a way of generalization of the features that are learned by filters.
On the top of the convolutional and pooling layers, the model is completed with one or more fully connected layers (FC).
The most powerful feature of the CNN models is their ability to capture the local dependencies with the help of filters.
Therefore, the number and size of the filters in a CNN directly affects what kind of features the model learns from the input.
It is often reported that as the number of filters increases, the model becomes better at recognizing patterns.
We proposed a CNN-based prediction model that comprises two separate CNN blocks, each of which aims to learn representations from SMILES strings and protein sequences.
For each CNN block, we used three consecutive 1D- convolutional layers with increasing number of filters.
The second and the third convolutional layers had double and triple number of filters the first one had, respectively.
The convolutional layers were then followed by max-pooling layer.
The final features of the max-pooling layers were concatenated and fed into three fully-connected (FC) layers, that we named as DeepDTA.
We used 1024 nodes in the first two FC layers, each followed by a dropout layer of rate 0.1. Dropout is a regularization technique that is used to avoid over-fitting by setting the activation of some of the neurons to 0 [48].
The third layer consisted of 512 nodes and was followed by the output layer.
The proposed model that combines two CNN blocks is illustrated in Figure 3.
As activation function, we used Rectified Linear Unit (ReLU) [38], g(x) = max(0, x), since it has been widely used in deep learning studies [34].
A learning model tries to minimize the difference between the expected (real) value and the prediction during training.
Since we work on a regression task, we employed mean squared error (MSE) as loss function, in which P is the prediction vector, whereas Y corresponds to the vector of actual outputs.
n indicates the number of samples.
M SE = (Pi − Yi)2 i=1 (2) 7 Figure 3.
DeepDTA model with two CNN blocks to learn from compound SMILES and protein sequences.
The learning was completed with 100 epochs and mini-batch size of 256 was used to update the weights of the network.
Adam was used as the optimization algorithm to train the networks [33] with the default learning rate of 0.001.
We also compared two input representation techniques, one-hot and label encoding, therefore we experimented with two ways of feeding data into the prediction sys- tem.
With one-hot encoding, we directly fed the encoded data into the model, whereas in label-encoding we used Keras’ Embedding layer to represent charac- ters with 128-dimensional dense vectors.
The input consisted of (85,128) and (1200, 128) dimensional matrices for the compounds and proteins, respectively.
Results Baseline As baseline we chose the model presented by Pahikkala and coworkers where they employed Kronecker Regularized Least Squares (KronRLS) algorithm for bind- ing affinity prediction [42].
KronRLS aims to minimize the following function, where f is the prediction function [42]: J(f ) = i=1 (yi − f (xi))2 + λ||f ||2 (3) ||f ||2 k is the norm of f , that is related to the kernel function k, and λ > 0 is a regularization hyper-parameter defined by the user.
A minimizer for Equation 3 can be defined as follows [32]: f (x) = i=1 aik(x, xi) (4) where k is the kernel function.
In order to represent compounds, they utilized a similarity matrix which was computed using SIMCOMP, a tool that utilizes 2D properties of the compounds [27].
As for proteins, the Smith-Waterman algorithm was used to construct a protein similarity matrix [47].
Evaluation To evaluate the performance of a model that outputs continuous values, Concor- dance Index (CI) was used [24]: CI = Z X δi>δj h(bi − bj) (5) where bi is the prediction value for the larger affinity δi, bj is the prediction value for the smaller affinity δj, Z is a normalization constant, h(m) is the step function [42]: h(x) = if x > 0 if x = 0 if x < 0 (6) 1, 0.5, 0,   We used paired-t test for the statistical significance tests with 95% confidence interval.
Experiment Setup We evaluated the performance of the proposed model on the Davis data set [16] similarly to [42].
They used nested-cross validation to decide the best parameters for each test set.
In order to learn a generalized model, we randomly divided our data set into six equal parts in which one part is selected as the independent test set.
The remaining parts of the data set were used to determine the hyper- parameters via five-fold cross validation.
Figure 4 illustrates the partitioning of the data set.
The same setting was run for [42] for a fair comparison.
Figure 4.
Experiment setup.
We decided on three hyper-parameters for our model, the number of the filters (same for proteins and compounds), the length of the filter size for compounds, and the length of the filter size for proteins.
We chose to experiment with different filter lengths for compounds and proteins instead of a common one, 9 due to the fact that they have different alphabets in terms of characters.
The hyper-parameter combination that provided the best average CI score over the five-folds was chosen as the best combination in order to model the test set.
We first experimented with hyper-parameters chosen from a wide range and then fine-tuned the model.
For example, to determine the number of filters we performed a search over [16, 32, 64, 128, 512].
As explained in the Proposed Model subsection, the second convolution layer was set to contain twice the number of filters of the first layer, and the third one was set to contain three times the number of filters of the first layer.
32 filters obtained the best results over the cross-validation experiments.
Therefore, in the final model, each CNN block consisted of three 1D convolutions of 32, 64, 96 filters, respectively.
For all test results reported in Table 3 we used the same structure summarized in Table 2 except for the lengths of the filters that were used for the compound CNN-block and protein CNN-block.
Table 2.
Parameters setting for DTA model Parameters Number of filters Filter length (compounds) Filter length (proteins) epoch hidden neurons batch size dropout optimizer learning rate (lr) Range 32*1; 32*2; 32*3 [4,5,6,8] [4,6,8,12] 100 1024; 1024; 512 256 0.1 Adam 0.001 In order to provide a more robust performance measure, we evaluated the performance over the independent test set, when the model was trained with the learned parameters in Table 2 on the five training sets that we used in five-fold cross validation (note that the validation sets were not used).
The final CI score was reported as the average of these five results.
Keras [12] with Tensorflow [1] back-end was used as development framework.
Our experiments were run on OpenSuse 13.2 (3.50GHz Intel(R) Xeon(R) and GeForce GTX 1070 (8GB)).
The work was accelerated by running on GPU with cuDNN [11].
Performance In this study, we proposed a deep-learning based model that uses two CNN- blocks to learn representation for drugs and targets using their sequences.
As a baseline for comparison, the KronRLS algorithm that uses similarity matrices for proteins and compounds as input was chosen.
The Smith-Waterman (S-W) and SIMCOMP algorithms were used to compute the pairwise similarities for the proteins and ligands, respectively.
We also illustrated how well the CNN blocks are able to represent proteins and ligands.
We first, directly used the S-W and SIMCOMP similarity scores as inputs and fed the combination of these scores to the FC part of our model (DeepDTA), which consists of three 10 hidden layers and an output layer.
We then experimented with two alternative combinations: (i) learning only compound representation with a CNN block and using S-W similarity as protein representation and (ii) learning only protein sequence representation with a CNN block and using SIMCOMP to describe compounds.
We also reported the performance of the models that use CNN blocks both with one-hot and categorical representations.
Table 3 reports the average MSE and CI scores over the independent test set of the five models trained with the same parameters (shown in Table 2) using the five different training sets.
Table 3.
The average CI and MSE scores over the test set on five different training sets.
Proteins Smith-Waterman KronRLS [42] Smith-Waterman DeepDTA CNN DeepDTA (label) DeepDTA (one-hot) CNN DeepDTA (label, one hot) CNN CNN DeepDTA (label) CNN DeepDTA (one-hot) Smith-Waterman CNN DeepDTA (label) DeepDTA (one-hot) Smith-Waterman CNN Compounds CI (std) SIMCOMP 0.871 (0.0008) SIMCOMP 0.795 (0.003) 0.871 (0.006) CNN CNN 0.873 (0.003) CNN 0.878 (0.005) SIMCOMP 0.838 (0.004) SIMCOMP 0.826 (0.004) 0.888 (0.004) 0.894 (0.003) MSE 0.379 0.548 0.297 0.272 0.277 0.393 0.445 0.260 0.308 Using only the fully-connected part of the neural networks (DeepDTA) with S-W and SIMCOMP similarity scores to describe proteins and drugs was outper- formed by the baseline, KronRLS algorithm.
The combined CNN model that we proposed, on the other hand, performs as well as the baseline with both one-hot and label encoded inputs.
The model where only compound represen- tation was built by a CNN block, however, achieved the best CI score with a statistical significance over the baseline with both one-hot and label encoding (p-value=0.0004 and p-value=0.0014, respectively).
The MSE values of these models were also significantly less than the MSE of the baseline model.
In the model where only protein representations were built with a CNN block, with both one-hot encoding and label-encoding, the model performed poorly.
This might be due to two reasons: i) The CNN model could not effectively learn from amino-acid sequences, and ii) SIMCOMP can not represent compounds as successfully as the SMILES based CNN representation.
One-hot encoding is usually used when there is no ordered relationship be- tween the variables, since label encoding brings in ordinal relationships into the data even if they don’t exist.
For the model in which compounds were repre- sented via CNN-based learning and proteins were represented with S-W simi- larity scores, the difference between the performances of one-hot encoding and label encoding was considered as statistically significant with p-value of 0.012.
Despite performing better with one-hot encoding on CI score (ranking) based evaluation metric, we observed that the model produced the smallest MSE value with label-encoded SMILES inputs.
We also used the one-hot encoded CNN and S-W based DeepDTA model to evaluate the performance of a harder DTI prediction problem, which was to 11 predict new drugs for known proteins.
The model (optimized using Stochastic Gradient Descent [6], lr=0.01) produced an average CI score of 0.701, while the KronRLS algorithm with SIMCOMP and S-W had an average CI score of 0.65, thus outperforming the baseline with statistical significance (p-value=0.035).
For the CNN-based protein sequence and SIMCOMP-similarity based com- pound representation model, however, we see that the label-encoding model performed better than the one-hot encoding model with a statistical signifi- cance (p-value=0.016).
The results were indeed complementary to our existing knowledge of amino-acid substitution matrices [29] indicating the order of the amino-acids is important in protein sequences.
Therefore, we also tested a com- bined CNN based model in which amino-acid sequences were represented with label encoding and SMILES were represented as one-hot encoding.
The results indicated that CI score (0.878) improved upon both of the homogeneous models, though not significantly.
Table 4.
The average CI scores over the test set on five different training sets with one-hot encoding.
Proteins Compounds CI (std) DeepDTA (SMIlen=103) Smith-Waterman CNN Smith-Waterman CNN DeepDTA (SMIlen=64) DeepDTA (SMIlen=85) Smith-Waterman CNN 0.892 (0.004) 0.893 (0.003) 0.894 (0.003) MSE 0.330 0.274 0.308 As we obtained the best performance with one-hot encoded CNN and S-W combined DTA model, we decided to observe whether the maximum length of the SMILES string affected the performance of the prediction.
Table 4 reports the performances of the CNN and S-W combined models when the maximum length of the SMILES was chosen as the length of the longest SMILES (103), the average of SMILES in Davis (64) and our choice (85).
We observed that there is not a significant difference between the CI scores.
Discussion In this study, we proposed a deep-learning based approach to predict drug-target binding affinity using only sequences of proteins and drugs.
We used Convolu- tional Neural Networks to learn representations from the raw sequence data of proteins and drugs.
We compared the performance of the proposed model with a recent study that employed the KronRLS regression algorithm [42] as our base- line.
The model with two CNN-blocks performed as well as the baseline, whereas the model that uses CNN to learn compound representations from SMILES and S-W to compute protein similarity from amino-acid sequences achieved better performance than the KronRLS based algorithm with statistical significance.
After showing that SMILES based compound representation coupled with S-W protein similarity had the highest score in the prediction of drug - target interactions in which the drugs and the targets were present in the training data set, we tested the effectiveness of our methodology on a data set in which the proteins were previously encountered but the drugs were novel.
Our model 12 performed significantly better than the baseline KronRLS algorithm in this pre- diction task, which requires better representation of the drugs since it aims to predict affinities for novel compounds.
This successful performance of the model on the task of predicting affinities for novel drugs supports the effectiveness of the CNN architecture in describing compounds using SMILES strings.
We investigated the effect of the use of different input representation tech- niques for SMILES and amino-acid sequences on the performance of the proposed models.
For SMILES strings, one-hot encoding based SMILES model produced the highest CI score, whereas label encoding based SMILES model produced the lowest mean square error (MSE) value.
On the other hand, amino-acid sequences were better represented with label-encoding, which considers the or- dinal information of the integers, rather than one-hot encoding.
This might be an indication that amino-acids indeed require a structure that can handle their ordered relationships, which the CNN architecture failed to capture successfully.
Long-Short Term Memory (LSTM), which is a special type of Recurrent Neural Networks (RNN), could be a more suitable approach to learn from protein se- quences, since the architecture has memory blocks that allow effective learning from a long sequence.
The major contribution of this study is the presentation of a novel deep learning-based model for drug - target affinity prediction that uses only character representations of proteins and drugs.
SMILES representation for compounds was shown to be effective in predicting affinities for novel compounds.
As future work, we focus on building an effective representation for protein sequences.
A large percentage of proteins remains untargeted either due to bias in the drug discovery field for a select group of proteins or due to their undruggability and this untapped pool of proteins has gained interest with protein deorphanizing efforts [18, 19, 39].
The methodology can be extended to predict the affinity of known compounds to novel protein targets with no previously identified ligands as well as to the prediction of the affinity of novel drug-target pairs.
Acknowledgments TUBITAK-BIDEB 2211-E Scholarship Program (to HO) and BAGEP Award of the Science Academy (to AO) are gratefully acknowledged.
We thank Ethem Alpaydın, Attila G¨ursoy and Pınar Yolum for the helpful discussions.
Funding This work is funded by Bogazici University Research Fund (BAP) Grant Number 12304.
References 1.
M.
Abadi, A.
Agarwal, P.
Barham, E.
Brevdo, Z.
Chen, C.
Citro, G.
S.
Corrado, A.
Davis, J.
Dean, M.
Devin, et al.
Tensorflow: Large-scale 13 machine learning on heterogeneous distributed systems.
arXiv preprint arXiv:1603.04467, 2016.
2.
R.
Apweiler, A.
Bairoch, C.
H.
Wu, W.
C.
Barker, B.
Boeckmann, S.
Ferro, E.
Gasteiger, H.
Huang, R.
Lopez, M.
Magrane, et al.
Uniprot: the uni- versal protein knowledgebase.
Nucleic acids research, 32(suppl 1):D115– D119, 2004.
3.
P.
J.
Ballester and J.
B.
Mitchell.
A machine learning approach to predict- ing protein–ligand binding affinity with applications to molecular docking.
Bioinformatics, 26(9):1169–1175, 2010.
4.
K.
Bleakley and Y.
Yamanishi.
Supervised prediction of drug–target in- teractions using bipartite local models.
Bioinformatics, 25(18):2397–2403, 2009.
5.
E.
E.
Bolton, Y.
Wang, P.
A.
Thiessen, and S.
H.
Bryant.
Pubchem: integrated platform of small molecules and biological activities.
Annual reports in computational chemistry, 4:217–241, 2008.
6.
O.
Bousquet and L.
Bottou.
The tradeoffs of large scale learning.
In Advances in neural information processing systems, pages 161–168, 2008.
7.
D.-S.
Cao, S.
Liu, Q.-S.
Xu, H.-M.
Lu, J.-H.
Huang, Q.-N.
Hu, and Y.-Z.
Liang.
Large-scale prediction of drug–target interactions using protein sequences and drug topological structures.
Analytica chimica acta, 752:1– 10, 2012.
8.
D.-S.
Cao, L.-X.
Zhang, G.-S.
Tan, Z.
Xiang, W.-B.
Zeng, Q.-S.
Xu, and A.
F.
Chen.
Computational prediction of drug- target interactions us- ing chemical, biological, and network features.
Molecular Informatics, 33(10):669–681, 2014.
9.
R.
Z.
Cer, U.
Mudunuri, R.
Stephens, and F.
J.
Lebeda.
Ic 50-to-k i: a web-based tool for converting ic 50 to k i values for inhibitors of enzyme activity and ligand binding.
Nucleic acids research, 37(suppl 2):W441– W445, 2009.
10.
K.
C.
Chan, Z.-H.
You, et al.
Large-scale prediction of drug-target in- teractions from deep representations.
In Neural Networks (IJCNN), 2016 International Joint Conference on, pages 1236–1243.
IEEE, 2016.
11.
S.
Chetlur, C.
Woolley, P.
Vandermersch, J.
Cohen, J.
Tran, B.
Catanzaro, and E.
Shelhamer.
cudnn: Efficient primitives for deep learning.
arXiv preprint arXiv:1410.0759, 2014.
12.
F.
Chollet et al.
Keras, 2015.
13.
D.
Ciregan, U.
Meier, and J.
Schmidhuber.
Multi-column deep neural networks for image classification.
In Computer Vision and Pattern Recog- nition (CVPR), 2012 IEEE Conference on, pages 3642–3649.
IEEE, 2012.
14 14.
M.
C.
Cobanoglu, C.
Liu, F.
Hu, Z.
N.
Oltvai, and I.
Bahar.
Predicting drug–target interactions using probabilistic matrix factorization.
Journal of chemical information and modeling, 53(12):3399–3409, 2013.
15.
G.
E.
Dahl, D.
Yu, L.
Deng, and A.
Acero.
Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition.
IEEE Transactions on Audio, Speech, and Language Processing, 20(1):30–42, 2012.
16.
M.
I.
Davis, J.
P.
Hunt, S.
Herrgard, P.
Ciceri, L.
M.
Wodicka, G.
Pallares, M.
Hocker, D.
K.
Treiber, and P.
P.
Zarrinkar.
Comprehensive analysis of kinase inhibitor selectivity.
Nature biotechnology, 29(11):1046–1051, 2011.
17.
J.
Donahue, Y.
Jia, O.
Vinyals, J.
Hoffman, N.
Zhang, E.
Tzeng, and T.
Darrell.
Decaf: A deep convolutional activation feature for generic visual recognition.
In ICML, pages 647–655, 2014.
18.
A.
M.
Edwards, R.
Isserlin, G.
D.
Bader, S.
V.
Frye, T.
M.
Willson, and H.
Y.
Frank.
Too many roads not taken.
Nature, 470(7333):163, 2011.
19.
O.
Fedorov, S.
M¨uller, and S.
Knapp.
The (un) targeted cancer kinome.
Nature chemical biology, 6(3):166, 2010.
20.
J.
Gabel, J.
Desaphy, and D.
Rognan.
Beware of machine learning-based scoring functions on the danger of developing black boxes.
Journal of chemical information and modeling, 54(10):2807–2815, 2014.
21.
J.
Gomes, B.
Ramsundar, E.
N.
Feinberg, and V.
S.
Pande.
Atomic con- volutional networks for predicting protein-ligand binding affinity.
arXiv preprint arXiv:1703.10603, 2017.
22.
R.
G´omez-Bombarelli, D.
Duvenaud, J.
M.
Hern´andez-Lobato, J.
Aguilera-Iparraguirre, T.
D.
Hirzel, R.
P.
Adams, and A.
Aspuru-Guzik.
Automatic chemical design using a data-driven continuous representation of molecules.
arXiv preprint arXiv:1610.02415, 2016.
23.
M.
G¨onen.
Predicting drug–target interactions from chemical and genomic kernels using bayesian matrix factorization.
Bioinformatics, 28(18):2304– 2310, 2012.
24.
M.
G¨onen and G.
Heller.
Concordance probability and discriminatory power in proportional hazards regression.
Biometrika, 92(4):965–970, 2005.
25.
A.
Graves, A.-r.
Mohamed, and G.
Hinton.
Speech recognition with deep In 2013 IEEE international conference on recurrent neural networks.
acoustics, speech and signal processing, pages 6645–6649.
IEEE, 2013.
26.
M.
Hamanaka, K.
Taneishi, H.
Iwata, J.
Ye, J.
Pei, J.
Hou, and Y.
Okuno.
Cgbvs-dnn: Prediction of compound-protein interactions based on deep learning.
Molecular Informatics, 2016.
15 27.
M.
Hattori, N.
Tanaka, M.
Kanehisa, and S.
Goto.
Simcomp/subcomp: chemical structure search servers for network analyses.
Nucleic acids re- search, 38(suppl 2):W652–W656, 2010.
28.
T.
He, M.
Heidemeyer, F.
Ban, A.
Cherkasov, and M.
Ester.
Simboost: a read-across approach for predicting drug–target binding affinities using gradient boosting machines.
Journal of cheminformatics, 9(1):24, 2017.
29.
S.
Henikoff and J.
G.
Henikoff.
Amino acid substitution matrices from protein blocks.
Proceedings of the National Academy of Sciences, 89(22):10915–10919, 1992.
30.
G.
Hinton, L.
Deng, D.
Yu, G.
E.
Dahl, A.-r.
Mohamed, N.
Jaitly, A.
Se- nior, V.
Vanhoucke, P.
Nguyen, T.
N.
Sainath, et al.
Deep neural networks for acoustic modeling in speech recognition: The shared views of four re- search groups.
IEEE Signal Processing Magazine, 29(6):82–97, 2012.
31.
S.
Jastrzkeski, D.
Lesniak, and W.
M.
Czarnecki.
Learning to smile (s).
arXiv preprint arXiv:1602.06289, 2016.
32.
G.
Kimeldorf and G.
Wahba.
Some results on tchebycheffian spline func- tions.
Journal of mathematical analysis and applications, 33(1):82–95, 1971.
33.
D.
Kingma and J.
Ba. Adam: A method for stochastic optimization.
arXiv preprint arXiv:1412.6980, 2014.
34.
Y.
LeCun, Y.
Bengio, and G.
Hinton.
Deep learning.
Nature, 521(7553):436–444, 2015.
35.
M.
K.
Leung, H.
Y.
Xiong, L.
J.
Lee, and B.
J.
Frey.
Deep learning of the tissue-regulated splicing code.
Bioinformatics, 30(12):i121–i129, 2014.
36.
H.
Li, K.-S.
Leung, M.-H.
Wong, and P.
J.
Ballester.
Low-quality struc- tural and interaction data improves binding affinity prediction via random forest.
Molecules, 20(6):10947–10962, 2015.
37.
J.
Ma, R.
P.
Sheridan, A.
Liaw, G.
E.
Dahl, and V.
Svetnik.
Deep neural nets as a method for quantitative structure–activity relationships.
Journal of chemical information and modeling, 55(2):263–274, 2015.
38.
V.
Nair and G.
E.
Hinton.
Rectified linear units improve restricted boltz- mann machines.
In Proceedings of the 27th international conference on machine learning (ICML-10), pages 807–814, 2010.
39.
M.
J.
O’Meara, S.
Ballouz, B.
K.
Shoichet, and J.
Gillis.
Ligand similarity complements sequence, physical interaction, and co-expression for gene function prediction.
PloS one, 11(7):e0160098, 2016.
40.
T.
Oprea and J.
Mestres.
Drug repurposing: far beyond new targets for old drugs.
The AAPS journal, 14(4):759–763, 2012.
16 41.
H.
¨Ozt¨urk, E.
Ozkirimli, and A.
¨Ozg¨ur.
A comparative study of smiles- based compound similarity functions for drug-target interaction predic- tion.
BMC bioinformatics, 17(1):128, 2016.
42.
T.
Pahikkala, A.
Airola, S.
Pietil¨a, S.
Shakyawar, A.
Szwajda, J.
Tang, and T.
Aittokallio.
Toward more realistic drug–target interaction predic- tions.
Briefings in bioinformatics, page bbu010, 2014.
43.
M.
Ragoza, J.
Hochuli, E.
Idrobo, J.
Sunseri, and D.
R.
Koes.
Protein– ligand scoring with convolutional neural networks.
J.
Chem.
Inf.
Model, 57(4):942–957, 2017.
44.
P.
W.
Rose, A.
Prli´c, A.
Altunkaya, C.
Bi, A.
R.
Bradley, C.
H.
Christie, L.
D.
Costanzo, J.
M.
Duarte, S.
Dutta, Z.
Feng, et al.
The rcsb protein data bank: integrative view of protein, gene and 3d structural information.
Nucleic acids research, page gkw1000, 2016.
45.
P.
A.
Shar, W.
Tao, S.
Gao, C.
Huang, B.
Li, W.
Zhang, M.
Shahen, C.
Zheng, Y.
Bai, and Y.
Wang.
Pred-binding: large-scale protein–ligand binding affinity prediction.
Journal of enzyme inhibition and medicinal chemistry, 31(6):1443–1450, 2016.
46.
K.
Simonyan and A.
Zisserman.
Very deep convolutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556, 2014.
47.
T.
F.
Smith and M.
S.
Waterman.
Identification of common molecular subsequences.
Journal of molecular biology, 147(1):195–197, 1981.
48.
N.
Srivastava, G.
E.
Hinton, A.
Krizhevsky, I.
Sutskever, and R.
Salakhut- dinov.
Dropout: a simple way to prevent neural networks from overfitting.
Journal of Machine Learning Research, 15(1):1929–1958, 2014.
49.
K.
Tian, M.
Shao, S.
Zhou, and J.
Guan.
Boosting compound-protein in- teraction prediction by deep learning.
In Bioinformatics and Biomedicine (BIBM), 2015 IEEE International Conference on, pages 29–34.
IEEE, 2015.
50.
T.
van Laarhoven, S.
B.
Nabuurs, and E.
Marchiori.
Gaussian interac- tion profile kernels for predicting drug–target interaction.
Bioinformatics, 2011.
51.
I.
Wallach, M.
Dzamba, and A.
Heifets.
Atomnet: a deep convolutional neural network for bioactivity prediction in structure-based drug discovery.
arXiv preprint arXiv:1510.02855, 2015.
52.
L.
Wang, Z.-H.
You, X.
Chen, S.-X.
Xia, F.
Liu, X.
Yan, Y.
Zhou, and K.-J.
Song.
A computational-based method for predicting drug–target interactions by using stacked autoencoder deep neural network.
Journal of Computational Biology, 2017.
17 53.
M.
Wen, Z.
Zhang, S.
Niu, H.
Sha, R.
Yang, Y.
Yun, and H.
Lu. Deep- learning-based drug–target interaction prediction.
Journal of Proteome Research, 16(4):1401–1409, 2017.
54.
H.
Y.
Xiong, B.
Alipanahi, L.
J.
Lee, H.
Bretschneider, D.
Merico, R.
K.
Yuen, Y.
Hua, S.
Gueroussov, H.
S.
Najafabadi, T.
R.
Hughes, et al.
The human splicing code reveals new insights into the genetic determinants of disease.
Science, 347(6218):1254806, 2015.
55.
Y.
Yamanishi, M.
Araki, A.
Gutteridge, W.
Honda, and M.
Kanehisa.
Prediction of drug–target interaction networks from the integration of chemical and genomic spaces.
Bioinformatics, 24(13):i232–i240, 2008.

With decades of development, the artificial neural network  (ANN) now shows powerful ability of inference, approaching or  even surpassing human level in several specific scenarios such  as image recognition [1, 2] and chess games [3].
One reason for  such success is that the ANN could manage a great number of  parameters  with  deep  networks  to  learn  highly  complex  functions.
However,  the  interpretability  of  ANN  is  often  criticized since ANN inferences are difficult to be explained as  concise interaction among the parameters and the network.
Several researches have attempted to open the “black box”  of ANN and to interpret its inferences.
How to get insight into  the features generated automatically in the hidden layer of ANN  is a key issue for opening this “box”.
Schwartz-Ziv and Tishby  proposed  to  visualize  ANN  with  the  Information  Plane,  and  revealed  the  function  of  efficient  representation  of  the  hidden  layers [4].
Zeiler et al.
investigated the activations of neural units  in feature layer of ANN, and highlighted the regions of the input  data  which  are  responsible  for  the  activations  [5,  6].
This  approach is useful to dissect ANN models and to suggest ways  to improve them [5].
Szegedy et al.
analyzed ANN models by  synthesizing samples that lead to high activations of the neural  units [7, 8], and found that ANN models are easily hacked by  adding certain structured noise in image space,   revealing  that  supervised learning of ANN models might ignore some common  sense of the target, e.g. mammals have four legs generally.
Recently,  neural  units  in  ANN  feature  layer  have  been  interpreted with human-interpretable semantic concepts with the  help  of  a  densely  labeled  dataset  [9,  10],  or  modeled  as  an  explanatory  graph  which  reveals  the  knowledge  hierarchy  hidden inside a pre-trained model of ANN [11].
Although these  techniques  have  improved  the  interpretability  of  ANN,  they  cannot interpret an inference as concisely as human, e.g. telling  why a sample is a dog rather than a cat.
The main reason is that  ANN is hard to learn knowledge of common sense from a single  limited empirical dataset.
Compared  with  ANN,  human  make  inferences  as  well  as  interpret  inferences  concisely  based  on  two  foundations,  i.e.  experiences  and  knowledge.
With  knowledge that  learned and  summarized from the development of science and technologies,  human  can  make  better  inferences  with  a  smaller  empirical  dataset.
Therefore, it is vital to “teach” ANN models knowledge  to  improve  ANN  in  several  aspects:  reducing  the  model  complexity  and  cost  learn  a  new  problem,  adding  interpretability with knowledge and increasing accuracy.
to  Knowledge  representation  have  been  used  to  describe  the  richness  of  the  world  in  computer  systems,  which  can  be  understood by artificial intelligence and used in reasoning and  inferring subsequently [12, 13].
Rule-based representation is one  of the most common formalisms of knowledge representation.
For example, expert systems based on a set of rules have been  widely used in computer-aided medical diagnosis [14, 15], data  processing and analyzing [16, 17], fault diagnosis [18], etc.
The  subsequent  question  is  how  knowledge  representation  could be combined with ANN.
The knowledge-based ANN is an  answer  which  designed  the  topological  structure  of  ANN  according to the knowledge of input variables and represented  the  knowledge  as  dependency  structures  of  rules  [19-21].
However, the answer is not appropriate for the deep ANN whose  input  variables  are  often  high-dimensional  raw  data  while  the  designers of  ANN often  have little  idea  about  the dependency  structures of high-dimensional raw data.
This  paper  proposes  the  Rule-embedded  Neural  Network  (ReNN)  which  makes  use  of  knowledge  to  improve  the  performance  of  ANN.
Rules  are  used  to  represent  domain  knowledge and common sense in a computable manner.
ReNN  disassembles  the  “black  box”  of  ANN  into  two  parts:  local- Figure 1.
Computational graph of ReNN  based  inference  and  global-based  inference.
The  local-based  inference handles local patterns intuitively which can be easily  learned  from  empirical  datasets,  while  the  global-based  inference introduces rules about local patterns which have been  accumulated by human for long time.
Accordingly, knowledge  from  human  teachers  and  experiences  from  empirical  datasets  are combined, and then contribute to the global-based inference  together  improve  the  performance.
Besides,  we  can  differentiate the contributions of local patterns and rules to the  final  inferences,  and  thus  improve  the  interpretability  of  the  neural  networks.
As  an  example,  we  apply  ReNN  to  a  time- series detection problem in the experiments.
to  METHOD  Figure  2.
Block  design  for  the  feature-mapping  and  the  global-mapping  with  FCN  architecture  for  time-series  data.
It  consists  of 12 convolution  layers,  6  batch  normalization  layers,  4  max  pooling  (2×)  layers,  4  deconvolution layers (2×), and 4 concatenation layers, which are ordered  as the hollow arrows show.
The filter size of last convolution layer is 1×1,  while others are 1×3.
The solid arrows show that shallower features from  the output of batch normalization layers are fused to deeper layers with the  concatenation layers.
A.
Basic computation graph  The Rule-embedded Neural Network (ReNN) is defined as:  With  the  rule-modulated  map  and  the  input  data,  ReNN  generates estimation of targets as follows:  ,  (1)     (4)  where  f,  r  and  g  represent  the  feature-mapping  block,  rule- modulating block, and global-mapping block, respectively.
The  computational graph for ReNN is shown in Fig.
1.
Given a supervised-learning problem with input data X and  targets Y.
ReNN first extracts feature map from the input data  with the feature-mapping block:   ,  (2)  where the output feature map F is designed to reflect features of  the targets.
The features could be subcomponents of the targets.
For example, a task is to detect all face occurrences with a two- dimensional image, and the feature map could consist of neural  activations of face organs, i.e. eyes, ears, nose, and mouth and  so on.
The  features  could  also be instances of  the  targets.
For  example, a task is to detect all R-peaks from electrocardiograph  (ECG) of 60 seconds, and the feature map could consist of neural  activations of multiple R-peaks.
Then, ReNN applies rules to the feature map with the rule- modulating block, and generates a rule-modulated map.
(3)  The  rules  are  designed  according  to  domain  knowledge,  activating nodes in the rule-modulated map if and only if rule- based  evidences  could  be  found  with  the  activations  of  the  feature  map.
In  the  scenario  of  face  detection,  the  relative  positions of the face subcomponents can be utilized to analyze  candidate  positions  and  scales  of  face  occurrences.
In  the  scenario  of  R-peak  detection  from  ECG,  periodicity  and  variability of heart rate can be used to estimate possibility of R- peak-occurrences at candidate positions.
The  feature  map  can  F  also  be  used  as  an  input  of  g  to  accelerate the model training procedure.
Rule-modulating plays an important role for global mapping  in  ReNN.
However,  rules  may  make  the  ReNN  model  non- differentiable,  leading  to  the  failure  of  model  training  with  gradient-descend-based  optimization  methods.
A  two-stage  optimization strategy is adopted to overcome this limitation.
The  feature-mapping block is firstly optimized with a training dataset  of  , and then the global-mapping block is optimized with  dataset   while fixing the feature-mapping block.
Same as  traditional ANN, gradient-descend-based optimization methods  are used at each stage of the strategy [22].
B.
Neural Block design  To further design the ReNN blocks, we should consider the  characteristics of the specific task, such as the data dimension  and  target  scales.
The  task  of  R-peak  detection  from  one- dimensional  ECG  will  be  considered  as  an  example  in  the  following  sections.
ECG  consists  of  time-series  data  of  the  electrical  activity  of  the  heart,  which  are  recorded  by  noninvasive electrodes placed on the skin.
The R-peak of ECG  is a key time point in the procedure of rapid depolarization of the  right and left ventricles.
R-peaks are very useful when we need  to estimate the exact time of each heartbeat, e.g. calculating heart  rate  variability  [23]  and  pulse  wave  transit  time  [24,  25].
The  sampling rate of ECG is 125 Hz in this paper, and the scale of  the R-peak morphology is about 80 time-points (0.64 seconds).
We  use  the  fully-convolutional  network  (FCN)  [26]  to  design the feature-mapping block and the global-mapping block.
ReNN{,,}frgFfXRrFˆ,YgXR,XF,XYFeature mapping blockRule modulating blockGlobal mapping blockInput dataLocal-based inferenceGlobal-based inferenceRulesConvolutionBatch normalizationMax pooling (2x)Deconvolution (2x)ConcatenationLayers representationThe FCN architecture for time series data is shown in Fig.
2.
The  convolution  layer  is  used  to  capture  local  patterns,  while  the  max-pooling  layer  is  used  to  summarize  local  patterns  and  enlarge the receptive field of a neural note in deeper layers.
The  batch  normalization  layer  is  used  to  overcome  the  vanishing  gradient  problem  and  accelerate  the  training  of  deep  neural  networks.
The  deconvolution  layer  is  used  to  upsample  the  pooled data, so that pointwise classification can be achieved in  one  feedforward  computation  [26].
The  concatenation  layer  is  used to reserve fine features from shallower into deeper layers  where features are coarser due to the pooling operations.
As  we  know,  a  neural  node  after  two  layers  of  1×3  convolution  has a receptive filed of five, and the max pooling  and deconvolution layers can double the receptive field.
In sum,  each output node in the FCN architecture has a receptive field of  about 80 time-points (0.64 seconds).
This receptive field covers  all local information of the R-peak morphology, i.e. P-Q-R-S-T  patterns  of  the  electrical  activity  of  a  heartbeat.
The  receptive  field can be enlarged with more max-pooling layers.
centers are set to  , where k represents the k-th center  before or after the time point t.
li can be estimated according to  SDNN and the three-sigma rule of thumb as:     (5)  With a constraint that the supporting regions should not be  overlapped  with  each  other,  we  can  determine  how  many  supporting regions could be used for voting.
For simplicity, we  set the maximum of the number of supporting regions as six.
Rt is then calculated as:     (6)  where wi denotes the confidence about the vote from each region:  The number of output channels of the convolution layers and  deconvolution layers are important hyper-parameters to control  the complexity of the feature layers.
We set output channels of  all layers to be the same in the experiments for simplicity.
If no supporting regions can be found due to arrhythmia or     (7)  C.
Rule-modulating block  The  rule-modulating  block  is  designed  according  to  our  knowledge about R-peaks in ECG.
To distinguish R-peaks from  noises, human beings first model a piece of ECG signals with  the knowledge of heart beats and ask questions.
For example, is  the  owner  of  the  ECG  with  normal  sinus  rhythm  or  with  arrhythmias?
What is the heart rate?
If he or she is with normal  sinus rhythm, the heart rate would be used to pick R-peaks out  from the noises according to the time distance from neighboring  R-peaks.
The knowledge can be represented as rules.
The first step is to analyze some knowledge-based concepts  from the feature map.
The concepts here are heart rate (HR) and  its  standard  deviation  (SDNN).
With  the  help  of  local-based  inference, most of the R-peaks are prominent and easy to detect  in the feature map with the Softmax function.
We refer the set  of time intervals of the R-peaks as Z, where a constraint from  0.3 to 1.5 seconds is applied to the intervals according to domain  knowledge  about  heart  rhythm  of  common  users.
With  the  assumption  of  Gaussian  distribution  for  Z,  we  can  eliminate  abnormal intervals caused by noises.
After that, the average and  the standard deviation of Z are calculated as the HR and SDNN.
Second,  we  apply  voting  algorithm  to  generate  the  rule- ,  where  Rt  here  is  defined  as  the  modulated  map  possibility  of  R-peaks  at  time  point  t  according  to  votes  from  supporting regions.
The supporting region is defined according  to  the  knowledge  about  heart  rhythm.
As  we  know,  R-peak  occurrences, which are approximately-integral multiples of HR  away  from  the  current  time  point,  can  be  used  as  supporting  evidences  of  the  possibility  of  the  R-peak  occurrence  at  the  current time point.
Therefore, the supporting regions consist of  the  time  points  which  are  approximately-integral  multiples  of  HR away.
Here we denote the supporting regions as  where  ci  and li  are  the centers  and  widths of each region.
The  insufficient time intervals of R-peaks, we set Rt = 0.
EXPERIMENTAL SETTING   This  section  sets  up  experiments  to  evaluate  ReNN  in  the  task of R-peak detection from ECG.
ECG has been used as the  firsthand information to analyze the structure and function of the  heart  for  about  100  years  [27],  and  it  is  becoming  popular  in  smart wearable devices to monitor the status of users [28, 29].
However,  it  is  still  challenging  to  accurately  detect  R-peaks  since there are several types of noises affecting their occurrences,  such as motion artifact, muscular activation interference and AC  interference [29].
A.
Dataset  In  the  experiments,  we  use  a  ECG  dataset  constructed  by  LOHAS Tech., where the ECG data are measured at sampling  rate  of  125  Hz  with  a  single-lead  device  between  electrodes  placed on left and right arms.
The dataset includes 16600 ECG  measurements from 684 users as well as the labels of R-peaks in  each  ECG  measurement.
Most  of  the  users  are  healthy  with  normal  sinus  rhythm.
Each  ECG  measurement  is  about  12  seconds long, which is filtered with a high-pass filter at 1Hz and  a low-pass filter at 32Hz to remove irrelative signal components.
The  dataset  includes  about  260k  labels  of  R-peaks.
The  labels  are  first  labeled  by  the  PT  algorithm  [30],  and  then  suspicious labels are carefully checked and re-labeled by human  experts.
The standard deviation of R-peak intervals (referred as  SDNN)  is  used  to  estimate  the  heart  rate  variability  for  each  ECG  measurement.
16036  (97%)  ECG  measurements  in  the  dataset  are  with  low  SDNN  (<0.06s),  and  only  26  ECG  measurements are with high SDNN (>0.1s).
We  split  the  dataset  into  a  training  dataset  and  a  testing  dataset which include 80% and 20% of the ECG measurements,  respectively.
They are used for model training and evaluation.
tRR,iicltkHR6ilkSDNNmax,1iiiiitiiwcllRl2argmax,212iiiiclcliiwelTable 1.
Detection performances of the Local-based inferences and Global- based inferences on the test dataset.
#C represents the number of channels  in the FCN model.
Type  Local  Local  Local  Local  Local  Global  Global  Global  Global  Global  #C  TP  4  8  16  32  64  4  8  16  32  64  50212  50299  50308  50317  50323  50294  50346  50330  50319  50321  FP  249  116  83  65  33  73  31  33  37  25  FN  151  64  55  46  40  69  17  33  44  42  F1-score  0.9960  0.9982  0.9986  0.9989  0.9993  0.9986  0.9995  0.9993  0.9992  0.9993  dataset.
According  to  the  F1-score,  ReNN  achieves  low  error  rate when the number of channels is only four, while the local- based  inference  achieves  comparable  performance  with  64  channels.
However,  the  computational  cost  of  local-based  inference  with  64  channels  is  about  several  times  of  that  of  ReNN.
The results reveal that ReNN can make use of the rule- modulated map to enhance the accuracy to detect the R-peaks.
The  results  also  reveal  that  ReNN  has  the  potential  to  reduce  computational cost of deep neural networks.
Fig.
4  illustrates  an  example  of  ECG  measurements  and  ReNN detection involving the output of feature-mapping block,  rule-modulating  block,  and  global-mapping  block.
With  the  voltage  range  of  only  0.2  mV,  the  signal  of  this  ECG  measurement  is  weaker  than  standard  ECG  where  the  voltage  range is about 1 mV.
There are also some noises that might be  wrongly recognized as R-peaks.
Intuitively, it is hard for us to distinguish the R-peaks from  the  signal  with  only  local  patterns.
Nevertheless,  we  have  knowledge  about  the  heart  rhythm  of  the  user.
This  ECG  is  measured from a 25-year-old female who is healthy and with no  history  of  heart  diseases.
Accordingly,  there  should  be  no  premature beat or other abnormal beat in this ECG measurement,  and thus we can discriminate the R-peaks from the noisy signal  with heart rate and neighboring R-peaks.
The above analyzing process for human from local patterns  to global patterns (HR) has also been reflected in the detection  process  of  ReNN.
Firstly,  the  feature-mapping  block  detects  potential  R-peaks  according  to  local  patterns  in  the  receptive  fields.
Secondly,  the  rule-modulating  block  analyzes  heart  rhythm from the feature maps, and then estimates distributions  of  heartbeats  based  on  the  heart  rhythm  and  features  at  neighboring  heartbeats.
Finally,  the  global-mapping  block  synthesizes  the  original  ECG  and  the  rule-modulated  map  to  refine the detection.
In the example shown in Fig.
4, we can find  that a noisy peak (false positive by local-based model) has been  suppressed  successfully,  and  a  distorted  R-peak  has  been  enhanced in the global-based inference.
Figure 3.
Loss curves in the training process of the local based model and  the ReNN model.
Horizontal axis represents the number of training epochs,  and the vertical axis represents the value of the loss function.
B.
Model training   To  train  ReNN  model  with  FCN  block,  we  use  the  Adam  optimizer  with  the  weighted  cross-entropy  between  the  labels  and  the  model  output  as  loss  function.
We  feed  one  ECG  measurement and its labels of R-peaks and apply the optimizer  at each training step.
All the ECG measurements are fed one by  one for 100 epochs, and thus each measurement is fed for 100  times.
The learning  rate  is  set  as 0.0001 at  the beginning,  and  then decayed every 1000 steps exponentially at a rate of 0.99.
The total number of training step is about 1.3 million.
C.
Model Evaluation  training  process.
To  evaluate  We use the value of loss function to analyze the convergence  of  model  the  detection  performance,  we  use  the  following  metrics:  the  number  of  R- peaks correctly detected by the model (true-positives, TP), the  number of R-peaks missed (false negatives, FN), and the number  of noise samples wrongly detected as R-peaks (false-positives,  FP).
We also calculate the F1-score which is the harmonic mean  of precision and sensitivity of a detection model.
(8)  Given that the exact location of a  R-peak may be unstable  between two neighboring sampling-points due to noises, we set  a tolerance interval for the detection.
That is, a correctly detected  R-peak  is  judged  if  and  only  if  the  model  outputs  a  positive  during the tolerance interval of a labeled R-peak.
The tolerance  interval is set as the length of two sampling points, i.e. 16ms.
RESULTS  A.
Convergence  Fig.
3  shows  the  convergence  of  loss  function  at  the  early  training phase.
The loss curves of the local-based model and the  ReNN model decrease rapidly during the first 20 training epochs,  and then tend to be stable gradually.
The final values of the loss  function  are  8.904e-4  and  7.648e-4  for  the  local-based  model  and ReNN model, respectively.
The results reveal that both loss  functions have converged on the training dataset, and the ReNN  model can achieve better fitting performance.
B.
Detection performance  Table 1  lists  the detection performances  of  the local-based  inference  and  the  global-based  inference  (ReNN)  on  the  test  C.
Interpretability  To  interpret  an  inference,  we  should  ask  and  answer  the  following questions:  2F1-score2TPTPFPFNFigure 4.
ReNN detection on an ECG measurement.
The top line is the input time-series data of the ECG measurement, where the ECG signals are very weak.
The other three lines from top to bottom are the outputs of feature-mapping block (line-F), rule-modulating block (line-R) and global-mapping block (line-O),  respectively.
The four lines are aligned according to time axis.
The solid circles anchored on the lines are the time points  labeled as R-peaks.
The downward  triangle shows a false positive (FP) by local-based inference (high value on line-F), while global-based inference (lower value on line-O) reduces the probability  of R-peak at this time point due to little support from heart rhythm (low value on line-R).
Besides, the first R-peak in front of the triangle is distorted due to  noise.
It is detected with higher probability with the support from heart rhythm (high value on line-R).
➢  What are the local patterns, and what are rules among them?
➢  What are the local-based inferences with only local patterns?
➢  What are the global-based inferences with the help of rules?
➢  How can we explain the conflicts between local-based and  global-based inferences?
We can answer these questions well with the ECG example  in Fig.
4:  ✓  The  local  patterns  are  local  morphologies  of  the  R-peaks,  which are detected by the feature-mapping block.
The rules  among the R-peaks are designed according to HR and SDNN.
Furthermore,  we  can  disassemble  the  local  morphologies  into more details, i.e. the P-Q-R-S-T peaks, and train feature- mapping  block  for  the  details.
Then,  rules  about  the  order  and time constraints about these peaks can be applied.
✓  Line-F shows the local-based inferences that only use local  patterns.
Some suspicious R-peaks are detected according to  the local-based inference.
✓  Line-O  shows  the  global-based  inferences  which  combine  local patterns and rules.
Line-O has less jitters than Line-F,  indicating  that  the  combination  of  local  patterns  and  rules  have reduced the uncertainty of global-based inferences.
✓  We should check and analyze the conflicts between the local- based and global-based inferences carefully.
There might be  two  possibilities.
Firstly,  indicate  exceptions of the rules, such as a premature beat of the heart,  which have the value of diagnosis if it is a new case.
If some  indications for the conflicts have already been diagnosed, we  should add a label to the user and suppress similar conflicts,  or we should take some actions such as medication.
Secondly,  the  conflicts  may  noise  may  cause  the  conflicts,  which  indicate  that  current  ECG  measuring  and  analyzing  system  is  not  efficient  to  discover real abnormalities of such kind.
The usage should  be checked, or the system should be improved.
The  above  question-answering  process  is  similar  to  the  inference procedure of human, leading to better interpretability  for the inference of ReNN.
DISCUSSION  This paper has proposed ReNN to overcome the limitation  of current ANNs – lack of knowledge, and further validated this  approach in an ECG R-peak detection problem.
By introducing  knowledge  with  the  rule-embedding  approach,  ReNN  could  improve detection accuracy as well as reduce model complexity.
The  needs  of  big  dataset  for  model  training  can  be  mitigated  when  the  model  complexity  is  reduced.
Besides,  ReNN  has  improved  the  interpretability  of  the  neural-network-based  technologies.
The  global-based  inference  of  ReNN  can  be  interpreted  as  the  interaction  between  local  patterns  and  rules  among the local patterns.
To model long-term dependencies is a difficult problem in  machine learning area, since the uncertainty grows rapidly when  the input data become higher-dimensional.
The current popular  solution  is  to  train  very  deep  neural  networks  with  very  big  dataset, such as LSTM [31] and ResNet [1].
We argue that the  rule-embedding approach could provide a new solution to make  use of long-term dependencies efficiently, especially  when we  have already accumulated some knowledge about the long-term  dependencies.
It might become a new way for human-computer  interaction  (HCI)  where  local-pattern-rules  are  the  contents  of  interaction.
Computers can tell us what local patterns they have  learnt and observed, and we can teach them what rules are there  among the local patterns.
It seems just like that we are teaching  a kid or a student, and thus the computers seem more intelligent.
There  might  be  a  question  that  why  the  rules  should  be  embedded in ANN rather than be placed ahead or behind.
Rules  are  not  placed  ahead because most  of  our  knowledge  is  about  some semantic features extracted from the raw data.
The hidden  layers of ANN can play the role to extract the features.
Rules are  not placed behind because we want to model the dependencies  between local patterns and the rules automatically and to make  inferences synthetically.
Rules just play a role to extract global  features, i.e. the rule-modulated map, and thus we can add rules  freely  without  worry  about  conflicts  between  rules  or  combinatorial explosion of multiple rules.
Besides, Rules in ReNN are independent of the optimization  procedure,  so  human  experts  with  domain  knowledge  could  design rules freely without constraints of differentiability.
This  makes the combination of artificial neural network and existing  knowledge much easier.
Furthermore, rules could be organized   and accumulated with cross-validation or the latest probabilistic  graphical model based reasoning systems [32, 33], leading to a  rule  base  where  rules  are  marked  with  applicable  tasks  or  domains.
With the rule base, computers could gradually know  how  and  when  to  apply  what  rules.
This  would  be  helpful  in  transfer  learning  where  very  few  samples  can  be  used  or  unsupervised learning where no labels can be used.
The local- pattern-rule  HCI  and  the  rule  base  might  lead  a  new  thinking  about artificial intelligence.
ACKNOWLEDGMENT  The  author  thanks  Prof.
Jue  Wang  from  Institute  of  Automation,  Chinese  Academic  of  Sciences  for  introducing  the  philosophy  of  “structure + average” for artificial intelligent, which have inspired the  idea of this research.
The author thanks Dr. Jidong Liu from Shandong  Provincial Hospital for his help to build the ECG dataset, and thanks  Yao Wang and Yudong Zhu from LOHAS Tech for their supports, and  thanks Zongbo Zhang, Shuyu Han, Dongyang Mei, Ran Yan, Yuzhi Mu  and Shuai Ma for weekly discussion of machine learning problems.
REFERENCES  [1]  K.
He,  X.
Zhang,  S.
Ren,  and  J.
Sun,  "Deep  residual  learning  for  image  recognition,"  in  Proceedings of the  IEEE  conference on computer vision  and pattern recognition, 2016, pp.
770-778.
[2]  A.
Krizhevsky,  I.
Sutskever,  and  G.
E.
Hinton,  "Imagenet  classification  with  deep  convolutional  neural  networks,"  in  Advances  in  neural  information processing systems, 2012, pp.
1097-1105.
[3]  D.
Silver, J.
Schrittwieser, K.
Simonyan, I.
Antonoglou, A.
Huang, A.
Guez,  et al., "Mastering the game of go without human knowledge," Nature, vol.
550, p.
354, 2017.
[4]  R.
Shwartz-Ziv  and  N.
Tishby,  "Opening  the  Black Box  of  Deep  Neural  Networks via Information," arXiv preprint arXiv:1703.00810, 2017.
[5]  M.
D.
Zeiler and R.
Fergus, "Visualizing and Understanding Convolutional  Networks,"  in  ECCV,  13th  European  Conference,  Proceedings,  Part  I,  Zurich, Switzerland, 2014, pp.
818-833.
[6]  J.
Yosinski,  J.
Clune,  T.
Fuchs,  and  H.
Lipson,  "Understanding  neural  networks  through  deep  visualization,"  in  In  ICML  Workshop  on  Deep  Learning, 2015.
[7]  C.
Szegedy, W.
Zaremba, I.
Sutskever, J.
Bruna, D.
Erhan, I.
Goodfellow,  et  al.,  "Intriguing  properties  of  neural  networks,"  arXiv  preprint  arXiv:1312.6199, 2013.
[8]  A.
Nguyen,  J.
Yosinski,  and  J.
Clune,  "Deep  neural  networks  are  easily  fooled:  High  confidence  predictions  for  unrecognizable  images,"  in  Proceedings  of  the  IEEE  Conference  on  Computer  Vision  and  Pattern  Recognition, 2015, pp.
427-436.
[9]  B.
Zhou,  A.
Khosla,  A.
Lapedriza,  A.
Oliva,  and  A.
Torralba,  "Object  detectors  emerge  in  deep  scene  cnns,"  in  ICLR  (arXiv  preprint  arXiv:1412.6856), 2015.
[10] D.
Bau,  B.
Zhou,  A.
Khosla,  A.
Oliva,  and  A.
Torralba,  "Network  Dissection: Quantifying Interpretability of Deep Visual Representations,"  in arXiv preprint arXiv:1704.05796, 2017.
[11] Q.
Zhang,  R.
Cao,  F.
Shi, Y.
N.
Wu,  and  S.-C.
Zhu,  "Interpreting  CNN  knowledge via an Explanatory Graph," in AAAI 2018, 2017.
[12] R.
Davis,  H.
Shrobe,  and  P.
Szolovits,  "What  is  a  knowledge  representation?," AI magazine, vol.
14, p.
17, 1993.
[13] J.
F.
Sowa,  Principles  of  semantic  networks:  Explorations  in  the  representation of knowledge: Morgan Kaufmann, 2014.
[14] K.
Polat  and  S.
Güneş,  "An  expert  system  approach  based  on  principal  component  analysis  and  adaptive  neuro-fuzzy  inference  system  to  diagnosis of diabetes disease," Digital Signal Processing, vol.
17, pp.
702- 710, 2007.
[15] S.
S.
A.
Naser  and A.
O.
Mahdi,  "A  proposed  Expert  System  for  Foot  Diseases  Diagnosis,"  American  Journal  of  Innovative  Research  and  Applied Sciences, vol.
2, pp.
155-168, 2016.
[16] E.
Avci,  "An  expert  system  based  on Wavelet  Neural  Network-Adaptive  Norm Entropy for scale invariant texture classification," Expert Syst.
Appl.,  vol.
32, pp.
919-926, 2007.
[17] G.
Winter,  "xia2:  an  expert  system  for  macromolecular  crystallography  data reduction," Journal of Applied Crystallography, vol.
43, pp.
186-190,  2010.
[18] M.
B.
Jain, M.
B.
Srinivas, and A.
Jain, "A novel Web based Expert System  Architecture for on-line and off-line fault diagnosis and control (FDC) of  transformers," in TENCON 2008 - 2008 IEEE Region 10 Conference, 2008,  pp.
1-5.
[19] S.
I.
Gallant, "Connectionist expert systems," Commun.
ACM, vol.
31, pp.
152-169, 1988.
[20] J.
W.
Shavlik,  "Combining  Symbolic  and  Neural  Learning,"  Machine  Learning, vol.
14, pp.
321-331, March 01 1994.
[21] G.
G.
Towell  and  J.
W.
Shavlik,  "Knowledge-based  artificial  neural  networks," Artificial Intelligence, vol.
70, pp.
119-165, 1994/10/01/ 1994.
[22] Y.
LeCun, Y.
Bengio, and G.
Hinton, "Deep learning," Nature, vol.
521, pp.
436-444, 2015.
[23] J.
F.
Thayer,  S.
S.
Yamamoto,  and  J.
F.
Brosschot,  "The  relationship  of  autonomic imbalance, heart rate variability and cardiovascular disease risk  factors," International Journal of Cardiology, vol.
141, p.
122, 2010.
[24] C.
Ahlstrom, A.
Johansson, F.
Uhlin, T.
Länne, and P.
Ask, "Noninvasive  investigation of blood pressure changes using the pulse wave transit time:  a novel approach in the monitoring of hemodialysis patients,"  Journal of  Artificial Organs the Official Journal of the Japanese Society for Artificial  Organs, vol.
8, p.
192, 2005.
[25] C.
Lin, Y.
Zhou, H.
Wang, and Y.
Wang, "Pulse waveform as an indicator  of baseline offset in pulse transit time based blood pressure estimation," in  2017  IEEE  Healthcare  Innovations and  Point of Care  Technologies  (HI- POCT), 2017, pp.
26-31.
[26] J.
Long, E.
Shelhamer, and T.
Darrell, "Fully convolutional networks for  semantic  segmentation,"  in  Proceedings  of  the  IEEE  Conference  on  Computer Vision and Pattern Recognition, 2015, pp.
3431-3440.
[27] N.
J.
Mehta and  I.
A.
Khan,  "Cardiology's 10  greatest discoveries  of  the  20th century," Texas Heart Institute Journal, vol.
29, p.
164, 2002.
[28] B.
Yu,  L.
Xu,  and  Y.
Li,  "Bluetooth  Low  Energy  (BLE)  based  mobile  electrocardiogram  monitoring  system,"  in  Information  and  Automation  (ICIA), 2012 International Conference on, 2012, pp.
763-767.
[29] P.
Rajpurkar,  A.
Y.
Hannun,  M.
Haghpanahi,  C.
Bourn,  and  A.
Y.
Ng,  "Cardiologist-Level  Arrhythmia  Detection  with  Convolutional  Neural  Networks," arXiv preprint arXiv:1707.01836, 2017.
[30] J.
Pan and W.
J.
Tompkins, "A Real-Time QRS Detection Algorithm," IEEE  Transactions on Biomedical Engineering, vol.
BME-32, pp.
230-236, 1985.
[31] S.
Hochreiter  and  J.
Schmidhuber,  "Long  Short-Term  Memory,"  Neural  Computation, vol.
9, pp.
1735-1780, 1997.
[32] L.
Bottou,  "From  machine  learning  to  machine  reasoning,"  Machine  learning, vol.
94, pp.
133-149, 2014.
[33] D.
Koller and N.
Friedman, Probabilistic graphical models: principles and  techniques: MIT press, 2009.

Various models of signals and images have been studied in recent years including dictionary models, tensor and manifold models.
Dictionaries that sparsely represent signals are used in applications such as compression, denoising, and medical image reconstruction.
Dictionaries learned from training data sets may outperform analytical models since they are adapted to signals (or signal classes).
The goal of dictionary learning is to ﬁnd a matrix D such that an input matrix P, representing the data set, can be written as P ≈ DZ, with Z denoting the (unknown) sparse representation matrix.
The learning of synthesis dictionary and sparsifying trans- form models has been studied in several works [1]–[11].
The convergence of speciﬁc learning algorithms has been studied in recent works [6], [7], [12]–[16].
The learning problems are typically non-convex and some of these works prove conver- gence to critical points in the problems [6], [7], [17].
Others (e.g., [15], [18]) prove recovery of generative models for speciﬁc (often computationally expensive) algorithms, but rely on many restrictive assumptions.
A very recent work considers a structured dictionary learning objective showing with high probability that there are no spurious local minimizers, and also provides a speciﬁc convergent algorithm [19], [20].
In this work, we analyze the convergence properties of a structured (unitary) dictionary or transform learning algorithm that involves computationally cheap updates and works well in applications such as image denoising and magnetic resonance image reconstruction [10], [16], [21].
Our goal is to simulta- neously ﬁnd an n × n sparsifying transformation matrix W and an n× N sparse coefﬁcients (representation) matrix Z for training data represented as columns of a given n× N matrix P, by solving the following constrained optimization problem: F s.t. WT W = Id,(cid:13)(cid:13)Z(.,j) (cid:13)(cid:13)0 ≤ s∀j.
(cid:107)WP − Z(cid:107)2 arg min W,Z (1) The columns Z(.,j) of Z have at most s non-zeros (corre- sponding to the (cid:96)0 “norm”), where s is a given parameter.
Alternatives to Problem (1) would replace the column-wise sparsity constraint with a constraint on the sparsity of the entire matrix Z (i.e., aggregate sparsity), or use a sparsity penalty (e.g., (cid:96)p penalties with 0 ≤ p ≤ 1).
Problem (1) also corresponds to learning a (synthesis) dictionary WT for representing the data P as WT Z.
Optimizing Problem (1) by alternating between updating W (operator update step) and Z (sparse coding step) would generate the following updates.
The tth Z update is given as (.,j) = Hs(Wt−1P(.,j)) ∀ j, where the thresholding operator Zt Hs(·) zeros out all but the s largest magnitude elements of a vector (leaving the s entries unchanged).
The subsequent W update is based on the full singular value decomposition (SVD) of ZtPT (cid:44) VΣUT with Wt = VUT .
The method is shown in Algorithm 1.
Recent works have shown convergence of Algorithm 1 or its variants to critical points in the equivalent unconstrained problems [16], [22], [23].
Here, we further prove local linear convergence of the method to the underlying generative (data) model under mild assumptions that depend on properties of the underlying/generating sparse coefﬁcients.
Our experiments show that the method is also robust or insensitive to initial- ization in practice.
II.
CONVERGENCE ANALYSIS The main contribution of this work is the local convergence analysis of Algorithm 1.
In particular, we show that under mild assumptions, the iterates in Algorithm 1 converge linearly to the underlying (generating) data model.
Algorithm 1: Alternating Optimization for (1) Input: Training data matrix P, maximum iteration count L, sparsity s Output: WL, ZL Initialize: W0 and t = 1 for t ≤ L do Zt (.,j) = Hs PZtT =UtΣtVtT Wt = VtUtT t = t + 1 (cid:0)Wt−1P(.,j) (cid:1) ∀ j end A.
Notation In the remainder of this work, we adopt the following nota- tion.
The unitary transformation matrix, sparse representation matrix, and training data matrix are denoted as W ∈ Rn×n, Z ∈ Rn×N , and P ∈ Rn×N , respectively.
For a matrix X, we denote its jth column, ith row, and entry (i, j) by X(·,j), X(i,·), and X(i,j) respectively.
The tth iterate or approximation in the algorithm is denoted using (·)t, with a lower-case t, and (·)T denotes the transpose.
The function S(y) returns the support (i.e., set of indexes) of a vector y ∈ Rn, or the locations of the nonzero entries in y.
Matrix Dk denotes an n × n diagonal matrix of ones and a zero at location (k, k).
Additionally, ˜Dk denotes an N × N diagonal matrix that has ones at entries (i, i) for i ∈ S(Z∗ (k,·)) and zeros elsewhere, and matrix Z∗ is deﬁned in Section II-B (see assumption (A1)).
The Frobenious norm, denoted (cid:107)X(cid:107)2 F , is the the sum of squared elements of X, and (cid:107)X(cid:107)2 denotes the spectral norm.
Lastly, Id denotes the appropriately sized identity matrix.
B.
Assumptions Before presenting our main results, we will brieﬂy discuss our assumptions and explain their implications: (A1) Generative model: There exists a Z∗ and unitary W∗ such that W∗P = Z∗, and (cid:107)P(cid:107)2 = 1 (normalized).
(A2) Sparsity: The columns of Z∗ are s−sparse, i.e., (cid:107)Z∗ (A3) Spectral property: The underlying Z∗ satisﬁes the bound κ4(cid:0)Z∗(cid:1) max1≤k≤n (cid:107)DkZ∗Z∗T Z∗ ˜Dk(cid:107)2 < 1, where κ(·) (·,j)(cid:107)0 ≤ s ∀j.
denotes the condition number (ratio of largest to smallest singular value).
(A4) Initialization: (cid:107)W0 − W∗(cid:107)F ≤  for an appropriate sufﬁciently small  > 0.
1 The generative model assumption simply states that there exists an underlying transform and representation matrix for the data set P.
So we would like to investigate if Algo- rithm 1 can ﬁnd such underlying models (minimizers in (1)).
(cid:18) Z∗ (cid:19) 1Although we do not specify the best (largest permissible)  explicitly,  < with β(·) denoting the smallest nonzero magnitude 2 minj β in a vector, will arise in one of our proof steps.
The actual permissible  is also dictated as per (convergence of) Taylor series expansions discussed in the proof.
(·,j) (·,j) (cid:107)Z∗ (cid:107)2 Assumption (A2) states that the columns of Z∗ have at most s nonzeros.
We assume that the coefﬁcients are “structured” in assumption (A3), satisfying a spectral property, which will be used to establish our theorems.
Later we discuss a conjecture that states that this property holds for a speciﬁc probabilistic model.
Assumption (A4) states that the initial sparsifying transform is sufﬁciently close to the solution W∗.
This assumption also simpliﬁes our proof and has been made in other works such as [15], [18], which address the issue of speciﬁc (good) initialization separately from the main algorithm.
In this work, we empirically show the effect of general initializations in Section III.
C.
Main Results and Proofs We state the convergence results in Theorems II.1 and II.2. Theorem II.1 establishes that Algorithm 1 converges to the underlying generative model under the assumptions discussed in Section II-B.
It also makes an additional assumption on Z∗ that Z∗Z∗T = Id, which simpliﬁes assumption (A3).
Theo- rem II.2 presents the more general result based on Assumption (A3).
We only include the (simpler) proof of Theorem II.1, while the full details of the general Theorem II.2’s proof can be found in [24].
Following Theorem II.2, Conjecture 1 states that Assumption (A3) holds under a commonly used probabilistic assumption on the sparse representation matrix Z∗.
Theorem II.1. Under Assumptions (A1)−(A4) and assuming Z∗Z∗T = Id, the Frobenius error between the iterates generated by Algorithm 1 and the underlying generative model in Assumption (A1) is bounded as follows: (cid:107)Zt − Z∗(cid:107)F ≤ qt−1, (cid:107)Wt − W∗(cid:107)F ≤ qt, (2) where q (cid:44) max1≤k≤n (cid:107)DkZ∗ ˜Dk(cid:107)2 and  is ﬁxed based on the initialization.
Since Z∗Z∗T = Id, by Assumption (A3) it follows that q < 1 above, and thus the Theorem establishes that the iterates converge at a linear rate to the underlying generative model.
In the following, we prove Theorem II.1 using induction on the approximation error of iterates with respect to Z∗ and W∗.
Let the series {Et} and {∆t} be deﬁned as Et := Wt − W∗, ∆t := Zt − Z∗.
(3) (4) By Assumption (A4), (cid:107)E0(cid:107)F ≤ .
We ﬁrst provide a proof for the base case of t = 1.
This proof involves two main parts.
First, we show that the error between Z1 and Z∗ is bounded (in norm) by .
Then, we show that (cid:107)W1 − W∗(cid:107)F ≤ q, where q is iteration-independent.
For the ﬁrst part, we use Assumptions (A1), (A2), and (A4).
Assumption (A4) ensures that a superset of the support of Z∗ Z1 is recovered in the ﬁrst iteration.
In particular, each column of the sparse coefﬁcients matrix Z1 in Algorithm 1 satisﬁes = Hs(W∗P(·,j) + E0P(·,j)) (·,j) + E0P(·,j)) j E0P(·,j), (·,j) = Hs(W0P(·,j)) = Hs(Z∗ = Z∗ (·,j) + Γ1 (Eq.3) (5) (A1) (A4) j is a diagonal matrix with a one in the (i, i)th entry where Γ1 if i ∈ S(Z1 (·,j)) and zero otherwise and E0 is as deﬁned in (3).
The last equality in (5) follows from the fact that the support (·,j) for small .
In particular, since of Z1 (·,j) includes that of Z∗ (cid:13)(cid:13)P(·,j) (cid:13)(cid:13)(cid:13)2 (cid:13)(cid:13)(cid:13)Z∗ (cid:13)(cid:13)2 = (cid:13)(cid:13)∞ ≤(cid:13)(cid:13)E0P(·,j) (cid:13)(cid:13)E0P(·,j) , we have (·,j) (cid:13)(cid:13)2 ≤(cid:13)(cid:13)E0(cid:13)(cid:13)F (cid:13)(cid:13)(cid:13)2 (cid:13)(cid:13)(cid:13)Z∗ (cid:16) Z∗ (·,j) (cid:17) Therefore, whenever (cid:107)E0(cid:107)F ≤  < 1 with β(·) denoting the smallest nonzero magnitude in a vector, the support of Z1 (·,j) (i.e., the entries of the perturbation term E0P(·,j) are not large enough to change the support).
The following results then hold: (·,j) includes that of Z∗ 2 minj β (·,j) (·,j)(cid:107)2 (cid:107)Z∗ (cid:107)Z1 − Z∗(cid:107)2 (Eq.5) = (cid:107)[Γ1 (i)≤ (cid:107)E0P(cid:107)2 (ii)≤ (cid:107)E0(cid:107)2 N E0P(·,N )](cid:107)2 = (cid:107)E0(cid:107)2 F .
1E0P(·,1), ..., Γ1 F(cid:107)P(cid:107)2 j; step (ii) holds for the 2 = 1, (A1) Step (i) follows by deﬁnition of Γ1 Frobenius norm of a product of matrices; and since (cid:107)P(cid:107)2 the last equality holds.
Therefore, we can conclude that (cid:107)Z1 − Z∗(cid:107)F ≤ (cid:107)E0(cid:107)F (A4)≤ .
(6) Next, we analyze the quality of the updated transform W1.
To bound (cid:107)W1 − W∗(cid:107)F by q, we rely on Taylor Series expansions of the matrix inverse and positive-deﬁnite square root functions.
Denote the full SVD of Z∗Z1T as U1 T .
zΣ1 Then, from Assumption (A1) and Algorithm 1, we have, zV1 W∗T Z∗Z1T (A1) , W1 = V1U1T Then, W1 is expressed in terms of the SVD of Z∗Z1T as = U1Σ1V1T = PZ1T W1 = V1 zU1 W∗.
Using (7), the error between W1 and W∗ satisﬁes (cid:107)W1 − W∗(cid:107)F = (cid:107)V1 = (cid:107)(V1 zU1 zU1 W∗ − W∗(cid:107)F zU1 T − Id(cid:107)F , (7) (8) where the matrix V1 T can be further rewritten as follows: T − Id)W∗(cid:107)F = (cid:107)V1 zU1 (cid:0)Σ1 (cid:123)(cid:122) (a) (cid:1)−1 (cid:125) )−1 = V1 = (Z∗Z1T (cid:124) (cid:124) (cid:123)(cid:122) (b) U1 U1 (Z∗Z1T zΣ1 zU1 Z1Z∗T ) (cid:125) (9) V1 zU1 It is easy to show that since Z∗Z∗T = Id, Z∗Z1T is invertible for all  < 1 (sufﬁcient condition).
Using (4) and the assumption Z∗Z∗T = Id, the Taylor Series expansions for the matrix inverse and positive-deﬁnite square root in (9), can be written as (a) = (Z∗Z1T )−1 = (Id + Z∗∆1T )−1 = Id − Z∗∆1T + O((∆1)2) (b) = (Z∗Z1T = Id + Z1Z∗T ) (Z∗∆1T + ∆1Z∗T ) + O((∆1)2).
Therefore we can rewrite (9) as V1 zU1 = (a)(b) = Id + (∆1Z∗T − Z∗∆1T ) + O((∆1)2), terms, and is bounded in norm by C(cid:13)(cid:13)∆1(cid:13)(cid:13)2 for some constant where O((∆1)2) denotes corresponding higher order series C (independent of the iterates).
the ﬁrst transform iterate W1 and W∗ is bounded as Substituting the above expressions in (8), the error between (cid:107)W1 − W∗(cid:107)F T − Id(cid:107)F zU1 (Eq.8) = (cid:107)V1 ≈ 1 (cid:107)∆1Z∗T − Z∗∆1T(cid:107)F .
(10) The approximation error in (10) is bounded in norm by C2, which is negligible for small .
So we only bound the (dominant) term 0.5(cid:107)∆1Z∗T − Z∗∆1T(cid:107)F .
Since the matrix ∆1Z∗T − Z∗∆1T clearly has a zero diagonal, we have the following inequalities: (cid:107)W1 − W∗(cid:107)F ≈ 1 (cid:107)∆1Z∗T − Z∗∆1T(cid:107)F (cid:118)(cid:117)(cid:117)(cid:116) n(cid:88) (cid:118)(cid:117)(cid:117)(cid:116) n(cid:88) (cid:107)DkZ∗ ˜Dk∆1 T(cid:107)2 2 ≤ (cid:107)DkZ∗ ˜Dk(cid:107)2 2(cid:107)∆1 (k,·)(cid:107)2 k=1 ≤ max (cid:107)DkZ∗ ˜Dk(cid:107)2 k=1 (cid:107)∆1 (k,·)(cid:107)2 2 = q(cid:107)Z1 − Z∗(cid:107)F (6)≤ q, (k,·) (cid:118)(cid:117)(cid:117)(cid:116) n(cid:88) k=1 where q (cid:44) maxk (cid:107)DkZ∗ ˜Dk(cid:107)2.
Thus, we have shown the results for the t = 1 case.
We complete the proof of Theorem II.1 by observing that for each subsequent iteration t = τ + 1, the same steps as above can be repeated along with the induction hypothesis (IH) to show that (cid:107)Zτ +1 − Z∗(cid:107)F = (cid:107)∆τ +1(cid:107)F ≤ (cid:107)Eτ(cid:107)F = (cid:107)Wτ − W∗(cid:107)F (IH)≤ qτ  (cid:107)Wτ +1 − W∗(cid:107)F ≤ q(cid:107)Zτ +1 − Z∗(cid:107)F ≤ q(qτ ).
The next result generalizes Theorem II.1 by removing the assumption Z∗Z∗T = Id. Theorem II.2. Under Assumptions (A1) − (A4), the iterates in Algorithm 1 converge linearly to the underlying generative (cid:4) Fig.
1: The performance of Algorithm 1 for recovering W∗ for s = 5 and s = 10.
model in Assumption (A1), i.e., the Frobenius error between the iterates and the generative model satisﬁes (cid:107)Zt − Z∗(cid:107)F ≤ qt−1, (cid:107)Wt − W∗(cid:107)F ≤ qt, where q (cid:44) κ4(cid:0)Z∗(cid:1) max1≤k≤n (cid:107)DkZ∗Z∗T Z∗ ˜Dk(cid:107)2 < 1 and (11)  is ﬁxed based on the initialization.
Note that dropping the unit spectral norm (normalization) condition on P in Assumption (A1) does not affect the (cid:107)Wt− W∗(cid:107)F bound in Theorem II.2 and only creates a scaling in the (cid:107)Zt − Z∗(cid:107)F bound, where the  gets replaced by (cid:107)P(cid:107)2.
Conjecture 1.
Suppose the locations of the s nonzeros in each column of Z∗ is chosen uniformly at random, and the non- zero entries are i.i.d. as Z∗ sN ).
Then, for ﬁxed, small for large enough N.
n , q (cid:44) κ4(cid:0)Z∗(cid:1) max1≤k≤n (cid:107)DkZ∗Z∗T Z∗ ˜Dk(cid:107)2 < 1 (i,j) ∼ N (0, n s√ Conjecture 1 thus states that under the assumed probabilistic model for Z∗, when N is large enough or there is sufﬁcient training data (or columns of P), then Algorithm 1 is assured to have rapid local linear iterate convergence to the underlying generative model.
This conjecture can be empirically veriﬁed through simulations and the numerical results supporting it can be found in [24].
The experiments presented in this paper will focus on illustrating the local convergence of Algorithm 1 and its robustness to initialization.
III.
EXPERIMENTS In this section, we show numerical experiments in support of our analytical conclusions.
We also provide results further illustrating the robustness of the algorithm to initializations.
In our experiments, we generated the training data using randomly generated W∗ and Z∗, with n = 50, N = 10000, and s = {5, 10}.
The transform W∗ is generated in each case by applying Matlab’s orth() function on a standard Gaussian matrix.
The representation matrix Z∗ is generated for each s as described in Conjecture 1, i.e., the support of Fig.
2: The performance of Algorithm 1 with various initial- izations for s = 5 (top) and s = 10 (bottom).
(cid:17) (·,j) (·,j)(cid:107)2 (cid:16) Z∗ In the ﬁrst experiment, each column of Z∗ is chosen uniformly at random and the nonzero entries are drawn i.i.d. from a Gaussian distribution with mean zero and variance n/sN.
the initial W0 in Algorithm 1 is chosen to satisfy (cid:107)W0 − W∗(cid:107)F ≤  with  = (see (5)).
Fig.
1 shows the behavior 0.49 minj β of the Frobenious error between Wt and W∗ in the algo- rithm.
The observed (linear) convergence of the iterates to the generative operator W∗ is in accordance with Theorem II.2 and our Conjecture 1.
Next, we study the performance of Algorithm 1 with differ- ent initializations for n = 50, N = 10000, and s = {5, 10}.
Fig.
2 shows the objective function in Problem (1) over the algorithm iterations.
Since the training data satisfy Assump- tions (A1) and (A2), the minimum objective value in (1) is 0.
Six different types of initializations are considered.
The ﬁrst, labeled ‘eps’, denotes an initialization as in Fig.
1 with .
The other initializations are as  = 0.49 minj β follows: entries of W0 drawn i.i.d. from a standard Gaussian distribution (labeled ‘rand’); an n × n identity matrix W0 labeled ‘id’; a discrete cosine transform (DCT) initialization labeled ‘dct’; entries of W0 drawn i.i.d. from a uniform distri- bution ranging from 0 to 1 (labeled ‘unif’); and W0 = 0n×n labeled ‘zero’.
For more general initializations (other than ‘eps’), we see that the behavior of Algorithm 1 is split into (cid:16) Z∗ (cid:107)Z∗ (cid:17) (cid:107)Z∗ (·,j) (·,j)(cid:107)2 [9] S.
Ravishankar and Y.
Bresler, “Learning sparsifying transforms,” IEEE Trans.
Signal Process., vol.
61, no.
5, pp.
1072–1086, 2013.
[10] S.
Ravishankar and Y.
Bresler, “Closed-form solutions within sparsifying transform learning,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013, pp.
5378–5382.
[11] B.
Wen, S.
Ravishankar, and Y.
Bresler, “Structured overcomplete sparsi- fying transform learning with convergence guarantees and applications,” International Journal of Computer Vision, vol.
114, no.
2-3, pp.
137– 167, 2015.
[12] D.
A.
Spielman, H.
Wang, and J.
Wright, “Exact recovery of sparsely- used dictionaries,” in Proceedings of the 25th Annual Conference on Learning Theory, 2012, pp.
37.1–37.18.
[13] S.
Arora, R.
Ge, and A.
Moitra, “New algorithms for learning incoherent and overcomplete dictionaries,” in Proceedings of The 27th Conference on Learning Theory, 2014, pp.
779–806.
[14] Y.
Xu and W.
Yin, “A fast patch-dictionary method for whole image recovery,” Inverse Problems and Imaging, vol.
10, no.
2, pp.
563–583, 2016.
[15] A.
Agarwal, A.
Anandkumar, P.
Jain, P.
Netrapalli, and R.
Tandon, “Learning sparsely used overcomplete dictionaries,” Journal of Machine Learning Research, vol.
35, pp.
1–15, 2014.
[16] S.
Ravishankar and Y.
Bresler, “(cid:96)0 sparsifying transform learning with IEEE Trans.
efﬁcient optimal updates and convergence guarantees,” Signal Process., vol.
63, no.
9, pp.
2389–2404, May 2015.
[17] C.
Bao, H.
Ji, Y.
Quan, and Z.
Shen, “Dictionary learning for sparse coding: Algorithms and convergence analysis,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol.
38, no.
7, pp.
1356– 1369, July 2016.
[18] A.
Agarwal, A.
Anandkumar, P.
Jain, and P.
Netrapalli, “Learning sparsely used overcomplete dictionaries via alternating minimization,” SIAM Journal on Optimization, vol.
26, no.
4, pp.
2775–2799, 2016.
[19] J.
Sun, Q.
Qu, and J.
Wright, “Complete dictionary recovery over the sphere I: Overview and the geometric picture,” IEEE Transactions on Information Theory, vol.
63, no.
2, pp.
853–884, Feb 2017.
[20] J.
Sun, Q.
Qu, and J.
Wright, “Complete dictionary recovery over IEEE the sphere II: Recovery by riemannian trust-region method,” Transactions on Information Theory, vol.
63, no.
2, pp.
885–914, Feb 2017.
[21] S.
Ravishankar and Y.
Bresler, “Data-driven learning of a union of IEEE sparsifying transforms model for blind compressed sensing,” Transactions on Computational Imaging, vol.
2, no.
3, pp.
294–309, 2016.
[22] S.
Ravishankar and Y.
Bresler, “Efﬁcient blind compressed sensing using sparsifying transforms with convergence guarantees and application to magnetic resonance imaging,” SIAM Journal on Imaging Sciences, vol.
8, no.
4, pp.
2519–2557, 2015.
[23] C.
Bao, H.
Ji, and Z.
Shen, “Convergence analysis for iterative data- driven tight frame construction scheme,” Applied and Computational Harmonic Analysis, vol.
38, no.
3, pp.
510–523, 2015.
[24] A.
Ma S.
Ravishankar and D.
Needell, “Analysis of fast structured dictionary learning,” Information and Inference, 2018, in preparation.
two phases.
In the ﬁrst phase, the iterates slowly decrease the objective.
When the iterates are close enough to a solution, the second phase occurs and during this phase, Algorithm 1 enjoys rapid convergence (towards 0).
Note that the objective’s convergence rate in the second phase is comparable to that of the ‘eps’ case.
The behavior of Algorithm 1 is similar for s = 5 and s = 10, with the latter case taking more iterations to enter the second phase of convergence.
This makes sense since there are more variables to learn for larger s.
IV.
CONCLUSION This work presented an analysis of a fast alternating min- imization algorithm for unitary sparsifying operator learning.
We proved local linear convergence of the algorithm to the underlying generative model under mild assumptions.
Numer- ical experiments illustrated this local convergence behavior, and demonstrated that the algorithm is robust to initialization in practice.
The full version of this work, including the proof of Theorem II.2 and numerical results that support Conjecture 1 can be found in [24].
A theoretical analysis of the algorithm’s robustness observed in Fig.
2 is left for future work.
ACKNOWLEDGMENTS Saiprasad Ravishankar was supported in part by the follow- ing grants: NSF grant CCF-1320953, ONR grant N00014-15- 1-2141, DARPA Young Faculty Award D14AP00086, ARO MURI grants W911NF-11-1-0391 and 2015-05174-05, NIH grants R01 EB023618 and U01 EB018753, and a UM-SJTU seed grant.
Anna Ma and Deanna Needell were supported by the NSF DMS #1440140 (while they were in residence at the Mathematical Science Research Institute in Berkeley, California, during the Fall 2017 semester), NSF CAREER DMS #1348721, and the NSF BIGDATA DMS #1740325.
REFERENCES [1] M.
Aharon, M.
Elad, and A.
Bruckstein, “K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation,” IEEE Transactions on Signal Processing, vol.
54, no.
11, pp.
4311–4322, 2006.
[2] M.
Yaghoobi, T.
Blumensath, and M.
Davies, “Dictionary learning for sparse approximations with the majorization method,” IEEE Transac- tions on Signal Processing, vol.
57, no.
6, pp.
2178–2191, 2009.
[3] J.
Mairal, F.
Bach, J.
Ponce, and G.
Sapiro, “Online learning for matrix factorization and sparse coding,” J.
Mach.
Learn.
Res., vol.
11, pp.
19–60, 2010.
[4] D.
Barchiesi and M.
D.
Plumbley, “Learning incoherent dictionaries for sparse approximation using iterative projections and rotations,” IEEE Transactions on Signal Processing, vol.
61, no.
8, pp.
2055–2065, 2013.
[5] L.
N.
Smith and M.
Elad, “Improving dictionary learning: Multiple IEEE Signal Processing dictionary updates and coefﬁcient reuse,” Letters, vol.
20, no.
1, pp.
79–82, Jan 2013.
[6] C.
Bao, H.
Ji, Y.
Quan, and Z.
Shen, “L0 norm based dictionary learning by proximal methods with global convergence,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014, pp.
3858– 3865.
[7] S.
Ravishankar, R.
R.
Nadakuditi, and J.
A.
Fessler, “Efﬁcient sum of outer products dictionary learning (SOUP-DIL) and its application to inverse problems,” IEEE Transactions on Computational Imaging, vol.
3, no.
4, pp.
694–709, Dec 2017.
[8] S.
Ravishankar, B.
E.
Moore, R.
R.
Nadakuditi, and J.
A.
Fessler, “Efﬁcient learning of dictionaries with low-rank atoms,” in 2016 IEEE Global Conference on Signal and Information Processing (GlobalSIP), Dec 2016, pp.
222–226.

Data acquisition and analysis is ubiquitous, but data often contains errors and can be highly incomplete.
For example, if data is obtained via user surveys, people may only choose to answer a subset of questions.
Ideally, one would not want to eliminate surveys that are only partially complete, as they still contain potentially useful information.
For many tasks, such as certain regression or classiﬁcation tasks, one may require complete or completed data [SG02].
Alternatively, consider the problem of collaborative ﬁltering, made popular by the classic Netﬂix problem [BL07], [BK07], [KBV09], in which one aims to predict user ratings for unseen movies based on available user-movie ratings.
In this setting, accurate data completion is the goal, as opposed to a data pre-processing task.
Viewing users as the rows in a matrix and movies as the columns, we would like to recover unknown entries of the resulting matrix from the subset of known entries.
This is the goal in many types of other applications, ranging from systems identiﬁcation [LV09] to sensor networks [BLWY06], [Sch86], [Sin08].
This task is known as matrix completion [Rec11].
If the underlying matrix is low-rank and the observed entries are sampled uniformly at random, one can achieve exact recovery with high probability under mild additional assumptions by using nuclear norm minimization (NNM) [CT10], [RFP10], [CR09], [Gro11], [CP10].
For many applications, however, we expect structural dif- ferences between the observed and unobserved entries, which violate these classical assumptions.
By structural differences, we mean that whether an entry is observed or unobserved need not be random or occur by some uniform selection mechanism.
Consider again the Netﬂix problem.
Popular, or well-received movies are more likely to have been rated by many users, A.
Nuclear Norm Matrix Completion Let M ∈ Rn1×n2 be the unknown matrix we would like to recover and Ω be the set of indices of the observed entries.
Let PΩ : Rn1×n2 → Rn1×n2, where (cid:40) [PΩ]ij = Mij (i, j) ∈ Ω (i, j) (cid:54)∈ Ω as in [CT10].
In many applications, it is reasonable to assume that the matrix M is low-rank.
For example, we expect that relatively few factors contribute to a user’s movie preferences as compared to the number of users or number of movies con- sidered.
Similarly, for health data, a few underlying features may contribute to many observable signs and symptoms.
The minimization, (cid:99)M = argmin rank(A) s.t. PΩ(A) = PΩ(M ) recovers the lowest rank matrix that matches the observed entries exactly.
Unfortunately, this minimization problem is NP-hard, so one typically uses the convex relaxation ||A||∗ s.t. PΩ(A) = PΩ(M ), (cid:99)M = argmin singular values, i.e. ||X||∗ := (cid:80) where || · ||∗ is the nuclear norm, given by the sum of the i σi(X) [CT10], [RFP10], (1) [CP10], [CR09].
1Of course, some applications will tend to have higher values in missing entries, in which case our methods can be scaled accordingly.
B.
Matrix Completion for Structured Observations We propose adding a regularization term on the unobserved entries to promote adherence to the structural assumption that we expect these entries to be close to 0.
We solve (cid:102)M = argmin L1 norm ||M||1 =(cid:80) ||A||∗ + α||PΩC (A)|| s.t. PΩ(A) = PΩ(M ), (2) where α > 0 and || · || is an appropriate matrix norm.
For example, if we expect most of the unobserved entries to be 0, but a few to be potentially large in magnitude, the entrywise ij |Mij| is a reasonable choice.
C.
Matrix Completion with Noisy Observations In reality, we expect that our data is corrupted by some amount of noise.
We assume the matrix M, that we would like to recover, satisﬁes PΩY = PΩM + PΩZ, where PΩY are the observed values, M is low-rank and PΩZ represents the noise in the observed data.
In [CP10], Cand´es and Plan suggest using the following minimization to recover the unknown matrix: ||A||∗ s.
t.
||PΩ(M − A)||F < δ.
(3) (cid:99)M = argmin (cid:113)(cid:80) alent to (cid:99)M = argmin Recall, ||X||F = ij X 2 ij.
The formulation above is equiv- ||PΩ(M − A)||F + ρ||A||∗ (4) for some ρ = ρ(δ).
The latter minimization problem is generally easier to solve in practice [CP10].
In order to account for the assumption that the unobserved entries are likely to be close to zero, we again propose adding a regularization term on the unobserved entries and aim to solve(cid:102)M = argmin ||PΩ(M − A)||F + ρ||A||∗ + α||PΩC (A)||.
(5) II.
NUMERICAL RESULTS A.
Recovery without Noise We ﬁrst investigate the performance of (2) when the ob- served entries are exact, i.e. there is no noise or errors in the observed values.
In Figure 1, we consider low-rank matrices M ∈ R30×30.
To generate M of rank r, we take M = MLMR, where ML ∈ R30×r and MR ∈ Rr×30 are sparse matrices (with density 0.3 and 0.5, respectively) and whose nonzero entries are uniformly distributed at random between zero and one.
We subsample from the zero and nonzero entries of the data matrix at various rates to generate a matrix with missing entries.
We compare performance of (2) using L1 regularization on the unobserved entries with standard NNM and report the error ratio ||(cid:102)M −M||F /||(cid:99)M −M||F for various sampling rates, where (cid:102)M and (cid:99)M are the solutions to (2) and (1), respectively.
The regularization parameter α used is selected optimally from the set {10−1, 10−2, 10−3, 10−4} (discussed below).
Values below one in Figure 1 indicate that the minimization with L1 regularization outperforms standard NNM.
Results are averaged over ten trials.
As expected, we ﬁnd that if the sampling of the nonzero entries is high, then the modiﬁed method (2) is likely to outperform standard NNM.
We choose the parameter α, for the regularization term, to be optimal among α ∈ {10−1, 10−2, 10−3, 10−4} and report the values used in Figure 2.
For large α, the recovered matrix will approach that for which all unobserved entries are predicted to be zero, and as α becomes close to zero, recovery by (2) approaches that of standard NNM.
When the sampling rate of the zero entries is low and the sampling of the nonzero entries is high, in addition to (2) outperforming NNM, we also see that a larger value for α is optimal, supporting the claim that regularization improves performance.
Higher α values are also sometimes optimal when the nonzero sampling rate is nearly zero.
If there are very few nonzero entries sampled then the low-rank matrix recovered is likely to be very close to the zero matrix.
In this setting, we expect that even with standard NNM the unobserved entries are thus likely to be recovered as zeros and so a larger coefﬁcient on the regularization term will not harm performance.
When α is close to zero, the difference in performance is minimal, as the regularization will have little effect in this case.
For (cid:102)M and (cid:99)M given by (2) and (1), respectively, with L1 plot ||(cid:102)M − M||F /||(cid:99)M − M||F .
We consider 30x30 matrices of various Fig.
1.
regularization on the recovered values for the unobserved entries, we ranks and average results over ten trials, with α optimal among α ∈ {10−1, 10−2, 10−3, 10−4}.
B.
Recovery with Noisy Observed Entries We generate matrices as in the previous section and now consider the minimization given in (4).
Suppose the entries of the noise matrix Z are i.i.d. N (0, σ2).
We set the parameter ρ, as done in [CP10], to be ρ = ( n1 + n2) (cid:115) |Ω| σ.
n1n2 C.
Matrix recovery of health data Next, we consider real survey data from 2126 patients responding to 65 particular questions provided by LymeDis- ease.org.
Data used was obtained from the LymeDisease.org patient registry, MyLymeData, Phase 1, June 17, 2017.
Ques- tion responses are integer values between zero and four and answering all questions was required, that is this subset of the data survey is complete (so we may calculate reconstruction errors).
All patients have Lyme disease and survey questions ask about topics such as current and past symptoms, treatments and outcomes.
For example, “I would say that currently in general my health is: 0-Poor, 1-Fair, 2-Good, 3-Very good, 4-Excellent.” Although, this part of the data considered is complete, we expect that in general, patients are likely to record responses for particularly noticeable symptoms, while a missing response in a medical survey may indicate a lack of symptoms.
Thus, in this setting, L1 regularization of the unobserved entries is a natural choice.
Due to computational constraints, for each of the ten trials executed, we randomly sample 50 of these patient surveys to generate a 50x65 matrix.
As in the previous experiments, we subsample from the zero and nonzero entries of the data matrix at various rates to generate a matrix with missing entries.
We complete this subsampled matrix with both NNM (1) and (2) using L1 regularization on the unobserved entries and report ||(cid:102)M −M||F /||(cid:99)M −M||F , averaged over ten trials in Figure 4.
The parameter α, for the regularization term, is chosen to be optimal among α ∈ {10−1, 10−2, 10−3, 10−4} and we report the values used in Figure 5.
The results for the Lyme disease data match closely those found in the synthetic experiments done with and without noise.
Regularizing the L1-norm of the unobserved entries improves performance if the sampling of non-zero entries is sufﬁciently high and sampling of zero entries is sufﬁciently low.
Fig.
2.
Average optimal α value among α ∈ {10−1, 10−2, 10−3, 10−4} for the minimization given in (2) with L1 regularization on the recovered values for the unobserved entries.
The matrices considered here are the same as in Figure 1.
We speciﬁcally consider low-rank matrices M ∈ R30×30 generated as in the previous section and a noise matrix Z with i.i.d. entries sampled from N (0, 0.01).
Thus we set ρ = 2 various sampling rates of the zero and nonzero entries of M in (cid:113)|Ω| 30 ·0.1. We again report ||(cid:102)M −M||F /||(cid:99)M −M||F for Figure 3.
Here,(cid:99)M and(cid:102)M are given by (4) and (5) respectively.
We see improved performance with regularization when the sampling rate of the zero entries is low and the sampling of the nonzero entries is high.
For (cid:102)M and (cid:99)M given by (2) and (1), respectively, with L1 ||(cid:102)M − M||F /||(cid:99)M − M||F .
We consider 30x30 matrices of various ranks Fig.
3.
regularization on the recovered values for the unobserved entries, we plot with normally distributed i.i.d. noise with standard deviation σ = 0.1 added.
We average results over ten trials and with α optimal among α ∈ {10−1, 10−2, 10−3, 10−4}.
For (cid:102)M and (cid:99)M given by (2) and (1), respectively, with L1 ||(cid:102)M −M||F /||(cid:99)M −M||F .
We consider 50 patient surveys with 65 responses Fig.
4.
regularization on the recovered values for the unobserved entries, we plot each chosen randomly from 2126 patient surveys.
We average results over ten trials and with α optimal among α ∈ {10−1, 10−2, 10−3, 10−4}.
A.
Connection to Robust Principal Component Analysis (RPCA) The program (2) very closely resembles the method pro- posed in [CLMW11], called Robust Principal Component Analysis (RPCA).
RPCA is a modiﬁed version of traditional Principal Component Analysis that is robust to rare corruptions of arbitrary magnitude.
In RPCA, one assumes that a low- rank matrix has some set of its entries corrupted.
The goal is to recover the true underlying matrix despite the corruptions.
More simply, for the observed matrix Y ∈ Rn1×n2, we have the decomposition Y = L + S, where L is the low-rank matrix we would like to recover and S is a sparse matrix of corruptions.
The strategy for ﬁnding this decomposition proposed in [CLMW11] is ||L||∗ + α||S||1 s.t. L + S = Y.
(6) argmin L,S This method can be extended to the matrix completion setting, in which one would like to recover unobserved values from observed values, of which a subset may be corrupted.
In this setting, [CLMW11] proposes solving the following minimiza- tion problem ||L||∗ + α||S||1 s.t. PΩ(L + S) = PΩ(Y ).
argmin L,S We now return to our original matrix completion problem, in which we assume the observed entries to be exact.
Let M ∈ Rn1×n2 again be the matrix we aim to recover.
If we expect the unobserved entries of M to be sparse, that is, only a small fraction of them to be nonzero, we can rewrite the minimization (2) in a form similar to RPCA in which we know the support of the corruptions is restricted to the set ΩC, i.e. S = PΩC (S).
We then have, ||A||∗ + α||S||1 s.t. A + S = PΩ(M ).
(7) argmin A,S This strategy differs from traditional RPCA in that we assume the observed data to be free from errors and therefore know that the corruptions are restricted to the set of unobserved entries.
the following result.
Directly applying Theorem 1.1 from [CLMW11], we have Proposition 2: Suppose M ∈ Rn1×n2 and M = U ΣV ∗ gives the singular value decomposition of M.
Suppose also max ||U∗ei||2 ≤ µr n1 ||V ∗ei||2 ≤ µr n2 ||X||∞ = maxi,j |Xi,j|, ei where r is the rank of M, is the ith standard basis vector and µ is the incoherence parameter as deﬁned in [CLMW11].
Suppose that the set of observed entries, Ω, is uniformly distributed among all sets of cardinality of m and the support set of S0 of non-zero unobserved entries is uniformly distributed among all sets of cardinality s contained in ΩC.
Then there is a numerical and ||U V ∗||∞ ≤ , max (cid:114) µr n1n2 Fig.
5.
Average optimal α value among α ∈ {10−1, 10−2, 10−3, 10−4} for the minimization given in (2) with L1 regularization on the recovered values for the unobserved entries in Lyme patient data.
III.
ANALYTICAL REMARKS We provide here some basic analysis of the regularization approach.
First, in the simpliﬁed setting, in which all of the unobserved entries are exactly zero, the modiﬁed recovery given in (2) will always perform at least as well as traditional NNM.
Proposition 1: Suppose M ∈ Rn1×n2 and Ω gives the set of index pairs of the observed entries.
Assume that all of the unobserved entries are exactly zero, i.e. PΩC (M ) = 0.
Then for (cid:99)M = argmin||A||∗ s.t. PΩ(A) = PΩ(M ), and(cid:102)M = argmin||A||∗ + α||PΩC (A)|| s.t. PΩ(A) = PΩ(M ), we have for any matrix norm || · ||.
Proof: From the deﬁnitions of (cid:99)M and (cid:102)M, ||(cid:102)M − M|| ≤ ||(cid:99)M − M|| ||(cid:99)M||∗ ≤ ||(cid:102)M||∗.
For α > 0, we have Using the inequality above, ||(cid:102)M||∗ + α||PΩC ((cid:102)M )|| ≤ ||(cid:99)M||∗ + α||PΩC ((cid:99)M )|| ≤ ||(cid:102)M||∗ + α||PΩC ((cid:99)M )||.
||PΩC ((cid:102)M )|| ≤ ||PΩC ((cid:99)M )||.
PΩ((cid:102)M ) = PΩ((cid:99)M ) = PΩ(M ) and under the assumption that PΩC (M ) = 0, as ||(cid:102)M − M|| = ||PΩC ((cid:102)M )|| ≤ ||PΩC ((cid:99)M )|| = ||(cid:99)M − M||.
The desired result then follows since constant c such that with probability at least 1 − cn−10 the minimization in (7) with α = 1/ n achieves exact recovery, provided that rank(L0) ≤ ρrn(2)µ−1(log n(1))−2 and s ≤ ρsn(1)n(2), where ρr and ρs are positive numerical constants.
This proposition is a direct application of Theorem 1.1 in [CLMW11] to the program given by (7).
Note that here, the corruptions are exactly the unobserved entries that are nonzero.
Thus, if s, the number of nonzero unobserved entries is small, this result may be stronger than corresponding matrix completion results that the larger, number of missing entries.
instead depend on m, The authors of [CLMW11] note that RPCA can be thought of as a more challenging version of matrix completion.
The reasoning being, that in matrix completion we aim to recover the set of unobserved entries, whose locations are known, whereas in the RPCA setting, we have a set of corrupted entries, whose locations are unknown, and for which we would like to both identify as erroneous and determine their correct values.
Figure 1 of [CLMW11] provides numerical evidence that in practice RPCA does in fact require more stringent conditions to achieve exact recovery than the corresponding matrix completion problem.
In image completion or repair, corruptions are often spatially correlated or isolated to speciﬁc regions of an image.
In [LRZM12], the authors provide exper- imental evidence that incorporating an estimate of the support of the corruptions aids in recovery.
By the same reasoning, we expect that a stronger result than suggested by Proposition 2 likely holds, as we do not make use of the fact that we are able to restrict the locations of the corruptions (nonzero, unobserved entries) to a subset of the larger matrix.
IV.
DISCUSSION For incomplete data in which we expect that unobserved entries are likely to be 0, we ﬁnd that regularizing the values of the unobserved entries when performing NNM improves performance under various conditions.
This improvement in performance holds for both synthetic data, with and without noise, as well as for Lyme disease survey data.
We speciﬁcally investigate the performance of L1 regularization on the unob- served entries as it is a natural choice for many applications.
Testing the validity of methods, such as (2), on real data is challenging, since this setting hinges on the assumption that unobserved data is structurally different than observed data and would require having access to ground truth values for the unobserved entries.
In this paper, we choose to take complete data and artiﬁcially partition it into observed and unobserved entries.
Another way to manage this challenge is to examine performance of various tasks, such as classiﬁcation or prediction, based on data that has been completed in different ways.
In this setting, relative performance of different completion strategies will likely depend on the speciﬁc task considered.
However, for many applications, one would like to complete the data in order to use it for a further goal.
In this setting, judging the performance of the matrix completion algorithm by its effect on performance of the ultimate goal is very natural.
We offer preliminary arguments as to why we might ex- pect the approach in (2) to work well under the structural assumption that unobserved entries are likely to be sparse or small in magnitude, however, stronger theoretical results are likely possible.
For example, we show that regularizing the values of the unobserved entries when performing NNM improves performance in the case when all unobserved entries are exactly zero, but based on empirical evidence we expect improved performance under more general conditions.
A range of papers, including [CT10], [RFP10], [CR09], [Gro11], discuss the conditions under which exact matrix completion is possible under the assumption that the observed entries of the matrix are sampled uniformly at random.
Under what reasonable structural assumptions on the unobserved entries might we still be able to specify conditions that will lead to exact recovery?
We save such questions for future work.
ACKNOWLEDGMENTS The authors would like to thank LymeDisease.org for the use of data derived from MyLymeData to conduct this study.
We would also like to thank the patients for their contributions to MyLymeData, and Anna Ma for her guidance in working with this data.
In addition, the authors were supported by NSF CAREER DMS #1348721, NSF BIGDATA DMS #1740325, and MSRI NSF DMS #1440140.
REFERENCES [BK07] [BL07] R.
M.
Bell and Y.
Koren.
Lessons from the Netﬂix prize challenge.
Acm Sigkdd Explorations Newsletter, 9(2):75–79, 2007.
J.
Bennett and S.
Lanning.
The Netﬂix prize.
In Proceedings of KDD cup and workshop, volume 2007, page 35.
New York, NY, USA, 2007.
[BLWY06] P.
Biswas, T.-C.
Lian, T.-C.
Wang, and Y.
Ye. Semideﬁnite programming based algorithms for sensor network localization.
ACM Trans.
Sensor Networks (TOSN), 2(2):188–220, 2006.
[CLMW11] E.
J.
Cand´es, X.
Li, Y.
Ma, and J.
Wright.
Robust principal [CP10] [CR09] [CT10] [Gro11] [KBV09] component analysis?
J.
of the ACM, 58(1):1–37, 2011.
E.
J.
Cand`es and Y.
Plan.
Matrix completion with noise.
Proceedings of the IEEE, 9(6):925–936, 2010.
E.
J.
Cand`es and B.
Recht.
Exact matrix completion via convex optimization.
Found.
Comput.
Math., 9(6):717–772, 2009.
E.
J.
Cand`es and T.
Tao.
The power of convex relaxation: Near-optimal matrix completion.
IEEE Trans.
Inform.
Theory, 56(5):2053–2080, 2010.
D.
Gross.
Recovering low-rank matrices from few coefﬁcients in any basis.
IEEE Trans.
Inform.
Theory, 57(3):1548–1566, 2011.
Y.
Koren, R.
Bell, and C.
Volinsky.
Matrix factorization techniques for recommender systems.
Computer, 42(8), 2009.
[LV09] [Rec11] [RFP10] [LRZM12] X.
Liang, X.
Ren, Z.
Zhang, and Y.
Ma. Repairing sparse low- rank texture.
Computer Vision–ECCV 2012, pages 482–495, 2012.
Z.
Liu and L.
Vandenberghe.
Interior-point method for nuclear norm approximation with application to system identiﬁcation.
SIAM J.
Matrix Analysis and Appl., 31(3):1235–1256, 2009.
B.
Recht.
A simpler approach to matrix completion.
J.
Machine Learning Research, 12(Dec):3413–3430, 2011.
B.
Recht, M.
Fazel, and P.
A.
Parrilo.
Guaranteed minimum rank solutions to linear matrix equations via nuclear norm minimization.
SIAM Review, 52(3):471–501, 2010.
R.
Schmidt.
Multiple emitter location and signal parameter estimation.
IEEE Trans.
Antennas and Propagation, 34(3):276– 280, 1986.
J.
L.
Schafer and J.
W.
Graham.
Missing data: our view of the state of the art.
Psychological methods, 7(2):147, 2002.
A.
Singer.
A remark on global positioning from local distances.
Proc.
National Academy of Sciences, 105(28):9507–9511, 2008.
[Sch86] [SG02] [Sin08]
Lung cancer is the most common cause of cancer-related death in men.
Low-dose lung CT screening provides an ef- fective way for early diagnosis, which can sharply reduce the lung cancer mortality rate.
Advanced computer-aided diagnosis systems (CADs) are expected to have high sensi- tivities while at the same time maintaining low false positive rates.
Recent advances in deep learning enable us to rethink the ways of clinician lung cancer diagnosis.
1https://github.com/uci-cbcl/DeepLung.git Current lung CT analysis research mainly includes nod- ule detection [6, 5], and nodule classiﬁcation [26, 25, 14, 33].
There is few work on building a complete lung CT cancer diagnosis system for fully automated lung CT can- cer diagnosis using deep learning, integrating both nodule detection and nodule classiﬁcation.
It is worth exploring a whole lung CT cancer diagnosis system and understanding how far the performance of current deep learning technol- ogy differs from that of experienced doctors.
To our best knowledge, this is the ﬁrst work for a fully automated and complete lung CT cancer diagnosis system using deep nets.
The emergence of large-scale dataset, LUNA16 [24], accelerated the nodule detection related research.
Typi- cally, nodule detection consists of two stages, region pro- posal generation and false positive reduction.
Traditional approaches generally require manually designed features such as morphological features, voxel clustering and pixel thresholding [20, 15].
Recently, deep ConvNets, such as Faster R-CNN [21, 17] and fully ConvNets [18, 37, 31, 30, 29], are employed to generate candidate bounding boxes [5, 6].
In the second stage, more advanced methods or com- plex features, such as carefully designed texture features, are used to remove false positive nodules.
Because of the 3D nature of CT data and the effectiveness of Faster R-CNN for object detection in 2D natural images [13], we design a 3D Faster R-CNN for nodule detection with 3D convolu- tional kernels and a U-net-like encoder-decoder structure to effectively learn latent features [22].
The U-Net structure is basically a convolutional autoencoder, augmented with skip connections between encoder and decoder layers [22].
Al- though it has been widely used in the context of semantic segmentation, being able to capture both contextual and lo- cal information should be very helpful for nodule detections as well.
Because 3D ConvNet has too large a number of pa- rameters and is difﬁcult to train on public lung CT datasets of relatively small sizes, 3D dual path network is employed as the building block since deep dual path network is more compact and provides better performance than deep resid- ual network at the same time [3].
Before the era of deep learning, manual feature engi- Figure 1.
The framework of DeepLung.
DeepLung ﬁrst employs 3D Faster R-CNN to generate candidate nodules.
Then it uses deep 3D DPN to extract deep features from the detected and cropped nodules.
Lastly, GBM with deep features, detected nodule size, and raw pixels is employed for classiﬁcation.
Patient-level diagnosis can be achieved by fusing the classiﬁcation results of detected nodules in the CT.
neering followed by classiﬁers was the general pipeline for nodule classiﬁcation [10].
After the large-scale LIDC-IDRI [2] dataset became publicly available, deep learning-based methods have become the dominant framework for nod- ule classiﬁcation research [25, 35].
Multi-scale deep Con- vNet with shared weights on different scales has been pro- posed for the nodule classiﬁcation [26].
The weight sharing scheme reduces the number of parameters and forces the multi-scale deep ConvNet to learn scale-invariant features.
Inspired by the recent success of dual path network (DPN) on ImageNet [3, 4], we propose a novel framework for CT nodule classiﬁcation.
First, we design a deep 3D dual path network to extract features.
As gradient boosting machines (GBM) are known to have superb performance given effec- tive features, we use GBM with deep 3D dual path features, nodule size, and cropped raw nodule CT pixels for the nod- ule classiﬁcation [8].
Finally, we built a fully automated lung CT cancer di- agnosis system, henceforth called DeepLung, by combin- ing the nodule detection network and nodule classiﬁcation network together, as illustrated in Fig.
1.
For a CT im- age, we ﬁrst use the detection subnetwork to detect candi- date nodules.
Next, we employ the classiﬁcation subnet- work to classify the detected nodules into either malignant or benign.
Finally, the patient-level diagnosis result can be achieved for the whole CT by fusing the diagnosis result of each nodule.
Our main contributions are as follows: 1) To fully ex- ploit the 3D CT images, two deep 3D ConvNets are de- signed for nodule detection and classiﬁcation respectively.
Because 3D ConvNet contains too many parameters and is difﬁcult to train on relatively small public lung CT datasets, we employ 3D dual path networks as the neural network architecture since DPN uses less parameters and obtains better performance than residual network [3].
Speciﬁcally, inspired by the effectiveness of Faster R-CNN for object detection [13], we propose 3D Faster R-CNN for nodule detection based on 3D dual path network and U-net-like encoder-decoder structure, and deep 3D dual path network for nodule classiﬁcation.
2) Our classiﬁcation framework achieves better performance compared with state-of-the-art approaches, and surpasses the performance of experienced doctors on the public dataset, LIDC-IDRI.
3) Our fully au- tomated DeepLung system, nodule classiﬁcation based on detection, is comparable to the performance of experienced doctors both on nodule-level and patient-level diagnosis.
2.
Related Work Traditional nodule detection involves hand-designed fea- tures or descriptors [19] requiring domain expertise.
Re- cently, several works have been proposed to use deep Con- vNets for nodule detection to automatically learn features, which is proven to be much more effective than hand- designed features.
Setio et al.
proposes multi-view Con- vNet for false positive nodule reduction [23].
Due to the 3D nature of CT scans, some work proposed 3D ConvNets to handle the challenge.
The 3D fully ConvNet (FCN) is proposed to generate region candidates, and deep ConvNet with weighted sampling is used for false positive reduction [6].
Ding et al.
and Liao et al.
use the Faster R-CNN to generate candidate nodules followed by 3D ConvNets to re- move false positive nodules [5, 17].
Due to the effective performance of Faster R-CNN [13, 21], we design a novel network, 3D Faster R-CNN with 3D dual path blocks, for the nodule detection.
Further, a U-net-like encoder-decoder scheme is employed for 3D Faster R-CNN to effectively learn the features [22].
Nodule classiﬁcation has traditionally been based on segmentation [7] and manual feature design [1].
Several works designed 3D contour feature, shape feature and tex- ture feature for CT nodule diagnosis [32, 7, 10].
Recently, deep networks have been shown to be effective for medical images.
Artiﬁcial neural network was implemented for CT nodule diagnosis [28].
More computationally effective net- work, multi-scale ConvNet with shared weights for differ- ent scales to learn scale-invariant features, is proposed for nection is that there might exist some redundancy in the ex- ploited features.
And dual path connection uses part of fea- ture maps for dense connection and part of them for resid- ual learning.
In implementation, the dual path connection splits its feature maps into two parts.
One part, F(x)[d :], is used for residual learning, the other part, F(x)[: d], is used for dense connection as shown in Fig.
2.
Here d is a hyper-parameter for deciding how many new features to be exploited.
The dual path connection can be formulated as y = G([x[: d], F(x)[: d], F(x)[d :] + x[d :]), (1) where y is the feature map for dual path connection, G is used as ReLU activation function, F is convolutional layer functions, and x is the input of dual path connection block.
Dual path connection integrates the advantages of the two advanced frameworks, residual learning for feature reuse and dense connection for the ability to exploit new features, into a uniﬁed structure which obtained success on the Ima- geNet dataset[4].
We design deep 3D neural nets based on 3D DPN because of its compactness and effectiveness.
The 3D Faster R-CNN with a U-net-like encoder- decoder structure and 3D dual path blocks is illustrated in Fig.
3.
Due to the GPU memory limitation, the input of 3D Faster R-CNN is cropped from 3D reconstructed CT im- ages with pixel size 96 × 96 × 96.
The encoder network is derived from 2D DPN [3].
Before the ﬁrst max-pooling, two convolutional layers are used to generate features.
Af- ter that, eight dual path blocks are employed in the encoder subnetwork.
We integrate the U-net-like encoder-decoder design concept in the detection to learn the deep nets efﬁ- ciently [22].
In fact, for the region proposal generation, the 3D Faster R-CNN conducts pixel-wise multi-scale learning and the U-net is validated as an effective way for pixel-wise labeling.
This integration makes candidate nodule gener- ation more effective.
In the decoder network, the feature maps are processed by deconvolution layers and dual path blocks, and are subsequently concatenated with the corre- sponding layers in the encoder network [34].
Then a convo- lutional layer with dropout (dropout probability 0.5) is used in the second to the last layer.
In the last layer, we design 3 anchors, 5, 10, 20, for scale references which are designed based on the distribution of nodule sizes.
For each anchor, there are 5 parts in the loss function, classiﬁcation loss Lcls for whether the current box is a nodule or not, regression loss Lreg for nodule coordinates x, y, z and nodule size d.
If an anchor overlaps a ground truth bounding box with the intersection over union (IoU) higher than 0.5, we con- sider it as a positive anchor (p(cid:63) = 1).
On the other hand, if an anchor has IoU with all ground truth boxes less than 0.02, we consider it as a negative anchor (p(cid:63) = 0).
The multi-task loss function for the anchor i is deﬁned as L(pi, ti) = λLcls(pi, p(cid:63) i ) + p(cid:63) i Lreg(ti, ti (cid:63)), (2) Figure 2.
Illustration of dual path connection [3], which beneﬁts both from the advantage of residual learning [11] and that of dense connection [12] from network structure design intrinsically.
nodule classiﬁcation [26].
Deep transfer learning and multi- instance learning is used for patient-level lung CT diagno- sis [25, 36].
A comparative study on 2D and 3D ConvNets is conducted and 3D ConvNet is shown to be better than 2D ConvNet for 3D CT data [33].
Furthermore, a multi- task learning and transfer learning framework is proposed for nodule diagnosis [14].
Different from their approaches, we propose a novel classiﬁcation framework for CT nod- ule diagnosis.
Inspired by the recent success of deep dual path network (DPN) on ImageNet [3], we design a novel 3D DPN to extract features from raw CT nodules.
In part to the superior performance of GBM with complete features, we employ GBM with different levels of granularity rang- ing from raw pixels, DPN features, to global features such as nodule size for the nodule diagnosis.
Patient-level diag- nosis can be achieved by fusing the nodule-level diagnosis.
3.
DeepLung Framework Our fully automated lung CT cancer diagnosis system consists of two parts: nodule detection and classiﬁcation.
We design a 3D Faster R-CNN for nodule detection, and propose GBM with deep 3D DPN features, raw nodule CT pixels and nodule size for nodule classiﬁcation.
3.1. 3D Faster R-CNN with Deep 3D Dual Path Net for Nodule Detection Inspired by the success of dual path network on the Ima- geNet [3, 4], we design a deep 3D DPN framework for lung CT nodule detection and classiﬁcation in Fig.
3 and Fig.
4.
Dual path connection beneﬁts both from the advantage of residual learning and that of dense connection [11, 12].
The shortcut connection in residual learning is an effective way to eliminate vanishing gradient phenomenon in very deep networks.
From a learned feature sharing perspective, residual learning enables feature reuse, while dense connec- tion has an advantage of exploiting new features [3].
Addi- tionally, densely connected network has fewer parameters than residual learning because there is no need to relearn redundant feature maps.
The assumption of dual path con- Fig.
4.
The main reason we employ dual modules for de- tection and classiﬁcation is that classifying nodules into be- nign and malignant requires the system to learn ﬁner-level features, which can be achieved by focusing only on nod- ules.
In addition, it allows to introduce extra features in the ﬁnal classiﬁcation.
We ﬁrst crop CT data centered at pre- dicted nodule locations with size 32 × 32 × 32.
After that, a convolutional layer is used to extract features.
Then 30 3D dual path blocks are employed to learn higher level fea- tures.
Lastly, the 3D average pooling and binary logistic regression layer are used for benign or malignant diagnosis.
The deep 3D dual path network can be used as a clas- siﬁer for nodule diagnosis directly and it can also be em- ployed to learn effective features.
We construct feature by concatenating the learned deep 3D DPN features (the sec- ond from the last layer (2,560 dimension)), nodule size, and raw 3D cropped nodule pixels.
Given complete and effec- tive features, GBM is a superb method to build an advanced classiﬁer [8].
We validate the feature combining nodule size with raw 3D cropped nodule pixels in combination with the GBM classiﬁer and obtained 86.12% average test accuracy.
Lastly, we employ GBM with the constructed feature and achieve the best diagnosis performance.
3.3. DeepLung System: Fully Automated Lung CT Cancer Diagnosis The DeepLung system includes the nodule detection us- ing the 3D Faster R-CNN and nodule classiﬁcation using GBM with constructed feature (deep 3D dual path features, nodule size and raw nodule CT pixels) as shown in Fig.
1.
Due to the GPU memory limitation, we ﬁrst split the whole CT into several 96 × 96 × 96 patches, process them through the detector, and combine the detected results to- gether.
We only keep the detected boxes of detection prob- abilities larger than 0.12 (threshold as -2 before sigmoid function).
After that, non-maximum suppression (NMS) is adopted based on detection probability with the intersection over union (IoU) threshold as 0.1. Here we expect to not miss too many ground truth nodules.
After we get the detected nodules, we crop the nodule with the center as the detected center and size of 32 × 32 × 32.
The detected nodule size is kept as a feature input for later downstream classiﬁcation.
The deep 3D DPN is em- ployed to extract features.
We use the GBM and construct features to conduct diagnosis for the detected nodules.
For pixel feature, we use the cropped size of 16 × 16 × 16 and center as the detected nodule center in the experiments.
For patient-level diagnosis, if one of the detected nodules is positive (cancer), the patient is classiﬁed as having cancer.
Conversely, if all detected nodules are negative, the patient is considered non-cancer.
Figure 3.
The 3D Faster R-CNN framework contains 3D dual path blocks and a U-net-like encoder-decoder structure.
We design 26 layers 3D dual path network for the encoder subnetwork.
The model employs 3 anchors and multi-task learning loss, including coordinates (x, y, z) and diameter d regression, and candidate box classiﬁcation.
The numbers in boxes are feature map sizes in the format (#slices*#rows*#cols*#maps).
The numbers above the connections are in the format (#ﬁlters #slices*#rows*#cols).
Figure 4.
The deep 3D dual path network framework in the nodule classiﬁcation subnetwork, which contains 30 3D dual path con- nection blocks.
After the training, the deep 3D dual path network feature is extracted for gradient boosting machine to do nodule diagnosis.
The numbers are of the same formats as Fig.
3.
where pi is the predicted probability for current anchor i being a nodule, ti is the predicted relative coordinates for nodule position, which is deﬁned as ti = ( x − xa da y − ya da z − za da , log( da )), (3) where (x, y, z, d) are the predicted nodule coordinates and diameter in the original space, (xa, ya, za, da) are the coor- dinates and scale for the anchor i.
For ground truth nodule position, it is deﬁned as x(cid:63) − xa y(cid:63) − ya z(cid:63) − za , log( )), (4) d(cid:63) da t(cid:63) i = ( da da da where (x(cid:63), y(cid:63), z(cid:63), d(cid:63)) are nodule ground truth coordinates and diameter.
The λ is set as 0.5. For Lcls, we used binary cross entropy loss function.
For Lreg, we used smooth l1 regression loss function [9].
3.2. Gradient Boosting Machine with 3D Dual Path Net Feature for Nodule Classiﬁcation For CT data, advanced method should be effective to ex- tract 3D volume feature [33].
We design a 3D deep dual path network for the 3D CT lung nodule classiﬁcation in 4.
Experiments We conduct extensive experiments to validate the DeepLung system.
We perform 10-fold cross validation us- ing the detector on LUNA16 dataset.
For nodule classiﬁ- cation, we use the LIDC-IDRI annotation, and employ the LUNA16’s patient-level dataset split.
Finally, we also val- idate the whole system based on the detected nodules both on patient-level diagnosis and nodule-level diagnosis.
In the training, for each model, we use 150 epochs in total with stochastic gradient descent optimization and mo- mentum as 0.9. The batch size parameter is limited by GPU memory.
We use weight decay as 1 × 10−4.
The initial learning rate is 0.01, 0.001 after half the total number of epoch, and 0.0001 after epoch 120.
4.1. Datasets LUNA16 dataset is a subset of the largest publicly avail- able dataset for pulmonary nodules, LIDC-IDRI [2, 24].
LUNA16 dataset only has the detection annotations, while LIDC-IDRI contains almost all the related information for low-dose lung CTs including several doctors’ annotations on nodule sizes, locations, diagnosis results, nodule texture, nodule margin and other informations.
LUNA16 dataset removes CTs with slice thickness greater than 3mm, slice spacing inconsistent or missing slices from LIDC-IDRI dataset, and explicitly gives the patient-level 10-fold cross validation split of the dataset.
LUNA16 dataset contains 888 low-dose lung CTs, and LIDC-IDRI contains 1,018 low-dose lung CTs. Note that LUNA16 dataset removes the annotated nodules of size smaller than 3mm.
For nodule classiﬁcation, we extract nodule annotations from LIDC-IDRI dataset, ﬁnd the mapping of different doc- tors’ nodule annotations with the LUNA16’s nodule annota- tions, and obtained the ground truth of nodule diagnosis by averaging different doctors’ diagnosis (discarding 0 score for diagnosis which corresponds to N/A.).
If the ﬁnal av- erage score is equal to 3 (uncertain about malignant or be- nign), we remove the nodule.
For the nodules with score greater than 3, we label them as positive.
Otherwise, we label them as negative.
Because CT slides were annotated by anonymous doctors, the identities of doctors (referred to as Drs 1-4 as the 1st-4th annotations) are not strictly con- sistent.
As such, we refer them as “simulated” doctors.
To make our results reproducible, we only keep the CTs within LUNA16 dataset, and use the same cross validation split as LUNA16 for classiﬁcation.
4.2. Preprocessing Three automated preprocessing steps are employed for the input CT images.
First, we clip the raw data into [−1200, 600].
Second, we transform the range linearly into [0, 1].
Finally, we use LUNA16’s given segmentation ground truth and remove the background.
Figure 5.
The 3D Faster R-CNN network with 3D residual blocks.
It contains several 3D residual blocks.
We employ a deep 3D resid- ual network of 18 layers as the encoder subnetwork, which is an extension from 2D Res18 net [11].
4.3. DeepLung for Nodule Detection We train and evaluate the detector on LUNA16 dataset following 10-fold cross validation with given patient-level split.
In training, we augment the dataset by randomly ﬂip- ping the image and use cropping scale betweeb 0.75 to 1.25.
The evaluation metric, FROC, is the average recall rate at the average number of false positives at 0.125, 0.25, 0.5, 1, 2, 4, 8 per scan, which is the ofﬁcial evaluation metric for LUNA16 dataset [24].
In the test phase, we use detec- tion probability threshold as -2 (before sigmoid function), followed by NMS with IoU threshold as 0.1. To validate the performance of proposed deep 3D dual path network for detection, we employ a deep 3D residual network as a comparison in Fig.
5.
The encoder part of this baseline network is a deep 3D residual network of 18 layers, which is an extension from 2D Res18 net [11].
Note that the 3D Res18 Faster R-CNN contains 5.4M trainable parameters, while the 3D DPN26 Faster R-CNN employs 1.4M trainable parameters, which is only 1 4 of 3D Res18 Faster R-CNN.
The FROC performance on LUNA16 is visualized in Fig.
6.
The solid line is interpolated FROC based on true prediction.
The 3D DPN26 Faster R-CNN achieves a FROC score of 84.2% without any false positive nodule reduc- tion stage, which is better than the previous 83.9% using two-stage training [6].
The 3D DPN26 Faster R-CNN us- ing only 1 4 of the parameters performs better than the 3D Res18 Faster R-CNN, which demonstrates the superior suit- ability of the 3D DPN for detection.
Ding et al.
obtains 89.1% FROC using 2D Faster R-CNN followed by extra false positive reduction classiﬁer [5], while we only employ enhanced Faster R-CNN with deep 3D dual path for detec- tion.
We have recently applied the 3D model to Alibaba Tianchi Medical AI on nodule detection challenge and were able to achieve top accuracy on a hold-out dataset.
4.4. DeepLung for Nodule Classiﬁcation We validate the nodule classiﬁcation performance of the DeepLung system on the LIDC-IDRI dataset with the LUNA16’s split principle, 10-fold patient-level cross vali- Table 2.
Nodule-level diagnosis accuracy (%) between nodule classiﬁcation subnetwork in DeepLung and experienced doctors on doctor’s individually conﬁdent nodules.
Dr 3 91.82 93.19 Dr 4 Average 91.25 86.03 92.74 90.89 Doctors DeepLung Dr 1 93.44 93.55 Dr 2 93.69 93.30 Table 3.
Statistical property of predicted malignant probability for borderline nodules (%) Prediction < 0.1 or > 0.9 Frequency 64.98 < 0.2 or > 0.8 80.14 < 0.3 or > 0.7 89.75 < 0.4 or > 0.6 94.80 Figure 6.
Sensitivity (Recall) rate with respect to false positives per scan.
The FROC (average recall rate at the false positives as 0.125, 0.25, 0.5, 1, 2, 4, 8) of 3D Res18 Faster R-CNN is 83.4%, while the FROC of 3D DPN26 Faster R-CNN is 84.2% with only 1 4 of the parameters as 3D Res18 Faster R-CNN.
The 3D Res18 Faster R-CNN has a total recall rate 94.6% for all the detected nodules, while 3D DPN26 Faster R-CNN has a recall rate 95.8%.
Table 1.
Nodule classiﬁcation comparisons on LIDC-IDRI dataset.
Models Multi-scale CNN [26] Slice-level 2D CNN [33] Nodule-level 2D CNN [33] Vanilla 3D CNN [33] Multi-crop CNN [27] Deep 3D DPN Nodule Size+Pixel+GBM All feat.+GBM Accuracy (%) Year 2015 2016 2016 2016 2017 2017 2017 2017 86.84 86.70 87.30 87.40 87.14 88.74 86.12 90.44 dation.
There are 1,004 nodules of which 450 are positive.
In the training, we ﬁrst pad the nodules of size 32× 32× 32 into 36 × 36 × 36, randomly crop 32 × 32 × 32 from the padded data, horizontal ﬂip, vertical ﬂip, z-axis ﬂip the data for augmentation, randomly set 4× 4× 4 patch to zero, and normalize the data with the mean and standard deviation obtained from training data.
The total number of epochs is 1,050.
The initial learning rate is 0.01, and reduce to 0.001 after epoch 525, and ﬁnally to 0.0001 after epoch 840.
Due to time and resource limitation for training, we use the fold 1, 2, 3, 4, 5 for test, and the ﬁnal performance is the average performance on the ﬁve test folds.
The nodule classiﬁcation performance is concluded in Table 1.
From the table 1, our deep 3D DPN achieves better per- formance than those of Multi-scale CNN [26], Vanilla 3D CNN [33] and Multi-crop CNN [27], because of the strong power of 3D structure and deep dual path network.
GBM with nodule size and raw nodule pixels with crop size as 16 × 16 × 16 achieves comparable performance as multi- scale CNN [26] because of the superior classiﬁcation per- formance of GBM.
Finally, we construct feature with deep 3D dual path network features, 3D Faster R-CNN detected nodule size and raw nodule pixels, and obtain 90.44% ac- curacy, which shows the effectiveness of deep 3D dual path network features.
4.4.1 Compared with Experienced Doctors on Their Individual Conﬁdent Nodules We compare our predictions with those of four “simulated” experienced doctors on their individually conﬁdent nodules (with individual score not 3).
Note that about 1/3 annota- tions are 3.
Comparison results are concluded in Table 2.
From Table 2, these doctors’ conﬁdent nodules are easy to be diagnosed nodules from the performance comparison between our model’s performances in Table 1 and Table 2.
To our surprise, the average performance of our model is 1.5% better than that of experienced doctors even on their individually conﬁdent diagnosed nodules.
In fact, our model’s performance is better than 3 out of 4 doctors (doc- tor 1, 3, 4) on the conﬁdent nodule diagnosis task.
The re- sult validates deep network surpasses human-level perfor- mance for image classiﬁcation [11], and the DeepLung is better suited for nodule diagnosis than experienced doctors.
We also employ Kappa coefﬁcient, which is a common approach to evaluate the agreement between two raters, to test the agreement between DeepLung and the ground truth [16].
The kappa coefﬁcient of DeepLung is 85.07%, which is signiﬁcantly better than the average kappa coefﬁcient of doctors (81.58%).
To evaluate the performance for all nod- ules including borderline nodules (labeled as 3, uncertain between malignant and benign), we compute the log likeli- hood (LL) scores of DeepLung and doctors’ diagnosis.
We randomly sample 100 times from the experienced doctors’ annotations as 100 “simulated” doctors.
The mean LL of doctors is -2.563 with a standard deviation of 0.23.
By contrast, the LL of DeepLung is -1.515, showing that the performance of DeepLung is 4.48 standard deviation better than the average performance of doctors, which is highly statistically signiﬁcant.
It is important to analysis the sta- Table 4.
Comparison between DeepLung’s nodule classiﬁcation on all detected nodules and doctors on all nodules.
Method Acc.
(%) TP Set 81.42 FP Set 97.02 Doctors 74.05-82.67 Table 5.
Patient-level diagnosis accuracy(%) between DeepLung and experienced doctors on doctor’s individually conﬁdent CTs. Dr 4 Average 82.31 77.80 84.28 81.41 Doctors DeepLung Dr 1 83.03 81.82 Dr 2 85.65 80.69 Dr 3 82.75 78.86 tistical property of predictions for borderline nodules that cannot be conclusively classiﬁed by doctors.
Interestingly, 64.98% of the borderline nodules are classiﬁed to be either malignant (with probability > 0.9) or benign (with proba- bility < 0.1) in Table 3.
DeepLung classiﬁed most of the borderline nodules of malignant probabilities closer to zero or closer to one, showing its potential as a tool for assisted diagnosis.
4.5. DeepLung for Fully Automated Lung CT Can- cer Diagnosis We also validate the DeepLung for fully automated lung CT cancer diagnosis on the LIDC-IDRI dataset with the same protocol as LUNA16’s patient-level split.
Firstly, we employ our 3D Faster R-CNN to detect suspicious nodules.
Then we retrain the model from nodule classiﬁcation model on the detected nodules dataset.
If the center of detected nodule is within the ground truth positive nodule, it is a pos- itive nodule.
Otherwise, it is a negative nodule.
Through this mapping from the detected nodule and ground truth nodule, we can evaluate the performance and compare it with the performance of experienced doctors.
We adopt the test fold 1, 2, 3, 4, 5 to validate the performance the same as that for nodule classiﬁcation.
Different from pure nodule classiﬁcation, the fully auto- mated lung CT nodule diagnosis relies on nodule detection.
We evaluate the performance of DeepLung on the detec- tion true positive (TP) set and detection false positive (FP) set individually in Table 4.
If the detected nodule of center within one of ground truth nodule regions, it is in the TP set.
If the detected nodule of center out of any ground truth nodule regions, it is in FP set.
From Table 4, the DeepLung system using detected nodule region obtains 81.42% accu- racy for all the detected TP nodules.
Note that the expe- rienced doctors obtain 78.36% accuracy for all the nodule diagnosis on average.
The DeepLung system with fully au- tomated lung CT nodule diagnosis still achieves above av- erage performance of experienced doctors.
On the FP set, our nodule classiﬁcation subnetwork in the DeepLung can reduce 97.02% FP detected nodules, which guarantees that our fully automated system is effective for the lung CT can- cer diagnosis.
4.5.1 Compared with Experienced Doctors on Their Individually Conﬁdent CTs We employ the DeepLung for patient-level diagnosis fur- ther.
If the current CT has one nodule that is classiﬁed as positive, the diagnosis of the CT is positive.
If all the nod- ules are classiﬁed as negative for the CT, the diagnosis of the CT is negative.
We evaluate the DeepLung on the doc- tors’ individually conﬁdent CTs for benchmark comparison in Table 5.
From Table 5, DeepLung achieves 81.41% patient-level diagnosis accuracy.
This is 99% of the average perfor- mance of four experienced doctors and better than Dr 4 alto- gether.
This performance gives conﬁdence that DeepLung can be a useful tool to assist doctors’ in their diagon- sis.
We further validate our method against the four doc- tors’ individual conﬁdential CTs. The Kappa coefﬁcient of DeepLung is 63.02%, while the average Kappa coefﬁ- cient of the doctors is 64.46%.
It implies the predictions of DeepLung are of good agreement with ground truths for patient-level diagnosis, and are comparable with those of experienced doctors.
5.
Discussion In this section, we will argue the utility of DeepLung by visualizing the nodule detection and classiﬁcation results.
5.1. Nodule Detection We randomly pick nodules from test fold 1 and visual- ize them in red circles in the ﬁrst row of Fig.
7.
Detected nodules are visualized in blue circles of the second row.
Be- cause CT is 3D voxel data, we can only plot the central slice for visualization.
The third row shows the detection prob- abilities for the detected nodules.
The central slice number is shown below each slice.
The diameter of the circle is relative to the nodule size.
From the central slice visualizations in Fig.
7, we ob- serve the detected nodule positions including central slice numbers are consistent with those of ground truth nodules.
The circle sizes are similar between the nodules in the ﬁrst row and the second row.
The detection probability is also very high for these nodules in the third row.
It shows 3D Faster R-CNN works well to detect the nodules from test fold 1.
5.2. Nodule Classiﬁcation We also visualize the nodule classiﬁcation results from test fold 1 in Fig.
8.
We choose nodules that is predicted right, but annotated incorrectly by some doctors.
The ﬁrst seven nodules are benign nodules, and the remaining nod- ules are malignant nodules.
The numbers below the ﬁg- Figure 7.
Visualization of central slices for nodule ground truths and detection results.
We randomly choose nodules (red circle boxes in the ﬁrst row) from test fold 1.
Detection results are shown in the blue circles of second row.
The center slice numbers are shown below the images.
The last row shows detection probability.
The DeepLung performs well for nodule detection.
ures are the DeepLung predicted malignant probabilities followed by which annotation of doctors is wrong.
For the DeepLung, if the probability is larger than 0.5, it predicts malignant.
Otherwise, it predicts benign.
For an experi- enced doctor, if a nodule is large and has irregular shape, it has a high probability to be a malignant nodule.
Figure 8.
Visualization of central slices for nodule classiﬁcation results on test fold 1.
We choose nodules that are predicted right by the DeepLung, but annotated incorrectly by some doctors.
The numbers below the nodules are model predicted malignant proba- bilities followed by which annotation of doctors is wrong.
The ﬁrst seven nodules are benign nodules.
The rest nodules are malignant nodules.
The DeepLung performs well for nodule classiﬁcation.
From Fig.
8, we can observe that doctors mis-diagnose some nodules.
The reason may be be that humans are not ﬁt to process 3D CT data which are of low signal to noise ra- tio.
Perhaps some doctors cannot ﬁnd some weak irregular boundaries or erroraneously consider some normal tissues as nodule boundaries leading to false negatives or false pos- itives.
In addition, doctors’ own internal bias may play a role in how conﬁdent he/she predicts these scans while be- ing limited to observing only one slice at a time.
Machine learning-based methods can overcome these limitations and are able to learn complicated rules and high dimensional features while utilizing all input slices at once without much problem.
From this perspective, DeepLung can potentially be of great use to doctors in their effort to make consistent and accurage diagonsis.
6.
Conclusion In this work, we propose a fully automated lung CT can- cer diagnosis system based on deep learning.
DeepLung consists of two parts, nodule detection and classiﬁcation.
To fully exploit 3D CT images, we propose two deep 3D convolutional networks based on 3D dual path networks, which is more compact and can yield better performance than residual networks.
For nodule detection, we design a 3D Faster R-CNN with 3D dual path blocks and a U-net- like encoder-decoder structure to detect candidate nodules.
The detected nodules are subsequently fed to nodule clas- siﬁcation network.
We use a deep 3D dual path network to extract classiﬁcation features.
Finally, gradient boost- ing machine with combined features are trained to classify candidate nodules into benign or malignant.
Extensive ex- perimental results on public available large-scale datasets, LUNA16 and LIDC-IDRI datasets, demonstrate the supe- rior performance of the DeepLung system.
References [1] H.
J.
Aerts et al.
Decoding tumour phenotype by noninva- sive imaging using a quantitative radiomics approach.
Na- ture communications, 5, 2014.
[2] S.
G.
Armato et al.
The lung image database consortium (lidc) and image database resource initiative (idri): a com- pleted reference database of lung nodules on ct scans.
Medi- cal physics, 38(2):915–931, 2011.
[3] Y.
Chen, J.
Li, H.
Xiao, X.
Jin, S.
Yan, and J.
Feng.
Dual path networks.
In NIPS, 2017.
[4] J.
Deng, W.
Dong, R.
Socher, L.-J.
Li, K.
Li, and L.
Fei- Fei.
Imagenet: A large-scale hierarchical image database.
In CVPR.
IEEE, 2009.
[5] J.
Ding, A.
Li, Z.
Hu, and L.
Wang.
Accurate pulmonary nodule detection in computed tomography images using deep convolutional neural networks.
In MICCAI, 2017.
[6] Q.
Dou, H.
Chen, Y.
Jin, H.
Lin, J.
Qin, and P.-A.
Heng.
Automated pulmonary nodule detection via 3d convnets with online sample ﬁltering and hybrid-loss residual learning.
In MICCAI, 2017.
[7] A.
El-Baz et al.
3d shape analysis for early diagnosis of malignant lung nodules.
In IPMI, 2011.
[8] J.
H.
Friedman.
Greedy function approximation: a gradient boosting machine.
Annals of statistics, 2001.
[9] R.
Girshick.
Fast r-cnn.
In ICCV, pages 1440–1448, 2015.
[10] F.
Han, G.
Zhang, H.
Wang, B.
Song, H.
Lu, D.
Zhao, H.
Zhao, and Z.
Liang.
A texture feature analysis for di- agnosis of pulmonary nodules using lidc-idri database.
In Medical Imaging Physics and Engineering.
IEEE, 2013.
[11] K.
He, X.
Zhang, S.
Ren, and J.
Sun.
Deep residual learning for image recognition.
In CVPR, 2016.
[12] G.
Huang, Z.
Liu, K.
Q.
Weinberger, and L.
van der Maaten.
Densely connected convolutional networks.
In CVPR, 2017.
[13] J.
Huang et al.
Speed/accuracy trade-offs for modern convo- lutional object detectors.
In CVPR, 2017.
[14] S.
Hussein, K.
Cao, Q.
Song, and U.
Bagci.
Risk stratiﬁca- tion of lung nodules using 3d cnn-based multi-task learning.
In IPMI, 2017.
[15] C.
Jacobs et al.
Automatic detection of subsolid pulmonary nodules in thoracic computed tomography images.
Medical image analysis, 2014.
[16] J.
R.
Landis and G.
G.
Koch.
The measurement of observer agreement for categorical data.
biometrics, 1977.
[17] F.
Liao, M.
Liang, Z.
Li, X.
Hu, and S.
Song.
Evaluate the malignancy of pulmonary nodules using the 3d deep leaky noisy-or network.
arXiv preprint arXiv:1711.08324, 2017.
[18] J.
Long, E.
Shelhamer, and T.
Darrell.
Fully convolutional networks for semantic segmentation.
In CVPR, 2015.
[19] E.
Lopez Torres et al.
Large scale validation of the m5l lung cad on heterogeneous ct datasets.
Medical physics, 2015.
[20] K.
Murphy et al.
A large-scale evaluation of automatic pul- monary nodule detection in chest ct using local image fea- tures and k-nearest-neighbour classiﬁcation.
Medical image analysis, 2009.
[21] S.
Ren, K.
He, R.
Girshick, and J.
Sun.
Faster r-cnn: Towards real-time object detection with region proposal networks.
In NIPS, 2015.
[22] O.
Ronneberger, P.
Fischer, and T.
Brox.
U-net: Convolu- tional networks for biomedical image segmentation.
In MIC- CAI, 2015.
[23] A.
A.
A.
Setio et al.
Pulmonary nodule detection in ct im- ages: false positive reduction using multi-view convolutional networks.
IEEE TMI, 2016.
[24] A.
A.
A.
Setio et al.
Validation, comparison, and combina- tion of algorithms for automatic detection of pulmonary nod- ules in computed tomography images: the luna16 challenge.
Medical Image Analysis, 2017.
[25] W.
Shen, M.
Zhou, F.
Yang, D.
Dong, C.
Yang, Y.
Zang, and J.
Tian.
Learning from experts: Developing transferable deep features for patient-level lung cancer prediction.
In MICCAI, 2016.
[26] W.
Shen, M.
Zhou, F.
Yang, C.
Yang, and J.
Tian.
Multi-scale convolutional neural networks for lung nodule classiﬁcation.
In IPMI, 2015.
[27] W.
Shen, M.
Zhou, F.
Yang, D.
Yu, D.
Dong, C.
Yang, Y.
Zang, and J.
Tian.
Multi-crop convolutional neural net- works for lung nodule malignancy suspiciousness classiﬁca- tion.
Pattern Recognition, 2017.
[28] K.
Suzuki, F.
Li, S.
Sone, and K.
Doi.
Computer-aided diag- nostic scheme for distinction between benign and malignant nodules in thoracic low-dose ct by use of massive training artiﬁcial neural network.
IEEE TMI, 2005.
[29] Z.
Wang et al.
Exploring ﬁsher vector and deep networks for action spotting.
In CVPRW, 2015.
[30] Z.
Wang et al.
Weakly supervised patchnets: Describing and aggregating local patches for scene recognition.
IEEE TIP, 2017.
[31] Z.
Wang et al.
Structed triplets learning with pos-tag guided attention for visual question answering.
In WACV, 2018.
[32] T.
W.
Way, L.
M.
Hadjiiski, B.
Sahiner, H.-P.
Chan, P.
N.
Cascade, E.
A.
Kazerooni, N.
Bogot, and C.
Zhou.
Computer-aided diagnosis of pulmonary nodules on ct scans: Segmentation and classiﬁcation using 3d active contours.
Medical Physics, 2006.
[33] X.
Yan, J.
Pang, H.
Qi, Y.
Zhu, C.
Bai, X.
Geng, M.
Liu, D.
Terzopoulos, and X.
Ding.
Classiﬁcation of lung nodule malignancy risk on computed tomography images using con- volutional neural network: A comparison between 2d and 3d strategies.
In ACCV, 2016.
[34] M.
D.
Zeiler, D.
Krishnan, G.
W.
Taylor, and R.
Fergus.
De- convolutional networks.
In CVPR.
IEEE, 2010.
[35] W.
Zhu et al.
Co-occurrence feature learning for skeleton based action recognition using regularized deep lstm net- works.
In AAAI, 2016.
[36] W.
Zhu, Q.
Lou, Y.
S.
Vang, and X.
Xie.
Deep multi-instance networks with sparse label assignment for whole mammo- gram classiﬁcation.
In MICCAI, 2017.
[37] W.
Zhu, X.
Xiang, T.
D.
T.
Tran, G.
D.
H.
Hager, and X.
Xie.
Adversarial deep structured nets for mass segmentation from mammograms.
IEEE International Symposium on Biomedi- cal Imaging, 2018.

Deep reinforcement learning is a general method that have been successful in solving complex control problems.
Mnih et al.
in [Mnih et al., 2015] combined Q learning with deep neural networks and proved to be successful in image based Atari games.
Policy gradient methods have been proved signiﬁcantly ef- ﬁcient in both continuous control problems ([Sutton et al., 1999], [Silver et al., 2014], [Heess et al., 2015]) and discrete control problems ([Silver et al., 2016], [Wang et al., 2016]).
Among policy gradient methods, actor-critic algorithms are at the heart of many signiﬁcant advances in reinforcement learn- ing ([Bhatnagar et al., 2009], [Degris et al., 2012], [Lillicrap et al., 2015], [Mnih et al., 2016]).
These algorithms estimate state-action value functions independently, and proved to be efﬁcient in policy optimization.
However, an enormous number of online simulation data is required for deep reinforcement learning.
Hence we attempt to learn from expert demonstrations and decrease the amount of online data required in deep reinforcement learning algo- rithms.
One of the representative method of learning from ex- pert demonstrations is inverse reinforcement learning.
Ng et al.
proposed the ﬁrst inverse reinforcement learning algo- rithm [Ng and Russell, 2000], which recovers reward function based on the assumption that the expert policy is the global optimal policy.
From recovered reward function, Abbeel et al.
are able to propose apprenticeship learning ([Abbeel and Ng, 2004]) to train a policy with expert demonstrations and a simulation environment that does not output reward.
Appren- ticeship learning inspired many similar algorithms ([Syed and Schapire, 2008], [Syed et al., 2008], [Piot et al., 2014], [Ho et al., 2016]), Ho et al.
[Ho and Ermon, 2016a] proposed a imitation learning method that merges inverse reinforce- ment learning and reinforcement learning, hence imitate the expert demonstrations with generative adversarial networks (GANs).
These algorithms proved successful in solving MDP\R ([Abbeel and Ng, 2004]).
However, MDP\R is different from original MDP since MDP\R environments do not output task based reward data.
And for this reason, inverse reinforce- ment based algorithms attempt to assume the expert demon- strations to be global optimal and imitate the expert demon- strations.
In order to learn from expert demonstrations for MDP, alongside with state-of-the-art reinforcement learning algorithms, different frameworks are required.
There are some prior work that attempt to make use of ex- pert demonstrations for reinforcement learning algorithms.
[Lakshminarayanan et al., 2016] Lakshminarayanan et al.
proposed a training method for DQN based on the assump- tion that expert demonstrations are global optimal, thus pre- train the state-action value function estimators.
Cruz Jr et al.
[Cruz Jr et al., 2017] focused on feature ex- tracting for high dimensional, especially image based simu- lation environments, and proposed a framework for discrete control problems that pretrains the neural networks with clas- siﬁcation tasks using supervised learning.
The purpose of this pretraining process is to speed up the training process by try- ing to extract features of high dimensional states.
However, this work is only suitable for image based, discrete action en- vironments, and ignored the fact that expert demonstrations perform better than current learned policies.
The ﬁrst published version of AlphaGo [Silver et al., 2016] is one of the most important work that pretrains the neural networks with human expert demonstrations.
In this work, a policy network and a value network is used.
The value net- work is trained with on-policy reinforcement learning, and the policy network is pretrained with expert demonstrations using supervised learning, then trained with policy gradient.
This work and [Cruz Jr et al., 2017] are quite similar, the role of expert demonstrations is to speed up the feature extraction, and to give policy a warm start.
The fact that expert demon- strations perform better is not fully used, and the framework is not extensive enough for other problems and other rein- forcement learning algorithms.
In this paper, we propose an extensive framework that pre- trains actor-critic reinforcement learning algorithms with ex- pert demonstrations, and use expert demonstrations for both policy functions and value estimators.
We theoretically derive a method for computing policy gradient and value estimators with only expert demonstrations.
Experiments show that our method improves the performance of baseline algorithms on both continuous control environments and high-dimensional- state discrete control environments.
2 Background and Preliminaries In this paper, we deal with an inﬁnite-horizon discounted Markov Decision Process (MDP), which is deﬁned by the tu- ple {S, A, P, r, ρ0, γ}.
In the tuple, S is a ﬁnite set of states, A is a ﬁnite set of actions, P : S × A × S → R is the transition probability distribution, r : S → R is the reward function, ρ0 : S → R is the probability distribution of initial state S0, and γ ∈ (0, 1) is the discount factor.
A stochastic policy πs : S × A → R returns the probabil- ity distribution of actions based on states, and a deterministic policy πd : S → A returns the action based on states.
In this paper, we deal with both stochastic policies and deterministic policies, and a ∼ π(s) means a ∼ πs(a|s) or a = πd(s) respectively.
Thus the state-action value function Qπis: Qπ(st, at) = Est+1,at+1,...
γτ r(st+τ ) The deﬁnitions of the value function V π and the advantage function Aπ are: (cid:34) ∞(cid:88) (cid:34) ∞(cid:88) τ =0 (cid:35) (cid:35) V π(st) = Eat,st+1,...
γτ r(st+τ ) Aπ(st, at) = Qπ(st, at) − V π(st) τ =0 And let η(π) denote the discounted reward of π: (cid:35) (cid:34) ∞(cid:88) t=0 η(π) = Es0,a0,...
γtr(st) For future convenience, let dπ(s) denote the limiting dis- tribution of states: dπ(s) = lim t→∞ P r(st = s) where in all of the deﬁnitions above: s0 ∼ ρ0(s0), at ∼ π(st), st+1 ∼ P (st+1|st, at) The goal of actor-critic reinforcement learning algorithms is to maximize the discounted reward, η(π), to obtain the op- timal policy, where we use a parameterized policy πθ.
While estimating η(π) or ∇θη(πθ) based on simulated samples, many algorithms use a state-action value estimator Qw, to estimate the state-value function Qπ for policy function πθ.
One typical deterministic actor-critic algorithm DDPG (Deep Deterministic Policy Gradient) [Lillicrap et al., 2015] uses estimator Qw = ˆQπ to estimate the gradient of an off-policy deterministic discounted reward ηβ(πθ) = s∈S dβ(s)V π(s) [Degris et al., 2012], where β is the roll- (cid:80) out policy: ∇θηβ(π) ≈ Est∼dβ (s) = Est∼dβ (s) (cid:2)∇θQw(s, a)|s=st,a∼πθ(st) (cid:3) (cid:2)∇aQw(s, a)|s=st,a∼πθ(st)π(cid:48) (cid:3) Where Qw is updated with sampled data from π using Bell- man equation, π(cid:48) θ = ∇θπθ(s)|s=st.
Another off-policy algorithm that has Qw as an estimator of policy πθ is ACER (Actor-Critic with Experience Replay) [Wang et al., 2016] that optimizes stochastic policy.
The algorithm maximizes off-policy deterministic discounted re- ward ηβ(πθ) as well, and modiﬁes the off-policy policy gra- dient ˆgacer = ∇θηβ(π) to: ˆgacer = ¯ρt∇θ log πθ(at|st)(cid:2)Qret(st, at) − V w(st)(cid:3) (cid:110) ∇θ log πθ(a|st)Aw(st, a) + Ea∼πθ(st) Where Aw(st, a) = Qw(st, a) − V w(st), ¯ρt = ; [x]+ = x if x > 0 and is zero otherwise; min β(a,st) ; st ∼ dβ(st) V w(st) = Ea∼πθ(st)(st, a); ρt(a) = π(a,st) and at ∼ β(st); Qret is the Retrace estimator of Qπ [Munos et al., 2016], which can be expressed recursively as follows: c, π(at,st) β(at,st) (cid:21) (cid:32)(cid:20) ρt(a) − c (cid:111) ρt(a) (cid:33) Qret(st, at) = rt + γ ¯ρt+1δQ(st+1, at+1) + γV w(s+1) where δQ(st+1, at+1) = Qret(st+1, at+1) − Qw(st+1, at+1) In ACER, state-action value function is updated using Qret as target, with gradient gQ: gQ = (Qret(st, at) − Qw(st, at))∇wQw(st, at) In this paper, we will apply our methods with expert demonstrations to DDPG and ACER.
3 Expert Based Pretraining Methods Suppose there exists an expert policy π∗ that performs better than π.
We deﬁne perform better with the following straight- forward constraint: η(π∗) (cid:62) η(π) (1) The deﬁnition of perform better above is based on the fact that the goal of actor-critic RL algorithms is to maximize η(π).
Here the expert policy π∗ is different from that of IRL [Ng and Russell, 2000], imitation learning [Ho and Ermon, 2016b] or LfD [Hester et al., 2017], since π∗ here is not the optimum policy of the MDPs. of (s, a) pairs, {(st, at)}t=0,1,2,..., sampled from π.
Here we deﬁne a demonstration of a policy π as a sequence Actor-critic RL algorithms tend to optimize η(πθ) as the target.
Thus pretraining procedures for these algorithms need to estimate η(πθ) as the optimization target using expert demonstrations.
Also, from deﬁnition (1), we need to esti- mate η(π∗) as well.
However, With only demonstrations of expert policy π∗ and a black-box simulation environment, η(π∗) and η(πθ) cannot be directly estimated.
Hence we introduce Theorem 1 (see [Schulman et al., 2015] and [Kakade and Langford, 2002]).
Theorem 1.
For two policies π and π∗: η(π∗) − η(π) = Es∗ 0 ,a∗ 0 ,...∼π∗ γtAπ(s∗ t , a∗ t ) (2) (cid:35) Proof.
(See also [Schulman et al., 2015] and [Kakade and Langford, 2002]) Note that Aπ(s, a) = Es(cid:48)∼P (s(cid:48)|s,a) [r(s) + γV π(s(cid:48)) − V π(s)] (cid:34) ∞(cid:88) t=0 (cid:35) we have: Es∗ 0 ,a∗ 0 ,...∼π∗ =Es∗ 0 ,a∗ 0 ,...∼π∗ (cid:34) ∞(cid:88) (cid:34) ∞(cid:88) t=0 γtAπ(s∗ t , a∗ t ) γt(r(st) + γV π(st+1) − V π(st)) (cid:35) (cid:35) (cid:34) ∞(cid:88) t=0 γtr(st) t=0 0∼ρ0 [V π(s0)] + Es∗ 0 ,a∗ 0 ,...∼π∗ = − Es∗ = − η(π) + η(π∗) For many actor-critic RL algorithms like DDPG and ACER, policy optimization is based on accurate estima- tions of state-action value functions or value functions of the learned policy πθ.
Typically, those algorithms use data sam- pled from πθ, {(st, at, rt)}t=0,1,2,..., to estimate Qπ and V π.
The estimating processes usually need a large amount of sim- ulations to be accurate enough.
Combine Theorem 1 with constraint (1), we have: (cid:34) ∞(cid:88) t=0 (cid:35) Es∗ 0 ,a∗ 0 ,...∼π∗ γtAπ(s∗ t , a∗ t ) (cid:62) 0 (3) This result links state-action value functions with expert demonstrations, allowing us to apply constraint (1) while training state-action value functions.
This constraint is for value estimators, like Qw and V w.
When value estimators are not accurate enough, constraint (3) would not be satis- ﬁed.
Hence if an algorithm update value estimators under constraint (3), the estimators would be more accurate, and in result improve the policy optimizing process.
Another pretraining process is policy optimization using expert demonstrations.
Like most actor-critic algorithms, we suppose advantage function Aπ(s, a) is already known while conducting policy optimization.
Then we can estimate the up- date step with expert demonstrations and estimations of value functions.
Considering Theorem 1, we estimate he policy gradient as the following: ∇θη(πθ) =∇θ(η(πθ) − η(π∗)) = − ∇θEs∗ 0 ,...∼π∗ 0 ,a∗ (cid:34) ∞(cid:88) (cid:35) γtAπ(s∗ t , a∗ t ) (4) t=0 Equation (4) provides an off-policy policy optimization procedure with data only from expert demonstrations.
It turns out that perform better is not a must in this procedure for ex- pert policy π∗.
Recently, people like to propose sample efﬁcient RL algo- rithms, like ACER and Q-Prop [Gu et al., 2017], since RL al- gorithms need a large amount of simulation time while train- ing.
With expert demonstrations, since there is no reward data, we cannot conduct sample efﬁcient policy optimization processes.
However, when we update policies with (4), no simulation time is needed.
We call the situation simulation ef- ﬁcient, which means the algorithms may need a large amount of data, but need few simulation data while training.
Note that sample efﬁcient algorithms are all simulation ef- ﬁcient algorithms, all of these methods intend to decrease the simulation time.
In this paper, we evaluate our method by how simulation efﬁcient it is.
In this section, we found two pretraining methods for actor- critic RL algorithms, namely (3) and (4).
Both of them are based on Theorem 1.
The theorem connects policy dis- counted reward η(πθ) and expert demonstration data, requir- ing no reward data from expert trajectories.
Equation (3) gives a constraint of value function estimators based on the deﬁnition of perform better, and equation (4) provides an off- policy method to optimize policy function regardless of how expert demonstrations perform.
4 Algorithms with Expert Demonstrations Theorem 1 provides a way to satisfy constraint (1) and update policies πθ with demonstrations of expert policy π∗, and does not need reward data sampled from π∗.
In this section, we organize the results in Section 3 in a more piratical way, then we apply the pretraining methods to two of the typical actor- critic RL algorithms, DDPG and ACER.
These actor-critic RL algorithms use neural networks Qw(s, a) to estimate the state-action value functions of pol- icy, Qπ(s, a), where π is the is the current learned policy while training, which is a parameterized function, πθ, always in the form of artiﬁcial neural networks.
For pretraining processes based on Theorem 1, we need an estimator of advantage function for policy πθ, Aπ(s, a).
Based on parameterized policy and state-action value func- tion estimator Qw, we obtain the advantage function estima- tor Aw,θ: Aw,θ(s∗ t , a∗ V w,θ(s∗ t , a∗ t ) = Qw(s∗ t ) = Ea∼πθ(s)Qw(s∗ t ) − V w,θ(s∗ t ) t , a) (5) (6) Figure 1: Example screenshots of MuJoCo simulation environments that we attend to experiment on with DDPG as baseline.
The tasks are: HalfCheetah (left), Hopper (middle), and Walker2d (right).
Considering the training processes of DDPG and ACER, at the beginning of the processes the policies are nearly random and estimators Qw(s, a) are not accurate, since there is little data from simulation.
Therefore if there exist some expert demonstrations that perform better than initial policies, we can introduce the data using constraint (3), in order to obtain a more accurate estimator Qw(s, a).
then Qw(s, a) is accurate enough for the fact that π∗ performs better.
Hence we update the estimator with expert demonstrations with the following gradient, in which [x]+ = x if x > 0, otherwise is zero: If constraint (3) is satisﬁed, (cid:34) Q = ∇w g∗ Es∗ 0 ,a∗ 0 ,...∼π∗ γtAw,θ(s∗ t , a∗ t ) (cid:34) ∞(cid:88) t=0 (cid:34) ∞(cid:88) (cid:35)(cid:35) (7) (cid:35) From equation (4), we optimize policy with expert demon- strations.
Since expert demonstrations do not contain reward data, we can update policy parameters with a simple policy gradient: π = −∇θEs∗ g∗ 0 ,a∗ 0 ,...∼π∗ γtAw,θ(s∗ t , a∗ t ) (8) t=0 For the reason that π∗ is not the optimal policy of the MDPs, we only train with expert demonstrations for a lim- ited period of time at the beginning of the training process, to guarantee π∗ performs better than πθ, hence we call the process pretraining.
ACER, we add gradients g∗ of the algorithms: To pretrain actor-critic RL algorithms like DDPG and π to the original gradients Q and g∗ Q = gQ + λQg∗ gpre π = gπ + λπg∗ gpre (9) (10) Q and gpre Where gQ and gπ are original gradients of baseline actor- critic RL algorithms, andgpre are pretraining gradi- ents for estimator Qw and parameterized policy function πθ respectively while pretraining.
We introduce expert demon- strations to the base algorithms instead of replacing them, since the state-action value functions are estimated with the baseline algorithms and gradient g∗ Q only makes Qw satisfy constraint (1).
4.1 Pretraining DDPG DDPG is a representative off-policy actor-critic deterministic RL algorithm.
The algorithm is for continuous action space MDPs, and optimizes the policy using off-policy policy gra- dient.
Two neural networks are used in DDPG at the same time.
One is named critic network, which is the state-action value function estimator Qw, and the other is named actor network, which is the parameterized policy πθ.
Since it is an algorithm for deterministic control, the input of the actor network is a state of MDPs, and the output is the corresponding action.
Two neural networks are trained simultaneously, with gra- dients gQ and gπ respectively.
gQ is based on Bellman equa- tion, and gπ is the off-policy policy gradient.
In order to introduce expert demonstrations for pretraining critic network and actor network, we apply (9) and (10) to pretrain the two neural networks.
Note that for a deterministic policy πθ, equation (6) be- comes V w,θ(s) = Qw(s, πθ(s)).
4.2 Pretraining ACER ACER is an off-policy actor-critic stochastic RL algorithm, which modiﬁes the policy gradient to make the process sam- ple efﬁcient.
ACER solves both discrete control problems and continuous control problems.
For discrete control problems, a double-output convolu- tional neural work (CNN) is used in ACER.
One output is a softmax policy πθ, and the other is Qw values.
Although θ and w share most of the parameters, they are updated sepa- rately with different gradients.
For stochastic control problems, a new structure named Stochastic Dueling Networks (SDNs) is used for value func- (cid:80)n tion estimation.
The network outputs a deterministic value es- timation V w(s), and a stochastic state-action value estimation Qw,θ(s, a) ∼ V w(s) + Aw(s, a) − 1 i=1 Aw(s, ˙a)| ˙a∼πθ.
t ) − Hence equation (5) becomes Aw,θ(s∗ t ) = Qw,θ(s∗ V w(s∗ t ).
t , a∗ t , a∗ In ACER, gradient gπ is the modiﬁed policy gradient, and gQ is based on Retrace.
Both of the gradients are explained in Section 2.
but in this paper, we compute pretraining gradients g∗ g∗ π directly with expert demonstrations.
Policy gradient is estimated using trust region in ACER, Q and Figure 2: Results of pretraining based on DDPG.
The ﬁgures each is a different task, and they are respectfully experimented on HalfCheetah (left), Hopper (middle) and Walker2d (right), The vertical dashed black lines represent the points when pretraining end, and the horizontal dashed brown lines represent the average episode reward of expert demonstrations.
The transparent blue and red lines are original training results, and the opaque lines are smoothed lines with sliding windows.
5 Experiments We test our algorithms based on DDPG and ACER on various environments, in order to investigate how simulation efﬁcient the pretraining methods are.
The baselines are DDPG and ACER without pretraining.
Because of the existence of [x]+, g∗ Q deﬁned in (7) could be inﬁnity sometimes.
Hence we clip the gradient during pre- training.
We set λQ and λπ = 1 in equations (9) and (10).
The expert policies that generate expert demonstrations are policies trained with baseline algorithms, i.e. DDPG and ACER.
With DDPG as baseline, we apply our algorithm to low dimensional simulation environments using the MuJoCo physics engine [Todorov et al., 2012], and test on tasks with action dimensionality are: HalfCheetah (6D), Hopper (3D), and Walker2d (6D).
These tasks are illustrated in Figure 1.
All the setups with DDPG as baseline share the same net- work architecture that compute policies and estimate value functions referring to [Lillicrap et al., 2015].
Adam [Kingma and Ba, 2014] is used for learning parameters and the learn- ing rate of actor network and critic network are respectively 10−3 and 10−4.
For critic network, L2 weight decay of 10−2 is used with γ = 0.99.
Both actor network and critic network have 2 hidden layers with 400 and 300 units respectively.
The results of our pretraining method based on DDPG are illustrated in Figure 2.
In the ﬁgures, the horizontal dashed brown lines represent the average episode reward of expert demonstrations.
It is obvious that the expert demonstrations are not global optimal demonstrations, and in order to guar- antee the expert policies perform better than learned policies, the pretraining process stops early with 30000 training steps and 60000 simulation steps.
As shown in Figure 2, it is obvious that DDPG with our pretraining method outperforms initial DDPG.
Results on HalfCheetah (Figure 2 left) is representative and clear, pre- training process gives training a warm start, and after pre- training stops, the performance drops because of the new learning gradient.
However, after pretraining, DDPG learns faster than the baseline, hence it outperforms initial DDPG.
Although the results of DDPG are unstable on Hopper (Fig- ure 2 middle) and Walker2d (Figure 2 right), smoothed results Figure 3: Example screenshots of Atari simulation environments that we attend to experiment on with ACER as baseline.
The tasks from left to right are: AirRaid, Breakout, Carnival, CrazyClimber and Gopher.
indicate that DDPG with pretraining processes learns faster than DDPG.
With ACER as baseline, we apply our algorithm to image based Atari games.
We only tested on discrete control prob- lems with ACER, and the environments we tested on are: Air- Raid, Breakout, Carnival, CrazyClimber and Gopher.
The en- vironments are illustrated in Figure 3.
The experiment settings are similar to [Wang et al., 2016], The double-output network consists of a convolutional layer with 32 8× 8 kernels with stride 4, a convolutional layer with 64 4 × 4 kernels with stride 2, a convolutional layer with 64 3 × 3 kernels with stride 1, followed by a fully connected layer with 512 units.
The network outputs a softmax policy and state-action value Q for every action.
Because of the limitation of memory, each thread of ACER only have a replay memory of 5000 frames, which is the only different setting from [Wang et al., 2016].
Entropy regular- ization with weight 0.001 is also adopted, and the discount factor γ = 0.99, importance weight truncation c = 10.
Trust region updating is used as described in [Wang et al., 2016], and all the settings of trust region update remain the same.
ACER without trust region update is not tested in this paper.
The results of our pretraining method based on ACER with trust region update is illustrated in Figure 4.
All of the envi- ronments are image based Atari games.
All the lines have the same meaning as Figure 2, and it is obvious that ACER with pretraining process outperforms initial ACER.
Unlike DDPG, the performance of learned policies does not fall after pretraining process ends.
This is because for 0246810Training Steps /1050100020003000400050006000RewardDDPGDDPG+Pretrain0246810Training Steps /10505001000150020002500300035000246810Training Steps /10501000200030004000Figure 4: Results of pretraining based on ACER with trust region update.
Similar to Figure 2, the vertical dashed black lines are the points when pretraining end, and the horizontal dashed brown lines are the average episode reward of expert demonstrations.
The transparent red and blue lines represent the original training results, and the opaque ones are smoothed results with sliding windows.
Q deﬁned in (7) is always zero, and g∗ stochastic discrete control, a random policy and a random state-action value estimator always satisﬁes constraint (1), hence g∗ π deﬁned in (8) is policy gradient based on expert demonstrations, similar to original gπ from baseline ACER, therefore the performance of learned policies does not fall after pretraining.
Note that learning with expert demonstrations use the same amount of simulation steps as baseline algorithms, our pre- training method is more simulation efﬁcient than baselines.
6 Conclusion In this work, we propose an extensive method that pretrains actor-critic reinforcement learning methods.
Based on The- orem 1, we design a method that takes advantage of expert demonstrations.
Our method does not rely on the global op- timal assumption of expert demonstrations, which is one of the key differences between our method and IRL algorithms.
Our method pretrains policy function and state-action value estimators simultaneously with gradients (9) and (10).
With experiments based on DDPG and ACER, we demonstrate that our method outperforms the raw RL algorithms.
One limitation of our framework is that it has to estimate the advantage function for expert demonstrations, and the framework is not suitable for algorithms like A3C [Mnih et al., 2016] and TRPO [Schulman et al., 2015] that only main- tain a value estimator V w(s).
On the other hand, the fact that expert demonstrations perform better is not considered during pretraining of policies (Equation (8)).
We left these extensions in our future work.
References [Abbeel and Ng, 2004] Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learn- ing.
In Proceedings of the twenty-ﬁrst international con- ference on Machine learning, page 1.
ACM, 2004.
[Bhatnagar et al., 2009] Shalabh Bhatnagar, Richard Sutton, Mohammad Ghavamzadeh, and Mark Lee.
Natural actor- critic algorithms.
Automatica, 45(11), 2009.
[Cruz Jr et al., 2017] Gabriel V Cruz Jr, Yunshu Du, and Matthew E Taylor.
Pre-training neural networks with human demonstrations for deep reinforcement learning.
arXiv preprint arXiv:1709.04083, 2017.
[Degris et al., 2012] Thomas Degris, Martha White, and Richard S Sutton.
Off-Policy Actor-Critic.pdf.
Icml, 2012.
[Gu et al., 2017] Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E Turner, and Sergey Levine.
Q- Prop : Sample-Efﬁcient Policy Gradient with An Off - Policy Critic.
ICLR, pages 1–13, 2017.
[Heess et al., 2015] Nicolas Heess, Gregory Wayne, David Silver, Tim Lillicrap, Tom Erez, and Yuval Tassa.
Learn- ing continuous control policies by stochastic value gradi- ents.
In Advances in Neural Information Processing Sys- tems, pages 2944–2952, 2015.
[Hester et al., 2017] Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Andrew 0.00.20.40.60.81.0Training Steps /107020406080100RewardAirRaidACER+pretrainACER0.00.20.40.60.81.0Training Steps /107051015202530Breakout0.00.20.40.60.81.0Training Steps /10710152025303540Carnival0.00.20.40.60.81.0Training Steps /1070255075100125150175RewardCrazyClimber0.00.20.40.60.81.0Training Steps /1070100200300400500Gopherence on Machine Learning and Knowledge Discovery in Databases, pages 549–564.
Springer, 2014.
[Schulman et al., 2015] John Schulman, Sergey Levine, Michael Jordan, and Pieter Abbeel.
Trust Region Policy Optimization.
Icml-2015, page 16, 2015.
[Silver et al., 2014] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms.
In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 387–395, 2014.
[Silver et al., 2016] David Silver, Aja Huang, Chris J Maddi- son, Arthur Guez, Laurent Sifre, George Van Den Driess- che, Julian Schrittwieser, Ioannis Antonoglou, Veda Pan- neershelvam, Marc Lanctot, et al.
Mastering the game of go with deep neural networks and tree search.
Nature, 529(7587):484–489, 2016.
[Sutton et al., 1999] Richard S.
Sutton, David Mcallester, Satinder Singh, and Yishay Mansour.
Policy Gradient Methods for Reinforcement Learning with Function Ap- proximation.
Advances in Neural Information Processing Systems 12, pages 1057–1063, 1999.
[Syed and Schapire, 2008] Umar Syed and Robert E Schapire.
A game-theoretic approach to apprenticeship In Advances in neural information processing learning.
systems, pages 1449–1456, 2008.
[Syed et al., 2008] Umar Syed, Michael Bowling, and Robert E Schapire.
Apprenticeship learning using linear In Proceedings of the 25th international programming.
conference on Machine learning, pages 1032–1039.
ACM, 2008.
[Todorov et al., 2012] Emanuel Todorov, Tom Erez, and Yu- val Tassa.
Mujoco: A physics engine for model-based control.
In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pages 5026–5033.
IEEE, 2012.
[Wang et al., 2016] Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, and Nando de Freitas.
Sample efﬁcient actor-critic with expe- rience replay.
arXiv preprint arXiv:1611.01224, 2016.
Sendonaris, Gabriel Dulac-Arnold, Ian Osband, John Aga- piou, et al.
Learning from demonstrations for real world reinforcement learning.
arXiv preprint arXiv:1704.03732, 2017.
[Ho and Ermon, 2016a] Jonathan Ho and Stefano Ermon.
In Advances Generative adversarial imitation learning.
in Neural Information Processing Systems, pages 4565– 4573, 2016.
[Ho and Ermon, 2016b] Jonathan Ho and Stefano Ermon.
Generative Adversarial Imitation Learning.
In Nips, pages 4565–4573, 2016.
[Ho et al., 2016] Jonathan Ho, Jayesh Gupta, and Stefano Ermon.
Model-free imitation learning with policy opti- mization.
In International Conference on Machine Learn- ing, pages 2760–2769, 2016.
[Kakade and Langford, 2002] Sham Kakade and John Lang- ford.
Approximately Optimal Approximate Reinforce- ment Learning.
Proceedings of the 19th International Conference on Machine Learning, pages 267–274, 2002.
[Kingma and Ba, 2014] Diederik Kingma and Jimmy Ba. arXiv Adam: A method for stochastic optimization.
preprint arXiv:1412.6980, 2014.
[Lakshminarayanan et al., 2016] Aravind Lakshmi- narayanan, Sherjil Ozair, and Yoshua Bengio.
Reinforce- ment Learning with Few Expert Demonstrations.
Neural Information Processing Systems - Workshop on Deep Learning for Action and Interaction, 2016.
[Lillicrap et al., 2015] Timothy P.
Lillicrap, Jonathan J.
Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yu- val Tassa, David Silver, and Daan Wierstra.
Continuous control with deep reinforcement learning.
arXiv preprint arXiv:1509.02971, pages 1–14, 2015.
[Mnih et al., 2015] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle- mare, Alex Graves, Martin Riedmiller, Andreas K Fidje- land, Georg Ostrovski, et al.
Human-level control through deep reinforcement learning.
Nature, 518(7540):529–533, 2015.
[Mnih et al., 2016] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
Asyn- chronous methods for deep reinforcement learning.
In International Conference on Machine Learning, pages 1928–1937, 2016.
[Munos et al., 2016] R´emi Munos, Tom Stepleton, Anna Harutyunyan, and Marc G.
Bellemare.
Safe and Efﬁcient Off-Policy Reinforcement Learning.
arXiv, (Nips), 2016.
[Ng and Russell, 2000] Andrew Ng and Stuart Russell.
Al- gorithms for inverse reinforcement learning.
Proceedings of the Seventeenth International Conference on Machine Learning, 0:663–670, 2000.
[Piot et al., 2014] Bilal Piot, Matthieu Geist, and Olivier Pietquin.
Boosted bellman residual minimization han- In Joint European Confer- dling expert demonstrations.

Imagine we are interested in setting up a classiﬁcation program for deciding whether a given word belongs to a certain language.
This can be seen as a mission in supervised machine learning, where the machine experiences labeled data about the target language.
The label is 1 if the datum is contained in the language and 0 otherwise.
The machines task is to infer some rule in order to generate words in the language of interest and thereby generalize from the training samples.
Inductive Inference provides a model and diﬀerent performance measures, allowing to abstract from likewise important questions concerning details of the implemantation and instead focuses on what general properties of the learning process can be achieved.
More formally, according to Gold [1967] the learner is modelled by a com- putable function, successively receiving sequences incorporating more and more data.
Thereby, it possibly updates the current description of the target language (his hypothesis).
Learning is considered successful, if after some ﬁnite time the 2 Martin Aschenbach, Timo K¨otzing, and Karen Seidel learner’s hypotheses yield good enough approximations to the target language.
The original and most common learning success criterion is called explanatory (Ex-)learning or (Lim-)learning in the limit and additionally requires that the learner ﬁnally settles on exactly one correct hypothesis, which precisely recog- nizes the words in the language to be learned.
Later on, allowing a vacillation between ﬁnitely or inﬁnitely many descriptions and admitting for ﬁnitely many anomalies, i.e., deviations between the described language and target language, have been considered, see for example Case [2016].
Analyzing diﬀerent measures of learning success answers natural questions re- garding the learning process: Will learning power reduce if only inconsistent hypotheses are allowed to be changed?
Can learning proceed strongly mono- tonically, that is, every hypothesis is a subset of all later hypotheses?
Are we successful on more collections of languages, in case we only demand monotonoc- ity for the sets of correctly inferred words?
How does this relate to requiring a cautious learner, which never guesses supersets of later hypotheses?
Osherson, Stob, and Weinstein [1986] analyze several restrictions for learning from informant and mention that cautious learning, which forbids to ever con- jecture a strict subset of an earlier conjecture, is a restriction to learning power; we extend this statement with our Proposition 22.
Furthermore, they consider a version of conservativeness where mind changes (changes of the hypothesis) are only allowed if there is positive data contradict- ing the current hypothesis, which they claim to restrict learning power.
In this paper, we stick to the deﬁnition of B¯arzdi¸nˇs [1977] and Blum and Blum [1975], who deﬁne conservativeness for learning from informant such that mind changes are allowed also when there is negative data contradicting the current hypoth- esis, which is arguably the more natural deﬁnition in the case of learning from informants.
While the work of Lange and Zeugmann [1994] considers variously restricted learning of indexable families, i.e., sets of languages for which there is a uniform decision procedure, we also deal with arbitrary collections of recur- sively enumerable sets.
See also Lange, Zeugmann, and Zilles [2008] for a survey on learning indexable families.
For purely learning from positive information, so-called texts, there are en- tire maps displaying the pairwise relations of diﬀerent learning restrictions, see K¨otzing and Palenta [2014] and Jain, K¨otzing, Ma, and Stephan [2016].
With this paper we give an equally informative map for the case of learning from in- formant.
To derive this, in Section 3 normal forms and a regularity property for learning from informants are provided.
Especially, it is shown that learners can be assumed total and the presentation of the informant to be canonical, that is, presenting the data and their labels following the common well-ordering of N.
Already Gold [1967] was interested in these normal forms and proved that they can be assumed without loss of generality in the basic setting, whereas our re- sults apply to all so called delayable learning success criteria and may be helpful to generalize insights, that are so far bounded to Lim-learning.
In Section 4 we proceed to analyze the set of delayable learning restrictions, which share the common feature that hypotheses can be delayed without violat- Learning Families of Formal Languages from Informants ing the learning restriction (by contrast, a hypothesis which is consistent now might not be consistent later due to new data, so the restriction of consistency is not delayable).
We show that the promiment requirement of conservativeness does not restrict learning power, even when paired with (strong) decisiveness, re- quiring the learner not to return to an abandoned conjecture.
This is essential for understanding the relations between the diﬀerent delayable learning restictions, when Lim-learning from informants, since with it a number of other learning criteria are also non-restrictive.
The proof is an intricate simulation argument, combining diﬀerent techniques in the ﬁeld.
We complete the map of delayable learning restrictions by showing that the afore- mentioned learning restrictions of cautiousness, avoiding hypotheses for proper subsets of previously guesses, and of monotonicity, never removing correct data from a conjecture, are incomparable in terms of learning power.
This yields all pairwise relations of learning criteria, being displayed in Figure 1 and Figure 2.
In Section 5 we generalize another result by Gold [1967], namely Lim-learning from texts, where negative information is only implicit in the absence of certain data, to be harder than Lim-learning from informants.
Relations between these presentation modi have been investigated by Lange and Zeugmann [1993], who focussed on the interdependencies when considering diﬀerent learning restriction concerning monotonicity.
We show that their observation of strongly monotonic Lim-learnability from informants implying Lim-learnability from texts is bound to indexable families by providing a collection of recursive languages separating these learning criteria.
Learning from text has been extensively studied, including many learning success criteria and other variations, see Jain, Osherson, Royer, and Sharma [1999] or Case [2016].
The observation of every indexable family being Lim-learnable from informants and thus also behaviorally correct learnable, where convergence of the hypothe- ses only needs to be semantic, fails when arbitrary collections of recursively enu- merable sets are in consideration.
This follows from results of B¯arzdi¸nˇs [1974] and Case and Smith [1983], as observed in Section 2.
Further, in Section 6 we prove that it still holds in case all delayable semantic restrictions are required.
Additionally, a strict hierarchy by allowing for an increasing ﬁnite number of anomalies as well as not more learning power by allowing any vacillation between ﬁnitely many correct hypotheses, are provided in these two sections, respectively.
Finally, we adress open questions arising from the paper and other challenges related to our investigations.
Informant Learning: Restrictions and Success We let N denote the natural numbers including 0 and write 8 for an inﬁnite cardinality.
Moreover, for a function f we write dompfq for its domain and ranpfq for its range.
If we deal with (a subset of) a cartesian product, we are going to refer to the projection functions to the ﬁrst or second coordinate by pr1 and pr2, respectively.
For sets X, Y and a P N we write X “a Y , if X equals Y with a anomalies, i.e., |pXzY q Y pY zXq| ď a, where |.| denotes the 4 Martin Aschenbach, Timo K¨otzing, and Karen Seidel cardinality function.
In this spirit we write X “˚ Y , if there exists some a P N such that X “a Y , i.e., pXzY qYpY zXq is ﬁnite.
Further, Xăø denotes the ﬁnite sequences over X, i.e., all functions from some n P N to X and X ø stands for the countably inﬁnite sequences over X, i.e., all functions from N to X.
Additionally, Xďø :“ Xăø Y X ø denotes the set of all countably ﬁnite or inﬁnite sequences over X.
For every f P Xďø and t P N, we let frts :“ tps, fpsqq | s ă tu denote the restriction of f to t.
Finally, for sequences σ, τ P Xăø their concatenation is τ and we write σ Ď τ , if σ is an initial segment of τ , i.e., there is denoted by σ some t P N such that σ “ τrts.
In our setting, we typically have X “ N ˆ t0, 1u.
We denote the set of all partial functions f : dompfq Ď Nˆt0, 1u Ñ N and total functions f : Nˆt0, 1u Ñ N by P and R, respectively.
As far as possible, notation and terminology on the learning theoretic side fol- low Jain, Osherson, Royer, and Sharma [1999], whereas on the computability theoretic side we refer to Odifreddi [1999], Rogers [1967] and K¨otzing [2009].
(cid:97) 2.1 The Backbone of Learning Restrictions Now we come to introduce the basic deﬁnitions in Gold-Style learning theory important in this paper.
Let L Ď N.
If L is recursively enumerable, i.e., there is a (partial) computable function f such that dompfq “ L, we call L a language.
In case L is even recursive, i.e., its characteristic function is computable, we say it is a recursive language.
Moreover, we call L Ď PowpNq a collection of languages, if every L P L is a language.
If every L P L is a recursive language, L is called a collection of recursive languages.
In case there exists an enumeration tLξ | ξ P Ξu of L, where Ξ Ď N is recursive and a computable function f with ranpfq Ď t0, 1u such that x P Lξ ô fpx, ξq “ 1 for all ξ P Ξ and x P N, we say L is an indexable family of recursive languages.
Further, we ﬁx a programming system ϕ as introduced in Royer and Case [1994].
Brieﬂy, in the ϕ-system, for a natural number p, we denote by ϕp the par- tial computable function with program code p.
We also call p an index for Wp :“ dompϕpq.
In reference to a Blum complexity measure, for all p, t P N, p Ď Wp the recursive set of all natural numbers less or equal we denote by W t to t, on which the machine executing p halts in at most t steps.
We are going to make use of these recursive sets in the proof of our essential Proposition 19, showing that conservativeness does not restrict (explanatory) Lim-learning from informants.
Moreover, by s-m-n we refer to a well-known recursion theoretic ob- servation, which gives nice ﬁnite and inﬁnite recursion theorems, like Case’s Operator Recursion Theorem ORT.
In the context of language learning, Gold [1967], in his seminal paper, distin- guished two major diﬀerent kinds of information presentation.
The focus of this paper is one of these methods of presenting the language to the learner, namely so called informants.
Intuitively, for any natural number x an informant for a language L answers the question whether x P L in ﬁnite time.
More precisely, for every natural number x the informant I has either px, 1q or px, 0q in its range, where the ﬁrst is interpreted as x P L and the second as x R L, respectively.
In Learning Families of Formal Languages from Informants order to grasp this more formally, we will have to deal with ﬁnite and inﬁnite sequences of pairs px, iq, where x P N and i P t0, 1u.
For f P pNˆt0, 1uqďø let pospfq :“ ty P N | Dx P N : pr1pfpxqq “ y ^ pr2pfpxqq “ 1u and negpfq :“ ty P N | Dx P N : pr1pfpxqq “ y ^ pr2pfpxqq “ 0u denote the sets of all natural numbers, about which f gives some positive or negative information, respectively.
As in Lange, Zeugmann, and Zilles [2008] according to B¯arzdi¸nˇs [1977] and Blum and Blum [1975] for A Ď N we deﬁne Conspf, Aq :ô pospfq Ď A ^ negpfq Ď NzA and say f is consistent with A or f is compatible with A in case Conspf, Aq is true.
Deﬁnition 1.
Let L be a language.
We call every function I : N Ñ Nˆt0, 1u such that pospIq Y negpIq “ N and pospIq X negpIq “ ∅ an informant.
Further, we denote by Inf the set of all informants and the set of all informants for the language L is deﬁned as InfpLq :“ tI P Inf | pospIq “ Lu. It is immediate, that negpIq “ NzL for every I P InfpLq. If the informant I for every time t P N reveals information about t itself, for short pr1pIptqq “ t, we call I a canonical informant or according to Gold [1967] methodical informant.
The learner, which can also be thought of as a scientist trying to infer a theory or a machine making a guess on the basis of evaluated data, is modelled by a (partial) computable function M : dompMq Ď pNˆt0, 1uqăø Ñ N.
In the following we introduce the fundamental notion of the learning sequence and clarify what successful learning means.
Deﬁnition 2.
Let M be a learner and L a language.
Further, let I P InfpLq be an informant for L presented to M .
(i) We call h “ phtqtPN P Nø, where ht :“ MpIrtsq for all t P N, the learning sequence of M on I.
Thus h is the sequence of M ’s hypotheses by observing I.
(ii) For a P NYt˚u and b P Ną0 Yt˚,8u we say that M learns L from I with a b -learns L bpM, Iq, if there is a time t0 P N such that for anomalies and vacillation number b in the limit, for short M Lima from I, or even shorter Lima all t ě t0 we have Wht “a L and |t ht | t ě t0 u| ď b.
The intuition behind the latter is that, sensing I, M eventually only vacillates between b-many hypotheses, where the case b “ ˚ stands for eventually ﬁnitely many diﬀerent hypotheses.
In convenience with the literature, we ommit the 6 Martin Aschenbach, Timo K¨otzing, and Karen Seidel superscript 0 and the subscript 1.
Lim-learning, also known as explanatory (Ex-)learning, is the most common deﬁnition for successful learning and cor- responds to the notion of identiﬁability in the limit by Gold [1967], where the learner eventually decides on one correct hypotheses.
On the other end of the hierarchy of convergence criteria is behaviorally correct learning, for short Bc- or Lim8-learning, which only requires the learner to be eventually correct, but al- lows inﬁnitely many syntactically diﬀerent hypotheses in the limit.
Behaviorally correct learning was introduced by Osherson and Weinstein [1982].
The general b -learning for a P N Y t˚u and b P Ną0 Y t˚u was ﬁrst men- deﬁnition of Lima tioned by Case [1999].
In our setting, we also allow b “ 8 and subsume all Lima under the notion of a convergence criterion, since they determine in which semi- topological sense the learning sequence needs to have L as its limit, in order to succeed in learning L.
In the following we review so-called learning restrictions, i.e., further potential properties of the learning sequence being investigated in this paper.
Learning re- strictions incorporate certain desired properties of the learners’ behavior relative to the information being presented.
For this, we employ the notion of consistency, stated above.
Deﬁnition 3.
Let M be a learner and I P Inf an informant.
As before we denote by h “ phtqtPN P Nø the learning sequence of M on I.
We write (i) ConspM, Iq (Angluin [1980]), if M is consistent on I, i.e., for all t ConspIrts, Whtq.
(ii) ConvpM, Iq (Angluin [1980]), if M is conservative on I, i.e., for all s, t with s ď t ConspIrts, Whsq ñ hs “ ht.
(iii) DecpM, Iq (Osherson, Stob, and Weinstein [1982]), if M is decisive on I, i.e., for all r, s, t with r ď s ď t Whr “ Wht ñ Whr “ Whs .
(iv) CautpM, Iq (Osherson, Stob, and Weinstein [1986]), if M is cautious on I, i.e., for all s, t with s ď t (cid:32)Wht Ĺ Whs.
(v) WMonpM, Iq (Jantke [1991],Wiehagen [1991]), if M is weakly monotonic on I, i.e., for all s, t with s ď t ConspIrts, Whsq ñ Whs Ď Wht .
(vi) MonpM, Iq (Jantke [1991],Wiehagen [1991]), if M is monotonic on I, i.e.,for all s, t with s ď t Whs X pospIq Ď Wht X pospIq. Learning Families of Formal Languages from Informants (vii) SMonpM, Iq (Jantke [1991],Wiehagen [1991]), if M is strongly monotonic on I, i.e., for all s, t with s ď t Whs Ď Wht.
(viii) NUpM, Iq (Baliga, Case, Merkle, Stephan, and Wiehagen [2008]), if M is non-U-shaped on I, i.e., for all r, s, t with r ď s ď t Whr “ Wht “ pospIq ñ Whr “ Whs.
(ix) SNUpM, Iq (Case and Moelius [2011]), if M is strongly non-U-shaped on I, i.e., for all r, s, t with r ď s ď t Whr “ Wht “ pospIq ñ hr “ hs.
(x) SDecpM, Iq (K¨otzing and Palenta [2014]), if M is strongly decisive on I, i.e., for all r, s, t with r ď s ď t Whr “ Wht ñ hr “ hs.
The following lemma states the implications between almost all of the above deﬁned learning restrictions, which form the foundation of our research.
Figure 1 on page 23 includes the resulting backbone, which is slightly diﬀerent from the one for learning from exclusively positive information, since WMon does not necessarily imply NU in the context of learning from informants.
There the implications are represented as black lines from bottom to top.
For the backbone of learning from positive information consult K¨otzing and Palenta [2014].
Lemma 4.
Let M be a learner and I P Inf an informant.
Then (i) ConvpM, Iq implies SNUpM, Iq and WMonpM, Iq. (ii) SDecpM, Iq implies DecpM, Iq and SNUpM, Iq. (iii) SMonpM, Iq implies CautpM, Iq, DecpM, Iq, MonpM, Iq and WMonpM, Iq. (iv) DecpM, Iq and SNUpM, Iq imply NUpM, Iq. (v) WMonpM, Iq does not imply NUpM, Iq. Proof.
Verifying the claimed implications is straightforward.
In order to verify (v), consider L “ 2N.
Fix p, q P N such that Wp “ 2N Y t1u and Wq “ 2N and deﬁne the learner M for all σ P Nˆt0, 1uăø by Mpσq “ if 1 P negpσq ^ 2 R pospσq; otherwise.
p, q, In order to prove WMonpM, Iq for every I P InfpLq, let I be an informant for L and sIpxq :“ mintt P N | pr1pIptqq “ xu, i.e., sIp1q and sIp2q denote the ﬁrst occurance of p1, 0q and p2, 1q in ranpIq, respectively.
Then we have for all t P N 2N Y t1u, 2N, Wht “ if sIp1q ă t ď sIp2q; otherwise.
8 Martin Aschenbach, Timo K¨otzing, and Karen Seidel We have Whs “ WMpIrssq “ 2N Y t1u as well as 1 P negpIrtsq for all s, t P N with sIp1q ă s ď sIp2q and t ą sIp2q.
Therefore, (cid:32)ConspIrts, Whsq because of negpIrtsq Ę NzWhs.
We obtain WMonpM, Iq since whenever s ď t in N are such that ConspIrts, Whsq, we know that Whs “ 2N Y t1u can only hold if likewise sIp1q ă t ď sIp2q and hence Wht “ 2N Y t1u, which yields Whs Ď Wht.
Furthermore, if Whs “ 2N all options for Wht satisfy Whs Ď Wht.
Otherwise, in case M observes the canonical informant I for L, we have Wh0 “ Wh1 “ 2N, Wh2 “ 2N Y t1u and Wht “ 2N for all t ą 2, which shows (cid:32)NUpM, Iq. 2.2 Learning Sequences and Success of Simulated Learners From deriving the backbone of the hierarchy of delayable learning restrictions when learning languages from informants, we now come to introduce general properties of learning restrictions and learning success criteria, which allow gen- eral observations, not bound to the setting of (explanatory) Lim-learning.
Deﬁnition 5.
Let T :“ P ˆ Inf denote the whole set of possible learners and informants.
We denote by ∆ :“ t Caut, Cons, Conv, Dec, SDec, WMon, Mon, SMon, NU, SNU, Tu the set of admissible learning restrictions and by Γ :“ t Lima b | a P N Y t˚u ^ b P Ną0 Y t˚,8uu the set of convergence criteria.
Further, if β P t nč i“0 δi X γ | n P N,@i ď npδi P ∆q and γ P Γ u Ď P ˆ Inf , we say that β is a learning success criterion.
Note that every convergence criterion is indeed a learning success criterion by letting n “ 0 and δ0 “ T, where the latter stands for no restriction.
In the liter- ature convergence criteria are also called identiﬁcaton criteria and then denoted by I or ID.
Let us now sum up and introduce the notation of a learning criterion.
In order to observe whether one way of learning is more powerful than another one, we are going to compare diﬀerent settings, always denoted in the form rαInf βs or rαTxtβs, where Inf and Txt may have indices.
Clearly, we distinguish the mode of information presentation, namely, whether the learner observes the language as solely positive information, i.e. a text, Txt, or an informant, Inf .
We also sometimes refer to results on learning collections of recursive functions, in which a text for the graph of the respective function is presented to the learner.
We denote the associated learning criteria in the form rαFnβs, where again indices to Fn are allowed.
Learning Families of Formal Languages from Informants Secondly, a learning criterion speciﬁes, what successful learning means.
This information is provided at position β, where the learning restrictions to meet are denoted in alphabetic order, followed by a convergence criterion.
Last but not least, at position α, we restrict the set of admissible learners by re- quiring for example totality.
The properties stated at position α are independent of learning success.
Note that it is also conventional to require M ’s hypothesis sequence to fulﬁll certain learning restrictions, not asking for the success of the learning process.
Deﬁnition 6.
Let α be a property of partial computable functions from the set pNˆt0, 1uqăø to N and β a learning success criterion.
We denote by rαInf βs the set of all collections of languages that are β-learnable from informants by a learner M with the property α.
In case the learner only needs to succeed on canonical informants, we denote the corresponding set of collections of languages by rαInf canβs.
The notations rαTxtβs and rαFnβs are deﬁned similarly.
In the case of learning from solely positive information, there have been plenty of investigations on the relation of diﬀerent success criteria, e.g., on the relation of rTxtLimbs and rTxtNULimbs for all b P Ną0 Y t˚,8u by Carlucci, Case, Jain, and Stephan [2008], Baliga, Case, Merkle, Stephan, and Wiehagen [2008] and Fulk, Jain, and Osherson [1994] as summed up in Case [2016].
Moreover, K¨otzing and Palenta [2014] and Jain, K¨otzing, Ma, and Stephan [2016] give the picture, of how the learning restrictions in Deﬁnition 3 relate, when learning languages from texts with possibly restricting attention to set-driven or iterative learners.
More interesting for understanding the power of learning from informants are results from function learning, as, by the next lemma, collections of functions separating two convergence criteria in the associated setting yield a separating collection for the respective convergence criteria, when learning languages from informants.
In the following we make use of a computable bijection x.
, .y : NˆN Ñ N with its computable inverses π1, π2 : N Ñ N such that x “ xπ1pxq, π2pxqy for all x P N.
Lemma 7.
For f P R let Lf :“ txx, fpxqy | x P Nu denote the language encoding its graph.
Let a P NYt˚u and b P Ną0 Yt˚,8u.
Then for every F Ď R F P rFnLima bs ô LF :“ t Lf | f P F u P rInf Lima bs.
Proof.
Let a, b and F be as stated.
First, assume there is a learner M on function sequences such that F P FnLima bpMq. In order to deﬁne the learner M1 acting on informant sequences and returning W -indices, we employ the following procedure for obtaining a W -code Gppq for Lϕp , when given a ϕ-code p: Given input n, interpreted as xx, yy, let the program encoded by p run on x “ π1pnq.
If it halts and returns y “ π2pxq, then halt; otherwise loop.
10 Martin Aschenbach, Timo K¨otzing, and Karen Seidel The learner M1 acts on σ P pNˆt0, 1uqăø by M1pσq :“ GpMpdecodeppospσqqqq, where decodeppospσqq denotes the from σ uniformly computable sequence τ with τpiq “ pπ1pniq, π2pniqq for all i ă |pospσq| “ |τ|, where pniqiă|pospσq| denotes the bpM1q as enumeration of pospσq according to σ.
By construction, LF P Inf Lima G preserves the number of anomalies.
For the other claimed direction let M be a learner on informant sequences with bpMq. As above we employ a computable function that for every LF P Inf Lima f P R transforms a W -index p for Lf into a ϕ-index Hppq such that ϕHppq “ f .
Thereby, we interpret each natural number i as xxu, vy, ty and check whether ϕp halts on xu, vy in at most t steps of computation.
If so, we check whether u is the argument x we want to compute fpxq for and in case the answer is yes, we return v.
Given input x, for i “ 0 till 8 do the following: If Φppπ1piqq ď π2piq and π1pπ1piqq “ x, then return π2pπ1piqq; otherwise increment i.
Deﬁne the learner M1 on σ P pNˆNqăø by M1pσq :“ HpMpˆσqq, pxxπ1piq, π2piqy, 1q pxxπ1piq, π2piqy, 0q where we transform σ “ ppx0, fpx0qq, .
.
.
,px|σ|´1, fpx|σ|´1qqq into an informant sequence ˆσ of length ˆ|σ| :“ maxtj | @i ă j π1piq ă |σ|u by letting if σpπ1piqq “ pxπ1piq, π2piqq ˆσpiq :“ for all i ă ˆt.
Note that for every f P R and every T P Txtpfq by letting IT :“ Trts, we obtain an informant for Lf .
We show pxx, fpxqy, 1q P IT for every x P N and leave the other details to the reader.
Let x P N and s minimal, such that px, fpxqq P ranpTrssq, i.e., xs´1 “ x.
Further, let t be such that s ď ˆt.
ITpxs ´ 1, fpxqyq “ y Then clearly Trtspxs ´ 1, fpxqyq “ pxx, fpxqy, 1q.
otherwise tPN Again, the claim follows, since H preserves the number of anomalies.
With this we obtain a hierarchy of learning restrictions.
Proposition 8.
Let b P t1,8u.
Then (i) for all a P N holds rInf Lima bs Ĺ rInf Lima`1 bs, bs Ĺ rInf Lim˚ (ii) (iii) rInf Lim˚s Ĺ rInf Lim8s Proof.
By Lemma 7 this results transfer from the corresponding observations for function learning by B¯arzdi¸nˇs [1974] and Case and Smith [1983].
aPNrInf Lima s, Learning Families of Formal Languages from Informants 11 rInf Lims Ĺ .
.
.
Ĺ rInf Limas Ĺ rInf Lima`1s Ĺ .
.
.
rInf Limas Ĺ rInf Lim˚s Ĺ rInf Lim8s Ĺ .
.
.
Ĺ rInf Lima8s Ĺ rInf Lima`18 s Ĺ .
.
.
rInf Lima8s Ĺ rInf Lim˚ 8s.
In particular, we have aPN aPN The lemma obviously also holds when considering TxtLima b -learning languages, where the construction of the text sequence from the informant sequence is folklore.
The next deﬁnition provides a properties of learning restrictions playing a cen- tral role in most of our proofs, since it applies to almost all of the learning restrictions introduced in Deﬁnition 3.
The analog for learning from text has been introduced in K¨otzing and Palenta [2014] and a generalization is studied in K¨otzing, Schirneck, and Seidel [2017], where the relations between rTxtδLims and rTxtδLim8s for diﬀerent δ P ∆ are investigated, respectively.
Deﬁnition 9.
Denote the set of all unbounded and non-decreasing functions by S, i.e., S :“ t s : N Ñ N | @x P NDt P N : sptq ě x and @t P N : spt ` 1q ě sptqu.
Then every s P S is a so called admissible simulating function.
A predicate β Ď P ˆ Inf is delayable, if for all s P S, all I, I1 P Inf and all partial functions M, M1 P P holds: Whenever we have pospI1rtsq Ě pospIrsptqsq, negpI1rtsq Ě negpIrsptqsq and M1pI1rtsq “ MpIrsptqsq for all t P N, from βpM, Iq we can conclude βpM1, I1q.
The name refers to tricks in order to delay mind changes of the learner which were used to obtain polynomial computation times for the learners hypothesis updates as discussed by Pitt [1989] and Case and K¨otzing [2009].
Moreover, it should not be confused with the notion of δ-delay by Akama and Zeugmann [2008], which allows satisfaction of the considered learning restriction δ steps later than in the un-δ-delayed version.
In order to give an intuition, think of β as a learning restriction or learning success criterion and imagine M to be a learner.
Then β is delayable if and only if it carries over from M together with an informant I to all learners M1 and informants I1 representing a delayed version of M on I.
More concretely, as long as the learner M1 conjectures hsptq “ MpIrsptqsq at time t and has, in form of I1rts, at least as much data available as was used by M for this hypothesis, M1 with I1 is considered a delayed version of M with I.
Note, that the simulating function’s unboundedness in particular guarantees pospIq “ pospI1q and negpIq “ negpI1q.
The next result guarantees that arguing with the just deﬁned properties covers all of the considered learning restrictions but consistency.
12 Martin Aschenbach, Timo K¨otzing, and Karen Seidel Lemma 10.
(i) Let δ P ∆.
Then δ is delayable if and only if δ ‰ Cons.
(ii) Every convergence criterion γ P Γ is delayable.
(iii) The intersection of ﬁnitely many delayable predicates on P ˆ Inf is again i“0 δi X γ with delayable.
Especially, every learning success criterion β “ δi P ∆ztConsu for all i ď n and γ P Γ , β is delayable.
Proof.
We approach piq by showing, that Cons is not delayable.
To do so, con- 2 uqq sider s P S with sptq :“ t t and I1pxq :“ px, 12Npxqq, where 12N stands for the characteristic function of all even natural numbers.
By s-m-n there are learners M and M1 such that for all σ P pNˆt0, 1uqăø 2u, I, I1 P Inf deﬁned by Ipxq :“ pt x 2 u, 12Npt x WMpσq “ tx P N | px even ^ x ď t WM1pσq “ tx P N | px even ^ x ď t uq _ px odd ^ x ą t uq _ px odd ^ x ą t |σ| |σ| |σ| |σ| uqu uqu.
Further, ConspM, Iq is easily veriﬁed since for all t P N pospIrtsq “ tx P N | x even ^ x ď t t ´ 1 uu Ď WMpIrtsq negpIrtsq “ tx P N | x odd ^ x ď t t ´ 1 uqu Ď NzWMpIrtsq but on the other hand (cid:32)ConspM1, I1q since for all t ą 2 pospI1rtsq “ tx P N | x even ^ x ă tu Ę tx P N | px even ^ x ď t t uq _ px odd ^ x ą t t uqu “ WM1pI1rtsq.
The remaining proofs for piq and piiq are straightforward.
Basically, for Dec, SDec, SMon and Caut, the simulating function s being non-decreasing and M1pI1rtsq “ MpIrsptqsq for all t P N would suﬃce, while for NU, SNU and Mon one further needs that the informants I and I1 satisfy pospIq “ pospI1q.
The proof for WMon and Conv to be delayable, requires all assumptions, but s’s unboundedness.
Last but not least, in order to prove that every convergence criterion γ “ Lima delayed variants, one essentially needs both characterizing properties of s and of course M1pI1rtsq “ MpIrsptqsq.
Finally, piiiq is obvious.
b , for some a P N Y t˚u and b P Ną0 Y t˚,8u, carries over to 3 Normal Forms: Canonical Informants and Totality To facilitate smooth proofs, in this section we discuss normal forms for learning from informants.
First we consider the notion of set-drivenness, which restricts the set of admissible learners to those not considering the order of presentation or number of occurances of a certain datum.
In Lemma 12 we show for delayable learning success criteria, that every collection of languages that is learnable from Learning Families of Formal Languages from Informants 13 canonical informants is also learnable set-drivenly from arbitrary informants.
Moreover, in Lemma 14 we observe that only considering total learners does not alter the learnability of a collection of languages in case of a delayable learning success criterion.
Further along the line, we provide a regularity property of learners, called syntactic decisiveness, for Lim-learning in Lemma 18.
Deﬁnition 11 (Wexler and Culicover [1980]).
A learner M is set-driven, if for all σ, τ P Nˆt0, 1uăø p pospσq “ pospτq ^ negpσq “ negpτqq ñ Ipσq “ Ipτq.
Intuitively, M is set-driven, if it does not care about the order in which the information is presented or whether there are repetitions.
Sch¨afer-Richter [1984] and Fulk [1985] showed that set-drivenness is a restriction when learning only from positive information and also the relation between the learning restrictions diﬀer as observed by K¨otzing and Palenta [2014].
In the next Lemma we observe that, by contrast, set-drivenness is not a re- striction in the setting of learning from informants.
Concurrently, we generalize Gold [1967]’s observation, stating that considering solely canonical informants to determine learning success does not give more learning power, to arbitrary delayable learning success criteria.
Lemma 12.
Let β be a delayable learning success criterion.
Then every lan- guage collection L that is β-learnable by a learner from canonical informants can also be β-learned by a set-driven learner from arbitrary informants, i.e., rInf canβs “ rSdInf βs Proof.
Clearly, we have rInf canβs Ě rSdInf βs.
For the other inclusion, let L be β-learnable by a learner M from canonical informants.
Let L P L and I1 P InfpLq. For every f P pNˆt0, 1uqďø, thus especially for I1 and all its initial segments, we deﬁne sf P S for all t for which frts is deﬁned, by sfptq “ suptx P N | @w ă x : w P pospfrtsq Y negpfrtsqu, i.e., the largest natural number x such that for all w ă x we know, whether w P pospfrtsq.
In the following f will either be I1 or one of its initial segments, which in any case ensures pospfrtsq Ď L for all appropriate t.
By construction, sf is non-decreasing and if we consider an informant I P Inf , since pospIqYnegpIq “ N, sI is also unbounded.
In order to employ the delayability of β, we deﬁne an operator Σ : pNˆt0, 1uqďø Ñ pNˆt0, 1uqďø such that for every f P pNˆt0, 1uqďø in form of Σpfq we obtain a canonically sound version of f .
Σpfq is deﬁned on all t ă sfp|f|q in case f is ﬁnite and on every t P N otherwise by Σpfqptq :“ if pt, 0q P ranpfq; pt, 0q, pt, 1q, otherwise.
14 Martin Aschenbach, Timo K¨otzing, and Karen Seidel Intuitively, in Σpfq we sortedly and without repetitions sum up all information contained in f up to the largest initial segment of N, f without interruption informs us about.
For a ﬁnite sequence σ the canonical version Σpσq has length sσp|σ|q.
Now consider the set-driven learner M1 deﬁned by M1pσq “ MpΣpσqq.
Since I :“ ΣpI1q is a canonical informant for L, we have βpM, Iq. Moreover, for all t P N holds pospIrsI1ptqsq Ď pospI1rtsq and negpIrsI1ptqsq Ď negpI1rtsq by the deﬁnitions of sI1 and of I using Σ.
Finally, M1pI1rtsq “ MpΣpI1rtsqq “ MpΣpI1qrsI1ptqsq “ MpIrsI1ptqsq and the delayability of β yields βpM1, I1q.
Therefore, while considering delayable learning from informants, looking only at canonical informants already yields the full picture also for set-driven learners.
We will make use of this reduction in other proofs.
Note that the construction of the canonical sound version ΣpIq for an informant I corresponds to the construction of the corresponding one-one text T1´1 for an arbitrary text T , as repeatedly employed in K¨otzing, Schirneck, and Seidel [2017].
Clearly, a similar result can be obtained, when learning recursive functions from their graphs being presented in the canonical or an arbitrary order.
The next proposition answers the arising question, whether Lemma 12 also holds, when requiring the non-delayable learning restriction of consistency, negatively.
In the following let K :“ t p P N | ϕpppqÓu denote the halting problem.
Proposition 13.
The collection of languages L :“ t2K Y 2pK Y txuq ` 1 | x P Nu is consistently, conservatively, strongly decisively and strongly monotonically Lim-learnable from canonical informants.
Further, L is not consistently Lim- learnable from arbitrary informants.
For short we have L P rInf canConsConvSDecSMonLimszrInf ConsLims.
Proof.
Let p : N Ñ N such that Wppxq “ 2K Y 2pK Y txuq ` 1 for every x P N and let k be a W -index for 2K Y 2K ` 1.
Consider the learner M deﬁned by Mpσq “ ppxq, k, if x with 2x P negpσq and 2x ` 1 P pospσq exists; otherwise.
for every σ P pNˆt0, 1uqăø.
Clearly, M conservatively, strongly decisively and strongly monotonically Lim-learns L from informants and on canonical infor- mants for languages in L it is consistent.
Learning Families of Formal Languages from Informants 15 Now, assume there is a learner M such that L P Inf ConsLimpMq. By Lemma 16 there is a locking sequence σ for 2K Y 2K ` 1.
By s-m-n there is a computable function χpxq “ if Mpσq “ Mpσ 1, 0, otherwise.
(cid:97)p2x ` 1, 1qq; By the consistency of M on L, we immediately obtain that χ is the characteristic function for K, a contradiction.
Note, that there must not be an indexable family witnessing the diﬀerence stated in the previous proposition, since every indexable family is consistently and con- servatively Lim-learnable by enumeration.
Further, the connatural observation rConsFnLims Ĺ rConsFncanLims by Jantke and Beick [1980], when learning collections of recursive functions, may be helpful to reprove this result with a generalization of Lemma 7 to consistent learning.
Gold [1967] further introduces request informants for M and L.
As the name already suggests, there is an interaction between the learner and the informant in the sense that the learner decides, about which natural number the informant should inform it next.
His observation rInf Lims “ rInf canLims “ rInf reqLims seems to hold true when facing arbitrary delayable learning success criteria, but fails in the context of the non-delayable learning restriction of consistency.
Moreover, by the following lemma, in most of the proofs in the remainder of this paper we are going to consider only total learners.
Lemma 14.
Let β be a delayable learning success criterion.
Then every lan- guage collection β-learnable by a learner from informants can also be β-learned by a total learner from informants, i.e., rInf βs “ rRInf βs.
Proof.
Let L P rInf βs and M be a learner witnessing this.
Without loss of generality we may assume that ∅ P dompMq. We deﬁne the total learner M1 by letting sM : pNˆt0, 1uqăø Ñ N, σ ÞÑ supts P N | s ď |σ| and M halts on σrss after at most |σ| stepsu and M1pσq :“ MpσrsMpσqsq.
The convention supp∅q “ 0 yields that sM is total and it is computable, since for M only the ﬁrst |σ|-many steps have to be evaluated on σ’s ﬁnitely many initial segments.
One could also employ a Blum complexity measure here.
Hence, M1 is a total computable function.
In order to observe, that M1 Inf β-learns L, let L P L and I be an informant for L.
By letting sptq :“ sMpIrtsq, we clearly obtain an unbounded non-decreasing 16 Martin Aschenbach, Timo K¨otzing, and Karen Seidel function, hence s P S.
Moreover, for all t P N from sptq ď t immediately follows pospIrsptqsq Ď pospIrtsq, negpIrsptqsq Ď negpIrtsq as well as M1pIrtsq “ MpIrsMpIrtsqsq “ MpIrsptqsq.
By the delayability of β and with I1 “ I, we ﬁnally obtain βpM1, Iq. In the following we transfer an often employed observation by Blum and Blum [1975] to the setting of learning from informants and generalize it to all conver- gence criteria introduced in Deﬁnition 2.
Deﬁnition 15.
Let M be a learner, L a language and a P N Y t˚u as well as b P Ną0 Y t˚,8u.
We call σ P pNˆt0, 1uqăø a Lima b -locking sequence for M on L, if Conspσ, Lq and ˘˘˘ DD Ď N p |D| ď b ^ @τ P pNˆt0, 1uqăø Conspτ, Lq ^ Mpσ WMpσ(cid:97)τq “a L ^ Mpσ τq P D `` τqÓ (cid:97) (cid:97) Further, a locking sequence for M on L is a Lim-locking sequence for M on L.
Intuitively, the learner M is locked by the sequence σ onto the language L in the sense that no presentation consistent with L can circumvent M guessing admis- sible approximations to L and additionally all guesses based on an extension of σ are captured by a ﬁnite set of size at most b.
Note that the deﬁnition implies MpσqÓ, WMpσq “a L and Mpσq P D.
Lemma 16.
Let M be a learner, a P NYt˚u, b P Ną0Yt˚,8u and L a language Lima b -identiﬁed by M .
Then there is a Lima b -locking sequence for M on L.
Proof.
This is a straightforward contradictory argument.
Without loss of gen- erality M is deﬁned on ∅.
Assume for every σ with Conspσ, Lq, MpσqÓ and WMpσq “a L and for every ﬁnite D Ď N with at most b elements there exists a sequence τ D σ P pNˆt0, 1uqăø with σ qÓ ^ τ D σ , Lq ^ Mpσ (cid:97) Conspτ D Let IL denote the canonical informant for L.
We obtain an informant for L on which M does not Lima b -converge by letting (cid:97) σ q R D τ D σ q “a L _ Mpσ (cid:32)WMpσ(cid:97)τ D I :“ σ0 :“ ILr1s, (cid:97) n τ Dn σn i q | maxt0, n´b`1u ď i ď nu we collect M ’s σn, with nPN (cid:97) ILpn ` 1q σn`1 :“ σ for all n P N, where in Dn :“ t Mpσ´ at most b-many last relevant hypotheses.
Since I is an informant for L by having interlaced the canonical informant for L, the learner M Lima b -converges on I.
n0 Ď Irts we have Wht “a L.
Then Therefore, let n0 be such that for all t with σ´ certainly t Mpσ´ i q | n0 ď i ď n0 ` bu has cardinality b ` 1, a contradiction.
Learning Families of Formal Languages from Informants 17 Obviously, an appropriate version also holds when learning from text is consid- ered.
Before we determine the relations between the introduced learning restrictions for (explanatory) Lim-learning from informants, we introduce a further beneﬁ- cial property, requiring a learner never to syntactically return to an abandoned hypothesis.
Deﬁnition 17 (K¨otzing and Palenta [2014]).
Let M be a learner, L a lan- guage and I an informant for L.
We write SynDecpM, Iq, if M is syntactically decisive on I, i.e., @r, s, t : pr ď s ď t ^ hr “ htq ñ hr “ hs.
The following easy observation shows that this variant of decisiveness can always be assumed in the setting of Lim-learning from informants.
This is employed in the proof of Proposition 19.
Lemma 18.
Every language collection Lim-learnable from informants can also be syntactically decisively Lim-learned from informants, i.e., rInf Lims “ rInf SynDecLims Proof.
Since obviously rInf SynDecLims Ď rInf Lims, it suﬃces to show that every Inf Lim-learnable collection of languages is also Inf SynDecLim-learnable.
For, let L P rInf Lims and M witnessing this.
In the deﬁnition of the learner M1, we make use of a one-one computable padding function pad : NˆN Ñ N such that Wp “ dompϕpq “ dompϕpadpp,xqq “ Wpadpp,xq for all p, x P N.
Now, consider M1 deﬁned by M1pσq :“ padpMpσq,|σ|q, M1pσq, if Mpσ´q ‰ Mpσq; otherwise M1 behaves almost like M with the crucial diﬀerence, that whenever M performs a mind change, M1 semantically guesses the same language as M did, but syn- tactically its hypothesis is diﬀerent from all former ones.
The padding function’s deﬁning property and the assumption that M Inf Lim-learns L immediately yield the Inf SynDecLim-learnability of L by M1.
Note that SDec implies SynDec, which is again a delayable learning restriction and therefore also afsoep.
Thus, in the proof of Lemma 18 we could have also restricted our attention to canonical informants.
4 Relations between Delayable Learning Restrictions In order to reveal the relations between the delayable learning restrictions in (explanatory) Lim-learning from informants, in Proposition 19 we acquire that 18 Martin Aschenbach, Timo K¨otzing, and Karen Seidel conservativeness and strongly decisiveness do not restrict informant learning.
After this, Propositions 20 and 22 provide that cautious and monotonic learning are incomparable, implying that both these learning settings are strictly stronger than strongly monotonic learning and strictly weaker than unrestricted learning.
The overall picture is summarized in Figure 1 and stated in Theorem 24.
Proposition 19.
Every collection of languages Lim-learnable from methodical informants can also be conservatively and strongly decisively Lim-learned by a total, set-driven learner from informants, i.e., rInf Lims “ rRSdInf ConvSDecLims Especially, rInf Lims “ rInf ConvSDecLims Proof.
Obviously rInf Exs Ě rRSdInf ConvSDecExs and by the Lemmas 12, 14 and 18 it suﬃces to show rRInf SynDecExs Ď rInf mConvSDecExs.
Now, let L P rRInf SynDecExs and M a learner witnessing this.
In particular, M is total and on informants for languages in L we have that M never returns to a withdrawn hypothesis.
For every set X and t P N, let Xrts denote the methodical informant sequence of the ﬁrst t elements of X.
We want to deﬁne a learner M1 which uses hypotheses ppσq; these hypotheses should mimic the hypotheses Mpσq, but suitably poisoned, i.e., modiﬁed such that, if σ is a locking sequence, then ppσq is codes the same language as Mpσq.
However, if σ is not a locking sequence, then the hypothesis should no include data that we want to change our mind on.
In order to do so formally in a computable way, we use the following deﬁnitions.
For any given σ, D Ď W s Mpσq and s, we let zspσ, Dq “ mintz ď s | D Ď W z Mpσqu.
For any given σ, D0 and s, we let‹ Qpσ, D0, sq “ tD Ď W s Mpσq | maxpDq ă minpW s MpσqzDq and D0 Ă D and Mpσq “ MpW s Mpσqrzspσ, Dqsqu.
Intuitively, given σ, ppσq should enumerate more and more elements enumerated by Mpσq.
If we have currently enumerated a set D0 and consider a time bound of s, then Q gives all the candidate sets we can use for extending D0.
Note that, for all σ, D0 and s, Qpσ, D0, sq is totally ordered by Ď.
We consider the following auxiliary sets for all σ.
A0pσq “ pospσq; $’&’%W s @s P N : As`1pσq “ Mpσq, maxĎ Qpσ, Aspσq, sq, Aspσq, Mpσq ‰ H; if negpσq X W s else if Qpσ, Aspσq, sq ‰ H; otherwise.
‹ We suppose minH “ 8 for convenience.
Learning Families of Formal Languages from Informants 19 Furthermore, using s-m-n, we deﬁne p for all σ such that sPN Wppσq “ Aspσq.
$’&’%ppσq, Finally, we deﬁne our new learner M1 such that M1pσq “ ppσq, M1pσ´q, otherwise.
if |σ| “ 0; else if Mppσ´q1q ‰ Mpσq ^ (cid:32)Conspσ, A|σ|ppσ´q1qq; That is, M1 follows the mind changes of M once a suitably inconsistent hypoth- esis has been seen.
All hypotheses of M by are poisoned in a way to ensure that we can decide inconsistency.
Claim 1: Let L be a language Inf Ex-learned by M .
Then M1 Inf Ex-learns L.
Let t be minimal such that, for all t1 ě t, MpLrtsq “ MpLrt1sq.
Thus, MpLrtsq is a correct hypothesis for L, we denote by e.
Case 1: M1 does not make a mind change after t.
Then M1 converged already before that mind change of M , so there is t0 ă t minimal with, for all t1 ě t0, e1 :“ M1pLrt0sq “ M1pLrt1sq.
From the deﬁnition of M1 we get, for all t1 ě t, ConspLrt1s, At1pLrt0sqq.
Thus, We1 contains all elements of L and no other, i.e., We1 “ L as desired.
Case 2: M1 makes a mind change after t.
Let t1 ě t be the time of that mind change.
Clearly, M1 will converge to ppLrt1sq, denoted by e1.
We get We1 Ď L immediately.
Suppose now there is a minimal element x P LzWe1.
Then We1 is ﬁnite and equal to some AspLrt1sq (since the next added elements would necessarily contain x by deﬁnition).
Let y “ maxpAspLrt1sq Y txuq and let s1 ą s be large enough such that Lry ` 1s e ry ` 1s, i.e., all element of L up to (and including) y are enumerated equals W s1 by time s1.
Let z :“ zs1pLrt1s, pospLry ` 1sqq be the time window considered in the third condition of pospLry` 1sq P QpLrt1s, AspLrt1sq, s1q, for any s1 ě s1.
Let e rzs “ Lrzs, i.e., all elements within the s2 ě s1 be large enough such that W s2 time window are enumerated.
Then pospLry ` 1sq P QpLrt1s, AspLrt1sq, s2q, as M is converged after t ă t1 on L, a contradiction to x (which is in pospLry` 1sq) not being included in We1.
Claim 2: Let L be a language Inf Ex-learned by M .
Then M1 in conservative on L.
Let t be such that M1pLrtsq ‰ M1pLrt ` 1sq.
Let e1 :“ M1pLrtsq and let t0 ď t be minimal such that M1pLrt0sq “ e1.
From the mind change of M1 we get that there was a mind change of M , i.e. MpLrt0sq ‰ MpLrt ` 1sq.
Suppose, by way of contradiction, We1 is consistent with Lrt ` 1s, i.e. ConspLrt ` 1s, We1q.
(1) (2) 20 Martin Aschenbach, Timo K¨otzing, and Karen Seidel From the fact that M1 made a mind change we get (cid:32)ConspLrt ` 1s, At`1pLrt0sqq.
(3) If there was an element x P At`1pLrt0sq which is listed negatively in Lrt ` 1s, then x P At`1pLrt0sq Ď We1, contradicting Equation (2).
In particular, we obtain negpLrt1sq X WMpLrt1sq “ H from the ﬁrst case in the deﬁnition of A.
Thus, to satisfy Equation (3), there is x such that x P pospLrt ` 1sqzAt`1pLrt0sq.
(4) Since pospLrt ` 1sq Ď We1 from Equation (2), there is t1 ą t ` 1 minimal such that pospLrt ` 1sq Ď At1pLrt0sq.
This implies pospLrt ` 1sq P QpLrt ` 1s, At1´1pLrt0sq, t1q.
Thus, zt1pLrt0s, pospLrt ` 1sqq ă t ` 1, using the mind change of M , see Equa- tion (1).
We now get pospLrt ` 1sq P QpLrt ` 1s, AtpLrt0sq, t ` 1q, leading to Lrt ` 1s Ď At`1pLrt0sq, a contradiction to Equation (4).
Finally, M1 behaves strongly decisively on the methodical informant for L since M1 is consistent after a mind change and only makes a mind change when in- consistent (i.e., M1 is conservative).
The auxiliary sets transfer the technique of poisoning a conjecture introduced in K¨otzing and Palenta [2014] and tailor it to this special setting.
The next two propositions show that monotonic and cautious learning are in- comparable on the level of indexable families.
In the ﬁrst proposition the learner can even be assumed cautious on languages it does not identify.
Thus, according to Deﬁnition 6 we write this success independent property of the learner on the left side of the mode of presentation.
Proposition 20.
The indexable family L :“ t2X Y p2pNzXq ` 1q | X Ď N ﬁnite or X “ Nu is Lim-learnable by a cautious learner from informants.
Further, L is not mono- tonically Lim8-learnable from informants.
For short we have L P rCautInf LimszrInfMonLim8s.
Particularly, rInfMonLims Ĺ rInf Lims.
Proof.
We ﬁrst show L R rInfMonLim8s.
Let M be a Inf Lim8-learner for L.
Further, let I0 be the methodical informant for L0 :“ 2N P L.
Then there exists Learning Families of Formal Languages from Informants 21 t0 such that WMpI0r2t0sq “ 2N.
Moreover, consider the methodical informant I1 for L1 :“ 2t0, .
.
.
, t0u Y p2pNzt0, .
.
.
, t0uq ` 1q P L and let t1 ą t0 such that WMpI1r2t1sq “ L1.
Similarly, we let I2 be the methodical informant for L2 :“ 2t0, .
.
.
, t0, t1 ` 1u Y p2pNzt0, .
.
.
, t0, t1 ` 1uq ` 1q P L and choose t2 ą t1 with WMpI2r2t2sq “ L2.
Since 2pt1 ` 1q P pL0 X L2qzL1 and by construction I2r2t0s “ I0r2t0s as well as I2r2t1s “ I1r2t1s, we obtain 2pt1 ` 1q R WMpI2r2t1sq X L2 2pt1 ` 1q P WMpI2r2t0sq X L2 and and therefore M does not learn L2 monotonically from I2.
Let us now adress L P rCautInf Lims.
Fix p P N such that Wp “ 2N.
Further, by s-m-n there is a computable function q : N Ñ N with WqpxXyq “ X Yp2NzXq` 1, where xXy stands for a canonical code of the ﬁnite set X.
We deﬁne the learner M for all σ P Nˆt0, 1uăø by Mpσq “ p, qpxpospσq X 2Nyq, otherwise.
if pospσq Ď 2N; Intuitively, M guesses 2N as long as no odd number is known to be in the language L to be learned.
If for sure L ‰ 2N, then M assumes that all even numbers known to be in L so far are the only even numbers therein.
It is easy to verify that M is computable and by construction it learns L.
For establishing the cautiousness, let L be any language, I an informant for L and s ď t.
Furthermore, assume WMpIrssq ‰ WMpIrtsq.
In case pospIrssq Ę 2N, we have x P ppospIrtsq X 2Nq with x R ppospIrssq X 2Nq and therefore as desired WMpIrtsqzWMpIrssq ‰ ∅.
Then again, pospIrssq Ď 2N implies WMpIrssq “ 2N and thus again WMpIrtsqzWMpIrssq ‰ ∅.
Corollary 21.
There exists an indexable family cautiously Lim-learnable from informants, but not strictly monotonically Lim-learnable from informants.
In particular, rInf SMonLims Ĺ rInf CautLims.
The following proposition extends the overservation of Osherson, Stob, and We- instein [1986] for cautious learning to restrict learning power.
Proposition 22.
The indexable family L :“ tNzX | X Ď N ﬁniteu is monotonically Lim-learnable from informants.
Further, L is not cautiously behaviorally correct learnable from informants.
For short we have L P rInfMonLimszrInf CautLim8s.
Particularly, rInf CautLims Ĺ rInf Lims.
22 Martin Aschenbach, Timo K¨otzing, and Karen Seidel Proof.
In order to approach L R rInf CautLim8s, let M be a GInf Lim8- learner for L and I0 the methodical informant for N.
Moreover, let t0 be such that WMpI0rt0sq “ N.
Let I1 be the methodical informant for L1 :“ Nztt0 ` 1u.
Since M learns L1, there is t1 ą t0 such that WMpI1rt1sq “ L1.
We have I1rt0s “ I0rt0s and hence M is not cautiously learning L1 from I1.
By s-m-n there is a computable function p : N Ñ N such that for all ﬁnite sets X holds WppxXyq “ NzX, where xXy denotes a canocical code for X as already employed in the proof of Proposition 20.
We deﬁne the learner M by letting for all σ P Nˆt0, 1uăø Mpσq “ ppxnegpσqyq.
The corresponding intuition is that M includes every natural number in its guess, not explicitally excluded by σ.
Clearly, M learns L and behaves monotonically on L, since for every D Ď N ﬁnite, every informant I for NzD and every t P N, we have WMpIrtsq Ě NzD and therefore WMpIrtsq X NzD “ NzD.
This reproves the following result by Lange, Zeugmann, and Kapur [1996].
Corollary 23.
There exists an indexable family monotonically Lim-learnable from informants, but not strictly monotonically Lim-learnable from informants.
In particular, rInf SMonLims Ĺ rInfMonLims.
We sum up the preceding results in the next theorem and also represent them in Figure 1, in which black lines denote the backbone given by Lemma 4.
The therein claimed proper inclusions were already stated in the Propositions 20 and 22 as well as the Corollaries 21 and 23 thereafter.
Theorem 24.
All learning restrictions introduced in deﬁnition 3 but Caut, Mon and SMon do not restrict fully informed Lim-learning from informants.
Further, for this kind of learning Caut and Mon are incomparable.
For short we have (i) @δ P tConv, Dec, SDec, WMon, NU, SNUu : rInf δLims “ rInf Lims (ii) rInfMonLims K rInf CautLims Proof.
The ﬁrst part is an immediate consequence of Proposition 19 and so is the second part of the Propositions 20 and 22.
5 Outperforming Learning from Texts In the following we relate informant learning to the more prominent concept of learning from solely positive information, i.e., texts.
Learning Families of Formal Languages from Informants 23 Fig.
1.
Relations between delayable learning restrictions in full-information (explana- tory) Lim-learning of languages from informants.
Deﬁnition 25.
Let L be a language.
The set of all texts for the language L is TxtpLq :“ t T P pN Y t#uqø | cntpTq “ Lu, where cntpTq “ ranpTqzt#u is the content of T .
Thus, a text is just an enumeration of the language, since the pause symbol # is interpreted as no new information but necessary for the empty language.
In case L is presented to the learner M in form of a text, M will never know for sure that some natural number x is not in L.
The learner, which was originally thought of as a child while language acquisition, is in this setting modelled by a (partial) computable function M : dompMq Ď Năø Ñ N.
Already Gold [1967] observed rTxtLims Ĺ rInf Lims and lateron Lange and Zeugmann [1993] further investigated the interdependencies when considering the diﬀerent monotonicity learning restrictions.
For instance, they showed that there exists an indexed family L P rInfMonLimszrTxtLims ‰ ∅ and in contrast that for indexed families Inf SMonLim-learnability implies TxtLim- learnability.
We show that this inclusion fails on the level of families of recursive languages.
InfLimSdInfLimTNUDecSMonMonWMonCautSDecSNUConv24 Martin Aschenbach, Timo K¨otzing, and Karen Seidel Proposition 26.
The class of recursive languages L :“ t2pL Y txuq Y 2L ` 1 | L is recursive ^ WminpLq “ L ^ x ě minpLqu is strongly monotonically Lim-learnable from informants.
Moreover, L is not Lim-learnable from texts, i.e., L P rInf SMonLimszrTxtLims.
Proof.
Let pm denote an index for 2Wm Y 2Wm ` 1 and pm,x an index for 2pWmYtxuqY2Wm`1.
The learner M will look for the minimum of the possible L-generating language of the presented recursively enumerable set and moreover try to detect the exception x, in case it exists.
Thus, it checks for all m such that 2m P pospσq or 2m ` 1 P pospσq whether for all k ă m holds 2k P negpσq or 2k ` 1 P negpσq.
In case m has this property relative to σ, we write σL minpmq.
Further, M tries to ﬁnd x such that 2x P pospσq and 2x ` 1 P negpσq and we excpxq that x is as wished.
Consider the learner M deﬁned by abbreviate by σL $’&’%indp∅q, pm, pm,x, Mpσq “ if there is no m with σL if σL if σL minpmq and there is no x with σL minpmq and σL excpxq.
minpmq; excpxq; for all σ P pNˆt0, 1uqăø.
Clearly, M strongly monotonically Lim-learns L.
To observe L R rTxtLims, assume there exists M such that L P TxtLimpMq. By s-m-n there exists e P N such that for all i P N Aσpiq “ t k P N | Mpσq ‰ Mpσ Bσpiq “ t k P N | Mpσq ‰ Mpσ (cid:97)p2e ` 4iqkqu; (cid:97)p2e ` 4i ` 2qkqu; σ0 “ p2e, 2e ` 1q; σi, (cid:97) i p2e ` 4iqinfpAσipiqq(cid:97)p2e ` 4i ` 1q, (cid:97) i p2e ` 4i ` 2qinfpBσipiqq(cid:97)p2e ` 4i ` 3q, tn | 2n ` 1 P ranpσiqu.
if Aσipiq “ Bσipiq “ ∅ or i ą 0 ^ σi´1 “ σi; if Aσipiq ‰ ∅ ^ infpAσipiqq ď infpBσipiqqq; if Bσipiq ‰ ∅ ^ infpBσipiqq ă infpAσipiqq; $’’’’’’’’&’’’’’’’’% iPN σi`1 “ We “ We is recursive, because it is either ﬁnite or we can decide it along the construc- tion of the σi.
Thus, 2WeY2We`1 P L.
If for some index i holds σi`1 “ σi, then M fails to learn either 2pWeYte`2iuqY2We`1 or 2pWeYte`2i`1uqY2We`1.
On the other hand, if there is no such i, by letting T :“ iPN σi we obtain a text for 2We Y 2We ` 1, on which M performs inﬁnitely many mindchanges.
Note that the learner witnessing the SMonLim-learnability of L is also conser- vative and strongly decisive, which was not stated because of Theorem 24.
Learning Families of Formal Languages from Informants 25 6 Duality of the Vacillatory Hierarchy After having investigated the relations between the diﬀerent delayable learning restrictions in the setting of Lim-learning from informants and its relation to learning from texts, we link it to other convergence criteria.
In Proposition 8 we already observed a hierarchy, when varying the number of anomalies and will now show that allowing the learner to vacillate between ﬁnitely many correct hypothesis in the limit does not give more learning power.
On the contrary, only requiring semantic convergence, i.e., allowing inﬁnitely many correct hypotheses in the limit, does allow to learn more collections of languages even with an arbitrary semantic learning restriction at hand.
As every indexable family of recursive languages is Lim-learnable from infor- mants by enumeration, the vacillatory hierarchy collapses for such collections bs for all of languages, i.e., for all indexable families L we have L P rInf Lima a P N Y t˚u and b P Ną0 Y t˚,8u.
In contrast, we strengthen Proposition 8 (iii) by separating Inf Lim- and Inf Lim8-learning at the level of families of recursive languages, even when requiring the Lim8-learning sequence to meet all introduced delayable semantic learning restrictions.
Proposition 27.
The collection of recursive languages L :“ tL Y txu | L Ď N is recursive ^ WminpLq “ L ^ x ě minpLqu is strongly monotonically Lim8-learnable from informants.
Moreover, L is not Lim-learnable from informants, i.e., L P rInf SMonLim8szrInf Lims.
Especially, for every δ P tCaut, Dec, Mon, SMon, WMon, NUu holds L P rInf δLim8szrInf Lims.
Proof.
By the Lemmas 4 and 12 it suﬃces to show L P rInf canSMonLim8szrInf canLims.
By s-m-n there are p : Nˆt0, 1uăø ˆ N Ñ N and a learner M such that for all σ P Nˆt0, 1uăø and x P N Wppσ,xq “ Wminppospσqq Y txu and $’&’%indp∅q, minppospσqq, ppσ, xq, Mpσq “ if pospσq “ ∅; else if pospσqzW else if x “ minppospσqzW |σ| minppospσqq “ ∅; |σ| minppospσqqq; 26 Martin Aschenbach, Timo K¨otzing, and Karen Seidel where indp∅q refers to the canonical index for the empty set.
For every L in Inf canSMonLim8pMq, let L Y txu P L with L Ď N recursive, WminpLq “ L and x ě minpLq and let I be the methodical informant for LYtxu.
Then for all t ą minpLq we have WminppospIrtsqq “ WminpLq “ L.
Further, let m be minimal such that ty P L | y ă xu Ď W m minpLq. Since x ě minpLq the construction yields for all t P N $’&’%∅, Wht “ L, L Y txu, otherwise.
if t ď minpLq; else if minpLq ď t ă maxtx ` 1, mu; This can be easily veriﬁed, since in case y P L we have L “ L Y tyu and shows the Inf canSMonLim8-learnability of L by M .
In order to approach L R rInf canLims, assume to the contrary that there is a learner M that Inf canLim-learns L.
We are going to deﬁne a recursive language L with WminpLq “ L helpful for showing that not all of L is Inf canLim-learned by M .
In order to do so, for every methodical σ P Nˆt0, 1uăø we deﬁne sets σ stand for the methodical informant of pospσq, whereas σ, A1 A0 σ we collect all t ą |σ| σ denotes the methodical informant of pospσqYt|σ|u.
In A0 I 1 for which M ’s hypothesis on I 0 σ we σrts makes a guess diﬀerent from Mpσq.
capture all t ą |σ| such that M on I 1 This reads as follows σrts is diﬀerent from Mpσq.
Similarly, in A1 σ Ď N.
For this let I 0 σ :“ t t P N | t ą |σ| ^ MpI 0 A0 σ :“ t t P N | t ą |σ| ^ MpI 1 A1 σrtsq ‰ Mpσqu, σrtsq ‰ Mpσqu.
Note that for every t ą |σ| σrts “ σ I 0 σrts “ σ I 1 (cid:97)pp|σ|, 0q,p|σ|`1, 0q, .
.
.
,pt ´ 1, 0qq, (cid:97)pp|σ|, 1q,p|σ|`1, 0q, .
.
.
,pt ´ 1, 0qq.
By s-m-n there exists p P N such that (we use the convention infpHq “ 8) @i P N : σi`1 “ σ0 “ pp0, 0q, .
.
.
,pp ´ 1, 0q,pp, 1qq, σi “ A1 if A0 if infpA0 σiqs, σiqs, otherwise; σirminpA0 I 0 σirminpA1 I 1 pospσiq.
Wp “ $’&’%σi, iPN σi “ H; σiq ď infpA1 σiq; Learning Families of Formal Languages from Informants 27 By construction p “ minpWpq and Wp is recursive, which immediately yields L :“ Wp P L.
Further, for every i P N from σi ‰ σi`1 follows Mpσiq ‰ Mpσi`1q.
Aiming at a contradiction, let I be the methodical informant for L, which implies iPN σi Ď I.
Since M explanatory learns L and thus does not make inﬁnitely many mind changes on I, there exists i0 P N such that for all i ě i0 we have σi “ σi0 .
But then for all t ą |σi0| holds MpI 0 σi0 rtsq “ Mpσi0q “ MpI 1 σi0 rtsq, thus M does not learn at least one of L “ pospσi0q and L Y t|σi0|u from their methodical informants.
On the other hand both of them lie in L and therefore, M had not existed in the beginning.
Since allowing inﬁnitely many diﬀerent correct hypotheses in the limit gives more learning power, the question arises, whether ﬁnitely many hypotheses al- ready allow to learn more collections of languages.
The following proposition shows that as observed by B¯arzdi¸nˇs and Podnieks [1973] and Case and Smith [1983] for function learning the hierarchy of vacillatory learning collapses when learning languages from informants.
Note that this contrasts the results in lan- guage learning from texts by Case [1999], observing for every a P N Y t˚u a hierarchy rTxtLimas Ĺ .
.
.
Ĺ rTxtLima rTxtLima bs Ĺ rTxtLima bs Ĺ rTxtLima˚s Ď rTxtLima8s.
b`1s Ĺ .
.
.
bPNą0 Proposition 28.
Let a P N Y t˚u.
Then rInf Limas “ rInf Lima˚s.
Proof.
Clearly, rInf Limas Ď rInf Lima˚s.
For the other inclusion let L be in rInf Lima˚s and M a learner witnessing this.
By Lemma 14 we assume that M is total.
In the construction of the Lima-learner M1, we employ the recursive function Ξ : pNˆt0, 1uqăø ˆ N Ñ N, which given σ P pNˆt0, 1uqăø and p P N |σ| Ξpσ,pq X negpσq “ ∅ and moreover, if σ Ď τ are such that alters p such that W |τ| |σ| p X negpτq, then Ξpσ, pq “ Ξpτ, pq.
One way to do this is p X negpσq “ W by letting Ξpσ, pq denote the unique program, which given x successively checks, whether x “ yi, where pyiqiă|negpσq| is the increasing enumeration of negpσq.
As soon as the answer is positive, the program goes into a loop.
Otherwise it executes the program encoded in p on x, which yields ϕΞpσ,pqpxq “ if x P negpσq; Ò, ϕppxq, otherwise.
Now, M1 works as follows: I.
Compute pi :“ Mpσrisq for all i ď |σ|.
II.
Withdraw all pi with the property |negpσq X W |σ| pi | ą a.
28 Martin Aschenbach, Timo K¨otzing, and Karen Seidel III.
Deﬁne M1pσq to be a code for the program coresponding to the union vote of all Ξpσ, piq, for which pi was not withdrawn in the previous step: Given input x, for n from 0 till 8 do the following: If i :“ π1pnq ď |σ|, |σ| |negpσqXW pi | ď a and ΦΞpσ,piqpxq ď π2pnq, then return 0; otherwise increment n.
This guarantees ϕM1pσqpxq “ 0, Ò, if D i ď |σ| p|negpσq X W otherwise.
|σ| pi | ď a ^ ϕΞpσ,piqpxqÓq; Intuitively, M1pσq eliminates all commission errors in guesses of M on initial segments of σ, not immediately violating the allowed number of anomalies, and then asks whether one of them converges on the input, which implies WM1pσq “ iď|σ|,|negpσqXW |σ| pi |ďa WΞpσ,Mpσrisqq.
In order to show L P Inf LimapMq, let L P L and I P InfpLq. As L P Lima˚pMq, there is t0 such that all of M ’s hypotheses are in ths | s ď t0u and additionally | W t0 hs X NzL| ą a for all s ď t0 with | Whs X NzL| ą a.
Moreover, we can assume that for all s ď t0 with | Whs X NzL| ď a we have observed all commission errors in at most t0 steps, which formally reads as Whs X NzL “ W t0 Then for all t ě t0 we obtain the same set of indices hs X NzL.
A :“ t ΞpIrts, piq | i ď t ^ |negpIrtsq X W t pi| ď au X NzL “ ∅.
Further, since ϕh1 and therefore M1 will return syntactically the same hypothesis, namely, h1 “a L.
By construction and the choice of t0 there It remains to argue for Wh1 pxq exists t0 are no commission errors, i.e., Wh1 in case there is at least one p P A such that ϕppxq exists, there are at most a t0 arguments, on which ϕh1 t0 Similar to results for language learning from texts for b P t1,8u by Case and Lynes [1982], we gain a strict hierarchy when bounding the number of inﬁnitely often occuring correct hypotheses for the target by a ﬁxed number a P N Y t˚u as already observed in Proposition 8.
is undeﬁned.
t0 t0 7 Conclusion and Future Work This paper investigates learning formal languages in the limit from informants, i.e., inferring from a steadily growing sample set of positive and negative data one or more (almost) correct enumeration procedure(s) for the target.
Whereas Section 2 provides necessary deﬁnitions and the back-bone of delayable learning restrictions in this setting, Section 3 establishes the normal form of being to- tal and additionally that, for every delayable learning success criterion, we only Learning Families of Formal Languages from Informants 29 need to consider learning from canonical informants.
Thereafter, in Section 4, the complete picture for (set-driven) Lim-learning from informants with a de- layable learning restriction, as depicted in the diagram in Figure 2, is derived.
It is also valid in case exclusively indexable families are considered.
Important examples are all levels in the Chomsky hierarchy but type-0, i.e. recursively enumerable, formal languages.
The proofs combine diﬀerent techniques and em- ploy the connections provided beforehand.
In contrast to the observed hierarchies when learning success allows for deducing approximations to the target language of diﬀerent quality in Section 2, Section 6 provides that requiring the learner to eventually output exactly one correct enumeration procedure is as powerful as allowing any ﬁnite number of correct descriptions in the limit.
Thereafter we show that, in learning from informants, even when facing all semantic learning restrictions at hand, we gain more learning power, in case learning is considered successful also for an inﬁnite number of correct descriptions in the limit.
Inf Ex SdInf Ex NU SNU WMon Dec SDec Conv Mon Caut SMon Fig.
2.
Diagram depicting the relations between delayable learning restrictions in (ex- planatory) Lim-learning of languages from informants.
Further research could investigate the relationships between the diﬀerent de- layable learning restrictions for other convergence criteria, where the general results in Section 3 may be helpful.
Further, whether requiring the learner to be total restricts consistent learning from informants, seems like an appropri- ate indicator concerning the conjecture of delayability being the right structural property to gain deeper insights into the connections and diﬀerences between all the available deﬁnitions of learning success.
To this end, results by Akama and Zeugmann [2008] and Case and K¨otzing [2008] may be helpful.
Another open question regards the relation between learning recursive functions from texts for their graphs and learning languages from either informants or texts.
It seems like delayability plays a crucial role in order to obtain normal forms and investigate how learning restrictions relate in each setting.
It is yet not clear, whether delayability is the right assumption to generalize Lemma 7.
Consult the survey by Zeugmann and Zilles [2008] and the standard textbook 30 Martin Aschenbach, Timo K¨otzing, and Karen Seidel by Jain, Osherson, Royer, and Sharma [1999] for more results in the setting of function learning which may transfer to learning collections of languages from informants with such a generalization.
According to Osherson, Stob, and Weinstein [1986] requiring the learner to base its conjecture only on the previous one and the current datum, makes Lim- learning harder.
While the relations between the delayable learning restrictions for these so called iterative learners in the presentation mode of solely positive information has been investigated by Jain, K¨otzing, Ma, and Stephan [2016], so far this has not been done when learning from informants.
For indexable families, this was already of interest to Lange and Grieser [2003] and may oﬀer surprising observations.
For automatic structures as alternative approach to model a learner, there have already been investigations on how diﬀerent types of text aﬀect the explanatory learnability, see Jain, Luo, and Stephan [2010] and H¨olzl, Jain, Schlicht, Seidel, and Stephan [2017].
The latter started investigating how learning from canonical informants and learning from text relate to one another in the automatic setting.
A natural question seems to be what eﬀect other kinds of informants and learning success criteria have.
Last but not least, rating the model’s value for other research aiming at under- standing the capability of human and machine learning seems the most challeng- ing task to tackle.
This work was supported by the German Research Foundation (DFG) under Grant KO 4635/1-1 (SCL).
Bibliography Y.
Akama and T.
Zeugmann.
Consistent and coherent learning with δ-delay.
Information and Computation, 206(11):1362–1374, 2008.
D.
Angluin.
Inductive inference of formal languages from positive data.
Infor- mation and control, 45(2):117–135, 1980.
G.
Baliga, J.
Case, W.
Merkle, F.
Stephan, and W.
Wiehagen.
When unlearning helps.
Information and Computation, 206:694–709, 2008.
J.
B¯arzdi¸nˇs.
Two theorems on the limiting synthesis of functions.
In Theory of Algorithms and Programs, Latvian State University, Riga, 210:82–88, 1974.
J.
B¯arzdi¸nˇs.
Inductive inference of automata, functions and programs.
In Amer.
Math.
Soc.
Transl., pages 107–122, 1977.
J.
B¯arzdi¸nˇs and K.
Podnieks.
The theory of inductive inference.
In Mathematical Foundations of Computer Science, 1973.
L.
Blum and M.
Blum.
Toward a mathematical theory of inductive inference.
Information and Control, 28:125–155, 1975.
L.
Carlucci, J.
Case, S.
Jain, and F.
Stephan.
Non-U-shaped vacillatory and team learning.
Journal of Computer and System Sciences, 74:409–430, 2008.
J.
Case.
The power of vacillation in language learning.
SIAM Journal on Com- puting, 28(6):1941–1969, 1999.
J.
Case.
Gold-style learning theory.
In Topics in Grammatical Inference, pages 1–23.
2016.
J.
Case and T.
K¨otzing.
Dynamically delayed postdictive completeness and consistency in learning.
In Proc.
of ALT (Algorithmic Learning Theory), pages 389–403, 2008.
J.
Case and T.
K¨otzing.
Diﬃculties in forcing fairness of polynomial time in- ductive inference.
In Proc.
of Algorithmic Learning Theory, pages 263–277, 2009.
J.
Case and C.
Lynes.
Machine inductive inference and language identiﬁcation.
In Proc.
of ICALP (International Colloquium on Automata, Languages and Programming), pages 107–115, 1982.
J.
Case and S.
Moelius.
Optimal language learning from positive data.
Infor- mation and Computation, 209:1293–1311, 2011.
J.
Case and C.
Smith.
Comparison of identiﬁcation criteria for machine inductive inference.
Theoretical Computer Science, 25(2):193–220, 1983.
M.
Fulk.
A Study of Inductive Inference Machines.
PhD thesis, SUNY at Buﬀalo, 1985.
M.
Fulk, S.
Jain, and D.
Osherson.
Open problems in Systems That Learn.
Journal of Computer and System Sciences, 49(3):589–604, December 1994.
E.
Gold.
Language identiﬁcation in the limit.
Information and Control, 10: 447–474, 1967.
R.
H¨olzl, S.
Jain, P.
Schlicht, K.
Seidel, and F.
Stephan.
Automatic learning from repetitive texts.
In Proc.
of Algorithmic Learning Theory, pages 129–150, 2017.
32 Martin Aschenbach, Timo K¨otzing, and Karen Seidel S.
Jain, D.
Osherson, J.
Royer, and A.
Sharma.
Systems that Learn: An Intro- duction to Learning Theory.
MIT Press, Cambridge, Massachusetts, second edition, 1999.
S.
Jain, Q.
Luo, and F.
Stephan.
Learnability of automatic classes.
In LATA, pages 321–332, 2010.
S.
Jain, T.
K¨otzing, J.
Ma, and F.
Stephan.
On the role of update constraints and text-types in iterative learning.
Information and Computation, 247:152–168, 2016.
K.
Jantke and H.
Beick.
Combining postulates of naturalness in inductive infer- ence.
Humboldt-Universit¨at zu Berlin.
Sektion Mathematik, 1980.
K.
P.
Jantke.
Monotonic and nonmonotonic inductive inference of functions and patterns.
In Nonmonotonic and Inductive Logic, 1st International Workshop, Proc., pages 161–177, 1991.
T.
K¨otzing.
Abstraction and Complexity in Computational Learning in the Limit.
PhD thesis, University of Delaware, 2009.
T.
K¨otzing and R.
Palenta.
A map of update constraints in inductive inference.
In Algorithmic Learning Theory, pages 40–54, 2014.
T.
K¨otzing, M.
Schirneck, and K.
Seidel.
Normal forms in semantic lan- guage identiﬁcation.
In Proc.
of Algorithmic Learning Theory, pages 493–516.
PMLR, 2017.
S.
Lange and G.
Grieser.
Variants of iterative learning.
Theoretical computer science, 292(2):359–376, 2003.
S.
Lange and T.
Zeugmann.
Monotonic versus non-monotonic language learning.
In Proc.
of Nonmonotonic and Inductive Logic, pages 254–269, 1993.
S.
Lange and T.
Zeugmann.
Characterization of language learning from in- formant under various monotonicity constraints.
Journal of Experimental & Theoretical Artiﬁcial Intelligence, 6(1):73–94, 1994.
S.
Lange, T.
Zeugmann, and S.
Kapur.
Monotonic and dual monotonic language learning.
Theoretical Computer Science, 155(2):365–410, 1996.
S.
Lange, T.
Zeugmann, and S.
Zilles.
Learning indexed families of recursive languages from positive data: A survey.
Theoretical Computer Science, 397 (1):194–232, 2008.
P.
Odifreddi.
Classical Recursion Theory, volume II.
Elsivier, Amsterdam, 1999.
D.
Osherson and S.
Weinstein.
Criteria of language learning.
Information and Control, 52:123–138, 1982.
D.
Osherson, M.
Stob, and S.
Weinstein.
Learning strategies.
Information and Control, 53:32–51, 1982.
D.
Osherson, M.
Stob, and S.
Weinstein.
Systems that Learn: An Introduc- tion to Learning Theory for Cognitive and Computer Scientists.
MIT Press, Cambridge, Mass., 1986.
L.
Pitt.
Inductive inference, DFAs, and computational complexity.
In Proc.
of AII (Analogical and Inductive Inference), pages 18–44, 1989.
H.
Rogers.
Theory of Recursive Functions and Eﬀective Computability.
McGraw Hill, New York, 1967.
Reprinted, MIT Press, 1987.
J.
Royer and J.
Case.
Subrecursive Programming Systems: Complexity and Suc- cinctness.
Research monograph in Progress in Theoretical Computer Science.
Birkh¨auser Boston, 1994.
Learning Families of Formal Languages from Informants 33 G.
Sch¨afer-Richter.
¨Uber Eingabeabh¨angigkeit und Komplexit¨at von Inferenzs- trategien, 1984.
Dissertation, RWTH Aachen.
K.
Wexler and P.
Culicover.
Formal Principles of Language Acquisition.
MIT Press, Cambridge, Massachusetts, 1980.
R.
Wiehagen.
A thesis in inductive inference.
In Nonmonotonic and Inductive Logic, 1st International Workshop, Proc., pages 184–207, 1991.
T.
Zeugmann and S.
Zilles.
Learning recursive functions: A survey.
Theoretical Computer Science, 397:4–56, 2008.

system security analysis with Guided Dropout Benjamin Donnot‡ †∗, Isabelle Guyon‡•, Marc Schoenauer‡, Antoine Marot†, Patrick Panciatici† ‡ UPSud and Inria TAU, Université Paris-Saclay, France.
• ChaLearn, Berkeley, California.
† RTE France.
Abstract.
We propose a new method to eﬃciently compute load-ﬂows (the steady-state of the power-grid for given productions, consumptions and grid topology), substituting conventional simulators based on diﬀer- ential equation solvers.
We use a deep feed-forward neural network trained with load-ﬂows precomputed by simulation.
Our architecture permits to train a network on so-called “n-1” problems, in which load ﬂows are evalu- ated for every possible line disconnection, then generalize to “n-2” problems without re-training (a clear advantage because of the combinatorial nature of the problem).
To that end, we developed a technique bearing similarity with “dropout”, which we named “guided dropout”.
1 Background and motivations Electricity is a commodity that consumers take for granted and, while govern- ments relaying public opinion (rightfully) request that renewable energies be used increasingly, little is known about what this entails behind the scenes in additional complexity for Transmission Service Operators (TSOs) to operate the power transmission grid in security.
Indeed, renewable energies such as wind and solar power are less predictable than conventional power sources (mainly thermal power plants).
A power grid is considered to be operated in “security” (i.e. in a secure state) if it is outside a zone of “constraints”, which includes that power ﬂowing in every line does not exceed given limits.
To that end, it is standard practice to operate the grid in real time with the so-called “n-1” criterion: this is a preventive measure requiring that at all times the grid would remain in a safe state even if one component (generators, lines, transformers, etc.) were disconnected.
Today, the complex task of dispatchers, which are highly trained engineers, consists in analyzing situations and checking prospectively their eﬀect using sophisticated (but slow) high-end simulators.
As part of a larger project to assist TSOs in their daily operations [3], our goal in this paper is to emulate the power grid with a neural network to provide fast estimations of power ﬂows in all lines given some “injections” (electricity productions and consumptions).
2 The guided dropout method Due to the combinatorial nature of changes in power grid topology, it is imprac- tical (and slow) to train one neural network for each topology.
Our idea is to ∗Benjamin Donnot corresponding authors: benjamin.donnot@inria.fr train a single network with architecture variants to capture all elementary grid topology variants (occurring either by willful or accidental line disconnections) around a reference topology for which all lines are in service.
We train simultane- ously on samples obtained with the reference topology and elementary topology changes, limited to one line disconnection (“n-1” cases), which are encoded in the neural network by activating “conditional” hidden units.
Regular generalization is evaluated by testing the neural network with additional {injections, power ﬂows} input/output pairs for “n-1” cases.
We also evaluate super-generalization for “n-2” cases, in which a pair of lines is disconnected, by activating simultane- ously the corresponding conditional hidden units (though the network was never trained on such cases).
While our work was inspired by “dropout” [6] and relates to other eﬀorts in the literature to learn to “sparsify”[2] to increase network capacity without increasing computational time (used in automatic translation [5]) and to “mixed models” (used e.g. for person identiﬁcation [7] or source identiﬁcation [4]), we believe that our idea to encode topological changes in the grid in the network architecture is novel and so is the type of application that we address.
3 Baseline methods We compared the performance of our proposed Guided Dropout (GD) method with multiple baselines (Figure 1): One Model: One neural network is trained for each grid topology.
One Var (OV): One single input variable encodes which line is disconnected (0 for no line disconnected, 1 for line 1 is disconnected, 2 for line 2, etc.) One Hot (OH): If n is the number of lines in the power grid, n extra binary input variables are added, each one coding for connection/disconnection.
DC approximation: A standard baseline in power systems.
This is an approximation of the AC (Alternative Current) non-linear powerﬂow equations.
One Model is a brute force approach that does not scale well with the size of the power grid.
A power grid with n lines would require training n(n−1)/2 neural networks to implement all “n-2” cases.
One Variable is our simplest encoding allowing us to train one network for all “n-1” cases.
However, it does not allow us to generalize to “n-2” cases.
One Hot is a reference architecture, which allows us to generalize to “n-2” cases.
The DC approximation of power ﬂows neglects reactive power and permits to compute relatively fast an approximation of power ﬂows using a matrix inversion, given a detailed physical model of the grid.
See our supplemental material for all details on neural network architectures and DC calculations.
To conduct a fair comparison towards this DC baseline, we use no more input variables than active power injections.
However we should perform even better if we were using additional variables such as voltages as used in AC power ﬂow, not considering them constant as in this DC approximation1.
1A supplemental material will be available at https://hal.archives-ouvertes.fr/hal- 01649938v2 or in the github repository FPSSA-GuidedDropout.
Fig.
1: Neural network architectures being compared.
We overlay all types of architectures under consideration.
Lines denoting trainable parameters are not present in all models, as indicated in the legend.
Biases not repre- sented.
The injection inputs are split into two submodules for productions and consumptions.
4 Experiments Our goal in this section is to demonstrate empirically that multiple grid topolo- gies can be modeled with a single neural network with the purpose of provid- ing fast current ﬂow predictions (measured in Amps), for given injections and given topologies, to anticipate whether some lines might exceed their ther- mal limit should a contingency occur.
We show a phenomenon that we call super-generalization: with the “guided dropout” topology encoding, a neural network, trained with “n-1” topology cases only, generalizes to “n-2” cases.
We demonstrate that our approach would be computationally viable on a grid as large as the French Extra High Voltage power grid.
We ﬁrst conducted systematic experiments on small size benchmark grids from Matpower [8], a library commonly used to test power system algorithms [1].
We report results on the largest case studied: a 118-node grid with n = 186 lines.
We used all 187 variants of grid topologies with zero or one disconnected line (“n-1” dataset) and randomly sampled 200 cases of pairs of disconnected lines (“n-2” dataset), out of 186∗ 185/2.
Training and test data were obtained by generating for each topology considered 10,000 input vectors (including active and reactive injections).
To generate semi-realistic data, we used our knowledge of the French gri, to mimic the spatio-temporal behavior of real data [3].
For example, we enforced spatial correlations of productions and consumptions and mimicked production ﬂuctuations, which are sometimes disconnected for main- tenance or economical reasons.
Target values were then obtained by computing (a) Regular generalization.
(b) Super-generalization.
Fig.
2: Mini grid of 118 buses (nodes).
We show the L2 error in Amperes on a log scale as a function of training epochs.
The neural network in both cases is trained for all “n-1” cases with multiple examples of injections.
(a) Regular generalization.
Test set made of (all) test injections for “n-1” cases.
(b) Super-generalization.
Test set made of a subset of test injections for “n-2” cases.
Error bars are 25-75% quantiles over 10 runs having converged.
resulting ﬂows in all lines with the AC power ﬂow simulator Hades2,2.
This resulted in a “n-1” dataset of 1, 870, 000 samples (we include in the “n-1” dataset samples for the reference topology) and a “n-2” dataset of 2, 000, 000 samples.
We used 50% of the “n-1” dataset for training, 25% for hyper-parameter selection, and 25% for testing.
All “n-2” data were used solely for testing.
In all experiments, input and output variables were standardized.
We opti- mized the “L2 error” (mean-square error) using the Adam optimizer of Tensor- ﬂow.
Figure 2 shows generalization and super-generalization learning curves for the various methods.
For reasons given above, super-generalization can only be achieved by One Hot and Guided Dropout, which explains that Figure 2-b has only two curves.
The DC approximation is represented as a horizontal dashed line (since it does not involve any training).
The test error is represented in a log scale.
Hence, it can be seen in Figure 2-a that neural networks are very pow- erful at making load ﬂow predictions since the “One Model” approach (yellow curve) outperforms the DC approximation by an order of magnitude.3 However the “One Model” approach is impractical for larger grid sizes.
The “One Hot” approach is signiﬁcantly worse than both “Guided Dropout” and “DC approx- imation”.
Of all neural network approaches, “Guided Dropout” gives the best results, and it beats the DC approximation for “n-2” cases (super-generalization).
The super-generalization capabilities of neural networks trained with “Guided Dropout” are obtained by combining in a single network “shared” units trained 2A freeware version of Hades2 is available at http://www.rte.itesla-pst.org/ 3We note in the yellow curve some slight over-ﬁtting as evidenced by the test error increase after 10 training epochs, which could be alleviated with early stopping.
050100150200250300Epoch2.01.51.00.50.0L2 error (log10.
scale)L2 error on n-1 dataset (training on n-1)DC approxOne ModelOne VarOne hot*G.
Dropout050100150200250300Epoch1.00.80.60.40.20.0L2 error (log10.
scale)L2 error on n-2 dataset (training on n-1)DC approx.One hot*G.
Dropoutwith all available data for many similar (yet diﬀerent) grid topologies and spe- cialized units activated only for speciﬁc topologies.
This economy of resources, similar in spirit to weight sharing in convolutional neural networks, performs a kind of regularization.
Obtaining a good performance on new “unseen” grid topologies (not available for training) is the biggest practical advantage of “Guided Dropout”: Acquiring data to train a model for all “n-2” cases for the Extra High Voltage French power grid (counting (cid:39) 1, 400 nodes, (cid:39) 2, 700 lines) would require computing (cid:39) 50 million power ﬂow simulations, which would take almost half a year, given that computing a full AC power ﬂow simulation takes about 300 ms for RTE current production software.
Conversely, RTE stores almost all “n-1” cases as part of “security analyses” conducted every 5 minutes, so the “n-1” dataset is readily available.
To check whether our method scales up to the size of a real power transmis- sion grid, we conducted preliminary experiments using real data of the French Extra High Voltage power grid with over 1000 nodes.
To that end, we extracted grid state data from September 13th 2011 to October 20th 2014.
This represents: 281, 543 grid states, 2735 lines, 297 generations units and 1203 individual loads (accounting for dynamic node splitting, this represented between 1400 − 1600 power nodes in that period of time).
We did not simulate line disconnections in these preliminary experiments (as we would normally do to apply our method).
This is strictly an evaluation of computational performance at run time.
We trained a network with the following dimensions (its architecture remains to be optimized): 400 units in the (ﬁrst) encoder layer, 2735 conditional units in the second (guided dropout) layer, and 400 units in the last (decoder) layer.
Training takes of the order of one day.
With this architecture, when the data are loaded in the computer RAM memory, we are able to perform more than 1000 load-ﬂows per second, which would enables to compute 300 more load-ﬂows in the same amount of time than current AC power ﬂow simulators.
5 Conclusions and future work Our comparison of various approaches to approximate “load ﬂows" using neural networks has revealed the superiority of “Guided Dropout”.
This novel method we introduced allows us to train a single neural network to predict power ﬂows for variants of grid topology.
Speciﬁcally, when trained on all variants with one single disconnected line (“n-1” scenarios), the network generalizes to variants with TWO disconnected lines (“n-2” scenarios).
Given the combinatorial nature of the problem, this presents signiﬁcant computational advantages.
Our target application is to pre-ﬁlter serious grid contingencies such as com- binations of line disconnections that might lead to equipment damage or service discontinuity.
We empirically demonstrated on standard benchmarks of AC power ﬂows that our method compares favorably with several reference baseline methods including the DC approximation, both in terms of predictive accuracy and computational time.
In daily operations, only “n-1” situations are examined by RTE because of the computational cost of AC simulations.
“Guided Dropout” would allow us to rapidly pre-ﬁlter alarming “n-2” situations, and then to further investigate them with AC simulation.
Preliminary computational scaling simu- lations performed on the Extra High Voltage French grid indicate the viability of such hybrid approach: A neural network would be (cid:39) 300 times faster than the currently deployed AC power ﬂow simulator.
Given that our new method is strictly data driven – no knowledge used of the physics of the system (e.g. reactance, resistance or admittance of lines) or the topology of grid –, the empirical results presented in this paper are quite encouraging, since the DC approximation DOES make use of such knowledge.
This prompted RTE management to commit additional eﬀorts to pursue this line of research.
Further work will include incorporating such prior knowledge.
The ﬂip side of using a data driven method (compared to the DC approxima- tion) is the need for massive amounts of training data, representative of actual scenarios or situations, in an ever changing environment, and the loss of explain- ability of the model.
The ﬁrst problem will be addressed by continuously ﬁne tuning/adapting our model.
To overcome the second one, we are working on the theoretical foundations of the method.
A public version of our code that we will release on Github is under preparation.
References [1] O Alsac and B Stott.
Optimal load ﬂow with steady-state security.
IEEE transactions on power apparatus and systems, (3):745–751, 1974.
[2] Y.
Bengio and et al.
Estimating or propagating gradients through stochastic neurons for conditional computation.
arXiv:1308.3432, 2013.
[3] B.
Donnot and et al.
Introducing machine learning for power system opera- tion support.
In IREP Symposium, Espinho, Portugal, August 2017.
[4] S.
Ewert and M.
B.
Sandler.
Structured dropout for weak label and multi- instance learning and its application to score-informed source separation.
In Proc.
ICASSP, pages 2277–2281, 2017.
[5] N.
Shazeer and et al.
Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.
arXiv:1701.06538, 2017.
[6] N.
Srivastava and et al.
Dropout: a simple way to prevent neural networks from overﬁtting.
JMLR, 15(1):1929–1958, 2014.
[7] T.
Xiao and et al.
Learning deep feature representations with domain guided dropout for person re-identiﬁcation.
In Proc.
CVPR, pages 1249–1258, 2016.
[8] R.
D.
Zimmerman and et al.
Matpower.
IEEE Trans.
on Power Systems, pages 12–19, 2011.

As deep learning penetrates more and more application areas, there is a natural demand to adapt deep learning techniques to area and task-speciﬁc requirements and constraints.
An immediate consequence of this is the expectation to perform well with respect to task-speciﬁc performance measures.
However, this can be challenging, as these performance measures can be quite complex in their structure and be motivated by legacy, rather than algorithmic convenience.
Examples include the F-measure that is popular in retrieval tasks, various ranking performance measures such as area-under-the-ROC-curve, and the Kullback-Leibler divergence that is popular in class-ratio estimation problems.
Optimizing these performance measures across application areas has proved to be challenging even when learning linear models, as is evidenced by the recent surge in progress in optimizing “non-decomposable” loss functions for learning linear models, as we review in Section 2.
The challenge becomes doubly hard when ∗amartya18x@gmail.com †kpawan@cse.iitk.ac.in ‡purushot@cse.iitk.ac.in §schawla@qf.org.qa ¶fsebastiani@gmail.com trying to do so while training neural network architectures such as multi-layer perceptrons and convolutional or recurrent neural networks.
The vast majority of training techniques used for neural network at present consist of using simple per-sample loss functions such as least-squares loss or cross-entropy.
While their use has allowed research directions to focus more on developing more evolved network architectures, as well as developing highly optimized implementations of training routines on GPU architectures, we show that this is suboptimal and that a sound eﬀort towards training with task-speciﬁc loss functions pays oﬀ handsomely.
Our Contributions Our work advances the state-of-the-art in training neural networks on a wide variety of non-decomposable performance measures.
1.
We show how to train neural networks directly with respect to performance measures that are concave, pseudolinear, or nested concave functions.
2.
Our algorithms are readily adapted to neural architectures such as multi-layered perceptrons and recurrent networks, as well be integrated into popular symbolic gradient frameworks such as Theano, TensorFlow, and PyTorch.
3.
Our methods oﬀer far superior performance than traditional cross-entropy based training routines – on an F-measure maximization task on a benchmark dataset a9a, our method achieves an F-measure of around 0.68 in less than 10 mini-batch iterations whereas it takes traditional cross-entropy based training more than 80 iterations to reach similar performance levels.
4.
Our methods also outperform recently proposed techniques for training deep networks with ranking performance [17].
On a benchmark dataset IJCNN, the technique of Song et al.
is only able to oﬀer a min-TPR/TNR performance of around 0.55 whereas our technique is able to reach performance over 0.95 in very few iterations.
5.
We apply our techniques to an end-to-end sentimental analysis quantiﬁcation network and achieve near perfect quantiﬁcation scores on a challenge dataset using a substantially less number of training iterations.
6.
We oﬀer formal stabilization guarantees for all our algorithms.
2 Related Work The recent years have seen much interest, as well as progress, in training directly with task-speciﬁc per- formance measures in the ﬁeld of classiﬁcation and ranking.
Some notable works include those of [10, 15] that investigate the statistical properties of plug-in classiﬁers for various non-decomposable objectives in- cluding F-measure, and [7, 8, 12, 13] which propose stochastic gradient-style algorithms for optimizing non-decomposable performance measures such as F-measure, KL-divergence, area under the ROC curve (AUC), precision recall curve (AUCPR), recall at ﬁxed precision (R@P), etc.
However, all the works cited above focus only on training linear models.
Although this allows for simple algorithms for which the works provide very detailed analyses and theoretical guarantees, the approaches do not directly extend to deep networks.
Algorithms for deep learning which directly optimize non-decomposable performance measures are relatively unexplored.
This can be attributed to the de-facto use of the backprop- agation algorithm for training neural networks which crucially depends on the loss function being decom- posable.
We are aware of two signiﬁcant eﬀorts towards training deep networks with non-decomposable perfor- mance measures.
Below we discuss both to put our contributions in perspective.
1.
Song et.
al.
[17] introduce an algorithm for training neural networks for ranking tasks with the average precision as the performance measure.
The most key contribution of [17] is a result that shows that for nicely behaved non-decomposable loss functions, the expected gradient of the loss function with respect to the network weights can be expressed in terms of standard decomposable loss functions such as cross-entropy and least squares loss.
2.
Eban et.
al.
[3] introduce algorithms for optimizing ranking objectives e.g. area under the precision- recall curve and precision at a ﬁxed recall rate.
Both the works above are focussed on ranking measures whereas our work addresses classiﬁcation and class-ratio estimation (quantiﬁcation) measures.
The applications of classiﬁcation are various in machine learning and data analysis.
The problem of quantiﬁcation expects accurate estimation of relative prevalence of class labels (e.g. fraction of positive vs negative reviews) and is useful in social engineering and epidemiology.
The work of [17] only considers average precision as the performance measure and does not address performance measures we study such as F-meaure and KL divergence.
Moreover, we adapted the method proposed in [17] to performance measures we study and our experiments show that our precise primal dual techniques far outperform the method of [17].
Although the work of [3] does consider the F-measure which we also study, they do not report any exper- imentation with F-measure.
A possible reason for this might be that their algorithm requires a constrained optimization problem to be solved that is challenging over deep networks.
We, on the other hand, provide very generic methods for solving three classes of performance measures which include a large number of widely used measures e.g. H-mean, G-mean, Jaccard coeﬃcient, Q-measure etc which [3] cannot handle.
Furthermore, neither of [17, 3] oﬀer any convergence guarantees for their proposed algorithms whereas we do oﬀer stabilization and ﬁrst order stationarity guarantees for our methods.
As a concluding remark, we note that our methods do adapt techniques that were earlier proposed for training linear models, such as in [14].
However our work diﬀers from existing works, including [14], in a signiﬁcant manner and constitutes an independent contribution.
Previous works, such as [14] only consider linear models which lead to convex problems.
We note later in this paper, that a naive and direct application of existing techniques to deep networks yields poor results.
The techniques in [14] cannot be integrated into modern deep learning frameworks like Theano, TensorFlow, PyTorch in scalable manner.
Our techniques show how to do so.
Moreover, we also provide formal stationarity guarantees for our algorithms when applied to deep networks that [14] cannot provide since they crucially assume convexity of their problems.
3 Problem Setting For sake of simplicity, we restrict ourselves to binary classiﬁcation problems.
Let X ⊂ Rd be the space of feature vectors and Y = {−1, +1} be the label set.
The training data set S shall be sampled i.i.d. from some ﬁxed but unknown distribution D over X × Y.
The proportion of positives in the population and sample S will be denoted by p = P (x,y)∼D [y = +1] and ˆpS respectively.
In sharp contrast to most previous work in multivariate optimization that considers only linear models, we concentrate on non-linear models, especially those induced by deep neural networks.
We will assume that the neural architecture (number of layers, nodes, activation functions and connectivity) has been ﬁxed and let W denote the space of all models (weights on the network edges).
To perform learning, we will use a neural model, whose edge weights are indexed by w ∈ W, to assign a score to every data point x ∈ X (that can be converted into labels, class probability estimates etc).
Linear models typically assign a score by simply computing (cid:104)w, x(cid:105).
However, we will use a more general notation f (x; w) to denote the score given to the data point x by the neural model indexed by the weights w.
The function f can be seen as encoding all the neural connections and activations.
We stress that the function f is, in general, neither convex nor concave.
We note that this lack of structure in the scoring function precludes a large body of work in linear multivariate optimization and quantiﬁcation from being applied to deep models.
We will consider performance measures that can be expressed in terms of the true positive rate (TPR) and true negative rate (TNR) of the model.
Since TPR and TNR are count-based measures, they are unsuitable for numerical optimization algorithms.
For this reason, we consider the use of reward functions as surrogates Table 1: List of performance measures Ψ(P, N ) where p, n denote the TPR and TNR values obtained by the model.
Name Type Min [18] Q-Mean [9] Concave Concave Fβ [11] KLD [1] Pseudolinear Nested Concave Expression (P, N ) 1 −(cid:113) (1−P )2+(1−N )2 min{P, N} (1+β2)·P β2+n/p+P−n/p·N see text of the TPR and TNR values.
A reward function r assigns a reward r(ˆy, y) when the true label is y ∈ Y but the prediction is ˆy ∈ R.
Given a reward function r, a model w ∈ W, data point (x, y) ∈ X × Y, and scoring function f , we will use r+(w; x, y) = r−(w; x, y) = 1 − p · r(f (x; w), y) · I{y = 1} · r(f (x; w), y) · I{y = −1} to calculate rewards on positive and negative points (I{·} denotes the indicator function).
The expected value of these rewards will be treated as surrogates of TPR and TNR.
Note that since E [r+(w; x, y)] = E [r(f (x; w), y)|y = 1], setting r0-1(ˆy, y) = I{y · ˆy > 0} i.e. classiﬁcation accuracy as the reward function yields E [r+(w; x, y)] = TPR(w).
We will use the shorthand P (w) = E [r+(w; x, y)] to denote population averages of the reward function and, given a sample of n data points S = {(x1, y1), .
.
.
, (xn, yn)}, denote the sample average as ˆPS(w) = 1 i=1 r+(w; xi, yi) and similarly deﬁne N (w), ˆNS(w).
Unlike previous work [7, 14], we will not restrict ourselves to concave surrogate reward functions.
In particular we will utilize the sigmoidal reward, which is widely used as an activation function in neural networks is non-concave: rsigmoid(ˆy, y) = (1 + exp(−y · ˆy))−1 (cid:80)n 3.1 Performance Measures We will consider three general classes of performance measures, namely, (i) Concave Performance Measures, (ii) Pseudo-linear Performance Measures and (iii) Nested Concave Performance Measures.
In our experi- ments, we present results on a selection of these performance measures which are listed in Table 1.
Concave Performance Measures: These measures can be written as a concave function of the TPR and TNR values: PΨ(w) = Ψ (TPR(w), TNR(w)) for some concave link function Ψ : R2 → R.
These measures are frequently used for cost-sensitive classiﬁ- cation in cases with severe label imbalance, for example detection theory [18].
A popularly used member of this family is the so-called Min-function assigns the value min{TPR(w), TNR(w)} to a model w.
Note that this compels the model to pay equal attention to both classes.
Other examples include the Q-mean and H-mean measures.
Pseudo-linear Performance Measures: These measures can be written as a ratio of two linear functions of the TPR and TNR values of the model, i.e. they have a fractional linear link function.
More speciﬁcally, given given coeﬃcients a, b ∈ R3, P(a,b)(w) = a0 + a1 · TPR(w) + a2 · TNR(w) b0 + b1 · TPR(w) + b2 · TNR(w) The popularly used F-measure [11] is actually a pseudo-linear performance measure in terms of the TPR, TNR values of a model although it is more commonly represented as the harmonic mean of precision and recall.
Other members include the Jaccard coeﬃcient and the Gower-Legendre measure.
Nested Concave Performance Measures: Recent works e.g. [1, 7] in problem areas such as quantiﬁcation and class ratio estimation problems, have brought focus on performance measures that can be written as concave combinations of concave performance measures.
More formally, given three concave functions Ψ, ζ1, ζ2 : R2 → R, we deﬁne a performance measure P(Ψ,ζ1,ζ2)(w) = Ψ(ζ1(w), ζ2(w)), where ζi(w) := ζi(TPR(w), TNR(w)), i = 1, 2.
A widely used measure for quantiﬁcation tasks is the KLD: Kullback-Leibler Divergence [1, 5, 6] which can be shown to be a sum of concave functions of the TPR and TNR.
If p ∈ R2 is the vector of true class priors for a binary classiﬁcation task and ˆp an estimate thereof, then KLD(p, ˆp) = p(y) log p(y) ˆp(y) (1) (cid:88) y∈Y KLD(p, ˆp) = 0 indicates perfect quantiﬁcation.
We note that there are several other performance measures that our techniques can handle but which we do not discuss here due to lack of space.
These include measures for class-imbalanced classiﬁcation such as H-mean, G-mean, Jaccard coeﬃcient (see [14]), as well as quantiﬁcation measures such as Q-measure, NSS and CQB (see [7]).
4 Deep Optimization Algorithms The task of training deep models directly for quantiﬁcation performance measures requires us to address the problem of optimizing the concave, nested concave, and pseudolinear performance measures we discussed in Section 3.1 which is challenging due to several reasons: 1) these measures are non-decomposable and do not lend themselves to straightforward training methods such as gradient descent or backpropagation, 2) deep models oﬀer no convenience of convexity, and 3) existing methods for optimizing such measures e.g. [7, 13] fail to apply directly to deep models.
In fact, we will see in Section 5 that direct application of traditional techniques yields poor results.
This section will show how to overcome these challenges to arrive at scalable methods for training deep networks directly on complex non-decomposable measures.
A very desirable trait of our methods is that they all enjoy local convergence guarantees.
Our techniques also oﬀer far superior empirical performance as compared to typical training methods for deep models.
In the following, the procedure NN-init(din, dout, conf) initializes a neural network with din input nodes, dout output nodes, and internal conﬁguration (hidden layers, number of internal nodes, connectivity) speciﬁed by conf.
4.1 DUPLE: A Deep Learning Technique for Concave Performance Measures We present DUPLE (Algorithm 1), a highly scalable stochastic mini-batch primal dual algorithm for training deep models with concave performance measures.
We shall ﬁnd it convenient to deﬁne the (concave) Fenchel conjugate of the link functions for our performance measures.
For any concave function Ψ and α, β ∈ R, deﬁne Ψ∗(α, β) = inf By the concavity of Ψ, we have, for any u, v ∈ R, u,v∈R{αu + βv − Ψ(u, v)} .
(2) (3) Ψ(u, v) = inf α,β∈R{αu + βv − Ψ∗(α, β)} .
Algorithm 1 DUPLE: Dual UPdates for Learning dEep-models Require: Primal step sizes ηt, network conﬁguration {din, conf}, batch size b 1: w0 ← NN-init(din, 1, conf) 2: (cid:8)α0, β0, r+, r−, n+, n−(cid:9) ← 0 3: for t = 1, 2, .
.
.
, T do 4: i )}i=1,...,b i, yt St ← SAMPLE mini-batch of b data points {(xt wt ← wt−1 + ηt · ∇wg(wt; St, αt−1, βt−1) r+ ← r+ + 1 r− ← r− + 1 n+ ← n+ + 1 n− ← n− + 1 (αt, βt) ← arg min (cid:80)b (cid:80)b (cid:80)n i=1 r+(wt; xt (cid:80)n i=1 r−(wt; xt (cid:20) − Ψ∗(α, β) i, yt i ) i, yt i ) i = +1} I{yt i = −1} I{yt r− r+ + β n− n+ (cid:21) i=1 i=1 5: 6: 7: 8: 9: 10: (cid:46) Primal Step (cid:46) Tot.
reward on +ves (cid:46) Tot.
reward on -ves (cid:46) Total # positives (cid:46) Total # negatives (cid:46) Dual Step (α,β) 11: end for 12: return wT The motivation for DUPLE comes from a realization that by an application of the Danskin’s theorem, a gradient with respect to the Ψ function may be found out by obtaining the maximizer α, β values in (3).
However since this may be expensive, it is much cheaper to update these “dual” variables using gradient descent techniques instead.
This results in the DUPLE algorithm, a primal dual stochastic-gradient based technique that maintains a primal model w ∈ W and two dual variables α, β ∈ R and alternately updates both using stochastic gradient steps.
At every time step it uses its current estimates of the dual variables to update the model, then ﬁx the model and update the dual variables.
We note that DUPLE draws upon the SPADE algorithm proposed in [14].
However, its application to deep models requires non-trivial extensions.
1.
SPADE enjoys the fact that gradient updates are very rapid with linear models and is carefree in performing updates on individual data points.
Doing so with neural models is too expensive.
2.
Deep model training frameworks are highly optimized to compute gradients over deep networks, espe- cially on GPU platforms.
However, they assume that the objective function with respect to which they compute these gradients is static across iterations.
SPADE violates this principle since it can be seen as taking gradients with respect to a diﬀerent cost-weighted classiﬁcation problem at every iteration.
3.
The theoretical convergence guarantees oﬀered by SPADE assume that the reward surrogate functions being used are concave functions with respect to the model.
As noted in Section 3, for neural models, even the scoring function f (x; w) is not a concave/convex function of w.
DUPLE addresses all of the above issues and makes crucial design changes that make it highly optimized for use with deep networks.
1.
DUPLE overcomes the issue of expensive gradients by amortizing the gradient computation costs over mini-batches.
We found this to also improve the stability properties of the algorithm.
2.
To overcome the issue of changing objective functions, DUPLE works with an augmented objective function.
Given a model w ∈ W, a set S of labeled data points, and scalars α, β, we deﬁne g(w; S, α, β) = α · ˆPS(w) + β · ˆNS(w).
(see Section 3 for notation).
At all time steps, DUPLE takes gradients with respect to this augmented objective function instead.
We exploit symbolic computation capabilities oﬀered by frameworks such as Theano [2] to allow the scalars α, β to be updated dynamically and train the network eﬃciently on a diﬀerent objective function at each time step.
3.
Our analysis for DUPLE makes absolutely no assumptions on the convexity/concavity of the reward and scoring functions.
It only requires both functions r+, r− to be diﬀerentiable almost-everywhere.
Thus, DUPLE only assumes the bare minimum to allow itself to take gradients.
We are able to show the following convergence guarantee for DUPLE (see Appendix A) assuming that the reward functions r(f (x; w), y) are L-smooth functions of the model w.
This is satisﬁed by all reward functions we consider.
Note, however, that nowhere will we assume that the reward functions are concave in the model parameters.
We will use the shorthand ∇t = ∇wg(wt; St, αt, βt) and F (wt, αt) = g(wt; St, αt, βt).
Notice that this result assures us that the DUPLE procedure will stabilize rapidly and not oscillate indeﬁnitely.
Theorem 1.
Consider a concave performance measure deﬁned using a link function Ψ that is concave and L(cid:48)-smooth.
Then, if executed with a uniform step length satisfying η < 2 L , then DUPLE -stabilizes within (cid:101)O(cid:0) 1 (cid:1) iterations.
More speciﬁcally, within T iterations, DUPLE identiﬁes a model wt such that (cid:18)(cid:113) (cid:19) 2 (cid:107)∇t(cid:107)2 ≤ O L(cid:48) log T 4.2 DENIM: Deep Learning with Nested Concave Performance Measures We extend the DUPLE algorithm to performance measures that involve a nesting of concave functions.
To reiterate, the KLD performance measure which is used extensively for quantiﬁcation, falls in this category.
These measures are challenging to optimize using DUPLE due to their nested structure which prevents a closed form solution for the Fenchel conjugates.
To address this challenge, we present DENIM (Algorithm 2) that itself nests its update to parallel the nesting of the performance measures.
DENIM follows a similar principle as DUPLE and is based on the NEMSIS algorithm of [7].
However, the NEMSIS algorithm faces the same drawbacks as the SPADE algorithm and is unsuitable for training deep models.
Due to the more complex nature of the performance measure, DENIM works with a slightly diﬀerent augmented objective function.
h(w; S, α, β, γ) = (γ1α1 + γ2β1) · ˆPS(w) + (γ1α2 + γ2β2) · ˆNS(w) Note that DENIM performs inner and outer dual updates that are themselves nested.
DENIM enjoys similar convergence results as DUPLE which we omit for lack of space.
4.3 DAME: A Deep Learning Technique for Pseudolinear Performance Mea- sures We now present DAME (Algorithm 3), an algorithm to for training deep models on pseudolinear perfor- mance measures such as F-measure which are extremely popular in several areas and direct optimization routines are sought after.
We recall that although the work of [3] does discuss F-measure optimization, we do not have access to any scalable implementations of the same.
Our algorithm DAME, on the other hand, is based on an alternating strategy, is very scalable and gives superior performance across tasks and datasets.
For sake of simplicity, we represent the pseudolinear performance measure as P(a,b)(w) = Pa(w) Pb(w) a0 + a1 · TPR(w) + a2 · TNR(w) b0 + b1 · TPR(w) + b2 · TNR(w) We now deﬁne the notion of a valuation function.
Deﬁnition 1 (Valuation Function).
The valuation of a pseudolinear measure P(a,b)(w) at any level v > 0, is deﬁned to be V (w, v) = Pa(w) − v · Pb(w) DAME makes use of two simple observations in its operation: 1) A model w has good performance i.e. P(a,b)(w) > v iﬀ it satisﬁes V (w, v) > 0, and 2) the valuation function itself is a performance measure but 1r(wt; xt i, yt i ) = (r+(wt; xt i, yt i ), r−(wt; xt i, yt i )) Algorithm 2 DENIM: A DEep Nested prImal-dual Method Require: Primal step sizes ηt, network conﬁguration {din, conf}, batch size b 1: w0 ← NN-init(din, 1, conf) 2: (cid:8)r0, q0, α0, β0, γ0(cid:9) ← (0, 0) 3: for t = 1, 2, .
.
.
, T do 4: i )}i=1,...,b i, yt St ← SAMPLE mini-batch of b data points {(xt wt ← wt−1 + ηt · ∇wh(wt; St, αt, βt, γt) qt ← (t − 1) · qt−1 + (αt−1 qt ← qt + (αt−1 , βt−1 i=1 r−(wt; xt )(cid:80)b qt ← t−1(cid:0)qt − (ζ∗ rt ← t−1(cid:16) )(cid:80)b 2 (βt))(cid:1) (t − 1) · rt−1 +(cid:80)b , βt−1 1 (αt), ζ∗ (cid:17) 1 i, yt i ) i=1(r(wt; xt i, yt i )) i=1 r+(wt; xt i, yt i ) αt = arg min βt = arg min γt = arg min 1 (α)} {α · rt − ζ∗ {β · rt − ζ∗ 2 (β)} {γ · qt − Ψ∗(γ)} 5: 6: 7: 8: 9: 10: 11: 12: (cid:46) Primal Step (cid:46) Inner Dual Step 1 (cid:46) Inner Dual Step 2 (cid:46) Outer Dual Step 13: end for 14: return wT a decomposable one, corresponding to a cost-weighted binary classiﬁcation problem with the costs given by the weights a, b and v.
We will use the notation P(a,b),S(w) and VS(w, v) to denote respectively, the performance measure, and the valuation function as deﬁned on a data sample S.
At every time step t, DAME looks at vt = P(a,b)(wt) and attempts to approximate the task of optimizing F-measure (or any other pseudolinear measure) using a cost weighted classiﬁcation problem described by the valuation function at level vt.
After updating the model with respect to this approximation, DAME reﬁnes the approximation again, and so on.
We note that similar alternating strategies have been studied in literature in the context of F-measure before [10, 14] and oﬀer provable convergence guarantees for linear models.
However, a direct implementation of these methods gives extremely poor results as we shall see in the next section.
The complex nature of these performance measures, that are neither convex nor concave, make it more challenging to train deep models.
To solve this problem, DAME utilizes a two-stage training procedure, involving pretraining the en- tire network (i.e. both upper and lower layers) on a standard training objective such as cross-entropy or least squares, followed by ﬁne tuning of only the upper layers of the network to optimize F-measure.
The pretraining is done using standard stochastic mini-batch gradient descent.
For sake of simplicity we will let (w1, w2) denote a stacking of the neural networks described by the models w1 and w2.
More speciﬁcally w2 denotes a network with input dimensionality din and output dimensionality dint whereas w1 denotes a network with input dimensionality dint and output dimensionality dout.
To ensure diﬀerentiability, DAME uses valuation functions with appropriate reward functions replacing the TPR and TNR functions.
We are able to show a stronger ﬁrst order stationary convergence guarantee for DAME.
For sake of simplicity, we present the proof for the batch version of the algorithm in Appendix B.
We only assume that the valuation functions are L-smooth functions of the upper model w1.
It is also noteworthy that we present the guarantee only for the ﬁne-tuning phase since the pre-training phase enjoys local convergence guarantees by standard arguments.
For this reason, we will omit the lower network in the analysis.
We also assume that the performance measure satisﬁes Pa(w) ≤ M for all w ∈ W and ·Pb(w) ≥ m for all w ∈ cW .
We note that these assumptions are standard [7, 14] and also readily satisﬁed by F-measure, Jaccard coeﬃcient etc for which we have m, M = Θ(1) (see [14]).
Let κ = 1 + M/m.
Then we have the following result.
Theorem 2.
If executed with a uniform step length satisfying η < 2 Lκ , DAME discovers an -stable model lengths ηt, network conﬁguration , yt,t(cid:48) (cid:46) New features dataset Algorithm 3 DAME: A Deep Alternating Maximization mEthod Require: Training step 1: w−1 2: w−1 3: (w0,0 {din, dint, dout, conf1, conf2}, batch size b 1 ← NN-init(dint, 1, conf1) 2 ← NN-init(din, dint, conf2) 1 , w0 4: Create new dataset ˜T =(cid:8)(f (xi, w0 2) ← Pre-train on cross-entropy on dataset T 2), yi)(cid:9)n {(xi, yi)}n i=1, i=1 5: for t = 1, 2, .
.
.
, T do 6: 7: 8: 9: 10: St,0 ← SAMPLE mini-batch of b data points (zt,t(cid:48) vt ← P(a,b),St,0 (wt−1 for t(cid:48) = 1, 2, .
.
.
, T (cid:48) do , w0 2) St,t(cid:48) ← SAMPLE mini-batch of b data points (zt,t(cid:48) wt−1,t(cid:48) VSt,t(cid:48) ((wt−1,t(cid:48)−1 end for 1 ← wt−1,T (cid:48) wt ← wt−1,t(cid:48)−1 + ηt · ∇ wt−1,t(cid:48)−1 , yt,t(cid:48) , w0 2), vt) 11: 12: 13: end for 14: return (wT 1 , w0 2) Data Set # Points Feat.
Positives KDDCup08 PPI CoverType Letter IJCNN-1 Adult Twitter 100K 240K 580K 20K 140K 50K 10K 117 85 54 16 22 123 NA 0.61% 1.19% 1.63% 3.92% 9.57% 23.93% 77.4% Source KDDCup08 [16] UCI UCI UCI UCI SEMEVAL16 (cid:1) inner iterations.
More speciﬁcally, for t ≤ κ2 within O(cid:0) 1 that(cid:13)(cid:13)∇wP(a,b)(w)(cid:13)(cid:13) ≤ .
2 Table 2: Statistics of data sets used.
η(1− Lκη 2 )2 , DAME identiﬁes a model wt 1 such 5 Experimental Results We performed extensive evaluation of DUPLE, DENIM and DAME on benchmark and real-life challenge datesets and found it to outperform both traditional techniques for training neural networks, as well as the more nuanced task-driven training techniques proposed in the work of [17].
Datasets: We use the datasets listed in Table 2.
Twitter refers to the dataset revealed as a part of the SEMEVAL 2016 sentiment detection challenge [4].
Competing Methods: We implemented and adapated several benchmarks from past literature in an attempt to critically assess the performance of our methods.
1.
ANN 0-1 refers to a benchmark multi-layer perceptron model trained using the cross-entropy loss functions to minimize the misclassiﬁcation rate.
2.
STRUCT-ANN refers to an adaptation of the structured optimization algorithm from [17] to various performance measures (implementation details in the Appendix C).
3.
ANN-PG refers to an implementation of a plug-in classiﬁer for F-measure as suggested in [10].
4.
DENIMS-NS refers to a variant of the NEMSIS algorithm that uses a count based reward instead of sigmoidal rewards.
A similar benchmark was constructed for DUPLE as well.
(a) PPI (b) KDD08 (c) COVT (d) IJCNN Figure 1: Experiments on maximizing MinTPRTNR, a concave performance measure For lack of space, some experimental results for the DAME algorithm are included in Appendix B.
All hyper-parameters including model architecture were kept the same for all algorithms.
Learning rates were optimized to give best results.
5.1 Experiments on Concave Measures The results with DUPLE (Figures 1 and 2) on optimizing the MinTPRTNR and the QMean performance measures, show that DUPLE oﬀers very fast convergence in comparison to ANN 0-1.
It is to be noted that on MinTPRTNR, ANN 0-1 has a very hard time obtaining a non-trivial score.
For the experiment on IJCNN1, we ran the experiment for a longer time to allow ANN 0-1 and STRUCT-ANN to converge and we observe that they are highly time intensive, when compared to DUPLE.
These experiments show that DUPLE and its variant DUPLE-NS outperform the competitors both in terms of speed as well as accuracy.
It is also to be noted that DUPLE not only takes lesser iterations than STRUCT-ANN but each iteration of DUPLE is at least 10X faster than that of STRUCT-ANN.
(a) PPI (b) KDD08 (c) IJCNN1 (d) A9A Figure 2: Experiments on maximizing QMean, a concave performance measure (a) KDD08 (b) COD-RNA (c) LETTER (d) A9A Figure 3: Experiments on maximizing F-measure, a pseudolinear performance measure 10 0510152025303540Iterations0.00.10.20.30.40.50.60.70.8Min TPR TNRppiDUPLEANN-0-1Struct-ANN0510152025303540Iterations0.00.10.20.30.40.50.60.70.80.9Min TPR TNRkdd08DUPLEANN-0-1Struct-ANN05101520Iterations0.00.10.20.30.40.50.60.70.80.9Min TPR TNRcovtypeDUPLEANN-0-1Struct-ANN0100200300400500Iterations0.00.20.40.60.81.0Min TPR TNRijcnn1DUPLEANN-0-1Struct-ANN0510152025303540Iterations0.20.30.40.50.60.70.8QMeanppiDUPLEDUPLE-NSANN-0-1Struct-ANN051015202530Iterations0.20.30.40.50.60.70.80.9QMeankdd08DUPLEDUPLE-NSANN-0-1Struct-ANN01020304050Iterations0.20.30.40.50.60.70.80.91.0QMeanijcnn1DUPLEDUPLE-NSANN-0-1Struct-ANN01020304050Iterations0.20.30.40.50.60.70.80.9QMeana9aDUPLEDUPLE-NSANN-0-1Struct-ANN05101520Iterations0.00.10.20.30.40.50.6F-Measurekdd08DAMEANN01ANN-PGStruct-ANN05101520Iterations0.00.10.20.30.40.50.60.70.80.9F-Measurecod-rnaDAMEANN01ANN-PGStruct-ANN05101520Iterations0.00.10.20.30.40.5F-MeasureletterDAMEANN01ANN-PGStruct-ANN020406080100Iterations0.00.10.20.30.40.50.60.7F-Measurea9aDAMEANN01ANN-PGStruct-ANN5.2 Experiments with Nested Performance Measures In Figure 4, we can see the results obtained by DENIM while optimizing the KLD performance measure.
It shows rapid convergence to near-perfect quantiﬁcation (class ratio estimation) scores.
The experiments also show that DENIM and DENIMS-NS require far less iterations than its competitor ANN 0-1 (whenever ANN 0-1 is successful at all).
The STRUCT-ANN benchmark is not shown for these experiments since it always got a value close to 0 by trivially predicting every data point as negative as the datasets are highly biased.
(a) PPI (b) Letter (c) COVT (d) IJCNN Figure 4: Experiments on minimizing Kullback Leibler divergence, a nested concave performance measure 5.3 Experiments with Pseudolinear Measures Figure 7 (in the Supplementary material) shows the performance of DAME on optimizing the F1-Measure.
A naive training with misclassiﬁcation loss yields extremely poor F-measure performance.
Moreover, plug-in methods such as those proposed in [10] linear models also perform very poorly.
DAME on the other hand is able to rapidly oﬀer very good F-measure scores after looking at a fraction of the total data.
As, it can be seen, STRUCT-ANN oﬀers a consistently poor performance whereas it seems to perform well in the linear case.
This is because our implementation of the Struct ANN is a minibatch method and the gradient obtained from the structual method has almost no real information due to this.
In other variants of the application [17] of the STRUCT-ANN algorithm, full batch methods were used.
We would like to point out that the use of the entire training dataset for every update is extremely expensive with respect to memory and computation time, especially when working with GPU architectures.
5.4 Case Study: Quantiﬁcation for Sentiment Analysis We report the results of experiments comparing the performance of the DENIM on a Twitter sentiment detection challenge problem.
The task in this challenge was to ascertain correctly the fraction of tweets exhibiting various sentiments.
The performance was measured using the Kullback-Leibler divergence (see (1)).
We trained an end-to-end DeepLSTM model trained using DENIM.
We also trained an attention- enabled network for the same task using the DENIM.
Our models accepted raw text in the standard one-hot encoding format and performed task speciﬁc optimization and generated task speciﬁc vocabulary embeddings.
Our representations were 64-dimensional and were learnt jointly with other network parameters.
Implementation details: All our LSTM models used a single hidden layer with 64 hidden nodes, which gave rise to 64-dimensional hidden state representations.
For the LSTM model, the ﬁnal label was obtained by applying a linear model with a logistic wrapper function.
For the attention models (referred to as AM), the decoder hidden states were set to be 64-dimensional as well.
The alignment model was set to be a feed-forward model with a softmax layer.
Step lengths were tuned using standard implementations of the ADAM method.
Training was done by adapting the DENIM method.
DENIM is able to obtain near perfect quantiﬁcation on both LSTM (KLD = 0.007) as well as AM (KLD = 0.00002) models (see Figure 5(a)).
In contrast, the classical cross-entropy method with attention model (AM-CE) is unable to obtain satisfactory performance.
DENIM converges to optimal test KLD 11 0510152025303540Iterations0.500.450.400.350.300.250.200.150.100.05NegKLDppiDENIMDENIM-NSANN-0-10510152025Iterations2.01.51.00.50.0NegKLDletterDENIMDENIM-NSANN-0-10510152025303540Iterations0.400.350.300.250.200.15NegKLDcovtypeDENIMDENIM-NSANN-0-10510152025303540Iterations2.52.01.51.00.50.0NegKLDijcnn1DENIMDENIM-NSANN-0-1performance in not only far lesser iterations, but also by using far less data samples.
Also note that the AM models trained with DENIM give KLD losses that are 2 orders of magnitude smaller than what LSTMs oﬀer when trained with DENIM.
(a) Convergence to optimal test KLD performance for diﬀerent RNN models.
(b) Change in Quantiﬁcation performance with dis- tribution drift.
Figure 5: Results on the Twitter Sentiment Analysis Task We also experiment with artiﬁcially changing the fraction of positive and negative examples in order to see the performance of our model under distribution drift (see Figure 5(b)).
The fraction of negatives and positives in the test set was distorted from their original values by resampling.
As the test distribution priors are distorted more and more, AM-CE (Attention Model trained with Cross Entropy) performs extremely poorly.
DENIM with LSTMs displays some degree of robustness to drift but succumbs at extremely high level of drift.
DENIM with AM models on the other hand, remains extremely robust to even high degree of distribution drift, oﬀering near-zero KLD error.
The beneﬁts of the attention models employed by DENIM allow it to identify critical words in a tweet that clearly signal its polarity.
The highlighted words (see Figure 6) are those for which DENIM assigned an attention score α ≈ 1.
TGIF!! Make it a great day, Robbie!! Monsanto’s Roundup not good for you I may be in love with Snoop anyone having problems with Windows 10?
may be coincidental but since i downloaded, my WiFi keeps dropping out.
@NariahCFC against barca pre season stand out player 1st half..
@alias8818 Hey there! We’re excited to have you as part of the T-Mobile family! listening to Fleetwood Mac and having my candles lit is the perfect Sunday evening Figure 6: Figuring where the attention is.
Highlighted words got high attention scores.
A red (green) highlight indicates that the tweet was tagged with a negative (positive) sentiment.
References [1] Barranquero, J., D´ıez, J., del Coz, J.J.: Quantiﬁcation-oriented learning based on reliable classiﬁers.
Pattern Recognition 48(2), 591–604 (2015) [2] Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde- Farley, D., Bengio, Y.: Theano: A CPU and GPU math compiler in Python.
In: Proceedings of the 9th Python in Science Conference (SciPy 2010), pp.
1–7.
Austin, USA (2010) [3] Eban, E., Schain, M., Mackey, A., Gordon, A., Saurous, R., Elidan, G.: Scalable Learning of Non- Decomposable Objectives.
In: Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS) (2017) 12 051015202530354045Iterations876543210NegKLDTwitterLSTM-DENIMAM-CEAM-DENIM[4] Esuli, A.: ISTI-CNR at SemEval-2016 Task 4: Quantiﬁcation on an ordinal scale.
In: Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval 2016).
San Diego, US (2016) [5] Esuli, A., Sebastiani, F.: Optimizing text quantiﬁers for multivariate loss functions.
ACM Transactions on Knowledge Discovery and Data 9(4), Article 27 (2015) [6] Gao, W., Sebastiani, F.: Tweet sentiment: From classiﬁcation to quantiﬁcation.
In: Proceedings of the 7th International Conference on Advances in Social Network Analysis and Mining (ASONAM 2015), pp.
97–104.
Paris, FR (2015) [7] Kar, P., Li, S., Narasimhan, H., Chawla, S., Sebastiani, F.: Online Optimization Methods for the In: Proceedings of the 22nd ACM International Conference on Knowledge Quantiﬁcation Problem.
Discovery and Data Mining (SIGKDD 2016), pp.
1625–1634.
San Francisco, USA (2016) [8] Kar, P., Sriperumbudur, B.K., Jain, P., Karnick, H.: On the Generalization Ability of Online Learning Algorithms for Pairwise Loss Functions.
In: 30th International Conference on Machine Learning (ICML) (2013) [9] Kennedy, K., Namee, B.M., Delany, S.J.: Learning without default: a study of one-class classiﬁca- tion and the low-default portfolio problem.
In: International Conference on Artiﬁcial Intelligence and Cognitive Science (ICAICS), Lecture Notes in Computer Science, vol.
6202, pp.
174–187 (2010) [10] Koyejo, O.O., Natarajan, N., Ravikumar, P.K., Dhillon, I.S.: Consistent binary classiﬁcation with generalized performance metrics.
In: Proceedings of the 28th Annual Conference on Neural Information Processing Systems (NIPS 2014), pp.
2744–2752.
Montreal, CA (2014) [11] Manning, C.D., Raghavan, P., Sch¨utze, H.: Introduction to Information Retrieval.
Cambridge University Press (2008) [12] Narasimhan, H., Agarwal, S.: SVMtight pAUC: A New Support Vector Method for Optimizing Partial AUC Based on a Tight Convex Upper Bound.
In: ACM SIGKDD Conference on Knowledge, Discovery and Data Mining (KDD) (2013) [13] Narasimhan, H., Kar, P., Jain, P.: Optimizing non-decomposable performance measures: A tale of two classes.
In: Proceedings of the 32nd International Conference on Machine Learning (ICML 2015), pp.
199–208.
Lille, FR (2015) [14] Narasimhan, H., Kar, P., Jain, P.: Optimizing Non-decomposable Performance Measures: A Tale of Two Classes.
In: Proceedings of the 32nd International Conference on Machine Learning (ICML 2015), pp.
199–208.
Lille, FR (2015) [15] Narasimhan, H., Vaish, R., Agarwal, S.: On the Statistical Consistency of Plug-in Classiﬁers for Non- decomposable Performance Measures.
In: 28th Annual Conference on Neural Information Processing Systems (NIPS) (2014) [16] Qi, Y., Bar-Joseph, Z., Klein-Seetharaman, J.: Evaluation of diﬀerent biological data and computational classiﬁcation methods for use in protein interaction prediction.
Proteins 63, 490–500 (2006) [17] Song, Y., Schwing, A.G., Zemel, R.S., Urtasun, R.: Training Deep Neural Networks via Direct Loss In: Proceedings of the 33rd International Conference on Machine Learning (ICML) Minimization.
(2016) [18] Vincent, P.: An Introduction to Signal Detection and Estimation.
Springer-Verlag New York, Inc.
(1994) 13 A Proof of Theorem 1 Theorem 3.
Consider a concave performance measure deﬁned using a link function Ψ that is concave and L(cid:48)-smooth.
Then, if executed with a uniform step length satisfying η < 2 L , then DUPLE -stabilizes within (cid:101)O(cid:0) 1 (cid:1) iterations.
More speciﬁcally, within T iterations, DUPLE identiﬁes a model wt such that (cid:18)(cid:113) (cid:19) 2 (cid:107)∇t(cid:107)2 ≤ O L(cid:48) log T Proof.
Recall that we assume that the reward functions r(f (x; w), y) are L-smooth functions of the model w.
This is satisﬁed by all reward functions we consider.
Note, however, that nowhere will we assume that the reward functions are concave in the model parameters.
We will use the shorthand ∇t = ∇wg(wt; St, αt, βt) and F (wt, αt) = g(wt; St, αt, βt).
We will prove this result for the batch version of the DUPLE algorithm for the sake of simplicity and to present the key ideas.
The extension to the mini-batch version is straightforward and will introduce an additional error of the order of 1√ The batch version of DUPLE makes the following model update wt+1 = wt + η · ∇t.
Using the where b is the batch size.
smoothness of the reward functions, we get F (wt+1, αt) ≥ F (wt, αt) +(cid:10)∇t, wt+1 − wt(cid:11) − L (cid:13)(cid:13)wt+1 − wt(cid:13)(cid:13)2 2 , which, upon rearranging, give us (cid:107)∇t(cid:107)2 (cid:13)(cid:13)∇t(cid:13)(cid:13)2 2 ≤ T(cid:88) t=1 (cid:16) 1 − ηL η(1− ηL 2 ) 2 ≤ F (wt+1,αt)−F (wt,αt) T(cid:88) F (wT +1, αT ) + (cid:17)(cid:32) t=2 , which, upon summing up, gives us (cid:33) F (wt, αt−1) − F (wt, αt) However, by a forward regret-analysis of the dual updates which execute the follow-the-leader algorithm and the fact that due to the L(cid:48)-smoothness of Ψ, the functions F (w, α) are 1 L(cid:48) strongly convex, T(cid:88) F (wt, αt−1) − F (wt, αt) ≤ O (L(cid:48) log T ) .
This completes the proof upon applying an averaging argument.
t=2 B DAME: A Deep Learning Technique for Pseudolinear Perfor- mance Measures We present an algorithm for training deep models on pseudolinear performance measures such as F-measure.
These are extremely popular in several areas and direct optimization routines are sought after.
We know of only one proposed algorithm for training deep models with F-measure in the work of [3].
However, their algorithm involves constrained optimization routines over deep models and the authors do not discuss the details of implementing the same.
Our algorithm DAME, on the other hand, is based on an alternating strategy, is very scalable and gives superior performance across tasks and datasets.
For sake of simplicity, we represent the pseudolinear performance measure as P(a,b)(w) = Pa(w) Pb(w) a0 + a1 · TPR(w) + a2 · TNR(w) b0 + b1 · TPR(w) + b2 · TNR(w) Given the above, we deﬁne the notion of a valuation function.
14 (a) CT (b) IJCNN (c) IJCNN1 (d) IJCNN1 (e) IJCNN1 (f) IJCNN1 Figure 7: Experiments with DAME Deﬁnition 2 (Valuation Function).
The valuation of a pseudolinear measure P(a,b)(w) at any level v > 0, is deﬁned as V (w, v) = Pa(w) − v · Pb(w) We will use the notation P(a,b),S(wt) and VS(w, v) to denote respectively, the performance measure, and the valuation function as deﬁned on a data sample S.
At every time step t, DAME looks at vt = P(a,b)(wt) and attempts to approximate the task of optimizing F-measure (or any other pseudolinear measure) using a cost weighted classiﬁcation problem described by the valuation function at level vt.
After making updates to the model with respect to this approximation, DAME reﬁnes the approximation again, and so on.
We note that similar alternating strategies have been studied in literature in the context of F-measure before [10, 14] and oﬀer provable convergence guarantees for linear models.
However, a direct implementation of these methods gives extremely poor results as we shall see in the next section.
The complex nature of these performance measures, that are neither convex nor concave, make it more challenging to train deep models.
To solve this problem, DAME utilizes a two-stage training procedure, involving pretraining the en- tire network (i.e. both upper and lower layers) on a standard training objective such as cross-entropy or least squares, followed by ﬁne tuning of only the upper layers of the network to optimize F-measure.
The pretraining is done using standard stochastic mini-batch gradient descent.
The details of the algorithm are given in Algorithm 3.
For sake of simplicity we will let (w1, w2) denote a stacking of the neural networks described by the models w1 and w2.
More speciﬁcally w2 denotes a network with input dimensionality din and output dimensionality dint whereas w1 denotes a network with input dimensionality dint and output dimensionality dout.
To ensure diﬀerentiability, DAME uses valuation functions with appropriate reward functions replacing the TPR and TNR functions.
We are able to show stronger local convergence guarantees for DAME.
Due to lack of space, we present only a sketch of the proof for the batch version i.e. St,i = ˜T for all time steps t, i, with constant step lengths.
We will continue to assume that the valuation functions are L-smooth functions of the upper model.
It is also noteworthy that we present the guarantee only for the ﬁne-tuning phase since the pre-training phase 15 05101520Iterations0.000.050.100.150.200.25F-MeasurecovtypeDAMEANN01ANN-PGStruct-ANN05101520Iterations0.00.10.20.30.40.50.6F-Measurekdd08DAMEANN01ANN-PGStruct-ANN05101520Iterations0.00.10.20.30.40.50.60.70.8F-Measureijcnn1DAMEANN01ANN-PGStruct-ANN05101520Iterations0.00.10.20.30.40.50.60.70.80.9F-Measurecod-rnaDAMEANN01ANN-PGStruct-ANN05101520Iterations0.00.10.20.30.40.5F-MeasureletterDAMEANN01ANN-PGStruct-ANN020406080100Iterations0.00.10.20.30.40.50.60.7F-Measurea9aDAMEANN01ANN-PGStruct-ANNenjoys local convergence guarantees by standard arguments.
For this reason, we will omit the lower network in the analysis.
We will assume that the performance measure satisﬁes Pa(w) ≤ M for all w ∈ W and ·Pb(w) ≥ m for all w ∈ cW .
We note that these assumptions are standard [7, 14] and also readily satisﬁed by F-measure, Jaccard coeﬃcient etc for which we have m, M = Θ(1) (see [14]).
Let κ = 1 + M/m.
∇wV (w,P(a,b)) To prove Theorem 2, we ﬁrst show the following result.
Since we have ∇wP(a,b)(w) = Pb(w) model within O(cid:0) 1 and Pb(w) ≥ m, Theorem 2 will follow Theorem 4.
If executed with a uniform step length satisfying η < 2 (cid:1) inner iterations.
More speciﬁcally, within κ2 (cid:13)(cid:13)(cid:13)∇wt V ˜St((wt ≤ .
2 (cid:13)(cid:13)(cid:13)2 1 such that 2), vt−1) 1, wt model wt Proof.
It is easy to see that V (wt−1 , vt) = 0 and that V (w1, v) is a Lκ-smooth function of the model parameter w1 for any realizable valuation i.e. v = P(a,b)(w) for some w ∈ W.
Now, the batch version of the DAME algorithm makes the following model updates within the inner loop Lκ , then DAME discovers an -stable 2 )2 iterations, DAME identiﬁes a η(1− Lκη where ∇(t−1,t(cid:48)) = ∇ V (wt−1,t(cid:48)−1 wt−1,t(cid:48)−1 V (wt−1,t(cid:48) , vt) ≥ V (wt−1,t(cid:48)−1 wt−1,t(cid:48) = wt−1,t(cid:48)−1 + η · ∇(t−1,t(cid:48)), = V (wt−1,t(cid:48)−1 (cid:69) − Lκ , vt).
Using the smoothness of the reward functions, we get (cid:68)∇(t−1,t(cid:48)), wt−1,t(cid:48) (cid:18) (cid:13)(cid:13)(cid:13)2 , vt) + wt−1,t(cid:48)−1 wt−1,t(cid:48)−1 (cid:13)(cid:13)(cid:13)wt−1,t(cid:48) (cid:13)(cid:13)(cid:13)∇(t−1,t(cid:48))(cid:13)(cid:13)(cid:13) > , the valuation of the model θ(t+1,i) goes up by (cid:19)(cid:13)(cid:13)(cid:13)∇(t−1,t(cid:48))(cid:13)(cid:13)(cid:13)2 1 − Lκη , vt) + η 1, vt) ≥ c then P(wt 1) ≥ P(wt−1 M .
Since the m , putting these results together tell us that such 2 )2 inner iterations without encountering a model wt,t(cid:48) ≤  (cid:13)(cid:13)(cid:13)∇wP(wt,t(cid:48) m as ) + c (cid:13)(cid:13)(cid:13)2 Now this shows that at each step where 2.
at least η maximum value of the performance measure for any model is M DAME cannot execute more than M 2 It is easy to see that if V (wt η(1− Lκη (cid:17) 1 − Lκη (cid:16) (cid:13)(cid:13)(cid:13)∇(t−1,t(cid:48))(cid:13)(cid:13)(cid:13)2 that well.
≤ .
An easy calculation shows that for such a model we also have The following experiments Figure:[7] show the performance of DAME on the F1-Measure.
A naive training with misclassiﬁcation loss yields extremely poor F-measure performance.
Moreover, a naive implementation of methods proposed for linear models such as the plug-in method also performs very poorly.
DAME on the other hand is able to rapidly oﬀer very good F-measure scores after looking at a fraction of the total data.
As, it can be seen, struct ANN provides a consistent poor performance whereas it seems to perform well in the linear case.
This is because our implementation of the Struct ANN is a minibatch method and the gradient obtained from the structual method has almost no real information due to this.
In other variants of the application[17] of the structual ANN, people have usually used full batch methods.
We would like to point out that such a case is almost intractable with respect to memory and computation time.
C Details of implementation of the Structual ANN from [17] Here assume that ∆ is the loss function we are looking at and its input is the two dimensional confusion matrix.
Keeping this is mind, we deﬁne the following functions.
a(ˆy, y) = I{yi = 1}I{ˆyi = 1} (cid:88) 16 (cid:88) (cid:88) (cid:88) b(ˆy, y) = c(ˆy, y) = d(ˆy, y) = I{yi = 1}I{ˆyi = 0} I{yi = 0}I{ˆyi = 1} I{yi = 0}I{ˆyi = 0} Finally, m(·) is the artiﬁcial neural network (cid:40) f (w) = max ˆy ∆ (a(ˆy, y), b(ˆy, y), c(ˆy, y), d(ˆy, y)) + (cid:41) (ˆyi − yi)m(xi) n(cid:88) i=1 If then ˜y ∈ arg max ˆy Hence, to ﬁnd ˜y, we need to solve the following g(w) = ∂m(xi) + C · f (w) ∂g(w) (cid:51) w + ∂f (w) min (cid:40) n(cid:88) ∆ (a(ˆy, y), b(ˆy, y), c(ˆy, y), d(ˆy, y)) + (ˆyi − yi)m(xi) i=1 (˜yi − yi)∂m(xi) ∈ ∂f (w) n(cid:88) i=1 (cid:40) ∆ (p, q, r, s) + (cid:41) (cid:41) (cid:41) , (cid:41) , (ˆyi − yi)m(xi) i=1 n(cid:88) (cid:40) n(cid:88) (cid:40) n(cid:88) i=1 arg max ˆy such that a(ˆy,y)=p,b(ˆy,y)=q,c(ˆy,y)=r,d(ˆy,y)=s (ˆyi − yi)si arg max (p,q,r,s) arg max (p,q,r,s) a(ˆy,y)=p,b(ˆy,y)=q,c(ˆy,y)=r,d(ˆy,y)=s arg max ˆy such that ∆ (p, q, r, s) + ∆ (p, q, r, s) + n(cid:88) (˜yi − yi)∂m(xi) arg max (p,q,r,s) arg max ˆy such that ˆyisi i=1 a(ˆy,y)=p,b(ˆy,y)=q,c(ˆy,y)=r,d(ˆy,y)=s This is very amiable to a symbolic gradient operation, as we need to ﬁnd the gradient which looks like i=1 However, by linearity, this is the same as n(cid:88) i=1 (˜yi − yi)m(xi) Therefore , we need to do a forward pass over the symbolic graph to get the value of m(xi) and then feed to our solver for the most violated constraint, which will give us ˜yi and then we deﬁne the symbolic gradient as n(cid:88) (˜yi − yi)m(xi) i=1 17
Although a wide selection of clustering methods are available [1, 2], most of them assume concurrent access to all data being clustered.
Our interest is in efﬁciently clustering each datum as it becomes available, for applications that require unsupervised learning in real time.
The Links approach is to estimate the probability distribution of each cluster based on its current constituent vectors, to use those estimates to assign new vectors to clusters, and to update estimated distributions with each added vector.
The update step includes ﬁx- ing past cluster assignments where indicated by taking the additional data into account, although this is primarily to improve the internal model over time, since in typical online usage scenarios, each clus- ter assignment is provided once, at the time a new vector is made available.
Prior work [3] addressing online clustering of unit vectors employs a small-variance approximation and is applied to low- dimensional problems such as segmentation of surface normals in 3D.
Our approach is complementary in that it uses a high- dimensional approximation, and has been applied to problems with relatively high variance.
Links has been used to cluster CNN-based FaceNet embeddings [4] and LSTM-based voice embeddings [5].
The results of the latter experiment are presented in a separate paper [6].
The current paper focuses on the technical details of the algorithm.
2.
MODEL 2.1. Generative model for a cluster Let X = {xi} be a set of unit-length vectors in RN .
They are conﬁned to the submanifold SN−1, and to determine proximity for the purpose of clustering these vectors, we will use the natural metric on this submanifold, which is simply the angle between vectors: ∠(x, x ′) = arccos(x · x ′).
(1) We address the problem of cluster distributions within this sub- manifold with the following properties: 1.
Each cluster has a center vector µ and its member vectors x are generated by a probability density that is isotropic in the sense that it only depends on distance from the center, x ∼ ρ(x; µ) = f (∠(x, µ)).
2.
The function f is the same for every cluster, so that probabil- ity densities for different clusters are related by isometry.
3.
f (θ) decreases exponentially with θ; for example, as a Gaus- sian suitably normalized on SN−1: f (θ) ∝ e θ2 2σ2 .
(2) This ensures that the distribution is reasonably localized, since the exponential decrease compensates for a polynomial factor in the marginal distribution of θ: ρ(∠(x, µ) = θ) = A(sin θ)N−2f (θ) (3) where A is a constant equal to the hypersurface area of SN−2, A = (4) 2π(N−1)/2 Γ(cid:0) N−1 2 (cid:1) 4.
The prior distribution ρ(µ) for the center of a cluster µ is constant on SN−1 (no unit vector is preferred).
2.2. Estimated distribution Given a set X = {xi} chosen randomly from the same cluster, but without knowledge of the center of the cluster, we would like to estimate the cluster’s probability distribution.
The likelihood of the center value µ is L(µ; X) =Yi ρ(xi; µ) Pi(∠(xi,µ))2 ∝ e 2σ2 (5) Since the prior ρ(µ) is constant, the posterior ρ(µ|X) is also pro- portional to the expression in equation 5.
The maximum likelihood (and maximum a posteriori) center is therefore ˆµ = argmin µ Xi (cid:0)∠(xi, µ)(cid:1)2 (6) which is the same as the centroid of the vectors {xi} as deﬁned for a hypersphere according to [7].
The estimated probability distribution for the cluster is ˆρ(x; X) ∝ZSN −1 ∝ZSN −1 L(µ; X)ρ(x; µ)dµ (∠(x,µ))2+Pi(∠(xi,µ))2 (7) 2σ2 dµ.
The probability that a new vector x belongs to the same cluster can then be estimated as the cumulative amount strictly increasing function of k, the variance of the estimated distri- bution decreases with k.
Z{y∈SN −1 | ˆρ(y;X)≥ ˆρ(x;X)} ˆρ(y; X)dy.
(8) Similarly, to assess whether two clusters are the same, we de- termine a threshold on the cosine similarity between their centroids µc · µ c ≥ s(k, k′) where, for N ≫ k and N ≫ k′, 2.3. High-dimensional approximation Our primary interest is in problems with relatively large N .
For example, our typical embedding vectors have N ≥ 128.
For large enough N , the following are true: Lemma 1 Two randomly chosen vectors x, x′ are almost always almost perpendicular, i.e., s(k, k′) = q(cid:0)1 + 1 s(cid:18)1 + 1 k tan2 θc(cid:1)(cid:0)1 + 1 k (cid:16) 1 − 1(cid:17)(cid:19)(cid:18)1 + 1 k′ tan2 θc(cid:1) k′ (cid:16) 1 T 2 T 2 (16) − 1(cid:17)(cid:19) P (x · x ′ > δ) < ǫ (9) Note that equation 13 is the special case with k′ = 1, for some positive numbers δ ≪ 1 and ǫ ≪ 1.
Lemma 2 The angle θ between a cluster center and a random vector from that cluster is almost always almost equal to a global constant θc, i.e., P (|θ − θc| > δ) < ǫ (10) for some positive numbers δ ≪ π and ǫ ≪ 1.
Lemma 3 Given two randomly chosen vectors from a cluster with center µ, their components perpendicular to µ will almost al- ways be almost perpendicular to each other, i.e., P(cid:16)(cid:0)x − (x · µ)µ(cid:1) ·(cid:0)x ′ − (x ′ · µ)µ(cid:1) > δ(cid:17) < ǫ for some positive numbers δ ≪ 1 and ǫ ≪ 1.
(11) To assess whether to add a new vector x to an existing cluster known to include the k vectors {xi}k i=1, we determine a threshold x · ˆµ ≥ s(k) on the cosine similarity between the new vector and the centroid ˆµ of the existing vectors.
Using the approximation in lemmas 2 and 3, and assuming N ≫ k, we can compute vector (x−cos θcµ) components in an orthonormal basis including µ, sin θc andn 1 sin θc (xi − cos θcµ)ok i=1 .
This yields ˆµ = and a threshold of pk2 cos2 θc + k sin2 θc xi Xi=1 (12) (13) s(k) = k cos2 θc T 2 pk2 cos2 θc + k sin2 θc q 1 k +(cid:0)1 − 1 k(cid:1) T 2 where Tc = cos θc, which we call the cluster similarity threshold.
Note that and lim k→∞ ˆµ = µ lim k→∞ s(k) = Tc, (14) (15) which conﬁrms that as we accumulate more vectors in a given clus- ter, the center and cosine similarity threshold of the estimated dis- tribution approach the center and cosine similarity threshold of the generative distribution (i.e., the estimate improves).
Since s(k) is a and s(k, 1) = s(k), lim k,k′→∞ s(k, k′) = 1.
(17) (18) The latter conﬁrms that the centers estimated from the two sets of cluster points converge.
3.
ALGORITHM 3.1. Online clustering Each new input vector is assigned to a cluster as soon as it is pro- duced, with no knowledge of future vectors and no backtracking.
A unique ID for that cluster is returned.
The clusterer keeps statisti- cal information about the vectors received so far.
Although it cannot change a previous answer, it can change the internal representation of cluster statistics, such as improvements to estimated distributions as well as cluster splits and merges when indicated by new informa- tion.
3.2. Internal representation The Links algorithm’s internal representation is a two-level hierar- chy: clusters are collections of subclusters, and subclusters are col- lections of input vectors.
The subclusters are represented as nodes in a graph whose edges join ‘nearby’ nodes (meaning subclusters that likely belong to the same cluster given the data so far), and clusters are deﬁned as connected components of the graph.
Whereas sub- clusters are indivisible, clusters can become split along graph edges in response to changes in subcluster estimated probability distribu- tions as new data is added.
Alternatively, subclusters joined by an edge can become merged in response to changes.
The reasons for maintaining this two-level hierarchy (rather than, say, an arbitrary number of levels) are efﬁciency and practi- cality.
It is efﬁcient because the algorithm scales with number of subclusters rather than number of vectors.
It is practical because the key cluster substructure that can affect future cluster IDs is the set of potential split points.
3.3. Assessing cluster membership When a new vector x is available, compute its cosine similarity to each subcluster centroid ˆµj, and add it to the most-similar subcluster if the similarity is above a ﬁxed threshold Ts. In other words, let J = argmax {x · ˆµj}.
(19) If 3.6. Hyperparameter Tuning The similarity thresholds Tc, Ts and Tp need to be tuned to best rep- resent the data source.
This is done by manually labeling a dataset with cluster IDs, running the clusterer on the data, and adjusting hyperparameters to improve the accuracy of the output cluster IDs. Accuracy is simply fraction of correct IDs. Prior to evaluation, the Hungarian algorithm [8] is used to map a subset of output cluster IDs bijectively to a subset of ground truth cluster IDs in such a way that produces the best possible accuracy.
For some applications an alternate objective has been used; for example, one that gives dif- ferent weights for conﬂating IDs vs.
fracturing IDs, to reﬂect the seriousness of each type of error in practise.
4.
ACKNOWLEDGEMENTS The authors would like to thank Dr. Brian Budge and Dr. Navid Shiee for help with APIs and evaluation frameworks used in the im- plementation of the Links algorithm.
x · ˆµJ ≥ Ts (20) then add x to subcluster J.
Ts, called the subcluster similarity threshold, is a hyperparameter determining the granularity of cluster substructure appropriate for the data.
If inequality 20 does not hold, then start a new subcluster con- taining just x.
Next, use the estimated probability distribution of subcluster J to determine whether to include the new subcluster in the same cluster as J, by thresholding the cumulative probability in expression 8.
In the high-dimensional approximation, this means the subcluster is included in the cluster whenever x · ˆµJ ≥ s(kJ ) (21) where kJ is the number of vectors in the subcluster J.
To a ﬁrst approximation, s(k) is as given in equation 13.
This will be further reﬁned in section 3.5. If inequality 21 does hold, then add an edge to the graph joining the new subcluster to subcluster J.
3.4. Updating clusters When a new vector is added to an existing subcluster, the subclus- ter’s centroid may change.
If this brings it within the subcluster simi- larity threshold of the centroid of another subcluster currently joined to the ﬁrst by an edge, then the two are merged.
In other words, if ˆµi · ˆµj ≥ Ts, then nodes i and j are replaced with a single node con- taining the vectors of both, and with the edge connections of both.
Since the merging process also results in a new subcluster centroid, this check is continued recursively on affected subclusters.
Next, the edges joining affected nodes are checked for validity.
The edge joining subclusters i and j is removed if the following does not continue to hold: ˆµi · ˆµj ≥ s(ki, kj) (22) where s(ki, kj) is approximately as given in equation 16, but with improvements to follow in section 3.5. After severing a cluster in two by removing an edge, an attempt is made to re-join the two parts by adding an edge from the affected node to a new partner node that does satisfy inequality 22.
If no such partner is found, then the cluster remains permanently split.
3.5. Anisotropy Equations 13 and 16 were used to determine thresholds for member- ship in the same cluster as a given subcluster, effectively treating the subcluster’s members as randomly chosen from the cluster and not correlated with each other.
If one were to properly take into account intra-subcluster correlations, then one consequence is that the limit in equation 18 would be reduced to a positive number Tp < 1, which we call the pair similarity maximum, lim k,k′→∞ s(k, k′) = Tp, (23) whereas the value of s(1, 1), which is T 2 c , would remain unchanged.
Any implicit anisotropy in the cluster distribution, such as an elon- gation along a preferred axis, will further reduce the value of Tp without changing s(1, 1).
A simple though approximate way to in- corporate these adjustments into the algorithm is to replace s(k, k′) and s(k) by the following interpolated versions: ˜s(k, k′) = T 2 c + ˜s(k) = ˜s(k, 1).
Tp − T 2 1 − T 2 c (cid:0)s(k, k′) − T 2 c(cid:1) (24) (25) 5.
REFERENCES [1] Brian Everitt, Cluster Analysis, John Wiley & Sons, 2011.
[2] Christian Hennig, Marina Meila, Fionn Murtagh, and Roberto Rocci, Handbook of Cluster Analysis, Chapman and Hall/CRC, December 2015.
[3] Julian Straub, Trevor Campbell, Jonathan P.
How, and John W.
Fisher, “Small-variance nonparametric clustering on the hyper- sphere,” in 2015 IEEE Conference on Computer Vision and Pat- tern Recognition (CVPR), June 2015, pp.
334–342.
[4] F.
Schroff, D.
Kalenichenko, and J.
Philbin, “Facenet: A uni- ﬁed embedding for face recognition and clustering,” in 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015, pp.
815–823.
[5] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez Moreno, “Generalized end-to-end loss for speaker veriﬁcation,” arXiv preprint arXiv:1710.10467, 2017.
[6] Quan Wang, Carlton Downey, Li Wan, Philip Andrew Mans- “Speaker diarization with ﬁeld, and Ignacio Lopez Moreno, lstm,” arXiv preprint arXiv:1710.10468, 2017.
[7] Samuel R.
Buss and Jay P.
Fillmore, “Spherical averages and applications to spherical splines and interpolation,” ACM Trans- actions on Graphics, vol.
20, no.
2, pp.
95–126, 2001.
[8] Harold W.
Kuhn, “The hungarian method for the assignment problem,” Naval Research Logistics Quarterly, vol.
2, pp.
83– 97, 1955.

Artiﬁcial intelligence (AI)-enabled systems are playing an in- creasingly prominent role in our society and signiﬁcantly dis- rupting how commercial and government organizations oper- ate.
In particular, the rate of advances in machine learning (ML) using Deep Neural Networks (DNNs) is staggering, en- abling AI-enabled systems to master complex tasks includ- ing Go [Silver et al., 2016], autonomous driving [Bojarski and others, 2016], and even predicting census data [Gebru et al., 2017].
These incredible advances, however, have come at a signiﬁcant cost: DNNs are complex, opaque, and derive their power from millions of parameters that must be trained on large data sets.
Thus, understanding or explaining why a DNN came to a particular conclusion is a difﬁcult task.
Yet as AI-enabled systems become more prevalent in our lives, this lack of explainability and intepretability comes with serious societal consequences.
For example, ML is now being used to predict recidivism in the criminal justice system [Berk, 2017], interventions in the child welfare system [Cuccaro-Alamin et al., 2017], and cancer from radiology images [Esteva et al., 2017].
Lack of explainability in these applications can have life or death consequences.
The need for explainability and interpretability has cer- tainly not gone unnoticed in the AI and social science com- munities.
Important strides are being made to either imbue DNNs with inherent explainability or to achieve the predic- tive power of DNNs using more explainable methods.
In par- ticular, there has been a signiﬁcant amount of progress ex- plaining Convolutional Neural Networks (CNNs) in the im- age domain, most likely due to the ease of visualizing expla- nations.
For example, GradCAM [Selvaraju et al., 2016] and LRP [Binder et al., 2016] are two popular methods of gen- erating saliency maps that indicate the relevance of pixels in the image to the output of the CNN.
While these methods have vastly improved the explainabil- ity landscape, they lack one critical element needed for true understanding by humans: They have limited causal interpre- tations.
Causality as a means of explanation has deep roots within the AI community [Pearl, 2009].
A causal explana- tion of a DNN’s operation provides the end–consumer of the DNN output (i.e., a human) with the understanding of how and what can be changed about a DNN or its input that re- sults in an impactful change in the output.
In highly sensitive domains like credit scoring or money laundering, a causal ex- planation is critical as system implementers must justify the operation of their ML models to government regulators.
One major challenge, however, is that any causal explanation must be formulated in terms of concepts or variables that are under- standable to a human; otherwise the explanation may end up as obfuscated as the original model.
The hypothesis that we explore in this paper posits that a human–understandable causal model of the operation of a DNN that allows for arbitrary causal interventions and queries is an effective, and often necessary, tool for explain- ability and interpretability.
Arbitrary interventions allows the user to understand the chain of causal effects from DNN input, to low–level features of the domain, to high–level human–understandable concepts, to DNN outputs.
Critically, such a model, if constructed accurately, could support intro- spective queries on a DNN not supported by other methods such as counterfactual queries.
For example, one might be able to ask ”What is the probability the car would have turned right had there not been a pedestrian present in the input im- age?” Such a capability is a powerful tool for debugging, un- derstanding bias, and ensuring safe operation of AI systems.
In this paper, we explore this hypothesis and demonstrate that a causal approach to DNN explanation is possible and yields valuable information about various classiﬁcation sys- tems.
As part of this approach, we extract low–dimensional concepts from DNNs to generate a human–understandable “vocabulary” of variables.
We then perform interventional experiments to learn a graphical causal model (i.e., Bayesian network [Pearl, 2009]) that relates the DNN’s inputs to the concepts, and the concepts to the DNN’s outputs.
Finally, we demonstrate the explanatory power of such a model by identifying the concepts in several networks with the highest expected causal effect on the outputs.
Our contributions can be summarized as follows: human–understandable concepts to aid explainability • A causal model of a DNN’s operation formulated in • An unsupervised technique to extract concepts from DNNs that have high likelihood of being human– understandable • A proposed way to measure the causal effects of inputs and concepts on a DNN’s outputs The remainder of this paper is organized as follows.
In Sec- tion 2 we discuss related explainability work.
In Section 3 we formulate the notions of causality in DNNs. We discuss the extraction of human–understandable concepts from DNNs in Section 4.
We run several experiments and present examples of our results in Section 5, and ﬁnally conclude in Section 6 2 Related Work Many recent works employ saliency for visual explanations.
Several previous works have visualized predictions by em- phasizing pixels which have inﬂuential values (i.e., if the pixel values change signiﬁcantly, the output of the network also changes signiﬁcantly).
In an early work in this domain [Erhan et al., 2009], an input image which maximally acti- vated a neuron of interest was found by gradient ascent in the image domain; this work was extended in [Simonyan et al., 2013] to obtain class-speciﬁc saliency maps.
Beyond manipulating the image space and monitoring the impact on the output, other works have considered an analysis on the learned features of the network in order to glean understand- ing on how the network functions.
In the seminal work by [Zeiler and Fergus, 2014], a multi-layer deconvolutional net- work was used to project features activations (the output from convolution maps on images) back to input pixel space.
Gra- dient information ﬂowing into the penultimate convolutional layer was used in GradCAM [Selvaraju et al., 2016] to iden- tify discriminating patterns in input images.
Rather than manipulating input images and their activa- tions within a network, methods have been explored which generate images which produce a desired response in the net- work; the constructed result can help explain what input has a maximal desired response in the network.
Techniques have been developed to invert the network by constructing images that activate the network identically [Mahendran and Vedaldi, 2015], or maximally [Erhan et al., 2009].
Fascinating images can be constructed which maximally activate deep convolu- tional ﬁlters in a network [Mordvintsev et al., 2015], which illustrate the ellaborate features generated by the larger deep net architectures.
Methods which do not rely on gradients have also recently gained traction for visual explanations.
Layer-Wise Rele- vance Propagation [Bach et al., 2015] relies on a conservation principle to redistribute the prediction of the network back- wards until a relevance score is computed for each element in input space, and has been shown to produce interpretable heatmaps to explain individual classiﬁcations [Binder et al., 2016].
The method reported in [Xie et al., 2017], aims to re- late human understandable concepts to network outputs, by employing a deconvolution and masking- based technique to ﬁnd and score the strength of distributed representations of input concepts across late stage feature maps.
Other meth- ods do not directly consider the network at all, but rather locally approximate the ”blackbox” models for simpler, ex- plainable ones have shown to generate results which inspire trust in users [Ribeiro et al., 2016].
3 Causal Modeling Causality has a long history in AI and numerous ML efforts have focused on building realistic and accurate causal mod- els of the world instead of statistical models [Pearl, 2018].
This has resulted in a plethora of causal formalisms and se- mantics in various ﬁelds [Granger, 1980].
However, in this work, we frame the semantics of causality in terms of the in- terventional effects of Pearl’s do-calculus [Pearl, 2009].
For a directed graphical model G deﬁning a causal diagram on a set of variables X = x1, ..., xn, we deﬁne the joint probability distribution over X as (from Pearl Eqn 3.5) P (xi|pai) P (x1, ..., xn) = (cid:89) (1) i, denoted as do(x(cid:48) where pai are the parents of xi.
An intervention on G by setting xi = x(cid:48) i), induces a modiﬁed graph G(cid:48) where the edges from pai to xi are removed, resulting in the postintervention distribution (from Pearl Eqn.
3.10): if xi = x(cid:48) if xi (cid:54)= x(cid:48) P (x1, ..., xn|do(x(cid:48) P (xj|paj)  (cid:81) j(cid:54)=i i)) = (2) The semantics of the causality are important for explain- ing DNNs because, in essence, explanations must be causal models.
That is, when one seeks an explanation of a net- work’s decisions, one is equivalently asking ”What changes can be made to the input for the output to change or stay the same?”.
This formulation of causation as explanation is well supported in the literature [Woodward, 2005].
We consider a causal model that deﬁnes the joint distribution P (O, P, X) over a set of DNN outputs O, inputs P, and intermediate vari- ables X.
More importantly, the notion of interventions provides a clear and mathematically sound mechanism for the user to understand why a DNN produces different output values.
To put it another way, the explanation of an observed DNN out- put can be formulated as an intervention.
For example, if we say “the DNN recognized a pedestrian in the image because it saw a head”, that implies that the DNN thinks there is both a pedestrian and a head in the image.
If the DNN had not detected a head, or if we intervene on the input to remove the head from the image, then the probability of detecting a pedestrian would have changed.
Existing explanation methods for DNNs, in particular on image classiﬁcation tasks, lack the ability to provide this con- crete causal interpretation to the user.
For example, gradient– based methods of explanation such as layerwise relevance propagation (LRP) [Bach et al., 2015] and Grad–CAM [Sel- varaju et al., 2016] attempt to explain the output activation of a class C in terms of the input P activations, realized for a speciﬁc input Pj. While these methods are certainly use- ful, they don’t provide the causal intervention semantics that are sufﬁcient for robust explanation.
Due to discontinuous and saturated gradients, they only indicate causality over a restricted domain where the function deﬁned by the network can be approximated with a linear function.
Adversarial in- stances generated using gradient descent [Goodfellow et al., 2014] provide indication that the local behavior of functions deﬁned by trained DNNs does not have semantic relevance, which suggests that in addition to the interventions deﬁned by gradient based methods being restricted to a small domain, they are also uninterpretable in that they are considering a se- mantically dubious aspect of DNNs. Methods like LRP avoid some of the practical issues with the gradient based methods by “redistributing” activation levels to the most relevant pix- els, but again do not provide the explicit causal intervention semantics desired for effective explanation.
3.1 Causal Representation in DNNs Given that we want a causal model that reﬂects the interven- tion semantics, the question arises as to what is represented the joint distribution P (O, P, X).
With full access to the in- ternals a DNN, we already have a causal representation of the DNN in terms of its structure and weights.
That is, for a given DNN, we can deﬁne X = R, where R is the set of all neu- rons in the network, and learn this joint distribution by ex- perimentally intervening on network representations [Pearl, 2009].
A user could then ask counterfactual questions about the network, i.e. P (O, P, X|do(x(cid:48) i)) for any input, output, or internal neuron in the network.
While this method is, on a technical level, correct, it serves poorly as a model for explanation.
This is due to the lack of human–level concepts that underlie any arbitrary neuron in the network: saying neuron ri caused the network to detect a pedestrian may be technically correct but does not satisfy the needs of the eventual human that will ingest the explanations.
In addition, the language of interventions that a human would use to understand the network are not represented as individ- ual neurons.
For example, the user cannot inquire about the causal impact of a head towards the detection of a pedestrian if the only method of intervention available is at neuron gran- ularity.
As a result, we posit that a DNN causal model must be constructed at a conceptual granularity that is meaningful to humans.
We propose that a causal model for DNNs should be rep- resented by joint distribution over O, P, and a set of concepts C.
The process for deriving C is described by a function fR : R → C over a speciﬁc DNN that transforms the repre- sentation of neurons and their activations into a set of concept variables.
Ideally, fR would have the following properties: (cid:90) (cid:90) P (O, P, R) = P (O, P, C) i)) = P (O, P|C, do(p(cid:48) i)) P (O, P|R, do(p(cid:48) (3) (4) That is, we want the joint distribution over the inputs and out- puts to be the same for both the neuron–level and concept– level causal models.
Furthermore, we also want the same causal dependencies to hold with respect to input interven- tions.
The semantics of C are open and can range from sim- ple groups of neuron to high–level human concepts like arms, legs, and heads.
This subjectivity in the concepts is quite powerful, as it presents a method to explain DNN operation without compromising the true causal semantics of the net- work, and provides the ability to allow users propose human– understandable interventions and queries.
3.2 Computing Causal Effects Given a causal model deﬁned by P (O, P, C), there are a num- ber of interesting queries one might ask to better understand and explain the operation of a DNN.
In this work, we propose a measure we call the expected causal effect.
First, we deﬁne the causal effect of an intervention on x(cid:48) i on Xj = xj given any evidence Z as: Eﬀect(xi → xj, Z) = P (xj|do(x(cid:48) i), ZXi) − P (xj|ZXi ) (5) where ZXi is all evidence that is not a descendant of Xi. This deﬁnition is similar to traditional measures of causal ef- fect [Rosenbaum and Rubin, 1983] except the key difference here is that we are comparing the effect of intervention on Xi to no intervention at all.
Given the effect, we then deﬁne the expected causal effect as: EXi[Eﬀect(xi → xj, Z)] = P (Xi = xi|Z) Eﬀect(xi → xj, Z) (6) (cid:88) xi∈Xi Note that to compute the expectation, we use all of the ev- idence Z, as we only want to consider effects on outcomes that are possible given the evidence.
For example, if we ob- serve the output of a DNN is true, then the causal effect of any variable on a false output is always zero (for a binary DNN output).
Using this formulation, we have a simple yet effec- tive measure to quantify the impact of various DNN inputs and concepts on outputs.
4 Concept Extraction In order to constuct our causal model we would ﬁrst like to create a set of interpretable concepts C that satisfy the above causal semantics.
One way to construct concepts that sat- isfy these semantics would be to consider network activa- tions.
The goal would then be to learn a causal model relating these activations.
The concepts that one chooses, however, do not need to be restricted to those represented explicitly by the network activations to properly satisfy the semantics.
As a simple example, one could consider inserting two linear dense layers in a deep neural network such that their weight matrices W1 and W2 were inverses of each other.
The ac- tivations after multiplication by W1 could take the form of any linear combination of the prior activations, yet the ﬁnal network output would be unaffected.
So the speciﬁc repre- sentation of instance features given by activation values does not necessarily have any special relevance.
We instead choose to ﬁnd a concept representation, that is, a transformation on the activations, that’s maximally interpretable.
In addition to satisfying the causal intervention criteria described in section 3, these interpretable concepts should satisfy a few additional criteria: 1.
Concepts should be low-dimensional to minimize the amount of investigation a human would need to employ.
2.
Concepts should be interpretable - in the case of images, we would like activations to be restricted to contiguous areas containing consistent, interpretable visual features.
3.
Concepts should contain all of the relevent information needed for achieving the target network’s task (image classiﬁcation in the cases we consider).
We create an auxilliary neural network model that con- structs concept representations satisfying these properties through training with a specially designed loss function.
Speciﬁcally, the form of this model would be that of an au- toencoder, whereby speciﬁc compression and interpretability losses could be applied, and retention of information required for classiﬁcation can be ensured by the application of recon- struction losses.
This approach has been recently employed to construct interpretable representations of learned features used in classiﬁcation [Qi and Li, 2017].
Our approach differs in two main ways.
First, rather than training the autoencoder to match output classiﬁcation based on a linear function of our coded features, we employ two different reconstruction losses in our autoencoder.
Second, we train multiple autoen- coders throughout the deep neural network to construct in- terpretable representations of the activations throughout the network.
To elaborate on the loss function we employ, a ”shallow” reconstruction loss is applied that is simply the L1 norm of the difference between the input and the output activations of our autoencoder.
We also employ a ”deep” reconstruction loss that ensures that reconstructed activations result in the same classiﬁcation output after being passed through the rest of the network.
This loss takes the form of the KL-divergence be- tween the output probability distributions for the original net- work and copy of the network with an autoencoder inserted at a given activation.
That is, for the target activations ai, coding function cθ, decoding function dθ, and function r describing the application of the rest of the network layers following the autoencoded activation layer: Lshallow(θ; ai) = |dθ(cθ(ai)) − ai|1 Ldeep(θ; ai) = KL(r(ai)||r(dθ(cθ(ai)))) (7) (8) The deep reconstruction loss is enforced much more strongly with a large loss weighting hyperparameter λdeep >> λshallow.
This allows our decoder to reconstruct slightly altered network activations while ensuring that im- portant downstream network activations are unaffected.
The added ﬂexibility of ﬁtting the deep loss instead of the shallow loss enables our autoencoder to learn more interpretable rep- resentations by relaxing the requirement that activations are decoded precisely.
We additionally apply an ”interpretabil- ity” loss which serves to mathematically quantify properties that one would associate with interpretable concept images, much like prior work [Qi and Li, 2017] - in particular we em- ploy a sparsity loss, a cross-entropy loss and a total-varation loss to encourage spatially smooth, independent coded con- cept features.
Our total autoencoder loss is then: L(θ; xi) =λshallowLshallow(θ; xi)+ λdeepLdeep(θ; xi)+ λinterpretabilityLinterpretability(θ; xi) (9) The weighting hyperparameters were chosen by inspect- ing results and tuning by factors of 10 until output seemed reasonable - this only took 1 or 2 iterations of manual reﬁne- ment per dataset.
We trained autoencoders in this manner on activations at multiple layers throughout the network.
The deep reconstruction loss is of particular beneﬁt at the shal- lower layers, where one might not expect to be able to ﬁt a linear classiﬁer based on simple edge detectors.
The training process proceeds by ﬁrst training an autoencoder for the shal- lowest desired layer, then inserting the trained autoencoder into the network and training the next deepest autoencoder, iterating until all autoencoders have been trained.
For our ex- periments we train 3-4 autoencoders spaced evenly through- out the convolutional layers of the target network.
Each au- toencoder consists of 3 convolutional layers in the coding and decoding networks.
See Figure 1 for a depiction of the archi- tecture used for this training.
See Figures 2 and 3 for de- pictions of the resulting coded activations for sample input image instances.
Figure 1: A schematic depiction of training autoencoders for activa- tion compression.
This depicts the architecture used for training an autoencoder for a single layer.
Having trained these autoencoders, we now have a plau- sible approach for intervening on our network.
If one were to intervene directly on network activations to probe its func- tion it would be difﬁcult to maintain the complex correlational statistics between the various components layer activations.
Violating these statistics could result in activation values that would be impossible to recreate with any input sample, re- sulting in misleading output.
In the autoencoder, on the other hand, we expect that correlations between features in the net- work activations would be captured by the encoding network.
Figure 2: Sample display of coded features for an instance from VGG16 trained on the Inria pedestrian dataset.
In the top left corner is the original input image.
Each row corresponds to the extracted coded feature images from a different autoencoder, while each col- umn corresponds to a different extracted feature image from that autoencoder.
Figure 3: Sample display of coded features for an instance from VGG19 trained on birds200.
Intervening on the resulting code should ensure that decoded activations retain their statistics, as well as greatly reduce the size of possible interventions that one might consider.
For our experiments, since we only autoencode with a few convolu- tional layers and do not fully encode all the way to a vector valued distribution, we restrict interventions to zeroing out individual concept feature images of the coded activations.
5 Experiments Having trained autoencoders throughout the network, we now have a set of concepts we can intervene on (by changing the autoencoders code) and a known causal structure represent- ing the relationship of these concepts.
Given this, we con- struct a causal model describing the relationships between our concepts and the output prediction via known methods [Pearl, 2009].
In order to ﬁt our causal model, we construct a large synthetic dataset containing some training set of in- put images and the values of their concept variables.
We also randomly intervene on coded images in the autoencoders by zeroing out the entire feature image.
This has a causal ef- fect on all downstream layers that is captured in the pooled coded values of downstream variables.
We intervene in this way on each coded feature image independently randomly with probability 0.1 and record the resulting values.
We iden- tify active coded feature images through a simple variance threshold, as many of them are always zero as a result of the sparsifying loss terms.
These coded feature images are ﬁnally mean-pooled and binned into k ﬁnite bins.
We mean- pool concept images to make the construction of our bayes nets tractable - in future work we intend to improve on this approach.
We found 2 bins were sufﬁcient to maximize the probability of the data under our model according to these techniques.
The feature values are then treated as variables in Figure 4: A graphical depiction of our learned causal Bayes net for JN6 applied to the Inria pedestrian dataset.
The crossed boxes serve to indicate that all nodes of a given level have edges incident on each of the nodes of the subsequent level.
Figure 5: A resulting sample feature image displaying head identi- ﬁcation on birds200 for the shallowest autoencoder (level0 feat11) displayed alongside the nearest neighbors for that feature in a subset of an augmented dataset.
The query image is the leftmost, as with the rest of these ﬁgures.
a causal Bayes net where each layer of autoencoded variables is dependent only on the variables of the previous layer.
The shallowest autoencoded layer is treated as causally dependent on the class label as well as any other labels associated with the data instance.
Then, having built a large synthetic dataset capturing which interventions were made and the causal effect of those inter- ventions, we construct a bayes net as described above and ﬁt the CPDs of each node to this dataset.
See Figure 4 for a graphical depiction of a sample learned causal model.
With the resulting model we perform a query for individual input instances that ranks variables in the network accord- ing to their maximum causal effect on the output classiﬁca- tion (see Eqn.
6).
This is a single example of the types of causal queries that we can perform on this constructed model.
These experiments were performed on 3 network architec- tures / datasets: (1) VGG 19 applied to Birds200 [Welinder et al., 2010], (2) VGG 16 applied to the Inria pedestrian dataset [Dalal, 2005], and (3) A small 6 layer conv net we refer to as JN6 also applied to the Inria dataset.
We then visualize the top k variables according to their ex- Figure 6: A resulting feature image displaying body color identiﬁ- cation on birds200 for the 2nd autoencoder (level1 feat2).
Expected Causal Effect 0.174704302 0.09731648 0.056770524 0.028265387 0.023817493 0.016577831 0.01370528 0.0123624 0.007728 0.007587164 0.006091733 0.002876711 0.001066667 7.24E-04 Variable level4 feat6 level3 feat3 level2 feat10 level1 feat3 level1 feat17 level0 feat27 level2 feat27 level0 feat1 level4 feat3 level4 feat22 level0 feat14 level4 feat21 level1 feat28 level3 feat28 Figure 9: The resulting expected causal effect query across the entire dataset applied to VGG16 on Inria .
’Level’ denotes which autoen- coder (the shallowest being level0) and ’feat’ indicates which coded feature image channel it refers to (of those that are active after prun- ing).
6 Conclusion To summarize, we describe an approach to explaining the predictions of deep neural networks using causal semantics to relate the output prediction of a network to concepts rep- resented within.
We use a series of autoencoders with loss functions encouraging interpretable properties to construct concepts representing the information content in activations throughout a target network.
These autoencoders are trained with a novel ”deep” loss that allows increased ﬂexibility in representation.
We pool these features and intervene on our autoencoded network to construct variables that we use to build a causal bayesian network which ﬁts the causal rela- tionship deﬁned by the network structure.
We ﬁnally use this network to identify features of signiﬁcant causal relevance to individual classiﬁcations which are then visualized via our described approach.
This an early investigation of ideas in this domain.
There are a number of interesting possible directions for future work.
One clear area of potential improvement is the use of more sophisticated methods to construct variable obser- vations for our bayes net.
In the future we intend to explore the construction of variational autoencoders where all image structure is encoded away to allow for the compression of ir- relevant image structure.
This would greatly increase the size of bayes nets (in terms of incident edges on nodes), which suggests that it may be prudent to consider structure learning for reducing the size of the bayes net skeleton.
Additionally, we’d like to consider the causal relationship between rich in- put labels, network concept features and ultimate classiﬁca- tion.
This could enable direct identiﬁcation of the parts of the network that identify relevant input concepts (eg.
what part of the network detects heads?) and how those components contribute to ultimate classiﬁcation, as well as direct identiﬁ- cation of confounding concepts that could result in incorrect classiﬁcation (eg.
classiﬁcations are often incorrect when it is dark out).
Finally, we are interested in extending these ap- proaches to non-image domains.
Figure 7: A feature image from the 4th autoencoder trained on vgg16 (level3 feat28) applied to the Inria dataset depicting the iden- tiﬁcation of feet.
This concept image has a very low average causal impact on the ﬁnal classiﬁcation (see Figure 9), indicating that visi- bility of feet may not have a major impact on classiﬁcation.
Figure 8: A feature image from the 4th autoencoder trained on vgg16 (level3 feat3) applied to the Inria dataset depicting the iden- tiﬁcation of a person’s outline.
This feature has a large expected causal effect on the output of 0.097.
On this input instance, this con- cept image (level3 feat3) has an individual causal effect of 0.13, the largest causal effect of any concept images on this instance.
i − C k pected causal effect by displaying images along with the cor- responding coded feature image.
We additionally visualize the nearest neighbors in the dataset according to l1 distance between these concept feature images, i.e. |C j i |1 for the speciﬁed concept feature image Ci and input instances j and k.
This helps the user to better interpret the feature image.
See Figures 5,6,7 and 8 for instances of this nearest neighbor visualization.
The ﬁnal goal should enable a user to inter- rogate an input image instance of interest by automatically identifying concepts in the network that are highly relevant to classiﬁcation (as measured by causal effect) and then visu- alize them in the context of other instances that contain that concept in a similar manner.
In Figure 9 we list the expected causal effect over a dataset for VGG16 trained on the Inria pedestrian dataset.
In addition to the depicted average causal effect table, we can query the causal model for individual classiﬁcation instances.
In this way we can identify the feature images with the maximum causal effect for the instance in question and analyze what they represent through nearest neighbor queries.
This could be a highly useful tool for, for instance, debugging misclas- siﬁcations in a DNN.
As an example, in Figure 8 we depict the concept feature image with the largest causal effect on that instance, which is level3 feat3 with an individual causal effect of 0.13.
We intend to develop an interactive tool to en- able queries of this type and enable explanation of instances of interest.
We omit additional speciﬁc results due to lack of space, though we intend to include more in a future release of this work.
Acknowledgments This material is based upon work supported by the United States Air Force under Contract No. FA8750-17-C-0018 for the DARPA XAI program.
Distribution A.
Approved for pub- lic release.
Distribution is unlimited.
References [Bach et al., 2015] Sebastian Bach, Alexander Binder, Gr´egoire Montavon, Frederick Klauschen, Klaus-Robert M¨uller, and Wojciech Samek.
On pixel-wise explanations for non-linear classiﬁer decisions by layer-wise relevance propagation.
PloS one, 10(7):e0130140, 2015.
[Berk, 2017] Richard Berk.
An impact assessment of ma- chine learning risk forecasts on parole board decisions Journal of Experimental Criminology, and recidivism.
13(2):193–216, 2017.
[Binder et al., 2016] Alexander Binder, Sebastian Bach, Gregoire Montavon, Klaus-Robert M¨uller, and Wojciech Samek.
Layer-wise relevance propagation for deep neural network architectures.
In Information Science and Appli- cations (ICISA) 2016, pages 913–922.
Springer, 2016.
[Bojarski and others, 2016] Mariusz Bojarski et al.
End arXiv preprint to end learning for self-driving cars.
arXiv:1604.07316, 2016.
[Cuccaro-Alamin et al., 2017] Stephanie Cuccaro-Alamin, Regan Foust, Rhema Vaithianathan, and Emily Putnam- Hornstein.
Risk assessment and decision making in child protective services: Predictive risk modeling in context.
Children and Youth Services Review, 79:291–298, 2017.
[Dalal, 2005] Navneet Dalal.
Histograms of oriented gradi- ents for human detection.
2005.
[Erhan et al., 2009] Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent.
Visualizing higher-layer features of a deep network.
University of Montreal, 1341:3, 2009.
[Esteva et al., 2017] Andre Esteva, Brett Kuprel, Roberto A Novoa, Justin Ko, Susan M Swetter, Helen M Blau, and Sebastian Thrun.
Dermatologist-level classiﬁcation of skin cancer with deep neural networks.
Nature, 542(7639):115, 2017.
[Gebru et al., 2017] Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, and Li Fei-Fei.
Fine- grained car detection for visual census estimation.
In AAAI, volume 2, page 6, 2017.
[Goodfellow et al., 2014] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy.
Explaining and harnessing adver- sarial examples.
arXiv preprint arXiv:1412.6572, 2014.
[Granger, 1980] Clive WJ Granger.
Testing for causality: a personal viewpoint.
Journal of Economic Dynamics and control, 2:329–352, 1980.
[Mahendran and Vedaldi, 2015] Aravindh Mahendran and Andrea Vedaldi.
Understanding deep image representa- tions by inverting them.
In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 5188–5196, 2015.
[Mordvintsev et al., 2015] Alexander Mordvintsev, Christo- pher Olah, and Mike Tyka.
Inceptionism: Going deeper into neural networks.
Google Research Blog.
Retrieved June, 20:14, 2015.
[Pearl, 2009] Judea Pearl.
Causality.
Cambridge University Press, 2009.
[Pearl, 2018] Judea Pearl.
Theoretical impediments to ma- chine learning with seven sparks from the causal revolu- tion.
arXiv preprint arXiv:1801.04016, 2018.
[Qi and Li, 2017] Zhongang Qi and Fuxin Li. Embedding deep networks into visual explanations.
arXiv preprint arXiv:1709.05360, 2017.
[Ribeiro et al., 2016] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
Why should i trust you?: Explaining the predictions of any classiﬁer.
In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1135–1144.
ACM, 2016.
[Rosenbaum and Rubin, 1983] Paul R Rosenbaum and Don- ald B Rubin.
The central role of the propensity score in observational studies for causal effects.
Biometrika, 70(1):41–55, 1983.
[Selvaraju et al., 2016] Ramprasaath R Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh, and Dhruv Batra.
Grad-cam: Why did you say that?
visual explanations from deep networks via gradient- arXiv preprint arXiv:1610.02391, based localization.
2016.
[Silver et al., 2016] David Silver, Aja Huang, Christopher J.
Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Do- minik Grewe, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis.
Mas- tering the game of go with deep neural networks and tree search.
Nature, 529:484–503, 2016.
John Nham, Nal Kalchbrenner, [Simonyan et al., 2013] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.
Deep inside convolutional networks: Visualising image classiﬁcation models and saliency maps.
arXiv preprint arXiv:1312.6034, 2013.
[Welinder et al., 2010] P.
Welinder, S.
Branson, T.
Mita, C.
Wah, F.
Schroff, S.
Belongie, and P.
Perona.
Caltech- ucsd birds 200.
Technical Report CNS-TR-2010-001, Cal- ifornia Institute of Technology, 2010.
[Woodward, 2005] James Woodward.
Making things hap- pen: A theory of causal explanation.
Oxford university press, 2005.
[Xie et al., 2017] Ning Xie, Md Kamruzzaman Sarker, Derek Doran, Pascal Hitzler, and Michael Raymer.
Re- lating input concepts to convolutional neural network de- cisions.
arXiv preprint arXiv:1711.08006, 2017.
[Zeiler and Fergus, 2014] Matthew D Zeiler and Rob Fergus.
Visualizing and understanding convolutional networks.
In European conference on computer vision, pages 818–833.
Springer, 2014.

Convolutional networks (CNNs) have greatly acceler- ated the progress of many computer vision areas and appli- cations in recent years.
Despite their powerful visual repre- sentational capabilities, CNNs are bottlenecked by their im- mense computational demands.
Recent CNN architectures such as Residual Networks (ResNets) [8, 9] and Inception [34] require billions of ﬂoating-point operations (FLOPs) to perform inference on just one single input image.
Fur- thermore, as the amount of visual data grows, we need in- creasingly higher-capacity (thus higher complexity) CNNs which have shown to better utilize these large visual data compared to their lower-capacity counterparts [33].
There have been works which tackle the efﬁciency is- sues of deep CNNs, mainly by lowering numerical preci- sions (quantization) [14, 27, 41], pruning network weights [6, 21, 39, 10, 23], or adopting separable convolutions [16, 3, 38].
These methods result in more efﬁcient models which have ﬁxed inference costs (measured in ﬂoating-point oper- ations or FLOPs).
Models with ﬁxed inference costs cannot work effectively in certain resource-constrained vision sys- tems, where the computational budget that can be allocated to CNN inference depends on the real-time resource avail- ability.
When the system is lower in resources, it is prefer- able to allocate a lower budget for more efﬁcient or cheaper inference, and vice versa.
Moreover, in some cases, the ex- act inference budget cannot be known beforehand during training time.
As a simple solution to such a concern, one could train several CNN models such that each has a different inference cost, and then select the one that matches the given budget at inference time.
However, it is extremely time-consuming to train many models, not to mention the computational stor- age required to store the weights of many models.
In this work, we focus on CNNs whose computational costs are dynamically adjustable at inference time.
A CNN with cost- adjustable inference only has to be trained once, and it al- lows users to control the trade-off of inference cost against network accuracy/performance.
The different inference in- stances (each with different inference cost) are all derived from the same model parameters.
For cost-adjustable inference in CNNs, we propose a novel training method - Stochastic Downsampling Point (SDPoint).
A SDPoint instance is a network conﬁguration consisting of a unique downsampling point (layer index) in the network layer hierarchy as well as a unique downsam- pling ratio.
As illustrated in Fig.
1, at every training itera- tion, a SDPoint instance is randomly selected (from a list of instances), and downsampling happens based on the down- sampling point and ratio of that instance.
The earlier the downsampling happens, the lower the total computational costs will be, given that spatially smaller feature maps are cheaper to process.
During inference, a SDPoint instance can be determinis- Figure 1: Progression of feature map spatial sizes during training of a (Left) conventional CNN, (Right) with SDPoint.
The costs here refer to computational costs measured in numbers of ﬂoating-point operations (FLOPs).
tically handpicked (among the SDPoint instances seen dur- ing training) to match the given inference budget.
Existing approaches [20, 36, 18] to achieve cost-adjustable inference in CNNs work by evaluating just subparts of the network (e.g., skipping layers or skipping subpaths), and therefore not all network parameters are utilized during cheaper in- ference.
In contrast to existing approaches, SDPoint makes full use of all network parameters regardless of the infer- ence costs, thus making better use of network represen- tational capacity.
Moreover, the (scale-related) parameter sharing across the SDPoint instances (each with a different downsampling and downsampling ratio) provides signiﬁ- cant improvement in terms of model regularization.
On top of these advantages, SDPoint is architecture-neutral, and it adds no parameter or training overheads.
We carry out ex- periments on image classiﬁcation with a variety of recent network architectures to validate the effectiveness of SD- Point in terms of cost-accuracy performances and regular- ization beneﬁts.
The code to reproduce experiments will be released.
2.
Related Work Cost-adjustable Inference: One representative method to achieve cost-adjustable inference is to train “intermediate” classiﬁers [20, 19, 36] which branch out of intermediate network layers.
A lower inference cost can be attained by early-exiting, based on the intermediate classiﬁers’ out- put conﬁdence [20] or entropy [36] threshold.
The lower the threshold is, the lower the inference cost will be, and vice versa.
In [20], intermediate softmax classiﬁers are trained (second stage) after the base network has been com- pletely trained (ﬁrst stage).
The downside of [20] is that the intermediate classiﬁer losses are not backpropagated for ﬁne-tuning the base network weights.
To make the net- works more aware of intermediate classiﬁers, BranchyNet [36] has intermediate classiﬁers (each with more layers per branch than [20]) and ﬁnal classiﬁer trained jointly, us- ing a weighted sum of classiﬁcation losses.
Unlike these works, our SDPoint method relies on the same ﬁnal clas- siﬁer for different inference costs.
FractalNets [18] which are CNNs designed to have many parallel subnetworks or “paths” which can be stochastically dropped for regulariza- tion during training.
For cost-adjustable inference, some FractalNet’s “paths” can be left out.
But the path-dropping regularization gives inconsistent/marginal improvements if data augmentation is being used.
Another line of work somehow related to cost-adjustable inference is adaptive computation in recurrent networks [5] and CNNs [4].
The inference costs of adaptive com- putation networks are adaptive to the given inputs - harder examples cost more than easier ones.
The learned policies of choosing the amount of computation however cannot be modiﬁed during inference for cost-adjustable inference.
Stochastic Regularization: Our work is closely related to stochastic regularization methods which apply certain stochastic operations to network training for regularization.
Dropout [32] drops network activations, while DropCon- nect [37] drops network weights.
Stochastic Depth [13] allows nonlinear residual building blocks to be dropped during training.
These 3 methods are similar in the way that during inference, all stochastically dropped elements (activations, weight, residual blocks) are to be present.
For any of the methods, its different stochastic instances seen during training have rather comparable forward pass costs, making them unﬁt for cost-adjustable inference.
Multiscale parameter-sharing: Multiscale training of CNNs, ﬁrst introduced by [7] is quite similar to SDPoint.
In the training algorithm of [7], the network is trained with 224×224 and 180×180 images alternatively (one scale per epoch).
The same idea has also been applied to CNN train- ing for other tasks [2, 28].
While multiscale training down- samples the input images to different sizes, SDPoint only downsamples feature maps (at feature level).
Downsam- pling at feature level encourages earlier network layers to learn to better preserve information, to compensate for loss of spatial information caused by stochastic downsampling later.
This does not apply to multiscale training, where the input images are downsampled through interpolation oper- ations which happen before network training takes place.
3.
Preliminaries: Conventional CNNs with Fixed Downsampling Points Conventionally, downsampling of feature maps happens in CNNs at several predeﬁned ﬁxed locations/points in the layer hierarchy, depending on the architectural designs.
For example, in ResNet-50, spatial pooling (happens af- ter the ﬁrst ReLU layer, and after the last residual block) and strided convolutions (or convolution with strides > 1 which happens right after the 3rd, 7th, and 13th residual blocks) are used to achieve downsampling.
Between these downsampling layers are network stages.
Downsampling in CNNs trades low-level spatial information for richer high- level semantic information (needed for high-level visual tasks such as image classiﬁcation) in a gradual fashion.
During network inference, these ﬁxed downsampling points have to be followed exactly as how they are conﬁg- ured during training, for optimal accuracy performance.
In this work, we go beyond ﬁxed downsampling points - we develop a novel stochastic downsampling method named Stochastic Downsampling Point (SDPoint) which does not restrict downsampling to happen every time at same ﬁxed points in the layer hierarchy.
The proposed method is com- plementary to the ﬁxed downsampling points in existing network architectures, and do not replace them.
SDPoint can be simply plugged into existing network architectures, and no major architectural modiﬁcations are required.
4.
Stochastic Downsampling Point A Stochastic Downsampling Point (SDPoint) instance has a unique downsampling point p ∈ Z and a unique down- sampling ratio r ∈ R which are stochastically/randomly se- lected during network training.
A p and a r are stochasti- cally selected at the beginning of each network training iter- ation, and downsampling occurs to the selected point (based on the selected ratio) for all samples in the current training mini-batch.
The downsampling points and a downsampling ratios will be discussed more thoroughly in the upcoming sections.
Downsampling is performed by a downsampling function D(·) which makes use of some downsampling op- erations.
When the selected point falls at the lower layer in the layer hierarchy, the downsampling happens earlier (in the forward propagation), causing quicker loss of spa- tial information in the feature maps, but more computation savings.
Conversely, spatial information can be better pre- served at higher computational costs, if the stochastic down- sampling happens later.
SDPoint can effectively turn the feature map spatial sizes right before prediction layers to be different from original sizes, and this could cause shape incompatibility between the prediction layer weights (as well as labels) and the con- volutional outputs (before prediction layers).
To prevent this, we preserve the feature map spatial size in the last net- work stage, regardless of stochastic downsampling taking place or not, by adjusting convolution strides and/or pool- ing sizes accordingly.
For example, in image classiﬁcation networks, we consider the global average pooling layer [22] and the ﬁnal classiﬁcation layer to be the last network stage.
Therefore, regardless of the spatial size (variable due to SD- Point) of the incoming feature maps, we globally pool them to have spatial size of 1 × 1.
4.1. Downsampling Operation As discussed in Sect.
3, the downsampling operation employed in D(·) can be either pooling [1] (average or max variations) or strided convolution.
We opt for average pooling (the corresponding downsampling function is de- noted as Davg(·)), rather than strided convolutions or max pooling for several reasons.
Strided convolutions are the preferred way to do downsampling in recent network ar- chitectures, because they add extra parameters (convolution weights) and therefore improving the representational capa- bility.
In this work, we want to rule out the possible perfor- mance improvements from increase in parameter numbers (rather than the SDPoint itself).
Moreover, strided convo- lutions with integer-valued strides cannot work well with arbitrary downsampling ratios (see Sect.
4.3).
On the other hand, average pooling is preferred over max pooling in this paper due to the fact that max pooling itself is a form of non- linearity.
Using max pooling as the downsampling opera- tion could either push for a greater non-linearity in the net- work (positive outcome) which is unfair to the baselines, or could exacerbate the vanishing gradient problem [11] com- monly associated with deep networks (negative outcome).
Besides, the effectiveness of average pooling has been val- idated through its extensive roles in recent CNN architec- tures (e.g., global average pooling [22, 8], DenseNets’ tran- sition [12]).
4.2. Downsampling Points At every training iteration, a downsampling point p for a SDPoint instance can be drawn from a discrete uniform distribution on a set of predeﬁned downsampling point in- dices P = {0, 1, 2, ...,N -1,N}, with N + 1 number of points.
In this work, the downsampling point candidates are the points between two consecutive CNN “basic build- ing blocks”, mirroring the placements of ﬁxed downsam- pling layers in conventional CNNs. We keep the original network (without stochastic downsampling) as an instance by assigning the index p = 0 to it, so that we can perform full-cost inference later.
Let F (·) denote the function car- ried out by the i-th basic building block, wi denote the net- work weights involved in the block.
For a given input xi and downsampling ratio r, the downsampling is carried out as following: yi = Davg(F (xi; wi); si, r) (1) to obtain the output yi.
The downsampling switch denoted as si ∈ {True, False} is turned on if p = i.
For non-residual CNNs (e.g., VGG-Net [30]), the ba- sic building block comprises 3 consecutive convolutional, Batch Normalization (BN) [15], non-linear activation lay- ers.
On the other hand, for residual networks, residual blocks are considered as the basic building blocks.
the downsampling point p can be stochastically selected to be any point between any 2 basic building blocks in the net- work, where downsampling happens.
Since a residual block involves two streams of information - (i.) the identity skip connection and (ii.) the non-linear function consisting of several network layers, we apply stochastic downsampling function Davg(·) to the point right after the residual addition operation.
We also experiment with Densely Connected Networks (DenseNets) [12] in this paper.
For DenseNets, the SDPoint downsampling points are the points right be- hind each block concatenation operation, mirroring the ﬁxed downsampling in DenseNets.
In principle, each mini-batch sample could have its unique downsampling point pi (for stronger stochasticity), but due to practical reasons (e.g., training efﬁciency, ease of implementation), we resort to using the same pi for all samples in a mini-batch.
While it is possible to have more than one downsampling points in each training iteration, the number of possible combinations or SDPoint instances would become excessively large.
Some of the instances would deviate too much from the original network, in terms of computational cost and accuracy performance.
We opt for single stochastic downsampling point in this work.
4.3. Downsampling Ratios We consider a set of downsampling ratios R, which the SDPoint instance can stochastically draw a downsam- pling ratio r from, for use at current training iteration.
As with Sect.
4.2, downsampling ratios are drawn accord- ing to discrete uniform distributions.
The ratios cannot be too low that they hamper the training convergence (due to parameter-sharing unfeasibility).
And, we consider only a small number of downsampling ratios in R to prevent an excessive number of SDPoint instances, which would cause great difﬁculty in experimentally evaluating all SDPoint in- stances for cost-adjustable inference.
A recent experimental study [24] on CNNs ﬁnds that it is sufﬁcient to make quali- tative conclusions about optimal network structure that hold for the full-sized (224 × 224 image resolution) ImageNet [29] classiﬁcation task, by using just 128 × 128 (roughly half the original resolution) input images.
Conceivably, the same network structure/architecture that works well with a certain image resolution is likely to work well with a resolu- tion double/half of that.
Motivated by the above-mentioned heuristics and experimental ﬁnding, we come up with the downsampling ratio set R = {0.5, 0.75}.
The same ratios have also been used by [2] for “multiscale-input” semantic segmentation.
The same hyperpameter R is used across all experiments in this paper.
Downsampling with such fractional downsampling ra- tios cannot be trivially achieved with integer-valued pooling hyperparameters.
For example, pooling a 28 × 28 feature map to a 21 × 21 one (with r of 0.75 and minimal over- laps) cannot be easily done by tuning just the pooling size and stride.
To this end, we adopt a spatial pooling strat- egy (which works along with the pooling choice in Sect.
4.1) akin to that of Spatial Pyramid Pooling [7] that gener- ates ﬁxed-length representation via adaptive calculations of pooling sizes and strides.
(cid:46) Downsampling Points (cid:46) Downsampling Ratios (cid:46) Forward pass Randomly draw p from P Randomly draw r from R x1 = x for i ∈ {1, 2, ...,N -1,N} do Algorithm 1 : Training with SDPoint 1: P = {0, 1, 2, ...,N -1,N} 2: R = {0.5, 0.75} 3: while given a training mini-batch x do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: end while end for Compute loss with xN +1 Backward pass Parameter updates end if xi+1 = Davg(F (xi; wi); si, r) si = False if i = p then si = True else 4.4. Training with SDPoint SDPoint gives rise to a new training algorithm for CNNs. The training algorithm consolidating all the previously in- troduced SDPoint concepts is given in Algorithm 1.
F (·) denotes the generic nonlinear building network block in CNNs. For simplicity sake, we omit the other network lay- ers which are not basic building blocks - typically the start- ing and ending layers.
In a nutshell, Algorithm 1 shows that whenever a building block index i is equal to the down- sampling point p, the downsampling switch s is turned on.
Stochastic downsampling then happens to the output of i-th building block, with the stochastic downsampling ratio r.
It is important to point out that the (stochastic) downsampling does not happen, if p is drawn to be 0, allowing the network to work in its original “unadulterated” form.
4.5. Regularization SDPoint can be seen as a regularizer for CNNs. When stochastic downsampling takes place, the receptive ﬁeld size becomes larger and it causes a sudden shrinkage of spatial information in the feature maps.
The network has to learn to adapt to such variations during training, and per- form parameter-sharing across the downsampled feature maps and the originally sized feature maps (when p = 0).
In addition to robustness in terms of receptive ﬁeld size and spatial shrinkage, SDPoint also necessitates the convolu- tional layers to accommodate for different “padded pixel to non-padded pixel” ratios.
For example, applying a 3 × 3 convolutional ﬁlter (with zero-padding of 1) to a 8 × 8 fea- ture map gives a padded-pixel ratio of 0.44, compared to 0.56 ratio resulted from applying the same ﬁlter to 6 × 6 feature map.
Zero-padded pixels are quite similar to the zero-ed out activations caused by Dropout [32], in the sense that they both are missing values.
Thus, a higher padded- pixel ratio is akin to having a higher number of dropped-out activations, vice versa.
This form of variation provides fur- ther regularization boost.
Experimentally, we ﬁnd that even with the use of heavy data augmentation - such as “scale + aspect ratio” augmentation [35, 34], SDPoint can still help.
5.
Cost-adjustable Inference A network that can perform inference at different computational costs depending on the user requirements, is considered to be capable of cost-adjustable inference.
Opting for a lower inference cost usually results in a lower prediction accuracy, and vice versa.
SDPoint naturally supports cost-adjustable inference, given that SDPoint instances have varying computational costs, given the different downsampling point locations and downsampling ratios.
More importantly, the instances have all been trained to minimize the same prediction loss, and this helps them to work relatively well for inference.
During inference, one may handpick a SDPoint instance (with its downsampling point p and downsampling ratio r) to make the inference cost ﬁt a particular inference budget.
5.1 Instance-Speciﬁc Batch Normalization As men- tioned in Sect.
4, SDPoint instances are trained in such a way that every training mini-batch and iteration shares the same SDPoint instance.
For a SDPoint instance, the pre- diction and loss minimization during training are based on the Batch Normalization (BN) statistics (means and stan- dard deviations) of that particular instance.
Therefore, us- ing the BN statistics accumulated over many training itera- tions (and thus many different SDPoint instances) for infer- ence causes inference-training “mismatch”.
A similar form of inference-training “mismatch” caused by BN statistics has also been observed by [31] in another context.
The BN statistics required for one SDPoint instance should differ from that of another instance.
When using the same (accu- mulated) BN statistics to perform cost-adjustable inference, the inference accuracies could be jeopardized.
To address the “mismatch” issue, we compute SD- Point instance-speciﬁc BN statistics, and use them for cost- adjustable inference.
Disentangling the different SDPoint instances by unsharing BN statistics makes the inference more accurate.
The computational storage overhead re- sulted from instance-speciﬁc BN statistics is relatively low, as BN statistics of some earlier layers can be shared1 among certain SDPoint instances that downsample at later layers.
6.
Experiments Experiments are carried out on image classiﬁcation tasks to evaluate SDPoint.
We consider image classiﬁcation datasets with varying dataset scales in terms of numbers of categories/classes and sample counts: CIFAR-10 [17] (50k training images, 10k validation images, 10 classes), CIFAR- 100 [17] (50k training images, 10k validation images, 100 classes), ImageNet [29] (1.2M training images, 50k valida- tion images, 1000 classes).
For inference cost comparison, we measure the model costs in terms of ﬂoating-point op- eration numbers (FLOPs) needed for forward propagation of single image.
We treat addition and multiplication as 2 separate operations.
Implementations are in PyTorch [25].
6.1. CIFAR [40] For CIFAR-10 and CIFAR-100, the baseline archi- tectures are Wide-ResNet (WRN-d28-w10 and WRN-d40-w4) and DenseNetBC-d40-g60 [12].
‘d’, ‘w’, ‘g’ stand for the network depth, widen factor of WRN, and growth rate of DenseNetBC, respectively.
The training hyperparameters (e.g., learning rates, schedules, batch sizes, augmentation) follow the ones in original papers, except for training epoch numbers which we ﬁx to 400 for all.
The original learning rate schedules still apply (e.g., learning rates are dropped at 50% and 75% of total number of training epochs).
The numbers of SDPoint downsam- pling points (N ) for {WRN-d28-w10, WRN-d40-w4, and DenseNetBC-d40-g60} are {12, 18 ,12} respectively.
As mentioned in Sect.
4.3, the downsampling ratios are drawn uniformly from R = {0.5, 0.75}.
6.1.1 Baseline Comparison: We compare SDPoint with some baseline methods related to ours, in terms of cost-adjustable inference performance.
The classiﬁcation error-cost performance plots on CIFAR-10 and CIFAR-100 1refer to supplementary materials for more about storage overheads.
Figure 2: WRNs’ and DenseNetBC’s cost-error plots on CIFAR-10 (Top) and CIFAR-100 (Bottom).
It is observed that models trained with SDPoint consistently outperform their non-SDPoint counterparts, given the same computational budgets.
10.8 21.4 10.1 18.7 2.6 2.5/2.6 10.5 10.5 36.5M 6.5/10.1 34.4M 68.1M 15.3M 25.6M 8.9M 8.9M 36.5M 36.5M Model ResNeXt-d29-c08 [38] ResNeXt-d29-c16 [38] DenseNetBC-d250-g24 [12] DenseNetBC-d190-g40 [12] WRN-d40-w4 [40] WRN-d40-w4 [40] with SDPoint WRN-d28-w10 [40] WRN-d28-w10 [40] with Dropout [32] WRN-d28-w10 [40] with SDPoint DenseNetBC-d40-g60 [12] DenseNetBC-d40-g60 [12] with SDPoint # Params GFLOPs CIFAR-10 CIFAR-100 17.77 17.31 17.60 17.18 20.78 19.55 (↓ 1.23) 18.51 18.05 (↓ 0.46) 17.53 (↓ 0.98) 20.00 19.25 (↓ 0.75) Table 1: CIFAR-10 and CIFAR-100 validation errors (%).
The GFLOPs with 2 values separated by “/” are for CIFAR- 10 and CIFAR-100 respectively.
3.65 3.58 3.62 3.46 4.29 3.73 (↓ 0.56) 3.84 3.86 (↑ 0.02) 3.35 (↓ 0.49) 3.99 3.39 (↓ 0.60) 4.3M 4.3M 3.6 2.7/3.6 are shown in Fig.
2.
Note that for SDPoint and baseline methods, not all instances of the same model appear on the plots; if a higher-cost instance performs worse than any lower-cost instance, it is not shown.
Each model (evaluated on a dataset) is trained only once to obtain its cost-error plot.
(i) Early-Exits (EE) We train models based on the WRN with intermediate classiﬁers (branches) which allow early- exits (EE), following the design of BranchyNet [36].
Each network stage in the main network has two evenly spaced branches, and the branches each have single-repetition of building block per branch network stage.
The blocks in the branches follow the same hyperparameters (e.g., #channels) as the blocks in the original network.
For cost-adjustable inference, we evaluate every branch, and make all samples “exit” at the same branch.
The early-exit models have considerably more parameters than both the baseline models and SDPoint-based models.
We conjecture that the relatively worse performance of EE is due to lack of full network parameter ultilization.
Also, EE forces CNN features to be classiﬁcation-ready in early stage, thus causing higher layers to rely heavily on the classiﬁcation- ready features, instead of learning better features on their own.
(ii) Multiscale Training (MS) Multiscale (MS) training is a baseline method inspired by [7, 2, 28].
The input images are downsampled using bilinear interpolations, to an integer-valued size randomly chosen from sizes ranging from half (16 × 16) to full size (32 × 32), with step size of 1 pixel.
This is done for every training iteration, similar to SDPoint.
The number of “instances” (16) resulted from multiscale training is close to the downsampling point num- bers of applying SDPoint to WRNs and DenseNetBC(s).
Also, the ranges of cost-adjustable inference costs among them are comparable.
Instance-speciﬁc BN statistics are applied.
The cost-adjustable performance of MS consis- tently trails behind that of SDPoint, as input downsampling causes more drastic information loss than feature map downsampling (see Sect.
2).
(iii) Uniform Batch Normalization (UBN) To validate the effectiveness of SDPoint instance-speciﬁc BN, we show the results of a SDPoint baseline whose BN statistics are averaged from many training iterations, and are uniform for all of its instances.
There are consistent classiﬁcation performance gaps between using UBN statistics and instance-speciﬁc BN statistics, suggesting that it is prefer- able to keep instance-speciﬁc statistics for inference.
6.1.2 State-of-the-art Comparison: Table 1 reports Model ResNeXt-d101-c64 [38] DenseNetBC-d264 [26] ResNeXt-d101-c32 [38] ResNeXt-d101-c32 [38] with SDPoint PreResNet-d101 [9] PreResNet-d101 [9] with SDPoint PreResNet-d101 [9] with SACT [4] PreResNet-d101 [9] with SDPoint # Params GFLOPs ∼32 ∼26 16.0 16.0 ∼89M ∼73M 44.3M 44.3M 15.7 15.7 11.1 44.7M 44.7M 45.0M 44.7M Top-1 20.4 20.4 21.2 20.4 (↓ 0.8) 22.0 21.4 (↓ 0.6) 24.4 Top-5 5.3 5.6 5.3 (↓ 0.3) 6.1 5.6 (↓ 0.5) 7.2 7.7 24.3 7.2 Table 2: ImageNet top-1 and top-5 validation errors (%), with model parameter numbers and giga-FLOPs (GFLOPs).
the CIFAR validation results of state-of-the-art (SOTA) ResNeXt [38] and DenseNetBC [12] models, for compar- ison with ours.
For each SDPoint-enabled model, we show the results (giga-FLOPs, classiﬁcation errors) from the best- performing SDPoint instance among its instances.
Notably, WRN-d28-w10 with SDPoint is competitive to SOTA mod- els on CIFAR-100, and it outperforms them on CIFAR- 10.
Overall, SDPoint considerably improves classiﬁcation performance without bringing in additional parameters and computational costs, unlike the SOTA models which re- quire about 2× model complexity to attain slight improve- ments.
In fact, the best SDPoint-enabled models on CIFAR- 10 have reduced inference costs (FLOPs).
We reckon that a prolonged preservation of spatial details (i.e., no early downsampling) in CNN feature maps is not crucial to a dataset with relatively low label complexity such as CIFAR- 10.
This reveals a drawback of current practice of using CNNs in “one-size-ﬁts-all” fashion.
6.2. ImageNet We consider ResNeXt-d101-c32 [38] and PreResNet- d101 [9] as baseline architectures.
‘c’ stands for ResNeXt’s cardinality.
With SDPoint, there are 33 downsampling points (N ) per model.
We train the models on ImageNet-1k [29] training set, and evaluate them on the validation set (224×224 center crops).
All models are trained using training hyperparameters and “scale + aspect ratio” aug- mentation [35] identical to [38].
Note that we do not allocate more training epochs to models with SDPoint.
The cost-error plots are given in Fig.
3 and 4, for PreResNet- d101 and ResNeXt-d101-c32 respectively, along with some ﬁxed-cost & carefully designed2 baseline models from the same architecture families.
Overall, models trained with SDPoint can roughly match the performance of baseline models in the lower-cost range, and surpass them in the upper-cost range.
Notably, to obtain cost-error plots, SDPoint-enabled models only have to be trained once.
The 2model hyperparameters are carefully chosen by the authors [9, 38] to optimize accuracy performances under some budget constraints.
Figure 3: PreResNets’ [9] cost-error plots on ImageNet.
PreResNet-d101 (SDPoint) only has to be trained once (as a single model), while the baseline models (without SDPoint) has to be trained separately with huge training and storage costs.
Figure 4: ResNeXts’ [38] cost-error plots on ImageNet.
Like Fig.
3, any of ResNeXt-d101-c32 (SDPoint..) only has to be trained once (as a single model).
baseline models are trained separately, resulting in a huge total number of epochs (#models × #epochs per model) and storage cost.
6.2.1 Ablation Study: We study the effects of choice of SDPoint downsampling points and downsampling ratios on cost-adjustable inference performance.
For this, we train a ResNeXt-d101-c32 with default SDPoint hyperparameters (downsampling points at the end of every residual block, downsampling ratios of {0.5,0.75}), as well as 2 baseline models with (i) downsampling points at the end of every other residual block dubbed alternate (ii) downsampling ratio of just {0.75} dubbed 075.
They are shown on Fig.
4.
Either removing the 0.5 downsampling ratio or alternating blocks for downsampling gives worse results, due to reduced stochasticity (and regularization strengths).
6.2.2 State-of-the-art Comparison: compare our models with SOTA ResNeXt-d101-c64 [38] and DenseNetBC-d264-g48 [26] models in Table 2.
SDPoint pushes the top-1 and top-5 validation errors of ResNeXt- We 51015202530GFLOPs22232425262728Top-1 Validation Error (%)PreResNet-d34PreResNet-d50PreResNet-d101PreResNet-d152PreResNet-d200PreResNet-d101 (SDPoint)51015202530GFLOPs21222324252627Top-1 Validation Error (%)ResNeXt-d50-c32ResNeXt-d101-c32ResNeXt-d101-c64ResNeXt-d101-c32 (SDPoint-075)ResNeXt-d101-c32 (SDPoint-alternate)ResNeXt-d101-c32 (SDPoint)Figure 5: Some Imagenet validation examples grouped according to the minimum inference costs (FLOPs) required by ResNeXt-d101-c32 (with SDPoint) to classify them correctly, in terms of top-5 accuracy.
The ground-truth label names are shown below their corresponding images.
d101-c32 down to 20.4% and 5.3% respectively, which are (previously) only attainable by SOTA models with roughly 2× inference costs and parameter counts.
We also display the results (and mean FLOPs) of Spatially Adaptive Computation Time (SACT) [4] paired with PreResNet-d101, and compare it to a SDPoint instance of our PreResNet-d101 that achieves similar classiﬁcation errors.
SDPoint merely needs 69% of FLOPs needed by SACT to achieve similar results.
SACT saves computation by skipping layers (and network parameters) for certain locations in feature maps according to learned policy and inputs, while SDPoint downsamples feature maps to save computation (but makes full use of network parameters & capacity during inference).
We contend that in cost- accuracy trade-off for inference, reducing feature map spatial sizes is less harmful to accuracy than skipping network parameters/layers.
6.2.3 Analysis: We provide some analyses of ResNeXt- d101-c32 (trained with SDPoint on ImageNet) with regards to certain aspects of downsampling and SDPoint.
Cost-dependent misclassiﬁcations: We group ImageNet validation images (which are correctly classiﬁed with full inference cost) according to the minimum inference costs required to classify them correctly, and present some examples on Fig.
5.
More difﬁcult examples that require higher inference costs (9.9, 16.0 GFLOPs) to be classi- ﬁed correctly, generally have size-dominant interfering objects/scenes (e.g., hair dryer, cab, caldron, cock, tench), in contrast to the easier examples (4.3 GFLOPs).
Intuitively, pooling-based downsampling causes more information loss to smaller objects than to larger (size-dominant) objects, especially when it occurs at some early layer, where the semantic/context information is still relatively weak to distinguish objects of interest from interfering objects.
So, for those difﬁcult examples, it makes sense to preserve spatially informative object details longer in the CNN layer hierarchy, and downsample the feature maps only after they are semantically rich enough.
Scale sensitivity: Training CNNs with SDPoint involves stochastic downsampling of intermediate feature maps, which we hypothesize to be beneﬁcial for scale sensitiv- ity/invariance, as mentioned in Sect.
4.5. To validate this hypothesis, we vary the pre-cropping3 sizes of ImageNet validation images in the range of 256, ..., 352 with step size of 16, resulting in 7 pre-cropping sizes.
For every pre- cropping size, 224 × 224 center image regions are cropped out for evaluation.
The models involved are SDPoint- enabled ResNeXt-d101-c32, and the baseline without SD- Point.
We compute the mean of all pairwise cosine simi- larities (a total of 21 pairs) resulted from the different pre- cropping sizes, in terms of ImageNet 1k-class probabil- ity scores.
This is done for entire ImageNet validation set.
The pairwise cosine-similarity mean obtained for baseline model is 0.944, while for the SDPoint-enabled model, it is 0.961.
A higher cosine similarity is a strong indicator of the model being less sensitive to scales.
This demonstrates that SDPoint can indeed beneﬁt CNNs, in terms of scale sensi- tivity.
7.
Conclusion We propose Stochastic Downsampling Point (SDPoint), a novel approach to train CNNs by downsampling inter- mediate feature maps.
At no extra parameter and training costs, SDPoint facilitates effective cost-adjustable inference and greatly improves network regularization (thus accuracy performance).
Through experiments, we additionally ﬁnd out that SDPoint can help to identify more optimal (yet less costly) sub-networks (Sect.
6.1.2), sort input examples by various levels of classiﬁcation difﬁculties (Fig.
5), and mak- ing CNNs less scale-sensitive (Sect.
6.2.3).
3It is a standard practice [8, 9, 38, 12] to resize images to have a shorter side of 256 (pre-cropping size) before doing 224 × 224 center-cropping.
References [1] Y.-L.
Boureau, J.
Ponce, and Y.
LeCun.
A theoretical analy- sis of feature pooling in visual recognition.
In International Conference on Machine Learning (ICML), 2010.
3 [2] L.-C.
Chen, G.
Papandreou, I.
Kokkinos, K.
Murphy, and A.
L.
Yuille.
Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully con- nected crfs.
IEEE Transactions on Pattern Analysis and Ma- chine Intelligence (TPAMI), 2017.
2, 4, 6 [3] F.
Chollet.
Xception: Deep learning with depthwise sepa- rable convolutions.
In Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
1 [4] M.
Figurnov, M.
D.
Collins, Y.
Zhu, L.
Zhang, J.
Huang, D.
Vetrov, and R.
Salakhutdinov.
Spatially adaptive compu- In Conference on Com- tation time for residual networks.
puter Vision and Pattern Recognition (CVPR), 2017.
2, 7, [5] A.
Graves.
Adaptive computation time for recurrent neural networks.
arXiv preprint arXiv:1603.08983, 2016.
2 [6] S.
Han, J.
Pool, J.
Tran, and W.
Dally.
Learning both weights and connections for efﬁcient neural network.
In Conference on Neural Information Processing Systems (NIPS), 2015.
1 [7] K.
He, X.
Zhang, S.
Ren, and J.
Sun.
Spatial pyramid pooling in deep convolutional networks for visual recognition.
In European Conference on Computer Vision (ECCV), 2014.
2, 4, 6 [8] K.
He, X.
Zhang, S.
Ren, and J.
Sun.
Deep residual learning In Conference on Computer Vision for image recognition.
and Pattern Recognition (CVPR), 2016.
1, 3, 8 [9] K.
He, X.
Zhang, S.
Ren, and J.
Sun.
Identity mappings in deep residual networks.
In European Conference on Com- puter Vision (ECCV), 2016.
1, 7, 8 [10] Y.
He, X.
Zhang, and J.
Sun.
Channel pruning for accelerat- ing very deep neural networks.
In International Conference on Computer Vision (ICCV), 2017.
1 [11] S.
Hochreiter.
Untersuchungen zu dynamischen neuronalen netzen.
Diploma, Technische Universit¨at M¨unchen, 91, 1991.
3 [12] G.
Huang, Z.
Liu, L.
van der Maaten, and K.
Q.
Weinberger.
Densely connected convolutional networks.
In Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
3, 4, 5, 6, 7, 8 [13] G.
Huang, Y.
Sun, Z.
Liu, D.
Sedra, and K.
Q.
Weinberger.
Deep networks with stochastic depth.
In European Confer- ence on Computer Vision (ECCV, pages 646–661.
Springer, 2016.
2 [14] I.
Hubara, M.
Courbariaux, D.
Soudry, R.
El-Yaniv, and In Conference on Y.
Bengio.
Binarized neural networks.
Neural Information Processing Systems (NIPS), 2016.
1 [15] S.
Ioffe and C.
Szegedy.
Batch normalization: Accelerating deep network training by reducing internal covariate shift.
In International Conference on Machine Learning (ICML), pages 448–456, 2015.
4 [16] M.
Jaderberg, A.
Vedaldi, and A.
Zisserman.
Speeding up convolutional neural networks with low rank expansions.
In British Machine Vision Conference (BMVC), 2014.
1 [17] A.
Krizhevsky.
Learning multiple layers of features from tiny images.
2009.
5 [18] G.
Larsson, M.
Maire, and G.
Shakhnarovich.
Fractal- In In- net: Ultra-deep neural networks without residuals.
ternational Conference on Learning Representations (ICLR), 2017.
2 [19] C.-Y.
Lee, S.
Xie, P.
Gallagher, Z.
Zhang, and Z.
Tu. Deeply- supervised nets.
In Artiﬁcial Intelligence and Statistics (AIS- TATS, 2015.
2 [20] S.
Leroux, S.
Bohez, T.
Verbelen, B.
Vankeirsbilck, P.
Simoens, and B.
Dhoedt.
Resource-constrained classiﬁ- cation using a cascade of neural network layers.
In Interna- tional Joint Conference on Neural Networks (IJCNN), 2015.
[21] H.
Li, A.
Kadav, I.
Durdanovic, H.
Samet, and H.
P.
Graf.
Pruning ﬁlters for efﬁcient convnets.
In International Con- ference on Learning Representations (ICLR), 2017.
1 [22] M.
Lin, Q.
Chen, and S.
Yan.
Network in network.
In In- ternational Conference on Learning Representations (ICLR), 2014.
3 [23] J.-H.
Luo, J.
Wu, and W.
Lin.
Thinet: A ﬁlter level pruning In Interna- method for deep neural network compression.
tional Conference on Computer Vision (ICCV), 2017.
1 [24] D.
Mishkin, N.
Sergievskiy, and J.
Matas.
Systematic eval- uation of convolution neural network advances on the ima- genet.
Computer Vision and Image Understanding (CVIU), 2017.
4 [25] A.
Paszke, S.
Gross, S.
Chintala, and G.
Chanan.
Pytorch.
http://pytorch.org/.
5 [26] G.
Pleiss, D.
Chen, G.
Huang, T.
Li, L.
van der Maaten, and K.
Q.
Weinberger.
Memory-efﬁcient implementation of densenets.
arXiv preprint arXiv:1707.06990, 2017.
7 [27] M.
Rastegari, V.
Ordonez, J.
Redmon, and A.
Farhadi.
Xnor- net: Imagenet classiﬁcation using binary convolutional neu- ral networks.
In European Conference on Computer Vision (ECCV), 2016.
1 [28] J.
Redmon and A.
Farhadi.
Yolo9000: Better, faster, stronger.
In Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
2, 6 [29] O.
Russakovsky, J.
Deng, H.
Su, J.
Krause, S.
Satheesh, S.
Ma, Z.
Huang, A.
Karpathy, A.
Khosla, M.
Bernstein, et al.
Imagenet large scale visual recognition challenge.
In- ternational Journal of Computer Vision (IJCV, 115(3):211– 252, 2015.
4, 5, 7 [30] K.
Simonyan and A.
Zisserman.
Very deep convolutional networks for large-scale image recognition.
In International Conference on Learning Representations (ICLR), 2015.
4 [31] S.
Singh, D.
Hoiem, and D.
Forsyth.
Swapout: Learning an ensemble of deep architectures.
In Conference on Neural Information Processing Systems (NIPS), pages 28–36, 2016.
[32] N.
Srivastava, G.
E.
Hinton, A.
Krizhevsky, I.
Sutskever, and R.
Salakhutdinov.
Dropout: a simple way to prevent neu- ral networks from overﬁtting.
Journal of Machine Learning Research (JMLR), 15(1):1929–1958, 2014.
2, 5, 6 [33] C.
Sun, A.
Shrivastava, S.
Singh, and A.
Gupta.
Revisiting unreasonable effectiveness of data in deep learning era.
In International Conference on Computer Vision (ICCV), 2017.
[34] C.
Szegedy, S.
Ioffe, V.
Vanhoucke, and A.
A.
Alemi.
Inception-v4, inception-resnet and the impact of residual connections on learning.
In AAAI Conference on Artiﬁcial Intelligence, 2017.
1, 5 [35] C.
Szegedy, W.
Liu, Y.
Jia, P.
Sermanet, S.
Reed, D.
Anguelov, D.
Erhan, V.
Vanhoucke, and A.
Rabinovich.
Going deeper with convolutions.
In Conference on Computer Vision and Pattern Recognition (CVPR), 2015.
5, 7 [36] S.
Teerapittayanon, B.
McDanel, and H.
Kung.
Branchynet: Fast inference via early exiting from deep neural networks.
In International Conference on Pattern Recognition (ICPR), 2016.
2, 6 [37] L.
Wan, M.
Zeiler, S.
Zhang, Y.
L.
Cun, and R.
Fergus.
Reg- ularization of neural networks using dropconnect.
In Inter- national Conference on Machine Learning (ICML), 2013.
2 [38] S.
Xie, R.
Girshick, P.
Doll´ar, Z.
Tu, and K.
He. Ag- gregated residual transformations for deep neural networks.
In Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
1, 6, 7, 8 [39] T.-J.
Yang, Y.-H.
Chen, and V.
Sze.
Designing energy- efﬁcient convolutional neural networks using energy-aware In Conference on Computer Vision and Pattern pruning.
Recognition (CVPR), 2017.
1 [40] S.
Zagoruyko and N.
Komodakis.
Wide residual networks.
In British Machine Vision Conference (BMVC), 2016.
5, 6 [41] C.
Zhu, S.
Han, H.
Mao, and W.
J.
Dally.
Trained ternary quantization.
In International Conference on Learning Rep- resentations (ICLR), 2017.
1
A network anomaly is a deviation from the normal operation of the network, which is learned through observation and is signified by decreased network performance.
In this paper, we are interested in four network performance measurements: throughput, packet loss rate, one-way delay (OWD), and traceroute.
Throughput measures the amount of data that can be transferred over a time interval.
The interval was chosen to be 25 seconds and is deemed not too long to cause undue stress on a link yet not too short to have unreliable measurements.
Packet loss is the percentage of lost packets over the total transferred packets.
To reach a sensitivity of 10​-5 we measure it at 10 Hz and average result in one-minute bins.
One-way delay measures delay (in ms) separately for each direction of a path.
Traceroute is the path and transition time between the source and destination.
Anomalies can last anywhere from one hour to multiple days (if not noticed) and can be caused     by a multitude of factors.
Possibilities include: full connectivity disruption, in which all packets are lost; a device on the path is close to saturation, signified by an increase in one-way delay as packets spend more time in the device’s buffers; a device on the path is saturated, where packets are lost as a device’s buffers overflow, signified by increase in one-way delay; routing changes leading to asymmetrical paths which takes more time to reorder packets, signified by large variance in one-way delays; or dirty fibers or other problems with optics, signified by an increase in packet loss rate.
We collect our data from perfSONAR [1] servers.
perfSONAR is a network measurement toolkit that monitors and stores network performance data between pairs of endpoints ("links").
While PerfSONAR is installed on thousands of servers, we are especially interested in ones that are part of the WLCG (Worldwide LHC Computing Grid) and OSG (Open Science Grid) meshes.
The network mesh size is extremely large.
With an ever-increasing amount of links and issues in network performance, it becomes increasingly difficult to identify where, when, and why these issues arise and whether or not they are significant enough to ignore.
Furthermore, due to the high variance and quantity of data collected, it becomes difficult to analyze and develop models of normal network behavior and of anomalies.
Machine learning algorithms are thus favorable as they can learn what is normal     Abstract  1    behavior and subsequently what is anomalous behavior and adapt to changes in the structure of normal data.
Ideally an optimal anomaly detection method would satisfy the following criteria: Have the capability to naturally combine disparate data features and even data sources, e.g. all the links to or from a     site, Packet loss, OWDs, paths, throughputs, FTS measurements, etc.; give information on what features (combinations of features) are causing the anomaly, and alert appropriate persons responsible for solving the issue; have tunable sensitivity, as we are not interested in short duration (order of 1 hour) flukes as no action can be taken at time scales shorter than that; and perform at a practical level.
Packet loss  We are especially interested in packet loss due to its extraordinary influence on throughput.
As seen in Figure 1, even a 0.02% change in packet loss can cause a 10​3​ magnitude change in throughput.
Figure 1.​ The relation of packet loss (%) to throughput.
Related Work     Machine learning techniques have been widely used in anomaly detection [2].
Su presented a density method of using the k-nearest neighbors algorithm (kNN) to detect denial-of-service attacks [3].
Sakurada et al.
published a reconstruction method of using auto autoencoders to detect anomalies on both artificial data generated from Lorenz system and real data from a spacecraft’s telemetry data [4].
Rajasegarar et al.
described a boundary method of using one-class support vector machines for anomaly detection in sensor networks [5].
Recently, Catmore proposed a split-sample classification method and other ideas for anomaly detection [6].
This paper presents two new methods based on split-sample classification and reconstruction [6].
2.
Datasets  2    We use two different datasets to test the functionality of the new machine learning algorithms as applied to network anomaly detection.
Figure 2 shows an example of simulated data.
Simulated Dataset      A simulation of the actual data was generated to test the functionality of our methods.
We defined six time series (features) spanning a seven-day period from 08-01-2017 00:00:00 to 08-07-2017 23:59:59.
Data for each time series was assigned a value between 0 and 1 and was generated for each second.
Normal data was generated for each time series by generating a random value less than 0.5 and then generating random values with a normal distribution between 0.00625 and 0.05 around that number.
Normal data was flagged as 0.
Table 1 is an example of such generated data.
Anomalous data was 2 or 5 standard deviations from the normal noise distribution.
Anomalous data was generated six times throughout the entire time period and had a maximum duration of 4 hours.
Table 2 is an example of generated anomalous data.
Figure 2 is an example of simulated data.
Table 1.​ The first five seconds of generated normal data.
Time  2017-08-03 07:36:42 2017-08-03 07:58:06  2017-08-01 06:23:52 2017-08-01 07:06:09  2017-08-05 18:30:38 2017-08-05 19:24:01  2017-08-02 11:27:58 2017-08-02 12:21:16  2017-08-05 07:20:14 2017-08-05 10:35:35  2017-08-03 19:20:06 2017-08-03 20:17:46  Affected  [2, 5]  [2, 0, 1, 4]  [1, 3, 2, 4, 5]  [5, 2, 3]  [2, 1, 4, 0, 5]  [3, 1]  Table 2.​ The time period and features affected for each anomaly generated by the simulated data.
Feature numbers  were listed in descending order of significance towards the data.
3    Figure 2.
This figure is an example of simulated data.
Each link, all assigned a specific color, is a different feature of the dataset and the pink columns signify anomalies.
The vertical axis is the value of the data point for each feature and has arbitrary units.
Real-World Datasets  The data collected between various links measuring their one-way delay, throughput, and packet loss was used to test the functionality of our methods to see if they work on a practical level.
Figures 3a and 3b show examples of real-world data that would be analyzed.
Figure 3a.
This graph shows packet loss measured over a 20 day period between 2017-05-10 and 2017-05-30 for one link where the source is PIC and the destination is CERN.
CERN is a Tier-0 site in Geneva, Switzerland, PIC is a Spanish Tier-1 center.
4    Figure 3b.
This graph shows the square root of packet loss (%)  over a six-day period between 05-14-2017 and 05-20-2017 for 5 sites: CERN-PROD, FZK-LCG2, UFlorida-HPC, UKI-NORTHGRID-MAN-HEP, CSCS-LCG2.
3.
New Anomaly Detection Method  There are numerous algorithms for anomaly detection in time series.
Among more general methods used are eg.
ARIMA, SVM, and more specific ones eg.
Bayesian inspired approaches [7].
We have no annotated historical data so that limits us to unsupervised methods.
The amount of data that has to be continuously processed excludes several otherwise promising approaches.
Similar to the split-sample classification method [6], in the following we compare time-dependent features in the period under examination (subject period) and in the directly preceding period (referent period).
We do this by trying to train a Boosted Decision Tree (BDT)[8] or a Neural Network (NN) to correctly assign unlabeled samples to subject or referent periods.
We expect that any significant difference in data between the two periods will be exploited by the BDT/NN.
It follows that the accuracy of the classification will be proportional to the dissimilarity of the samples.
We selected the referent periods of 24 hours as significantly long to not be sensitive to short duration incidents but short enough to capture long-term changes in link performance.
Subject period of one hour is roughly the minimal period that one could expect a human intervention to happen in case alert was received.
For both approaches, we flag the reference data with zero and the subject data with one.
Further, 70% of the data from both the reference and subject periods are combined and used to train the machine learning models.
Training effectiveness was then tested on the remaining 30% of the data.
A.
Boosted Decision Trees  A decision tree is a rule-based learning method [2] that creates a classification model, which  predicts the value of a target variable by learning simple decision rules inferred from the data features.
A  decision tree of depth 1 is known as a decision stump.
Decision trees make a split based on the Gini impurity, a measure of how often a randomly  chosen element from the data set would be incorrectly labeled if it were labeled randomly.
A higher Gini  impurity suggests a less pure split.
For a set of items with ​A​ classes, where ​i​ ϵ {1, 2, … , ​A​}.
Let ​p​i​ be the fraction of items labeled as  class ​i ​in the set.
The Gini impurity can be calculated as follows [9]:  Ig (p) = 1 − ∑ i=1 pi 2   5  As examples of decision trees and stumps, Figures 4a and 4b show decision trees of different  depths.
Figure 4a.​ This diagram is an example of a decision tree of depth 6.
Figure 4b.
This diagram is an example of three different decision stumps or decision trees of depth 1.
Taking the first stump as an example, the tree splits the data on feature 2 with a Gini impurity of 0.0768 taking 63001 pieces of sample data.
It was able to separate the data whereby the right column has a Gini impurity of 0.0, signifying that it was perfectly categorized.
We apply AdaBoost [10], a boosted decision tree algorithm, to our datasets.
Boosting is a family of machine learning algorithms that start from weak classifiers, i.e. classifiers that label examples slightly better than random guessing, and return a weighted vote of all of them, the result of which is much more accurate at labeling.
Boosted decision trees train a decision tree by dividing data on one of the features.
Then, it validates the decision tree against the training data to find misclassified data values.
Each value is assigned a weight, determining its importance.
The weight of misclassified values is increased and the weight of correctly classified values is decreased, then a new tree is built.
Afterwards, it uses the original tree and the newly formed tree and tests against the training data again to find misclassified values.
Another tree is formed, and this process is repeated for however many weak classifiers, i.e. estimators, desired.
Each tree will produce a weighted vote between 0 or 1 on whether or not something is an anomaly.
The majority vote is used to determine the classification (e.g., anomaly or not) of datasets.
B.
Simple feedforward neural network  With recent advances in both hardware performance and availability (GPUs), and software stack (Keras, Tensorflow), it became possible to relatively quickly train neural networks with a large number of trainable parameters.
We chose to start with a neural network consisting of one input layer with as many ReLU activation neurons as the number of time series data under investigation, one hidden layer with twice as many ReLU neurons and one sigmoid activated output neuron.
We selected this topology as a 6    surely sufficient to capture any effects in training data.
In a future work, we will investigate the performance of a simpler, one layer network as this would simplify finding which time series contributed most to a period being flagged as anomalous.
In order to increase performance, will also optimize learning rates, try different neuron activations, optimizers,  etc.
Training and testing data were prepared in the same way as for the BDT model, except that training data gets shuffled after each epoch.
We train for 60 epochs in batches of 256 samples using Adam optimizer [11] and as a loss function use binary cross-entropy.
Training for one period takes around 20 seconds on a Tesla 20K NVidia GPU and roughly three times less on NVidia GTX 1080Ti. How well the trained network performed on the test samples is given by the binary accuracy.
An anomaly is flagged based on how likely is the accuracy of that magnitude likely to happen by pure chance.
4.
Experimental Results     A.
Boosted Decision Trees  In this section, we study the effectiveness of BDT in detecting unusual events in network performance data.
We primarily analyze packet loss and one-way delay.
50 estimators were used on the testing data.
An AUC score was determined, and if the score was above a certain threshold then the result was determined to be an anomaly.
A.1. Decision Stumps vs.
Decision Trees      We tested the effectiveness of both boosted decision trees utilizing trees of depth 6 vs.
that of boosted decision trees utilizing decision stumps on our simulated data.
It can be seen in Figures 5a and 5b that using decision stumps versus using decision trees produced roughly equal results.
Both methods were able to identify anomalies of one hour in length.
It is interesting to note that both methods were not able to detect the first anomaly.
This is most likely because those anomalies occurred before the first day (our method uses a day’s worth of data as training data), and as such did not have enough training data previously to identify them.
Though the two results were very comparable, the decision stumps took on average 4 seconds per hour worth of data to run, whereas decision trees took 15 seconds per hour worth of data, nearly four times longer.
We determined that decision stumps applied to BDT were a more practical and efficient method for anomaly detection and were thus used for the remainder of the experiment.
7    Figure 5a.
​This graph shows the results for BDT using decision stumps.
The anomaly detection was performed on simulated data of a period of 7 days.
Red lines indicate where anomalous data was generated, blue shading indicates where the algorithm predicted an anomaly was, and the green is the AUC score for each hour.
Figure 5b.
​This graph shows the results for BDT using decision trees of depth 6.
A.2. Anomaly Duration vs.
Degree of Anomalous Behavior  We wanted to know which had a greater effect on whether an interval was determined to be anomalous: the magnitude of the offset between an anomaly from normal data, the duration of anomalous behavior, or the number of time series affected.
Six anomalies were generated on the same set of normal data.
Figure 6 shows a visual representation of said generated anomalies.
The start of each anomaly was separated by 24 hours.
We fluctuated the duration of anomalous behavior, the number of features affected, and the anomaly offset for each one.
The degree of anomalous behavior encompasses both the number of features affected and the anomaly offset.
Data considered to have affected many features affected three times as many as data that was not (3 features vs 1).
Data with a long duration of anomalous behavior was three times longer than data (3 hours vs 1).
We generated offsets (anomalies) of two amplitudes: small - 2𝝈 and large - 5𝝈 shift.
Normal data had an accuracy of roughly 0.50.
The threshold for anomalous behavior was determined as any AUC score above 0.55.
From the results shown in Table 3 and Figure 6, we see that the change in the anomaly offset had the most significant effect on whether an anomaly was determined to be so.
8    Holding the number of features affected and the duration of anomalous behavior constant, an increase in anomaly offset increased the AUC score by 0.379, a roughly 63.91% increase.
The number of features affected had a less significant effect.
Holding the anomaly offset and anomaly duration constant, an increase in the number of features affected increased the AUC score by 0.267, a roughly 45.03% increase.
The duration of anomalous behavior had the smallest effect on determining an anomaly.
Holding the other two variables constant, an increase in the duration of anomalous behavior decreased the AUC score by 0.068, a roughly 11.47% change.
The results suggest that the extent to which an anomaly is considered as such is more dependent on the anomaly offset and the features affected, that is, the degree of anomalous behavior, more than the anomaly duration.
Anomaly  Offset [𝝈]  Duration  1  2  3  4  5  6  2  2  2  5  5  5  Features  Affected  1  1  3  1  1  3  [h]  1  3  1  1  3  1  AUC score  Hour  Before   0.499  0.500  0.499  0.499  0.500  0.500  Hour  1  0.593  0.525  0.860  0.972  0.693  0.999  Hour  2  Hour  3  0.511  0.501  0.702  0.502  Table 3.
This table shows the AUC scores of six different simulated anomalies all having some combination of anomaly duration, amplitude, and a number of features affected.
Anomaly numbers come in the order of when the anomaly was generated.
Results above our 0.55 cut level are shown in bold letters.
Figure 6.​ This figure shows 6 anomalies generated over a 7 day period.
Each anomaly has some combination of  anomaly duration, amplitude, and a number of features affected.
A.3 Application on real data  We tested the effectiveness of using a boosted decision tree onto both the packet loss and one-way delay data over a 4-day time span between two sites: PIC​and CERN-PROD.
The AUC threshold was set to 0.55.
From Figure 7, it is evident that the AUC threshold is too high, as it detects anomalies far too often to be practical.
9        Figure 7.
This graph shows the boosted decision tree as applied to one-way delay and packet loss data between PIC and CERN-PROD between .
The light blue columns signify areas where the algorithm detected anomalous activity, using an AUC threshold of 0.55.
For each anomaly that was detected, we were able to generate both the feature importance of each time series and a ROC curve, thus allowing us to tell which time serie(s) caused the anomaly.
Figure 8 shows an example of said importance and curve.
Figure 8.
​This figure shows the first anomaly detected, the relative importance of features, and its ROC curve.
By looking at the feature importance, we see that feature 2 had the greatest influence on the AUC score, then feature 3, feature 1, and finally feature 4, which did not factor in at all in generating the anomaly.
Due to the high frequency at which anomalies were detected, the AUC threshold of 0.55 was determined to be impractical.
After adjusting the AUC score to 0.8, the result as seen in Figure 9, became much more practical to be used.
10    Figure 9.​ This graph shows the boosted decision tree as applied to one-way delay and packet loss data between PIC  and CERN-PROD in a 30-day period.
The AUC threshold was 0.80.
B.
Simple feedforward neural network  In this preliminary study, we used two hidden layers with twice as many ReLU neurons as time series and a single sigmoid output neuron.
This results in a network with 2521 trainable parameters.
The result of the test is a binary classification accuracy - defined as a ratio of correctly labeled and total number of samples.
The distribution of accuracies that can be expected by chance depends only on the number of samples in referent and subject intervals and can be seen in Figure 10.
figure shows Figure 10.
This the distribution of accuracies obtained by chance (binomial distribution) corresponding to 12h referent and 1h subject intervals with one sample per second (p=0.92308).
Accuracy larger than ​0.9285​ can happen with <1% chance.
B1.
Applied to simulated data, impact of duration, magnitude of anomaly, and number of affected time  series  We used the simulated data described in ​A2​.
A 12h period prior to the 1h subject period was used as a reference.
This choice gives an accuracy of 0.923 by pure chance, and we consider anomaly detected if the calculated accuracy has less than 1% chance of appearing randomly (0.9285).
From results shown in Table 4 and Figure 11, we see that only the last three anomalies (with offsets of 5𝝈) have been detected, and only it the first hour of anomaly appearance.
Accuracy levels during the first three anomalies and during the last two hours of the fourth anomaly were not simply under the threshold but actually exactly equal to pure chance accuracy.
As it can be seen from Figure 12 loss and accuracy change in a stepwise     11    manner and in case backpropagation optimization does not find any minima result will be equal to chance.
This does not limit the applicability of the method as we are anyhow not interested in small effect anomalies affecting single time series, or repeated identification of the same anomaly.
Anomaly  Offset [𝝈]  Duration  1  2  3  4  5  6  2  2  2  5  5  5  Features  Affected  1  1  3  1  1  3  [h]  1  3  1  1  3  1  Accuracy  hour  before   0.923  0.923  0.923  0.923  0.923  0.923  hour  1  0.923  0.923  0.923  0.942  0.991  0.999  hour  3  0.923  hour  2  0.923  0.923  0.923  Table 4.
This table shows the accuracies of six different simulated anomalies all having some combination of anomaly duration, amplitude, and a number of features affected.
Anomaly numbers come in the order of time the anomaly was generated.
Results above our 0.928 cut level are shown in bold letters.
Figure 11.
This figure shows 6 anomalies generated over a 7 day period.
Anomalies are described in ​Table 4​.
Only the last three anomalies have been detected.
Figure 12.​ Loss and accuracy for each of 60 training epochs.
Anomaly threshold was already reached at epoch 15.
One  possible optimization is early training termination as soon as the  threshold has been reached.
12    B3.
Performance on actual data  When using this method on actual data we use referent interval of 24 hours to average possible anomalies in referent data over longer periods.
Data is measured packet loss between CERN and 20 other sites (from 73 that we have values measured for) covering a four day period.
There is one data point per minute.
Missing data was filled with zero values.
Figure 13.
shows results obtained by training the network for 100 epochs for each interval, using a batch size of 10 and having 70:30 split between training and testing data.
Mean of the chance accuracy distribution is at 0.96 and chance accuracy of more than 0.97 has less than 1% probability.
.
Figure 13.
Packet loss between CERN and 20 other sites (here given as an IP address of their PerfSONAR node).
Shown is the square root of the value to better visualize small values.
The blue line shows binary classification accuracy improvement over the chance value.
Thick black line (“Detected”) marks intervals flagged as anomalous.
5.
Conclusion  This paper presents two new methods of detecting network performance anomaly based on split-sample classification: AdaBoost and Simple feedforward neural network.
Both methods are first tested on simulated datasets to check their sensitivity with respect to duration and amplitude of anomaly.
The boosted decision tree method proved to be very fast (4 seconds evaluation per one hour of data tested) and detected all the simulated anomalies.
An added benefit is that it directly returns ordered list of series according to their contribution to the anomaly being flagged.
With appropriately selected AUC threshold it is possible to tune desired sensitivity/false positive level.
The simple neural network model used was not hyper-parameter optimized and the one network tried proved less sensitive to short and low amplitude changes.
Given that we are looking for the most significant anomalies this is a good feature.
While the evaluation is slower at 20 seconds per hour of data tested, it is still fast enough to be of practical use.
A more significant issue is that it requires a GPU for processing.
Since this is a three-layer network it is difficult to get information on the importance of different time series to the resulting decision.
While results on the actual data are encouraging, before using it in a production environment, different network configurations should be tested (two layers, fewer neurons per layer, etc.) and hyper-parameter tuned.
13    Appendix  All the codes and test data can be found in this repository:  https://github.com/ATLAS-Analytics/AnomalyDetection  References  [1] ​Tierney B., Metzger J., Boote J., Brown A., Zekauskas M., Zurawski J., Swany M., Grigoriev M.,  “perfSONAR: Instantiating a Global Network Measurement Framework”, 4th Workshop on Real  Overlays and Distributed Systems (ROADS’09) Co-located with the 22nd ACM Symposium on  Operating Systems Principles (SOSP), January 1, 2009, LBL-1452E.
[2] V.
Chandola, A.
Banerjee, V.
Kumar.
“Anomaly detection: A survey.” ACM Computing Surveys  (CSUR), Volume 41, Issue.
3, July 2009.
[3] M.Y. Su. “Real-time anomaly detection systems for Denial-of-Service attacks by weighted  k-nearest-neighbor classifiers.” Expert Systems with Applications.
April 2011.
[4] M.
Sakurada, T.
Yairi.
“Anomaly Detection Using Autoencoders with Nonlinear Dimensionality  Reduction.” MLSDA'14 Proceedings of the MLSDA 2014 2nd Workshop on Machine Learning for  Sensory Data Analysis.
2, December 2014.
[5] S.
Rajasegarar, C.
Leckie, J.
C.
Bezdek and M.
Palaniswami, "Centered Hyperspherical and  Hyperellipsoidal One-Class Support Vector Machines for Anomaly Detection in Sensor Networks," in  IEEE Transactions on Information Forensics and Security, vol.
5, no.
3, pp.
518-533, Sept.
2010.
[6] Catmore, James.
“Ideas on Anomaly Detection for Data Quality Monitoring.” University of Oslo.
17,  June 2016.
​http://bit.ly/2GtohJ7  [7] N.
R.
Zhang and D.
O.
Siegmund.
“Model selection for high-dimensional, multi-sequence changepoint  problems.” Statistica Sinica, 22(4):1507–1538, 2012.
[8] ​Friedman, J.
H.
"​Stochastic Gradient Boosting.​" March 1999.
[9] D.
Coppersmith, S.J. Hong, and J.R. Hosking.
“Data Mining and Knowledge Discovery.” June 1999.
[10] Freund Y., Schapire R.E. “A decision-theoretic generalization of on-line learning and an application  to boosting.” In: Vitányi P.
(eds) Computational Learning Theory.
EuroCOLT 1995.
Lecture Notes in  Computer Science (Lecture Notes in Artificial Intelligence), vol 904.
1995.
[11] ​Kingma, D.
P., & Ba, J.
L.
(2015).
Adam: a Method for Stochastic Optimization.
International  Conference on Learning Representations, 1–13.
14
T O reconstruct the complete and precise 3D geometry of an object is essential for many graphics and robotics applications, from AR/VR [1] and semantic understand- ing, to robot grasping [2] and obstacle avoidance.
Classic approaches use the off-the-shelf low-cost depth sensing devices such as Kinect and RealSense cameras to recover the 3D shape of an object from captured depth images.
Those approaches typically require multiple depth images from different viewing angles of an object to estimate the complete 3D structure [3] [4] [5].
However, in practice it is not always feasible to scan all surfaces of an object before reconstruction, which leads to incomplete 3D shapes with occluded regions and large holes.
In addition, acquiring and processing multiple depth views require more computing power, which is not ideal in many applications that require real-time performance.
In this paper, we aim to tackle the problem of estimating the complete 3D structure of an object using a single depth view.
This is a very challenging task, since the partial observation of the object (i.e. a depth image from one view- ing angle) can be theoretically associated with an inﬁnite number of possible 3D models.
Traditional reconstruction approaches typically use interpolation techniques such as plane ﬁtting, Laplacian hole ﬁlling [6] [7], or Poisson surface estimation [8] [9] to infer the underlying 3D structure.
How- ever, they can only recover very limited occluded or missing regions, e.g. small holes or gaps due to quantization arti- facts, sensor noise and insufﬁcient geometry information.
Interestingly, humans are surprisingly good at solving such ambiguity by implicitly leveraging prior knowledge.
• Bo Yang, Stefano Rosa, Andrew Markham and Niki Trigoni are with the Department of Computer Science, University of Oxford, UK.
E-mail:{bo.yang,stefano.rosa,andrew.markham,niki.trigoni}@cs.ox.ac.uk • Corresponding to Hongkai Wen, who is with the Department of Computer Science, University of Warwick, UK.
E-mail: hongkai.wen@dcs.warwick.ac.uk For example, given a view of a chair with two rear legs occluded by front legs, humans are easily able to guess the most likely shape behind the visible parts.
Recent advances in deep neural networks and data driven approaches show promising results in dealing with such a task.
In this paper, we aim to acquire the complete and high- resolution 3D shape of an object given a single depth view.
By utilizing the high performance of 3D convolutional neu- ral nets and large open datasets of 3D models, our approach learns a smooth function to map a 2.5D view to a complete and dense 3D shape.
In particular, we train an end-to-end model which estimates full volumetric occupancy from a single 2.5D depth view of an object.
While state-of-the-art deep learning approaches [10] [11] [2] for 3D shape reconstruction from a single depth view achieve encouraging results, they are limited to very small resolutions, typically at the scale of 323 voxel grids.
As a result, the learnt 3D structure tends to be coarse and inaccurate.
However, to increase the 3D shape resolution without sacriﬁcing recovery accuracy is challenging, as even a slightly higher resolution would exponentially increase the search space of potential 2.5D to 3D mapping functions, resulting in difﬁculties in convergence of neural nets.
Recently, deep generative models achieve impressive success in modeling complex high-dimensional data dis- tributions, among which Generative Adversarial Networks (GANs) [12] and Variational Autoencoders (VAEs) [13] emerge as two powerful frameworks for generative learn- ing, including image and text generation [14] [15], and latent space learning [16] [17].
In the past few years, a number of works [18] [19] [20] [21] applied such generative models to learn latent space to represent 3D object shapes, in order to solve simple discriminative tasks such as new image gener- ation, object classiﬁcation, recognition and shape retrieval.
In this paper, we propose 3D-RecGAN++, a novel model that combines a skip-connected 3D autoencoder with adver- sarial learning to generate a complete and ﬁne-grained 3D structure conditioned on a single 2.5D view.
Particularly, our model ﬁrstly encodes the 2.5D view to a low-dimensional latent space vector which implicitly represents general 3D geometric structures, then decodes it back to recover the most likely full 3D shape.
The rough 3D shape is then fed into a conditional discriminator which is adversarially trained to distinguish whether the coarse 3D structure is plausible or not.
The autoencoder is able to approximate the corresponding shape, while the adversarial training tends to add ﬁne details to the estimated shape.
To ensure the ﬁnal generated 3D shape corresponds to the input single partial 2.5D view, adversarial training of our model is based on a conditional GAN [22] instead of random guessing.
The above novel and efﬁcient network design excels the competing approaches [2] [11] [23], which either use a single fully connected layer [2], a low capacity decoder [11], or the multi-stage and inefﬁcient LSTMs [23] to estimate the full 3D shapes.
Our contributions are as follows: (1) We propose a novel generative model to reconstruct the complete and accurate 3D structure using a single ar- bitrary depth view.
Particularly, our model takes a simple occupancy grid map as input without requiring object class labels or any annotations, while predicting a compelling shape within a high resolution of 2563 voxel grid.
By drawing on both autoencoder and GAN, our approach is end-to-end trainable with high level of generality.
To the best of our knowledge, this is the ﬁrst work that reconstructs such high resolution of 3D shapes using a single view.
(2) We exploit conditional GAN during training to reﬁne the 3D shape estimated by the autoencoder.
Our contribu- tion here is that we use the mean value of a latent vector feature, instead of a single scalar, as the output of the discriminator to stabilize GAN training.
(3) We conduct extensive experiments for single category and multi-category object reconstruction, outperforming the state of the art.
Importantly, our approach is also able to generalize to previously unseen object categories.
At last, our model also performances robustly on real-world dataset collected by Kinect, after being trained purely on synthetic datasets.
(4) To the best of our knowledge, there are no good open datasets which have the ground truth for occluded/missing parts and holes for each 2.5D view in real world scenarios.
We therefore contribute our real world testing dataset to the community.
A preliminary version of this work has been published in ICCV 2017 workshops [24].
Our code and data are available at: https://github.com/Yang7879/3D-RecGAN-extended 2 RELATED WORK We review different pipelines for 3D reconstruction or shape completion.
Both conventional geometry based techniques and the state of the art deep learning based approaches are covered.
(1) 3D Model/Shape Completion.
[25] uses plane ﬁtting to complete small missing regions, while [26] [27] [28] [29] [30] apply shape symmetry to ﬁll in holes.
Although these methods show good results, relying on predeﬁned geometric regularities fundamentally limits the structure space to hand-crafted shapes.
Besides, these approaches are likely to fail when missing or occluded regions are relatively big.
Another similar ﬁtting pipeline is to leverage database priors.
Given a partial shape input, [31] [32] [33] [34] [35] [36] try to retrieve an identical or most likely 3D model and align it with the partial scan.
However, these approaches explicitly assume the database contains identical or very similar shapes, thus being unable to generalize to novel objects or categories.
(2) Multiple RGB/Depth Images Reconstruction.
Tra- ditionally, 3D dense reconstruction requires a collection of RGB images [37].
Geometric shape is recovered by dense feature extraction and matching [38], or by directly minimiz- ing reprojection errors [39] from color images.
Recently, [40] [41] [42] [43] [44] [45] leverage deep neural nets to learn the 3D shape from multiple RGB images.
However, resolution of the recovered occupancy shape is usually up to a small scale of 323.
With the advancement of depth sensors, depth images are also used to recover the object shape.
Classic approaches usually fuse multiple depth images through iterative closest point (ICP) algorithms [3] [46] [47], while recent work [48] learns the 3D shape using deep neural nets from multiple depth views.
(3) Single RGB Image Reconstruction.
Predicting a complete 3D object model from a single view is a long- standing and extremely challenging task.
When reconstruct- ing a speciﬁc object category, model templates can be used.
For example, morphable 3D models are exploited for face recovery [49] [50].
This concept was extended to reconstruct simple objects in [51].
For general and complex object recon- struction from a single RGB image, recent works [52] [53] [54] aim to infer 3D shapes using multiple RGB images for weak supervision.
However, the training procedure of [54] is two stage rather than end-to-end, while [53] uses a sim- ple autoencoder instead of designing sophisticated learn- ing frameworks for shape learning, and [52] still requires 3D shape priors for constraints.
Shape prior knowedge is also required in [55] [56] [57].
To recover high resolution 3D shapes, [58] [59] use Octree representation, while [60] proposed an inverse discrete cosine transform (IDCT) tech- nique.
Lin et al.
[61] designed a pseudo-renderer to predict dense 3D shapes, while [62] sequentially estimates 2.5D sketches and dense 3D shapes from a single RGB image.
(4) Single Depth View Reconstruction.
The task of reconstruction from a single depth view is to complete the occluded 3D structures behind the visible parts.
3D ShapeNets [10] is among the early work using deep neural nets to estimate 3D shapes from a single depth view.
Firman et al.
[63] trained a random decision forest to infer unknown voxels.
Originally designed for shape denoising, VConv- DAE [1] can also be used for shape completion.
To facilitate robotic grasping, Varley et al.
proposed a neural network to infer the full 3D shape from a single depth view in [2].
However, all these approaches are only able to generate low resolution voxel grids which are less than 403 and unlikely to capture ﬁne geometric details.
Recent works [11] [64] [23] [65] can infer higher resolution 3D shapes.
However, 3D- EPN [11] relies on a shape database to synthesize higher resolution shapes after learning a small 323 voxel grid from a depth view, while SSCNet [64] requires strong voxel-level annotations for supervised scene completion and semantic 3 Fig.
2: Overview of the network architecture for training.
Fig.
3: Overview of the network architecture for testing.
After generating training pairs, we feed them into our network.
The ﬁrst part of our network loosely follows the idea of an autoencoder with the U-net architecture [74].
The skip-connected autoencoder serves as an initial coarse generator which is followed by an up-sampling module to further generate a high resolution 3D shape within a 2563 voxel grid.
This whole generator aims to learn a correlation between partial and complete 3D structures.
With the super- vision of complete 3D labels, the generator is able to learn a function f and infer a reasonable 3D shape given a brand new partial 2.5D view.
In the testing phase, however, the results tend to be grainy and without ﬁne details.
To address this issue, in the training phase, the re- constructed 3D shape from the generator is further fed into a conditional discriminator to verify its plausibility.
In particular, a partial 2.5D input view is paired with its corresponding complete 3D shape, which is called the ‘real reconstruction’, while the partial 2.5D view is paired with its corresponding output 3D shape from generator, which is called the ‘fake reconstruction’.
The discriminator aims to discriminate all ‘fake reconstruction’ from ‘real recon- struction’.
In the original GAN framework [12], the task of the discriminator is to simply classify real and fake in- put, but its Jensen-Shannon divergence-based loss function is difﬁcult to converge.
The recent WGAN [75] leverages Wasserstein distance with weight clipping as a loss function to stabilize the training procedure, whilst the extended work WGAN-GP [76] further improves the training process using a gradient penalty with respect to its input.
In our 3D- RecGAN++, we apply WGAN-GP as the loss function of our conditional discriminator, which guarantees fast and stable convergence.
The overall network architecture for training is shown in Figure 2, while the testing phase only needs the well trained generator as shown in Figure 3.
Overall, the main challenge of 3D reconstruction from an arbitrary single view is to generate new information includ- ing ﬁlling the missing and occluded regions from unseen views, while keeping the estimated 3D shape corresponding to the speciﬁc input 2.5D view.
In the training phase, our 3D-RecGAN++ ﬁrstly leverages a skip-connected autoen- coder together with an up-sampling module to generate a reasonable ‘fake reconstruction’ within a high resolution occupancy grid, then applies adversarial learning to reﬁne the ‘fake reconstruction’ to make it as similar to ‘real recon- struction’ by jointly updating parameters of the generator.
Fig.
1: t-SNE embeddings of 2.5D partial views and 3D complete shapes of multiple object categories.
label prediction.
Both [23] and [65] are originally designed for shape inpainting instead of directly reconstructing the complete 3D structure from a partial depth view.
The re- cent 3D-PRNN [66] predicts simple shape primitives using RNNs, but the estimated shapes do not have ﬁner geometric details.
(5) Deep Generative Frameworks.
Deep generative frameworks, such as VAEs [13] and GANs [12], have achieved impressive success in image super-resolution [67], image generation [15], text to image synthesis [68], etc.
Recently, [69] [70] [71] [21] applied generative networks for 3D structure generation.
However, most of them generate 3D shapes from random noise instead of reconstructing structures from a speciﬁc single image.
3 3D-RECGAN++ 3.1 Overview Our method aims to estimate a complete and dense 3D structure of an object, which only takes an arbitrary single 2.5D depth view as input.
The output 3D shape is auto- matically aligned with the corresponding 2.5D partial view.
To achieve this task, each object model is represented in a high resolution 3D voxel grid.
We use the simple occupancy grid for shape encoding, where 1 represents an occupied cell and 0 an empty cell.
Speciﬁcally, the input 2.5D partial view, denoted as x, is a 643 occupancy grid, while the output 3D shape, denoted as y, is a high resolution 2563 probabilistic voxel grid.
The input partial shape is directly calculated from a single depth image given camera parameters.
We use the ground truth dense 3D shape with aligned orientation as same as the input partial 2.5D depth view to supervise our network.
To generate ground truth training and evaluation pairs, we virtually scan 3D objects from ShapeNet [72].
Figure 1 is the t-SNE visualization [73] of partial 2.5D views and the corresponding full 3D shapes for multiple general chair and bed models.
Each green dot represents the t-SNE embedding of a 2.5D view, whilst a red dot is the embedding of the cor- responding 3D shape.
It can be seen that multiple categories inherently have similar 2.5D to 3D mapping relationships.
Essentially, our neural network is to learn a smooth function, denoted as f, which maps green dots to red dots as close as possible in high dimensional space as shown in Equation 1.
The function f is parametrized by convolutional layers in general.
, where Z = {0, 1}(cid:17) (1) (cid:16) y = f (x) x ∈ Z 643 encoderdecoderconditional discriminatorU-netinput 2.5D viewtrue full 3D shapereal reconstructionfake reconstructionxconcatconcatlossup-samplingyencoderdecoderU-netinput 2.5D viewup-samplingxyIn the testing phase, given a novel 2.5D view as input, the jointly trained generator is able recover a full 3D shape with satisfactory accuracy, while the discriminator is no longer used.
3.2 Architecture Figure 4 shows the detailed architecture of our proposed 3D- RecGAN++.
It consists of two main networks: the generator as in Figure 4a and the discriminator as in Figure 4b.
The generator consists of a skip-connected autoencoder and an up-sampling module.
Unlike the vanilla GAN gen- erator which generates data from arbitrary latent distribu- tions, our 3D-RecGAN++ generator synthesizes data from latent distributions of 2.5D views.
Particularly, the encoder has ﬁve 3D convolutional layers, each of which has a bank of 4x4x4 ﬁlters with strides of 1x1x1, followed by a leaky ReLU activation function and a max pooling layer with 2x2x2 ﬁlters and strides of 2x2x2.
The number of output channels of max pooling layer starts with 64, doubling at each sub- sequent layer and ends up with 512.
The encoder is lastly followed by two fully-connected layers to embed semantic information into a latent space.
The decoder is composed of 5 symmetric up-convolutional layers which are followed by ReLU activations.
Skip-connections between encoder and decoder guarantee propagation of local structures of the input 2.5D view.
The skip-connected autoencoder is fol- lowed by the up-sampling module which simply consists of two layers of up-convolutional layers as detailed in Figure 4a.
This simple yet efﬁcient up-sampling module directly upgrades the output 3D shape to a high resolution of 2563 without requiring complex network design and operations.
It should be noted that without the two fully connected layers and skip-connections, the vanilla autoencoder would be unable to learn reasonable complete 3D structures as the latent space is limited and the local structure is not preserved.
Without the efﬁcient up-sampling module, it is unable to ﬁnally generate high resolution 3D shapes.
Al- though a more complicated and dedicated network design could also output 2563 shapes, it would be unlikely to be effectively trained on a single GPU because of the ex- tremely high computation consumption for high resolution 3D shape generation.
The loss function and optimization methods are described in Section 3.3. The discriminator aims to distinguish whether the esti- mated 3D shapes are plausible or not.
Based on the condi- tional GAN, the discriminator takes both real reconstruction pairs and fake reconstruction pairs as input.
Particularly, it consists of six 3D convolutional layers, the ﬁrst of which concatenates the generated 3D shape (i.e. a 2563 voxel grid) and the input 2.5D partial view (i.e. a 643 voxel grid) which is reshaped as a 256x256x4 tensor.
The reshaping process is done straightforwardly using Tensorﬂow ‘tf.reshape()’.
Basically, this is to inject the condition information with a matched tensor dimension, and then leave the network itself to learn useful features from this condition input.
Each convolutional layer has a bank of 4x4x4 ﬁlters with strides of 2x2x2, followed by a ReLU activation function except for the last layer which is followed by a sigmoid activation func- tion.
The number of output channels of the convolutional layers starts with 8, doubling at each subsequent layer and ends up with 256.
At the early stage of GAN training, the high dimen- sional real and fake distributions may not overlap, then the discriminator can separate them perfectly using a single scalar output, which is theoretically analyzed in [77].
In our experiments, the original WGAN-GP always crashes in the early 3 epochs due to the extremely high dimensionality (i.e. 2563 + 643 dimensions).
To stabilize it, we propose to use mean feature (i.e. mean of a vector feature) for discriminator.
As the mean vector feature captures more information from the input overall, it is more difﬁcult for the discriminator to easily distinguish whether the mean feature is from fake or real input.
This enables useful information to back- propagate to the generator.
A theoretical study of the mean feature matching method for GAN is in [78]; mean feature matching is also applied in [79] to stabilize GAN.
Therefore, our discriminator is to distinguish the distri- butions of mean feature of fake and real reconstructions, while the generator is trained to make the two distributions of mean feature as similar as possible.
We apply WGAN-GP as loss functions for our modiﬁed mean feature matching.
3.3 Objectives The objective function of our 3D-RecGAN++ includes two main parts: an object reconstruction loss (cid:96)ae for the genera- tor; the objective function (cid:96)gan for the conditional GAN.
(1) (cid:96)ae For the generator, inspired by [80], we use mod- iﬁed binary cross-entropy loss function instead of the stan- dard version.
The standard binary cross-entropy weights both false positive and false negative results equally.
How- ever, most of the voxel grid tends to be empty, so the net- work easily gets a false positive estimation.
In this regard, we impose a higher penalty on false positive results than on false negatives.
Particularly, a weight hyper-parameter α is assigned to false positives, with (1-α) for false negative results, as shown in Equation 2.
−α ¯yi log(yi)−(1−α)(1− ¯yi) log(1−yi) (2) (cid:20) N(cid:88) i=1 (cid:96)ae = (cid:21) where ¯yi is the target value {0,1} of a speciﬁc ith voxel in the ground truth voxel grid ¯y, and yi is the corresponding estimated value (0,1) in the same voxel from the autoen- coder output y.
We calculate the mean loss over the total N voxels in the whole voxel grid.
(2) (cid:96)gan For the discriminator, we leverage the state of the art WGAN-GP loss functions.
Unlike the original GAN loss function which presents an overall loss for both real and fake inputs, we separately represent the loss function (cid:96)g gan in Equation 3 for generating fake reconstruction pairs and (cid:96)d gan in Equation 4 for discriminating fake and real reconstruction pairs.
Detailed deﬁnitions and derivation of the loss functions can be found in [75] [76], but we modify them for our conditional GAN settings.
(cid:96)g gan = −E(cid:2)D(y|x)(cid:3) gan = E(cid:2)D(y|x)(cid:3) − E(cid:2)D(¯y|x)(cid:3) (cid:17)2(cid:21) (cid:20)(cid:16)(cid:13)(cid:13)∇ ˆyD(ˆy|x)(cid:13)(cid:13)2 − 1 +λE (cid:96)d (3) (4) 5 (a) Generator for 3D shape estimation from a single depth view.
(b) Discriminator for 3D shape reﬁnement.
Fig.
4: Detailed architecture of 3D-RecGAN++, showing the two main building blocks.
Note that, although these are shown as two separate modules, they are trained end-to-end.
where ˆy = ¯y + (1 − )y,  ∼ U [0, 1], x is the input partial depth view, y is the corresponding output of autoencoder, ¯y is the corresponding ground truth.
λ controls the trade-off between optimizing the gradient penalty and the original objective in WGAN.
in all epochs.
As we do not use dropout or batch normal- ization, the testing phase is exactly the same as the training stage.
The whole network is trained on a single Titan X GPU from scratch.
For the generator in our 3D-RecGAN++ network, there are two loss functions, (cid:96)ae and (cid:96)g gan, to optimize.
As we discussed in Section 3.1, minimizing (cid:96)ae tends to learn the overall 3D shapes, whilst minimizing (cid:96)g gan estimates more plausible 3D structures conditioned on input 2.5D views.
To minimize (cid:96)d gan is to improve the performance of discrimi- nator to distinguish fake and real reconstruction pairs.
To jointly optimize the generator, we assign weights β to (cid:96)ae and (1− β) to (cid:96)g gan.
Overall, the loss functions for generator and discriminator are as follows: (cid:96)g = β(cid:96)ae + (1 − β)(cid:96)g gan (cid:96)d = (cid:96)d gan (5) (6) 3.4 Training We adopt an end-to-end training procedure for the whole network.
To simultaneously optimize both generator and discriminator, we alternate between one gradient decent step on discriminator and then one step on generator.
For the WGAN-GP, λ is set as 10 for gradient penalty as in [76].
α ends up as 0.85 for our modiﬁed cross entropy loss function, while β is 0.2 for the joint loss function (cid:96)g.
The Adam solver [81] is used for both discriminator and generator with a batch size of 4.
The other three Adam parameters are set to default values.
Learning rate is set to 0.0001 for the discriminator and 0.0005 for the generator 3.5 Data Synthesis For the task of 3D dense reconstruction from a single depth view, obtaining a large amount of training data is an obsta- cle.
Existing real RGB-D datasets for surface reconstruction suffer from occlusions and missing data and there is no ground truth of complete and high resolution 2563 3D shapes for each single view.
The recent work 3D-EPN [11] synthesizes data for 3D object completion, but their map encoding scheme is the complicated TSDF which is different from our network requirement.
To tackle this issue, we use the ShapeNet [72] database to generate a large amount of training and testing data with synthetically rendered depth images and the corresponding complete 3D shape ground truth.
Particularly, a subset of object categories is selected for our experiments.
For each category, we generate training data from around 220 CAD models, while synthesizing testing data from around 40 CAD models.
For each CAD model, we create a virtual depth camera to scan it from 125 different angles, 5 uni- formly sampled views for each of roll, pitch and yaw space.
For each virtual scan, both a depth image and the corre- sponding complete 3D voxelized structure are generated with regard to the same camera angle.
That depth image is simultaneously transformed to a partial 2.5D voxel grid using virtual camera parameters.
Then a pair of partial 2.5D view and the complete 3D shape is synthesized.
Overall, concat 6431 channel 32364 channels 163128 channels 83256 channels 43512 channels32768 2000 3276843512 channels83256 channels163128 channels32364 channels 64316 channels12838 channels25631 channelconcatconcatconcatconcat43 convlrelu23 maxpool43 convlrelu23 maxpool43 convlrelu23 maxpool43 convlrelu23 maxpoolflattendensereludenserelu43 deconvrelureshape43 deconvrelu43 deconvrelu43 deconvrelu43 deconvrelu43 deconvreluyx256x256x41 channelfake reconstructionreal reconstructionground truth128x128x130          8 channels6431 channely25631 channel256x256x2601 channel43 convlrelu43 convlrelu43 convlrelu43 convlrelu43 convlrelu43 convlreluflatten64x64x65     16 channels32x32x33     32 channels16x16x17     64 channels 8x8x9128 channels 4x4x5256 channelsxloss20480reshape25631 channelup-sampling moduleconcat 6431 channel 32364 channels 163128 channels 83256 channels 43512 channels32768 2000 3276843512 channels83256 channels163128 channels32364 channels 64316 channels12838 channels25631 channelconcatconcatconcatconcat43 convlrelu23 maxpool43 convlrelu23 maxpool43 convlrelu23 maxpool43 convlrelu23 maxpoolflattendensereludenserelu43 deconvrelureshape43 deconvrelu43 deconvrelu43 deconvrelu43 deconvrelu43 deconvreluyx256x256x41 channelfake reconstructionreal reconstructionground truth128x128x130          8 channels6431 channely25631 channel256x256x2601 channel43 convlrelu43 convlrelu43 convlrelu43 convlrelu43 convlrelu43 convlreluflatten64x64x65     16 channels32x32x33     32 channels16x16x17     64 channels 8x8x9128 channels 4x4x5256 channelsxloss20480reshape25631 channelup-sampling modulearound 20K training pairs and 4K testing pairs are gener- ated for each 3D object category.
All data are produced in Blender.
Besides the large quantity of synthesized data, we also collect real world data in order to test the proposed network.
We use a Microsoft Kinect camera to manually scan a set of common objects, such as chairs, tables, etc., from multiple angles.
Then, we use ElasticFusion [47] to reconstruct the full 3D shapes of the objects, as well as the camera pose in each scan.
The 3D objects are manually segmented from the background.
We then extract ground truth information by aligning the full 3D objects with the partial 2.5D views.
It should be noted that, due to noise and quantization artifacts of low-cost RGB-D sensors, and the inaccuracy of the algorithm, the full 3D ground truth is not 100% accurate.
4 EVALUATION In this section, we evaluate our 3D-RecGAN++ with com- parison to the state of the art approaches and an ablation study to fully investigate the proposed network.
4.1 Metrics To evaluate the performance of 3D reconstruction, we con- sider the mean Intersection-over-Union (IoU) between pre- dicted 3D voxel grids and their ground truth.
The IoU for an individual voxel grid is formally deﬁned as follows: (cid:80)N (cid:104) (cid:80)N i=1 (cid:2)I(yi > p) ∗ I( ¯yi)(cid:3) I(cid:0)I(yi > p) + I( ¯yi)(cid:1)(cid:105) i=1 IoU = where I(·) is an indicator function, yi is the predicted value at the ith voxel, ¯yi is the corresponding ground truth, p is the threshold for voxelization, N is the total number of voxels in a whole voxel grid.
In all our experiments, p is set as 0.5. If the predicted value is over 0.5, it is more likely to be occupied from the probabilistic aspect.
The higher the IoU value, the better the reconstruction of a 3D model.
4.2 Competing Approaches We compare against three state of the art deep learning based approaches for single depth view reconstruction.
We also compare against the generator alone in our network, i.e. without the GAN, named as 3D-RecAE for short.
(1) 3D-EPN.
In [11], Dai et al.
proposed a neural network to reconstruct the 3D shape up to a 323 voxel grid, after which a high resolution shape is retrieved from an existing 3D shape database.
For fair comparison, we only compared with their neural network performance.
Besides, occupancy grid representation is used for the network training the testing.
(2) Varley et al.
In [2], a network was designed to complete the 3D shape from a single 2.5D depth view for robot grasping.
The output of their network is a 403 voxel grid.
(3) Han et al.
In [23], a global structure inference network and a local geometry reﬁnement network are proposed to complete a high resolution shape from a noisy shape.
The network is not originally designed for single depth view reconstruction, but its output shape is up to a 2563 voxel grid and is comparable to our network.
For fair comparison, the same occupancy grid representation is used for their net- work.
It should be noted that their network involves many convoluted designs, yet the training procedure is extremely slow and inefﬁcient due to many LSTMs involved.
(4) 3D-RecAE.
As for our 3D-RecGAN++, we remove the discriminator and only keep the generator to infer 3D complete shape from a single depth view.
This comparison illustrates the beneﬁts from adversarial learning.
4.3 Single-category Results (1) Results.
All networks are separately trained and tested on four different categories with the same network conﬁgurations.
To fairly compare the IoU between different approaches, we down sample all results of 2563 voxel grids to 323 using max pooling with a stride of 8 along the three axes.
Table 1 shows the IoU comparison of all methods on 323 voxel grids, while Table 2 shows the IoU comparison of [23] and our approaches on higher resolution of voxel grids.
Figure 5 shows the qualitative results of single category re- construction.
In this paper, the meshgrid function in Matlab is used to plot all 3D shapes for better visualization.
(2) Analysis.
The proposed 3D-RecGAN++ signiﬁcantly outperforms the competing approaches in terms of the IoU at both lower (323 voxel grids) and higher resolutions (2563 voxel grids).
The 3D shapes generated by our 3D- RecGAN++ are much more visually compelling than others in terms of the shape accuracy and the geometrical details.
TABLE 1: Per-category IoU (323 voxel grids).
3D-EPN [11] Varley et al.
[2] Han et al.
[23] 3D-RecAE (ours) 3D-RecGAN++ (ours) bench 0.758 0.653 0.611 0.800 0.806 chair 0.739 0.619 0.524 0.790 0.793 coach 0.834 0.818 0.505 0.858 0.868 table 0.772 0.678 0.615 0.808 0.821 TABLE 2: Per-category IoU (up to 2563 voxel grids).
Han et al.
[23] (643) Han et al.
[23] (1283) Han et al.
[23] (2563) 3D-RecAE (ours) (643) 3D-RecAE (ours) (1283) 3D-RecAE (ours) (2563) 3D-RecGAN++ (ours) (643) 3D-RecGAN++ (ours) (1283) 3D-RecGAN++ (ours) (2563) bench 0.544 0.492 0.417 0.733 0.669 0.552 0.745 0.683 0.564 chair 0.469 0.432 0.389 0.736 0.693 0.622 0.741 0.699 0.628 coach 0.483 0.469 0.431 0.832 0.812 0.722 0.844 0.825 0.733 table 0.560 0.524 0.476 0.759 0.715 0.644 0.772 0.730 0.659 4.4 Multi-category Results (1) Results.
All networks are also trained and tested on multiple categories without being given any class labels.
The networks are trained on four categories: {bench, chair, coach, table}; and then tested separately on an individual category.
Table 3 shows the IoU comparison of all methods on the resolution of 323 voxel grids, while Table 4 shows the IoU comparison of [23] and our methods on higher resolution of voxel grids.
Figure 6 shows the qualitative results of all approaches on multiple categories.
(2) Analysis.
The proposed 3D-RecGAN++ signiﬁcantly outperforms the state of the art by a large margin in all categories which are trained together on a single model.
7 Fig.
5: Qualitative results of per-category reconstruction from different approaches.
Besides, the performance of our network trained on multiple categories, does not degrade compared with training the network on individual categories.
This conﬁrms that our network has enough capacity and capability to learn diverse features from multiple categories.
TABLE 3: Multi-category IoU (323 voxel grids).
3D-EPN [11] Varley et al.
[2] Han et al.
[23] 3D-RecAE (ours) 3D-RecGAN++ (ours) bench 0.758 0.681 0.531 0.789 0.806 chair 0.734 0.630 0.512 0.767 0.784 coach 0.843 0.823 0.469 0.850 0.863 table 0.772 0.691 0.596 0.795 0.806 4.5 Cross-category Results (1) Results.
To further investigate the generality of net- works, we train all networks on {bench, chair, coach, table}, and then test them on another 6 totally different categories: {airplane, car, faucet, guitar, gun, monitor}.
For each of the 6 categories, it has 4625 single arbitrary views from random selected 37 objects for testing, which is the same data size as used in the previous {bench, chair, coach, table}.
Table 5 Han et al.
[23] (643) Han et al.
[23] (1283) Han et al.
[23] (2563) 3D-RecAE (ours) (643) 3D-RecAE (ours) (1283) 3D-RecAE (ours) (2563) TABLE 4: Multi-category IoU (up to 2563 voxel grids).
table 0.534 0.496 0.458 0.741 0.695 0.623 0.753 0.710 0.638 3D-RecGAN++ (ours) (643) 3D-RecGAN++ (ours) (1283) 3D-RecGAN++ (ours) (2563) bench 0.453 0.408 0.364 0.719 0.653 0.537 0.741 0.679 0.562 chair 0.451 0.416 0.390 0.710 0.665 0.595 0.730 0.688 0.618 coach 0.442 0.427 0.398 0.825 0.804 0.715 0.838 0.819 0.728 shows the IoU comparison of all approaches on 323 voxel grids, while Table 6 shows the IoU comparison of [23] and our approaches on higher resolution of voxel grids.
Figure 7 shows the qualitative results of all methods on 6 unseen categories.
We further evaluate the generality of our 3D-RecGAN++ on a speciﬁc category.
Particularly, we conduct four groups of experiments.
In the ﬁrst group, we train our 3D- RecGAN++ on bench, then separately test on the remaining 3 categories: {chair, coach, table}.
In the second group, the 2.5D input(643)3D-EPN(323)Varley et al.(403)Han et al.
(2563)3D-RecAE(2563)3D-RecGAN++(2563)Ground Truth(2563)8 Fig.
6: Qualitative results of multi-category reconstruction from different approaches.
TABLE 5: Cross-category IoU (323 voxel grids).
airplane faucet guitar gun monitor 3D-EPN [11] Varley et al.
[2] Han et al.
[23] 0.715 0.051 0.585 0.722 0.738 TABLE 6: Cross-category IoU (up to 2563 voxel grids).
car 0.723 0.683 0.753 0.742 0.069 0.025 0.041 0.065 0.414 0.431 0.448 0.401 0.805 0.723 0.762 0.725 0.802 0.731 0.769 0.739 3D-RecAE (ours) 3D-RecGAN++ (ours) 0.667 0.524 0.466 0.670 0.683 airplane car faucetguitar gun monitor Han et al.
[23] (643) Han et al.
[23] (1283) Han et al.
[23] (2563) 3D-RecAE (ours) (643) 3D-RecAE (ours) (1283) 3D-RecAE (ours) (2563) 0.373 0.372 0.356 0.341 0.296 0.517 0.318 0.350 0.319 0.286 0.245 0.470 0.248 0.299 0.284 0.225 0.198 0.414 0.602 0.774 0.678 0.700 0.665 0.661 0.547 0.745 0.645 0.643 0.605 0.614 0.365 0.557 0.520 0.413 0.381 0.529 0.615 0.769 0.687 0.709 0.682 0.680 3D-RecGAN++ (ours) (643) 3D-RecGAN++ (ours) (1283) 0.559 0.740 0.653 0.651 0.625 0.633 3D-RecGAN++ (ours) (2563) 0.379 0.555 0.529 0.424 0.399 0.547 network is trained on chair and separately tested on {bench, coach, table}.
Similarly, another two groups of experiments are conducted.
Basically, this experiment is to investigate how well our approach learns features from one category and then generalizes to a different category, and vice versa.
Table 7 shows the cross-category IoU of our 3D-RecGAN++ trained on individual category over 2563 voxel grids.
(2) Analysis.
The proposed 3D-RecGAN++ achieves much higher IoU across the unseen categories than compet- ing approaches.
Our network not only learns rich features from different object categories, but also is able to generalize well to completely new types of categories.
This implies that our network may learn geometric relationships such as lines, planes, curves which are common across various object categories.
It can be also observed that our model trained on bench tends to be more general than others, because bench is likely to have more general features to be learned, while simple categories such as coach are unlikely to consist of many general features that are shared across different categories.
4.6 Real-world Experiment Results (1) Results.
Lastly, in order to evaluate the domain adap- tation capability of the networks, we train all networks on synthesized data of categories {bench, chair, coach, table}, 2.5D input(643)3D-EPN(323)Varley et al.(403)Han et al.
(2563)3D-RecAE(2563)3D-RecGAN++(2563)Ground Truth(2563)9 Fig.
7: Qualitative results of cross-category reconstruction from different approaches.
TABLE 7: Cross-category IoU of 3D-RecGAN++ trained on individual category (2563 voxel grids).
Group 1 (trained on bench) Group 2 (trained on chair) Group 3 (trained on coach) Group 4 (trained on table) bench 0.482 0.387 0.477 chair 0.510 0.460 0.495 coach 0.549 0.509 0.449 table 0.592 0.549 0.401 and then test them on real-world data collected by a Mi- crosoft Kinect camera.
The real-world data were collected in different environments, including ofﬁces, homes, and outdoor university parks, as shown in Figure 8.
Compared to synthesized data, real-world partial 2.5D views are nois- ier and largely incomplete.
For each object, we randomly selected 20 different 2.5D depth views for testing.
Table 8 shows the IoU performance of all approaches using 323 voxel grids, while Table 9 compares the IoU of [23] and our approaches on higher resolutions.
Figure 9 shows some qualitative results for all methods.
(2) Analysis.
There are two reasons why the IoU is signiﬁcantly lower compared with testing on the synthetic dataset.
First, the ground truth objects obtained from Elastic- Fusion are empty rather than solid, and are only occupied on the surface.
However, all networks predict dense and solid voxel grids, so the interior of bulky objects like couches is not matching.
Secondly, the input 2.5D depth view from real world dataset is noisy and incomplete, due to the limitation of the RGB-D sensor (e.g., reﬂective surfaces, outdoor light).
In many cases, the input 2.5D view does not capture the whole object and only contains a small part of the object, which also leads to failure cases (e.g. the 6th row in Figure 9) and a lower IoU scores overall.
However, our proposed network is still able to reconstruct reasonable 3D dense shapes given the noisy and incomplete 2.5D input 2.5D input(643)3D-EPN(323)Varley et al.(403)Han et al.
(2563)3D-RecAE(2563)3D-RecGAN++(2563)Ground Truth(2563)10 Fig.
8: Real world objects for reconstruction.
depth views, while the competing algorithms (e.g. Varley et al.) are not robust to real world noise and unable to generate compelling results.
TABLE 8: Real-world multi-category IoU (323 voxel grids).
3D-EPN [11] Varley et al.
[2] Han et al.
[23] 3D-RecAE (ours) 3D-RecGAN++ (ours) bench 0.350 0.043 0.291 0.352 0.347 chair 0.273 0.0.008 0.240 0.295 0.297 coach 0.173 0.038 0.159 0.199 0.201 table 0.246 0.009 0.228 0.268 0.268 TABLE 9: Real-world multi-category IoU (up to 2563 voxel grids).
Han et al.
[23] (643) Han et al.
[23] (1283) Han et al.
[23] (2563) 3D-RecAE (ours) (643) 3D-RecAE (ours) (1283) 3D-RecAE (ours) (2563) 3D-RecGAN++ (ours) (643) 3D-RecGAN++ (ours) (1283) 3D-RecGAN++ (ours) (2563) bench 0.184 0.117 0.070 0.231 0.145 0.071 0.230 0.147 0.074 chair 0.148 0.092 0.045 0.178 0.096 0.032 0.174 0.097 0.032 coach 0.101 0.067 0.040 0.107 0.055 0.024 0.109 0.057 0.025 table 0.126 0.069 0.031 0.148 0.071 0.024 0.146 0.069 0.023 4.7 Impact of Adversarial Learning (1) Results.
In all above experiments, the proposed 3D- RecGAN++ tends to outperform the ablated network 3D- RecAE which does not include the adversarial learning of GAN part.
In all visualization of experiment results, the 3D shapes from 3D-RecGAN++ are also more compelling than 3D-RecAE.
To further investigate how the adversarial learning improves the ﬁnal 3D results comparing with 3D- RecAE, we calculate the mean precision and recall from the above multi-category experiment results.
Table 10 shows the mean precision of 3D-RecGAN++ and 3D-RecAE on individual categories using the network trained on multiple categories, while Table 11 shows the mean recall.
(2) Analysis.
It can be seen that the results of 3D- RecGAN++ have much higher precision scores than 3D- RecAE, which means 3D-RecGAN++ has much less false positive estimations, while 3D-RecAE tends to estimate much more false positives.
Therefore, the estimated 3D shapes from 3D-RecAE are likely to be ’fatter’ and ’big- ger’, while 3D-RecGAN++ tends to output ’thinner’ shapes with much more shape details being exposed.
Both 3D- RecGAN++ and 3D-RecAE can achieve extremely high re- call scores (i.e. above 0.9), although 3D-RecGAN++ has lower recall scores compared with 3D-RecAE.
This means both 3D-RecGAN++ and 3D-RecAE are capable of estimat- ing almost all of the object shapes without too many false negatives.
In other words, the ground truth 3D shape tends to be a subset of the estimated shape result.
TABLE 10: Multi-category mean precision (up to 2563 voxel grids).
3D-RecAE (323) 3D-RecAE (643) 3D-RecAE (1283) 3D-RecAE (2563) 3D-RecGAN++ (323) 3D-RecGAN++ (643) 3D-RecGAN++ (1283) 3D-RecGAN++ (2563) bench 0.823 0.755 0.688 0.564 0.856 0.798 0.737 0.608 chair 0.802 0.746 0.700 0.626 0.832 0.781 0.739 0.663 coach 0.868 0.843 0.822 0.730 0.888 0.866 0.847 0.751 table 0.825 0.773 0.727 0.651 0.845 0.797 0.755 0.676 TABLE 11: Multi-category mean recall (up to 2563 voxel grids).
3D-RecAE (323) 3D-RecAE (643) 3D-RecAE (1283) 3D-RecAE (2563) 3D-RecGAN++ (323) 3D-RecGAN++ (643) 3D-RecGAN++ (1283) 3D-RecGAN++ (2563) bench 0.952 0.939 0.930 0.921 0.934 0.915 0.900 0.887 chair 0.945 0.935 0.929 0.924 0.931 0.918 0.908 0.902 coach 0.978 0.975 0.974 0.972 0.968 0.964 0.961 0.959 table 0.956 0.947 0.940 0.935 0.946 0.934 0.925 0.918 Overall, with regard to experiments on per-category, multi-category, and cross-category experiments, our 3D- RecGAN++ outperforms others by a large margin, although all other approaches can reconstruct reasonable shapes.
In terms of the generality, Varley et al.
[2] and Han et al.
[23] are inferior because [2] uses a single fully connected layers, instead of 3D ConvNets, for shape generation which is unlikely to be general for various shapes, and [23] applies LSTMs for shape blocks generation which is inefﬁcient and 11 Fig.
9: Qualitative results of real world objects reconstruction from different approaches.
unable to learn general 3D structures.
However, our 3D- RecGAN++ is superior thanks to the generality of simple yet efﬁcient 3D autoencoder and the 3D convolutional dis- criminator.
Besides, the 3D-RecAE tends to over estimate the 3D shape, while the adversarial learning of 3D-RecGAN++ is likely to remove the over-estimated parts, so as to leave the estimated shape to be clearer with more shape details.
5 DISCUSSION Although our 3D-RecGAN++ achieves the state of the art performance in 3D object reconstruction from a single depth view, it has limitations.
Firstly, our network takes the volumetric representation of a single depth view as input, instead of taking a raw depth image.
Therefore, a preprocessing of raw depth images is required for our network.
However, in many application scenarios such as robot grasping, such preprocessing would be trivial and straightforward given the depth camera parameters.
Sec- ondly, the input depth view of our network only contains a clean object information without cluttered background.
One possible solution is to leverage an existing segmentation algorithm such as Mask-RCNN [82] to clearly segment the target object instance from the raw depth view.
6 CONCLUSION In this work, we proposed a novel framework 3D- RecGAN++ that reconstructs the full 3D structure of an object from an arbitrary depth view.
By leveraging the generalization capabilities of autoencoders and generative adversarial networks, our 3D-RecGAN++ predicts dense and accurate 3D structures with ﬁne details, outperforming the state of the art in single-view shape completion for individual object category.
We further tested our network’s ability to reconstruct multiple categories without providing any object class labels during training or testing, and it showed that our network is still able to predict precise 3D shapes.
Besides, we investigated the network’s recon- struction performance on unseen categories, our proposed approach can also predict satisfactory 3D structures.
Finally, our model is robust to real world noisy data and can infer accurate 3D shapes although the model is purely trained on synthesized data.
This conﬁrms that our network has the capability of learning general 3D latent features of the objects, rather than simply ﬁtting a function for the training datasets, and the adversarial learning of 3D-RecGAN++ learns to add geometric details for estimated 3D shapes.
In summary, our network only requires a single depth view to recover a dense and complete 3D shape with ﬁne details.
2.5D input(643)3D-EPN(323)Varley et al.(403)Han et al.
(2563)3D-RecAE(2563)3D-RecGAN++(2563)Ground Truth(2563)[2] REFERENCES [1] A.
Sharma, O.
Grau, and M.
Fritz, “VConv-DAE : Deep Volumetric Shape Learning Without Object Labels,” ECCV, 2016.
J.
Varley, C.
Dechant, A.
Richardson, J.
Ruales, and P.
Allen, “Shape Completion Enabled Robotic Grasping,” IROS, 2017.
[3] R.
A.
Newcombe, S.
Izadi, O.
Hilliges, D.
Molyneaux, D.
Kim, A.
J.
Davison, P.
Kohli, J.
Shotton, S.
Hodges, and A.
Fitzgibbon, “KinectFusion: Real-time dense surface mapping and tracking,” ISMAR, 2011.
[4] M.
Nießner, M.
Zollh¨ofer, S.
Izadi, and M.
Stamminger, “Real-time 3D reconstruction at scale using voxel hashing,” ACM Transactions on Graphics, vol.
32, no.
6, pp.
1–11, 2013.
[5] F.
Steinbrucker, C.
Kerl, J.
Sturm, and D.
Cremers, “Large-Scale Multi-Resolution Surface Reconstruction from RGB-D Sequences,” ICCV, 2013.
[6] A.
Nealen, T.
Igarashi, O.
Sorkine, and M.
Alexa, “Laplacian Mesh Optimization,” SIGGRAPH, 2006.
[7] W.
Zhao, S.
Gao, and H.
Lin, “A robust hole-ﬁlling algorithm for triangular mesh,” The Visual Computer, vol.
23, no.
12, pp.
987–997, 2007.
[8] M.
Kazhdan, M.
Bolitho, and H.
Hoppe, “Poisson Surface Recon- struction,” Symposium on Geometry Processing, 2006.
[9] M.
Kazhdan and H.
Hoppe, “Screened poisson surface reconstruc- tion,” ACM Transactions on Graphics, vol.
32, no.
3, pp.
1–13, 2013.
[10] Z.
Wu, S.
Song, A.
Khosla, F.
Yu, L.
Zhang, X.
Tang, and J.
Xiao, “3D ShapeNets: A Deep Representation for Volumetric Shapes,” CVPR, 2015.
[11] A.
Dai, C.
R.
Qi, and M.
Nießner, “Shape Completion using 3D- Encoder-Predictor CNNs and Shape Synthesis,” CVPR, 2017.
[12] I.
J.
Goodfellow, J.
Pouget-Abadie, M.
Mirza, B.
Xu, D.
Warde- Farley, S.
Ozair, A.
Courville, and Y.
Bengio, “Generative Adver- sarial Nets,” NIPS, 2014.
[13] D.
P.
Kingma and M.
Welling, “Auto-Encoding Variational Bayes,” ICLR, 2014.
[14] Z.
Hu, Z.
Yang, X.
Liang, R.
Salakhutdinov, and E.
P.
Xing, “Controllable Text Generation,” ICML, 2017.
[15] T.
Karras, T.
Aila, S.
Laine, and J.
Lehtinen, “Progressive Growing of GANs for Improved Quality, Stability, and Variation,” ICLR, 2018.
[16] X.
Chen, Y.
Duan, R.
Houthooft, J.
Schulman, I.
Sutskever, and P.
Abbeel, “InfoGAN: Interpretable Representation Learning by In- formation Maximizing Generative Adversarial Nets,” NIPS, 2016.
[17] T.
D.
Kulkarni, W.
F.
Whitney, P.
Kohli, and J.
B.
Tenenbaum, “Deep Convolutional Inverse Graphics Network,” NIPS, 2015.
[18] E.
Grant, P.
Kohli, and M.
V.
Gerven, “Deep Disentangled Repre- sentations for Volumetric Reconstruction,” ECCV Workshops, 2016.
[19] R.
Girdhar, D.
F.
Fouhey, M.
Rodriguez, and A.
Gupta, “Learning a Predictable and Generative Vector Representation for Objects,” ECCV, 2016.
[20] H.
Huang, E.
Kalogerakis, and B.
Marlin, “Analysis and synthe- sis of 3D shape families via deep-learned generative models of surfaces,” Computer Graphics Forum, vol.
34, no.
5, pp.
25–38, 2015.
[21] J.
Wu, C.
Zhang, T.
Xue, W.
T.
Freeman, and J.
B.
Tenenbaum, “Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling,” NIPS, 2016.
[22] M.
Mirza and S.
Osindero, “Conditional Generative Adversarial Nets,” arXiv, 2014.
[23] X.
Han, Z.
Li, H.
Huang, E.
Kalogerakis, and Y.
Yu, “High- Resolution Shape Completion Using Deep Neural Networks for Global Structure and Local Geometry Inference,” ICCV, 2017.
[24] B.
Yang, H.
Wen, S.
Wang, R.
Clark, A.
Markham, and N.
Trigoni, “3D Object Reconstruction from a Single Depth View with Adver- sarial Learning,” ICCV Workshops, 2017.
[25] A.
Monszpart, N.
Mellado, G.
J.
Brostow, and N.
J.
Mitra, “RAPter: Rebuilding Man-made Scenes with Regular Arrangements of Planes,” ACM Transactions on Graphics, vol.
34, no.
4, pp.
1–12, 2015.
[26] N.
J.
Mitra, L.
J.
Guibas, and M.
Pauly, “Partial and Approximate Symmetry Detection for 3D Geometry,” SIGGRAPH, 2006.
[27] M.
Pauly, N.
J.
Mitra, J.
Wallner, H.
Pottmann, and L.
J.
Guibas, “Discovering structural regularity in 3D geometry,” ACM Transac- tions on Graphics, vol.
27, no.
3, p.
1, 2008.
[28] I.
Sipiran, R.
Gregor, and T.
Schreck, “Approximate Symmetry Detection in Partial 3D Meshes,” Computer Graphics Forum, vol.
33, no.
7, pp.
131–140, 2014.
[29] P.
Speciale, M.
R.
Oswald, A.
Cohen, and M.
Pollefeys, “A Symme- try Prior for Convex Variational 3D Reconstruction,” ECCV, 2016.
12 [30] S.
Thrun and B.
Wegbreit, “Shape from symmetry,” ICCV, 2005.
[31] Y.
M.
Kim, N.
J.
Mitra, D.-M.
Yan, and L.
Guibas, “Acquiring 3D Indoor Environments with Variability and Repetition,” ACM Transactions on Graphics, vol.
31, no.
6, 2012.
[32] Y.
Li, A.
Dai, L.
Guibas, and M.
Nießner, “Database-Assisted Ob- ject Retrieval for Real-Time 3D Reconstruction,” Computer Graphics Forum, vol.
34, no.
2, pp.
435–446, 2015.
[33] L.
Nan, K.
Xie, and A.
Sharf, “A Search-Classify Approach for Cluttered Indoor Scene Understanding,” ACM Transactions on Graphics, vol.
31, no.
6, pp.
1–10, 2012.
[34] T.
Shao, W.
Xu, K.
Zhou, J.
Wang, D.
Li, and B.
Guo, “An interactive approach to semantic modeling of indoor scenes with an RGBD camera,” ACM Transactions on Graphics, vol.
31, no.
6, pp.
1–11, 2012.
[35] Y.
Shi, P.
Long, K.
Xu, H.
Huang, and Y.
Xiong, “Data-driven contextual modeling for 3d scene understanding,” Computers & Graphics, vol.
55, pp.
55–67, 2016.
[36] J.
Rock, T.
Gupta, J.
Thorsen, J.
Gwak, D.
Shin, and D.
Hoiem, “Completing 3D Object Shape from One Depth Image,” CVPR, 2015.
[37] R.
Hartley and A.
Zisserman, Multiple View Geometry in Computer Vision.
Cambridge University Press, 2004.
[38] R.
A.
Newcombe, S.
J.
Lovegrove, and A.
J.
Davision, “DTAM: Dense Tracking and Mapping in Real-time,” ICCV, 2011.
[39] S.
Baker and I.
Matthews, “Lucas-Kanade 20 Years On : A Unify- ing Framework : Part 1,” International Journal of Computer Vision, vol.
56, no.
3, pp.
221–255, 2004.
[40] C.
B.
Choy, D.
Xu, J.
Gwak, K.
Chen, and S.
Savarese, “3D- R2N2: A Uniﬁed Approach for Single and Multi-view 3D Object Reconstruction,” ECCV, 2016.
[41] X.
Di, R.
Dahyot, and M.
Prasad, “Deep Shape from a Low Number of Silhouettes,” ECCV, 2016.
[42] Z.
Lun, M.
Gadelha, E.
Kalogerakis, S.
Maji, and R.
Wang, “3D Shape Reconstruction from Sketches via Multi-view Convolu- tional Networks,” 3DV, 2017.
[43] D.
J.
Rezende, S.
M.
A.
Eslami, S.
Mohamed, P.
Battaglia, M.
Jader- berg, and N.
Heess, “Unsupervised Learning of 3D Structure from Images,” NIPS, 2016.
[44] A.
Kar, C.
H¨ane, and J.
Malik, “Learning a Multi-View Stereo Machine,” NIPS, 2017.
[45] M.
Ji, J.
Gall, H.
Zheng, Y.
Liu, and L.
Fang, “SurfaceNet: An End- to-end 3D Neural Network for Multiview Stereopsis,” ICCV, 2017.
[46] T.
Whelan, J.
McDonald, M.
Kaess, M.
Fallon, H.
Johannsson, and J.
J.
Leonard, “Kintinuous: Spatially Extended Kinectfusion,” RSS Workshops, 2012.
[47] T.
Whelan, S.
Leutenegger, R.
F.
Salas-moreno, B.
Glocker, and A.
J.
Davison, “ElasticFusion : Dense SLAM Without A Pose Graph,” RSS, 2015.
[48] G.
Riegler, A.
O.
Ulusoy, H.
Bischof, and A.
Geiger, “OctNetFusion: Learning Depth Fusion from Data,” 3DV, 2017.
[49] V.
Blanz and T.Vetter, “Face Recognition based on Fitting a 3D Morphable Model,” IEEE Transactions on Pattern Analysis and Ma- chine Intelligence, vol.
25, no.
9, pp.
1063–1074, 2003.
[50] P.
Dou, S.
K.
Shah, and I.
A.
Kakadiaris, “End-to-end 3D face reconstruction with deep neural networks,” CVPR, 2017.
[51] A.
Kar, S.
Tulsiani, J.
Carreira, and J.
Malik, “Category-speciﬁc object reconstruction from a single image,” CVPR, 2015.
[52] J.
Gwak, C.
B.
Choy, M.
Chandraker, A.
Garg, and S.
Savarese, “Weakly supervised 3D Reconstruction with Adversarial Con- straint,” arXiv, 2017.
[53] S.
Tulsiani, T.
Zhou, A.
A.
Efros, and J.
Malik, “Multi-view Su- pervision for Single-view Reconstruction via Differentiable Ray Consistency,” CVPR, 2017.
[54] X.
Yan, J.
Yang, E.
Yumer, Y.
Guo, and H.
Lee, “Perspective Transformer Nets: Learning Single-View 3D Object Reconstruction without 3D Supervision,” NIPS, 2016.
[55] C.
Kong, C.-H.
Lin, and S.
Lucey, “Using Locally Corresponding CAD Models for Dense 3D Reconstructions from a Single Image,” CVPR, 2017.
[56] A.
Kurenkov, J.
Ji, A.
Garg, V.
Mehta, J.
Gwak, C.
Choy, and S.
Savarese, “DeformNet: Free-Form Deformation Network for 3D Shape Reconstruction from a Single Image,” NIPS, 2017.
[57] J.
K.
Murthy, G.
V.
S.
Krishna, F.
Chhaya, and K.
M.
Krishna, “Reconstructing Vechicles from a Single Image : Shape Priors for Road Scene Understanding,” ICRA, 2017.
[58] H.
Christian, S.
Tulsiani, and J.
Malik, “Hierarchical Surface Pre- diction for 3D Object Reconstruction,” 3DV, 2017.
13 [59] M.
Tatarchenko, A.
Dosovitskiy, and T.
Brox, “Octree Generat- ing Networks: Efﬁcient Convolutional Architectures for High- resolution 3D Outputs,” ICCV, 2017.
[60] A.
Johnston, R.
Garg, G.
Carneiro, I.
Reid, and A.
v.
d.
Hengel, “Scaling CNNs for High Resolution Volumetric Reconstruction from a Single Image,” ICCV Workshops, 2017.
[61] C.-H.
Lin, C.
Kong, and S.
Lucey, “Learning Efﬁcient Point Cloud Generation for Dense 3D Object Reconstruction,” AAAI, 2018.
[62] J.
Wu, Y.
Wang, T.
Xue, X.
Sun, W.
T.
Freeman, and J.
B.
Tenenbaum, “MarrNet: 3D Shape Reconstruction via 2.5D Sketches,” NIPS, 2017.
[63] M.
Firman, O.
M.
Aodha, S.
Julier, and G.
J.
Brostow, “Structured Prediction of Unobserved Voxels From a Single Depth Image,” CVPR, 2016.
[64] S.
Song, F.
Yu, A.
Zeng, A.
X.
Chang, M.
Savva, and T.
Funkhouser, “Semantic Scene Completion from a Single Depth Image,” CVPR, 2017.
[65] W.
Wang, Q.
Huang, S.
You, C.
Yang, and U.
Neumann, “Shape In- painting using 3D Generative Adversarial Network and Recurrent Convolutional Networks,” ICCV, 2017.
[66] C.
Zou, E.
Yumer, J.
Yang, D.
Ceylan, and D.
Hoiem, “3D-PRNN: Generating Shape Primitives with Recurrent Neural Networks,” ICCV, 2017.
[67] C.
Ledig, L.
Theis, F.
Huszar, J.
Caballero, A.
Cunningham, A.
Acosta, A.
Aitken, A.
Tejani, J.
Totz, Z.
Wang, and W.
Shi, “Photo-Realistic Single Image Super-Resolution Using a Genera- tive Adversarial Network,” CVPR, 2017.
[68] S.
Reed, Z.
Akata, X.
Yan, L.
Logeswaran, B.
Schiele, and H.
Lee, “Generative Adversarial Text to Image Synthesis,” ICML, 2016.
[69] M.
Gadelha, S.
Maji, and R.
Wang, “3D Shape Induction from 2D Views of Multiple Objects,” 3DV, 2017.
[70] E.
Smith and D.
Meger, “Improved Adversarial Systems for 3D Object Generation and Reconstruction,” CoRL, 2017.
[71] A.
A.
Soltani, H.
Huang, J.
Wu, T.
D.
Kulkarni, and J.
B.
Tenen- baum, “Synthesizing 3D Shapes via Modeling Multi-View Depth Maps and Silhouettes with Deep Generative Networks,” CVPR, 2017.
[72] A.
X.
Chang, T.
Funkhouser, L.
Guibas, P.
Hanrahan, Q.
Huang, Z.
Li, S.
Savarese, M.
Savva, S.
Song, H.
Su, J.
Xiao, L.
Yi, and F.
Yu, “ShapeNet: An Information-Rich 3D Model Repository,” arXiv, 2015.
[73] L.
v.
d.
Maaten and G.
Hinton, “Visualizing Data using t-SNE,” Journal of Machine Learning Research, vol.
9, no.
Nov, pp.
2579–2605, 2008.
[74] O.
Ronneberger, P.
Fischer, and T.
Brox, “U-Net : Convolutional Networks for Biomedical Image Segmentation,” MICCAI, 2015.
[75] M.
Arjovsky, S.
Chintala, and L.
Bottou, “Wasserstein GAN,” ICML, 2017.
tion,” ICLR, 2015.
ICCV, 2017.
[76] I.
Gulrajani, F.
Ahmed, M.
Arjovsky, V.
Dumoulin, and A.
Courville, “Improved Training of Wasserstein GANs,” NIPS, 2017.
[77] M.
Arjovsky and L.
Bottou, “Towards Principled Methods for Training Generative Adversarial Networks,” ICLR, 2017.
[78] Y.
Mroueh, T.
Sercu, and V.
Goel, “McGAN: Mean and Covariance Feature Matching GAN,” ICML, 2017.
[79] J.
Bao, D.
Chen, F.
Wen, H.
Li, and G.
Hua, “CVAE-GAN: Fine- Grained Image Generation through Asymmetric Training,” ICCV, 2017.
[80] A.
Brock, T.
Lim, J.
M.
Ritchie, and N.
Weston, “Generative and Discriminative Voxel Modeling with Convolutional Neural Networks,” NIPS Workshops, 2016.
[81] D.
P.
Kingma and J.
Ba, “Adam: A method for stochastic optimiza- [82] K.
He, G.
Gkioxari, P.
Dollar, and R.
Girshick, “Mask R-CNN,”
Deep learning is an attractive area of research in constant growth [1].
In partic- ular, in the neuro-computing ﬁeld, the study of deep neural networks composed by multiple non-linear layers has proved able to learn feature representations at progressively higher levels of abstraction, leading to eminent performance e.g. in vision tasks.
Extending the beneﬁts of depth to recurrent neural networks (RNNs) is an intriguing research direction that is recently gaining an increas- ing attention [2].
In this context, the study of deep RNNs has pointed out that hierarchically organized recurrent models have the potentiality of devel- oping multiple time-scales representations of the input history in their internal states, which can be of great help, e.g., when dealing with text processing tasks [3].
More recently, studies in the area of Reservoir Computing (RC) [4, 5] have shown that the ability of developing such a structured state space organization is indeed an intrinsic property of layered RNN architectures [6, 7].
The study of deep RC networks on the one hand allowed the development of eﬃciently trained deep models for learning in the temporal domain, and on the other hand it paved the way to further studies on the properties of deep RNNs dynamics even in the absence of (or prior to) learning of the recurrent connections.
An aspect of prominent relevance in the study of dynamical models is rep- resented by the analysis of their memory abilities.
In this paper, exploiting the ground provided by the deep RC framework, we explicitly address the problem of analyzing the short-term memory capacity of individual (progressively higher) layers in deep recurrent architectures.
Contributing to highlight the intrinsic di- versiﬁcation of transient state dynamics in hierarchically constructed recurrent networks, our investigation aims at shedding more light on the bias of layering in the RNN architectural design.
Framed in the RC area, our analysis is also intended to provide insights on the process of reservoir network construction.
2 Deep Stacked RNN We consider deep RNNs [3] whose recurrent architecture is obtained by a stacked composition of multiple non-linear recurrent hidden layers, as illustrated in Fig.
1.
The state computation proceeds by following the hierarchical network organization, from the lowest layer to the highest one.
Speciﬁcally, at each time step t the ﬁrst recurrent layer in the network is fed by the external input while each successive layer is fed by the activation of the previous one.
Fig.
1: Hierarchical organization of hidden layers in a deep RNN.
Under a dynamical system perspective, a deep RNN implements an input- driven discrete-time non-linear dynamical system, in which the state evolution in each layer i is ruled by a state transition function F (i).
Here we denote the input dimension by NU and assume, for the sake of simplicity, that each hidden In the following, we use u(t) and x(i)(t), layer contains NR recurrent units.
respectively, to indicate the external input and state of i-th hidden layer at time step t.
Based on this notation, the state in the ﬁrst layer is updated according to the following equation: x(1)(t) = F (1)(u(t), x(1)(t − 1)) = tanh(W(1)u(t) + ˆW(1)x(1)(t − 1)), (1) where W(1) ∈ RNR×NU is the input weight matrix and ˆW(1) ∈ RNR×NR is the recurrent weight matrix for the ﬁrst layer.
For each successive layer i > 1, the state is updated according to: x(i)(t) = F (i)(x(i−1)(t), x(i)(t−1)) = tanh(W(i)x(i−1)(t)+ ˆW(i)x(i)(t−1)), (2) where W(i) ∈ RNR×NR collects the weights for the inter-layer connections from layer i − 1 to layer i and ˆW(i) ∈ RNR×NR is the recurrent weight matrix for layer i.
Note that in both above eq.
1 and 2, a tanh non-linearity is used as element-wise applied activation function of recurrent units and bias terms are omitted for the ease of notation.
Here it is also worth observing that, although the deep recurrent dynamics globally evolve as a whole, locally to each layer i the state information coming from the previous level i − 1 actually acts as an independent input information that encodes the history of the external input up to the present time step.
Taking aside the aspects related to learning of the recurrent connections (and the speciﬁc aspects involved by diﬀerent training strategies), here we focus our analysis on the case of untrained deep recurrent dynamics.
To do so, we resort to the recently introduced deep RC framework [6], according to which the recurrent part of the deep RNN architecture is left untrained after initialization subject to stability constraints [7].
Speciﬁcally, the network is initialized with weights from a uniform distribution in [−1, 1] and then re-scaled to control for each layer i the values of kW(i)k2 and ρ( ˆW(i)), where ρ(·) denotes the spectral radius of its matrix argument (i.e. the maximum among the eigenvalues magnitudes).
These quantities are hyper-parameters of the model that inﬂuence its state dynamics and that are typically set to small values in order to guarantee a stable regime, a standard initialization approach also for trained networks.
Notice that this framework allows us on the one hand to investigate the ﬁxed characterization of state dynamics in successive levels of a deep RC network, and on the other hand to study of the bias due to layering in deep recurrent architectures.
Output computation is implemented by using an output layer of size NY .
Though diﬀerent choices are possible for the state-output connection settings (see e.g. [3, 8]), following from our analysis aims here we consider output modules that are individually applied to each layer of the recurrent network.
This enables us to study separately the characteristics of the state behavior emerging at the diﬀerent levels in the architecture.
We use linear output modules, such that for (i) outx(i)(t), where matrices each layer i the output is computed as y(i)(t) = W (i) out ∈ RNY ×NR are trained for each layer individually, using a direct method such as pseudo-inversion.
In the RC framework this setting also ensures the same training cost for every layer.
3 Experiments We investigate the short-term memory abilities of deep RNN architectures by resorting to the Memory Capacity (MC) task [9].
This aims at measuring the extent to which past input events can be recalled from present state activations.
Speciﬁcally, the recurrent system is tested in its ability to reconstruct delayed versions of a stationary uni-variate driving input signal (NU = NY = 1), with the MC at layer i computed as a squared correlation coeﬃcient, as follows: M C(i) = k=1 M C(i) k = k=1 Cov2(u(t − k), y(i) V ar(u(t)) V ar(y(i) k (t)) k (t)) (3) (i) where y k (t) is the activation of the output unit trained to reconstruct the u(t−k) signal from the state of layer i, while Cov and V ar respectively denote the covariance and variance operators.
In order to maximally exercise the memory capability of the systems, we used i.i.d. input signals from a uniform distribution in [−0.8, 0.8], i.e. an unstructured temporal stream in which u(t) does not carry information on previous inputs .
.
.
, u(t−2), u(t−1).
For this task, we considered a 6000 time-step long sequence, where the ﬁrst 5000 time steps were used for training1 and the remaining 1000 for MC assessment.
1The ﬁrst 1000 time steps were considered as transient to washout the initial conditions.
We considered deep RNNs with NL = 10 recurrent layers, each of which containing NR = 100 recurrent units.
Input and inter-layer weights were re- scaled such that kW(i)k2 = 1 for i = 1, .
.
.
, NL.
Weights of recurrent connections were re-scaled to the same spectral radius in all the layers, i.e. ρ = ρ( ˆW(i)) for i = 1, .
.
.
, NL, with ρ values ranging in [0.1, 1.5].
Note that, under the considered experimental settings, for higher values of ρ > 1 the network dynamics tend to exhibit a chaotic behavior, as shown in previous works in terms of local Lyapunov exponents [10, 11].
Although recurrent dynamics in chaotic regimes are generally not interesting under a practical point of view, in this paper we consider also these cases (ρ > 1) for the scope of analysis.
For each choice of ρ we independently generated 50 networks realizations (with diﬀerent seeds for random generation), and averaged the achieved results over such realizations.
For practical assessment of the MC values, it is useful to recall a basic the- oretical result provided in [9], which states that the MC of an NR-dimensional recurrent system driven by an i.i.d. uni-variate input signal is upper bounded by NR.
Accordingly, we considered a maximum value for the delay k in eq.
3 equal to twice the size of the state space, i.e. 200, which is suﬃcient to account for the correlations that are practically involved in our experimental settings.
Fig.
2 shows the MC values achieved in correspondence of progressively higher layers in the architecture, and for the diﬀerent cases of ρ considered for network initialization.
Results clearly point out that for recurrent networks in the ordered regime (ρ not exceeding 1) higher layers in the deep architecture are naturally biased toward progressively longer short-term memory abilities.
For networks in a chaotic regime (ρ above 1) higher layers tend to show a poorer MC.
The MC performance shown in Fig.
2 has a peak in correspondence of 10 9  8  7  6  5  4  3  2  1  50 45 40 35 30 25 20 15 10 0.2 0.4 0.6 0.8 1   1.2 1.4 Spectral Radius ( ) Fig.
2: MC of diﬀerent layers in deep RNNs for increasing values of ρ.
ρ = 0.9, in which case the score improves from 22.2 in the 1st layer, up to 50.1 in the 10th layer.
Interestingly, our results also highlight the eﬀectiveness of layering and its striking advantage as a convenient process for RC networks architectural design.
The memory of an NR-dimensional reservoir can be indeed easily improved by using an underlying stack of recurrent layers to ﬁlter the external input signal.
Note that such improvement comes at the only price of a modestly increased cost for the state computation (that increases linearly with the number of layers), while the cost for output training remains the same.
0.8 0.6 0.4 0.2 layer 1 layer 4 layer 7 layer 10 10 20 30 40 50 60 70 80 Delay (k) Fig.
3: k-delay memory capacity of deep RNN layers at increasing height.
We further inquire into the memory structure developed by the layers of deep RNNs by analyzing the MC values for increasing delays.
Fig.
3 shows the forgetting curves of individual (progressively higher) layers, i.e. the values of M C(i) as a function of k, obtained in the case of ρ = 0.9. The plot in Fig.
3 clearly reveals the diversiﬁcation of memory spans in the components of the deep recurrent architecture: higher layers are able to store information about past inputs for longer times.
While for the 1st layer the memory recall is almost null after delay 30, the dynamics developed in the 10th layer lead to a value that is above zero even for a delay of 60.
We can also see that input signals with smaller delays are better reconstructed in the lower layers, while higher layers are characterized by a peak that tends to shift to the right (more evident for layer 10 in Fig.
3), and by a slope of the forgetting curve that tends to be increasingly smoother.
Besides, the highlighted diversiﬁcation of short-term memory spans among the successive layers in the deep RNN architecture is also interesting as a way of characterizing (in a quantitative way) the intrinsic richness of state representations globally developed by the deep recurrent system.
4 Conclusions In this paper we have provided a computational analysis of short-term memory in deep RNNs. To do so, we have resorted to the MC task as a mean to quantify the memory of state dynamics in successive levels of a deep recurrent system.
Our results clearly showed that higher layers in a hierarchically organized RNN architecture are inherently featured, even prior to learning of recurrent connections, by an improved ability to latch input information for longer time spans.
The analysis provided in this paper also revealed interesting insights on the diversiﬁcation of the memory structure developed within deep stacked RNN dynamics, showing that higher layers tend to forget the past input history more slowly and more smoothly compared lower ones.
Furthermore, framed within the deep RC framework, our results provided evidence that support the practical beneﬁt of the layered recurrent organization as a way to improve the memory skills of reservoir networks in a cost-eﬀective fashion.
Overall, though further studies in this research direction are certainly de- manded (e.g. on the theoretical side), we believe that the outcomes provided in this paper can contribute to better understand and characterize the bias due to layering in deep recurrent neural models.
References [1] I.
Goodfellow, Y.
Bengio, and A.
Courville.
Deep learning.
MIT press, 2016.
[2] P.
Angelov and A.
Sperduti.
Challenges in deep learning.
In Proc.
of the 24th European Symposium on Artiﬁcial Neural Networks (ESANN), pages 489–495.
i6doc.com, 2016.
[3] M.
Hermans and B.
Schrauwen.
Training and analysing deep recurrent neural networks.
In NIPS, pages 190–198, 2013.
[4] D.
Verstraeten, B.
Schrauwen, M.
d’Haene, and D.
Stroobandt.
An experimental uniﬁca- tion of reservoir computing methods.
Neural networks, 20(3):391–403, 2007.
[5] M.
Lukoˇseviˇcius and H.
Jaeger.
Reservoir computing approaches to recurrent neural network training.
Computer Science Review, 3(3):127–149, 2009.
[6] C.
Gallicchio, A.
Micheli, and L.
Pedrelli.
Deep reservoir computing: A critical experi- mental analysis.
Neurocomputing, 268:87–99, 2017.
[7] C.
Gallicchio and A.
Micheli.
Echo state property of deep reservoir computing networks.
Cognitive Computation, 9(3):337–350, 2017.
[8] R.
Pascanu, C.
Gulcehre, K.
Cho, and Y.
Bengio.
How to construct deep recurrent neural networks.
arXiv preprint arXiv:1312.6026v5, 2014.
[9] H.
Jaeger.
Short term memory in echo state networks.
Technical report, German National Research Center for Information Technology, 2001.
[10] C.
Gallicchio, A.
Micheli, and L.
Silvestri.
Local lyapunov exponents of deep rnn.
In Proc.
of the 25th European Symposium on Artiﬁcial Neural Networks (ESANN), pages 559–564.
i6doc.com, 2017.
[11] C.
Gallicchio, A.
Micheli, and L.
Silvestri.
Local lyapunov exponents of deep echo state networks.
Neurocomputing, 2017.
(Accepted).

Kernel methods make use of non-linear patterns in data whilst being able to use linear solution methods, through a non-linear transformation of data examples into a fea- ture space where inner products correspond to the appli- cation of a kernel function between data examples (Hof- mann et al., 2008).
Many kernel methods have been conceived as the direct application of well-known linear methods in this feature space, occasionally reformulated to be expressed entirely in the form of inner products.
This is the case for kernel PCA, obtained through the ap- plication of linear PCA in feature space (Sch¨olkopf et al., 1998) and involving an eigendecomposition of the kernel matrix.
It has been shown to outperform linear PCA in a number of applications (Chin and Suter, 2007).
Incremental algorithms, where a solution is updated for additional data examples, are often desirable.
If data ar- Department of Statistical Science University College London London WC1E 6BT, United Kingdom p.northrop@ucl.ac.uk rives sequentially in time and a solution is required for each additional data example, more efﬁcient incremen- tal algorithms are often available than repeated applica- tion of a batch procedure.
Furthermore, incremental al- gorithms often have a lower memory footprint than their batch counterparts.
In this paper, we propose a novel algorithm for incremen- tal kernel PCA, which accounts for the change in mean in the covariance matrix from each additional data example.
It works by writing the expanded mean-adjusted kernel matrix from an additional data point in terms of a num- ber of rank one updates, to which a rank one update al- gorithm for the eigendecomposition can be applied.
We use a rank one update algorithm based on work in Golub (1973) and Bunch et al.
(1978).
A few previous exact incremental algorithms for kernel PCA have been proposed, some of which are based on the application of an incremental linear PCA method in feature space (Kim et al., 2005; Chin and Suter, 2007; Hoegaerts et al., 2007).
Rank one update algorithms for the eigendecomposition have not previously been applied to kernel PCA, to the best of our knowledge.
If the mean of the feature vectors is not adjusted, our algorithm corre- sponds to an incremental procedure for the eigendecom- position of the kernel matrix, which can be more widely applied.
Our algorithm has the same time and memory complex- ities as existing algorithms for incremental kernel PCA and it is more computationally efﬁcient than the com- parable algorithm in Chin and Suter (2007), which also allows for a change in mean.
Furthermore, it can be con- sidered more ﬂexible, since it is straightforward to apply a different rank one update algorithm to the one we have used, for potentially improved efﬁciency.
Approximate algorithms could also be applied, for example from ran- domized linear algebra (Mahoney, 2011).
The usefulness of kernel methods is limited by their large computational requirements in time and memory, which scale in the number of data points, since the dimension of the transformed variables often is very large, or they are not explicitly available, and one therefore must ex- press a solution in terms of transformed data examples.
This is particularly true for kernel PCA since it requires an eigendecomposition of the kernel matrix, an expen- sive operation.
As a remedy, various approximate meth- ods have been introduced, such as the Nystr¨om method (Williams and Seeger, 2001), which creates a low-rank approximation to the kernel matrix based on a randomly sampled subset of data examples.
We also extend our algorithm for incremental kernel PCA to incremental calculation of the Nystr¨om approxi- mation to the kernel matrix.
We incrementally add data examples to the subset used to create the Nystr¨om ap- proximation to kernel PCA.
This allows one to evalu- ate empirically the accuracy of the Nystr¨om approxima- tion for each added data example.
Rudi et al.
(2015) presented an incremental updating procedure for the Nystr¨om approximation to kernel ridge regression, based on rank one updates to the Cholesky decomposition.
Our proposed incremental procedure can be applied to any kernel method requiring the eigendecomposition or in- verse of the kernel matrix.
Combining an incremental algorithm with the Nystr¨om method also leads to further improvements in memory efﬁciency, compared with ei- ther method on its own.
2 BACKGROUND 2.1 KERNEL METHODS Kernel methods allow for the application of linear meth- ods to discover non-linear patterns between variables, through a non-linear transformation of data points φ(x) into a feature space where linear algorithms can be ap- plied (Hofmann et al., 2008).
They rely on two things.
First, the calculation of inner products between trans- formed data examples through a symmetric positive def- inite kernel k(x, y); second, the expression of a solution linearly in the space of transformed data examples, rather than in the space of transformed variables.
We have a set of n observations {xi}n i=1.
Linear meth- ods generally scale in the dimension of the observa- tions.
For example, if each xi is a real vector xi = (x(1) ), a linear method will scale as the number of variables d.
Let each xi be an element from a set X .
In general, no further restrictions need to be placed on the set X , which is a great beneﬁt of kernel methods.
For example, X can be a collection of text strings or graphs (Lodhi et al., , ..., x(d) , x(2) i.e. 2002; Vishwanathan et al., 2010).
Let H be a Hilbert space of real-valued functions on X , with inner product (cid:104)· , ·(cid:105)H.
If X is a vector space, then H is a closed sub- space of X ∗, the dual space of bounded linear functionals on X .
Consider H(cid:48), the dual space of linear functionals on H.
For each x∈X there is an element δx∈H(cid:48) such that δx(f ) = f (x), termed the evaluation functional.
If δx continuous), then by the Riesz rep- is bounded (i.e. resentation theorem there is a unique element gx∈H such that δx(f ) =(cid:104)gx, f(cid:105)H (Bollob´as, 1999).
If we con- sider gx as a function of x, say k(x,·), then k(x,·) (cid:104)k(x,·), f (·)(cid:105)H = has the reproducing property, f (x).
Furthermore, by the reproducing property, we have (cid:104)k(x,·), k(y,·)(cid:105)H = k(x, y).
Then k(x, y) is a symmetric positive deﬁnite function by the symmetric positive deﬁ- nite property of the inner product.
The function k(x,·) is also often denoted by φ(x), termed a feature map.
The space H has uncountable dimension, but since ev- ery (separable) Hilbert space is isometrically isomorphic to (cid:96)2, the space of square-summable sequences (Bol- lob´as, 1999), each element φ(xi) has a representation as a vector φ(xi) = (φ1(xi), φ2(xi), ..., φd(xi)) over R k=1 φk(xi)φk(xj).
We call these feature vectors.
However, this representation is often not known, or d is very large, so it might not be possible to apply a linear method directly on the vari- ables φ1(x), φ2(x), ..., φd(x).
Thanks to the represen- ter theorem (Sch¨olkopf et al., 2001), a solution can in- stead often be expressed in terms of elements in H, as with (cid:104)φ(xi), φ(xj)(cid:105)H =(cid:80)d f (x) =(cid:80)n i=1 αik(xi, x) with coefﬁcients αi.
We arrange the feature vectors along the rows of a data matrix Φ.
The kernel matrix is given by K := (k(xi, xj))∈Rn×n = ΦΦT .
2.2 KERNEL PCA PCA ﬁnds the set of orthogonal linear combinations of variables that maximizes the variance of each linear com- bination in turn.
PCA can be used for dimensionality re- duction, in regression and classiﬁcation problems, and to detect outliers, among other applications (Jolliffe, 2002).
The principal components are obtained by calculating the eigendecomposition of the sample covariance matrix n X T X, for a data matrix of (centred) observations C = 1 X, where each observation occupies a row.
This gives the decomposition C = V ΛV T where the columns of V are the directions of maximum variance.
The principal components can also be obtained through the related sin- gular value decomposition (SVD).
Assuming centered data, kernel PCA performs the eigen- decomposition of the covariance matrix in feature space through (Sch¨olkopf et al., 1998) ΦT Φv = λv n ΦT Φ = V ΣV T .
Hence- resulting in the decomposition 1 forth we will ignore the factor 1 n and only be con- cerned with the eigendecomposition of ΦT Φ.
Noting that span{ΦT} = span{V }, we can write v in terms of an n-dimensinal vector u as v = ΦT u.
Left-multiplying the eigenvalue equation by Φ we obtain Ku = λu and the decomposition K = U ΛU T .
If the data vectors in feature space are not assumed to be centred, we need to subtract the mean of each variable from Φ and instead calculate the eigendecomposition of K(cid:48) = (Φ− 1nΦ)(Φ− 1nΦ)T = K − 1nK − K1n + 1nK1n (1) where 1n is an n× n matrix for which (1n)i,j = 1 with every element equal to 1 n.
n, i.e. 2.3 INCREMENTAL KERNEL PCA Incremental algorithms update an existing solution for one or several additional data examples, also referred to as online learning.
The goal is that specialized algo- rithms will achieve greater time or memory performance than repeated application of batch procedures.
There are many use cases for incremental versions of batch al- gorithms, for example when memory capacity is con- strained, or when data examples arrive sequentially in time, termed streaming data, and a solution is desired for each additional data example.
A few algorithms for exact incremental kernel PCA have been proposed.
The algorithm in Chin and Suter (2007) is based on the incremental linear PCA algorithm from Lim et al.
(2004).
The time complexity is O(n3) and the memory complexity O(n2).
Hoegaerts et al.
(2007) write the kernel matrix expanded with an additional data example in terms of two rank one updates, without ad- justing for a change in mean, and hence propose an al- gorithm to update a subset of m dominant eigenvalues and corresponding eigenvectors.
If the algorithm is ap- plied to update all eigenpairs, the complexities in time and memory are O(n3) and O(n2), respectively.
Iterative algorithms produce a sequence of improving ap- proximate solutions that converges to the exact solution as the number of steps increases (Golub and Van Loan, 2013).
An iterative algorithm can often be made to op- erate efﬁciently in an incremental fashion, by expanding the data set with additional data examples and restart- ing the iterative procedure.
An example of an iterative method for kernel PCA that can be made to operate in- crementally is the kernel Hebbian algorithm (Kim et al., 2005), based on the generalized Hebbian algorithm (Oja, 1982) applied in feature space.
Various approximations to incremental kernel PCA have also been proposed.
See for example Tokumoto and Ozawa (2011) or Sheikholeslami et al.
(2015).
Since we present an exact algorithm for incremental kernel PCA, we will not describe these or similar works further.
2.4 THE NYSTR ¨OM METHOD The Nystr¨om method (Williams and Seeger, 2001) ran- domly samples m data examples from the full dataset, often uniformly, and calculates a low-rank approxima- tion ˜K to the full kernel matrix through ˜K = Kn,mK−1 m,mKm,n where Kn,m is an n× m matrix obtained by choosing m columns from the original matrix K, Km,n is its trans- pose and Km,m contains the intersection of the same m columns and rows.
3 KERNEL PCA THROUGH RANK ONE UPDATES In this section we present an algorithm for incremen- tal kernel PCA based on rank one updates to the eigen- decomposition of the kernel matrix K, or the mean- adjusted kernel matrix K(cid:48).
Any incremental algorithm for the eigendecomposition of the kernel matrix K can be applied where the explicit or implicit inverse of the same is required, such as kernel regression and kernel SVM.
Various methods other than kernel PCA are also based on the eigendecomposition of the kernel matrix, such as kernel FDA (Mika et al., 1999).
Even when more efﬁcient solution methods are available, access to the eigendecomposition can be highly useful for statisti- cal regularization or controlling numerical stability.
In contrast to the covariance matrix in linear PCA, the kernel matrix expands in size for each additional data point, which needs to be taken into account, and the ef- fect on the eigensystem determined.
We write the ker- nel matrix K(cid:48) m+1,m+1 created with m+1 data examples in terms of an expansion and a sequence of symmetric rank one updates to the kernel matrix K(cid:48) m,m, and apply a rank one update algorithm to the eigendecomposition of K(cid:48) m,m to obtain the eigendecomposition of K(cid:48) m+1,m+1.
A number of algorithms have been suggested to perform rank one modiﬁcation to the symmetric eigenproblem.
Golub (1973) presented a procedure to determine the eigenvalues of a diagonal matrix updated through a rank one perturbation.
Bunch et al.
(1978) extended the re- sults to the determination of both eigenvalues and eigen- vectors of an arbitrary perturbed matrix, including an im- proved procedure to determine the eigenvalues.
Stability issues in the calculation of the eigenvectors, including loss of numerical orthogonality, later motivated several improvements (Dongarra and Sorensen, 1987; Sorensen and Tang, 1991; Gu and Eisenstat, 1994).
Alternatively, one could potentially employ update algorithms for the singular value decomposition, such as the algorithm sug- gested in Brand (2006) for the thin singular value decom- position.
We use the rank one update algorithm for eigenvalues from Golub (1973) and the determine the eigenvectors according to Bunch et al.
(1978).
In the experiments our approach seems to be sufﬁciently stable and accurate for most use cases.
We assume throughout that the kernel matrix remains non-singular after each update.
Our algorithm has the same time and memory complex- ities as competing methods.
The algorithm most com- parable to ours is the one in Chin and Suter (2007), which also accounts for a change in mean.
If one addi- tional data example is added incrementally, and all eigen- pairs are retained, it requires the eigendecomposition of an m+2×m+2 matrix, the eigendecomposition of the m×m unadjusted kernel matrix, and a multiplication of two m×m matrices at each step.
Since a multiplication of two m×m matrices requires 2m3 ﬂops, and the state- of-the-art QR algorithm for the symmetric eigenproblem about 9m3 ﬂops (Golub and Van Loan, 2013), the algo- rithm thus requires 20m3 ﬂops to the O(m3) factor.
Our proposed algorithm requires 8m3 ﬂops to the O(m3) fac- tor if the mean is adjusted, and 4m3 ﬂops otherwise, from one multiplication of two m+1×m+1 matrices for each rank one update.
Our algorithm is thus more than twice as efﬁcient.
3.1 RANK ONE UPDATE PROCEDURE m and write K(cid:48) If we know the eigendecomposition of K(cid:48) m,m= m+1,m+1 in terms of an ex- UmΛmU T pansion and number of symmetric rank one updates to K(cid:48) m,m, we can then apply a rank one update algo- rithm to obtain the eigendecomposition of K(cid:48) m+1,m+1= Um+1Λm+1U T m+1.
3.1.1 Zero-mean data If we assume that the data examples have zero mean in feature space, then the mean does not need to be updated for previous data points and Km,m only needs to be ex- panded with an additional row and column.
In this case we can devise a rank one update procedure from Km,m to Km+1,m+1 in two steps.
We denote ki,j =k(xi,xj) and a = [k1,m+1 k2,m+1 ··· km,m+1]T , i.e. a column vector with elements k1,m+1, k2,m+1, ..., km,m+1 and let v1=[aT 1 km+1,m+1]T v2=[aT 1 σ=4/km+1,m+1 km+1,m+1]T Then we have Km+1,m+1= (cid:20)Km,m (cid:21) 4 km+1,m+1 1 −σv2vT 0m 0T m,m+σv1vT :=K 0 +σv1vT 1 −σv2vT (2) m,m and corresponding to an expansion of Km,m to K 0 two rank one updates, where 0m is a column vector of ze- m,m will ros.
Compared to the eigensystem of Km,m, K 0 have an additional eigenvalue λm+1= 1 4 km+1,m+1 and corresponding eigenvector um+1=[0 0 ··· 0 1]T .
The matrix K 0 m,m is symmetric positive deﬁnite (SPSD), since all eigenvalues are positive.
It will remain SPSD after the ﬁrst update, since it is a sum of two SPSD matrices, as v1vT 1 is a Gram matrix, if each element is instead seen as a separate vector.
The resulting ma- trix after the second update will be SPSD since this holds for Km+1,m+1.
The algorithm for one updating iteration is described in Algorithm 1, given a function rankoneupdate(σ,v,L,U ) that updates the eigenvalues L and eigenvectors U from a rank one additive perturba- tion σvvT .
(cid:20)U i=1 ; row vector of eigenvalues L and ma- Algorithm 1 Incremental eigendecomposition of kernel matrix Input: Dataset {xi}m+1 trix of eigenvectors U of Km,m; kernel function k(·,·) Output: Eigenvalues L and eigenvectors U of Km+1,m+1 1: L←[L km+1,m+1/4] 2: U← km+1,m+1/4 3: sigma ←4/km+1,m+1 4: k1←[k1,m+1 k2,m+1 ...
km+1,m+1/2] 5: k0←[k1,m+1 k2,m+1 ...
km+1,m+1/4] 6: L,U←rankoneupdate(sigma, k1, L, U ) 7: L,U←rankoneupdate(−sigma, k0, L, U ) (cid:21) If we limit ourselves to kernel functions for which k(x,x) is constant, without loss of generality we can set k(x,x)=1 and the above expression simpliﬁes.
m+1,m+1, all the elements of K(cid:48) 3.1.2 Mean-adjusted data To construct a rank one update procedure from K(cid:48) m,m to K(cid:48) m,m need to be ad- justed in addition to the expansion with another row and column.
We ﬁrst devise two rank one updates that adjust the mean of K(cid:48) m,m to account for the additonal data ex- ample.
We then expand the resulting matrix and perform symmetric updates to set the last row and column to the required values, similarly to (2).
Recall that when taking the mean into account, one per- forms an eigendecomposition of the adjusted kernel ma- trix K(cid:48)=K−1nK+K1n−1nK1n.
The elements of K(cid:48) m,m can thus be adjusted through the following for- mula m+1,m+1)1:m,1:m K(cid:48)(cid:48) m,m:=(K(cid:48) m,m+1mKm,m+Km,m1m−1mKm,m1m =K(cid:48) +(−1m+1Km+1,m+1−Km+1,m+11m+1 +1m+1Km+1,m+11m+1)1:m,1:m where (· )1:m,1:m denotes the ﬁrst m rows and columns of a matrix.
The latter six terms are all rank one matrices.
The matrices 1mKm,m and −(1m+1Km+1,m+1)1:m,1:m are constant along the columns, and hence their the rows of Km,m1m− sum, (Km+1,m+11m+1)1:m,1:m.
The matrix 1mKm,m1m has constant entries, equal to the sum of all elements of Km,m multiplied by a factor 1/m2, and similarly for (1m+1Km+1,m+11m+1)1:m,1:m.
Consequently, all terms can be written as two rank one updates.
We have and similarly for 1mKm,m−(1m+1Km+1,m+1)1:m,1:m (1mKm,m−1maT ) Km,m1m−(Km+1,m+11m+1)1:m,1:m m+1 m+1 (Km,m1m−a1T m) with a as in section 3.1.1 above and where 1m is a column vector of ones.
Since Km,m is symmetric for all m, we have 1mKm,m= (Km,m1m)T and 1:m,1:m, (1m+1Km+1,m+1)1:m,1:m= (Km+1,m+11m+1)T and can set u= m(m+1) Km,m1m− 1 m+1 a+ C1m C=− 1 m2 Σm+ (m+1)2 Σm+1 mKm,m1m, the sum of where we have denoted Σm=1T all elements of Km,m, to obtain K(cid:48)(cid:48) m,m=K(cid:48) m,m+1muT +u1T (1m+u)(1m+u)T − 1 =K(cid:48) m,m+ which is two symmetric rank one updates to K(cid:48) m,m.
Σm and Km,m1m can easily be updated between iterations like so (1m−u)(1m−u)T Σm+1=Σm+2aT 1m+km+1,m+1 Km+1,m+11m+1=[Km,m1m+a; aT 1m+km+1,m+1] where [b; c] denotes a column vector b expanded with an additional element c.
We now expand K(cid:48)(cid:48) m,m to K(cid:48) m+1,m+1, analogously to (2), but taking the adjusted mean into account.
The required last row and column is given by v := k− 1 m+1 (1m+11T m+1k+Km+1,m+11m+1 − 1 m+1 Σm+11m+1) with k=[aT k(xm+1,xm+1)]T .
If we let v1=[(v)1:m; σ=4/(v)m+1 v2=[(v)1:m; (v)m+1] (v)m+1] where (v)1:m is a vector of the ﬁrst m elements of v, and (v)m+1 is its last element, we have K(cid:48) m+1,m+1= +σv1vT 1 −σv2vT (cid:20)K(cid:48)(cid:48) :=K 0 (cid:21) 1 −σv2vT 0m m,m 0T m,m+σv1vT 4 (v)m+1 (3) We have thus devised a procedure to update K(cid:48) m,m to K(cid:48) m+1,m+1 using four symmetric rank one updates, for which a rank one eigendecomposition update algorithm can be applied.
The full procedure is described in Algo- rithm 2.
Note that the matrix K(cid:48) m,m or its expansion do not need to be kept in memory.
The procedure is linear in time and memory, since all constituent quantities are updated incrementally.
3.2 UPDATE ALGORITHM FOR THE EIGENDECOMPOSITION Here we describe an algorithm for updating the eigen- decomposition after a rank one perturbation.
Suppose Algorithm 2 Incremental eigendecomposition of adjusted kernel matrix Input: Dataset {xi}m+1 i=1 ; row vector of eigenvalues L and ma- trix of eigenvectors U of Km,m; kernel function k(·,·); sum of all elements of Km,m, denoted S; sum of rows of Km,m, i.e. Km,m1m, denoted K1 Output: Eigenvalues L and eigenvectors U of Km+1,m+1 1: a←[k1,m+1 k2,m+1 ...
km,m+1] 2: S2←S +2∗sum(a)+km+1,m+1 3: C←−S/m2 +S2/(m+1)2 4: u←K1/(m∗(m+1))2−a/(m+1)+0.5∗C∗ones(m) 5: L,U←rankoneupdate(0.5, 1+u, L, U ) 6: L,U←rankoneupdate(−0.5, 1−u, L, U ) 7: K1←[K1+a sum(a)+k] 8: S←S2 9: m←m+1 10: v←k−(ones(m)∗(sum(a)+k)+K1−S/m)/m 11: v0←v[m] 12: v←v[1:m−1] 13: L←[L v0/4] 14: U← v0/4 15: sigma←4/v0 16: v1←[v v0/2] 17: v2←[v v0/4] 18: L,U←rankoneupdate(sigma, v1, L, U ) 19: L,U←rankoneupdate(−sigma, v2, L, U ) (cid:20)U (cid:21) we know the eigendecomposition of a symmetric matrix A=U ΛU T .
Let B =U ΛU T +σvvT =U (Λ+σzzT )U T where z=U T v, and look for the eigendecomposition of ˜B =Λ+σzzT := ˜U ˜Λ ˜U T (Bunch et al., 1978).
Then the eigendecomposition of B is given by U ˜U ˜Λ ˜U T U T with unchanged eigenvalues and eigenvectors U B := U ˜U, since the product of two orthogonal matrices is orthogonal and since the eigendecomposition is unique, provided all eigenvalues are distinct.
The eigenvalues of ˜B can be calculated in O(n2) time by ﬁnding the roots of the secular equation (Golub, 1973) n(cid:88) i=1 ω(˜λ):=1+σ z2 λi−˜λ (4) The eigenvalues of the modiﬁed system are subject to the following bounds λi≤˜λi≤λi+1 λn≤˜λn≤λn +σzT z λi−1≤˜λi≤λi λ1 +σzT z≤˜λ1≤λ1 i=1,2,...,n−1, σ>0 σ>0 i=2,3,...,n, σ<0 σ<0 (5) which can be used to supply initial guesses for the root ﬁnding algorithm.
Note that after expanding the eigen- system, as described above, the eigenpairs need to be re- ordered for the bounds to be valid.
Once the updated eigenvalues have been calculated the eigenvectors of the perturbed matrix B are given by (Bunch et al., 1978) uB i = U D−1 i z i z(cid:107) (cid:107)D−1 (6) where Di :=Λ−˜λiI.
Since U and D−1 are m×m and Di is diagonal the denominator is O(m) and the numer- ator is O(m2), leading to O(m3) time complexity to up- date all eigenvectors.
The number of ﬂops for the full procedure is 2n3 +O(n2).
Equation (6) requires the cre- ation of an additional n×n matrix, hence the full proce- dure is quadratic in memory.
INCREMENTAL NYSTR ¨OM In this section we extend our proposed algorithm to in- cremental calculation of the Nystr¨om approximation to the kernel matrix.
Having access to an incremental pro- cedure for the Nystr¨om method can be highly useful.
Different sizes of subsets used in the approximation can efﬁciently be evaluated, to determine a suitable size for the problem at hand or for empirical investigation of the characteristics of the Nystr¨om method for subsets of dif- ferent sizes.
For very large datasets, the combination of the Nystr¨om method with incremental calculation results in further gains in memory efﬁciency.
Rudi et al.
(2015) previously proposed an incremental algorithm for the Nystr¨om approximation applied to ker- nel ridge regression, based on rank one updates to the Cholesky decomposition.
Our proposed procedure can be seen as a generalization of their work.
To the best of our knowledge, it is the ﬁrst incremental algorithm for calculation of the full Nystr¨om approximation to the ker- nel matrix.
Given the eigenvalues Λ and eigenvectors U of the matrix Km,m, the corresponding approximate eigenval- ues and eigenvectors of K are given by (Williams and Seeger, 2001) Λnys := U nys := (cid:114) m Kn,mU Λ−1 (7) an obtain procedure incremental ˜K = To U nysΛnysU nysT , calculate U and Λ incrementally using Algorithm (2), then at each iteration add an extra column to Kn,m corresponding to the additional data example, and calculate the rescaling (7).
The rescaling for Figure 1: Difference between batch and incremental calculation of K(cid:48) of size 20+m for the two datasets.
has O(m2n) time complexity from the matrix product in (7).
Note that the proposed incremental calculation of the Nystr¨om approximation exactly reproduces batch com- putation at each m, save for numerical differences.
The accuracy of the Nystr¨om approximation has been exten- sively studied, including comparisons with other meth- ods (Gittens and Mahoney, 2016; Yang et al., 2012).
5 EXPERIMENTAL ANALYSIS In this section we present the results of a number of experiments1.
We run the experiments on two differ- ent datasets from the UCI Machine Learning Reposi- tory (Lichman, 2013), the simulated Magic gamma tele- scope dataset and the Yeast dataset, containing cellular protein location sites.
Where applicable, we remove the target variable when this is categorical and not continu- ous.
Throughout the experiments we use the radial basis functions kernel (cid:18) −(cid:107)x−y(cid:107)2 (cid:19) k(x,y)=exp where σ is a parameter.
For each dataset, we set σ to be the median of the distances between all pairs of data ex- amples (in a subset of the full dataset), a common heuris- tic.
1Source code in Python is available at https://github.com/cfjhallgren/inkpca 5.1 INCREMENTAL KERNEL PCA We implement and evaluate our algorithm for incremen- tal kernel PCA both with and without adjustment of the mean of the feature vectors.
Numerical accuracy is generally good, whether adjusting the mean or not.
A slight loss of orthogonality is discov- ered in the eigenvectors, as measured by how close U U T is to the identity, particularly for mean-adjusted data that requires four updates at each step and involves more nu- merical operations.
We have previously assumed that the kernel matrix re- mains of full rank after each added data example.
This will always be the case in theory if data contains noise, however near numerical rank deﬁciency can cause issues in practice.
Equation (4) may then lack the required num- ber of roots.
In this instance one can deﬂate the ma- trix (see e.g. Bunch et al.
(1978) for details), but for the purposes of our experiments we have contended with excluding the speciﬁc data example from the algorithm.
An excluded data point does not add any time overhead to the O(n3) factor.
Every numerical operation leads to a small loss in ac- curacy, due to the ﬁnite representation of ﬂoating-point numbers, which is propagated, with varying severity, over subsequent operations.
An incremental procedure involves substantionally more operations than a batch procedure, which leads to worse accuracy in compari- son, often termed drift.
We illustrate this by plotting the Frobenius, spectral and trace norms of the difference be- tween the m×m adjusted kernel matrix K(cid:48) m,m and the reconstruction using the incrementally calculated eigen- decomposition, for different numbers of data points m, i.e. (cid:107)K(cid:48) m (cid:107).
We plot the difference for (cid:48)T m,m−U(cid:48) mΛ(cid:48) mU 01020304050607080m0.00000.00050.00100.00150.0020NormmagicFrobeniustracespectralFrobenius meantrace meanspectral mean01020304050607080m0.00000.00050.00100.00150.0020NormyeastFrobeniustracespectralFrobenius meantrace meanspectral meanFigure 2: Difference between K and ˜K of size 20+m for the two datasets.
one run of the algorithm as well as the mean difference for each value of m over 50 runs.
Please see Figure 1.
The drift for reconstruction of the unadjusted matrix is smaller and is not plotted.
Our results show that the drift is small.
5.2 INCREMENTAL NYSTR ¨OM We implement the proposed incremental calculation of the Nystr¨om approximation, using the ﬁrst 1000 observa- tions from each dataset.
Having access to an incremen- tal algorithm for calculating the Nystr¨om approximation lets us investigate explicitly how the approximation im- proves with each additional data point for a speciﬁc data set.
We calculate the Frobenius norm, spectral norm and trace norm of the difference between the the Nystr¨om ap- proximation and the full kernel matrix at each step of the algorithm.
All these three norm can be of interest to a downstream machine learning practitioner (Gittens and Mahoney, 2016).
Again, we plot the results for one run of the algorithm and for an average of 50 runs.
Please see Figure 2.
As seen in the plots, the Nystr¨om approximation seems to provide a high degree of accuracy in approximating the matrix K, even for a fairly small number of basis points.
6 CONCLUSION We have in this paper presented an algorithm for incre- mental kernel PCA based on rank one updates to the eigendecomposition of the kernel matrix K or the mean- adjusted kernel matrix K(cid:48), which we extended to incre- mental calculation of the Nystr¨om approximation to the kernel matrix.
Rank one update algorithms for the eigen- decomposition other than the one chosen in this paper could also be applied to the kernel PCA problem, for potentially improved accuracy and efﬁciency, including algorithms potentially not yet conceived.
Furthermore, it could be straightforward to adapt the proposed algorithm for incremental kernel PCA to only maintain a subset of the eigenvectors and eigenvalues.
An incremental procedure for the Nystr¨om method can aid in determining a suitable size of the subset used for the approximation through empirical evaluation.
A fairly limited amount of work has been dedicated to the de- termination of this hyperparameter or equivalent hyper- parameters for other approximate kernel methods.
Var- ious bounds on the statistical accuracy of the Nystr¨om method and related approximations have been derived, which could guide the choice of this hyperparameter, but this might not be the most suitable strategy.
Acknowledgements We would like to thank Ricardo Silva at the Department of Statistical Science at UCL for helpful comments and guidance.
References Bollob´as, B.
(1999).
Linear analysis.
Cambridge Uni- versity Press, Cambridge, UK, 2nd edition.
Brand, M.
(2006).
Fast low-rank modiﬁcations of the thin singular value decomposition.
Linear Algebra and its Applications, 415(1):20–30.
Bunch, J.
R., Nielsen, C.
P., and Sorensen, D.
C.
(1978).
01020304050607080m020406080100120140160NormmagicFrobeniustracespectralFrobenius meantrace meanspectral mean01020304050607080m050100150200250NormyeastFrobeniustracespectralFrobenius meantrace meanspectral meanMika, S., R¨atsch, G., Weston, J., Sch¨olkopf, B., and M¨uller, K.-R.
(1999).
Fisher discriminant analysis with kernels.
In Neural Networks for Signal Process- ing IX: Proceedings of the 1999 IEEE Signal Process- ing Society Workshop, pages 41–48.
IEEE.
Oja, E.
(1982).
Simpliﬁed neuron model as a principal component analyzer.
Journal of Mathematical Biol- ogy, 15(3):267–273.
Rudi, A., Camoriano, R., and Rosasco, L.
(2015).
Less is more: Nystr¨om computational regularization.
In Advances in Neural Information Processing Systems, pages 1657–1665.
Sch¨olkopf, B., Herbrich, R., and Smola, A.
(2001).
A In Computational generalized representer theorem.
Learning Theory (COLT), pages 416–426.
Springer.
Sch¨olkopf, B., Smola, A., and M¨uller, K.-R.
(1998).
Nonlinear component analysis as a kernel eigenvalue problem.
Neural computation, 10(5):1299–1319.
Sheikholeslami, F., Berberidis, D., and Giannakis, G.
B.
(2015).
Kernel-based low-rank feature extraction on a budget for big data streams.
In IEEE Global Confer- ence on Signal and Information Processing (Global- SIP), pages 928–932.
IEEE.
Sorensen, D.
C.
and Tang, P.
T.
P.
(1991).
On the or- thogonality of eigenvectors computed by divide-and- SIAM Journal on Numerical conquer techniques.
Analysis, 28(6):1752–1775.
Tokumoto, T.
and Ozawa, S.
(2011).
A fast incremen- tal kernel principal component analysis for learning stream of data chunks.
In International Joint Confer- ence on Neural Networks (IJCNN), pages 2881–2888.
IEEE.
Vishwanathan, S.
V.
N., Schraudolph, N.
N., Kondor, R., and Borgwardt, K.
M.
(2010).
Graph kernels.
Journal of Machine Learning Research, 11(Apr):1201–1242.
Williams, C.
and Seeger, M.
(2001).
Using the Nystr¨om method to speed up kernel machines.
In Advances in Neural Information Processing Systems, pages 682– 688.
Yang, T., Li, Y.-F., Mahdavi, M., Jin, R., and Zhou, Z.-H.
(2012).
Nystr¨om method vs random Fourier features: A theoretical and empirical comparison.
In Advances in Neural Information Processing Systems, pages 476– 484.
Rank-one modiﬁcation of the symmetric eigenprob- lem.
Numerische Mathematik, 31(1):31–48.
Chin, T.-J.
and Suter, D.
(2007).
Incremental kernel prin- cipal component analysis.
IEEE Transactions on Im- age Processing, 16(6):1662–1674.
Dongarra, J.
J.
and Sorensen, D.
C.
(1987).
A fully par- allel algorithm for the symmetric eigenvalue problem.
SIAM Journal on Scientiﬁc and Statistical Computing, 8(2):139–154.
Gittens, A.
and Mahoney, M.
W.
(2016).
Revisiting the Nystr¨om method for improved large-scale machine Journal of Machine Learning Research, learning.
17(Dec):1–65.
Golub, G.
H.
(1973).
Some modiﬁed matrix eigenvalue problems.
Siam Review, 15(2):318–334.
Golub, G.
H.
and Van Loan, C.
F.
(2013).
Matrix com- putations.
John Hopkins University Press, Baltimore, MD, 4th edition.
Gu, M.
and Eisenstat, S.
C.
(1994).
A stable and efﬁcient algorithm for the rank-one modiﬁcation of the sym- metric eigenproblem.
SIAM Journal on Matrix Analy- sis and Applications, 15(4):1266–1276.
Hoegaerts, L., De Lathauwer, L., Goethals, I., Suykens, J.
A., Vandewalle, J., and De Moor, B.
(2007).
Ef- ﬁciently updating and tracking the dominant kernel principal components.
Neural Networks, 20(2):220– 229.
Hofmann, T., Sch¨olkopf, B., and Smola, A.
J.
(2008).
Kernel methods in machine learning.
The Annals of Statistics, 36(3):1171–1220.
Jolliffe, I.
(2002).
Springer, New York, NY, 2nd edition.
Principal component analysis.
Kim, K.
I., Franz, M.
O., and Sch¨okopf, B.
(2005).
It- erative kernel principal component analysis for image modeling.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(9):1351–1366.
Lichman, M.
(2013).
UCI machine learning repository.
Lim, J., Ross, D.
A., Lin, R.-S., and Yang, M.-H.
(2004).
Incremental learning for visual tracking.
In Advances in Neural Information Processing Systems, pages 793– 800.
Lodhi, H., Saunders, C., Shawe-Taylor, J., Cristianini, N., and Watkins, C.
(2002).
Text classiﬁcation using string kernels.
Journal of Machine Learning Research, 2(Feb):419–444.
Mahoney, M.
W.
(2011).
Randomized algorithms for ma- trices and data.
Foundations and Trends R(cid:13) in Machine Learning, 3(2):123–224.

Sigma-Pi-Sigma neural networks (SPSNNs) [1,4,7,8] as a kind of high-order neural networks can provide more powerful mapping capability [2-3,5,6] than the traditional feedforward neural networks (Sigma-Sigma neural networks).
In an SPSNN, a Pi layer (denoted by Π layer hereafter) is inserted in between the two Sigma layers.
Each Pi node (Π node) in the Π layer corresponds to a monomial, of which the variables are the outputs of the Sigma nodes (Σ nodes) of the ﬁrst Sigma layer (Σ1 layer).
Each node in the second Sigma layer (Σ2 layer) implements a linear combination of the outputs of the Π layer, and therefore represents a multinomial expansion of the output σ = (σ1,··· , σN ) of the Σ1 layer.
Then, the multinomial expansion is processed by an activation function in the Σ2 layer to give the ﬁnal output of the network.
At the beginning of the development of SPSNN, researchers have realized that it is not a good idea to include all the possible monomials in the Π layer, i.e., to get a complete multinomial expansion of the Σ1 layer, since it results in too many Π nodes in the Π layer.
In the existing literature, in order to reduce the number of Π nodes, a special multinomial Ps (called multi-linear multinomial) is used in SPSNNs. The monomials in Ps are linear with respect to each particular variable σi when taking the other variables as constants.
Therefore, the monomials such as σn i σj with n > 1 are not included in Ps. An intuitive idea behind this strategy may be the following: A Π node should receive at most one signal, rather than two or more signals, from each Σ1 node.
i or σn But from general numerical approximation point of view, each monomial plays equally important role for approximating nonlinear mappings by using multinomial.
Thus, the special multi-linear multinomial Ps may not be the best choice for the SPSNN to approximate a particular nonlinear mapping.
To this end, we propose an adaptive approach to ﬁnd a better multinomial for a given problem.
To elaborate, we start from a complete multinomial with a given order.
Then we employ a regularization technique in the learning process for the given problem to reduce the number of monomials used in the multinomial, and end up with a modiﬁed SPSNN (MSPSNN) involving the ∗W.
Wu is the corresponding author (e-mail: wuweiw@dlut.edu.cn).
F.
Li, K.S. Mohamed and W.
Wu are with the School of Mathematical Sciences, Dalian University of Technology, Dalian 116024, China.
Y.
Liu is with School of Information Science and Engineering, Dalian Polytechnic University, Dalian, China.
same number of monomials (= the number of nodes in the Π layer) as in Ps. In particular, a smoothing L1/2 regularization term [10,15] is used as an example in our method, which has been successfully applied for various kinds of neural network regularization.
We divide the learning process of MSPSNN into two phases.
The ﬁrst phase is a structural optimization phase.
Here, we insert a regularization term into the usual error function for SPSNN involving a complete set of multinomials, and perform a usual gradient learning process.
In the end, we delete the Π nodes with smaller Π-Σ2 weights, and obtain a network with the same number of Π nodes as in Ps. The second learning phase is a reﬁnement phase.
We re-start a gradient learning process for the network obtained from the ﬁrst learning phase, and use the weights that survived the ﬁrst phase as the initial weights.
The aim of the reﬁnement phase is to make up for the loss caused by the deleted nodes in the ﬁrst learning phase.
Numerical experiments are performed on some benchmark problems including two approximation problems and two classiﬁcation problems.
It is shown that our new MSPSNN behaves better than the traditional SPSNN with Ps. The rest of the paper is arranged as follows.
The proposed MSPSNN with smoothing L1/2 In Section 3, Supporting numerical simulations are regularization term is described in Section 2.
presented.
Some conclusions are given in Section 4.
2 MSPSNN method with smoothing L1/2 regularization 2.1 Sigma-Pi-Sigma neural network An SPSNN is composed of an input layer, two hidden layers of summation node layer (Σ1 layer) and product node layer (Π layer), and an output layer (Σ2 layer).
The numbers of nodes of these layers are M + 1, N, Q and 1, respectively.
Denote by x = (x0,· · ·, xM )T ∈ RM +1 the input vector, where the M components x0,··· , xM−1 are the “real” input, while xM is an extra artiﬁcial input, ﬁxed to -1.
The output vector σ ∈ RN of Σ1 layer with respect to x can be written as σ = (σ1,··· , σN ) = (g(w1 · x), g(w2 · x),· · ·, g(wN · x))T , (1) where g(·) is a given nonlinear activation function, wn = (wn0,· · ·, wnM )T ∈ RM +1 (1 ≤ n ≤ N ) is the weight vector connecting the n-th summation node of Σ1 layer and the input layer, and wn · x denotes the inner product of wn and x.
Here we remark that the component wnM usually represents the bias of the n-th summation node of Σ1 layer.
In Π layer, Each Π node connects with certain nodes of Σ1 layer, receives signals from these nodes, and outputs a particular monomial such as (2) Denote by ∧q (1 ≤ q ≤ Q) the index set of all the nodes in Σ1 layer that are connected to the q-th Π node.
For instance, let us assume that the above three examples in (2) correspond to the ﬁrst, third and ﬁfth nodes of Π layer, respectively.
Then, we have σ1, σ1σ2, σ2 1.
∧1 = {1},∧3 = {1, 2},∧5 = {1, 1}.
(3) The output vector τ = (τ1,··· , τQ)T ∈ RQ of Π layer is computed by τq = σi, 1 ≤ q ≤ Q.
(cid:89) i∈∧q Here we make a convention that τq =(cid:81) i∈∧q σi ≡ 1, when ∧q = φ, i.e., when the q-th Π node is not Before we concentrate our attention on the choice of ∧q’s, let us describe the output of Σ2 layer.
connected to any node of Σ1 layer.
The choice of ∧q’s is our main concern in this paper.
(4) The output of the single node of Σ2 layer, i.e., the ﬁnal output of the network, is (5) where f (·) is another given activation function, and w0 = (w0,1, w0,2,··· , w0,Q)T ∈ RQ is the weight vector connecting Π layer and Σ2 layer.
When the network is used for approximation problems, we y = f (w0 · τ ), usually set f (t) = t.
On the other hand, when the network is used for classiﬁcation problems, f (t) is usually chosen to be a Sigmoid function.
In both the cases, we can see from (1), (4) and (5) that the input w0 · τ to Σ2 layer is actually a multinomial expansion of the output values of Σ1 layer, where the components of τ correspond to the monomials, and the components of w0 are the coeﬃcients, involved in the multinomial expansion.
As comparison, we recall that for the usual feedforward neural networks, the input to the Σ2 layer is a linear combination of the output values of Σ1 layer.
Now we discuss the choice of ∧q’s in detail and explain the main idea of the paper.
For convenience and clarity, we take the third order multinomial of three variables as an example in this introduction section.
Therefore, we have N = 3, i.e., Σ1 layer has three nodes.
We consider three choices of ∧q’s, resulting in three diﬀerent multinomial expansions: the complete mutinomial, the partially linear multinomial (the traditional approach), and the adaptive multinomial (our proposed approach).
The choice of the complete mutinomial means that the input to Σ2 layer is a complete multinomial as follows: w0,1 + w0,2σ1 + w0,3σ2 + w0,4σ3 + w0,5σ1σ2 + w0,6σ1σ3 + w0,7σ2σ3 + w0,8σ2 1 + w0,13σ1σ2 + w0,14σ3σ2 3 + w0,11σ2σ2 1 + w0,12σ3σ2 2 + w0,10σ2 1 + w0,9σ2 2 + w0,15σ1σ3 3 + w0,16σ3σ2 2 + w0,17σ3 1 + w0,18σ3 2 + w0,19σ3 3 + w0,20σ1σ2σ3.
(6) We see that there are twenty monomials in the multinomial expansion, corresponding to twenty Π nodes in Π layer.
More generally, when Σ1 layer has N nodes, the number of the monomials is CN N +3, which grows very rapidly when N increases.
Therefore, the complete multinomial approach is not a good choice in practice.
complete = C3 The traditional choice in the existing literature is the partially linear multinomial approach: A partially linear multinomial is linear with respect to each particular variable σi, with the other variables taken as constants.
For instance, the partially linear multinomial corresponds to (6) is w0,1 + w0,2σ1 + w0,3σ2 + w0,4σ3 + w0,5σ1σ2 + w0,6σ1σ3 + w0,7σ2σ3 + w0,8σ1σ2σ3.
(7) We see that there are only eight monomials in (7), i.e., only eight nodes left in Π layer.
Generally, when Σ1 layer has N nodes, the number of the monomials is CN N .
Table 1 shows the comparison of CN linear with diﬀerent N .
It can be seen that the diﬀerence becomes bigger when N increases.
complete and CN linear = C0 N + C1 N + C2 N + C3 Table 1: Comparison of CN complete and CN linear with diﬀerent N .
CN complete CN linear Diﬀerence 20 12 35 15 20 56 26 30 84 42 42 120 165 64 56 93 72 220 130 90 10 286 176 110 The network structure corresponding to (7) is shown in Fig.
3(a).
The corresponding ∧q’s are as follows: ∧1 = {φ},∧2 = {1},∧3 = {2},∧4 = {3},∧5 = {1, 2},∧6 = {1, 3},∧7 = {2, 3},∧8 = {1, 2, 3}.
(8) We observe that in (6) and (7), the ﬁrst product node, corresponding to the bias w0,1, does not connect with any node in the Σ1 layer, so ∧1 = {φ}.
We also notice that there are no repeated indexes in each ∧q in (8).
Our proposed choice is as follows: We start from a complete multinomial with a given order.
Then we employ a regularization technique in the learning process to reduce the number of monomials used in the multinomial, and end up with a new SPSNN involving the same number of monomials as in the traditional choice.
For instance, in the Example 1 given in Section 4.1 below, a new SPSNN is obtained with the following multimonial: w0,1 + w0,2σ1 + w0,3σ2σ3 + w0,4σ1σ2 2 + w0,5σ2σ2 3 + w0,6σ3 1 + w0,7σ3 2 + w0,8σ3 3.
And correspondingly, ∧1 = (∅),∧2 = {1},∧2 = {2, 3},∧3 = {1, 2, 2},∧4 = {2, 2, 3}, ∧5 = {2, 3, 3},∧6 = {1, 1, 1},∧7 = {2, 2, 2},∧8 = {3, 3, 3}.
(9) (10) We notice that now there are some repeated indexes in six ∧q’s.
2.2 Error function with L1/2 regularization Let the training samples be {xj, Oj}J M )T is the j-th input sample and Oj is its corresponding ideal output.
Let yj ∈ R (1 ≤ j ≤ J) be the network output for the input xj.
The aim of the training process is to build up a network such that the errors |yj − Oj| (1 ≤ j ≤ J) are as small as possible.
A conventional square error function with no regularization term is as follows: j=1 ⊂ RM +1 × R, where xj = (xj 0,··· , xj J(cid:88) J(cid:88) ˜E(W) = where W = (wT 0 , wT 1 ,··· , wT N ), gj(t) = (yj − Oj)2 = gj(w0 · τ j), j=1 j=1 (g(t) − Oj)2, t ∈ R, 1 ≤ j ≤ J.
Let us derive the gradient of the error function ˜E(W).
Notice i ,· · ·, σj 2 ,··· , τ j τ j = (τ j Q)T = ( 1 , τ j σj i , (cid:89) i∈∧1 (cid:89) i∈∧2 (cid:89) i∈∧Q σj i )T (11) (12) (13) (14) (15) (16) (17) (18) and σj = (σj 1, σj 2,··· , σj N )T = (g(w1 · xj), g(w2 · xj),··· , g(wN · xj))T .
Then, the partial derivative of ˜E(W) with respect to w0,q (1 ≤ q ≤ Q) is ˜Ew0,q (W) = j(w0 · τ j)τ j g(cid:48) q .
J(cid:88) j=1 Moreover, for 1 ≤ n ≤ N , 0 ≤ m ≤ M and 1 ≤ q ≤ Q, we have ∂τq ∂wnm i∈∧q\n σi)g(cid:48)(wn · x)xm, if q (cid:54)= 1, and n ∈ ∧q, if q = 1, or n /∈ ∧q.
According to (4) and (16), for any 1 ≤ n ≤ N, 0 ≤ m ≤ M , we have ˜Ewnm(W) = j(w0 · τ j) g(cid:48) j(w0 · τ j) g(cid:48) Q(cid:88) (cid:88) q∈(cid:87) q=1 w0,q ∂τ j ∂wnm (cid:89) w0,q( i∈∧Q\n i )g(cid:48)(wn · xj)xj σj m, (cid:40) ((cid:81) 0, J(cid:88) J(cid:88) j=1 j=1 where ∂τ j ∂wnm denotes the value of ∂τq ∂wnm with σi = σj i and x = xj in (16).
The error function with the L1/2 regularization term is E(W) = ˜E(W) + λ[ |w0,q|1/2 + |wnm|)1/2].
Q(cid:88) q=1 N(cid:88) M(cid:88) m=0 n=1 The gradient method with L1/2 regularization for training the network is: Starting with an arbitrary initial value W0, the weights {Wk} are updated iteratively by: Wk+1 = Wk − (cid:52)Wk. Here, (cid:52)Wk = ((cid:52)wk 0,1,··· ,(cid:52)wk 0,Q,··· ,(cid:52)wk 10,··· ,(cid:52)wk J(cid:88) 0,q = −ηEw0,q (Wk) = −η[ (cid:52)wk N M )T with g(cid:48) j(wk 0 · τ j)τ j q + j=1 (19) (20) λsgn(wk 0,q) 2|wk 0,q|1/2 and (cid:52)wk nm = −ηEwnm(Wk) = −η[ g(cid:48) j(wk 0 · τ k,j) J(cid:88) j=1 (cid:88) q∈∨n (cid:89) i∈∧Q\n σk,j wk 0,q( )g(cid:48)(wk n · xj)xj m + 2(|wk λsgn(wk nm) n0| + ··· + |wk nm|)1/2 ].
(21) Here, 1 ≤ j ≤ J; 1 ≤ n ≤ N ; 0 ≤ m ≤ M ; 1 ≤ q ≤ Q; k = 0, 1,··· ; η > 0 is the learning rate; and λ > 0 is the regularization parameter.
2.3 Error function with smoothing L1/2 regularization We note that the usual L1/2 regularization term in (18) is a non-diﬀerentiable function at the origin.
In previous studies [][], it has been replaced by a smoothing function as follows E(W) = ˜E(W) + λ[ |f (w0,q)|1/2 + |f (wnm)|)1/2], N(cid:88) n=1 M(cid:88) m=0 Q(cid:88) (cid:40)|x|, q=1 (22) (23) where f (x) is the following piecewise multinomial function: f (x) = It is easy to obtain that − x4 8a3 + 3x2 4a + 3a 8 , if |x| ≥ a, if |x| < 0.
f (x) ∈ [ 3a , +∞), f(cid:48)(x) ∈ [−1, 1], and f(cid:48)(cid:48)(z) ∈ [0, 2a ].
The gradient of the error function can be written as EW(W) = (Ew0,1(W), Ew0,2(W),··· , Ew0,Q(W), Ew10(W), Ew11(W),··· , EwNM (W))T , (24) where Ew0,q (W) = Ewnm(W) = J(cid:88) J(cid:88) j=1 j=1 q + j(w0 · τ j)τ j g(cid:48) (cid:88) j(w0 · τ j) g(cid:48) q∈∨n λf(cid:48)(w0,q) 2(f (w0,q))1/2 (cid:89) w0,q( i∈∧Q\n i )g(cid:48)(wn · xj)xj σj m + λf(cid:48)(w0,q) 2(f (wn0) + ··· + f (wnm))1/2 ).
Starting from an arbitrary initial value W0, the gradient method with the smoothing L1/2 regular- ization updates the weights {Wk} iteratively by Wk+1 = Wk − (cid:52)Wk (25) with and (cid:52)wk 0,q = −ηEw0,q (Wk) = −η[ J(cid:88) j=1 g(cid:48) j(wk 0 · τ k,j)τ j q + λf(cid:48)(wk 2(f (wk 0,q) 0,q))1/2 (26) (cid:52)wk = −η[ nm = −ηEnm(Wk) J(cid:88) 0 · τ k,j) g(cid:48) j(wk j=1 (cid:88) q∈∨n (cid:89) i∈∧Q\n σk,j wk 0,q( )g(cid:48)(wk n · xj)xj m + λf(cid:48)(wk nm) 2(f (wk n0) + ··· + f (wk nM ))1/2 ], (27) where 1 ≤ j ≤ J; 1 ≤ n ≤ N ; 0 ≤ m ≤ M ; 1 ≤ q ≤ Q; k = 0, 1,··· ; η > 0 is the learning rate; and λ > 0 the regularization parameter.
3 Algorithm As mentioned in the Introduction, We divide the learning process into two phases: a structural optimization phase for choosing the structure of the network, followed by a reﬁnement phase for ﬁnally choosing the weights.
Detailed descriptions of these two training phases are given in the following Algorithms 1 and 2, respectively.
Algorithm 1 Structural optimization Input.
Input the dimension M , the number N of the Σ1 nodes, the number Q of the Π nodes, the maximum iteration number I, the learning rate η, the regularization parameter λ, and the training samples {xj, Oj}J 0,Q)T ∈ RQ and Initialization.
Initialize randomly the initial weight vectors w0 w0 n = (w0 Training.
For k = 1, 2,· · ·, I do nM )T ∈ RM +1 (1 ≤ n ≤ N ).
j=1 ⊂ RM +1 × R.
0,1,··· , w0 n1,··· , w0 0 = (w0 n0, w0 Compute the error function (22).
Compute the gradients (26) and (27).
Update the weights wk 0 and wk n (1 ≤ n ≤ N ) by using (25).
end Structural optimization.
ˆQ = CN Output.
Output the ﬁnal weight vectors ˆw0 and ˆwn = wI linear largest weights in absolute value to form a vector ˆw0 = { ˆw1, ˆw2,··· , ˆw ˆQ}.
0 = (wI n (1 ≤ n ≤ N ).
In the obtained weight vector wI 0,1,··· , w0,Q)T , select the Algorithm 2 Reﬁnement training Input.
Input the dimension M , the number N of the Σ1 nodes, the number ˆQ of the Π nodes, the maximum iteration number K, the learning rate η, and the training samples {xj, Oj}J j=1 ⊂ RM +1 × R.
Initialization.
Set w0 Reﬁnement Training.
for k = 1, 2,··· , K do n = ˆwn (1 ≤ n ≤ N ), and λ = 0.
0 = ˆw0 and w0 Compute the error function by (22).
Compute the gradient of the weights (cid:52)wk Update the weights wk 0 and (cid:52)wk n (1 ≤ n ≤ N ) by (25).
0 and wk n (1 ≤ n ≤ N ) by (26) and (27), respectively.
end Output.
Output the ﬁnal weight vectors wK 0 and wK n (1 ≤ n ≤ N ).
4 Numerical experiments In this section, the proposed method is performed on four numerical benchmark problems: Mayas’ function problem, Gabor function problem, Sonar problem and the Pima Indians diabetes data clas- siﬁcation with diﬀerent learning rates.
4.1 Example 1: Mayas’ function approximate In this example, a network is considered to approximate the Mayas’ function as below: HM (x, y) = 0.26(x2 + y2) − 0.48xy.
(28) The training samples of the network are 36 input points selected from an even 6 × 6 grid on −0.5 ≤ x ≤ 0.5 and −0.5 ≤ y ≤ 0.5. Similarly, the test samples are 400 input points selected from 20 × 20 grid on −0.5 ≤ y ≤ 0.5 and −0.5 ≤ y ≤ 0.5. After performing Algorithms 1 with η = 0.005, λ = 0.0001 and iterationmax = 5000, we select 3, to approximate the complete multinomial.
The eight monomials, 1, σ2σ3, σ1σ2 new structure corresponds to Fig.
3(b).
The new weighted linear combination is 2, σ3σ2 2, σ3 2, σ3 1, σ3 w0,1 + w0,2σ1 + w0,3σ2σ3 + w0,4σ1σ2 2 + w0,5σ2σ2 3 + w0,6σ3 1 + w0,7σ3 2 + w0,8σ3 (29) From Fig.
3(b), the ﬁrst product node, corresponding to the bias w0,1, does not connect with any node in the Σ1 layer, so ∧1 = φ.
And we have ∧1 = ∅,∧2 = {1},∧2 = {2, 3},∧3 = {1, 2, 2},∧4 = {2, 2, 3}...
∧5 = {2, 3, 3},∧6 = {1, 1, 1},∧7 = {2, 2, 2},∧8 = {3, 3, 3} (30) Then, we perform Algorithms 2 and use the test samples to evaluate our method.
The average error with diﬀerent parameter η over the 20 tests and the improvement of the performance have been shown in Table 2.
The persuasive comparison shows that the new structure attains the best eﬀectiveness, i.e., the smallest error.
From Fig.
1, we see that the surface of Mayas’ error function in new structures is monotonically decreasing and converge to 0.
Table 2: Comparison of average error for Mayas’ approximate error function.
Learning Rate Average Old Average New Improvement % 0.001 0.005 0.01 0.05 0.1 0.0042 0.0043 0.0040 0.0039 0.0040 0.0041 0.0040 0.0039 0.0033 0.0035 2.38 6.98 2.5 15.38 12.5 Figure 1: Comparison of error for Mayas approximation problem.
4.2 Example 2: Gabor function approximate In this example, a MPSPNN is used to approximate the Gabor function as: (cid:18) x2 + y2 (cid:19) HG = 2π(0.5)2 exp 2(0.5)2 cos(2π(x + y)) (31) The training samples of the neural network are 36 input points selected from an evenly 6 × 6 grid on −0.5 ≤ x ≤ 0.5 and −0.5 ≤ y ≤ 0.5. Similarly, the test samples are 400 input points selected from 20 × 20 grid on −0.5 ≤ y ≤ 0.5 and −0.5 ≤ y ≤ 0.5. By performing Algorithms 1 with η = 0.009, λ = 0.0001 and iterationmax = 5000, 1, σ1, σ2σ3, σ1σ2 3 are selected to approximate the complete multinomial.
The new structure corresponds to Fig.
3(c) and the new weighted linear combination is 2, and σ3 2, σ2σ2 3, σ3 1, σ3 w0,1 + w0,2σ1 + w0,3σ2σ3 + w0,4σ1σ2 2 + w0,5σ2σ2 3 + w0,6σ3 1 + w0,7σ3 2 + w0,8σ3 (32) and we have ∧1 = ∅,∧2 = {1},∧3 = {2, 3},∧4 = {1, 2, 2},∧5 = {2, 3, 3},∧6 = {1, 1, 1},∧7 = {2, 2, 2},∧8 = {3, 3, 3} (33) Then, we perform Algorithms 2 and use the test samples to evaluate our method.
The average error and the improvement of the performance have been shown in Table 3.
The results show that the new structure attains the smallest error.
From Fig.
2, we see that the surface of Gabor error function in new structures is monotonically decreasing and converge to 0, as predicted by Theorem 1.
Table 3: Comparison of average error for Gabor approximate error function.
Learning Rate Average Old Average New Improvement % 0.001 0.005 0.01 0.05 0.1 0.0131 0.0133 0.0130 0.0132 0.0131 0.0075 0.0065 0.0064 0.0063 0.0055 42.75 51.13 50.77 52.27 58.02 Figure 2: Comparison of error for Gabor approximation problem.
4.3 Example 3: Sonar data classiﬁcation Sonar problem is a well-known benchmark dataset, which aims to classify reﬂected sonar signals into two categories (metal cylinders and rocks).
The related data set comprises 208 input vectors, each with 60 components.
In this example, 4-fold cross validation is used to perform experiments, that is, 75% samples for training and 25% samples for testing are stochastically selected from the 208 samples.
After performing our method, 1, σ3, σ1σ2, σ2 2 are selected to to approximate the complete multinomial.
The new structure corresponds to Fig.
3(d) and the new weighted linear combination is 3 and σ3 3, σ2σ2 1, σ2σ2 1, σ2 w0,1 + w0,2σ3 + w0,3σ1σ2 + w0,4σ2 1 + w0,5σ2 3 + w0,6σ2σ2 1 + w0,7σ2σ2 3 + w0,8σ3 (34) Then, we have ∧1 = ∅,∧2 = {2},∧3 = {1, 2},∧4 = {1, 1},∧5 = {3, 3},∧6 = {1, 1, 2},∧7 = {2, 3, 3},∧8 = {2, 2, 2} (35) Table 4: Comparison of average classiﬁcation accuracy for sonar problem.
Round Old Train New Train Impprovment% Old Test New Test Improvement% Overall 79.42 86.22 85.12 80.77 79.97 82.30 89.26 95.67 87.51 90.87 84.94 89.65 11.67 10.39 2.77 11.77 6.03 8.67 71.16 81.02 78.85 68.52 76.28 75.17 83.18 90.74 81.73 82.85 80.45 83.79 15.58 11.32 3.59 18.93 5.32 10.85 Table 5: Comparison of the best classiﬁcation accuracy for sonar problem.
Round Old Train New Train Impprovment% Old Test New Test Improvement% Overall 89.47 94.87 95.62 84.62 88.46 90.61 99.36 99.36 96.15 100.0 100.0 98.97 10.48 4.62 0.55 16.66 12.25 8.82 84.62 89.47 88.46 89.47 79.49 86.30 98.11 95.62 92.31 100.0 88.46 94.90 14.76 6.65 4.26 11.12 10.65 9.49 Table 6: Comparison of the worst classiﬁcation accuracy for sonar problem.
Round Old Train New Train Impprovment% Old Test New Test Improvement% Overall 71.79 73.08 73.08 71.79 75.00 72.95 80.77 91.03 78.21 75.00 78.21 80.64 11.77 21.88 6.85 4.37 4.19 10.01 57.69 71.79 61.54 50.0 71.79 62.56 69.23 84.62 65.38 71.79 78.21 73.85 18.18 16.41 6.05 35.78 8.56 16.55 In both structures, 20 trials are carried out for each learning algorithm.
In Tables 4, 5 and 6, we compare average accuracy, best accuracy and worst accuracy of classiﬁcation of both structures, respectively.
In all three tables, it can be seen that new structure is more advantageous than the old structure.
These show that our new structure is better and monotonically decreasing and converge to 0 during the iterative learning as predicted by Theorem 1.
4.4 Example 4: Pima Indians diabetes data classiﬁcation To verify the theoretical evaluation of MSPSNNs, we used a Pima Indians Diabetes Database, which comprises 768 samples with 8 attributes.
The dataset is available at UCI machine learning repository (http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes).
4-fold cross validation is used to perform our method.
After that, 1, σ1σ2, σ1σ3, σ1σ4, σ2σ3, σ2σ4, σ3σ4 , σ2 1, σ2 2, σ2 3, σ2 4, σ1σ2 2, σ3σ2 2, σ1σ2 3, σ4σ2 3 are selected.
The new structure corresponds to Fig.
5 and the new weighted linear combination is w0,1 + w0,2σ1σ2 + w0,3σ1σ3 + w0,4σ1σ4 + w0,5σ2σ3 + w0,6σ2σ4 + w0,7σ3σ4 + w0,8σ2 w0,9σ2 2 + w0,13σ3σ2 3 + w0,15σ4σ2 2 + w0,14σ1σ2 2 + w0,10σ2 3 + w0,11σ2 4 + w0,12σ1σ2 1...
(36) Then, we have ∧1 = ∅,∧2 = {1, 2},∧3 = {1, 3},∧4 = {1, 4},∧5 = {2, 3},∧6 = {2, 4},∧7 = {3, 4},∧8 = {1, 1}...
∧9 = {2, 2},∧10 = {3, 3},∧11 = {4, 4},∧12 = {1, 2, 2},∧13 = {2, 2, 3},∧14 = {1, 3, 3},∧15 = {3, 3, 4} (37) The results of comparative analysis experiments using old and new structure for four-order are also presented, paying particular attention to average error, average best error and average wort correct classiﬁcation shown in Tables 6, 7 and 8.
These lead to verify the theoretical evaluation of SPSNNs learning with new structure is better and monotonically decreasing and converge to 0 during the iterative learning as predicted by Theorem 1.
5 Conclusion In this study, we use the smoothing L1/2 regularization to automatically select some appropriate terms to approximate the complete Kolmogorov-Gabor Multinomial for the product layer of SP- SNNs.Numerical experiments are implemented for Mayas’ function problem, Gabor function prob- lem, Sonar data classiﬁcation and Pima Indians diabetes data classiﬁcation.
The numerical results (a) (b) (c) (d) Figure 3: (a) Old structure; (b) New structure based on Mayas’ function approximation; (c) New structure based on Gabor function approximation; (d) New structure based on Sonar problem ap- proximation.
Table 7: Comparison of average error for Pima Indians problem.
Round Old Train New Train Impprovment% Old Test New Test Improvement% Overall 78.76 73.04 84.33 81.01 75.97 78.62 82.53 79.38 84.70 85.81 81.22 82.73 4.67 8.32 0.44 5.75 6.68 5.09 72.31 68.24 79.44 75.86 71.52 73.47 77.57 84.62 65.38 71.79 78.21 77.4 7.02 9.2 0.24 4.18 5.89 7.79 Table 8: Comparison of best error for Pima Indians problem.
Round Old Train New Train Impprovment% Old Test New Test Improvement% Overall 88.13 77.01 89.17 87.08 78.75 84.03 89.39 85.97 91.37 90.00 83.76 88.10 1.42 11.0 2.44 3.30 6.17 4.73 76.67 74.58 85.97 81.53 75.63 78.88 786.61 78.75 88.06 83.93 79.70 83.41 12.18 5.44 2.40 2.90 5.24 5.70 demonstrate that the terms, or equivalently the product nodes selected by using smoothing L1/2 regularization have been proved the capability of providing more possibility powerful mapping, which is diﬀerent from that in old structure.
References [1] C.K. Li, A sigma-pi-sigma neural network, Neural Process.
Lett.
17 (2003), pp.
1-9.
∏ ∏ ∏ ∏ ∏ ∏ ∏ 1 Input layer ∑1 ∏ x ∈ RM ∑2 τ ∈ RQ σ ∈ RN w0,8 w0,1 wNM w11 ∑ ∑ ∑ ∑ x1 xM x2 x3 ∏ ∏ ∏ ∏ ∏ 1 1 ∏ ∏ ∏ ∏ ∏ ∏ 1 Input layer ∑1 ∏ x ∈ RM ∑2 τ ∈ RQ σ ∈ RN w0,8 w0,1 wNM w11 ∑ ∑ ∑ ∑ x1 xM x2 x3 ∏ ∏ ∏ ∏ ∏ 1 1 ∏ ∏ ∏ ∏ ∏ ∏ 1 Input layer ∑1 ∏ x ∈ RM ∑2 τ ∈ RQ σ ∈ RN w0,8 w0,1 wNM w11 ∑ ∑ ∑ ∑ x1 xM x2 x3 ∏ ∏ ∏ ∏ ∏ 1 1 ∏ ∏ ∏ ∏ ∏ ∏ ∏ 1 Input layer ∑1 ∏ x ∈ RM ∑2 τ ∈ RQ σ ∈ RN w0,8 w0,1 wNM w11 ∑ ∑ ∑ ∑ x1 xM x2 x3 ∏ ∏ ∏ ∏ ∏ 1 1 Figure 4: Four-input three-order Old structure Figure 5: Four-input three-order new structure based on Pima Indians diabetes data classiﬁcation.
Table 9: Comparison of wort error for Pima indians problem.
Round Old Train New Train Impprovment% Old Test New Test Improvement% Overall 73.61 63.49 76.04 69.44 73.96 71.31 75.41 73.0 77.08 76.04 79.44 76.19 2.42 13.94 1.36 9.07 7.2 13.0 69.74 61.98 69.44 61.98 69.44 66.52 71.59 66.76 71.40 69.44 71.98 70.23 2.62 7.43 2.78 11.35 3.59 5.43 [2] Y.
Liu, Z.
Li, D.
Yang, K.S. Mohamed, J.
Wang, W.
Wu, Convergence of batch gradient learning algorithm with smoothing L1/2 regularization for Sigma-pi-sigma neural networks.
Neurocom- puting 151 (2015), pp.
333-341.
[3] M.M. Gupta, N.
Homma, Z.G. Hou, M.G. Solo, I.
Bukovsky, Higher order neural networks: fundamental theory and applications.
Artiﬁcial Higher Order Neural Networks for Computer Science and Engineering: Trends for Emerging Applications pp.
397-422, 2010.
[4] D.E. Rumelhart, G.E. Hinton, R.J. Williams, (1985) Learning internal representations by error propagation (No. ICS-8506).
California Univ San Diego La Jolla Inst for Cognitive Science.
∏ ∏ 1 Input layer ∑1 ∏ x ∈ RM ∑2 τ ∈ RQ σ ∈ RN w0,15 w0,1 wNM w11 ∑ ∑ ∑ ∑ x1 xM x2 x3 ∏ ∏ ∏ ∏ ∏ 1 1 x4 ∑ ∏ ∏ ∏ ∏ ∏ ∏ ∏ ∏ ∏ 1 Input layer ∑1 ∏ x ∈ RM ∑2 τ ∈ RQ σ ∈ RN w0,15 w0,1 wNM w11 ∑ ∑ ∑ ∑ x1 xM x2 x3 ∏ ∏ ∏ ∏ ∏ 1 1 x4 ∑ ∏ ∏ ∏ ∏ ∏ ∏ ∏ [5] G.P. Liu, (2012) Nonlinear identiﬁcation and control: a neural network approach.
Springer Science & Business Media.
[6] M.Fallahnezhad,M.H. Moradi,S.
Zaferanlouei, (2011) A hybrid higher order neural classiﬁer for handling classiﬁcation problems.
Expert Systems with Applications, 38(1), 386-393.
[7] A.
R.
barron, Predicted Squared Error: A criterion for automatic model selection,” Self- Organizing Methods in Modeling: GMDH Type Algorithms (S.J. Farlow, Ed.), Marcel Dekker, Inc., NY, Chap.4 (1984), pp.
87-103.
[8] V.
S.
Stometta and B.
A.
Hubermann, An improved three-layer back propagation algorithm, in Proc.
IEEE IJCNN, vol.
2, (1987), pp.637-643.
[9] H.
Zhang, Y.
Tang, X.
Liu, (2015) Batch gradient training method with smoothing l0 regular- ization for feedforward neural networks.
Neural Computing and Applications, 26(2), 383-390.
[10] W.
Wu, Q.W. Fan, J.M. Zurada et al., Batch gradient method with smoothing L1/2 regularization for training of feedforward neural networks,Neural Networks 50 (2014) 72-78.
[11] R.
Reed, Pruning algorithms-a survey,IEEE Transactionson Neural Networks 8 (1997) 185-204.
[12] S.
Shalev-Shwartz, T.
Zhang, (2014) Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization.
In International Conference on Machine Learning (pp.
64-72).
[13] J.
Tang, S.
Alelyani, H.
Liu, (2014) Feature selection for classiﬁcation: A review.
Data Classiﬁ- cation: Algorithms and Applications, pp.
37.
[14] S.
Scardapane, D.
Comminiello, A.
Hussain, et al.
(2017) Group sparse regularization for deep neural networks.
Neurocomputing 241:81-89.
[15] Z.
Xu, H.
Zhang, Y.
Wang, X.
Chang, L1/2 regularization, Science ChinaInformation Sciences, Vol.
53, No. 6 (2010) pp.
1159-1165.
[16] W.
Wu, H.
Shao, Z.
Li, Convergence of batch BP algorithm with penalty for FNN training, in Neural Information Processing, (2006) 562-569.
[17] J.
Wang, W Wu, J.
M.
Zurada, 2012, Computational properties and conver-gence analysis of BPNN for cyclic and almost cyclic learning with penalty, Neural Networks, Vol.
33, pp.
127-135.
[18] A.
S.
Weigend, D.
E.
Rumelhart, B.
Huberman, Generalization by weight-elimination applied to currency exchange rate prediction, in Neural Networks, IJCNN 1991 International Joint Confer- ence on, Seattle, (1991), PP.837-841.
[19] Z.
Xu, X.
Chang, F.
Xu, H.
Zhang, L1/2 Regularization: A Thresholding Representation Theory and a Fast Solver, Neural Networks and Learning Sys-tems, IEEE Transactions on, Vol.
23, No. 7, (2012) pp.
1013-1027.
[20] M.
Yuan, Y.
Lin, Model selection and estimation in regression with grouped variables, Journal of the Royal Statistical Society: Series B (Statistical Methodology), Vol.
68 (2006) pp.
49-67.

1.1 Related Works .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1.2 Problem Setting .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1.3 Our Results .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1.4 Our Techniques .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
(cid:101)O(k2)-Space Algorithm with Chen’ Framework .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
(cid:101)O(k)-Space Algorithm (Nearly Optimal) with Sensitivity-based Sampling .
.
1.4.1 1.4.2 1.4.3 Max-Cut and Average-Distance .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1.5 Meta Algorithms and a Roadmap .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1.6 Concluding Remark .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1.7 Acknowledgments .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
References A Notation B Preliminaries B.1 Deﬁnitions of Dynamic Streaming Model for k-Clustering Problems .
.
.
.
.
.
.
.
.
.
B.2 Deﬁnitions of k-means Clustering .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
B.3 Deﬁnitions of M-estimator Clustering .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
B.4 Basic Probability Tools .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
B.5 Tools from Previous Work .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
C Why Do Previous Techniques Fail?
D Coreset Construction for k-means Based on Chen’s Framework D.1 Deﬁnitions and Properties .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
D.2 Recursive Partition Approach .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
D.3 Bounding the Close Parts .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
D.4 Bounding the Far Parts .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
D.5 Our Coreset Construction .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
E Coreset Over the Dynamic Data Stream E.1 Streaming Coreset Construction .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
E.2 The Dynamic Point-Cell Storing Data Structure .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
E.3 Main Algorithm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
F General Clustering Problem F.1 M-Estimator Clustering .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
F.2 Improvements Over k-median .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
G Applications G.1 A Dynamic Streaming Approximation to Max-CUT .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
G.2 A Dynamic Streaming Approximation to Average Distance .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
10 12 12 13 13 14 20 20 20 20 21 21 22 22 23 24 26 28 30 33 35 35 36 38 43 43 45 45 46 46 H (cid:101)O(k) Space Algorithm Based on Sensitivity Sampling H.1 Deﬁnitions and Preliminaries .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
H.2 Reducing Original Problem to Important Points .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
H.3 Sampling Scores of Important Points .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
H.4 Algorithm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
47 47 47 49 51 1 Introduction Clustering is one of the central problems in modern research of computation, including algorithmic design, machine learning and many more.
Among all the clustering methods, k-means is one of the most important approaches for clustering geometric datasets.
The ﬁrst k-means algorithm dates back to the 1950s.
Over the last half of the century, varies works have studied this problem (see [Jai10] for a complete survey).
It also inspired tremendous other variants of clustering methods, e.g. aﬃnity propagation, mean-shift,spectral clustering, mini batch k-means and many more (e.g., [Scu10, CM02, SM00, MS01, NJW02, VL07]).
This problem has also been studied in diﬀerent settings and computational models, e.g. distributed computing, parallel computing, map-reduce, streaming setting and even quantum computing.
k-means clustering has been successfully used in various research topics, e.g., database, data-mining, computer vision, geostatistics, agriculture, and astrophysics (e.g., [SM00, GKL+17, LIY+15, MS01]).
In 2004, Indyk [Ind04] introduced the model for dynamic geometric data streams, in which a set of geometric points from a d-dimensional discrete space [∆]d are updated dynamically, for some large ∆, i.e., the data stream is of the form insert(p) and delete(p) for p ∈ [∆]d.
For a ﬁxed dimension d, Frahling and Sohler [FS05] develop the ﬁrst eﬃcient streaming (1 + )-approximation algorithms for k-means, k-median, and as well as other geometric problems over dynamic data streams.
In their paper, they propose an algorithm to maintain a coreset of size k−O(d) for obtaining a (1 ± ) approximation of k-clustering, including k-means and k-median.
A coreset for clustering is a small weighted point set that summarizes the original dataset.
Solving the clustering problem over the coreset provides an approximate solution for the original problem.
It is one of the fundamental tool for solving k-means clustering problem.
A very recent work by Braverman, Frahling, Lang, Sohler & Yang [BFL+17] provides a data structure for maintaining a coreset for k-median in dynamic stream using space polynomial in d.
However their technique cannot be extended to k-means, since the technique relies heavily on the p∈P dist(p, C), where dist(·,·) is the Euclidean metric and C is a set of points.
Later we show a hard example (in Section C) that the error based on their design is unbounded for k-means, which is a form of sum of squares, i.e., p dist2(p, C).
In this paper, we ﬁrst show that by combining the coreset framework of [Che09] together with the grid structure constructed in [BFL+17] gives a data structure for maintaining a k-means coreset over dynamic stream that use space polynomial in d.
Such a data structure uses fact that k-median cost function is a form of sum of absolutes, i.e.,(cid:80) (cid:80) space (cid:101)O(k2 poly(d)).
Although the space is not optimal in k, but it is already the ﬁrst (to the best of our knowledge) of its kind obtaining space polynomial in d.
To obtain an algorithm with space nearly optimal in k (i.e., linear in k), new idea has to be introduced.
In 2011, [FL11] introduced a revolutionary coreset framework for constructing coreset in batch-setting and insertion-only stream.
Their coreset framework is by sampling data points based on the “sensitivity” of each point.
It is deﬁned as the maximum percentage change of the cost function over all possible clustering solutions after removing the point from the dataset.
Very recently, [BFL16] improves upon this framework and gives a better insertion-only streaming algorithm.
One of our major contributions is to show that the sensitivity-based sampling scheme is achievable even in dynamic update streams.
Hence we obtain another algorithm that is not only space nearly optimal in k but also polynomial in dimension d.
We further show that our data structure for maintaining coreset can be extended as a uniﬁed approach for a more general classes of k-clustering, including k-median, M-estimator clustering, and clusterings with a more general set of cost functions over a distance.
For all these tasks, the space/time of our algorithm is similar to k-means with only a poly(d) factor diﬀerence.
Here an M- estimator represents a speciﬁc function over the distance, e.g., the Huber norm [Hub64] is speciﬁed by a parameter τ > 0, and its measure function H is given by H(x) = x2/2τ, if |x| ≤ τ and H(x) = |x| − τ /2 otherwise.
Recently, M-estimators attracted interests in a variety of computing ﬁelds (e.g., [NYWR09, CW15b, CW15a, SWZ17b]).
1.1 Related Works It is well known that ﬁnding the optimal solution for k-means is NP-hard even for k = 2 or d = 2 [DFK+04, ADHP09, MNV09, Vat09].
The most success algorithm used in practice is Lloyd’s algorithm, which is also known as “the” k-means method [Llo82].
Since k-means can’t be solved in polynomial time, there has several works trying to understand the “local search method” for k-means.
Kanungo et al.
[KMN+02] proved that a very simple local search heuristic is able to obtain a polynomial-time algorithm with approximation ratio 9 +  for any ﬁxed  > 0 for k- means in Euclidean space.
Very recently, two groups improved the ratio to 1 +  independently, Friggstad, Rezapour and Salavatipour [FRS16] showed that, for any error parameter  > 0, the local search algorithm that considers swaps of up to dO(d) · (1/)O(d/) centers at a time will produce a solution using exactly k centers whose cost is at most (1 + )-factor greater than the optimum solution.
Cohen-Addad, Klein and Mathieu [CAKM16] proved that the number of swapped centers is poly(1/).
A large number of works [IKI94, Mat00, BHPI02, DLVKKR03, HPM04] proposed (1 + )-approximation algorithm with ineﬃcient running time in Rd. There is a line of works targeting insertion-only streaming k-means or k-median.
For example, [BS80, GMMO00, COP03, BDMO03, AHPV04, HPM04, HPK05, Che09, FL11, FS12, AMR+12, BFL16] and many others have developed and improved streaming algorithms for computing a so- lution for k-means and k-median approximately.
Recent years, there have been lots of interest in dynamic streaming algorithms for other settings, e.g. [BYJK+02, FKM+05, Bas08, KL11, AGM12a, AGM12b, GKP12, GKK12, AGM12b, BKS12, CMS13, CMS13, AGM13, McG14, BGS15, BHNT15, BS16, ACD+16, ADK+16, BWZ16, KLM+17, SWZ17a, SWZ17b].
In addition, there are many works related to k-median or k-means in diﬀerent settings, e.g., [IP11, BIP+16, BCMN14, CCGG98].
1.2 Problem Setting In this section, we formally deﬁne the problem of interests.
We consider about the streaming setting, where the space of an algorithm is limited, i.e., we cannot store all the input points.
In the dynamic data stream setting, we allow points to be deleted.
Formally, let Q denote a set of points on high dimensional grids [∆]d1, initialized as an empty set.
At time t, we observe a tuple (pt, opt) where pt ∈ [∆]d and opt ∈ {insertion, deletion} meaning we insert a point to or delete a point from Q.
Note that some of the points we observed in the stream might not belong to Q at the end of the stream since deletion is allowed.
After one-pass of the data-stream, we want to output a small multi-set of points (i.e., of size o(|Q|)) S (associated with some weight-function w to each point) which summarizes the the important properties of the ground-truth points set Q.
Also we require our algorithm to have a ﬁxed memory budget at any update time.
This rules out the naïve approach of storing Q in memory explicitly.
Formally speaking, we require S to be an  k-means coreset for Q, which satisﬁes, ∀Z ⊂ [∆]d,|Z| = k : (1 − ) cost(Q, Z) ≤ costw(S, Z) ≤ (1 + ) cost(Q, Z), 1We restrict the setting to be on the integer grid for the sake of representation simplicity.
Our algorithm and analysis can be trivially extended to non-integer grid setting.
where (cid:88) p∈Q min z∈Z cost(Q, Z) = dist2(p, z) and costw(S, Z) = (cid:88) p∈Q wp dist2(p, z) min z∈Z ⊂ [∆]d with |C∗ | = k such that the function cost(C) :=(cid:80) For other diﬀerent clustering objectives, e.g., M-estimator clustering, our problem setting is roughly the same except the dist2(·) function is changed to the corresponding function.
1.3 Our Results k-Means For a discrete geometric point set P ⊂ [∆]d, the k-means problem is to ﬁnd a set C∗ p∈P dist2(p, C) is minimized, where dist(p, C) = minc∈C dist(p, c) describing the minimal distance from p to a point in C.
We de- velop the ﬁrst (1 + )-approximation algorithm for the k-means clustering problem in dynamic data streams that uses space polynomial in the dimension d.
To the best of our knowledge, all previ- ous algorithms for k-means in the dynamic streaming setting required space exponentially in the dimension.
Formally, our main theorem states, Theorem 1.1 (k-means).
Fix , δ ∈ (0, 1/2), k, ∆ ∈ N+, let L = log ∆.
There is a data struc- ture supporting insertions and deletions of a point set Q ⊂ [∆]d, maintaining a weighted set S with positive weights for each point, such that with probability at least 1 − δ, at any time of the (cid:101)O(−2k2 · log2(1/δ) · poly(d, L)) bits in the worst case2.
For each update of the input, the algo- stream, S is an -coreset for k-means of size −2k2 · log(1/δ) · poly(d, L).
The data structure uses rithm needs poly(d, 1/, L, log k) time to process.
After one pass, it outputs the coreset in time poly(d, k, L, 1/, log 1/δ).
Note that, for the k-means problem, [CEM+15] shows that one can always do the random projection to reduce d to O(k/2).
However since random projection does not preserve the grid- like structure, it remains a caveat to use random projection method in dynamic geometric stream.
Nevertheless, the most interesting setting would be that when d ≤ O(k/2) and d (cid:29) log k.
The theorem is restated in Theorem E.5 in Section E and the proof is presented therein.
We further present another one-pass algorithm with diﬀerent trade-oﬀs in k and .
This algo- rithm has space nearly linear in k while still polynomial in d.
The dependence on k is ensentially nearly optimal (for ﬁxed  and d)! The guarantee of our result is presented in the following theorem.
Theorem 1.2. Fix , δ ∈ (0, 1/2), k, ∆ ∈ N+, let L = log ∆.
There is a data structure supporting insertions and deletions of a point set Q ⊂ [∆]d, maintaining a weighted set S with positive weights k-means of size k· log(1/δ)· poly(d, L, 1/),3 The data structure uses (cid:101)O(k· log2(1/δ)· poly(d, L, 1/)) for each point, such that with probability at least 1−δ, at any time of the stream, S is an -coreset for bits in the worst case.
For each update of the input, the algorithm needs poly(d, 1/, L, log k) time to process.
After one pass, it outputs the coreset in time poly(d, k, L, 1/, log 1/δ).
M-estimator Clustering and More General Cost Functions Further we show that our algorithm and analysis on k-means can be extended to general functions (including M-estimator) over distances, i.e., a function that satisﬁes approximately sub-additivity: there is a ﬁxed constant C > 0, 2For any function f, we deﬁne (cid:101)O(f ) to be f · logO(1)(f ).
∀x, y ≥ 0, M (x + y) ≤ C · (M (x) + M (y)).
3The exact dependence of  is determined by the coreset framework in [FL11] and [BFL16], we show that −3 dependence is suﬃcient.
See the Section H for details.
Notice that the above equation automatically holds when M (·) is a non-decreasing function satis- fying ∀x > 0, M (x) > 0 and ∀c > 0, M (cx) ≤ f (c)M (x) where f (c) > 0 is a bounded function.
In this case, we are aiming to solve (cid:88) z∈Q min Z⊂[∆]d:|Z|≤k min z∈Z M (dist(q, z)) for a given point set Q.
Although M-estimator clustering problem has been studied by [FS12, BFL16], their algorithm can only be applied in insertion only streaming model.
We show the ﬁrst dynamic streaming algo- rithms for this problem.
Our data structure maintains a coreset with size similar to k-means, with only slightly diﬀerent dependence on the dimension d.
More precisely, if we extend the algorithm and the analysis of Theorem 1.1 to the M-estimator clustering setting, then we can get an algorithm with success probability at least 1 − δ which can output an -coreset for M-estimator clustering the worst case, where the poly(d) factor depends on the M-estimator function.
For each update of the input, the algorithm needs poly(d, 1/, L, log k) time to process and outputs the coreset in time poly(d, k, L, 1/, log 1/δ).
We can also generalize the algorithm and the analysis of Theorem 1.2 to the M-estimator clustering setting.
In this case, we can get a coreset of size k · log(1/δ)· poly(d, L, 1/), In Section F, we show the details of how to generalize the algorithm and analysis of Theorem 1.1 of size −2k2 · log(1/δ) · poly(d, L).
The data structure uses (cid:101)O(−2k2 · log2(1/δ) · poly(d, L)) bits in and the data structure uses (cid:101)O(k · log2(1/δ) · poly(d, L, )) bits in the worst case.
to M-estimator clustering setting.
Maintaining Approximate Solutions for Max-Cut and Average Distance We also show that our data-structure can be used to maintain an approximate solution for the Max-Cut and Average-Distance problems, where the ﬁrst problem asks to ﬁnd a cut of the streaming point set Q, such that the sum of distances of all possible pairs across the cut are maximized; the later problem asks to estimate average distance over all pairs.
Our data structure supports maintaining a 1/2- approximation to Max-Cut over all times of the stream, and estimating the cost of the cut up to (1±) factor.
For Average-Distance, our data structure supports maintaining a (1±)-approximation over all times of the stream.
Furthermore, our data structure maintains approximate solutions for the generalized version of these problems, i.e., the M-estimator over distances.
The data structure for these problems uses space polynomial in 1/, d, log ∆, log 1/δ, where δ is the failure probability.
The formal results are presented in Section G.
(cid:101)O(k2)-Space Algorithm with Chen’ Framework 1.4 Our Techniques 1.4.1 We ﬁrst show a data structure design whose underlying framework is based on [Che09].
Denote Xk = {C ⊂ [∆]d : |C| ≤ k} as the set of all k-sets.
For a ﬁnite point set P ⊂ [∆]d, an -coreset for k-means of P is a weighted point set S ⊂ [∆]d such that where cost(S, C) =(cid:80) ∀C ∈ Xk : | cost(P, C) − cost(S, C)| ≤  cost(P, C) s∈S ws dist(s, C)2, where ws is the weight of point s.
Our coreset algorithm can be viewed as a combination of several techniques in both clustering algorithms and dynamic streaming algorithms.
From a high level, we apply an oblivious grid structure over the point sets as used in [FS05] and [BFL+17] to form an implicit partition of the point sets.
This partition satisﬁes the crucial property as required by the coreset framework of Chen [Che09].
We then build a dynamic streaming algorithm to simulate the random sampling of Chen’s framework.
These combinations are highly non-trivial.
We highlight several diﬃculties and how we resolve it in this paper.
• Firstly, a naïve application of the grid structure in [FS05] gives a coreset with size exponentially depending on d.
To resolve this problem, we randomly shift the grid struture and show that the exponential dependence on d becomes polynomial.
• Secondly, in Chen’s framework, points are stored in memory, one can do straightforward sampling from the dataset.
However, for us, the memory of the algorithm is limited and points can even be deleted.
We do not know any of the sampling parameters before the stream coming.
But these parameters are crucial to implement Chen’s framework.
We resolve this problem by applying a clever data structure, which is similar to the K-set data structure as in [Gan05].
This data structure has a small memory budget, and can output the set of points under insertions and deletions if the size of ﬁnal point set is smaller than the memory budget.
Without knowing any parameters of the true dataset, we guess polylogarithmically many possibilities of all the parameters in Chen’s framework.
We show that if the guessed parameters are close to the true parameters, the data sample is guaranteed to be smaller than the memory budget.
This set of points form a coreset of the dataset and will be output by the sampling data structure.
Next we elaborate the details of each framework that we have been applying.
We start with the introduction of Chen’s coreset framework.
for some t ≤ αk and(cid:80)t Chen’s Coreset Framework The bottom level of our algorithm is the framework of Chen [Che09].
As shown in Figure 1, the core idea is to ﬁnd a partition of the dataset P = P1∪ P2∪ .
.
.
Pt, i=1 |Pi| diam(Pi)2 ≤ β OPT, where OPT is the optimal cost of k-means on P .
ple (cid:101)O(β2k/(2)) points uniformly at random from each part Pi such that the error of estimating the We denote such a partition as an (α, β)-partition.
If an (α, β)-partition is known, then one can sam- cost of the contribution of Pi is |Pi| diam(Pi)2/β.
More formally, denote Xk = {C ⊂ [∆]d : |C| ≤ k} as the set of all k centers and Si as the set of samples for Pi. Assigning each point in Si with a weight |Pi|/|Si|, then by a Hoeﬀding bound, we expect, with high probability, Notice that size of the coreset is (cid:101)O(αk2β2).
Combining the samples of each part, we obtain that S = ∪t (cid:12)(cid:12) cost(Pi, C) − cost(Si, C) ∀C ∈ Xk : (cid:12)(cid:12) ≤ |Pi| diam(Pi)2/β.
i=1Si is a coreset for k-means of P .
Obtaining An (α, β)-partition in a Dynamic Stream In an oﬄine setting, i.e., the case that all points are stored in memory, obtaining an (α, β)-partition is quite straightforward, i.e., by using Indyk’s (α, β)-bi-criterion algorithm [Ind00a].
But it becomes challenging once the algorithm has limited memory and points can be deleted.
We overcome this diﬃculty by applying a similar grid structure as used in [FS05] and [BFL+17].
As shown in Figure 1(b), we build log ∆ many grids over the dataset.
Each higher layer reﬁnes its parent layer by splitting each cell into 2d many sub-cells.
We stress that the grid structure is oblivious to the point set.
Because of this crucial property, we are able to insert and delete points from the point set, and the grids stay intacted.
Similar to [FS05], one can show that for each level i, the number of cells containing more than O(OPT·4i/∆2/k) Figure 1: (a) The coreset framework of Chen [Che09].
The point set is partitioned in to a set of sets, which are called a (α, β)-partition.
Points are then sampled from each partition.
See texts for details.
(b)The grid structure over the point set.
From top to bottom, we have four levels of grids.
Each higher level partition a cell in the parent level into 2d many sub-cells, where d is the dimension of d.
points is bounded by 2O(d) · k, independent of the number of points in P .
We shall call these cells as heavy cells, which also form a tree.
Notice that the grid structure has only log ∆ levels, the number of heavy cells is O(log ∆· 2O(d) · k).
Since each heavy cell has diameter √d· ∆/2i and the non-heavy children of a heavy cell in level i contains at most O(2d · OPT·4i/∆2/k) points, thus in each level, the (number of points in non-heavy children) · (diameter of the heavy cell)2 is bounded by O(2d OPT·4i/∆2) · (√d · ∆/2i)2 = O(2d OPT /k).
Applying the deﬁnition of the (α, β)-partition, we then show that these non-heavy children of heavy cells form a (α, β)-partition, here α = O(2d·log ∆) and β = O(2d·log ∆).
The majority of their work algorithm does not apply Chen’s framework, and thus their ﬁnally -coreset is of size (cid:101)O(k/O(d)) is showing how a dynamic streaming algorithm can maintain such a partition.
We stress that their and the algorithm uses (cid:101)O(k2/O(d)) space.
Removing Exponential Dependence on d In the last paragraph, we show that an (α, β)- partition can be implicitly obtained by simply building a grid structure over the dataset.
However, the size of this partition is of exponential in d.
To get rid of this exponential dependence, we apply a random shift to the grid structure, as shown in Figure 2.
We now explain why a random shift brings down the dependence on d.
As shown in the last paragraph, a heavy cell is deﬁned by the number of points in it.
Let us ﬁx an optimal solution of k-means, i.e., a set of k-centers z1, z2, .
.
.
, zk.
A random shift grantees that, with high probability, any of the zis is “far” away from a boundary of a cell.
Conditioning on this event, for the cells that do not contain an optimal center, every point in it contributes to the cost at least the distance of the optimal center to the boundary of a cell, which is “far”.
Therefore, we are able to bound the number of cells containing no center but containing too many points .
With the k cells containing a optimal center, the total number of P1P2P3P4P5P6Figure 2: Random shift of a grid brings down the number of heavy cells.
In the left panel, we have a worst case alignment of points and grids that many cells contain lots of points.
In the right panel, after the random shift, only two cells are containing many points.
“heavy cell”s is bounded by O(α · k), where α = O(d · log ∆).
With additional tricks, we show an implicit (α, β)-partition can be constructed from the heavy cells, where β = (poly(d log ∆)).
Sample Maintainance and Rejection Data Structure With the random-shifted grid struc- ture, we aim to sample points from the (α, β)-partition, as it does in Chen’s construction.
However, without knowing the parameters of the point sets, i.e., the optimal cost OPT, we have to guess its value.
We guess logarithmically many possibilities of OPT, i.e., oi = 2i for i = 1, 2, .
.
.
, log(d∆d).
For each guess, we run an sampler and attempt to get a coreset.
Notice that, all these guesses and sampler based on Chen’s construction are oblivious to the input point set.
Later we show that if the guess oi ≤ OPT, then the above mentioned samples (with the sampling probability and heavy cells deﬁned by oi) form an -coreset to the point set.
However if oi/ OPT is too small, the coreset is too large.
We construct a new dynamic set-point sampler data structure that has a ﬁxed memory budget and rejects large data samples but preserve data samples with a small number of points.
With this data structure, we can pick the smallest oi, whose resulting samples are preserved by the data structure.
As a result, these samples are the correct coreset.
Now we elaborates the details of the data structure.
This data structure supports insertions and deletions over pairs of the form (C, p), where C represents a set and p ∈ [∆]d is point.
The data structure supports querying all the points in sets with small number of points, at any time of the stream.
Suppose there are at most α non-empty sets and at most θ sets containing at least β points and the rest of the sets containing γ points.
Then with a memory budget of [Θ(θβ + γ)], the algorithm is able to maintain the number of points in each cell, and the γ points in cells with less than β points.
The high level idea is to use two level of pair-wise hash functions.
In the ﬁrst level, we hash the ID of the set (i.e., the name or the coordinate if the set is a cell) to a universe [Θ(α)] and the point to a universe [Θ(β)], then we hash the pair of hash values of the set ID and the point to a universe [Θ(θβ + γ)].
It can be shown that if a point c from a set C with less than β points, then the third hash value is unique to all other pairs, with at least constant probability.
Use this unique hash value, plus a sanity checker based on parity we can recover each bit of the point and the ID of the set.
By repeating log 1/δ times, we can recover all the γ points from sets with less than β points.
randomlyshiftheavycellonlysmallnumberofcellscontainingmanypoints1.4.2 (cid:101)O(k)-Space Algorithm (Nearly Optimal) with Sensitivity-based Sampling Next we show our techniques for improving the space complexity from (cid:101)O(k2) to (cid:101)O(k).
The high level idea of our data structure is to simulate a sensitivity-based sampling using an oblivious linear sketch.
Our sampling scheme takes the advantages of most of the data structures constructed in the last section.
To begin, we ﬁrst brieﬂy review the coreset framework of [FL11] and [BFL16].
In [FL11] and [BFL16], A Brief Review of the Coreset Framework in [FL11] and [BFL16] they have proposed a framework called “sensitivity” sampling.
Since techniques are similar for distance functions other than (cid:96)2 2, we take k-means for an example.
Let Q be the set of points, let X k ⊂ [∆]d×k be the set of all possible k-centers.
Then the sensitivity of a point q ∈ Q is deﬁned as s(q) = max Z∈X k dist2(q, Z)/ dist2(p, Z) (cid:18)(cid:88) p∈Q (cid:19) the point q in Q.
It can be easily shown that(cid:80) Namely, s(q) denotes the maximum possible change in the cost function of any k-set Z after removing each point gets a probability s(cid:48)(q)/((cid:80) q s(q) ≈ k.
Suppose one designs a uniform upper bound s(cid:48)(·) such that ∀q ∈ Q : s(cid:48)(q) ≥ s(q), and denote a probability distribution D over Q as and let R be a set of i.i.d. samples sampling from D with |R| (cid:38) (cid:80) p∈Q s(cid:48)(p)).
The framework in [FL11] and [BFL16] shows that, q∈Q s(cid:48)(q)/2 and assign each point q in Q a weight (|R|s(cid:48)(q))−1, then R is with high probability a (1 ± ) k-means coreset for theory.
If(cid:80) q∈Q s(q), then a (cid:101)O(k)-sized coreset is constructed.
Q.
The proof is by a establishing connection between the VC-dimension theory and the coreset q∈Q s(cid:48)(q) is not too diﬀerent from(cid:80) Vaguely speaking, the reason that sensitivity sampling removes a k factor from our k2-construction is by constructing a new functional space (from the the (cid:38) k samples), whose VC-dimension is O(d) instead of O(kd).
Sensitivity Sampling Over Dynamic Data Stream Analogously to the framework described in the last section, we describe an algorithm that simulates the sensitivity sampling over the dynamic stream.
As we have shown, to correctly assign the sampling probability, all we need is an upper bound on the sensitivity.
For each point p ∈ [∆]d, if p is in the set Q at the end of the stream, then it must have a corresponding sensitivity.
Denote its true sensitivity as s(p).
We want to design a “good” upper bound s(cid:48)(p) of the sensitivity of each point p such that 1.
the sum of s(cid:48)(p) is not too large; 2.
we are able to i.i.d. sample m points from the ﬁnal undeleted points such that each point is chosen with probability proportional to s(cid:48)(p) in each sample, where the magnitude of m is inﬂuenced by the sum of s(cid:48)(p); 3.
at the end of the stream, we are able to approximate s(cid:48)(p) for each given point p.
The ﬁrst property ensures that m will not be too large which means that the size of the coreset is small.
The second property ensures that we can ﬁnally obtain the points in the coreset by implementing such sampling procedure.
The third property shows that we can obtain the weights of the points in the coreset.
In the previous section, we gave the concepts of “heavy cell” and the (α, β) partition based on the random shifted grid.
Precisely, if a cell in the ith level which contains at least Ti number of points, then we say the cell is “heavy” where Ti is a threshold parameter which is set to be Θ( d2 (∆/2i)2 ) k · OPT 10 in our work.
As discussed in the previous section, all the non-“heavy” cells whose parent cell is “heavy” formed a partition.
We call such non-“heavy” cell as a partition cell.
Since the cells formed a partition, it is easy to see, for each undeleted point, it must belong to a unique partition cell.
Furthermore, if the unique partition cell which contains point p is in the ith level of the grid, then we say p is a partition point in the ith level.
Then we show that all the points which are partition points in the ith level have a universal sensitivity upper bound which is Θ(d3/Ti).
Furthermore, if p∈Q s(cid:48)(p) we set the sensitivity upper bound of p as s(cid:48)(p) = Θ(d3/Ti), it is easy to argue that (cid:80) cannot be too large.
The reason is that (cid:88) p∈Q (cid:48) (p) = (# of partition points in the ith level) · Θ(d3/Ti) (# of heavy cells in the ith level) · Θ(d3) level i (cid:88) (cid:88) = (# of heavy cells) · Θ(d3) ≤ α · k · Θ(d3), level i Since (cid:80) where the ﬁrst inequality follows by that each level i heavy cell contains at least Ti number of points.
The second inequality follows by the last section, and α is as the same as mentioned in the last section.
p∈Q s(cid:48)(p) is not large, we know that the size of the coreset will be small.
Also notice that when we know a point p is a partition point in the ith level, then we already know s(cid:48)(p).
By last section, we know that we can have a streaming algorithm which can ﬁnd out all the heavy cells which means that we can know the shape of the whole partition at the end of the stream.
Thus, for any given point p, we can determine its partition cell, and thus know the s(cid:48)(p) which means that for each point in the coreset, we could calculate its weight.
Now, the problem remaining is to get m i.i.d. sample points, and for each sample, each point p is chosen with probability proportional to s(cid:48)(p).
A challenge to implement the sampling procedure is that the sampling scheme here is diﬀerent from the sampling procedure described in the previous section.
Note that in the previous section, the sampling scheme is that we independently determine each point should be chosen or not, but here we need to get m independent samples where each sample is a point in Q, and the probability that p is chosen is proportional to s(cid:48)(p).
We cannot directly use the sampling scheme described in the previous section.
However we can handle this issue by two-stage sampling.
A good property of our s(cid:48)(p) is that all the partition points in the ith level have the same s(cid:48)(p).
To sample a point p with probability proportional to s(cid:48)(p) is equivalent to ﬁrstly sample a level i, where each level j is j = s(cid:48)(p) sampled with probability proportional to (# of partition points in the jth level)·s(cid:48) for point p which is a partition point in the jth level; then we uniformly sample a partition point in the ith level.
To implement the ﬁrst sampling stage, we just need to know the number of partition points in each level.
Fortunately, we can achieve this by using the streaming algorithm described by the last section.
Now let us focus on the second stage.
In this stage, we want to implement the uniform sampling oracle over all the partition points in the ith level.
We cannot apply the traditional (cid:96)0 sampler here since we do not have any information of the partition points in each level at the beginning of the stream.
To achieve our goal, we should be more carefully.
We ﬁrstly subsample all the points in the ith level, we just use the streaming data structure described in the last section to maintain all the survived points.
We show that by using the data structure we are able to recover all the survived partition points.
Then for all the survived partition points in the ith level, we then uniformly choose a survivor as the output.
A potential problem is that if in the ith level, the number of partition points is small, then it is highly possible that none of the partition j where s(cid:48) 11 Algorithm 1 A Meta-Algorithm for Point Sampling From a Dynamic Data Stream procedure PointSampler(P ) Let O = {1, 2, 4, 8,··· , 2(cid:100)d log(d∆)(cid:101) Choose randomly shifted |L| layers grids Create |O| independent KSet instances (with limited memory budget) for each layer l ∈ L For each o, l, create a set of hash functions Hl,o, each h ∈ Hl,o is a function maps [∆]d → {0, 1} for each update of a point p ∈ ∆d in the data-stream do },L = {−1, 0, 1,··· ,(cid:100)log ∆(cid:101)}.
(cid:46) reads points set P in the data-stream Create |Hl,o| copies of p as the form (p, i) for each hi ∈ Hl,o for (o, l) ∈ O × L and hi ∈ Ho,l do end for Update (o, l)-th sketch: if hi(p) = 1 then KSeto,l.update(p, i) end for Choose the smallest o∗ ∈ O such that all the {KSeto∗,l}l∈L succeed.
Output the sampled point sets and grid cells given by {KSeto∗,l}l∈L end procedure point can be survived.
But this will not introduce a problem since if the number of partition points in the ith level is small, the probability that level i is chosen in the ﬁrst stage is small.
More (cid:17) (cid:16)(cid:80) precisely, suppose we need m i.i.d. samples in total, then with high probability the number of times that the ﬁrst sampling stage samples level i is about m · (# of partition points in the ith level) · s(cid:48) j(# of partition points in the jth level) · s(cid:48) (cid:17) (cid:16)(cid:80) .
Suppose for each level i, we prepared 1.1m i/ many uniform sampling oracles, we just need to guarantee that the number of success oracle is at least m·(# of partition points in the ith level)·s(cid:48) j(# of partition points in the jth level) · s(cid:48) then it is guaranteed to have suﬃcient many samples.
In our work, we show that, for the uniform sampling oracle in level i, we just need the drop probability to be poly(d, −1, log(∆), log(1/δ))· 1 it is enough to achieve our goal.
Thus, we have enough success uniform samplers for each level.
i/ kTi 1.4.3 Max-Cut and Average-Distance We use our coreset data structure to obtain solutions for max-cut and average distance over a dynamic dataset.
The basic idea is to use the 1-means coreset as a proxy for estimating pairwise distance.
We reduce the Max-Cut instance and Average-Distance instance to a instance of estimating the distance of a point to a subset of the original point set.
Hence 1-means coreset is suﬃcient for this case.
1.5 Meta Algorithms and a Roadmap All our algorithms share a similar meta-structure to sample points from a dynamic data stream.
The meta-algorithm is presented in Algorithm 1.
This meta algorithm is an oblivious linear sketch over the input data set (i.e., the algorithm does not need to know the actual data set).
If we view the data set as a binary vector in {0, 1}∆d, then our meta-algorithm converts the vector linearly into Θ(d log ∆) binary vectors in {0, 1}∆d and Θ(d log ∆) vectors in Z∆d, i.e., it gives Θ(d log ∆) level of points samples and counts of the number of points in grid cells of each level.
These output vectors are very sparse, and hence can be stored in limited memory budget.
In the meta algorithm, we ﬁrst build random shifted grids G−1, G0, G1,··· , GL where L = log ∆ and each grid Gi reﬁnes its parent by splitting each cell into 2d cells evenly.
Then for each pair of (o, l) where o ∈ {1, 2, 4,··· , 2(cid:100)log(d·∆d+1)(cid:101) }, l ∈ {−1, 0,··· , L}, we choose a set of random hash 12 Pr[hi(p) = 1] is deﬁned according to the speciﬁc tasks (i.e., for the (cid:101)O(k2) and (cid:101)O(k) algorithms, we functions Hl,o,,δ, where each hi ∈ Hl,o,,δ is a function hi : [∆]d → {0, 1} such that ∀p ∈ [∆]d, chose diﬀerent sampling probability), and we initialize a point maintainer KSet (see Algorithm 2).
Each KSet data structure has a limited memory budget.
The KSet data structure can succeed only if the number of points and grid cells sampled (by the hash functions hi) is under the memory budget (otherwise, the information stored by KSet is treated as garbage).
When we scan the stream, for each insert/delete operation, suppose the point is p, we ﬁrst check whether hi(p) = 1.
If it is 0, we just ignore this operation.
Otherwise, for each level l ﬁnd the cell C ∈ Gl which contains p, and for each o, we use the information of C, p and the operation type (ins/del) to update the point maintainer which corresponds to (o, l).
We then build our coreset from the succeeded KSet instances with the smallest parameter o.
We provide some basic notation in Section A.
Section B states some deﬁnitions and useful tools from previous works.
In Section D, explain how to construct a coreset using randomized grid structure and hash functions over the universe [∆]d.
In Section E, we show how to maintain the coreset in a dynamic stream.
Section F presents the general result for M-estimator and also improvement compared to previous k-median result.
Section G shows how to extend our k-means to some other geometric problems.
We provide improved one-pass algorithm for k-means based on sensitivity sampling in Section H.
1.6 Concluding Remark In this paper we present two algorithm for obtaining -coreset for the k-means problem in high- dimensional dynamic geometric data streams.
Our ﬁrst algorithm is a one-pass algorithm and takes space (cid:101)O(k2−2 poly(d, log ∆)) based on Chen’s framework [Che09].
Our second algorithm is a one- pass algorithm and takes space (cid:101)O(k poly(d, log ∆, 1/)) based on sensitivity sampling.
Both of the algorithm take space polynomial in d.
To the best of our knowledge, these are the ﬁrst results for obtianing k-means coreset using space polynomial in d.
In particular, our second algorithm is nearly optimal with regard to parameter k.
Furthermore, the coresets output by our algorithms consist only positive weighted points.
One can run her favorite oﬄine algorithms to obtain the desired approximated solutions.
Both our algorithm can be extended to a much general set of cost functions, e.g., the M-estimator.
1.7 Acknowledgments The authors would like to thank Alexandr Andoni, Vladimir Braverman, Lijie Chen, Kenneth Clarkson, Harry Lang, Cameron Musco, Christopher Musco, Vasileios Nakos, Jelani Nelson, Eric Price, Aviad Rubinstein, Chen Shao, Christian Sohler, Cliﬀord Stein, Huacheng Yu, Zhengyu Wang, David P.
Woodruﬀ, and Hongyang Zhang for useful discussions.
LY would like to thank Professor Vladimir Braverman for generous support.
13 References [ACD+16] [ADHP09] [ADK+16] [AGM12a] [AGM12b] [AGM13] Ittai Abraham, Shiri Chechik, Daniel Delling, Andrew V Goldberg, and Renato F Werneck.
On dynamic approximate shortest paths for planar graphs with worst- case costs.
In Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 740–753.
Society for Industrial and Applied Mathematics, 2016.
Daniel Aloise, Amit Deshpande, Pierre Hansen, and Preyas Popat.
Np-hardness of euclidean sum-of-squares clustering.
Machine learning, 75(2):245–248, 2009.
Ittai Abraham, David Durfee, Ioannis Koutis, Sebastian Krinninger, and Richard Peng.
On fully dynamic graph sparsiﬁers.
In 2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS), pages 335–344.
IEEE, 2016.
Kook Jin Ahn, Sudipto Guha, and Andrew McGregor.
Analyzing graph structure via linear measurements.
In Proceedings of the twenty-third annual ACM-SIAM sym- posium on Discrete Algorithms, pages 459–467.
Society for Industrial and Applied Mathematics, 2012.
Kook Jin Ahn, Sudipto Guha, and Andrew McGregor.
Graph sketches: sparsiﬁcation, spanners, and subgraphs.
In Proceedings of the 31st ACM SIGMOD-SIGACT-SIGAI symposium on Principles of Database Systems, pages 5–14.
ACM, 2012.
Kook Jin Ahn, Sudipto Guha, and Andrew McGregor.
Spectral sparsiﬁcation in dynamic graph streams.
In Approximation, Randomization, and Combinatorial Op- timization.
Algorithms and Techniques, pages 1–10.
Springer, 2013.
[AHPV04] Pankaj K Agarwal, Sariel Har-Peled, and Kasturi R Varadarajan.
Approximating extent measures of points.
Journal of the ACM (JACM), 51(4):606–635, 2004.
[AMR+12] Marcel R Ackermann, Marcus Märtens, Christoph Raupach, Kamil Swierkot, Chris- tiane Lammersen, and Christian Sohler.
Streamkm++: A clustering algorithm for data streams.
Journal of Experimental Algorithmics (JEA), 17:2–4, 2012.
[Bas08] [BCMN14] [BDMO03] [BFL16] [BFL+17] Surender Baswana.
Streaming algorithm for graph spanners-single pass and constant processing time per edge.
Information Processing Letters, 106(3):110–114, 2008.
Sayan Bhattacharya, Parinya Chalermsook, Kurt Mehlhorn, and Adrian Neumann.
New approximability results for the robust k-median problem.
In Scandinavian Work- shop on Algorithm Theory, pages 50–61.
Springer, 2014.
Brain Babcock, Mayur Datar, Rajeev Motwani, and Liadan O’Callaghan.
Maintain- ing variance and k-medians over data stream windows.
In Proceedings of the twenty- second ACM SIGMOD-SIGACT-SIGART symposium on Principles of database sys- tems, pages 234–243.
ACM, 2003.
Vladimir Braverman, Dan Feldman, and Harry Lang.
New frameworks for oﬄine and streaming coreset constructions.
arXiv preprint arXiv:1612.00889, 2016.
Vladimir Braverman, Gereon Frahling, Harry Lang, Christian Sohler, and Lin F Yang.
Clustering high dimensional dynamic data streams.
In ICML.
https://arxiv.
org/pdf/1706.03887, 2017.
14 [BGS15] [BHNT15] [BHPI02] [BIP+16] [BKS12] [BR94] [BS80] [BS16] [BWZ16] [BYJK+02] [CAKM16] Surender Baswana, Manoj Gupta, and Sandeep Sen.
Fully dynamic maximal match- ing in o(log n) update time.
SIAM Journal on Computing, 44(1):88–113, 2015.
Sayan Bhattacharya, Monika Henzinger, Danupon Nanongkai, and Charalampos Tsourakakis.
Space-and time-eﬃcient algorithm for maintaining dense subgraphs on one-pass dynamic streams.
In Proceedings of the forty-seventh annual ACM sym- posium on Theory of computing, pages 173–182.
ACM, 2015.
Mihai B¯adoiu, Sariel Har-Peled, and Piotr Indyk.
Approximate clustering via core- sets.
In Proceedings of the thiry-fourth annual ACM symposium on Theory of com- puting, pages 250–257.
ACM, 2002.
Arturs Backurs, Piotr Indyk, Eric Price, Ilya Razenshteyn, and David P Woodruﬀ.
Nearly-optimal bounds for sparse recovery in generic norms, with applications to k-median sketching.
In Proceedings of the Twenty-Seventh Annual ACM-SIAM Sym- posium on Discrete Algorithms, pages 318–337.
SIAM, 2016.
Surender Baswana, Sumeet Khurana, and Soumojit Sarkar.
Fully dynamic ran- domized algorithms for graph spanners.
ACM Transactions on Algorithms (TALG), 8(4):35, 2012.
Mihir Bellare and John Rompel.
Randomness-eﬃcient oblivious sampling.
In Foun- dations of Computer Science, 1994 Proceedings., 35th Annual Symposium on, pages 276–287.
IEEE, 1994.
Jon Louis Bentley and James B Saxe.
Decomposable searching problems i.
static-to- dynamic transformation.
Journal of Algorithms, 1(4):301–358, 1980.
Aaron Bernstein and Cliﬀ Stein.
Faster fully dynamic matchings with small approxi- mation ratios.
In Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms, pages 692–711.
Society for Industrial and Applied Mathe- matics, 2016.
Christos Boutsidis, David P Woodruﬀ, and Peilin Zhong.
Optimal principal compo- nent analysis in distributed and streaming models.
In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing (STOC), pages 236–249.
ACM, https://arxiv.org/pdf/1504.06729, 2016.
Ziv Bar-Yossef, TS Jayram, Ravi Kumar, D Sivakumar, and Luca Trevisan.
Counting distinct elements in a data stream.
In International Workshop on Randomization and Approximation Techniques in Computer Science, pages 1–10.
Springer, 2002.
Vincent Cohen-Addad, Philip N Klein, and Claire Mathieu.
Local search yields ap- proximation schemes for k-means and k-median in euclidean and minor-free metrics.
In Foundations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pages 353–364.
IEEE, 2016.
[CCGG98] Moses Charikar, Chandra Chekuri, Ashish Goel, and Sudipto Guha.
Rounding via trees: deterministic approximation algorithms for group steiner trees and k-median.
In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages 114–123.
ACM, 1998.
15 [CEM+15] [Che09] [CM02] [CMS13] [COP03] [CW15a] [CW15b] [DFK+04] Michael B Cohen, Sam Elder, Cameron Musco, Christopher Musco, and Madalina Persu.
Dimensionality reduction for k-means clustering and low rank approxima- tion.
In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing (STOC), pages 163–172.
ACM, https://arxiv.org/pdf/1410.6801, 2015.
Ke Chen.
On coresets for k-median and k-means clustering in metric and euclidean spaces and their applications.
SIAM Journal on Computing, 39(3):923–947, 2009.
Dorin Comaniciu and Peter Meer.
Mean shift: A robust approach toward feature space analysis.
IEEE Transactions on pattern analysis and machine intelligence, 24(5):603–619, 2002.
Michael S Crouch, Andrew McGregor, and Daniel Stubbs.
Dynamic graphs in the sliding-window model.
In European Symposium on Algorithms, pages 337–348.
Springer, 2013.
Moses Charikar, Liadan O’Callaghan, and Rina Panigrahy.
Better streaming algo- rithms for clustering problems.
In Proceedings of the thirty-ﬁfth annual ACM sym- posium on Theory of computing, pages 30–39.
ACM, 2003.
Kenneth L Clarkson and David P Woodruﬀ.
Input sparsity and hardness for robust subspace approximation.
In 2015 IEEE 56th Annual Symposium on Foundations of Computer Science (FOCS), pages 310–329.
IEEE, https://arxiv.org/pdf/1510.
06073, 2015.
Kenneth L Clarkson and David P Woodruﬀ.
Sketching for m-estimators: A uniﬁed approach to robust regression.
In Proceedings of the Twenty-Sixth Annual ACM- SIAM Symposium on Discrete Algorithms (SODA), pages 921–939.
SIAM, 2015.
Petros Drineas, Alan Frieze, Ravi Kannan, Santosh Vempala, and V Vinay.
Clustering large graphs via the singular value decomposition.
Machine learning, 56(1):9–33, 2004.
[DLVKKR03] W Fernandez De La Vega, Marek Karpinski, Claire Kenyon, and Yuval Rabani.
In Proceedings of the thirty-ﬁfth Approximation schemes for clustering problems.
annual ACM symposium on Theory of computing, pages 50–58.
ACM, 2003.
[FIS05] [FKM+05] [FL11] [FRS16] Gereon Frahling, Piotr Indyk, and Christian Sohler.
Sampling in dynamic data streams and applications.
In Symposium on Computational Geometry 2005, pages 142–149, 2005.
Joan Feigenbaum, Sampath Kannan, Andrew McGregor, Siddharth Suri, and Jian Zhang.
On graph problems in a semi-streaming model.
Theoretical Computer Science, 348(2-3):207–216, 2005.
Dan Feldman and Michael Langberg.
A uniﬁed framework for approximating and clustering data.
In Proceedings of the 43rd ACM Symposium on Theory of Computing, STOC 2011, San Jose, CA, USA, 6-8 June 2011, pages 569–578, 2011.
Zachary Friggstad, Mohsen Rezapour, and Mohammad R Salavatipour.
Local search yields a ptas for k-means in doubling metrics.
In Foundations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pages 365–374.
IEEE, 2016.
16 [FS05] [FS12] [Gan05] [GKK12] Gereon Frahling and Christian Sohler.
Coresets in dynamic geometric data streams.
In Proceedings of the thirty-seventh annual ACM symposium on Theory of computing (STOC), pages 209–217.
ACM, 2005.
Dan Feldman and Leonard J Schulman.
Data reduction for weighted and outlier- resistant clustering.
In Proceedings of the twenty-third annual ACM-SIAM sympo- sium on Discrete Algorithms, pages 1343–1354.
Society for Industrial and Applied Mathematics, 2012.
Sumit Ganguly.
Counting distinct items over update streams.
In International Sym- posium on Algorithms and Computation, pages 505–514.
Springer, 2005.
Ashish Goel, Michael Kapralov, and Sanjeev Khanna.
On the communication and streaming complexity of maximum bipartite matching.
In Proceedings of the twenty- third annual ACM-SIAM symposium on Discrete Algorithms, pages 468–485.
SIAM, 2012.
[GKL+17] Shalmoli Gupta, Ravi Kumar, Kefu Lu, Benjamin Moseley, and Sergei Vassilvitskii.
Local search methods for k-means with outliers.
Proceedings of the VLDB Endow- ment, 10(7):757–768, 2017.
[GKP12] Ashish Goel, Michael Kapralov, and Ian Post.
Single pass sparsiﬁcation in the stream- ing model with edge deletions.
arXiv preprint arXiv:1203.4900, 2012.
[GMMO00] Sudipto Guha, Nina Mishra, R.
Motwani, and L.
O’Callaghan.
Clustering data streams.
In FOCS, pages 359–366, 2000.
[HPK05] [HPM04] [Hub64] [IKI94] [Ind00a] [Ind00b] [Ind04] Sariel Har-Peled and Akash Kushal.
Smaller coresets for k-median and k-means clustering.
In Proceedings of the twenty-ﬁrst annual symposium on Computational geometry, pages 126–134.
ACM, 2005.
Sariel Har-Peled and Soham Mazumdar.
On coresets for k-means and k-median clustering.
In Proceedings of the thirty-sixth annual ACM symposium on Theory of computing, pages 291–300.
ACM, 2004.
Peter J.
Huber.
Robust estimation of a location parameter.
The Annals of Mathe- matical Statistics, 35(1):73–101, 1964.
Mary Inaba, Naoki Katoh, and Hiroshi Imai.
Applications of weighted voronoi dia- grams and randomization to variance-based k-clustering.
In Proceedings of the tenth annual symposium on Computational geometry, pages 332–339.
ACM, 1994.
Piotr Indyk.
High-dimensional computational geometry.
PhD thesis, stanford univer- sity, 2000.
Piotr Indyk.
Stable distributions, pseudorandom generators, embeddings and data stream computation.
In Foundations of Computer Science, 2000.
Proceedings.
41st Annual Symposium on, pages 189–197.
IEEE, 2000.
Piotr Indyk.
Algorithms for dynamic geometric problems over data streams.
In Proceedings of the thirty-sixth annual ACM symposium on Theory of computing, pages 373–380.
ACM, 2004.
17 [IP11] [Jai10] [KL11] [KLM+17] [KMN+02] [LIY+15] [Llo82] [LNNT16] [Mat00] [McG14] [MNV09] [MS01] [NJW02] Piotr Indyk and Eric Price.
K-median clustering, model-based compressive sensing, and sparse recovery for earth mover distance.
In Proceedings of the forty-third annual ACM symposium on Theory of computing, pages 627–636.
ACM, 2011.
Anil K Jain.
Data clustering: 50 years beyond k-means.
Pattern recognition letters, 31(8):651–666, 2010.
J.
Kelner and A.
Levin.
Spectral sparsiﬁcation in the semi-streaming setting.
Symposium on Theoretical Aspects of Computer Science (STACS), 2011.
In Michael Kapralov, Yin Tat Lee, CN Musco, CP Musco, and Aaron Sidford.
Sin- gle pass spectral sparsiﬁcation in dynamic streams.
SIAM Journal on Computing, 46(1):456–477, 2017.
Tapas Kanungo, David M Mount, Nathan S Netanyahu, Christine D Piatko, Ruth Silverman, and Angela Y Wu. A local search approximation algorithm for k-means clustering.
In Proceedings of the eighteenth annual symposium on Computational geometry, pages 10–18.
ACM, 2002.
Zaoxing Liu, Nikita Ivkin, Lin Yang, Mark Neyrinck, Gerard Lemson, Alexander Sza- lay, Vladimir Braverman, Tamas Budavari, Randal Burns, and Xin Wang.
Streaming algorithms for halo ﬁnders.
In e-Science (e-Science), 2015 IEEE 11th International Conference on, pages 342–351.
IEEE, 2015.
Stuart Lloyd.
Least squares quantization in pcm.
IEEE transactions on information theory, 28(2):129–137, 1982.
Kasper Green Larsen, Jelani Nelson, Huy L Nguyen, and Mikkel Thorup.
Heavy hit- ters via cluster-preserving clustering.
In Foundations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pages 61–70.
IEEE, https://arxiv.org/ pdf/1604.01357, 2016.
Jiri Matoušek.
On approximate geometric k-clustering.
Discrete & Computational Geometry, 24(1):61–84, 2000.
Andrew McGregor.
Graph stream algorithms: a survey.
ACM SIGMOD Record, 43(1):9–20, 2014.
Meena Mahajan, Prajakta Nimbhorkar, and Kasturi Varadarajan.
The planar k- means problem is np-hard.
In International Workshop on Algorithms and Computa- tion, pages 274–285.
Springer, 2009.
Marina Meila and Jianbo Shi.
A random walks view of spectral segmentation.
2001.
., Andrew Y Ng, Michael I Jordan, and Yair Weiss.
On spectral clustering: Analysis and an algorithm.
In Advances in neural information processing systems, pages 849–856, 2002.
[NYWR09] Sahand Negahban, Bin Yu, Martin J Wainwright, and Pradeep K Ravikumar.
A uniﬁed framework for high-dimensional analysis of m-estimators with decomposable regularizers.
In Advances in Neural Information Processing Systems, pages 1348– 1356, 2009.
18 [Scu10] [SM00] [SWZ17a] [SWZ17b] [Vat09] David Sculley.
Web-scale k-means clustering.
In Proceedings of the 19th international conference on World wide web (WWW), pages 1177–1178.
ACM, 2010.
Jianbo Shi and Jitendra Malik.
Normalized cuts and image segmentation.
Transactions on pattern analysis and machine intelligence, 22(8):888–905, 2000.
IEEE Zhao Song, David P Woodruﬀ, and Peilin Zhong.
Low rank approximation with entrywise (cid:96)1-norm error.
In Proceedings of the 49th Annual Symposium on the Theory of Computing (STOC).
ACM, https://arxiv.org/pdf/1611.00898, 2017.
Zhao Song, David P Woodruﬀ, and Peilin Zhong.
Relative error tensor low rank approximation.
arXiv preprint arXiv:1704.08246, 2017.
Andrea Vattani.
The hardness of k-means clustering in the plane.
In Manuscript, volume 617.
http://cseweb.ucsd.edu/avattani/papers/kmeans_hardness.pdf, 2009.
[VL07] Ulrike Von Luxburg.
A tutorial on spectral clustering.
Statistics and computing, 17(4):395–416, 2007.
19 A Notation For an n ∈ N+, let [n] denote the set {1, 2,··· , n}.
Given integers m ≤ n, we denote [m, n] = For any function f, we deﬁne (cid:101)O(f ) to be f · logO(1)(f ).
In addition to O(·) notation, for two {m, m + 1, m + 2, .
.
.
, n} as the integer interval.
functions f, g, we use the shorthand f (cid:46) g (resp.
(cid:38)) to indicate that f ≤ Cg (resp.
≥) for an absolute constant C.
We use f (cid:104) g to mean cf ≤ g ≤ Cf for constants c, C.
B Preliminaries B.1 Deﬁnitions of Dynamic Streaming Model for k-Clustering Problems In this section, we give the deﬁnition of the input form and the computational model.
Deﬁnition B.1 (Dynamic streaming model for k-clustering).
Let P ⊂ [∆]d initially be an empty set.
In the dynamic streaming model, there is a stream of update operations such that the qth operation has the form (opq, pq) where opq ∈ {ins, del}, pq ∈ [∆]d which means that the set P should add a point pq or should remove the point pq.
An algorithm is allowed 1-pass/2-pass over the stream.
At the end of the stream, the algorithm stores some information regarding P.
The space complexity of an algorithm in this model is deﬁned as the total number of bits required to describe the information the algorithm stores during the stream.
B.2 Deﬁnitions of k-means Clustering For representation simplicity, we restrict the dataset to be from the discrete space [∆]d for some large integer ∆ and d.
The extension of our results to Rd is straightforward.
We will discuss this issue more in the concluding remarks.
We use dist(·,·) as the Euclidean distance in space [∆]d, i.e., for any p, q ∈ [∆]d, dist(p, q) = (cid:107)p − q(cid:107)2 , where (cid:107)·(cid:107) is the (cid:96)2 norm.
We further extend the distance deﬁnition to set and point, and set to set.
Formally, let sets P, Q ⊂ [∆]d and point p ∈ [∆]d, then dist(p, q) and dist(P, Q) = min dist(p, q).
dist(p, Q) = dist(Q, p) = min q∈Q p∈P,q∈Q We also denote diam(Q) diameter of Q, i.e., the largest distance of any pair of points in Q.
Deﬁnition B.2 (k-means clustering).
Given an input point set Q ⊂ [∆]d, the k-means clustering problem is to ﬁnd a set of k points Z ⊂ [∆]d, such that the following objective is minimized.
(cid:88) q∈Q cost(Q, Z) = dist2(q, Z).
Each point of Z is also called a center.
Note that a set Z deﬁnes a partition of the point set Q by assigning each point q ∈ Q to the closest center in Z (ties are broken arbitrarily).
We use OPT to denote the minimum cost of the k-means problem.
Deﬁnition B.3 (Coreset for k-means).
Given an input point set Q ⊂ [∆]d.
An -coreset S of Q is a multiset, usually of smaller size, and summarizes the important structures of Q.
The so- lution of the optimization problem on S is an approximate solution on Q.
Formally, let S = 20 {(s1, w1), (s2, w2), .
.
.
,} be an -coreset for Q, where each si ∈ [∆]d and wi ∈ R is the weight of si.
Then S satisﬁes (cid:12)(cid:12) cost(S, Z) − cost(Q, Z) (cid:12)(cid:12) ≤  cost(Q, Z), ∀Z ⊂ [∆]d,|Z| = k : where cost(S, Z) := wi dist2(si, Z).
(cid:88) si∈S (cid:88) q∈Q Deﬁnition B.4 (Coreset for k-means in dynamic stream).
Given a point set P ⊂ [∆]d described by a dynamic stream, an error parameter  ∈ (0, 0.5), and an fail probability parameter δ ∈ (0, 1), the goal is to design an algorithm in the dynamic streaming model (Deﬁnition B.1) which can with probability at least 1 − δ output an -coreset for k-means (Deﬁnition B.3) with minimal space.
B.3 Deﬁnitions of M-estimator Clustering Our coreset framework can also be extended to arbitrary clustering of using M-estimators.
Deﬁnition B.5 (M-estimator Clustering).
We deﬁne function M : R → R to be a M-estimator.
We deﬁne costM (Q, Z) = min z∈Z M (dist(q, z)).
The goal of a M-estimator clustering to solve the following optimization problem.
min Z⊂[∆]d:|Z|≤k costM (Q, Z).
Deﬁnition B.6 (Coreset for M-estimator).
Given an input point set Q ⊂ [∆]d.
An -coreset S of Q is a multiset, usually of smaller size, and summarizes the important structures of Q.
The solution of the optimization problem on S is an approximate solution on Q.
Formally, let S = {(s1, w1), (s2, w2), .
.
.
,} be an -coreset for Q, where each si ∈ [∆]d and wi ∈ R is the weight of si.
Then S satisﬁes that ∀Z ⊂ [∆]d with |Z| = k, holds, where costM (S, Z) :=(cid:80) (cid:12)(cid:12) costM (S, Z) − costM (Q, Z) (cid:12)(cid:12) ≤  costM (Q, Z), si∈S wi minz∈Z M (dist(si, z)).
Deﬁnition B.7 (Coreset for M-estimator clustering in dynamic stream).
Given a point set P ⊂ [∆]d described by a dynamic stream, an error parameter  ∈ (0, 0.5), and an failure probability parameter δ ∈ (0, 1), the goal is to design an algorithm in the dynamic streaming model (Deﬁnition B.1) which can with probability at least 1 − δ output an -coreset for k-center M-estimator clusetering (Deﬁnition B.6) with minimal space.
dom variables.
Suppose that |Xi| ≤ b almost surely, for all i ∈ [n].
Let σ2 denote (cid:80)n B.4 Basic Probability Tools Lemma B.8 (Bernstein inequality [HPM04]).
Let X1, X2,··· , Xn be independent zero-mean ran- j ].
j=1 E[X 2 Then for all positive t, (cid:34) n(cid:88) i=1 Pr Xi > t (cid:35) ≤ exp(cid:0) 21 (cid:1).
t2 2σ2 + 2bt/3 B.5 Tools from Previous Work Theorem B.9 ([LNNT16]).
Given parameters k ≥ 1,  ∈ (0, 1/2), δ ∈ (0, 1/2).
There is a random- ized (one-pass) algorithm that uses O((k + 1/2) log n/δ · log m) bits of space, requires O(log n) time per update, needs O(k + 1/2) poly(log n) decoding time.
For all i ∈ [n], let fi ∈ [− poly(n), poly(n)] denote the frequency of i at the end of the data stream.
Without loss of generality, we assume that f1 ≥ f2 ≥ ··· ≥ fn.
The algorithm is able to output a set H with size O(k + 1/2) such that, with probability 1 − δ, Property (II) : for all i ∈ [n], if f 2 Property (I) : for all (i,(cid:98)fi) ∈ H, f 2 Property (III) : for all (i,(cid:98)fi) ∈ H, |(cid:98)fi − fi| ≤ ((cid:80)n j , then (i,(cid:98)fi) ∈ H ; j /k − 2(cid:80)n j /k + 2(cid:80)n j ; j=k+1 f 2 j=k+1 f 2 j=k+1 f 2 j ) 2 .
i ≥ i ≥ (cid:80)n (cid:80)n j=1 f 2 j=1 f 2 C Why Do Previous Techniques Fail?
[FIS05] is one of the early works using sampling procedure to solve dynamic streaming [FIS05] geometric problem.
They show that it is possible to use point samples over a dynamic point set as a subroutine to solve several geometric problem, e.g. Euclidean Minimum Spanning Tree.
However, they only show how to implement the uniform sampling by using counting distinct elements and subsampling procedure as subroutines.
In this work, we require assigning points diﬀerent “importance”.
The bottom level sampling scheme of ours is similar to theirs, but ours requires a much complicated framework to implement the importance sampling over the point sets.
a set of centers, and P be the point sets then cost(Z, P ) =(cid:82) ∞ [Ind04] This paper uses a critical observation to estimate the cost of k-median, that is: let Z be a summation with logarithmic levels, i.e.,(cid:82) ∞ |P − B(Z, r)|dr =(cid:80)∞ |P − B(Z, r)|dr, where B(Z, r) is union of neighborhoods (of radius r) of every point in Z.
Then this integration is approximated by i=0 |P − B(Z, ri+1)|(ri+1 − ri), where ri = O((1 + )i).
The critical part is to estimate |P − B(Z, ri−1)|.
This paper constructed a counting data structure based on grids with side length O(ri).
Then every input point is snapped to a grid point.
To obtain suﬃciently accurate counts for each |P − B(Z, ri−1)|, the data structure needs to query |Z|/d many grid points per Z.
Such a data structure is implemented using pair-wise independent hash functions, and uses memory O(|Z|/O(d)).
Notice that this paper gives only an approximation of the cost.
It is not a coreset.
Therefore, to obtain the k-median solution, an exhaustive search is used.
Furthermore, their technique fails to extend to k-means, which lacks the integration formula of the cost function.
Why Does [FS05] require exponential dependence in d, (1/)d?
The grid structure of our paper is inspired by [FS05].
In [FS05], they ﬁrst build a deterministic quadtree-like structure: Consider a big cube containing the entire data set.
This cube is treated as the root cell of the tree.
Going down a level, they partition the parent cell into 2d subcells.
Then each leaf of the tree contains at most one single point of the dataset.
Notice that the cell side length decrease geometrically as the level increase.
They mark a cell as “heavy” if the cell containing enough points that moving all points in the cell to the center of the cell incurs too much error in the cost of an optimal k-means/median solution.
Since the side length (or diameter) of cells decreases as level increases, the number of points required to have this eﬀect becomes larger.
Eventually, all cells are non-heavy after some level.
As such we also have a tree of heavy cells.
For each heavy cell, the coreset is constructed by assigning each point in the non-heavy children to its center.
It turns out if we want an epsilon-approximated coreset, the threshold of the non-heavy cell is exponential in d, 22 i.e., each non-heavy cell in level i cannot contain more than (cid:101)O(O(d) · OPT /2i) points, where OPT is the optimal cost.
This small threshold gives (cid:101)O(1/O(d)) many heavy cells.
Why do the previous insertion-only streaming coreset construction algorithms fail for dynamic stream model?
Many of the previous insertion only streaming coreset construction algorithms (e.g., [FS12]) heavily depend on a merge-reduce technique, i.e. read some points in the stream, construct a coreset, then read another part, construct a new coreset, and merge the two coresets.
They repeat this procedure until the stream ends.
This technique works well in insertion only streaming model.
But once some points get deleted, the framework fails immediately.
Though [BFL16] shows a new framework other than merge-reduce, their algorithm relies on a non- deleting structure of the data stream as well.
Why Do [BFL+17] fail to extend to k-means?
Though some k-median coreset construction techniques can be easily extended to k-means coreset construction (see e.g. [BFL16, FS12]), their construction can only be implemented in insertion only streaming model.
p∈P (dist(ci p∈P (dist(ci p, z), where each ci , z)− dist(cL−2 p, z) − dist(ci−1 [BFL+17] showed a k-median coreset construction in dynamic streaming model.
But their construction cannot be extended to k-means coreset construction directly.
Their k-median algorithm heavily relies on writing the cost of each point as a telescope sum.
For example, we consider the of the cell in the i-th level containing p.
Therefore, the total 1-median cost(cid:80) 1-median problem.
let z be a candidate center point and p ∈ P be a point, then dist(p, z) = dist(p, z)− dist(cL−1 , z) + dist(cL−1 set P on z can be split into L pieces, i.e.,(cid:80) p is the center , z) +···− dist(c0 p∈P dist(p, z) of point p, z)−dist(ci−1 , z)) is(cid:80) then the estimator to(cid:80) , z)) for each i ∈ [L].
[BFL+17] estimates the cost of each L pieces by sampling points, i.e., let Si be the samples in the i-th level, p, z) − dist(ci−1 p where p∈Si(dist(ci p is the probability that the point p is sampled in the samples Si. A crucial observation is that we ζ i have | dist(ci, z)−dist(ci−1, z)| ≤ ∆/2i – the cell size of level i which is independent from the location inequality to show the high concentration of the estimator(cid:80) of z.
Since this nice upper bound on | dist(ci, z)−dist(ci−1, z)|, [BFL+17] then can apply Bernstein’s only (cid:101)O(1/2) samples per level.
But this framework does not work for 1-means even though one can still write the telescope sum structure as(cid:80) p with estimator as(cid:80) , z)), and we can still setup the p, z) − dist2(ci−1 , z)| is not upper p∈Si(dist2(ci bounded by the cell size.
Instead, the magnitude is depending on the location z.
For example, it p, z) − dist2(ci−1 , z)| ≥ ∆.
If we apply the Bernstein’s inequality here, can be as large as | dist2(ci points per level, one may need (cid:101)Ω(2i) samples, which can be as large as ∆d.
Thus no space saving then since the upper bound of | dist2(ci , z)| is larger than ∆, if we still sample the is possible.
p, z)− dist2(ci−1 p.
But | dist2(ci p, z) − dist2(ci−1 p, z) − dist2(ci−1 p, z)− dist(ci−1 p(dist2(ci , z))/ζ i p∈Si(dist(ci , z))/ζ i , z))/ζ i D Coreset Construction for k-means Based on Chen’s Framework To formally introduce our coreset construction, we here explain the high level outline.
We impose a randomly shifted grid structure over the universe [∆]d.
This is done independently of dataset.
The grid structure can be stored in memory with only negligible amount of space.
We then prove there are good properties of the randomly shifted grid, for a ﬁxed point set.
Next we show how to extract a coreset using the help of the grid and hash functions over the universe.
After that we formally prove our construction.
23 center point Z, the total cost can be written as a telescope sum(cid:80) Figure 3: Telescope sum [BFL+17] fails for k-means.
In the k-median problem, for a ﬁxed set of p, Z)−dist(ci−1 , Z)).
For (cid:80) , p) which is independent each piece, | dist(ci from the choice of Z.
However, in the k-means problem, the telescope sum of the total cost is p, Z)2−dist(ci−1 , Z)2| , Z)2).
For each piece, the upper bound of | dist(ci may depend on the location of Z, and it can be at least ∆ in the worst case.
p∈P (dist(ci , Z)| is always upper bounded by dist(ci−1 p, Z)2−dist(ci−1 p∈P (dist(ci p, Z)−dist(ci−1 D.1 Deﬁnitions and Properties Without loss of generality, we assume the dataset is from [∆]d and ∆ = 2L for some positive integer L.
Otherwise we can enlarge ∆ to the closest power of 2.
The space [∆]d is partitioned by the grid structure recursively as follows.
The ﬁrst level of the grid contains a single cell, which is taken as the entire space.
For each higher level, we reﬁne our partition by splitting each cell into 2d equal square sub-cells.
In the ﬁnest level, i.e., the L-th level, each cell contains a single point.
We further randomly shift the boundary of the grids to achieve certain properties, which we will show.
Formally, our grid structure is deﬁned as follows.
Deﬁnition D.1 (Grids and cells).
Let g0 = ∆.
Let v be a vector chosen uniformly at random from [0, g0 − 1]d.
Partition the space {1, 2,··· , ∆}d into a regular Cartesian grid G0 with side-length g0 and translated so that a vertex of this grid falls on v.
Each cell C ⊂ [∆]d of this grid can be expressed as [v1 + n1g0, v1 + (n1 + 1)g0) × ··· × [vd + ndg0, vd + (nd + 1)g0) ⊂ [∆]d for some (n1, n2,··· , nd) ∈ Zd. (Note that each cell is cartesian product of batch of continuous interval) For i ≥ 1, we deﬁne the regular grid Gi as the grid with side-length gi = g0/2i aligned such that each cell of Gi−1 contains 2d cells of Gi. The ﬁnest grid is GL where L = (cid:100)log2 ∆(cid:101); the cells of this grid therefore have side-length at most 1 and thus contain at most a single input point.
Each grid forms a partition of the point-set Q.
There is a d-ary tree such that each vertex at depth i corresponds to a cell in Gi and this vertex has 2d children which are the cells of Gi+1 that it contains.
For convenience, we deﬁne G−1 as the entire dataset and it contains a single cell.
Center Cells Next, we show that the randomly shifted grid structure has very good properties.
First, we ﬁx an optimal k-set Z∗ = {z∗ k} ⊂ [∆]d as the optimal k-means solution for the (cid:88) input dataset Q, i.e., 2, .
.
.
, z∗ 1, z∗ dist2(q, Z) = OPT .
cost(Q, Z ) = min Z⊂[∆]d:|Z|=k q∈Q 24 z∆k-mediantelescopesumd(p,z)=d(p,z)−d(c2p,z)+d(c2p,z)−d(c1p,z)+d(c1p,z)−d(c0p,z)+d(c0p,z)where|d(p,z)−d(c2p,z)|≤d(p,c2p)where|d(c2p,z)−d(c1p,z)|≤d(c2p,c1p)where|d(c1p,z)−d(c0p,z)|≤d(c1p,c0p)k-meanstelescopesumd2(p,z)=d2(p,z)−d2(c2p,z)+d2(c2p,z)−d2(c1p,z)+d2(c1p,z)−d2(c0p,z)+d2(c0p,z)but|d2(p,z)−d2(c2p,z)|≥∆but|d2(c2p,z)−d2(c1p,z)|≥∆but|d2(c1p,z)−d2(c0p,z)|≥∆c0pc1pc2ppThen we call a cell C in level Gi a center cell if it is close to some centers in Z∗.
Formally dist(C, Z ) = min q∈C,z∗∈Z∗ dist2(q, z ) ≤ gi 2d (1) where gi = 2−i∆ is the side length of the cell and d is the dimension of the space.
We show that there are only a small number of center cells.
Lemma D.2. Let ξ denote the even that the number of center cells of all grids is upper bounded by 3kL/ρ.
Then event ξ holds with probability at least 1 − ρ.
Proof.
Fix an i and consider a grid Gi. For each optimal center z∗ j , denote Xj,α the indicator random variable for the event that the distance to the boundary in dimension α of grid Gi is at most gi/(2d).
Since in each dimension, if the center is close to a boundary, it contributes a factor at most 2 to the total number of center cells.
It follows that the number of cells that have distance at most gi/(2d) to z∗ j is at most (cid:80)d N = 2 α=1 Xj,α.
We denote Yj,α to be 2Xj,α, then E[N ] = E (cid:35) Yj,α (cid:34) d(cid:89) α=1 d(cid:89) α=1 E[Yj,α].
Using Pr[Xj,α = 1] ≤ (2gi/(2d))/gi = 1/d, we obtain E[Yj,α] ≤ E[1 + Xj,α] = 1 + E[Xj,α] ≤ 1 + 1/d.
Thus E[N ] =(cid:81)d α=1 E[Yj,α] ≤ (1 + 1/d)d ≤ e.
Thus the expected number of center cells in a single grid is at most (1 + 1/d)dk ≤ ek ≤ 3k.
By linearity of expectation, the expected number of center cells in all grids is at most ekL.
By Markov’s inequality, the probability that we have more than 3kL/ρ center cells in all grids is at most ρ.
Heavy Cells The property of small number of center cells allows us to bound the number of cells that containing too many points.
Next we introduce the notion of heavy cells, in which there are a large number of input points.
Ideally, we would use OPT, the optimal cost of the k-means solution to deﬁne our heavy cells.
However, OPT is not known to us.
We use an arbitrary guess o to OPT to deﬁne the heavy cells.
Deﬁnition D.3 (Heavy Cell).
Let o > 0 be ﬁxed number.
For each i ∈ {0, 1,··· , L − 1}, a cell in Gi is called o-heavy if it contains at least Ti(o) = d2 3k points.
The cell in G−1 is always heavy, g2 and no cell in GL is called heavy.
Here we call Ti(·) the threshold function in level i.
o is omitted if it is clear from the context.
i · ρo Since the side length gi is decreasing as i increases, the thresholding function Ti(o) is increasing as i increases.
In particular, Ti+1(o)/Ti(o) = 4 for any o > 0 and i ∈ {0, 1,··· , L − 1}.
Therefore, we have the following lemma.
Lemma D.4. For 0 ≤ i ≤ L, if a cell C in Gi is heavy, then its parent cell C(cid:48) in Gi−1 must be heavy.
If a cell C(cid:48) in Gi−1 is not heavy, then any of its children cells in Gi cannot be heavy.
25 Figure 4: Example of o-Partition The next lemma bounds the number of heavy cells.
Lemma D.5. If the number of center cells is at most 3kL/ρ, then the number of o-heavy cells is at most 16kL OPT /(ρo).
Proof.
For a non-center heavy cell C, the contribution of the input points in C to the optimal k-means cost is at least g2 4d2 Ti(o) ≥ oρ/(12k), since each of them is of distance at least gi/(2d) to an optimal center in Z∗.
Thus there are at most 12k OPT /(oρ) many non-center heavy cells in a grid.
In total there are at most 12kL OPT /(oρ) + 1 many non-center heavy cells, where the +1 term comes from G−1.
Since there are at most 3kL/ρ many center cells, the total number of heavy cells is at most 1 + (3kL/ρ)(1 + 4 OPT /o) ≤ 16kL OPT /(ρo).
The heavy cells allows us to construct the coreset.
The number of heavy cells essentially gives us a bound of how many samples we need obtain from the original dataset.
D.2 Recursive Partition Approach Recall that in our coreset construction, we ﬁrst partition the input datasets into L + 1 disjoint subsets.
And then we sample our points separately from these partitions.
To prove Theorem D.14, we ﬁrst reﬁne the partition conceptually as follows.
Deﬁnition D.6 (o-Partition).
Suppose parameter o > 0.
Let Q ⊂ [∆]d be the input set.
For each level 0 ≤ i ≤ L we deﬁne a set of sets P i as follows.
Initialize P i as an empty set.
For each heavy cell C in Gi−1, we group the non-heavy children cells of C in the following manner.
First note 26 that Ti(o) = 4Ti−1(o), so by non-heaviness each child cell contains at most 4Ti−1(o) points of Q.
If all the non-heavy children together contain less than Ti(o) points of Q, we take all the points of Q containing in them as a single set, and add it to P i.
Otherwise, we make groups such that each group of these non-heavy children cells contains at least Ti(o) points and at most 3Ti(o) points of Q.
We put the points of Q in each of these groups as a set and add it to P i.
Our full partition of Q is deﬁned as P = P 0 ∪P 1 ∪ .
.
.
∪ P L.
For each i ∈ {0, 1,··· , L}, we write set Pi in the following way, Pi = {Pi,1,Pi,2,··· ,Pi,|Pi|}.
For each j ∈ [|Pi|], if set Pi,j contains at least Ti(o) points, then say set Pi,j is a heavy set.
We use |Pi,j| to denote the number of points of Q that contained in partition Pi,j.
Remark D.7. We remark that the algorithm and coreset construction does not need to know and ﬁnd such a partition.
The partition we given here is only for analysis purposes.
Also note that for each i ∈ {0, 1,··· , L}, the sets in P i give a partition for Qi, i.e., the set of points falling in non-heavy children of a heavy cell.
Claim D.8 (Upper bound |Pi,j|).
For each i ∈ {0, 1,··· , L}, we write set Pi in the following way, Pi = {Pi,1,Pi,2,··· ,Pi,|Pi|}.
Then for each j ∈ [|Pi|], we have |Pi,j| ≤ 3Ti(o).
Proof.
It directly follows by Deﬁnition D.6. Fact D.9. For a partition P of points set Q, we have L(cid:88) |Pi|(cid:88) L(cid:88) |P| = |Pi|, and |Pi,j| = |Q| i=0 i=0 j=1 Deﬁnition D.10.
We say a partition P(d, g) is a α, β partition if |P| ≤ αk and |Pi,j|dg2 i−1 ≤ β OPT .
Proof.
Without loss of generality, we assume L = log ∆ is suﬃciently large, i.e., L ≥ 9.
We ﬁrst bound the cardinality of P.
For the partition sets, we split them into three groups.
The ﬁrst group are these sets containing a center cell.
Since there are at most 3kL/ρ center cells, thus the number of such sets is bounded by 3kL/ρ.
The second group are these non-heavy sets, i.e., these sets containing less than Ti points.
The number of these sets are bounded by the heavy cells since each heavy cell can produce at most one of such a set.
Lemma D.5 shows that the number of heavy cells is at most 16kL OPT /(ρo).
27 |P| ≤ αk and |Pi,j|dg2 i−1 ≤ β OPT Lemma D.11.
Given parameters o, α, β with 0 < o ≤ OPT, α = 36L OPT /(ρo), β = 120d3L, and conditioned on the number of center cells is at most 3kL/ρ.
Then the o-partition P satisﬁes L(cid:88) |Pi|(cid:88) i=0 j=1 L(cid:88) |Pi|(cid:88) i=0 j=1 The last group are the remaining sets.
Each of these sets contain at least Ti−1 many points by construction.
For any point in such a set, its distance to any center is at least gi/(2d).
Thus the contribution to OPT is at least Tig2 i /(4d2).
Thus the number of such sets per level is bounded by 12k OPT /(ρo).
In total, we can upper bound |P| in the following sense, |P| ≤ 16kL OPT /(ρo) + 12kL OPT /(ρo) ≤ 30kL OPT /(ρo).
(2) Next, we show L(cid:88) |Pi|(cid:88) i=0 j=1 |Pi,j|(gi−1√d)2 ≤ 3Ti(o)(gi−1√d)2 L(cid:88) L(cid:88) i=0 |Pi|(cid:88) |Pi|(cid:88) j=1 4d3oρ/k i=0 j=1 ≤ |P| · 4d3oρ/k ≤ 120d3L OPT .
where the ﬁrst step follows by |Pi,j| ≤ 3Ti(o) (Claim D.8), the second step follows by Ti(o) ≤ 4d3o 3k , the last step follows by Eq. (2).
D.3 Bounding the Close Parts Given a set Z of k points, we partition each P i for 0 ≤ i ≤ L into r = O(log(∆√d)) parts based on their distance to Z.
Formally, let i (Z) = {P ∈ P i : dist(P, Z) ≤ 2√dgi−1}.
P 0 and for 0 < j ≤ r (3) (4) i (Z) = {P ∈ P i : 2j√dgi−1 < dist(P, Z) ≤ 2j+1√dgi−1}.
P j Since the distance is upper bounded by ∆√d, thus r = (cid:100)log(∆√d)(cid:101) is suﬃcient to partition P i.
It is also easy to see that for any i ∈ [0, L] and j ≥ 1 we have max p∈P,P⊂P j i (Z) dist(p, Z) ≤ (2j+1 + 1)√dgi−1.
Based on this deﬁnition, we ﬁrst bound the error in P 0 Lemma D.12 (Cost of close parts).
Suppose we are given an input point set Q, its (α, β)-partition P 0 ∪P 1 ∪ .
.
.
∪ P L and an arbitrary ﬁxed set of k points Z ⊂ [∆]d.
For each 0 ≤ i ≤ L, let P 0 i (Z) be deﬁned in (3) as the partition sets that is in distance √dgi−1 to Z.
Let P 0 i denote all the points i with sampling probability ζ in P 0 and weight 1/ζ.
If i be the independent sample of points in P 0 i (Z) of the coreset in the next lemma.
i (Z).
Let multiset S0 (cid:20) 256 ζ ≥ min 2 · β2 kL · Ti(o) · ln (cid:18) 2 (cid:19) (cid:21) , 1 for some , δ ∈ (0, 1/2) and o ∈ (0, OPT], then, with probability at least 1 − δ, i |, kL · Ti(o)) · dg2 2β · max(|P 0 i , Z) − cost(S0 i , Z) i−1.
(cid:12)(cid:12) cost(P 0 (cid:12)(cid:12) ≤ 28 Proof.
First we recall that cost(P 0 i , Z) = (cid:88) p∈P 0 d(p, Z)2.
Let Ip be the indicator that a point p ∈ P 0 cost(S0 i , Z) = is sampled.
Then Ip d(p, Z)2.
(cid:88) p∈Pj The diﬀerence can be written as, | cost(P 0 i , Z) − cost(S0 i , Z)| ≤ (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) (cid:88) p∈P 0 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .
(1 − Ip )d(p, Z)2 We have that and for each p ∈ P 0 i , (cid:88) p∈P 0  = 0 (1 − Ip )d(p, Z)2 (cid:20) (1 − Ip )2d(p, Z)4 (cid:21) (1 − ζ)d(p, Z)4 16d2g4 i−1 Note that, |(1− Ip (cid:88) compute σ2 and b as follows, ζ )d(p, Z)2| ≤ d(p, Z)2/ζ.
In order to apply Bernstein’s inequality, Lemma B.8, we (cid:20) d(p, Z)2 16d2g4 (cid:21) i−1 (1 − Ip )2d(p, Z)4 ≤ |P 0 i | and b = max p∈P 0 4dg2 i−1 σ2 = p∈P 0 If |P 0 i | ≥ kLTi(o), we choose t =  i−1 and calculate By Bernstein’s inequality, we have that t2 i |dg2 i |2 · ζ 256β2 2β · |P 0 4σ2 = |P 0 (cid:18) (cid:19) d(p, Z)2 Ip 1 − (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) (cid:88) p∈P 0 Pr (cid:19) and t2 4bt/3 3|P 0 i | · ζ 32β (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) > t  ≤ 2 exp (cid:18) (cid:18) − min ≤ exp ≤ δ.
29 2σ2 + 2bt/3 t2 (cid:20) t2 t2 4bt/3 4σ2 , (cid:21)(cid:19) If |P 0 i | ≤ kLTi(o), we choose t =  kLTi(o) |P 0 i | t2 4σ2 = Thus Pr We thus conclude the proof.
2β · kLTi(o)dg2 kLTi(o)2 · ζ (cid:19) (cid:18) (cid:88) 256β2 Ip 1 − (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) p∈P 0 i−1 and calculate t2 and 3kLTi(o) · ζ 32β 4bt/3 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) > t  ≤ δ.
d(p, Z)2 D.4 Bounding the Far Parts The goal of this section is to prove Lemma D.13.
We show that the error of the coreset S over each P j i for i ∈ [0, L], j ∈ [1, r] is still bound.
Lemma D.13 (Cost of far parts).
Suppose we are given an input point set Q, its (α, β)-partition P 0 ∪P 1 ∪ .
.
.
∪ P L and an arbitrary ﬁxed set of k points Z ⊂ [∆]d.
For each 0 ≤ i ≤ L and j ∈ [r] where r = (cid:100)log(∆√d)(cid:101), let P j i (Z) be deﬁned in (4) as the partition sets that is in dis- tance (2j−1√dgi−1, 2j√dgi−1] to Z.
Let P j i be the independent sample of points in P j i denote all the points in P j i with sampling probability ζ and weight 1/ζ.
If i (Z).
Let multiset Sj ζ ≥ min (cid:19) (cid:21) , 1 2 · β2 Ti(o) · ln (cid:20) 256 (cid:18) 2 (cid:12)(cid:12) ≤  · cost(P j for some , δ ∈ (0, 1/2) and o ∈ (0, OPT], then, with probability at least 1 − δ, 2 β2 cost(Q, Z).
i , Z) − cost(Sj (cid:12)(cid:12) cost(P j i , Z) + i , Z) Proof.
The proof is similar to that of Lemma D.12 except that we need to consider the partition sets containing too few number of points.
We additionally partition P j i,2(Z) and denote P j i,1(Z)∪P j i (Z) = P j i,2(Z) such that i,1 and P j i,2 as the points in P j ∀P ∈ P j i,1 and Sj i,1(Z),|P| ≥ i,1(Z) and P j Ti(o) and ∀P β2 (cid:88) cost(P j i,r, Z) = d(p, Z)2.
(cid:48) ∈ P j i,2(Z),|P (cid:48) | < Ti(o) β2 .
Similarly we denote Sj that i,2 as the sampled points in each set.
For each r ∈ {0, 1}, we recall Let Ip be the indicator that a point p ∈ P j cost(Sj i,r, Z) = i,r is sampled.
Then Ip d(p, Z)2.
p∈P j i,b (cid:88) p∈Pi,r 30 The diﬀerence can be written as, | cost(P j i,r, Z) − cost(Sj i,b, Z)| ≤ We have that  (cid:88) p∈P j i,r (1 − Ip )d(p, Z)2 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) (cid:88) p∈P j i,r (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .
(1 − Ip )d(p, Z)2  = 0 and for each p ∈ P j i,r, (cid:20) (1 − (cid:21) Ip )2d(p, Z)4 (1 − ζ)d(p, Z)4 24jd2g4 i−1 In order to apply Bernstein’s inequality, Lemma B.8, we compute σ2 and b as follows, Note that, |(1 − Ip ζ )d(p, Z)2| ≤ d(p, Z)2/ζ.
(cid:20) (cid:88) σ2 = and p∈P j i,r (cid:21) (1 − Ip )2d(p, Z)4 ≤ |P j i,r| 24jd2g4 i−1 b = max p∈P j i,r d(p, Z)2 (2j+1 + 1)2dg2 i−1 Case 1.
r = 1.
For r = 1, we consider two cases: Case 1a.
|P j i,1| ≥ |P j By Bernstein’s inequality, we have (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  (cid:88) p∈P 0 i,1 Pr Since dist(P, Z) ≥ 2j−1dgi−1 for any P ∈ P j i,1, with probability at least 1 − δ, we have i−1 and compute, (cid:21)(cid:19) t2 4bt/3 ≤ δ.
i,1| · ζ 64β (cid:18) t2 and i,1|2 · ζ 256β2 8|P j i,1 |Ti(o), then we choose t =  i,1|22jdg2 4bt/3 ≥ |P j 4σ2 = |P j t2 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) > t  ≤ 2 exp (cid:20) t2 (cid:18) (cid:19) (cid:12)(cid:12)(cid:12) ≤  cost(P j i,1, Z) − cost(Sj − min d(p, Z)2 i,1, Z) 1 − Ip 4σ2 , i,1, Z).
(cid:12)(cid:12)(cid:12)cost(P j (cid:113) i,1| < |P j ≤ |P j |P j i,1| · |P j 8β · i,1 |Ti(o), then we choose i,1 | · Ti(o) · 22j · dg2 i−1 ≤ 8|P j i,1|22jdg2 i−1.
31 Case 1b.
| P j i,1 |Ti(o) β2 t = We compute, t2 4σ2 = |P j By Bernstein’s inequality, we have that i,1 |Ti(o) ·  · ζ 64β (cid:113) |P j i,1||P j (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) > t  t2 4bt/3 (cid:19) and (cid:18) i,1 |Ti(o)2 · ζ 256β2 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  (cid:88) (cid:18) p∈P j Pr i,1 Ip 1 − (cid:20) t2 d(p, Z)2 (cid:21)(cid:19) t2 4bt/3 − min 4σ2 , ≤ 2 exp ≤ δ.
(cid:12)(cid:12)(cid:12)cost(P j (cid:12)(cid:12)(cid:12) ≤  cost(P j i,1, Z).
i,1, Z) − cost(Sj i,1, Z) Thus with probability at least 1 − δ, we have Case 2.
r = 2.
Lastly, we consider b = 2.
Notice that each of these set contains at most Ti(o)/β2 points.
We now choose t =  8β2|P j i,2 |Ti(o)22jdg2 2bt/3 = i−1, and compute 8β2|P j i,2 |Ti(o) · 24jd2g4 i−1 · ζ ≥ 12 σ2 which implies (cid:1) (cid:1) t2 2σ2 + 2bt/3 t2  16bt + 2bt/3 t 18b 2 · ζ · Ti(o) 144 exp(cid:0) ≤ exp(cid:0) ≤ exp(− ≤ exp(− (cid:19) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ where the second step follows by 16 + 2/3 ≤ 18.
By taking a union bound, with probability 1 − δ, (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) (cid:18) (cid:88) p∈P j i,2 Ip 1 − d(p, Z)2 8β2|P j i,2 |Ti(o)22jdg2 i−1.
By the construction, each part P ∈ P j dist(P, Z) ≥ 2j√dgi−1 for j ≥ 1, thus dist(CP ∩ Q, Z) ≥ 2j−1√dgi−1.
Therefore i,2, it corresponding to a unique heavy cell CP in Gi−1.
Since 8β2|P j i,2 |Ti(o) · 22jdg2 i−1 ≤ 8β2 4|CP ∩ Q| · 4 · 22(j−1)dg2 i−1 ≤ 2 β2 cost(Q, Z).
(cid:88) P∈P j i,2 This concludes the proof.
32 Figure 5: (a) We show an example of partitioning the set of points from top to bottom.
There are four levels in total.
The top level is the ﬁrst level, and the bottom level is the fourth level.
The number of blocks is decreasing, when the levels is increasing.
Each blue bock is a heavy cell.
In the ﬁrst level, there is a single blue block.
In the second level, we partition it into four cells.
Only two of them are heavy (blue) cells.
In the third level, we partition the each of heavy cells (in the second level) into four cells.
One has one heavy (blue) cell, and the other has two (blue) heavy cells.
(b)We show an example of sampling points from those non-heavy (green) cells.
In the second level, there are two green cells, we sample 16 (red) points from each of them.
In the third level, there are ﬁve green cells, we sample 8 (red) points from each of them.
In the fourth (bottom) level, there are 12 green cells, we sample 4 (red) points from each of them.
D.5 Our Coreset Construction To explain our coreset construction, we ﬁrst deﬁned a probability of sampling in each level Gi, for i ∈ [0, L], as follows, (cid:20) 9 × 256 2 (144d3L)2 Ti(o) · ln πi(o, , δ) := min (cid:18) 2(L + 1)∆dk (cid:19) (cid:21) , 1 (5) where , δ ∈ (0, 1/2) and o > 0 are ﬁxed parameters.
We now deﬁne L + 1 sets of weighted samples as follows.
Denote the input point set Q ⊂ [∆]d.
For 0 ≤ i ≤ L, let Ci ⊂ Gi be the set of non-heavy cells, whose parent cell in Gi−1 is o-heavy.
Let Qi be the set of points falling in a cell of Ci. Notice that by Lemma D.4, Q0, Q1, .
.
.
, QL are disjoint sets and form a partition of Q.
Coreset Construction Let Si be a multiset, obtained by sampling each point q in Qi indepen- dently with probability πi(o, , δ).
Each point in Si has weight 1/(πi(o, , δ)).
Then our weight set is obtained as S(o, , δ) = S0 ∪ S1 ∪ .
.
.
∪ SL.
(6) 33 We omit (o, , δ) if it is clear from the context.
Notice that our coreset set construction is determined by the parameter o,  and δ.
We show in the next theorem the construction above gives a -coreset for k-means on Q.
Theorem D.14.
Suppose 0 < o ≤ OPT, and we given a point set Q ∈ [∆]d.
Let S(o, , δ) be multiset deﬁned in (6).
Then conditioning on event ξ holds, where we only have ekL/ρ center cells, with probability at least 1 − δ, S(o, , δ) is an -coreset of k-means on Q and (cid:19) −2k2d7L4 log(1/δ) · OPT·o (cid:20) 256 Proof.
Let (cid:48) = /3.
Take the sampling probability β2 Ti(o) · ln (cid:18) 2(L + log d)2∆dk E[|S|] = O( −1).
(cid:21) , 1 ζ = π(o, , δ) = min (cid:48)2 · Let S = S(o, , δ) be the obtained multiset.
(4) for i ∈ [0, L] and j ∈ [r] where r = (cid:100)log(∆√d)(cid:101).
Let P j falling in a part of P j For any ﬁxed set Z ⊂ [∆]d of k points, let P j i .
Let Sj i (Z) be the subsets of partitions deﬁned in (3) and i ⊂ Q be the corresponding point sets i be the corresponding sampled set of P j i .
Notice that S = ∪i∈[0,L],j∈[r]Sj i .
By Lemma D.12, Lemma D.13 and with a union bound over all possible i ∈ [0, L] and j ∈ [r], with probability at least 1 − δ ∆dk , Conditioning on the above inequalities, we have (cid:48) (cid:18) 2β · |P 0 i | · Ti(o) · dg2 i−1 (cid:88) (cid:48) |cost(S, Z) − cost(Q, Z)| ≤ cost(P j i∈[0,L] i , Z) + i∈[0,L],j∈[r] (cid:19) 2(cid:48) β2 cost(Q, Z) By Lemma D.11, thus (cid:88) i∈[0,L] |P 0 i | ≤ 12eLk OPT (cid:48) 2β · |P 0 i | · Ti(o) · dg2 i−1 ≤ (cid:48) (cid:48) 2β · 144d3L OPT =  OPT .
Since β = 144d3L, therefore Hence have (L + 1)(L + log d/2)/β2 ≤ (cid:88) 2(cid:48) β2 cost(Q, Z) ≤  (cid:48) i∈[0,L],j∈[r] cost(Q, Z).
34 and i , Z) (cid:12)(cid:12)cost(S0 (cid:12)(cid:12) cost(P j i , Z) − cost(P 0 (cid:12)(cid:12) ≤ i , Z)(cid:12)(cid:12) ≤  i , Z) − cost(Sj (cid:88) (cid:48) (cid:48) 2β · |P 0 i | · Ti(o) · dg2 i−1 · cost(P j i , Z) + 2(cid:48) β2 cost(Q, Z).
Overall, we have (cid:48) |cost(S, Z) − cost(Q, Z)| ≤ 3 cost(Q, Z).
Notice that 3(cid:48) = .
Since there are at most ∆dk possible Z, by a union bound, we have for all Z ⊂ [∆]d with |Z| ≤ k, |cost(S, Z) − cost(Q, Z)| ≤  cost(Q, Z).
Next we bound the number of points sampled.
By Lemma D.11, there are at most |P| ≤ αk = 12ekL OPT parts in the o-partition.
Each parts contains at most 3Ti(o) points.
Therefore, in expectation, (cid:88) i∈[0,L] 12ekL E(|S|) = |P i | · 3Ti(o) · πi(o, , δ) OPT (cid:18) k2d7L4 9 × 256 (cid:18) 1 (cid:19) 2 (cid:19) · (3 × 144d3L)2 · ln OPT = O log 2 (cid:18) 2(L + 1)∆dk (cid:19) Remark D.15.
The coreset can be constructed independently and oblivious to the point set, i.e., using hash functions ho,i : [∆]d → {0, 1}, such that the probability of ho,i(p) = 1 is exactly πi(o, , δ).
Later, we pick the set of points with hash value 1, when it is available.
Then we construct the coreset as in 6.
Observing that in our proof, the only requirement to bound the cost of partition.
Thus we have the following proposition.
Proposition D.16.
Suppose 0 < o ≤ OPT, and we given a point set Q ∈ [∆]d.
Let S(o, , δ) be mul- tiset deﬁned in (6).
Then conditioning on event that there exists an o-partition (see Deﬁnition D.6) satisfying |P | ≤ αk |P| · Ti(o) · dg2 i−1 ≤ d3L OPT and (cid:88) (cid:88) i∈[0,L] P∈P i then with probability at least 1 − δ, S(o, , δ) is an -coreset of k-means on Q and E[|S|] = O( −2k2d7L3α log 1/δ).
E Coreset Over the Dynamic Data Stream E.1 Streaming Coreset Construction As a ﬁrst step, we modify our coreset construction slightly such that an streaming algorithm can maintain the coreset construction.
The only diﬃculty is that we cannot obtain the number of points in a cell exactly.
We overcome this diﬃculty by modifying our heavy cell deﬁnitions as follows.
Deﬁnition E.1 ((, o)-Heavy Cell Scheme).
Fixing a number  ∈ [0, 1], a set of input points Q ⊂ [∆]d and a grid structure G = {G−1, G0, .
.
.
, GL} over [∆]d deﬁned in Deﬁnition D.1. We call a procedure an (o, )-heavy cell scheme if it satisﬁes, 35 1.
∀C ∈ G−1, C is o-heavy; ∀C ∈ GL, C is not heavy; 2.
for i ∈ [0, L], if a cell C ∈ Gi with |C ∩ Q| ≥ Ti(o), then C is identiﬁed as o-heavy; 3.
for i ∈ [−1, L], if a cell C with |C ∩ Q| < (1 − )Ti(o), then C is identiﬁed as not heavy; 4.
for i ∈ [−1, L − 1] a cell C ∈ Gi is identiﬁed not heavy, then all its children cells C(cid:48) As one can easily verify that the heavy cells in Deﬁnition D.1 is given by a (o, 0)-heavy cell is identiﬁed not heavy; ∈ Gi+1 scheme.
In particular, we have the following theorem.
Theorem E.2. Suppose 0 < o ≤ OPT, 0 ≤  ≤ 1, and we are given a point set Q ∈ [∆]d.
Let S(o, , δ) be multiset deﬁned in (6) using heavy cells deﬁned by any (o, )-heavy cell scheme.
Then conditioning on event ξ holds, where we only have ekL/ρ center cells, with probability at least 1− δ, S(o, , δ) is an -coreset of k-means on Q and −1).
E[|S|] = O( −2k2d7L4 log(L/δ) · OPT·o E.2 The Dynamic Point-Cell Storing Data Structure In this section, we introduce a data structure that maintains the coreset in a dynamic data stream.
To begin, we ﬁrst introduce the K-Set data structure, which we use as a sub-routine in our algorithm.
Lemma E.3 (K-Set).
Given parameter M ≥ 1, N ≥ 1, k ≥ 1, δ ∈ (0, 1/2).
There is a data structure that requires O(k(log M +log N ) log(k/δ)) bits and is able to process a data stream S(which contains both insertion and deletion).
The data structure processes each stream operation (i,±) (i ∈ [N ]) in O(log(k/δ)) time.
For each time t, let Mt denote the sum of the frequency of each the element.
Let M = maxt Mt. At each point of time t, it supports an operation Query, if the number of distinct elements is ≤ k, then with probability at least 1− δ it returns the set of items and the frequencies of each items.
It returns ∅ otherwise.
Our coreset critically relies on the data structure deﬁned in Algorithm 2.
It deﬁnes a data structure that supports inserting and deleting cell-point pairs.
For example, each input is of the form (C, p, op), where C is a cell in the grid, identiﬁed as its ID (including the information of its level and coordinates), p ∈ C and op is + or − representing insertion or deletion.
It has the following guarantees.
Lemma E.4. There is a data structure (procedure SampleStream+ in Algorithm 2) that supports insertion and deletions on cell-point pairs from [2L∆d] × [∆]d and a query operation.
At any time point, the query operation returns either of the follow two results, (1) C, f and S = {SC : ∀C ∈ C}, where C is all the cells undeleted, f is a function encoding the number of points in a cell in C, and ∀C ∈ C, SC is the set of points in C if |C| ≤ β; (2) ∅.
If conditioning on the following three events, (I) there are at most α non-empty cells; (II) there are at most θ cells that each of them contains more than β points; (III) the number of points in all the other cells that each containing most β points is at most γ, then the algorithm outputs ∅ with probability at most δ.
The algorithm uses bits of space in the worst case.
O[(θβ + γ) log(∆d) · log((θβ + γ)/δ)].
36 Proof.
We ﬁrst show that conditioning on the three events, our algorithm outputs the set of points with high probability.
We then show that if the three events do not happen, the output of the algorithm always matches the requirement of the statement.
Let C denote the set of all cells, let P denote the set of all points.
We also identify each cell C ∈ C by its coordinate.
We deﬁne hash function h : C → [a1 · α], g : [P ] → [a2 · β] and f : [a1 · α] × [a2 · β] → [a3(θβ + γ)], where constant a1, a2, a3 ≥ 1(will be decided later) and h, g, f are pair-wise independence function.
For each w ∈ [a3(θβ + γ)], j ∈ [2 log(∆d)] and z ∈ {0, 1}, we deﬁne counter counter(w, j, z) ∈ {0, 1}.
We initialize the all the entries of the counter to be 0.
Note the number of counters is 2(a3(θβ + γ) log(∆d)).
For each update (C, p, operation), we update the update the counter in the following way, counter(f (h(C), g(p)), j, (C, p)j) ← counter(f (h(C), g(p)), j, (C, p)j) ± 1 mod 2,∀j ∈ [2 log(∆d)], where (C, p)j is the j-th bit of the pair (C, p), “+” is corresponding to insertion, and “−” is corre- sponding to deletion.
We also initialize an K-Set structure to store at most α cells, and update the cell C to it.
For each w ∈ [a3(θβ + γ)] we deﬁne counter size(w) ∈ {0, 1,··· , ∆d}.
We initialize the all the entries of the counter to be 0.
Note the number of counters is a3(θβ + γ).
For each update (C, p, operation), we update the counter in the following way, size(f (h(c), g(p))) ← size(f (h(c), g(p))) ± 1, where “+” is corresponding to insertion, and “−” is corresponding to deletion.
For a ﬁxed cell C, for any cell C(cid:48) (cid:54)= C, we have (cid:48) [h(C) = h(C XC(cid:48) = 0 otherwise.
We deﬁne X =(cid:80) We deﬁne random boolean variable XC(cid:48) such that XC(cid:48) = 1 if h(C(cid:48)) = h(C) and b is non-empty; C(cid:48) /∈C \{C} xC(cid:48), and compute E[X] = Θ(1/a1).
By Markov’s Pr h∼H )] = Θ(1/(a1α)) inequality, Pr[X ≥ 1] ≤ E[X]/1 ≤ Θ(1/a1).
By choosing a1 to be suﬃciently large constant, we obtain with probability at least 0.99, there is no non-empty cell C(cid:48) ∈ C \{C}, satisfying that h(C(cid:48)) = h(C).
For a ﬁxed x ∈ [a1α], conditioned on only one non-empty cell c is hashed into this bin and c has at most β points.
We consider a ﬁxed y ∈ [a2β], suppose there is a point p such that g(p) = y.
For any other point p(cid:48)(belong to the same cell C), since g is a pair-wise independence hash function, then we have (cid:48) [g(p) = g(p Pr g∼G )] = Θ(1/(a2β)) 37 Y =(cid:80) We deﬁne random variable Yp(cid:48) such that Yp(cid:48) = 1 if g(p) = g(p(cid:48)); Yp(cid:48) = 0 otherwise.
We deﬁne p(cid:48) /∈C∩P\{p}, and compute E[Y ] ≤ Θ(1/a2).
By Markov’s inequality, Pr[Y ≥ 1] ≤ E[Y ]/1 ≤ Θ(1/a3).
By choosing a3 to be suﬃciently large constant, we obtain with probability at least 0.99, there is no p(cid:48) ∈ P ∩ C\{p}, satisfying that g(p) = g(p(cid:48)).
Moreover, there are at most (θβ + γ) distinct pairs (c, p).
Repeating the above argument, we observe that for give pair (h(C), g(p)), with probability at least 0.99, no other pairs (h(C(cid:48)), g(p(cid:48))) has the same hash value f (h(C), g(p)).
We say an entry (C, p) is good, if the cell C is non-empty and contains at most β points.
We can show for any good entry (C, p), one can recover point p with probability at least 0.9. Since the total number of good pairs is at most γ, thus repeating the above procedure Θ(log(γ/δ)) times, we have for any good entry (C, p), we can recover point p with probability at least 1 − δ/γ.
By taking a union bound over at most γ pairs, we have with probability at least 1 − δ/2, we can recover all the good entries (C, p).
By taking a union with the event that the K-Set data-structure is working correctly (with probability at least 1 − δ/2), we obtain the probability 1 − δ.
It remains to show that if any of the three events not happening our algorithm never outputs an incorrect result.
If the number of cells is greater than α, then either the K-set structure fails, or we know the number of cells are greater than α, thus the algorithm always outputs ∅.
If the remaining events does not happen or there are two good pairs collide in their hash values, the algorithm can always detect it by the counters.
Thus the points recovered are always from the stream.
A proper pruning detects any possible error.
Overall, for a single repeat, the number of bits used by hash function is O(log(∆d)), the number of bits used by counter counteri() is O(a3(θβ + γ)) log(∆d), the number of bits used by counter sizei() is O(a3(θβ + γ)) log(∆d).
The number of bits for K-Set is O(α log(∆d) log(α/δ)).
Putting it all together, th number of bits used over all repeats is O((θβ + γ) log(∆d) · log((θβ + γ)/δ)).
E.3 Main Algorithm Theorem E.5 (k-means).
Fix , δ ∈ (0, 1/2), k, ∆ ∈ N+, let L = log ∆.
There is a data structure supporting insertions and deletions of a point set P ⊂ [∆]d, maintaining a weighted set S with positive weights for each point, such that with probability at least 1− δ, S is an -coreset for k-means of size The data structure uses −2k2d7L4 log(1/δ)), O( (cid:101)O( −2k2d8L5 · (dkL + log(1/δ)) · log(1/δ)) bits in the worst case.
For each update of the input, the algorithm needs poly(d, 1/, L, log k) time to process and outputs the coreset in time poly(d, k, L, 1/, 1/δ, log k) after one pass of the stream.
38 Figure 6 Proof.
Let the point set at the end of the insertion and deletion operations be Q ⊂ [∆]d.
Without loss of generality, we assume the optimal cost for k-means is at most OPT and OPT > 0.
Indeed we if OPT = 0, then the set Q contains at most k points.
Thus we can always use a simple K-set structure to dynamically maintain k points.
The correctness of such an algorithm is guaranteed by Lemma E.3. Further we observe that OPT ≥ 1, since each pair of points is of distance at least 1.
To show our data structure, we ﬁrst pick a constant number ρ > 0 and use it as the failure probability.
Later we show we can boost it to arbitrarily small δ > 0.
Furthermore, we prove our theorem by showing a aδ failure probability, where a is some absolute constant.
We can easily boost the probability by only losing a constant factor in space.
The Data Structure Let ρ > 0 be a constant, i.e., take ρ = 0.01.
Our data structure ﬁrst initializes a randomized grid structure G−1, G0, G1, .
.
.
, GL over [∆]d, as in Deﬁnition D.1. Then we guess logarithmic many possible values for OPT, i.e., let O = {1, 2, 4, 8, .
.
.
, poly log(∆d+2d)}.
For each guess of o, we initialize a SampleStream+ data structure SSo with parameters α, β, γ, θ, δ, where α, β, γ, θ to be determined.
For each o and level i ∈ [−1, L], we initialize a new hash function ho,i : [∆]d → {0, 1} such that ∀p ∈ [∆]d : Pr[ho,i = 1] = πi(o, , δ) for some , δ ∈ (0, 0.5), where πi(·,·,·) is deﬁned in (5).
For representation simplicity, we ﬁrst assume that ho,is are fully independent hash functions.
Later we show that we can de-randomize them by using limited independence hash functions.
For each o ∈ O and i ∈ [−1, L], and each point p in operation, let Ci(p) be the cell containing p in Gi. We also identify Ci(p) with its ID.
For all i ∈ [−1, L], we ﬁrst compute its hash value ho,i(p), if ho,i(o) = 1, we update (Ci(p), p) to the data structure SSo with the corresponding operations on 39 Ch:C→[a1α]h:C→[a1α]Pg:[P]→[a2β]g:[P]→[a2β]f:[a1α]×[a2β]→[a3(θβ+γ)]f:[a1α]×[a2β]→[a3(θβ+γ)]wsize(w)counter(w,∗,0)counter(w,∗,1)Choose hi : C → [a1α] to be pair-wise independent hash function Choose gi : [P ] → [a2β] to be pair-wise independent hash function Choose fi : [a1α] × [a2β] → [a3(θβ + γ)] to be pair-wise independent hash function Initialize counter : counteri(w, j, z) ← 0,∀w ∈ [a3(θβ + γ)], j ∈ [2 log(∆d)], z ∈ {0, 1} Initialize counter : sizei(w) ← 0,∀w ∈ [a3(θβ + γ)] (cid:46) C, p the cell and the point, op ∈ {−, +} (cid:46) Hash cell into bins (cid:46) Hash point into bins (cid:46) Update counter (cid:46) Update counter Si,C ← ∅, x ← hi(C) for y ∈ [bβ] do w ← fi(x, y) if sizei(w) == 1 and ∀j ∈ [2 log(∆d)], counter(w, j, 0) + counter(w, j, 1) = 1 then end for end procedure procedure Update(C, p, op) procedure Init(∆, d, α, β, γ, θ, δ) KS.Update(C, op) for i = 1 → Rrepeats do for j = 1 → [log ∆d] do a1 ← Θ(1), a2 ← Θ(1), a3 ← Θ(1), Rrepeats ← Θ(log((θβ + γ)/δ)) Initialize K-Set data-structure : KS.Init(α, ∆d, δ/α) C ← the set of all cells (i.e., the L + 2 levels of grids).
for i = 1 → Rrepeats do x ← hi(C) y ← gi(p) w ← fi(x, y) z ← j-th bit of the bit representation of (C, p) counteri(w, j, z) ← counteri(w, j, z) op 1 mod 2 sizei(w) ← sizei(w) op 1 Algorithm 2 Better Data Structure with Small Number of Bits 1: procedure SampleStream+ 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: 37: 38: 39: 40: 41: 42: 43: 44: 45: 46: 47: 48: 49: 50: 51: 52: 53: 54: end procedure Si ← ∅ if KS.Check() (cid:54)= Fail or KS.size ≤ α then end for S ← Merge(S1, S2,··· , SRepeats) if ∃C ∈(cid:101)C,(cid:101)f (C) ≤ β : |SC| (cid:54)= (cid:101)f (C) then return (cid:101)C,(cid:101)f , S return ∅ (C, p) ← BitToNumber(counter(w,∗,∗)), Si,C ← Si,C ∪ {(C, p)} (cid:101)C,(cid:101)f ← KS.Query() for c ∈ (cid:101)C do end procedure procedure Query() for i = 1 → Rrepeats do else end for return ∅ end if S ← S ∪ Si else end if end procedure 40 end for end for end if end for p.
This completes the description of the data structure.
The complete data structure is presented in Algorithm 2.
Retrieve The Coreset Next we describe how to retrieve the coreset from the data maintained in the above data structure.
First we ﬁnd the smallest o∗ ∈ O such that SSo∗ does not return ∅.
From SSo∗, we obtain a set of cells (cid:101)C together with a function (cid:101)f that encodes the number of points in each cell in(cid:101)C.
We also obtain a set SC for each C ∈(cid:101)C ∩ Gi that is either ∅ or SC = C ∩ Qo,i, where Qo,i = {q ∈ Q : ho,i(q) = 1}.
For each C ∈(cid:101)C, we estimate the number of points in C, |C ∩ Q|, by (cid:101)f (C)(1 + /2)/πi(o, , δ).
With this, we identify all the heavy cells in(cid:101)C by following Deﬁnition D.14.
After this, we initialize our multiset S = ∅.
For each non-heavy cell C in (cid:101)C, we assign a weight 1/πi(o, , δ) to each of point in SC.
After this, add all the points in SC to S.
This completes the description of retrieving the coreset.
The complete data structure is presented in Algorithm 3.
Correctness with Constant Probability We ﬁrst show that by chosen a constant ρ, i.e., for a ρ > 0, if the event ξ happens, i.e., there are at most ekL/ρ center cells in the grid, then with probability at least 1 − δ, our data structure maintains an -coreset S for k-means.
Notice that Pr[ξ] ≥ 1 − ρ.
To show the correctness of the data structure, we show that there exists an 1 ≤ o ≤ OPT, the output S is an -coreset for k-means on Q.
Heavy Cell Identiﬁcation First, for an 1 ≤ o ≤ OPT, we show that with probability at least 1 − δ, for all i ∈ [0,−L] and for all C ∈ Gi, the estimator of points in a cell, (cid:98)co,i = |Qo,i ∩ C|/π(o, , δ) · (1 + /2), together with the operations in Deﬁnition D.14 give an (o, )-heavy cell scheme (Deﬁnition E.1) by identifying heavy cells from level −1 to L and removing those heavy cells with parent cells identiﬁed as non-heavy.
Here is deﬁned in (5).
Indeed it is suﬃce to show that for all heavy cells C ∈ Gi, |C ∩ Qo,i|/π(o, , δ) is (cid:18) an (1 ± /2)-approximation to |Qo,i ∩ C|.
Observing that 2 · (d3L)2 · E [|C ∩ Qo,i|] ≥ O (cid:19)(cid:19) dkL + log by the Chernoﬀ bound and an union bound over all heavy cell cells, we obtain the desired precision of estimation.
Correct Parameters of SampleStream In this section, we show that there exists an 1 ≤ o ≤ OPT such that with probability at least 1 − δ, an SampleStream instance SSo returns the set of points in non-heavy cells together with all the estimates of the number of points in heavy cells.
To show this, it is suﬃce to show that an OPT /2 ≤ o ≤ OPT satisﬁes the above statement with probability at least 1 − δ.
Firstly, we show that with probability at least 1 − δ/4 there are at most α (to be determined) non-empty cells in all grids containing a sampled point (i.e., ho,i(p) = 1).
41 (cid:18) (cid:19)(cid:19) πi(o, , δ) = O (d3L)2 Ti(o) · dkL + log (cid:18) 1 2 · (cid:18) 1 Indeed, for each point in a non center cell, it contributes to the OPT with at least g2 i /(4d2) (see Deﬁnition (1)).
Therefore, there are at most OPT g2 i /(4d2) 4d2 g2 OPT = 4Ti(o) · ek ρ · OPT points in non-center cells per level of grid, where Ti(o) = d2 ek is the thresholding function.
Let g2 Q(cid:48) o,i be the set of points in non-center cells of level i with ho,i value 1, i.e., the set of sampled points from non-center cells.
Therefore, i · ρo E [|Qo,i|] = 4Ti(o) · ek ρ · OPT · πi(o, , δ) = O dkL + log where we use the fact that OPT /o ≤ 2.
By the Chernoﬀ bound and a union bound over all grids, there are at most (cid:19)(cid:19) (cid:18) k (cid:18) 2 · (d3L)2 · (cid:19)(cid:19) points from non-center cells being sampled.
Hence there are at most this number of non-center cells containing a sampled point.
Together with the number of center cells, there are at most cells containing a sample point.
Next we pick (cid:18) kL (cid:18) kL (cid:18) 1 2 · (d3L)2 · dkL + log α = O 2 · (d3L)2 · dkL + log β = Θ 2 · (d3L)2 · dkL + log (cid:19)(cid:19) (cid:19)(cid:19) (cid:18) (cid:18) (cid:18) (cid:18) kL (cid:19) (7) (8) By the Chernoﬀ bound and a union bound over all cells, with probability at least 1 − δ/4, we have that each non-heavy cell containing at most β/2 sample points.
Next, since that are at most O(kL/ρ) heavy cells, we set θ = Θ (9) Then there are at most θ cells containing more than β sample points.
Lastly, we pick γ = α as an upper bound on the number of points in cells with at most β sample points.
As such, conditioning on the above events, by Lemma E.4, with probability at least 1 − δ, the data structure SSo outputs the desired set of points and the cells with counts.
And by Theorem E.2, the multiset is an -coreset for k-means on Q.
Boost Probability from 1 − ρ − O(δ) to 1 − O(δ) The probability boosting procedure is by δ ) times and take the output independently repeating the above procedure in parallel u = O(log 1 as follows.
For each r ∈ [u], suppose in the r-th repeat, the output guess (if exists) of OPT is or, the SampleStream structure is SSr or, the multiset is Sr and the number of heavy cells returned r, i.e, by SSr estimates the number of points in non-heavy children of a heavy cell, and divide Ti(o) to obtain α(cid:48)(cid:48) r.
Consider all the repeats with r.
Based on the heavy cell scheme, we also estimate the number of parts as α(cid:48)(cid:48) or is α(cid:48) for some large constant a1.
Let the set of repeats satisfying this requirement be R.
(cid:48)(cid:48) r ≤ a1kL 42 Let r∗ = arg minr∈R or.
Then the ﬁnal output is taken as Sr∗.
To show that the overall failure probability of such an operation is at most O(δ), the ﬁrst claim is that, if ∃or ≤ OPT and α(cid:48)(cid:48) r ≤ a1kL, then in the repeat r, with probability at least 1− δ, the number of parts in an or-partition (see Deﬁnition D.6) is at most O(kL).
This simply follows from the Chernoﬀ bound, since with probability at least 1 − δ, the number of points in a set with more than Ti(o) points can be accurately estimated.
Thus it is straight forward to verify that this partition satisﬁes the requirement of Proposition D.16.
Thus, by Proposition D.16, with probability at least 1 − δ the output multiset Sr is a -coreset for k-means on Q.
Therefore, if there exists a guess or ≤ OPT in the output sequence, then for the minimum, or∗ ≤ OPT and with probability at least 1 − O(δ) we obtain the desired coreset.
Next, we show that there exists an r ∈ [u], and or ≤ OPT.
Since each repeat is independent, then with probability at least 1 − O(δ), there exists an r with at most ekL/ρ centers cells the grid of the r-th repeat.
Conditioning on this event, the r-th repeat outputs the multiset with or ≤ OPT with probability at least 1 − O(δ).
This concludes the probability boosting.
Space Bound and Random Bits Lastly we show the space algorithm.
Each SampleStream data structure uses α log( ) · dL = O 2 · (d3L)2 · dL · dkL + log log(kLd/) + log log bits of space.
For all O(d log ∆) repeats, the overall space complexity is (cid:16) (cid:101)O (cid:18) kd2L3 (cid:17) (cid:18) kL (cid:18) (cid:18) (cid:19) (cid:19) = (cid:101)O (cid:18) (cid:19) (cid:18) kd8L5 (cid:18) (cid:19)(cid:19) (cid:19) log (cid:19) · (d3L)2 · 2 dkL + log · log 2 dkL + log Lastly for the independent hash function we use, we can de-randomize them by the method in [FS05] by using z-wise independent hash functions for suﬃciently large z, or use a pseudo-random generator as it in [Ind00b] and then apply a auxiliary algorithm argument as it in [BFL+17].
In either case, one can show that the space for the random bits is not dominating.
This completes the proof.
F General Clustering Problem F.1 M-Estimator Clustering Our framework can be extended easily to other clustering problems, e.g., we consider the following (cid:88) M-Estimator clustering problem, ﬁxing Q ⊂ [∆]d, (10) min Z⊂[∆]d:|Z|≤k p∈Q M (dist(p, Z)) Here M (·) is non-decreasing function satisﬁes ∀x > 0, M (x) > 0 and ∀c > 0, M (cx) ≤ f (c)M (x) where f (c) > 0 is a bounded function.
43 Algorithm 3 Data Structure 1: procedure SampleKMeans 2: 3: 4: 5: 6: 7: 8: procedure Init(k, ∆, d, , δ, δ) O ← {1, 2,··· , poly(d, ∆d)} ρ ← 0.01, Construct G−1, G0, .
.
.
, GL for o ∈ O do for i = 0 → L − 1 do (cid:46) grids deﬁned in Section D.1 using parameter ρ Construct hash function ho,i : [∆]d → {0, 1} s.t. Pr ho,i∼H α, θ, γ and β are determined in (7), (8) and (9) RecordPointso ← SampleStream.Init(∆, d, α, β, γ, θ, δ) end for [ho,i(p) = 1] = πi(o, , δ) (cid:46) op ∈ {−, +} (cid:46) So = {So,0, So,1,··· , S0,L−1} end for 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: end procedure return S end for end procedure procedure Update(p, op) for o ∈ O do for i = 0 → L − 1 do if ho,i(p) = 1 then c ←Cell(p) in Gi RecordPointso.Update(c, p, op) end if end for end procedure procedure Query() for o ∈ O do end for o∗ (cid:101)Co,(cid:101)f o, So ← RecordPointso.Query() S ← PointsToCoreset((cid:101)Co∗ ← arg mino∈O{So,i (cid:54)= ∅,∀i ∈ {0, 1,··· , L − 1}} ,(cid:101)f o∗ , So∗ end procedure Theorem F.1 (M-Estimator Clustering).
Fix , δ ∈ (0, 1/2), k, ∆ ∈ N+, let L = log ∆.
There is a data structure supporting insertions and deletions of a point set P ⊂ [∆]d, maintaining a weighted set S with positive weights for each point, such that with probability at least 1 − δ, S is an -coreset for M-estimator clustering problem deﬁned in (10) of size −2k2dηL4 log(1/δ)), O( where η is an absolute constant depending only on M.
The data structure uses (cid:101)O( −2k2dηL5 · (dkL + log(1/δ)) · log(1/δ)) bits in the worst case.
For each update of the input, the algorithm needs poly(d, 1/, L, log k) time to process and outputs the coreset in time poly(d, k, L, 1/, 1/δ, log k) after one pass of the stream.
44 Proof.
We can simply repeat the proof for k-means.
In the proof, we need to modify heavy cell deﬁnition, i.e., the thresholding function Ti(o), Ti(o) = M (gi/(2d)) · In the proof of Theorem D.14, we need to modify β to be, L · f (√d) (cid:32) β = Θ (cid:33) ρ · o ek f (1/(2d)) The rest of the proof follows by replacing dist2(·) with M (dist(·)).
We can verify that only the dependence on d changes.
Improvements Over k-median F.2 In this section, we show that our newly developed techniques can improve over the k-median con- struction in [BFL+17].
In particular, we have the following guarantee.
Theorem F.2 (k-median).
Fix , δ ∈ (0, 1/2), k, ∆ ∈ N+, let L = log ∆.
There is a data structure supporting insertions and deletions of a point set P ⊂ [∆]d, maintaining a weighted set S with positive weights for each point, such that with probability at least 1 − δ, S is an -coreset for k- median of size The data structure uses −2k · poly(d, L) · log(1/δ).
(cid:101)O( −2k · poly(d, L) · log(1/δ)) bits in the worst case.
For each update of the input, the algorithm needs poly(d, 1/, L, log k) time to process and outputs the coreset in time poly(d, k, L, 1/, 1/δ, log k) after one pass of the stream.
Proof.
Using the same construction in [BFL+17].
The improvements can be stated as follows.
Instead of take a union over the each level for union bounding the center cells, we bound the overall center cells.
This saves an L factor in the space.
However doing so can only be achieved by using our newly constructed SampleStream procedure, which additionally saves L factor.
Furthermore, we boost the failure probability from ρ to arbitrary δ > 0 using the procedure in proof Theorem E.5. Remark F.3. Using our newly developed technique, we save space complexity for O(1/δ) factor, comparing with that in [BFL+17].
Furthermore, our newly developed stream sampling algorithm also saves space up to poly(dL) factors.
G Applications In this section, we show our dynamic data structure can be used to approximately maintain a solution for many problems in the dynamic streaming setting.
45 G.1 A Dynamic Streaming Approximation to Max-CUT In this section, we show that our coreset construction can be used to obtain a 1/2-approximation of Max-CUT of the form as M (dist(p, q)) (11) (cid:88) (cid:88) p∈C1 q∈C2 max C1∪C2 where M (·) is an M-estimator and C1 and C2 partition the streaming point set P ⊂ [∆]d.
Formally, we obtain the following result.
Theorem G.1 (Max-CUT).
Fix , δ ∈ (0, 1/2), ∆ ∈ N+, let L = log ∆.
There is a data struc- O(cid:0) 1 (cid:1) bits in the worst case.
For each update of the input, the algorithm needs ture supporting insertions and deletions of a point set P ⊂ [∆]d, and outputs a 1/2-solution with cost estimation up to a (1 ± ) factor with probability at least 1 − δ.
The data structure uses poly(d, 1/, L) time to process and outputs the coreset in time poly(d, L, 1/, 1/δ) after one pass of the stream.
Proof.
To solve Max-CUT, we simply use a random solution, i.e. use a hash function to maintain two random cuts C1 and C2.
Let OPTCUT be the optimal value for Max-CUT.
It is a standard result that, 2 · poly(d, L) · log 1 (cid:88) (cid:88) p∈C1 q∈C2 (cid:88) (cid:88) p(cid:48)∈S1 (cid:88) (cid:88) q(cid:48)∈S2 p(cid:48)∈S1 q(cid:48)∈S2 (cid:48) M (dist(p (cid:48) , q OPTCUT M (dist(p, q))  ≥ (cid:88) (cid:88) = (1 ± /2)2 (cid:88) p(cid:48)∈S1 p∈C1 )) = (1 ± ) )) = (1 ± /2) q∈C2 (cid:88) (cid:88) (cid:88) q∈C2 M (dist(p (cid:48) , q)) M (dist(p, q)).
To approximate the cost, we use the coreset to obtain two (/2)-coresets for 1-M-clustering on both C1 and C2.
Let the coresets be S1 and S2.
Then we show M (dist(p (cid:48) (cid:48) , q M (dist(p, q)).
Indeed, q∈C2 as desired.
The probability boosting for the solution is standard.
p∈C1 G.2 A Dynamic Streaming Approximation to Average Distance For similar proof of the Max-CUT, we obtain a dynamic estimation for average distance, (cid:88) |Q| − 1 p,q∈Q M (dist(p, q)) where M (·) is an M-estimator.
Theorem G.2 (Average Distance).
Fix , δ ∈ (0, 1/2), ∆ ∈ N+, let L = log ∆.
There is a O(cid:0) 1 (cid:1) bits in the worst case.
For each update of the input, the algorithm needs data structure supporting insertions and deletions of a point set P ⊂ [∆]d, and outputs a (1 ± ) approximation to the average distance of P , with probability at least 1 − δ.
The data structure uses poly(d, 1/, L) time to process and outputs the coreset in time poly(d, L, 1/, 1/δ) after one pass of the stream.
2 · poly(d, L) · log 1 46 Proof.
The proof is similar to that of Theorem G.1 by constructing a corset for 1-M-Clustering on Q.
H (cid:101)O(k) Space Algorithm Based on Sensitivity Sampling H.1 Deﬁnitions and Preliminaries Deﬁnition H.1 (γ-important partition).
Given a set of points Q ⊂ [∆]d, and parameter γ ∈ (0, 1/10).
Let Pi,j be the partition sets deﬁned for each level i ∈ [−1, L].
Let P denote the partition, We deﬁne sets R, R ⊂ {0, 1,··· , L} as follows i (cid:12)(cid:12)(cid:12)(cid:12) |Pi|(cid:88) j=1 R = |Pi,j| ≤ γ · Ti(o) P = ∪L i=0Pi = ∪L i=0 ∪ |Pi| j=1 {Pi,j}.
 , and R = i (cid:12)(cid:12)(cid:12)(cid:12) |Pi|(cid:88) j=1  .
|Pi,j| > γ · Ti(o) We deﬁne important partition and non-important partition as follows P I = ∪i /∈RPi and P N = ∪i∈RPi. Let QI denote the set of γ-important points of Q that contained in γ-important partition P I.
Let QN denote the set of γ-non-important points of Q that contained in γ-non-important partition P N.
If in the context, the value of γ is clear, we sometimes will omit γ, and call QI as important points, and call P I as important partition.
Theorem H.2 (Oﬀ-line Algorithm, [FL11]).
Given a set of points Q ⊂ [∆]d, for each point p ∈ Q, let s(p) denote the sensitivity of point p which is deﬁned as follows s(p) = max Z∈[∆]d,|Z|=k (cid:80) dist2(p, Z) q∈Q dist2(q, Z) for each sample, it chooses point p with probability s(cid:48)(p)/t(cid:48) where s(cid:48)(p) ∈ [s(p), 1], t(cid:48) =(cid:80) Let A denote a sampling procedure deﬁned in the following way: it repeats m independent samples; p∈Q s(cid:48)(p); and for each sample, if some point p got chosen, we associated weight w(p) = t(cid:48)/(ms(cid:48)(p)) to that point.
Let (S, w(·)) denote the set of points and weights outputted by A.
If m ≥ Ω(t(cid:48)−2(log |Q| log t(cid:48)+ log(1/δ))), then with probability at least 1 − δ, (S, w(·)) is an (1 ± )-coreset of size m for Q.
H.2 Reducing Original Problem to Important Points In this section, we show that if we want to output a coreset for the original point set, we only need to ﬁnd the coreset for those important points.
It means that we can drop all the points which are in the “non-important” level.
Lemma H.3. Given a set of points Q ⊂ [∆]d associated with partition P deﬁned as follows, Let P N ⊂ Q denote the set of non-important points associated with partition P N deﬁned as follows, P = P0 ∪ P1 ∪ ··· ∪ PL.
P N = ∪i∈RPi, 47 (cid:80)|Pi| j=1 |Pi,j| ≤ γTi(o)} and γ ≤ /(200Ld3ρ).
Let QI denote the set of points that where R = {i | contained by non-important partitions P I.
Then for all Z ⊂ [∆]d with |Z|, we have (I) cost(Q, Z) ≥ cost(QI , Z) (II) cost(Q, Z) ≤ (1 + ) cost(QI , Z) Proof.
Proof of (I).
It is trivially true, because QN is a subset of Q.
Proof of (II).
We consider a i which satisﬁes |Pi|(cid:88) j=1 |Pi,j| ≤ γ · Ti(o), and ﬁx a j ∈ [|Pi|].
Let c(Pi,j) denote the cell in (i − 1)-th level which contains all the points in partition Pi,j.
For each point p ∈ Pi,j, let Ni,j denote the set of important points (⊂ QI) that contained by cell c(Pi,j), there exists a point q ∈ Ni,j satisﬁes that dist2(p, Z) ≤ 2 dist2(p, q) + 2 dist2(q, Z) ≤ 2dg2 ≤ 2dg2 i−1 + 2 dist2(q, Z) i−1 + 2 (cid:88) |Ni,j| q∈Ni,j dist2(q, Z) (12) where the ﬁrst step follows by triangle inequality, the second step follows by deﬁnition of the grids, the last step follows by an averaging argument.
According to deﬁnition of Ni,j, |Ni,j| ≥ Ti−1(o) − γTi(o) ≥ Ti−1(o).
48 We can lower bound cost(Q, Z) in the following sense, cost(Q, Z) = cost(QI , Z) + cost(QN , Z) = cost(QI , Z) + dist2(p, z) ≤ cost(QI , Z) + 2 |Pi,j| · i−1 + |Ni,j| dist2(q, Z) ≤ cost(QI , Z) + 2 |Pi,j| · i−1 + Ti−1(o) q∈Ni,j dist2(q, Z) ≤ cost(QI , Z) + 2 |Pi,j| · i−1 + Ti−1(o) dist2(q, Z) ≤ cost(QI , Z) + 2 i−1 + Ti−1(o) dist2(q, Z) ≤ cost(QI , Z) + 4LγTi(o) · Ti−1(o) q∈QI dist2(q, Z) (cid:19) dg2 = cost(QI , Z) + 4LγTi(o) · ≤ cost(QI , Z) + 4Lγ(d3ρo/(3k) + 8 cost(QI , Z)) ≤ cost(QI , Z) + 4Lγ(d3ρ cost(Q, Z)/(3k) + 8 cost(QI , Z)).
cost(QI , Z) Ti−1(o) i−1 + j=1 p∈Pi,j j=1 i∈R i∈R (cid:88) |Pi|(cid:88) (cid:88) (cid:88) |Pi|(cid:88) (cid:88) |Pi|(cid:88) |Pi|(cid:88) (cid:88) (cid:88) i∈R j=1 i∈R j=1 γTi(o) · i∈R dg2 dg2 dg2 dg2 dg2 (cid:18) i−1 + q∈Ni,j (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) q∈QI q∈QI      (cid:80)|Pi| where the second step follows by deﬁnition of cost, the third step follows by Eq. (12), the fourth step follows by |Ni,j| ≥ Ti−1(o)/2, the ﬁfth step follows by Ni,j ⊂ QI, the sixth step follows by i=1 |Pi,j| ≤ γTi(o), the seventh step follows by |R| ≤ L + 1 ≤ 2L, the ninth step follows by Ti(o) = 4Ti−1(o) and Ti(o) = d2ρo/(3g2 It implies that i k), and the last step follows by o ≤ OPT ≤ cost(Q, Z).
cost(Q, Z) cost(Q\QN , Z) ≤ 1 + 32Lγ 1 − 4Lγd3ρ/(3k) ≤ 1 + /4 1 − /4 ≤ 1 + , where the second step follows by γ ≤ /(200Ld3ρ), and the last step follows by γ < 1/2.
Therefore, we complete the proof.
H.3 Sampling Scores of Important Points In this section, we focus on the sensitivity of important points.
We ﬁrst give a good upper bound of sensitivity of important points.
And then we show the sum of the upper bound o the sensitivities is small.
Lemma H.4. Given a point set Q associated with partition P and parameter o ∈ (0, OPT].
For each p ∈ Q, there must exist a unique partition Pi,j that contains p.
Let QI ⊂ Q denote the set of 49 γ-important points, where γ ≤ /(200Ld3ρ).
Then we have, for all Z ⊂ [∆]d with |Z| = k, for all p ∈ QI, (cid:80) dist2(p, Z) q∈QI dist2(q, Z) ≤ 60 d3 Ti(o) Proof.
Fix a p ∈ QI.
Let Pi,j denote the partition that contains this point.
Let c(Pi,j) denote the cell in i − 1-th level which contains all the points in partition Pi,j.
Let Ni,j denote the set of non-important points belong to cell c(Pi,j).
It is easy to observe that (cid:48) ∃p ∈ Ni,j, dist2(p (cid:48) , Z) ≤ |Ni,j| q∈Ni,j dist2(q, Z) (13) (cid:88) (cid:80) + 2 dist2(p, p(cid:48)) q∈QI dist2(q, Z) q∈Ni,j dist2(q, Z) dist2(p(cid:48), Z) q∈QI dist2(q, Z) i−1 dg2 q∈QI dist2(q, Z) q∈QI dist2(q, Z) q∈QI dist2(q, Z) dg2 i−1 q∈QI dist2(q, Z) q∈QI dist2(q, Z) (cid:80) (cid:80) + 2 + 2 We have (cid:80) dist2(p, Z) q∈QI dist2(q, Z) ≤ 2 ≤ 2 ≤ 2 = 2 ≤ 2 ≤ 2 ≤ 2 (cid:80) (cid:80) (cid:80) (cid:80) (cid:80) |Ni,j| |Ni,j| |Ni,j| |Ni,j| |Ni,j| |Ni,j| (cid:80) + 2 + 4 + 16 dg2 i−1 q∈QI dist2(q, Z) dg2 i−1 OPT dg2 OPT + 48 d3ρo Ti(o)k OPT + 48 d3ρo Ti(o)k OPT + 48 d3 Ti(o) ≤ 10 ≤ 10 ≤ 60 Ti(o) Ti(o) d3 Ti(o) where the ﬁfth step follows by (cid:80) where the ﬁrst step follows by triangle inequality, the second step follows by Eq. (13) and p(cid:48) ∈ c(Pi,j), q∈QI dist2(q, Z) ≥ (1 − ) OPT ≥ OPT /2 (By Lemma H.3), the sixth step follows by g2 3k , the eighth step follows by Ti(o) = 4Ti−1(o) ≤ 5|Ni,j| ( according o-partition and γ-important partition, see Deﬁnition D.6 and Deﬁnition H.1), the ninth step follows by ρ ∈ (0, 1), 1/k ≤ 1, o ≤ OPT.
i , the seventh step follows by Ti(o) = d2 g2 i−1 ≤ 4g2 ρo Thus, we complete the proof.
Lemma H.5. Given a point set Q associated with partition P of parameter o ∈ (0, OPT].
Let QI ⊂ Q denote the set of γ-important points, where γ ≤ /(200Ld3ρ).
For each important level 50 i ∈ R, let(cid:80)|Pi| p∈QI Proof.
j=1 |Pi,j| = ai · Ti(o).
Then we have, for all Z ⊂ [∆]d with |Z| = k, for all p ∈ QI, (cid:88) i∈R ai · 3.
(cid:88) (cid:80) dist2(p, Z) q∈QI dist2(q, Z) ≤ 60d3 · (cid:88) (cid:88) (cid:88) (cid:80) (cid:88) dist2(p, Z) q∈QI dist2(q, Z) Pi,j p∈Pi,j∩QI p∈QI Ti(o) d3 d3 60 |Pi,j| · 60 Ti(o) Pi,j ≤60d3 · (cid:88) i∈R ai · 3.
where the ﬁrst inequality follows by Lemma H.4. H.4 Algorithm The goal of this Section is to prove Theorem H.6, Theorem H.6 (k-means, linear in k).
Fix , δ ∈ (0, 1/2), k, ∆ ∈ N+, and let L = log ∆.
There is a data structure supporting insertions and deletions of a point set P ⊂ [∆]d, maintaining a weighted set S with positive weights for each point, such that with probability at least 1 − δ, S is an -coreset for k-means of size The data structure uses −3k poly(d, L, log(1/δ)).
−3k poly(d, L, log(1/δ)) bits in the worst case.
For each update of the input, the algorithm needs poly(d, 1/, L, log k) time to process and outputs the coreset in time poly(d, k, L, 1/, 1/δ, log k) after one pass of the stream.
Remark H.7. For each level i, there are three kinds of points, (1) points in partition, (2) points in heavy cell (these points belong to the partition of some level j ∈ {i + 1,··· , L}), (3) points in non- partition and non-heavy cell (these points belong to the partition of some level j ∈ {0, 1,··· , i− 1}).
We ﬁrst introduce some auxiliary concentration results.
Theorem H.8 ([BR94]).
Let k be an even integer, and let X be the sum of n k-wise independent random variables taking values in [0, 1].
Let µ = E[X] and a > 0.
Then we have (cid:20) (cid:21) (cid:18) kµ + k2 (cid:19)k/2 a2 Pr |X − µ| > a ≤ 8 · 51 Algorithm 4 Sensitivity-Based Sampling 1: procedure Oracle() 2: 3: 4: procedure Init(i, o, Gi) (cid:46) Gi is the i-th grid m ← c max(d3Lk−3(Ld + log(1/δ)), d3Lk−2(dL log(dLk) + log(1/δ))) for some constant c RecordPoints ← SampleStream+.Init(∆, d, c1kL + c3mL, c2m/k, c3mL, c1kL, δ/(d∆d)c4) (cid:46) c1, c2, c3, c4 are four constants for j = 1 → 100m do Prhj∼H [hj(p) = 1] = 1/Ti(o)/k [∆]d → {0, 1} s.t. (cid:46) Ti(o) is given in Deﬁnition D.3 (log(1/δ))-wise independent hash hj independent Construct 5: 6: end procedure procedure Update(c, p, op) for j = 1 → 100m do if hj(p) = 1 then c ← Cell(p) in Gi RecordPoints.Update(c, (p, j), op) end if end for end for 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: end procedure end procedure procedure Query(j(cid:48)) (cid:101)C,(cid:101)f , S1, S2, .
.
.
, S100m ← RecordPoints.Query() (cid:46) Sj contains the samples of the form (p, j) (cid:98)Sj ← ∅ for C ∈ (cid:101)C do Return Fail if RecordPoints returns Fail Identify heavy cells using the samples from S1, S2, .
.
.
, S100m (cid:46) Partition cell: a cell is not heavy, but its parent is heavy; The criterion is based on (cid:101)f (C) (cid:98)Sj ← (cid:98)Sj ∪ Sj,C (cid:46) Sj,C denotes the set of samples of Sj containing in C if C is a partition cell in this level then end if end for end procedure procedure EstNumPts( ) Let j denotes the j(cid:48)-th non-empty (cid:98)Sj(cid:48).
If no such j(cid:48) exists, return Fail return A uniform sample from (cid:98)Sj Denote the same (cid:98)Sj as in Query return | ∪j (cid:98)Sj| · Ti(o) · k/(100m) Return 0 if any step in Query returns Fail end procedure Lemma H.9. Let π ∈ [0, 1] be a ﬁxed value.
Suppose we have m independent (4 log(1/δ))-wise hash functions h1, h2, .
.
.
, hm : [∆]d → {0, 1} with that ∀i ∈ [m], p ∈ [∆]d : Pr[hi(p) = 1] = π.
Let set S ⊂ [∆]d and denote hi(S) = {s | hi(s) (cid:54)= 0, s ∈ S}.
For each i ∈ [m], s ∈ S, we deﬁne Xi,s to be hi(s).
Let X =(cid:80)m s∈S Xi,s.
Then with probability at least 1 − δ, (cid:80) i=1 (cid:12)(cid:12)X − mπ|S| (cid:12)(cid:12) ≤ mπ|S| provided m ≥ c log δ(1/δ) π|S|2 for some suﬃciently large constant c ≥ 4.
52 O ← {1, 2, 4, 8, 16··· , poly(d, ∆d)} ρ ← 0.01, Construct G−1, G0,··· , GL Initialize oracleo,l ← oracle.Init(l, o, Gi) procedure Init(k, ∆, d, , δ) Algorithm 5 1: procedure SampleKMeansLinear() 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end procedure end procedure procedure Update(p, op) for l = 0 → L do end for oracleo,l.Update(p,op) for o ∈ O do end procedure end for (cid:46) grids deﬁned in Section D.1 using parameter ρ (cid:46) Algorithm 4 Ao,i, ﬂago,i ← Oracleo,i.EstNumPts() So,i ← ∅ end for γ ← /(200Ld3ρ) (cid:46) Important levels R ← {i | Ao,i > γ · Ti(o)} m ← c max(d3Lk−3(Ld + log(1/δ)), d3Lk−2(dL log(dLk) + log(1/δ))) for some constant c for j = 1 → m do Ao,i Sample a level i ∈ R with probability q, ﬂag ← Oracleo,i.Query(j) if ﬂag (cid:54)= Fail then i(cid:48)∈R Ao,i(cid:48)/Ti(cid:48)(o))/(m/Ti(o)) (cid:80) i(cid:48)∈R Ao,i(cid:48) for o ∈ O do procedure Query() for i = 0 → L do Algorithm 6 1: procedure SampleKMeansLinear() 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: end procedure So,i ← So,i ∪ (q, wq) So,i ← ∅ break wq ← ((cid:80) end for o∗ return So end procedure end if else end for ← arg mino∈O{So,i (cid:54)= ∅, ﬂago,i (cid:54)= Fail,∀i ∈ {0, 1,··· , L − 1}} (cid:46) Algorithm 4 (cid:46) weights of i-th level (cid:46) So = So,0 ∪ So,1 ∪ ··· ∪ S0,L−1 Proof.
It is obvious that m(cid:88) (cid:88) i=1 s∈S X = Xi,s = m(cid:88) (cid:88) i=1 s∈S hi(s) = m(cid:88) i=1 |hi(S)|.
Then we have each Xi,s is a (log(1/δ))-wise independent random variable.
Since m ≥ c log(1/δ) π|S|2 , we 53 have that Let k = 4 log(1/δ).
Thus by Lemma H.8, with probability at least mπ|S| ≥ c log(1/δ) and 2mπ|S| ≥ ck.
we have 1 − 8((kµ + k2)/(mπ|S|)2)k/2 ≥ 1 − (c/2)k/2 ≥ 1 − δ, (cid:12)(cid:12)X − mπ|S| (cid:12)(cid:12) ≤ mπ|S| provided c ≥ 4.
Lemma H.10.
Let k ≥ 1 be some ﬁxed value.
Suppose we have 100m independent pairwise hash functions h1, h2, .
.
.
, h100m : [∆]d → {0, 1} with that ∀i ∈ [100m], p ∈ [∆]d : |S|· Pr[hi(p) = 1] = 1/k.
i=1 1|hi(S)|>0.
Then with probability at least 1 − δ, Let set S ⊂ [∆]d and denote hi(S) = {s : hi(s) (cid:54)= 0, s ∈ S}.
Let X =(cid:80)m X > m/k, provided m/k ≥ c log(1/δ) for some suﬃciently large constant c.
Proof.
Denote, for j = 1, 2, .
.
.
, 10m/k, Yj = i=10(j−1)k+1 |hi(S)|.
Thus E[Yj] = 10 and Var[Yj] ≤ 10.
Thus by Chebyshev’s inequality, we have Pr[|Yj − 10| ≥ 5] ≤ 10/25 = 0.4. Thus E[X] ≥ 4m/k.
Denote Xj = i=10(j−1)k+1 1|hi(S)|>0.
j=1 |Pi,j| = ai · Ti(o).
Conditioned on StreamSampling+ Then each Xj is an independent random variable.
The lemma follows from applying Chernoﬀ bound over(cid:80) Xj. Claim H.11.
For each i ∈ R, let (cid:80)|Pi| doesn’t fail, then with probability at least 1 − δ, for all o, i, the number of non-empty oracleo,i.(cid:98)S1, oracleo,i.(cid:98)S1, ··· , oracleo,i.(cid:98)S100m is at least mai/k.
Notice that the expectation of |oracleo,i.(cid:98)Sj| = ai/k.
Since mai/k ≥ c(cid:48) log(∆d/δ) for a suﬃcient at least 1 − δ, the number of non-empty oracleo,i.(cid:98)S1, oracleo,i.(cid:98)S1, ··· , oracleo,i.(cid:98)S100m is at Claim H.12.
For each i ∈ R, let(cid:80)|Pi| o∗, with probability at least 1 − δ,(cid:80) Proof.
Since i ∈ R, and γ = /(200Ld3ρ).
we know that ai ≥ /(200Ld3ρ) by Deﬁnition H.1. large constant c(cid:48), then by Lemma H.10 and taking union bound over all o, i, we have with probability j=1 |Pi,j| = ai · Ti(o).
For the smallest valid o ∈ (0, OPT], i,e, i(cid:48)∈R ai(cid:48) > 100k.
least mai/k.
10jk(cid:88) 10jk(cid:88) 54 Proof.
Let us ﬁx an o ∈ O, i ∈ {0,··· , L − 1}.
E[memory size for oracleo,i] 100m kTi(o) · (#partition points + #non-partition and non-heavy points) 100m kTi(o) (cid:33) (cid:88) (cid:32) c2m k · #heavy cells in level i ai(cid:48)Ti(cid:48)(o) L(cid:88) ai · Ti(o) + i(cid:48)<i c2m k · #heavy cells in level i i=0 i=0 ai ai + 100m (cid:33) k · (100 + c2) c2m k · #heavy cells in level i (cid:32) L(cid:88) If o is the smallest and(cid:80)L i=0 ai ≤ 100k, then let o(cid:48) = o/2.
Since the probability that each point is sampled blow up by twice, the expected memory only blows up by twice.
Since m is large enough, with probability at least 1 − δ, the actual memory used by o(cid:48) is at most twice of the expectation of used memory size.
It means o(cid:48) is a smaller valid guess, thus it leads to a contradiction to o is the smallest valid guess.
Claim H.13.
Given parameters γ ∈ (0, 1/10) and the smallest valid guess o ∈ (0, OPT].
Let j=1 |Pi,j| = ai · Ti(o).
Let R = {i ∈ {0, 1,··· , L} | m denote the number of samples sampled from all the levels (Algorithm 6).
Then with probability at least 1 − δ, the number of samples sampled from level i is at most mai/(50k).
Proof.
Recall that m is number of samples in Algorithm 6.
We can compute the expectation of number samples choosing level i, j=1 |Pi,j| > γ · Ti(o)}.
For each i ∈ R, let (cid:80)|Pi| (cid:80)|Pi| E[#samples choosing level i] = m · = m · (cid:20) Pr ∃o, i, #samples choosing level i ≥ m · (cid:80) (cid:80) (cid:80) ai(cid:80) i(cid:48)∈R i(cid:48)∈R ai(cid:48) p∈Pi d3/Ti(o) p∈Pi(cid:48) d3/Ti(cid:48)(o) (cid:21) ai(cid:80) i(cid:48)∈R ai(cid:48) · ≤ δ, Since m is suﬃciently large, by using Chernoﬀ bound and taking union bound over all o, i, we have Further using Claim H.12, we have ai(cid:80) i(cid:48)∈R ai(cid:48) · m · 2 ≤ mai 50k Thus, with probability 1 − δ, for all o, i, the number of samples sampled from level i is at most mai/(50k).
55 Lemma H.14.
At the end of one-pass of stream, for each ﬁxed o ≤ OPT, for each i ∈ {0, 1,··· , L}, we know Pi is important or not.
If Pi is γ-important, then we can output Ao,i such that |Pi|(cid:88) j=1 |Pi|(cid:88) j=1 (1 − ) |Pi,j| ≤ Ao,i ≤ (1 + ) |Pi,j| with probability 1 − δ.
Proof.
Note that in the algorithm each point is copied 100m times.
And in the ith level, each point is sampled with probability 1/(Ti(o)k).
Since each heavy cell has at least Ti(o) number of points, then according to Lemma H.9, the number of points in the heavy cell which are sampled is at least (1 − )100m/k.
Thus, at the end of the stream, we are able to identify the heavy cells which means that for each given point, we are able to determine whether it is a partition point, a heavy cell point or a non-partition non-heavy cell point in the ith level.
Now consider a level i which is important, which means that the number of partition points is at least γTi(o).
Recall that γ = /(200Ld3ρ).
Then since m is suﬃciently large, we can apply Lemma H.9 again such that the number of partition points in the level i which are sampled is in the range (1±)100m/(kTi(o))· j=1 |Pi,j|.
j=1 |Pi,j| ≤ Ao,i ≤ (1+)(cid:80)|Pi| (cid:80)|Pi| j=1 |Pi,j|.
Thus, we have (1−)(cid:80)|Pi| (cid:80)|Pi| j=1 |Pi,j| > γ · Ti(o)}.
For each i ∈ R, let (cid:80)|Pi| probability at least 1 − δ,(cid:80) Proof.
We prove by contradiction.
Suppose(cid:80) i∈R ai ≤ O(kL).
Claim H.15.
Given parameters γ ∈ (0, 1/10) and a valid guess o ∈ (0, OPT].
Let R = {i ∈ j=1 |Pi,j| = ai · Ti(o).
Then with {0, 1,··· , L} | i∈R ai ≥ ω(kL).
According to Lemma H.9, the number of partition points stored in our data structure will be at least ω(kL) which contradicts to the total space used by our data structure.
Now in the following, we give the whole proof of our main theorem.
proof of Theorem H.6. It is easy to see the total space used is small: we actually maintained |O| × L number of oracles, and by Lemma E.4, each oracle uses space at most k−3 poly(L, d, log k, log d, log(1/δ)) bits.
Now, let us look at the correctness.
Firstly, we can argue that the algorithm will output a valid o∗ with o∗ < OPT .
Let o ∈ [OPT /2, OPT], we look at level i, the total number of non-heavy cell i /(2d)2) where the ﬁrst term is all the non-heavy cell points points is at most 3kL/ρ·Ti(o)+OPT /(g2 which are in the center cells, and the second term is all the non-heavy cell points which are not i /(2d)2) ≤ 24kTi(o) then we have that the total number of non- in the center cells.
Since OPT /(g2 heavy cell points in the level i is at most 27kLTi(o).
Thus, due to Lemma H.9, the total number of sampled non-heavy cell points is at most O(mL).
Furthermore, due to Lemma D.5, the total number of heavy cells is at most O(kL).
Thus, all the oracles will not FAIL.
Thus, the algorithm will choose an o∗ which is at most OPT .
Since o∗ is a valid guess, then according to Claim H.15, Lemma H.5 and Theorem H.2, m samples is enough to get a good coreset.
In the following of the proof, our goal is to prove that our m samples is actually good.
According to Lemma H.14, we can identify all the important levels.
And according to Lemma H.3, we only need to show that these m samples provide a coreset of the important points.
56 According to Lemma H.14, we can estimate the number of partition points in each important level up to a approximation factor within 1 ± .
Thus, we can sample each important level i with probability proportional to (cid:80) (cid:80) (cid:80) p∈Pi d3/Ti(o) i(cid:48)∈R p∈Pi(cid:48) d3/Ti(cid:48)(o) up to a factor 1 ± .
Now the only thing we remaining to prove is that we can implement uniform sampling over all the partition points for each important level.
Since o∗ is the smallest valid guess of o, Claim H.13 shows that the number of uniform samples needed from level i is upper bounded by mai/(50k).
Due to Claim H.11, the number of uniform samples the corresponding oracle can provided is at least mai/k.
Thus, we can get enough uniform samples.
Thus, we are able to sample m i.i.d. samples such that each point p is chosen with probability proportional to (cid:80) i(cid:48)∈R (1 ± ) (cid:80) d3/Ti(o) p∈Pi(cid:48) d3/Ti(cid:48)(o) where p is the partition point in the level i.
By applying Lemma H.4, Theorem H.2 and Lemma H.3, we complete the proof of the correctness.
57
