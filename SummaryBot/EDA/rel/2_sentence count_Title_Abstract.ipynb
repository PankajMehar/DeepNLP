{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미리 저장했었던 파일로부터 DataFrame을 읽어옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_pickle(\"data_title_source_df.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Training Neural Networks by Using Power Linear...</td>\n",
       "      <td>In this paper, we introduce ”Power Linear Uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hierarchical Aggregation Approach for Distribu...</td>\n",
       "      <td>—In this paper, we present a new approach of d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pretraining Deep Actor-Critic Reinforcement Le...</td>\n",
       "      <td>Pretraining with expert demonstrations have b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Obfuscated Gradients Give a False Sense of Sec...</td>\n",
       "      <td>We identify obfuscated gradients as a phe- no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DxNAT - Deep Neural Networks for Explaining No...</td>\n",
       "      <td>—Non-recurring trafﬁc congestion is caused by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fusarium Damaged Kernels Detection Using Trans...</td>\n",
       "      <td>. The present work shows the application of tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sometimes You Want to Go Where Everybody Knows...</td>\n",
       "      <td>. We introduce a new metric for measuring how ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Algorithmic Linearly Constrained Gaussian Proc...</td>\n",
       "      <td>We algorithmically construct multi-output Gau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A Survey of Recent Advances in Texture Represe...</td>\n",
       "      <td>Texture is a fundamental characteristic of ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RETHINKING THE SMALLER-NORM-LESS-INFORMATIVE A...</td>\n",
       "      <td>Model pruning has become a useful technique t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Learning Combinations of Activation Functions</td>\n",
       "      <td>—In the last decade, an active area of researc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Interpretable Deep Convolutional Neural Networ...</td>\n",
       "      <td>—Model interpretability is a requirement in ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Improving Active Learning in Systematic Reviews</td>\n",
       "      <td>Systematic reviews are essential to summarizi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ALTERNATING MULTI-BIT QUANTIZATION FOR RECURRE...</td>\n",
       "      <td>Recurrent neural networks have achieved excel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Object category learning and retrieval with we...</td>\n",
       "      <td>We consider the problem of retrieving objects...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Less is more: sampling chemical space with act...</td>\n",
       "      <td>The  development  of  accurate  and  transfe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Learning Families of Formal Languages from Pos...</td>\n",
       "      <td>. For 50 years, research in the area of induct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sensitivity Sampling Over Dynamic Geometric Da...</td>\n",
       "      <td>Sensitivity based sampling is crucial for con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Cardiac Arrhythmia Detection from ECG Combinin...</td>\n",
       "      <td>The most common type of arrhythmia is atrial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Scalable L´evy Process Priors for Spectral Ker...</td>\n",
       "      <td>Gaussian processes are rich distributions ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Kernel Distillation for Gaussian Processes</td>\n",
       "      <td>Gaussian processes (GPs) are ﬂexible models t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>On the Inter-relationships among Drift rate, F...</td>\n",
       "      <td>We propose two general and falsiﬁable hypothe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Causal Learning and Explanation of Deep Neural...</td>\n",
       "      <td>Deep neural networks are complex and opaque. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Matrix Completion for Structured Observations</td>\n",
       "      <td>— The need to predict or ﬁll-in missing data, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>On Polynomial time Constructions of Minimum He...</td>\n",
       "      <td>. A decision tree T in Bm := {0, 1}m is a bina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Clustering and Unsupervised Anomaly Detection ...</td>\n",
       "      <td>—Clustering is essential to many tasks in patt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>A Modiﬁed Sigma-Pi-Sigma Neural Network with A...</td>\n",
       "      <td>Sigma-Pi-Sigma neural networks (SPSNNs) as a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Fast Power system security analysis with Guide...</td>\n",
       "      <td>. We propose a new method to eﬃciently compute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>VR Goggles for Robots: Real-to-sim Domain Adap...</td>\n",
       "      <td>This paper deals with the reality gap from a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>An Incremental Path-Following Splitting Method...</td>\n",
       "      <td>The linearly constrained nonconvex nonsmooth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Deep Multi-view Learning to Rank</td>\n",
       "      <td>—We study the problem of learning to rank from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>A Generative Model for Natural Sounds Based on...</td>\n",
       "      <td>. Recent advances in analysis of subband ampli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Matrix completion with deterministic pattern a...</td>\n",
       "      <td>We consider the matrix completion problem wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Spherical CNNs</td>\n",
       "      <td>Convolutional Neural Networks (CNNs) have bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Low-rank Bandit Methods for High-dimensional D...</td>\n",
       "      <td>We consider high dimensional dynamic multi-pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>One-class Collective Anomaly Detection based o...</td>\n",
       "      <td>.  Intrusion  detection for computer network s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Leveraging Adiabatic Quantum Computation for E...</td>\n",
       "      <td>Accurate, reliable sampling from fully-connec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>FASTGCN: FAST LEARNING WITH GRAPH CONVOLU-TION...</td>\n",
       "      <td>The graph convolutional networks (GCN) recent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Analysis of Fast Alternating Minimization for ...</td>\n",
       "      <td>—Methods exploiting sparsity have been popular...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Robustness of classiﬁcation ability of spiking...</td>\n",
       "      <td>It is well-known that the robustness of artiﬁ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>The Intriguing Properties of Model Explanations</td>\n",
       "      <td>Linear approximations to the decision boundar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>DeepLung: Deep 3D Dual Path Nets for Automated...</td>\n",
       "      <td>In this work, we present a fully automated lu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>DeepDTA: Deep Drug-Target Binding Affinity Pre...</td>\n",
       "      <td>The identification of novel drug-target (DT) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>DISCRETE AUTOENCODERS FOR SEQUENCE MODELS</td>\n",
       "      <td>Recurrent models for sequences have been rece...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Fast Binary Compressive Sensing via \\ell_0 Gra...</td>\n",
       "      <td>—We present a fast Compressive Sensing algorit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Optimizing Non-decomposable Measures with Deep...</td>\n",
       "      <td>We present a class of algorithms capable of d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>LINKS: A HIGH-DIMENSIONAL ONLINE CLUSTERING ME...</td>\n",
       "      <td>We present a novel algorithm, called Links, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Deep Neural Networks with Data Dependent Impli...</td>\n",
       "      <td>Though deep neural networks (DNNs) achieve re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>EVALUATING THE ROBUSTNESS OF NEURAL NETWORKS: ...</td>\n",
       "      <td>The robustness of neural networks to adversar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>JointDNN: An Efficient Training and Inference ...</td>\n",
       "      <td>Deep neural networks are among the most influ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Elements of Effective Deep Reinforcement Learn...</td>\n",
       "      <td>Tactical driving decision making is crucial f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Nested LSTMs</td>\n",
       "      <td>JRMONIZ@ANDREW.CMU.EDU DAVID.KRUEGER@UMONTREA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Learning to Emulate an Expert Projective Cone ...</td>\n",
       "      <td>Projective cone scheduling deﬁnes a large cla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>COBRA: A Fast and Simple Method for Active Clu...</td>\n",
       "      <td>Clustering is inherently ill-posed: there oft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Evaluating approaches for supervised semantic ...</td>\n",
       "      <td>Relational data sources are still one of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Nonlinear Dimensionality Reduction on Graphs</td>\n",
       "      <td>—In this era of data deluge, many signal proce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Distributed Newton Methods for Deep Neural</td>\n",
       "      <td>Deep learning involves a difﬁcult non-convex ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Incremental kernel PCA and the Nystr¨om method</td>\n",
       "      <td>Incremental versions of batch algorithms are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>ACCELERATING RECURRENT NEURAL NETWORK LANGUAGE...</td>\n",
       "      <td>This paper presents methods to accelerate rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Learning the Reward Function for a Misspeciﬁed...</td>\n",
       "      <td>In model-based reinforcement learning it is t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               titles  \\\n",
       "0   Training Neural Networks by Using Power Linear...   \n",
       "1   Hierarchical Aggregation Approach for Distribu...   \n",
       "2   Pretraining Deep Actor-Critic Reinforcement Le...   \n",
       "3   Obfuscated Gradients Give a False Sense of Sec...   \n",
       "4   DxNAT - Deep Neural Networks for Explaining No...   \n",
       "5   Fusarium Damaged Kernels Detection Using Trans...   \n",
       "6   Sometimes You Want to Go Where Everybody Knows...   \n",
       "7   Algorithmic Linearly Constrained Gaussian Proc...   \n",
       "8   A Survey of Recent Advances in Texture Represe...   \n",
       "9   RETHINKING THE SMALLER-NORM-LESS-INFORMATIVE A...   \n",
       "10      Learning Combinations of Activation Functions   \n",
       "11  Interpretable Deep Convolutional Neural Networ...   \n",
       "12    Improving Active Learning in Systematic Reviews   \n",
       "13  ALTERNATING MULTI-BIT QUANTIZATION FOR RECURRE...   \n",
       "14  Object category learning and retrieval with we...   \n",
       "15  Less is more: sampling chemical space with act...   \n",
       "16  Learning Families of Formal Languages from Pos...   \n",
       "17  Sensitivity Sampling Over Dynamic Geometric Da...   \n",
       "18  Cardiac Arrhythmia Detection from ECG Combinin...   \n",
       "19  Scalable L´evy Process Priors for Spectral Ker...   \n",
       "20         Kernel Distillation for Gaussian Processes   \n",
       "21  On the Inter-relationships among Drift rate, F...   \n",
       "22  Causal Learning and Explanation of Deep Neural...   \n",
       "23      Matrix Completion for Structured Observations   \n",
       "24  On Polynomial time Constructions of Minimum He...   \n",
       "25  Clustering and Unsupervised Anomaly Detection ...   \n",
       "26  A Modiﬁed Sigma-Pi-Sigma Neural Network with A...   \n",
       "27  Fast Power system security analysis with Guide...   \n",
       "28  VR Goggles for Robots: Real-to-sim Domain Adap...   \n",
       "29  An Incremental Path-Following Splitting Method...   \n",
       "..                                                ...   \n",
       "41                   Deep Multi-view Learning to Rank   \n",
       "42  A Generative Model for Natural Sounds Based on...   \n",
       "43  Matrix completion with deterministic pattern a...   \n",
       "44                                     Spherical CNNs   \n",
       "45  Low-rank Bandit Methods for High-dimensional D...   \n",
       "46  One-class Collective Anomaly Detection based o...   \n",
       "47  Leveraging Adiabatic Quantum Computation for E...   \n",
       "48  FASTGCN: FAST LEARNING WITH GRAPH CONVOLU-TION...   \n",
       "49  Analysis of Fast Alternating Minimization for ...   \n",
       "50  Robustness of classiﬁcation ability of spiking...   \n",
       "51    The Intriguing Properties of Model Explanations   \n",
       "52  DeepLung: Deep 3D Dual Path Nets for Automated...   \n",
       "53  DeepDTA: Deep Drug-Target Binding Affinity Pre...   \n",
       "54          DISCRETE AUTOENCODERS FOR SEQUENCE MODELS   \n",
       "55  Fast Binary Compressive Sensing via \\ell_0 Gra...   \n",
       "56  Optimizing Non-decomposable Measures with Deep...   \n",
       "57  LINKS: A HIGH-DIMENSIONAL ONLINE CLUSTERING ME...   \n",
       "58  Deep Neural Networks with Data Dependent Impli...   \n",
       "59  EVALUATING THE ROBUSTNESS OF NEURAL NETWORKS: ...   \n",
       "60  JointDNN: An Efficient Training and Inference ...   \n",
       "61  Elements of Effective Deep Reinforcement Learn...   \n",
       "62                                       Nested LSTMs   \n",
       "63  Learning to Emulate an Expert Projective Cone ...   \n",
       "64  COBRA: A Fast and Simple Method for Active Clu...   \n",
       "65  Evaluating approaches for supervised semantic ...   \n",
       "66       Nonlinear Dimensionality Reduction on Graphs   \n",
       "67         Distributed Newton Methods for Deep Neural   \n",
       "68     Incremental kernel PCA and the Nystr¨om method   \n",
       "69  ACCELERATING RECURRENT NEURAL NETWORK LANGUAGE...   \n",
       "70  Learning the Reward Function for a Misspeciﬁed...   \n",
       "\n",
       "                                            abstracts  \n",
       "0    In this paper, we introduce ”Power Linear Uni...  \n",
       "1   —In this paper, we present a new approach of d...  \n",
       "2    Pretraining with expert demonstrations have b...  \n",
       "3    We identify obfuscated gradients as a phe- no...  \n",
       "4   —Non-recurring trafﬁc congestion is caused by ...  \n",
       "5   . The present work shows the application of tr...  \n",
       "6   . We introduce a new metric for measuring how ...  \n",
       "7    We algorithmically construct multi-output Gau...  \n",
       "8    Texture is a fundamental characteristic of ma...  \n",
       "9    Model pruning has become a useful technique t...  \n",
       "10  —In the last decade, an active area of researc...  \n",
       "11  —Model interpretability is a requirement in ma...  \n",
       "12   Systematic reviews are essential to summarizi...  \n",
       "13   Recurrent neural networks have achieved excel...  \n",
       "14   We consider the problem of retrieving objects...  \n",
       "15    The  development  of  accurate  and  transfe...  \n",
       "16  . For 50 years, research in the area of induct...  \n",
       "17   Sensitivity based sampling is crucial for con...  \n",
       "18    The most common type of arrhythmia is atrial...  \n",
       "19   Gaussian processes are rich distributions ove...  \n",
       "20   Gaussian processes (GPs) are ﬂexible models t...  \n",
       "21   We propose two general and falsiﬁable hypothe...  \n",
       "22   Deep neural networks are complex and opaque. ...  \n",
       "23  — The need to predict or ﬁll-in missing data, ...  \n",
       "24  . A decision tree T in Bm := {0, 1}m is a bina...  \n",
       "25  —Clustering is essential to many tasks in patt...  \n",
       "26   Sigma-Pi-Sigma neural networks (SPSNNs) as a ...  \n",
       "27  . We propose a new method to eﬃciently compute...  \n",
       "28   This paper deals with the reality gap from a ...  \n",
       "29   The linearly constrained nonconvex nonsmooth ...  \n",
       "..                                                ...  \n",
       "41  —We study the problem of learning to rank from...  \n",
       "42  . Recent advances in analysis of subband ampli...  \n",
       "43   We consider the matrix completion problem wit...  \n",
       "44   Convolutional Neural Networks (CNNs) have bec...  \n",
       "45   We consider high dimensional dynamic multi-pr...  \n",
       "46  .  Intrusion  detection for computer network s...  \n",
       "47   Accurate, reliable sampling from fully-connec...  \n",
       "48   The graph convolutional networks (GCN) recent...  \n",
       "49  —Methods exploiting sparsity have been popular...  \n",
       "50   It is well-known that the robustness of artiﬁ...  \n",
       "51   Linear approximations to the decision boundar...  \n",
       "52   In this work, we present a fully automated lu...  \n",
       "53   The identification of novel drug-target (DT) ...  \n",
       "54   Recurrent models for sequences have been rece...  \n",
       "55  —We present a fast Compressive Sensing algorit...  \n",
       "56   We present a class of algorithms capable of d...  \n",
       "57   We present a novel algorithm, called Links, d...  \n",
       "58   Though deep neural networks (DNNs) achieve re...  \n",
       "59   The robustness of neural networks to adversar...  \n",
       "60   Deep neural networks are among the most influ...  \n",
       "61   Tactical driving decision making is crucial f...  \n",
       "62   JRMONIZ@ANDREW.CMU.EDU DAVID.KRUEGER@UMONTREA...  \n",
       "63   Projective cone scheduling deﬁnes a large cla...  \n",
       "64   Clustering is inherently ill-posed: there oft...  \n",
       "65   Relational data sources are still one of the ...  \n",
       "66  —In this era of data deluge, many signal proce...  \n",
       "67   Deep learning involves a difﬁcult non-convex ...  \n",
       "68   Incremental versions of batch algorithms are ...  \n",
       "69   This paper presents methods to accelerate rec...  \n",
       "70   In model-based reinforcement learning it is t...  \n",
       "\n",
       "[71 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장의 수를 세서 추가하고 싶어서 만든 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getNumSent(x):\n",
    "    return len(sent_tokenize(x[\"abstracts\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장의 수를 세기 위해 만든 getNumSent 함수를로우 단위로 실행하고 그 결과를 \"NSent\" 칼럼에 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"NSent\"] = df.apply(getNumSent,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "      <th>NSent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Training Neural Networks by Using Power Linear...</td>\n",
       "      <td>In this paper, we introduce ”Power Linear Uni...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hierarchical Aggregation Approach for Distribu...</td>\n",
       "      <td>—In this paper, we present a new approach of d...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pretraining Deep Actor-Critic Reinforcement Le...</td>\n",
       "      <td>Pretraining with expert demonstrations have b...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Obfuscated Gradients Give a False Sense of Sec...</td>\n",
       "      <td>We identify obfuscated gradients as a phe- no...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DxNAT - Deep Neural Networks for Explaining No...</td>\n",
       "      <td>—Non-recurring trafﬁc congestion is caused by ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fusarium Damaged Kernels Detection Using Trans...</td>\n",
       "      <td>. The present work shows the application of tr...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sometimes You Want to Go Where Everybody Knows...</td>\n",
       "      <td>. We introduce a new metric for measuring how ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Algorithmic Linearly Constrained Gaussian Proc...</td>\n",
       "      <td>We algorithmically construct multi-output Gau...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A Survey of Recent Advances in Texture Represe...</td>\n",
       "      <td>Texture is a fundamental characteristic of ma...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RETHINKING THE SMALLER-NORM-LESS-INFORMATIVE A...</td>\n",
       "      <td>Model pruning has become a useful technique t...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Learning Combinations of Activation Functions</td>\n",
       "      <td>—In the last decade, an active area of researc...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Interpretable Deep Convolutional Neural Networ...</td>\n",
       "      <td>—Model interpretability is a requirement in ma...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Improving Active Learning in Systematic Reviews</td>\n",
       "      <td>Systematic reviews are essential to summarizi...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ALTERNATING MULTI-BIT QUANTIZATION FOR RECURRE...</td>\n",
       "      <td>Recurrent neural networks have achieved excel...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Object category learning and retrieval with we...</td>\n",
       "      <td>We consider the problem of retrieving objects...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Less is more: sampling chemical space with act...</td>\n",
       "      <td>The  development  of  accurate  and  transfe...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Learning Families of Formal Languages from Pos...</td>\n",
       "      <td>. For 50 years, research in the area of induct...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sensitivity Sampling Over Dynamic Geometric Da...</td>\n",
       "      <td>Sensitivity based sampling is crucial for con...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Cardiac Arrhythmia Detection from ECG Combinin...</td>\n",
       "      <td>The most common type of arrhythmia is atrial...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Scalable L´evy Process Priors for Spectral Ker...</td>\n",
       "      <td>Gaussian processes are rich distributions ove...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Kernel Distillation for Gaussian Processes</td>\n",
       "      <td>Gaussian processes (GPs) are ﬂexible models t...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>On the Inter-relationships among Drift rate, F...</td>\n",
       "      <td>We propose two general and falsiﬁable hypothe...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Causal Learning and Explanation of Deep Neural...</td>\n",
       "      <td>Deep neural networks are complex and opaque. ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Matrix Completion for Structured Observations</td>\n",
       "      <td>— The need to predict or ﬁll-in missing data, ...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>On Polynomial time Constructions of Minimum He...</td>\n",
       "      <td>. A decision tree T in Bm := {0, 1}m is a bina...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Clustering and Unsupervised Anomaly Detection ...</td>\n",
       "      <td>—Clustering is essential to many tasks in patt...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>A Modiﬁed Sigma-Pi-Sigma Neural Network with A...</td>\n",
       "      <td>Sigma-Pi-Sigma neural networks (SPSNNs) as a ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Fast Power system security analysis with Guide...</td>\n",
       "      <td>. We propose a new method to eﬃciently compute...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>VR Goggles for Robots: Real-to-sim Domain Adap...</td>\n",
       "      <td>This paper deals with the reality gap from a ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>An Incremental Path-Following Splitting Method...</td>\n",
       "      <td>The linearly constrained nonconvex nonsmooth ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Deep Multi-view Learning to Rank</td>\n",
       "      <td>—We study the problem of learning to rank from...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>A Generative Model for Natural Sounds Based on...</td>\n",
       "      <td>. Recent advances in analysis of subband ampli...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Matrix completion with deterministic pattern a...</td>\n",
       "      <td>We consider the matrix completion problem wit...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Spherical CNNs</td>\n",
       "      <td>Convolutional Neural Networks (CNNs) have bec...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Low-rank Bandit Methods for High-dimensional D...</td>\n",
       "      <td>We consider high dimensional dynamic multi-pr...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>One-class Collective Anomaly Detection based o...</td>\n",
       "      <td>.  Intrusion  detection for computer network s...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Leveraging Adiabatic Quantum Computation for E...</td>\n",
       "      <td>Accurate, reliable sampling from fully-connec...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>FASTGCN: FAST LEARNING WITH GRAPH CONVOLU-TION...</td>\n",
       "      <td>The graph convolutional networks (GCN) recent...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Analysis of Fast Alternating Minimization for ...</td>\n",
       "      <td>—Methods exploiting sparsity have been popular...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Robustness of classiﬁcation ability of spiking...</td>\n",
       "      <td>It is well-known that the robustness of artiﬁ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>The Intriguing Properties of Model Explanations</td>\n",
       "      <td>Linear approximations to the decision boundar...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>DeepLung: Deep 3D Dual Path Nets for Automated...</td>\n",
       "      <td>In this work, we present a fully automated lu...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>DeepDTA: Deep Drug-Target Binding Affinity Pre...</td>\n",
       "      <td>The identification of novel drug-target (DT) ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>DISCRETE AUTOENCODERS FOR SEQUENCE MODELS</td>\n",
       "      <td>Recurrent models for sequences have been rece...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Fast Binary Compressive Sensing via \\ell_0 Gra...</td>\n",
       "      <td>—We present a fast Compressive Sensing algorit...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Optimizing Non-decomposable Measures with Deep...</td>\n",
       "      <td>We present a class of algorithms capable of d...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>LINKS: A HIGH-DIMENSIONAL ONLINE CLUSTERING ME...</td>\n",
       "      <td>We present a novel algorithm, called Links, d...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Deep Neural Networks with Data Dependent Impli...</td>\n",
       "      <td>Though deep neural networks (DNNs) achieve re...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>EVALUATING THE ROBUSTNESS OF NEURAL NETWORKS: ...</td>\n",
       "      <td>The robustness of neural networks to adversar...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>JointDNN: An Efficient Training and Inference ...</td>\n",
       "      <td>Deep neural networks are among the most influ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Elements of Effective Deep Reinforcement Learn...</td>\n",
       "      <td>Tactical driving decision making is crucial f...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Nested LSTMs</td>\n",
       "      <td>JRMONIZ@ANDREW.CMU.EDU DAVID.KRUEGER@UMONTREA...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Learning to Emulate an Expert Projective Cone ...</td>\n",
       "      <td>Projective cone scheduling deﬁnes a large cla...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>COBRA: A Fast and Simple Method for Active Clu...</td>\n",
       "      <td>Clustering is inherently ill-posed: there oft...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Evaluating approaches for supervised semantic ...</td>\n",
       "      <td>Relational data sources are still one of the ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Nonlinear Dimensionality Reduction on Graphs</td>\n",
       "      <td>—In this era of data deluge, many signal proce...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Distributed Newton Methods for Deep Neural</td>\n",
       "      <td>Deep learning involves a difﬁcult non-convex ...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Incremental kernel PCA and the Nystr¨om method</td>\n",
       "      <td>Incremental versions of batch algorithms are ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>ACCELERATING RECURRENT NEURAL NETWORK LANGUAGE...</td>\n",
       "      <td>This paper presents methods to accelerate rec...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Learning the Reward Function for a Misspeciﬁed...</td>\n",
       "      <td>In model-based reinforcement learning it is t...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               titles  \\\n",
       "0   Training Neural Networks by Using Power Linear...   \n",
       "1   Hierarchical Aggregation Approach for Distribu...   \n",
       "2   Pretraining Deep Actor-Critic Reinforcement Le...   \n",
       "3   Obfuscated Gradients Give a False Sense of Sec...   \n",
       "4   DxNAT - Deep Neural Networks for Explaining No...   \n",
       "5   Fusarium Damaged Kernels Detection Using Trans...   \n",
       "6   Sometimes You Want to Go Where Everybody Knows...   \n",
       "7   Algorithmic Linearly Constrained Gaussian Proc...   \n",
       "8   A Survey of Recent Advances in Texture Represe...   \n",
       "9   RETHINKING THE SMALLER-NORM-LESS-INFORMATIVE A...   \n",
       "10      Learning Combinations of Activation Functions   \n",
       "11  Interpretable Deep Convolutional Neural Networ...   \n",
       "12    Improving Active Learning in Systematic Reviews   \n",
       "13  ALTERNATING MULTI-BIT QUANTIZATION FOR RECURRE...   \n",
       "14  Object category learning and retrieval with we...   \n",
       "15  Less is more: sampling chemical space with act...   \n",
       "16  Learning Families of Formal Languages from Pos...   \n",
       "17  Sensitivity Sampling Over Dynamic Geometric Da...   \n",
       "18  Cardiac Arrhythmia Detection from ECG Combinin...   \n",
       "19  Scalable L´evy Process Priors for Spectral Ker...   \n",
       "20         Kernel Distillation for Gaussian Processes   \n",
       "21  On the Inter-relationships among Drift rate, F...   \n",
       "22  Causal Learning and Explanation of Deep Neural...   \n",
       "23      Matrix Completion for Structured Observations   \n",
       "24  On Polynomial time Constructions of Minimum He...   \n",
       "25  Clustering and Unsupervised Anomaly Detection ...   \n",
       "26  A Modiﬁed Sigma-Pi-Sigma Neural Network with A...   \n",
       "27  Fast Power system security analysis with Guide...   \n",
       "28  VR Goggles for Robots: Real-to-sim Domain Adap...   \n",
       "29  An Incremental Path-Following Splitting Method...   \n",
       "..                                                ...   \n",
       "41                   Deep Multi-view Learning to Rank   \n",
       "42  A Generative Model for Natural Sounds Based on...   \n",
       "43  Matrix completion with deterministic pattern a...   \n",
       "44                                     Spherical CNNs   \n",
       "45  Low-rank Bandit Methods for High-dimensional D...   \n",
       "46  One-class Collective Anomaly Detection based o...   \n",
       "47  Leveraging Adiabatic Quantum Computation for E...   \n",
       "48  FASTGCN: FAST LEARNING WITH GRAPH CONVOLU-TION...   \n",
       "49  Analysis of Fast Alternating Minimization for ...   \n",
       "50  Robustness of classiﬁcation ability of spiking...   \n",
       "51    The Intriguing Properties of Model Explanations   \n",
       "52  DeepLung: Deep 3D Dual Path Nets for Automated...   \n",
       "53  DeepDTA: Deep Drug-Target Binding Affinity Pre...   \n",
       "54          DISCRETE AUTOENCODERS FOR SEQUENCE MODELS   \n",
       "55  Fast Binary Compressive Sensing via \\ell_0 Gra...   \n",
       "56  Optimizing Non-decomposable Measures with Deep...   \n",
       "57  LINKS: A HIGH-DIMENSIONAL ONLINE CLUSTERING ME...   \n",
       "58  Deep Neural Networks with Data Dependent Impli...   \n",
       "59  EVALUATING THE ROBUSTNESS OF NEURAL NETWORKS: ...   \n",
       "60  JointDNN: An Efficient Training and Inference ...   \n",
       "61  Elements of Effective Deep Reinforcement Learn...   \n",
       "62                                       Nested LSTMs   \n",
       "63  Learning to Emulate an Expert Projective Cone ...   \n",
       "64  COBRA: A Fast and Simple Method for Active Clu...   \n",
       "65  Evaluating approaches for supervised semantic ...   \n",
       "66       Nonlinear Dimensionality Reduction on Graphs   \n",
       "67         Distributed Newton Methods for Deep Neural   \n",
       "68     Incremental kernel PCA and the Nystr¨om method   \n",
       "69  ACCELERATING RECURRENT NEURAL NETWORK LANGUAGE...   \n",
       "70  Learning the Reward Function for a Misspeciﬁed...   \n",
       "\n",
       "                                            abstracts  NSent  \n",
       "0    In this paper, we introduce ”Power Linear Uni...     10  \n",
       "1   —In this paper, we present a new approach of d...      7  \n",
       "2    Pretraining with expert demonstrations have b...      8  \n",
       "3    We identify obfuscated gradients as a phe- no...      5  \n",
       "4   —Non-recurring trafﬁc congestion is caused by ...     10  \n",
       "5   . The present work shows the application of tr...      4  \n",
       "6   . We introduce a new metric for measuring how ...      9  \n",
       "7    We algorithmically construct multi-output Gau...      5  \n",
       "8    Texture is a fundamental characteristic of ma...      5  \n",
       "9    Model pruning has become a useful technique t...      7  \n",
       "10  —In the last decade, an active area of researc...      4  \n",
       "11  —Model interpretability is a requirement in ma...      9  \n",
       "12   Systematic reviews are essential to summarizi...      9  \n",
       "13   Recurrent neural networks have achieved excel...     12  \n",
       "14   We consider the problem of retrieving objects...      5  \n",
       "15    The  development  of  accurate  and  transfe...     10  \n",
       "16  . For 50 years, research in the area of induct...      8  \n",
       "17   Sensitivity based sampling is crucial for con...      5  \n",
       "18    The most common type of arrhythmia is atrial...      8  \n",
       "19   Gaussian processes are rich distributions ove...      8  \n",
       "20   Gaussian processes (GPs) are ﬂexible models t...      6  \n",
       "21   We propose two general and falsiﬁable hypothe...      6  \n",
       "22   Deep neural networks are complex and opaque. ...      6  \n",
       "23  — The need to predict or ﬁll-in missing data, ...     12  \n",
       "24  . A decision tree T in Bm := {0, 1}m is a bina...     33  \n",
       "25  —Clustering is essential to many tasks in patt...      8  \n",
       "26   Sigma-Pi-Sigma neural networks (SPSNNs) as a ...      9  \n",
       "27  . We propose a new method to eﬃciently compute...      5  \n",
       "28   This paper deals with the reality gap from a ...      6  \n",
       "29   The linearly constrained nonconvex nonsmooth ...      9  \n",
       "..                                                ...    ...  \n",
       "41  —We study the problem of learning to rank from...      6  \n",
       "42  . Recent advances in analysis of subband ampli...      7  \n",
       "43   We consider the matrix completion problem wit...      6  \n",
       "44   Convolutional Neural Networks (CNNs) have bec...      8  \n",
       "45   We consider high dimensional dynamic multi-pr...      4  \n",
       "46  .  Intrusion  detection for computer network s...     15  \n",
       "47   Accurate, reliable sampling from fully-connec...      5  \n",
       "48   The graph convolutional networks (GCN) recent...      8  \n",
       "49  —Methods exploiting sparsity have been popular...      6  \n",
       "50   It is well-known that the robustness of artiﬁ...      9  \n",
       "51   Linear approximations to the decision boundar...      4  \n",
       "52   In this work, we present a fully automated lu...      8  \n",
       "53   The identification of novel drug-target (DT) ...      9  \n",
       "54   Recurrent models for sequences have been rece...      8  \n",
       "55  —We present a fast Compressive Sensing algorit...      3  \n",
       "56   We present a class of algorithms capable of d...      5  \n",
       "57   We present a novel algorithm, called Links, d...      3  \n",
       "58   Though deep neural networks (DNNs) achieve re...      7  \n",
       "59   The robustness of neural networks to adversar...      7  \n",
       "60   Deep neural networks are among the most influ...      8  \n",
       "61   Tactical driving decision making is crucial f...      4  \n",
       "62   JRMONIZ@ANDREW.CMU.EDU DAVID.KRUEGER@UMONTREA...      5  \n",
       "63   Projective cone scheduling deﬁnes a large cla...      9  \n",
       "64   Clustering is inherently ill-posed: there oft...      6  \n",
       "65   Relational data sources are still one of the ...      8  \n",
       "66  —In this era of data deluge, many signal proce...      6  \n",
       "67   Deep learning involves a difﬁcult non-convex ...     12  \n",
       "68   Incremental versions of batch algorithms are ...      4  \n",
       "69   This paper presents methods to accelerate rec...      7  \n",
       "70   In model-based reinforcement learning it is t...      6  \n",
       "\n",
       "[71 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 abstract의 문장수에 대한 간단한 통계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    71.000000\n",
       "mean      7.323944\n",
       "std       3.875840\n",
       "min       3.000000\n",
       "25%       5.000000\n",
       "50%       7.000000\n",
       "75%       8.500000\n",
       "max      33.000000\n",
       "Name: NSent, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.NSent.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어의 수를 세기 위해 문장 단위 데이터와 단어 단위 데이터를 만들었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sentences=[]\n",
    "word_from_sentences=[]\n",
    "for onep in df.abstracts:\n",
    "    src_sentences = src_sentences + sent_tokenize(onep)\n",
    "    temp=[]\n",
    "    for oneSent in sent_tokenize(onep):\n",
    "        temp.append(word_tokenize(oneSent))\n",
    "    word_from_sentences.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "의도한 대로 단어단위와 문장단위 데이터가 만들어졌는 지 체크하는 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_from_sentences[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' In this paper, we introduce ”Power Linear Unit” (PoLU) which increases the nonlinearity capacity of a neural net- work and thus helps improving its performance.',\n",
       " 'PoLU adopts several advantages of previously proposed activa- tion functions.',\n",
       " 'First, the output of PoLU for positive inputs is designed to be identity to avoid the gradient vanishing problem.',\n",
       " 'Second, PoLU has a non-zero output for negative inputs such that the output mean of the units is close to zero, hence reducing the bias shift effect.',\n",
       " 'Thirdly, there is a satu- ration on the negative part of PoLU, which makes it more noise-robust for negative inputs.',\n",
       " 'Furthermore, we prove that PoLU is able to map more portions of every layer’s in- put to the same space by using the power function and thus increases the number of response regions of the neural net- work.',\n",
       " 'We use image classiﬁcation for comparing our pro- posed activation function with others.',\n",
       " 'In the experiments, MNIST, CIFAR-10, CIFAR-100, Street View House Numbers (SVHN) and ImageNet are used as benchmark datasets.',\n",
       " 'The neural networks we implemented include widely-used ELU- Network, ResNet-50, and VGG16, plus a couple of shallow networks.',\n",
       " 'Experimental results show that our proposed ac- tivation function outperforms other state-of-the-art models with most networks.',\n",
       " '—In this paper, we present a new approach of distributed clustering for spatial datasets, based on an innovative and efﬁcient aggregation technique.',\n",
       " 'This distributed approach consists of two phases: 1) local clustering phase, where each node performs a clustering on its local data, 2) aggregation phase, where the local clusters are aggregated to produce global clusters.',\n",
       " 'This approach is characterised by the fact that the local clusters are represented in a simple and efﬁcient way.',\n",
       " 'And The aggregation phase is designed in such a way that the ﬁnal clusters are compact and accurate while the overall process is efﬁcient in both response time and memory allocation.',\n",
       " 'We evaluated the approach with different datasets and compared it to well-known clustering techniques.',\n",
       " 'The experimental results show that our approach is very promising and outperforms all those algorithms.',\n",
       " 'Keywords—Big Data, spatial data, clustering, distributed min- ing, data analysis, k-means, DBSCAN, balance vector.',\n",
       " ' Pretraining with expert demonstrations have been found useful in speeding up the training process of deep reinforcement learning algorithms since less online simulation data is required.',\n",
       " 'Some people use supervised learning to speed up the process of fea- ture learning, others pretrain the policies by imitat- ing expert demonstrations.',\n",
       " 'However, these meth- ods are unstable and not suitable for actor-critic reinforcement learning algorithms.',\n",
       " 'Also, some existing methods rely on the global optimum as- sumption, which is not true in most scenarios.',\n",
       " 'In this paper, we employ expert demonstrations in a actor-critic reinforcement learning framework, and meanwhile ensure that the performance is not af- fected by the fact that expert demonstrations are not global optimal.',\n",
       " 'We theoretically derive a method for computing policy gradients and value estima- tors with only expert demonstrations.',\n",
       " 'Our method is theoretically plausible for actor-critic reinforce- ment learning algorithms that pretrains both policy and value functions.',\n",
       " 'We apply our method to two of the typical actor-critic reinforcement learning al- gorithms, DDPG and ACER, and demonstrate with experiments that our method not only outperforms the RL algorithms without pretraining process, but also is more simulation efﬁcient.',\n",
       " ' We identify obfuscated gradients as a phe- nomenon that leads to a false sense of security in defenses against adversarial examples.',\n",
       " 'While defenses that cause obfuscated gradients appear to defeat optimization-based attacks, we ﬁnd de- fenses relying on this effect can be circumvented.',\n",
       " 'For each of the three types of obfuscated gradients we discover, we describe indicators of defenses ex- hibiting this effect and develop attack techniques to overcome it.',\n",
       " 'In a case study, examining all defenses accepted to ICLR 2018, we ﬁnd obfus- cated gradients are a common occurrence, with 7 of 8 defenses relying on obfuscated gradients.',\n",
       " 'Using our new attack techniques, we successfully circumvent all 7 of them.',\n",
       " '—Non-recurring trafﬁc congestion is caused by tem- porary disruptions, such as accidents, sports games, adverse weather, etc.',\n",
       " 'We use data related to real-time trafﬁc speed, jam factors (a trafﬁc congestion indicator), and events collected over a year from Nashville, TN to train a multi-layered deep neural network.',\n",
       " 'The trafﬁc dataset contains over 900 million data records.',\n",
       " 'The network is thereafter used to classify the real- time data and identify anomalous operations.',\n",
       " 'Compared with traditional approaches of using statistical or machine learning techniques, our model reaches an accuracy of 98.73 percent when identifying trafﬁc congestion caused by football games.',\n",
       " 'Our approach ﬁrst encodes the trafﬁc across a region as a scaled image.',\n",
       " 'After that the image data from different timestamps is fused with event- and time-related data.',\n",
       " 'Then a crossover operator is used as a data augmentation method to generate training datasets with more balanced classes.',\n",
       " 'Finally, we use the receiver operating characteristic (ROC) analysis to tune the sensitivity of the classiﬁer.',\n",
       " 'We present the analysis of the training time and the inference time separately.',\n",
       " '.',\n",
       " 'The present work shows the application of transfer learning for a pre-trained deep neural network (DNN), using a small image dataset (≈ 12,000) on a single workstation with enabled NVIDIA GPU card that takes up to 1 hour to complete the training task and archive an overall average accuracy of 94.7%.',\n",
       " 'The DNN presents a 20% score of misclassiﬁcation for an external test dataset.',\n",
       " 'The accuracy of the proposed methodology is equivalent to ones using HSI methodology (81%-91%) used for the same task, but with the advantage of being independent on special equipment to classify wheat kernel for FHB symptoms.',\n",
       " '.',\n",
       " 'We introduce a new metric for measuring how well a model personalizes to a user’s speciﬁc preferences.',\n",
       " 'We deﬁne personalization as a weighting between performance on user speciﬁc data and performance on a more general global dataset that represents many diﬀerent users.',\n",
       " 'This global term serves as a form of regularization that forces us to not overﬁt to individual users who have small amounts of data.',\n",
       " 'In order to protect user privacy, we add the constraint that we may not centralize or share user data.',\n",
       " 'We also contribute a simple experiment in which we simulate classifying sentiment for users with very distinct vocabularies.',\n",
       " 'This experiment functions as an example of the tension between doing well globally on all users, and doing well on any speciﬁc individual user.',\n",
       " 'It also provides a concrete example of how to employ our new metric to help reason about and resolve this tension.',\n",
       " 'We hope this work can help frame and ground future work into personalization.',\n",
       " ' We algorithmically construct multi-output Gaus- sian process priors which satisfy linear differential equations.',\n",
       " 'Our approach attempts to parametrize all solutions of the equations using Gr¨obner bases.',\n",
       " 'If successful, a push forward Gaussian process along the paramerization is the desired prior.',\n",
       " 'We consider several examples, among them the full inhomogeneous system of Maxwell’s equations.',\n",
       " 'By bringing together stochastic learning and com- puteralgebra in a novel way, we combine noisy observations with precise algebraic computations.',\n",
       " ' Texture is a fundamental characteristic of many types of images, and texture representation is one of the essential and challenging problems in computer vision and pattern recognition which has attracted extensive research attention.',\n",
       " 'Since 2000, tex- ture representations based on Bag of Words (BoW) and on Con- volutional Neural Networks (CNNs) have been extensively stud- ied with impressive performance.',\n",
       " 'Given this period of remarkable evolution, this paper aims to present a comprehensive survey of advances in texture representation over the last two decades.',\n",
       " 'More than 200 major publications are cited in this survey covering dif- ferent aspects of the research, which includes (i) problem descrip- tion; (ii) recent advances in the broad categories of BoW-based, CNN-based and attribute-based methods; and (iii) evaluation is- sues, speciﬁcally benchmark datasets and state of the art results.',\n",
       " 'In retrospect of what has been achieved so far, the survey discusses open challenges and directions for future research.',\n",
       " ' Model pruning has become a useful technique that improves the computational efﬁciency of deep learning, making it possible to deploy solutions on resource- limited scenarios.',\n",
       " 'A widely-used practice in relevant work assumes that a smaller- norm parameter or feature plays a less informative role at the inference time.',\n",
       " 'In this paper, we propose a channel pruning technique for accelerating the compu- tations of deep convolutional neural networks (CNNs), which does not critically rely on this assumption.',\n",
       " 'Instead, it focuses on direct simpliﬁcation of the channel- to-channel computation graph of a CNN without the need of performing a compu- tational difﬁcult and not always useful task of making high-dimensional tensors of CNN structured sparse.',\n",
       " 'Our approach takes two stages: the ﬁrst being to adopt an end-to-end stochastic training method that eventually forces the outputs of some channels being constant, and the second being to prune those constant channels from the original neural network by adjusting the biases of their impacting layers such that the resulting compact model can be quickly ﬁne-tuned.',\n",
       " 'Our approach is mathematically appealing from an optimization perspective and easy to repro- duce.',\n",
       " 'We experimented our approach through several image learning benchmarks and demonstrate its interesting aspects and the competitive performance.',\n",
       " '—In the last decade, an active area of research has been devoted to design novel activation functions that are able to help deep neural networks to converge, obtaining better per- formance.',\n",
       " 'The training procedure of these architectures usually involves optimization of the weights of their layers only, while non-linearities are generally pre-speciﬁed and their (possible) parameters are usually considered as hyper-parameters to be tuned manually.',\n",
       " 'In this paper, we introduce two approaches to automatically learn different combinations of base activation functions (such as the identity function, ReLU, and tanh) during the training phase.',\n",
       " 'We present a thorough comparison of our novel approaches with well-known architectures (such as LeNet- 5, AlexNet, and ResNet-56) on three standard datasets (Fashion- MNIST, CIFAR-10, and ILSVRC-2012), showing substantial improvements in the overall performance, such as an increase in the top-1 accuracy for AlexNet on ILSVRC-2012 of 3.01 percentage points.',\n",
       " '—Model interpretability is a requirement in many applications in which crucial decisions are made by users relying on a model’s outputs.',\n",
       " 'The recent movement for “al- gorithmic fairness” also stipulates explainability, and therefore interpretability of learning models.',\n",
       " 'And yet the most successful contemporary Machine Learning approaches, the Deep Neural Networks, produce models that are highly non-interpretable.',\n",
       " 'We attempt to address this challenge by proposing a technique called CNN-INTE to interpret deep Convolutional Neural Networks (CNN) via meta-learning.',\n",
       " 'In this work, we interpret a speciﬁc hidden layer of the deep CNN model on the MNIST image dataset.',\n",
       " 'We use a clustering algorithm in a two-level structure to ﬁnd the meta-level training data and Random Forest as base learning algorithms to generate the meta-level test data.',\n",
       " 'The interpretation results are displayed visually via diagrams, which clearly indicates how a speciﬁc test instance is classiﬁed.',\n",
       " 'Our method achieves global interpretation for all the test instances without sacriﬁcing the accuracy obtained by the original deep CNN model.',\n",
       " 'This means our model is faithful to the deep CNN model, which leads to reliable interpretations.',\n",
       " ' Systematic reviews are essential to summarizing the results of diﬀerent clinical and social science studies.',\n",
       " 'The ﬁrst step in a systematic review task is to identify all the studies rel- evant to the review.',\n",
       " 'The task of identifying relevant studies for a given systematic review is usually performed manually, and as a result, involves substantial amounts of expensive human resource.',\n",
       " 'Lately, there have been some attempts to reduce this manual eﬀort using active learning.',\n",
       " 'In this work, we build upon some such existing techniques, and validate by experimenting on a larger and comprehensive dataset than has been attempted until now.',\n",
       " 'Our experiments provide in- sights on the use of diﬀerent feature extraction models for diﬀerent disciplines.',\n",
       " 'More importantly, we identify that a naive active learning based screening process is biased in favour of selecting similar documents.',\n",
       " 'We aimed to improve the performance of the screening process using a novel active learning algorithm with success.',\n",
       " 'Additionally, we propose a mechanism to choose the best feature extraction method for a given review.',\n",
       " ' Recurrent neural networks have achieved excellent performance in many applica- tions.',\n",
       " 'However, on portable devices with limited resources, the models are often too large to deploy.',\n",
       " 'For applications on the server with large scale concurrent requests, the latency during inference can also be very critical for costly computing resources.',\n",
       " 'In this work, we address these problems by quantizing the network, both weights and activations, into multiple binary codes {−1, +1}.',\n",
       " 'We formulate the quantization as an optimization problem.',\n",
       " 'Under the key observation that once the quantization coefﬁcients are ﬁxed the binary codes can be derived efﬁciently by binary search tree, alternating minimization is then applied.',\n",
       " 'We test the quantiza- tion for two well-known RNNs, i.e., long short term memory (LSTM) and gated recurrent unit (GRU), on the language models.',\n",
       " 'Compared with the full-precision counter part, by 2-bit quantization we can achieve ∼16× memory saving and ∼6× real inference acceleration on CPUs, with only a reasonable loss in the accuracy.',\n",
       " 'By 3-bit quantization, we can achieve almost no loss in the accuracy or even surpass the original model, with ∼10.5× memory saving and ∼3× real inference acceleration.',\n",
       " 'Both results beat the exiting quantization works with large margins.',\n",
       " 'We extend our alternating quantization to image classiﬁcation tasks.',\n",
       " 'In both RNNs and feedforward neural networks, the method also achieves excellent performance.',\n",
       " ' We consider the problem of retrieving objects from image data and learning to classify them into meaningful semantic categories with minimal supervision.',\n",
       " 'To that end, we propose a fully differentiable unsupervised deep clustering approach to learn semantic classes in an end-to-end fashion without individual class labeling using only unlabeled object proposals.',\n",
       " 'The key contributions of our work are 1) a kmeans clustering objective where the clusters are learned as parameters of the network and are represented as memory units, and 2) simultaneously building a feature representation, or embedding, while learning to cluster it.',\n",
       " 'This approach shows promising results on two popular computer vision datasets: on CIFAR10 for clustering objects, and on the more complex and challenging Cityscapes dataset for semantically discovering classes which visually correspond to cars, people, and bicycles.',\n",
       " 'Currently, the only supervision provided is segmentation objectness masks, but this method can be extended to use an unsupervised objectness-based object generation mechanism which will make the approach completely unsupervised.',\n",
       " '  The  development  of  accurate  and  transferable  machine learning  (ML)  potentials  for predicting  molecular  energetics  is  a  challenging  task.',\n",
       " 'The  process  of  data  generation  to  train  such  ML  potentials is a task neither well understood nor researched in detail.',\n",
       " 'In this work, we present a fully  automated  approach  for  the  generation  of  datasets  with  the  intent  of  training  universal  ML  potentials.',\n",
       " 'It  is based on the concept  of active learning  (AL) via Query by Committee (QBC),  which uses the disagreement between an ensemble of ML potentials to infer the reliability of the  ensemble’s prediction.',\n",
       " 'QBC allows our AL algorithm to automatically sample regions of chemical  space  where  the  machine  learned  potential  fails  to  accurately  predict  the  potential  energy.',\n",
       " 'AL  improves the overall fitness of ANAKIN-ME (ANI) deep learning potentials in rigorous test cases  by mitigating human biases in deciding what new training data to use.',\n",
       " 'AL also reduces the training  set size to a fraction of the data required when using naive random sampling techniques.',\n",
       " 'To provide  validation of our AL approach we develop the COMP6 benchmark (publicly available on GitHub),  which contains a diverse set of organic molecules.',\n",
       " 'We show the use of our proposed AL technique  develops  a  universal  ANI  potential  (ANI-1x),  which  provides  very  accurate  energy  and  force  predictions on the entire COMP6 benchmark.',\n",
       " 'This universal potential achieves a level of accuracy  on par with the best ML potentials for single molecule or materials, while remaining applicable to  the general class of organic molecules comprised of the elements CHNO.',\n",
       " '.',\n",
       " 'For 50 years, research in the area of inductive inference aims at investigating the learning of formal languages and is inﬂuenced by computability theory, complexity theory, cognitive science, machine learn- ing, and more generally artiﬁcial intelligence.',\n",
       " 'Being one of the pioneers, Gold [1967] investigated the most common formalization, learning in the limit both from solely positive examples as well as from positive and negative information.',\n",
       " 'The ﬁrst mode of presentation has been studied extensively, including insights in how diﬀerent additional requirements on the hypothesis sequence of the learner or requested properties of the latter itself, restrict what collections of languages are learnable.',\n",
       " 'We focus on the second paradigm, learning from informants, and study how imposing diﬀerent restrictions on the learning process eﬀects learn- ability.',\n",
       " 'For example, we show that learners can be assumed to only change their hypothesis in case it is inconsistent with the data (such learners are called conservative).',\n",
       " 'Further, we give a picture of how the most important learning restrictions relate.',\n",
       " 'Our investigations underpin the claim for delayability being the right structural property to gain a deeper understanding concerning the nature of learning restrictions.',\n",
       " ' Sensitivity based sampling is crucial for constructing nearly-optimal coreset for k-means / median clustering.',\n",
       " 'In this paper, we provide a novel data structure that enables sensitivity sampling over a dynamic data stream, where points from a high dimensional discrete Euclidean space can be either inserted or deleted.',\n",
       " 'Based on this data structure, we provide a one-pass coreset construction for k-means clustering using space (cid:101)O(k poly(d)) over d-dimensional geo- metric dynamic data streams.',\n",
       " 'While previous best known result is only for k-median[BFL+17], which cannot be directly generalized to k-means to obtain algorithms with space nearly linear in k. To the best of our knowledge, our algorithm is the ﬁrst dynamic geometric data stream algorithm for k-means using space polynomial in dimension and nearly optimal in k. We further show that our data structure for maintaining coreset can be extended as a uniﬁed approach for a more general classes of k-clustering, including k-median, M-estimator clustering, and clusterings with a more general set of cost functions over distances.',\n",
       " 'For all these tasks, the space/time of our algorithm is similar to k-means with only poly(d) factor diﬀerence.',\n",
       " '  The most common type of arrhythmia is atrial fibrillation  (AF), which causes an irregular and fast heartbeat [1].',\n",
       " 'Objectives: Atrial fibrillation  (AF) is a common heart  rhythm  disorder  associated  with  deadly  and  debilitating  consequences including heart failure, stroke, poor mental  health,  reduced  quality  of  life  and  death.',\n",
       " 'Having  an  automatic system that diagnoses various types of cardiac  arrhythmias  would  assist  cardiologists  initiate  appropriate  preventive  measures  and  to  improve  the  analysis  of  cardiac  disease.',\n",
       " 'To  this  end,  this  paper  introduces  a  new  approach  to  detect  and  classify  automatically cardiac arrhythmias in electrocardiograms  (ECG) recordings.',\n",
       " 'to  Methods: The proposed approach used a combination  of Convolution Neural Networks (CNNs) and a sequence  of Long Short-Term Memory (LSTM) units, with pooling,  dropout  and  normalization  techniques  to  improve  their  accuracy.',\n",
       " 'The network predicted a classification at every  18th input sample and we selected the final prediction for  classification.',\n",
       " 'Results  were  cross-validated  on  the  Physionet  Challenge  2017  training  dataset,  which  contains  8,528  single  lead  ECG  recordings  lasting  from  9s to just over 60s.',\n",
       " 'Results:  Using  the  proposed  structure  and  no  explicit  feature  selection,  10-fold  stratified  cross-validation  gave  an  overall  F-measure  of  0.83.10±0.015  on  the  held-out  test  data  (mean  ±  standard  deviation  over  all  folds)  and  0.80 on the hidden dataset of the Challenge entry server.',\n",
       " ' Gaussian processes are rich distributions over functions, with generalization prop- erties determined by a kernel function.',\n",
       " 'When used for long-range extrapolation, predictions are particularly sensitive to the choice of kernel parameters.',\n",
       " 'It is therefore critical to account for kernel uncertainty in our predictive distributions.',\n",
       " 'We propose a distribution over kernels formed by modelling a spectral mixture density with a L´evy process.',\n",
       " 'The resulting distribution has support for all sta- tionary covariances—including the popular RBF, periodic, and Mat´ern kernels— combined with inductive biases which enable automatic and data efﬁcient learn- ing, long-range extrapolation, and state of the art predictive performance.',\n",
       " 'The proposed model also presents an approach to spectral regularization, as the L´evy process introduces a sparsity-inducing prior over mixture components, allowing automatic selection over model order and pruning of extraneous components.',\n",
       " 'We exploit the algebraic structure of the proposed process for O(n) training and O(1) predictions.',\n",
       " 'We perform extrapolations having reasonable uncertainty estimates on several benchmarks, show that the proposed model can recover ﬂexible ground truth covariances and that it is robust to errors in initialization.',\n",
       " ' Gaussian processes (GPs) are ﬂexible models that can capture complex structure in large-scale dataset due to their non-parametric nature.',\n",
       " 'However, the usage of GPs in real-world application is limited due to their high computational cost at inference time.',\n",
       " 'In this paper, we introduce a new frame- work, kernel distillation, for kernel matrix approx- imation.',\n",
       " 'The idea adopts from knowledge distilla- tion in deep learning community, where we approx- imate a fully trained teacher kernel matrix of size n × n with a student kernel matrix.',\n",
       " 'We combine inducing points method with sparse low-rank ap- proximation in the distillation procedure.',\n",
       " 'The dis- tilled student kernel matrix only cost O(m2) stor- age where m is the number of inducing points and m (cid:28) n. We also show that one application of kernel distillation is for fast GP prediction, where we demonstrate empirically that our approximation provide better balance between the prediction time and the predictive performance compared to the al- ternatives.',\n",
       " ' We propose two general and falsiﬁable hypotheses about expecta- tions on generalization error when learning in the context of concept drift.',\n",
       " 'One posits that as drift rate increases, the forgetting rate that minimizes generaliza- tion error will also increase and vice versa.',\n",
       " 'The other posits that as a learner’s forgetting rate increases, the bias/variance proﬁle that minimizes generaliza- tion error will have lower variance and vice versa.',\n",
       " 'These hypotheses lead to the concept of the sweet path, a path through the 3-d space of alternative drift rates, forgetting rates and bias/variance proﬁles on which generalization er- ror will be minimized, such that slow drift is coupled with low forgetting and low bias, while rapid drift is coupled with fast forgetting and low variance.',\n",
       " 'We present experiments that support the existence of such a sweet path.',\n",
       " 'We also demonstrate that simple learners that select appropriate forgetting rates and bias/variance proﬁles are highly competitive with the state-of-the-art in incremental learners for concept drift on real-world drift problems.',\n",
       " ' Deep neural networks are complex and opaque.',\n",
       " 'As they enter application in a variety of impor- tant and safety critical domains, users seek meth- ods to explain their output predictions.',\n",
       " 'We develop an approach to explaining deep neural networks by constructing causal models on salient concepts contained in a CNN.',\n",
       " 'We develop methods to ex- tract salient concepts throughout a target network by using autoencoders trained to extract human- understandable representations of network activa- tions.',\n",
       " 'We then build a bayesian causal model us- ing these extracted concepts as variables in order to explain image classiﬁcation.',\n",
       " 'Finally, we use this causal model to identify and visualize features with signiﬁcant causal inﬂuence on ﬁnal classiﬁcation.',\n",
       " '— The need to predict or ﬁll-in missing data, often referred to as matrix completion, is a common challenge in to- day’s data-driven world.',\n",
       " 'Previous strategies typically assume that no structural difference between observed and missing entries exists.',\n",
       " 'Unfortunately, this assumption is woefully unrealistic in many applications.',\n",
       " 'For example, in the classic Netﬂix challenge, in which one hopes to predict user-movie ratings for unseen ﬁlms, the fact that the viewer has not watched a given movie may indicate a lack of interest in that movie, thus suggesting a lower rating than otherwise expected.',\n",
       " 'We propose adjusting the standard nuclear norm minimization strategy for matrix completion to account for such structural differences between observed and unobserved entries by regularizing the values of the unobserved entries.',\n",
       " 'We show that the proposed method outperforms nuclear norm minimization in certain settings.',\n",
       " 'thus violating the assumption of uniform sampling of observed entries across movies.',\n",
       " 'On the ﬂip side, a missing entry may indicate a user’s lack of interest in that particular movie.',\n",
       " 'Similarly, in sensor networks, entries may be missing because of geographic limitations or missing connections; in survey data, incomplete sections may be irrelevant or unimportant to the user.',\n",
       " 'In these settings, it is then reasonable to expect that missing entries have lower values1 than observed entries.',\n",
       " 'In this work, we propose a modiﬁcation to the traditional NNM for matrix completion that still results in a semi-deﬁnite optimization problem, but also encourages lower values among the unobserved entries.',\n",
       " 'We show that this method works better than NNM alone under certain sampling conditions.',\n",
       " '.',\n",
       " 'A decision tree T in Bm := {0, 1}m is a binary tree where each of its internal nodes is labeled with an integer in [m] = {1, 2, .',\n",
       " '.',\n",
       " '.',\n",
       " ', m}, each leaf is labeled with an assignment a ∈ Bm and each internal node has two outgoing edges that are labeled with 0 and 1, respectively.',\n",
       " 'Let A ⊂ {0, 1}m. We say that T is a decision tree for A if (1) For every a ∈ A there is one leaf of T that is labeled with a.',\n",
       " '(2) For every path from the root to a leaf with internal nodes labeled with i1, i2, .',\n",
       " '.',\n",
       " '.',\n",
       " ', ik ∈ [m], a leaf labeled with a ∈ A and edges labeled with ξi1 , .',\n",
       " '.',\n",
       " '.',\n",
       " ', ξik ∈ {0, 1}, a is the only element in A that satisﬁes aij = ξij for all j = 1, .',\n",
       " '.',\n",
       " '.',\n",
       " ', k. Our goal is to write a polynomial time (in n := |A| and m) algorithm that for an input A ⊆ Bm outputs a decision tree for A of minimum depth.',\n",
       " 'This problem has many applications that include, to name a few, computer vision, group testing, exact learning from membership queries and game theory.',\n",
       " 'Arkin et al.',\n",
       " 'and Moshkov [4,14] gave a polynomial time (ln|A|)- approx- imation algorithm (for the depth).',\n",
       " 'The result of Dinur and Steurer [6] for set cover implies that this problem cannot be approximated with ratio (1− o(1))· ln|A|, unless P=NP.',\n",
       " 'Moskov studied in [14] the combinatorial measure of extended teaching dimension of A, ETD(A).',\n",
       " 'He showed that ETD(A) is a lower bound for the depth of the decision tree for A and then gave an exponential time ETD(A)/ log(ETD(A))-approximation al- gorithm.',\n",
       " 'In this paper we further study the ETD(A) measure and a new com- binatorial measure, DEN(A), that we call the density of the set A.',\n",
       " 'We show that DEN(A) ≤ ETD(A) + 1.',\n",
       " 'We then give two results.',\n",
       " 'The ﬁrst result is that the lower bound ETD(A) of Moshkov for the depth of the decision tree for A is greater than the bounds that are obtained by the classical technique used in the literature.',\n",
       " 'The second result is a poly- nomial time (ln 2)DEN(A)-approximation (and therefore (ln 2)ETD(A)- approximation) algorithm for the depth of the decision tree of A.',\n",
       " 'We also show that a better approximation ratio implies P=NP.',\n",
       " 'We then apply the above results to learning the class of disjunctions of predicates from membership queries [5].',\n",
       " 'We show that the ETD of this class is bounded from above by the degree d of its Hasse diagram.',\n",
       " 'We then show that Moshkov algorithm can be run in polynomial time and is (d/ log d)-approximation algorithm.',\n",
       " 'This gives optimal algorithms when the degree is constant.',\n",
       " 'For example, learning axis parallel rays over constant dimension space.',\n",
       " '—Clustering is essential to many tasks in pattern recognition and computer vision.',\n",
       " 'With the advent of deep learn- ing, there is an increasing interest in learning deep unsupervised representations for clustering analysis.',\n",
       " 'Many works on this domain rely on variants of auto-encoders and use the encoder out- puts as representations/features for clustering.',\n",
       " 'In this paper, we show that an l2 normalization constraint on these representations during auto-encoder training, makes the representations more separable and compact in the Euclidean space after training.',\n",
       " 'This greatly improves the clustering accuracy when k-means clustering is employed on the representations.',\n",
       " 'We also propose a clustering based unsupervised anomaly detection method using l2 normalized deep auto-encoder representations.',\n",
       " 'We show the effect of l2 normalization on anomaly detection accuracy.',\n",
       " 'We further show that the proposed anomaly detection method greatly improves accuracy compared to previously proposed deep methods such as reconstruction error based anomaly detection.',\n",
       " ' Sigma-Pi-Sigma neural networks (SPSNNs) as a kind of high-order neural networks can provide more powerful mapping capability than the traditional feedforward neural networks (Sigma-Sigma neural networks).',\n",
       " 'In the existing literature, in order to reduce the number of the Pi nodes in the Pi layer, a special multinomial Ps is used in SPSNNs.',\n",
       " 'Each monomial in Ps is linear with respect to each particular variable σi when the other variables are taken as constants.',\n",
       " 'Therefore, the monomials like σn i or σn i σj with n > 1 are not included.',\n",
       " 'This choice may be somehow intuitive, but is not necessarily the best.',\n",
       " 'We propose in this paper a modiﬁed Sigma-Pi-Sigma neural network (MSPSNN) with an adaptive approach to ﬁnd a better multinomial for a given problem.',\n",
       " 'To elaborate, we start from a complete multinomial with a given order.',\n",
       " 'Then we employ a regularization technique in the learning process for the given problem to reduce the number of monomials used in the multinomial, and end up with a new SPSNN involving the same number of monomials (= the number of nodes in the Pi-layer) as in Ps.',\n",
       " 'Numerical experiments on some benchmark problems show that our MSPSNN behaves better than the traditional SPSNN with Ps.',\n",
       " '.',\n",
       " 'We propose a new method to eﬃciently compute load-ﬂows (the steady-state of the power-grid for given productions, consumptions and grid topology), substituting conventional simulators based on diﬀer- ential equation solvers.',\n",
       " 'We use a deep feed-forward neural network trained with load-ﬂows precomputed by simulation.',\n",
       " 'Our architecture permits to train a network on so-called “n-1” problems, in which load ﬂows are evalu- ated for every possible line disconnection, then generalize to “n-2” problems without re-training (a clear advantage because of the combinatorial nature of the problem).',\n",
       " 'To that end, we developed a technique bearing similarity with “dropout”, which we named “guided dropout”.',\n",
       " ' This paper deals with the reality gap from a novel perspective, targeting transferring Deep Reinforce- ment Learning (DRL) policies learned in simulated environments to the real-world domain for visual control tasks.',\n",
       " 'Instead of adopting the common so- lutions to the problem by increasing the visual ﬁ- delity of synthetic images output from simulators during the training phase, this paper seeks to tackle the problem by translating the real-world image streams back to the synthetic domain during the de- ployment phase, to make the robot feel at home.',\n",
       " 'We propose this as a lightweight, ﬂexible, and efﬁcient solution for visual control, as 1) no extra trans- fer steps are required during the expensive training of DRL agents in simulation; 2) the trained DRL agents will not be constrained to being deployable in only one speciﬁc real-world environment; 3) the policy training and the transfer operations are de- coupled, and can be conducted in parallel.',\n",
       " 'Besides this, we propose a conceptually simple yet very ef- fective shift loss to constrain the consistency be- tween subsequent frames, eliminating the need for optical ﬂow.',\n",
       " 'We validate the shift loss for artis- tic style transfer for videos and domain adaptation, and validate our visual control approach in real- world robot experiments.',\n",
       " 'A video of our results is available at: https://goo.gl/b1xz1s.',\n",
       " ' The linearly constrained nonconvex nonsmooth program has drawn much attention over the last few years due to its ubiquitous power of modeling in the area of machine learning.',\n",
       " 'A variety of important problems, including deep learning, matrix factorization and phase retrieval, can be reformulated as the problem of optimizing a highly nonconvex and nonsmooth objective function with some linear constraints.',\n",
       " 'However, it is challenging to solve a linearly constrained nonconvex nonsmooth program, which is much complicated than its unconstrained counterpart.',\n",
       " 'In fact, the feasible region is a polyhedron, where a simple projection is intractable in general, and moreover, the per-iteration cost is extremely expensive in real scenario, where the dimension of decision variable is high.',\n",
       " 'Therefore, it has been recognized promising to develop a provable and practical algorithm for solving linearly constrained nonconvex nonsmooth programs.',\n",
       " 'In this paper, we develop an incremental path-following splitting algorithm, denoted as IPFS, with a theoretical guarantee and a low computational cost.',\n",
       " 'In speciﬁc, we show that this algo- rithm converges to an ǫ-approximate stationary solution within O(1/ǫ) iterations with very low per-iteration cost.',\n",
       " 'To the best of our knowledge, this is the ﬁrst incremental method to solve linearly constrained nonconvex nonsmooth programs with a theoretical guarantee.',\n",
       " 'Experiments conducted on the constrained concave penalized linear regression (CCPLR) and nonconvex support vector ma- chine (NCSVM) demonstrate that the proposed algorithm is more eﬀective and stable than other competing methods.',\n",
       " '—The artificial neural network shows powerful ability  of inference, but it is still criticized for lack of interpretability and  prerequisite  needs  of  big  dataset.',\n",
       " 'This  paper  proposes  the  Rule- embedded  Neural  Network  (ReNN)  to  overcome  the  shortages.',\n",
       " 'ReNN first  makes local-based inferences to detect local patterns,  and  then  uses  rules  based  on  domain  knowledge  about  the  local  patterns to generate rule-modulated map.',\n",
       " 'After that, ReNN makes  global-based inferences that synthesizes the local patterns and the  rule-modulated map.',\n",
       " 'To solve the optimization problem caused by  rules, we use a two-stage optimization strategy to train the ReNN  model.',\n",
       " 'By  introducing  rules  into  ReNN,  we  can  strengthen  traditional  neural  networks  with  long-term  dependencies  which  are  difficult  to  learn  with  limited  empirical  dataset,  thus  improving inference accuracy.',\n",
       " 'The complexity of neural networks  can be reduced since long-term dependencies are not modeled with  neural  connections,  and  thus  the  amount  of  data  needed  to  optimize the neural networks can be reduced.',\n",
       " 'Besides, inferences  from  ReNN  can  be  analyzed  with  both  local  patterns  and  rules,  and thus have better interpretability.',\n",
       " 'In this paper, ReNN has been  validated with a time-series detection problem.',\n",
       " ' It is desirable to train convolutional networks (CNNs) to run more efﬁciently during inference.',\n",
       " 'In many cases how- ever, the computational budget that the system has for in- ference cannot be known beforehand during training, or the inference budget is dependent on the changing real-time resource availability.',\n",
       " 'Thus, it is inadequate to train just inference-efﬁcient CNNs, whose inference costs are not ad- justable and cannot adapt to varied inference budgets.',\n",
       " 'We propose a novel approach for cost-adjustable inference in CNNs - Stochastic Downsampling Point (SDPoint).',\n",
       " 'During training, SDPoint applies feature map downsampling to a random point in the layer hierarchy, with a random down- sampling ratio.',\n",
       " 'The different stochastic downsampling con- ﬁgurations known as SDPoint instances (of the same model) have computational costs different from each other, while being trained to minimize the same prediction loss.',\n",
       " 'Shar- ing network parameters across different instances provides signiﬁcant regularization boost.',\n",
       " 'During inference, one may handpick a SDPoint instance that best ﬁts the inference bud- get.',\n",
       " 'The effectiveness of SDPoint, as both a cost-adjustable inference approach and a regularizer, is validated through extensive experiments on image classiﬁcation.',\n",
       " '.',\n",
       " 'The extension of deep learning towards temporal data pro- cessing is gaining an increasing research interest.',\n",
       " 'In this paper we inves- tigate the properties of state dynamics developed in successive levels of deep recurrent neural networks (RNNs) in terms of short-term memory abilities.',\n",
       " 'Our results reveal interesting insights that shed light on the na- ture of layering as a factor of RNN design.',\n",
       " 'Noticeably, higher layers in a hierarchically organized RNN architecture results to be inherently biased towards longer memory spans even prior to training of the recurrent con- nections.',\n",
       " 'Moreover, in the context of Reservoir Computing framework, our analysis also points out the beneﬁt of a layered recurrent organization as an eﬃcient approach to improve the memory skills of reservoir models.',\n",
       " '—Unsupervised feature extractors are known to per- form an efﬁcient and discriminative representation of data.',\n",
       " 'Insight into the mappings they perform and human ability to understand them, however, remain very limited.',\n",
       " 'This is especially prominent when multilayer deep learning architectures are used.',\n",
       " 'This paper demonstrates how to remove these bottlenecks within the architecture of Nonnegativity Constrained Autoencoder (NC- SAE).',\n",
       " 'It is shown that by using both L1 and L2 regularization that induce nonnegativity of weights, most of the weights in the network become constrained to be nonnegative thereby resulting into a more understandable structure with minute deterioration in classiﬁcation accuracy.',\n",
       " 'Also, this proposed approach extracts features that are more sparse and produces additional output layer sparsiﬁcation.',\n",
       " 'The method is analyzed for accuracy and feature interpretation on the MNIST data, the NORB normalized uniform object data, and the Reuters text categorization dataset.',\n",
       " 'Index Terms—Sparse autoencoder, part-based representation, white-box model, deep learning, receptive ﬁeld.',\n",
       " ' The linear model uses the space deﬁned by the input to project the target or desired signal and ﬁnd the optimal set of model parameters.',\n",
       " 'When the problem is nonlinear, the adaption re- quires nonlinear models for good performance, but it becomes slower and more cumbersome.',\n",
       " 'In this paper, we propose a lin- ear model called Augmented Space Linear Model (ASLM), which uses the full joint space of input and desired signal as the projection space and approaches the performance of non- linear models.',\n",
       " 'This new algorithm takes advantage of the lin- ear solution, and corrects the estimate for the current testing phase input with the error assigned to the input space neigh- borhood in the training phase.',\n",
       " 'This algorithm can solve the nonlinear problem with the computational efﬁciency of lin- ear methods, which can be regarded as a trade off between accuracy and computational complexity.',\n",
       " 'Making full use of the training data, the proposed augmented space model may provide a new way to improve many modeling tasks.',\n",
       " ' While neural networks have achieved high accuracy on standard image classiﬁ- cation benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs.',\n",
       " 'Defenses based on regularization and ad- versarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses.',\n",
       " 'Can we somehow end this arms race?',\n",
       " 'In this work, we study this problem for neural networks with one hidden layer.',\n",
       " 'We ﬁrst pro- pose a method based on a semideﬁnite relaxation that outputs a certiﬁcate that for a given network and test input, no attack can force the error to exceed a certain value.',\n",
       " 'Second, as this certiﬁcate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks.',\n",
       " 'On MNIST, our approach produces a network and a certiﬁcate that no attack that perturbs each pixel by at most \\x01 = 0.1 can cause more than 35% test error.',\n",
       " ' Accurate and transparent prediction of cancer survival times on the level of indi- vidual patients can inform and improve patient care and treatment practices.',\n",
       " 'In this paper, we design a model that concurrently learns to accurately predict patient- speciﬁc survival distributions and to explain its predictions in terms of patient attributes such as clinical tests or assessments.',\n",
       " 'Our model is ﬂexible and based on a recurrent network, can handle various modalities of data including temporal measurements, and yet constructs and uses simple explanations in the form of patient- and time-speciﬁc linear regression.',\n",
       " 'For analysis, we use two publicly available datasets and show that our networks outperform a number of baselines in prediction while providing a way to inspect the reasons behind each prediction.',\n",
       " '—In this paper, we propose a novel approach, 3D-RecGAN++, which reconstructs the complete 3D structure of a given object from a single arbitrary depth view using generative adversarial networks.',\n",
       " 'Unlike existing work which typically requires multiple views of the same object or class labels to recover the full 3D geometry, the proposed 3D-RecGAN++ only takes the voxel grid representation of a depth view of the object as input, and is able to generate the complete 3D occupancy grid with a high resolution of 2563 by recovering the occluded/missing regions.',\n",
       " 'The key idea is to combine the generative capabilities of autoencoders and the conditional Generative Adversarial Networks (GAN) framework, to infer accurate and ﬁne-grained 3D structures of objects in high-dimensional voxel space.',\n",
       " 'Extensive experiments on large synthetic datasets and real-world Kinect datasets show that the proposed 3D-RecGAN++ signiﬁcantly outperforms the state of the art in single view 3D object reconstruction, and is able to reconstruct unseen types of objects.',\n",
       " ' Dimensionality reduction is in demand to reduce the complexity of solving large- scale problems with data lying in latent low-dimensional structures in machine learning and computer version.',\n",
       " 'Motivated by such need, in this work we study the Restricted Isometry Property (RIP) of Gaussian random projections for low-dimensional subspaces in RN , and rigorously prove that the projection Frobenius norm distance between any two subspaces spanned by the projected data in Rn (n < N ) remain almost the same as the distance between the original subspaces with probability no less than 1− e−O(n).',\n",
       " 'Previously the well-known Johnson-Lindenstrauss (JL) Lemma and RIP for sparse vec- tors have been the foundation of sparse signal processing including Compressed Sensing.',\n",
       " 'As an analogy to JL Lemma and RIP for sparse vectors, this work allows the use of random projections to reduce the ambient dimension with the theoretical guarantee that the distance between subspaces after compression is well preserved.',\n",
       " ' The backpropagation algorithm, which had been originally introduced in the 1970s, is the workhorse of learning in neural networks.',\n",
       " 'This back- propagation algorithm makes use of the famous machine learning algo- rithm known as Gradient Descent, which is a ﬁrst-order iterative opti- mization algorithm for ﬁnding the minimum of a function.',\n",
       " 'To ﬁnd a local minimum of a function using gradient descent, one takes steps propor- tional to the negative of the gradient (or of the approximate gradient) of the function at the current point.',\n",
       " 'In this paper, we develop an alter- native to the backpropagation without the use of the Gradient Descent Algorithm, but instead we are going to devise a new algorithm to ﬁnd the error in the weights and biases of an artiﬁcial neuron using Moore-Penrose Pseudo Inverse.',\n",
       " 'The numerical studies and the experiments performed on various datasets are used to verify the working of this alternative algo- rithm.',\n",
       " ' Rademacher complexity is often used to characterize the learn- ability of a hypothesis class and is known to be related to the class size.',\n",
       " 'We leverage this observation and introduce a new technique for estimating the size of an arbitrary weighted set, deﬁned as the sum of weights of all elements in the set.',\n",
       " 'Our technique provides upper and lower bounds on a novel gener- alization of Rademacher complexity to the weighted setting in terms of the weighted set size.',\n",
       " 'This generalizes Massart’s Lemma, a known upper bound on the Rademacher complexity in terms of the unweighted set size.',\n",
       " 'We show that the weighted Rademacher complexity can be estimated by solving a ran- domly perturbed optimization problem, allowing us to derive high-probability bounds on the size of any weighted set.',\n",
       " 'We apply our method to the problems of calculating the parti- tion function of an Ising model and computing propositional model counts (#SAT).',\n",
       " 'Our experiments demonstrate that we can produce tighter bounds than competing methods in both the weighted and unweighted settings.',\n",
       " '—We study the problem of learning to rank from multiple sources.',\n",
       " 'Though multi-view learning and learning to rank have been studied extensively leading to a wide range of applications, multi-view learning to rank as a synergy of both topics has received little attention.',\n",
       " 'The aim of the paper is to propose a composite ranking method while keeping a close correlation with the individual rankings simultaneously.',\n",
       " 'We propose a multi-objective solution to ranking by capturing the infor- mation of the feature mapping from both within each view as well as across views using autoencoder-like networks.',\n",
       " 'Moreover, a novel end- to-end solution is introduced to enhance the joint ranking with minimum view-speciﬁc ranking loss, so that we can achieve the maximum global view agreements within a single optimization process.',\n",
       " 'The proposed method is validated on a wide variety of ranking problems, including uni- versity ranking, multi-view lingual text ranking and image data ranking, providing superior results.',\n",
       " '.',\n",
       " 'Recent advances in analysis of subband amplitude envelopes of natural sounds have resulted in convincing synthesis, showing subband amplitudes to be a crucial component of perception.',\n",
       " 'Probabilistic latent variable analysis is particularly revealing, but existing approaches don’t incorporate prior knowledge about the physical behaviour of amplitude envelopes, such as exponential decay and feedback.',\n",
       " 'We use latent force modelling, a probabilistic learning paradigm that incorporates physical knowledge into Gaussian process regression, to model correlation across spectral subband envelopes.',\n",
       " 'We augment the standard latent force model approach by explicitly modelling correlations over multiple time steps.',\n",
       " 'Incorporating this prior knowledge strengthens the interpretation of the latent functions as the source that generated the signal.',\n",
       " 'We examine this interpretation via an experiment which shows that sounds generated by sampling from our probabilistic model are perceived to be more realistic than those generated by similar models based on nonnegative matrix factorisation, even in cases where our model is outperformed from a reconstruction error perspective.',\n",
       " ' We consider the matrix completion problem with a deterministic pattern of observed entries and aim to ﬁnd conditions such that there will be (at least locally) unique solution to the non-convex Minimum Rank Matrix Completion (MRMC) formulation.',\n",
       " 'We answer the question from a somewhat diﬀerent point of view and to give a geometric perspective.',\n",
       " 'We give a suﬃcient and “almost necessary” condition (which we call the well-posedness condition) for the local uniqueness of MRMC solutions and illustrate with some special cases where such condition can be veriﬁed.',\n",
       " 'We also consider the convex relaxation and nuclear norm minimization formulations.',\n",
       " 'Then we argue that the low-rank approximation approaches are more stable than MRMC and further propose a sequential statistical testing procedure to determine the rank of the matrix from observed entries.',\n",
       " 'Finally, numerical examples veriﬁed the validity of our theory.',\n",
       " ' Convolutional Neural Networks (CNNs) have become the method of choice for learning problems involving 2D planar images.',\n",
       " 'However, a number of problems of recent interest have created a demand for models that can analyze spherical images.',\n",
       " 'Examples include omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling.',\n",
       " 'A naive application of convolutional networks to a planar projection of the spherical signal is destined to fail, because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective.',\n",
       " 'In this paper we introduce the building blocks for constructing spherical CNNs.',\n",
       " 'We propose a deﬁnition for the spherical cross-correlation that is both expres- sive and rotation-equivariant.',\n",
       " 'The spherical correlation satisﬁes a generalized Fourier theorem, which allows us to compute it efﬁciently using a generalized (non-commutative) Fast Fourier Transform (FFT) algorithm.',\n",
       " 'We demonstrate the computational efﬁciency, numerical accuracy, and effectiveness of spherical CNNs applied to 3D model recognition and atomization energy regression.',\n",
       " ' We consider high dimensional dynamic multi-product pricing with an evolving but low-dimensional linear demand model.',\n",
       " 'Assuming the temporal variation in cross-elasticities exhibits low-rank structure based on ﬁxed (latent) features of the products, we show that the revenue maximization problem reduces to an online bandit convex optimization with side information given by the observed demands.',\n",
       " 'We design dynamic pricing algorithms whose revenue approaches that of the best ﬁxed price vector in hindsight, at a rate that only depends on the intrinsic rank of the demand model and not the number of products.',\n",
       " 'Our approach applies a bandit convex optimization algorithm in a projected low-dimensional space spanned by the latent product features, while simultaneously learning this span via online singular value decomposition of a carefully-crafted matrix containing the observed demands.',\n",
       " '.',\n",
       " 'Intrusion  detection for computer network systems has  been  becoming  one of the most critical tasks for network administrators today.',\n",
       " 'It  has an important role for organizations, governments and our society  due to the valuable resources hosted on computer networks.',\n",
       " 'Traditional  misuse detection strategies are unable to detect new and unknown intru-  sion  types.',\n",
       " 'In  contrast,  anomaly detection in  network security  aims  to  distinguish  between illegal  or  malicious events and  normal behavior of  network systems.',\n",
       " 'Anomaly detection can be considered as a classification  problem where  it  builds  models of normal network behavior, of which it  uses  to  detect  new patterns  that  significantly deviate  from the  model.',\n",
       " 'Most  of the  current  approaches on  anomaly  detection is  based  on the  learning of normal behavior and anomalous actions.',\n",
       " 'They  do not include  memory that is they do not take into account previous events classify new  ones.',\n",
       " 'In  this  paper, we propose a one-class collective anomaly detection  model based  on  neural  network learning.',\n",
       " 'Normally a  Long  Short-Term  Memory Recurrent Neural Network (LSTM  RNN) is trained only on nor-  mal  data,  and  it  is  capable of predicting several time-steps ahead of an  input.',\n",
       " 'In  our approach, a  LSTM  RNN  is  trained on normal time series  data  before performing a  prediction for each time-step.',\n",
       " 'Instead  of con-  sidering  each  time-step separately,  the  observation of prediction  errors  from a  certain number of time-steps is  now proposed as  a  new idea  for  detecting collective anomalies.',\n",
       " 'The  prediction  errors of a  certain  num-  ber  of the latest time-steps above a  threshold will  indicate  a  collective  anomaly.',\n",
       " 'The  model is  evaluated on a  time series version of the KDD  1999 dataset.',\n",
       " 'The  experiments demonstrate that the proposed model is  capable to detect collective anomaly efficiently.',\n",
       " ' Accurate, reliable sampling from fully-connected graphs with arbitrary correlations is a diﬃcult prob- lem.',\n",
       " 'Such sampling requires knowledge of the prob- abilities of observing every possible state of a graph.',\n",
       " 'As graph size grows, the number of model states be- comes intractably large and eﬃcient computation re- quires full sampling be replaced with heuristics and algorithms that are only approximations of full sam- pling.',\n",
       " 'This work investigates the potential impact of adiabatic quantum computation for sampling pur- poses, building on recent successes training Boltz- mann machines using a quantum device.',\n",
       " 'We inves- tigate the use case of quantum computation to train Boltzmann machines for predicting the 2016 Presi- dential election.',\n",
       " ' The graph convolutional networks (GCN) recently proposed by Kipf and Welling are an effective graph model for semi-supervised learning.',\n",
       " 'This model, however, was originally designed to be learned with the presence of both training and test data.',\n",
       " 'Moreover, the recursive neighborhood expansion across layers poses time and memory challenges for training with large, dense graphs.',\n",
       " 'To relax the require- ment of simultaneous availability of test data, we interpret graph convolutions as integral transforms of embedding functions under probability measures.',\n",
       " 'Such an interpretation allows for the use of Monte Carlo approaches to consistently esti- mate the integrals, which in turn leads to a batched training scheme as we propose in this work—FastGCN.',\n",
       " 'Enhanced with importance sampling, FastGCN not only is efﬁcient for training but also generalizes well for inference.',\n",
       " 'We show a compre- hensive set of experiments to demonstrate its effectiveness compared with GCN and related models.',\n",
       " 'In particular, training is orders of magnitude more efﬁcient while predictions remain comparably accurate.',\n",
       " '—Methods exploiting sparsity have been popular in imaging and signal processing applications including compres- sion, denoising, and imaging inverse problems.',\n",
       " 'Data-driven approaches such as dictionary learning enable one to discover complex image features from datasets and provide promising performance over analytical models.',\n",
       " 'Alternating minimization algorithms have been particularly popular in dictionary and transform learning.',\n",
       " 'In this work, we study the properties of alternating minimization for structured (unitary) sparsifying op- erator learning.',\n",
       " 'While the algorithm converges to the stationary points of the non-convex problem in general, we prove local linear convergence to the underlying generative model under mild assumptions.',\n",
       " 'Our experiments show that the unitary operator learning algorithm is robust to initialization.',\n",
       " ' It is well-known that the robustness of artiﬁcial neural networks (ANNs) is important for their wide ranges of applications.',\n",
       " 'In this paper, we focus on the robustness of the classiﬁcation ability of a spiking neural network which receives perturbed inputs.',\n",
       " 'Actually, the perturbation is al- lowed to be arbitrary styles.',\n",
       " 'However, Gaussian perturba- tion and other regular ones have been rarely investigated.',\n",
       " 'For classiﬁcation problems, the closer to the desired point, the more perturbed points there are in the input space.',\n",
       " 'In addition, the perturbation may be periodic.',\n",
       " 'Based on these facts, we only consider sinusoidal and Gaussian perturba- tions in this paper.',\n",
       " 'With the SpikeProp algorithm, we per- form extensive experiments on the classical XOR problem and other three benchmark datasets.',\n",
       " 'The numerical results show that there is not signiﬁcant reduction in the classiﬁca- tion ability of the network if the input signals are subject to sinusoidal and Gaussian perturbations.',\n",
       " ' Linear approximations to the decision boundary of a complex model have become one of the most popular tools for interpreting predictions.',\n",
       " 'In this paper, we study such linear explanations produced either post-hoc by a few recent methods or generated along with predictions with contextual explanation networks (CENs).',\n",
       " 'We focus on two questions: (i) whether linear explanations are always consistent or can be misleading, and (ii) when integrated into the prediction process, whether and how explanations affect performance of the model.',\n",
       " 'Our analysis sheds more light on certain properties of explanations produced by different methods and suggests that learning models that explain and predict jointly is often advantageous.',\n",
       " ' In this work, we present a fully automated lung computed tomography (CT) cancer diagnosis system, DeepLung.',\n",
       " 'DeepLung consists of two components, nodule detection (identifying the locations of candidate nodules) and clas- siﬁcation (classifying candidate nodules into benign or ma- lignant).',\n",
       " 'Considering the 3D nature of lung CT data and the compactness of dual path networks (DPN), two deep 3D DPN are designed for nodule detection and classiﬁca- tion respectively.',\n",
       " 'Speciﬁcally, a 3D Faster Regions with Convolutional Neural Net (R-CNN) is designed for nod- ule detection with 3D dual path blocks and a U-net-like encoder-decoder structure to effectively learn nodule fea- tures.',\n",
       " 'For nodule classiﬁcation, gradient boosting machine (GBM) with 3D dual path network features is proposed.',\n",
       " 'The nodule classiﬁcation subnetwork was validated on a public dataset from LIDC-IDRI, on which it achieved bet- ter performance than state-of-the-art approaches and sur- passed the performance of experienced doctors based on image modality.',\n",
       " 'Within the DeepLung system, candidate nodules are detected ﬁrst by the nodule detection subnet- work, and nodule diagnosis is conducted by the classiﬁ- cation subnetwork.',\n",
       " 'Extensive experimental results demon- strate that DeepLung has performance comparable to expe- rienced doctors both for the nodule-level and patient-level diagnosis on the LIDC-IDRI dataset.1',\n",
       " ' The identification of novel drug-target (DT) interactions is a substantial part of the drug discovery process.',\n",
       " 'Most of the computational methods that have been proposed to predict DT interactions have focused on binary classification, where the goal is to determine whether a DT pair interacts or not.',\n",
       " 'However, protein-ligand interactions assume a continuum of binding strength values, also called binding affinity and predicting this value still remains a challenge.',\n",
       " 'The increase in the affinity data available in DT knowledge-bases allow the use of advanced learning techniques such as deep learning architectures in the predic- tion of binding affinities.',\n",
       " 'In this study, we propose a deep-learning based model that uses only sequence information of both targets and drugs to predict DT interaction binding affinities.',\n",
       " 'The few studies that focus on DT binding affinity prediction either use 3D structure of protein-ligand complexes or 2D features of compounds.',\n",
       " 'One novel approach used in this work is the modeling of protein sequences and compound 1D representations with convolutional neural networks (CNNs).',\n",
       " 'The results show that the proposed deep learning based model that uses the 1D representations of targets and drugs is an effective approach for drug tar- get binding affinity prediction.',\n",
       " 'The model in which a high-level representation of a drug is constructed via CNNs and Smith-Waterman similarity is used for proteins achieved the best Concordance Index (CI) performance, outperforming KronRLS, a state-of-the-art algorithm for DT binding affinity prediction, with statistical significance.',\n",
       " ' Recurrent models for sequences have been recently successful at many tasks, es- pecially for language modeling and machine translation.',\n",
       " 'Nevertheless, it remains challenging to extract good representations from these models.',\n",
       " 'For instance, even though language has a clear hierarchical structure going from characters through words to sentences, it is not apparent in current language models.',\n",
       " 'We propose to improve the representation in sequence models by augmenting current approaches with an autoencoder that is forced to compress the sequence through an inter- mediate discrete latent space.',\n",
       " 'In order to propagate gradients though this dis- crete representation we introduce an improved semantic hashing technique.',\n",
       " 'We show that this technique performs well on a newly proposed quantitative efﬁ- ciency measure.',\n",
       " 'We also analyze latent codes produced by the model showing how they correspond to words and phrases.',\n",
       " 'Finally, we present an application of the autoencoder-augmented model to generating diverse translations.',\n",
       " '—We present a fast Compressive Sensing algorithm for the reconstruction of binary signals {0, 1}-valued binary signals from its linear measurements.',\n",
       " 'The proposed algorithm minimizes a non-convex penalty function that is given by a weighted sum of smoothed ℓ0 norms, under the [0, 1] box-constraint.',\n",
       " 'It is experimentally shown that the proposed algorithm is not only signiﬁcantly faster than linear-programming-based convex optimization algorithms, but also shows a better recovery per- formance under several different metrics.',\n",
       " ' We present a class of algorithms capable of directly training deep neural networks with respect to large families of task-speciﬁc performance measures such as the F-measure and the Kullback-Leibler divergence that are structured and non-decomposable.',\n",
       " 'This presents a departure from standard deep learning techniques that typically use squared or cross-entropy loss functions (that are decomposable) to train neural networks.',\n",
       " 'We demonstrate that directly training with task-speciﬁc loss functions yields much faster and more stable convergence across problems and datasets.',\n",
       " 'Our proposed algorithms and implementations have several novel features including (i) convergence to ﬁrst order stationary points despite optimizing complex objective functions; (ii) use of fewer training samples to achieve a desired level of convergence, (iii) a substantial reduction in training time, and (iv) a seamless integration of our implementation into existing symbolic gradient frameworks.',\n",
       " 'We implement our techniques on a variety of deep architectures including multi-layer perceptrons and recurrent neural networks and show that on a variety of benchmark and real data sets, our algorithms outperform traditional approaches to training deep networks, as well as some recent approaches to task-speciﬁc training of neural networks.',\n",
       " ' We present a novel algorithm, called Links, designed to perform online clustering on unit vectors in a high-dimensional Euclidean space.',\n",
       " 'The algorithm is appropriate when it is necessary to clus- ter data efﬁciently as it streams in, and is to be contrasted with tra- ditional batch clustering algorithms that have access to all data at once.',\n",
       " 'For example, Links has been successfully applied to embed- ding vectors generated from face images or voice recordings for the purpose of recognizing people, thereby providing real-time identiﬁ- cation during video or audio capture.',\n",
       " ' Though deep neural networks (DNNs) achieve remarkable performances in many artiﬁcial intel- ligence tasks, the lack of training instances re- mains a notorious challenge.',\n",
       " 'As the network goes deeper, the generalization accuracy decays rapidly in the situation of lacking massive amounts of training data.',\n",
       " 'In this paper, we propose novel deep neural network structures that can be inher- ited from all existing DNNs with almost the same level of complexity, and develop simple training algorithms.',\n",
       " 'We show our paradigm successfully resolves the lack of data issue.',\n",
       " 'Tests on the CI- FAR10 and CIFAR100 image recognition datasets show that the new paradigm leads to 20% to 30% relative error rate reduction compared to their base DNNs.',\n",
       " 'The intuition of our algorithms for deep residual network stems from theories of the par- tial differential equation (PDE) control problems.',\n",
       " 'Code will be made available.',\n",
       " ' The robustness of neural networks to adversarial examples has received great at- tention due to security implications.',\n",
       " 'Despite various attack approaches to crafting visually imperceptible adversarial examples, little has been developed towards a comprehensive measure of robustness.',\n",
       " 'In this paper, we provide a theoretical justiﬁcation for converting robustness analysis into a local Lipschitz constant es- timation problem, and propose to use the Extreme Value Theory for efﬁcient eval- uation.',\n",
       " 'Our analysis yields a novel robustness metric called CLEVER, which is short for Cross Lipschitz Extreme Value for nEtwork Robustness.',\n",
       " 'The proposed CLEVER score is attack-agnostic and computationally feasible for large neural networks.',\n",
       " 'Experimental results on various networks, including ResNet, Inception- v3 and MobileNet, show that (i) CLEVER is aligned with the robustness indica- tion measured by the (cid:96)2 and (cid:96)∞ norms of adversarial examples from powerful attacks, and (ii) defended networks using defensive distillation or bounded ReLU indeed achieve better CLEVER scores.',\n",
       " 'To the best of our knowledge, CLEVER is the ﬁrst attack-independent robustness metric that can be applied to any neural network classiﬁer.',\n",
       " ' Deep neural networks are among the most influential architectures of deep learning algorithms, being deployed in many mobile intelli- gent applications.',\n",
       " 'End-side services, such as intelligent personal assistants (IPAs), autonomous cars, and smart home services often employ either simple local models or complex remote models on the cloud.',\n",
       " 'Mobile-only and cloud-only computations are currently the status-quo approaches.',\n",
       " 'In this paper, we propose an efficient, adap- tive, and practical engine, JointDNN, for collaborative computation between a mobile device and cloud for DNNs in both inference and training phase.',\n",
       " 'JointDNN not only provides an energy and performance efficient method of querying DNNs for the mobile side, but also benefits the cloud server by reducing the amount of its workload and communications compared to the cloud-only ap- proach.',\n",
       " 'Given the DNN architecture, we investigate the efficiency of processing some layers on the mobile device and some layers on the cloud server.',\n",
       " 'We provide optimization formulations at layer granularity for forward and backward propagation in DNNs, which can adapt to mobile battery limitations and cloud server load con- straints and quality of service.',\n",
       " 'JointDNN achieves up to 18× and 32× reductions on the latency and mobile energy consumption of querying DNNs compared to the status-quo approaches, respec- tively.',\n",
       " ' Tactical driving decision making is crucial for au- tonomous driving systems and has attracted con- siderable interest in recent years.',\n",
       " 'In this paper, we propose several practical components that can speed up deep reinforcement learning algorithms towards tactical decision making tasks: 1) non- uniform action skipping as a more stable alternative to action-repetition frame skipping, 2) a counter- based penalty for lanes on which ego vehicle has less right-of-road, and 3) heuristic inference-time action masking for apparently undesirable actions.',\n",
       " 'We evaluate the proposed components in a realistic driving simulator and compare them with several baselines.',\n",
       " 'Results show that the proposed scheme provides superior performance in terms of safety, efﬁciency, and comfort.',\n",
       " ' JRMONIZ@ANDREW.CMU.EDU DAVID.KRUEGER@UMONTREAL.CA We propose Nested LSTMs (NLSTM), a novel RNN architecture with multiple levels of memory.',\n",
       " 'Nested LSTMs add depth to LSTMs via nesting as opposed to stacking.',\n",
       " 'The value of a memory cell in an NLSTM is computed by an LSTM cell, which has its own inner memory cell.',\n",
       " 'Speciﬁcally, = ft (cid:12) ct−1 + it (cid:12) gt, NLSTM instead of computing the value of the (outer) memory cell as couter memory cells use the concatenation (ft (cid:12) ct−1, it (cid:12) gt) as input to an inner LSTM (or NLSTM) memory cell, and set couter .',\n",
       " 'Nested LSTMs outperform both stacked and single-layer LSTMs with similar numbers of parameters in our experiments on various character-level language modeling tasks, and the inner memories of an LSTM learn longer term dependencies compared with the higher-level units of a stacked LSTM.',\n",
       " ' Projective cone scheduling deﬁnes a large class of rate-stabilizing policies for queueing models relevant to several applications.',\n",
       " 'While there exists considerable theory on the properties of projective cone schedulers, there is little practical guidance on choosing the parameters that deﬁne them.',\n",
       " 'In this paper, we propose an algorithm for designing an automated projective cone scheduling system based on observations of an expert projective cone scheduler.',\n",
       " 'We show that the estimated scheduling policy is able to emulate the expert in the sense that the average loss realized by the learned policy will converge to zero.',\n",
       " 'Speciﬁcally, for a system with n queues observed over a time horizon T , the average loss for .',\n",
       " 'This upper bound holds regardless of the statistical characteristics of the system.',\n",
       " 'the algorithm is O The algorithm uses the multiplicative weights update method and can be applied online so that additional observations of the expert scheduler can be used to improve an existing estimate of the policy.',\n",
       " 'This provides a data-driven method for designing a scheduling policy based on observations of a human expert.',\n",
       " 'We demonstrate the eﬃcacy of the algorithm with a simple numerical example and discuss several extensions.',\n",
       " ' Clustering is inherently ill-posed: there often ex- ist multiple valid clusterings of a single dataset, and without any additional information a cluster- ing system has no way of knowing which clus- tering it should produce.',\n",
       " 'This motivates the use of constraints in clustering, as they allow users to communicate their interests to the clustering sys- tem.',\n",
       " 'Active constraint-based clustering algorithms select the most useful constraints to query, aim- ing to produce a good clustering using as few con- straints as possible.',\n",
       " 'We propose COBRA, an active method that ﬁrst over-clusters the data by running K-means with a K that is intended to be too large, and subsequently merges the resulting small clus- ters into larger ones based on pairwise constraints.',\n",
       " 'In its merging step, COBRA is able to keep the number of pairwise queries low by maximally ex- ploiting constraint transitivity and entailment.',\n",
       " 'We experimentally show that COBRA outperforms the state of the art in terms of clustering quality and runtime, without requiring the number of clusters in advance.',\n",
       " ' Relational data sources are still one of the most popular ways to store enterprise or Web data, however, the issue with relational schema is the lack of a well-deﬁned semantic description.',\n",
       " 'A common ontology provides a way to represent the meaning of a relational schema and can facilitate the integration of heterogeneous data sources within a domain.',\n",
       " 'Semantic labeling is achieved by mapping attributes from the data sources to the classes and properties in the ontology.',\n",
       " 'We formulate this problem as a multi-class classiﬁcation problem where previously labeled data sources are used to learn rules for labeling new data sources.',\n",
       " 'The majority of existing approaches for semantic labeling have focused on data integration challenges such as naming conﬂicts and semantic heterogeneity.',\n",
       " 'In addition, machine learning approaches typically have issues around class imbalance, lack of labeled instances and relative importance of attributes.',\n",
       " 'To address these issues, we develop a new machine learning model with engineered features as well as two deep learning models which do not require extensive feature engineering.',\n",
       " 'We evaluate our new approaches with the state-of-the-art.',\n",
       " '—In this era of data deluge, many signal processing and machine learning tasks are faced with high-dimensional datasets, including images, videos, as well as time series gen- erated from social, commercial and brain network interactions.',\n",
       " 'Their efﬁcient processing calls for dimensionality reduction techniques capable of properly compressing the data while preserving task-related characteristics, going beyond pairwise data correlations.',\n",
       " 'The present paper puts forth a nonlinear dimensionality reduction framework that accounts for data lying on known graphs.',\n",
       " 'The novel framework turns out to encompass most of the existing dimensionality reduction methods as special cases, and it is capable of capturing and preserving possibly nonlinear correlations that are ignored by linear methods, as well as taking into account information from multiple graphs.',\n",
       " 'An efﬁcient algorithm admitting closed-form solution is developed and tested on synthetic datasets to corroborate its effectiveness.',\n",
       " 'Index Terms—Dimensionality reduction, nonlinear modeling, graph signal processing',\n",
       " ' Deep learning involves a difﬁcult non-convex optimization problem with a large number of weights between any two adjacent layers of a deep structure.',\n",
       " 'To handle large data sets or complicated networks, distributed training is needed, but the calculation of function, gradient, and Hessian is expensive.',\n",
       " 'In particular, the communication and the synchronization cost may become a bottleneck.',\n",
       " 'In this paper, we focus on situations where the model is distributedly stored, and propose a novel distributed Newton method for training deep neural networks.',\n",
       " 'By variable and feature-wise data partitions, and some careful designs, we are able to explicitly use the Jacobian matrix for matrix-vector products in the Newton method.',\n",
       " 'Some techniques are incorporated to reduce the running time as well as the memory con- sumption.',\n",
       " 'First, to reduce the communication cost, we propose a diagonalization method such that an approximate Newton direction can be obtained without com- munication between machines.',\n",
       " 'Second, we consider subsampled Gauss-Newton \\x0cmatrices for reducing the running time as well as the communication cost.',\n",
       " 'Third, to reduce the synchronization cost, we terminate the process of ﬁnding an ap- proximate Newton direction even though some nodes have not ﬁnished their tasks.',\n",
       " 'Details of some implementation issues in distributed environments are thoroughly investigated.',\n",
       " 'Experiments demonstrate that the proposed method is effective for the distributed training of deep neural networks.',\n",
       " 'In compared with stochastic gra- dient methods, it is more robust and may give better test accuracy.',\n",
       " ' Incremental versions of batch algorithms are often desired, for increased time efﬁciency in the streaming data setting, or increased memory efﬁciency in general.',\n",
       " 'In this paper we present a novel algorithm for incremen- tal kernel PCA, based on rank one updates to the eigendecomposition of the kernel matrix, which is more computationally efﬁcient than comparable existing algorithms.',\n",
       " 'We extend our algorithm to incremental calculation of the Nystr¨om approximation to the kernel matrix, the ﬁrst such algorithm proposed.',\n",
       " 'Incremen- tal calculation of the Nystr¨om approximation leads to further gains in memory efﬁciency, and allows for empirical evaluation of when a subset of sufﬁcient size has been obtained.',\n",
       " ' This paper presents methods to accelerate recurrent neural network based language models (RNNLMs) for online speech recognition systems.',\n",
       " 'Firstly, a lossy compression of the past hidden layer out- puts (history vector) with caching is introduced in order to reduce the number of LM queries.',\n",
       " 'Next, RNNLM computations are de- ployed in a CPU-GPU hybrid manner, which computes each layer of the model on a more advantageous platform.',\n",
       " 'The added overhead by data exchanges between CPU and GPU is compensated through a frame-wise batching strategy.',\n",
       " 'The performance of the proposed methods evaluated on LibriSpeech1 test sets indicates that the re- duction in history vector precision improves the average recognition speed by 1.23 times with minimum degradation in accuracy.',\n",
       " 'On the other hand, the CPU-GPU hybrid parallelization enables RNNLM based real-time recognition with a four times improvement in speed.',\n",
       " 'Index Terms— Online speech recognition, language model, re- current neural network, graphic processing unit',\n",
       " ' In model-based reinforcement learning it is typi- cal to treat the problems of learning the dynamics model and learning the reward function separately.',\n",
       " 'However, when the dynamics model is ﬂawed, it may generate erroneous states that would never occur in the true environment.',\n",
       " 'A reward func- tion trained only to map environment states to rewards (as is typical) would have little guidance in such states.',\n",
       " 'This paper presents a novel error bound that accounts for the reward model’s be- havior in states sampled from the model.',\n",
       " 'This bound is used to extend the existing Hallucinated DAgger-MC algorithm, which offers theoretical performance guarantees in deterministic MDPs that do not assume a perfect model can be learned.',\n",
       " 'Empirically, this approach to reward learning can yield dramatic improvements in control perfor- mance when the dynamics model is ﬂawed.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 만들었던 단어단위 데이터를 통해 아래와 같은 데이터 프레임을 만드려고 했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_names=[\"papersNum\",\"sentNum\",\"wordNumber\"]\n",
    "dfs = pd.DataFrame(columns=col_names)\n",
    "for ith in range(len(word_from_sentences)):\n",
    "    ithPaper = word_from_sentences[ith]\n",
    "    for jth in range(len(ithPaper)):\n",
    "        ithSent = ithPaper[jth]\n",
    "        dfs = pd.concat([dfs, pd.DataFrame([[ith+1, jth+1, len(ithSent)]], columns=col_names)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래와 같은 형태로 만든 이유는 abstract 단위, 문장 단위로 단어의 갯수를 파악하는 데 도움이 될 것 같아서 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>papersNum</th>\n",
       "      <th>sentNum</th>\n",
       "      <th>wordNumber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>67</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>68</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>68</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>68</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>68</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>68</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>68</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>68</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>68</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>68</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>68</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>69</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>69</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>70</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>70</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>70</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>71</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>71</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>71</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>520 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    papersNum sentNum wordNumber\n",
       "0           1       1         31\n",
       "1           1       2         11\n",
       "2           1       3         21\n",
       "3           1       4         30\n",
       "4           1       5         23\n",
       "5           1       6         42\n",
       "6           1       7         14\n",
       "7           1       8         25\n",
       "8           1       9         22\n",
       "9           1      10         17\n",
       "10          2       1         25\n",
       "11          2       2         41\n",
       "12          2       3         20\n",
       "13          2       4         32\n",
       "14          2       5         15\n",
       "15          2       6         16\n",
       "16          2       7         22\n",
       "17          3       1         27\n",
       "18          3       2         25\n",
       "19          3       3         16\n",
       "20          3       4         21\n",
       "21          3       5         36\n",
       "22          3       6         18\n",
       "23          3       7         19\n",
       "24          3       8         43\n",
       "25          4       1         22\n",
       "26          4       2         24\n",
       "27          4       3         29\n",
       "28          4       4         33\n",
       "29          4       5         14\n",
       "..        ...     ...        ...\n",
       "490        67       6         10\n",
       "491        68       1         24\n",
       "492        68       2         27\n",
       "493        68       3         14\n",
       "494        68       4         28\n",
       "495        68       5         29\n",
       "496        68       6         17\n",
       "497        68       7         28\n",
       "498        68       8         19\n",
       "499        68       9         29\n",
       "500        68      10         12\n",
       "501        68      11         17\n",
       "502        68      12         19\n",
       "503        69       1         26\n",
       "504        69       2         37\n",
       "505        69       3         22\n",
       "506        69       4         31\n",
       "507        70       1         21\n",
       "508        70       2         30\n",
       "509        70       3         26\n",
       "510        70       4         18\n",
       "511        70       5         34\n",
       "512        70       6         22\n",
       "513        70       7         17\n",
       "514        71       1         24\n",
       "515        71       2         23\n",
       "516        71       3         25\n",
       "517        71       4         24\n",
       "518        71       5         31\n",
       "519        71       6         22\n",
       "\n",
       "[520 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataframe을 기반으로 아래와 같은 간단한 정보를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1% 단위로 데이터를 나누어보면 다음과 같은 형태로 나누어집니다. 99% 값이 66.6개이며 이는 67의 인풋 사이즈를 설정시, 99%의 데이터들을 활용할 수 있다는 의미로 생각합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0, 1.0, 1.0, 2.710000000000001, 8.760000000000002, 10.0, 11.0,\n",
       "       12.0, 12.0, 13.0, 13.0, 14.0, 14.0, 14.0, 15.0, 15.0, 15.0, 16.0,\n",
       "       16.0, 16.0, 17.0, 17.0, 17.0, 18.0, 18.0, 18.0, 18.939999999999998,\n",
       "       19.0, 19.0, 19.0, 20.0, 20.0, 20.0, 21.0, 21.0, 21.0, 21.0, 21.0,\n",
       "       21.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 23.0, 23.0, 23.0,\n",
       "       24.0, 24.0, 24.0, 24.0, 25.0, 25.0, 25.0, 25.0, 26.0, 26.0, 26.0,\n",
       "       27.0, 27.0, 27.0, 28.0, 28.0, 28.0, 28.54000000000002, 29.0, 29.0,\n",
       "       30.0, 30.0, 30.0, 30.680000000000007, 31.0, 31.0, 32.0, 32.0, 33.0,\n",
       "       33.0, 33.0, 34.0, 34.0, 34.0, 35.0, 35.0, 36.0, 36.0, 36.0, 37.0,\n",
       "       38.0, 39.0, 40.0, 41.0, 42.0, 42.85999999999996, 43.049999999999955,\n",
       "       48.24000000000001, 52.43000000000001, 61.620000000000005,\n",
       "       66.61999999999989, 103.0], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(dfs.wordNumber,q=[i for i in range(0,101,1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10% 단위로 데이터를 나눈 값들과 평균, 중간값 등의 데이터입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0, 13.0, 17.0, 20.0, 22.0, 24.0, 27.0, 30.0, 34.0, 39.0, 103.0], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(dfs.wordNumber,q=[i for i in range(0,110,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.592307692307692"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(dfs.wordNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(dfs.wordNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(dfs.wordNumber)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
