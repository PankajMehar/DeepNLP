 Network anomaly detection refers problem detecting illegal malicious activities events normal connections expected behavior network systems [3,
 has become popular subjects network security domain due fact many organizations governments are seeking good solutions protect valuable resources computer networks unauthorized illegal accesses, network attacks malware.
 last decades, machine learning techniques are known common approach developing network anomaly detection models [2,
 Network anomaly detection is posed type classification problem: given dataset representing normal anomalous examples, goal is build learning classifier is capable signaling new anomalous data sample is encountered [4].
 Most existing approaches consider anomaly discrete single data point: cases occur “individually” “separately” [5,
 such approaches, anomaly detection models do have ability represent information previous data points events evaluating current point.
 network security domain however, kinds attacks (e.g. Denial Ser- vice DoS occur long period time (several minutes) [9], are represented sequence single data points.
 attacks be indicated sequence single data points are considered attacks.
 context, network data be considered time series data are sequences events obtained repeated measurements time.
 Many ap- proaches ranging statistical techniques machine learning techniques are employed analyzing time series data, efficiency has been proven time series forecasting problems.
 approaches are based informa- tion previous events forecast incoming step.
 order detect kinds attack mentioned anomaly detection models be capable memorizing information number previous events, representing relationship current event.
 model have ability estimate prediction errors anomalous scores individual time-step be able observe sequences time-steps are potential be collective anomaly.
 avoid important mistakes, consider outcome: sense anomalous value be linked normal condition, conversely.
 work, aim build anomaly detection model kind attacks (known collective anomaly detection [4]).
 Collective anomaly is term refer collection related anomalous data instances respect whole dataset [4].
 single data point collective anomaly be considered anomalies itself, occur- rence sequence single points indicate collective anomaly.
 Hidden Markov model, Probabilistic Suffix Trees, etc.
 are popular techniques collective anomaly detection [4].
 Recently, Long Short-Term Memory Recurrent Neural Network [7] has been recognized powerful technique represent relationship current event previous events, handles time se- ries problems [11,
 However, approaches are proposed predicting time series anomalies individual level (predicting prediction error anomalous score time-step), collective level (observing prediction errors sequence time-steps).
 Moreover, normal anomalous data are employed training stage: training process (constructing classified models) validation process (estimating model parameters).
 Thus, models are limited detect new kinds network attack.
 Collecting label- ing anomalous data are expensive time-consuming tasks.
 Therefore, propose collective anomaly detection model using predictive power LSTM RNN.
 ability detect collective anomaly proposed model be demonstrated DoS attack group KDD Cup dataset.
 rest paper is organized follows.
 briefly review work related anomaly detection LSTM RNN Section
 Section give short introduction LSTM RNN.
 is followed section proposing collective anomaly detection model using LSTM RNN.
 Experiments, Results Discussion are presented Section Section respectively.
 paper concludes highlights future directions Section
 Related Work considering time series dataset, point anomalies are linked value considered sample.
 However, attempting real time collective anomaly detection implies being aware previous samples, behavior.
 means time-step include evaluation current value combined evaluation preceding infor- mation.
 section, briefly previous work applying LSTM RNN time series collective anomaly detection problems [11,
 Olsson al.
 [14] proposed unsupervised approach detecting collective anomalies.
 order detect group anomalous examples, anomalous score group data points was aggregated contri- bution individual example.
 Obtaining collective anomalous score was carried unsupervised manner, is suitable unsupervised supervised approaches scoring individual anomalies.
 model was eval- uated artificial dataset industrial datasets, detecting anomalies moving cranes anomalies fuel consumption.
 [11], Malhotra al.
 applied LSTM network addressing anomaly detection problem time series fashion.
 stacked LSTM network trained only normal data was used predict values number time-steps (L steps) ahead.
 prediction model produced L prediction values period L time-steps single data point.
 resulted prediction error vector L elements data point.
 prediction error single point was computed modeling prediction error vector fit multivariate Gaus- sian distribution, was used assess likelihood anomaly behavior.
 model was demonstrated perform datasets.
 Marchi al.
 presented novel approach combining non-linear predictive denoising autoencoders (DA) LSTM identifying abnormal acoustic signals.
 Firstly, LSTM Recurrent DA was employed predict auditory spectral features next short-term frame previous frames.
 network trained normal acoustic recorders tends behave normal data, yields small reconstruction errors whereas reconstruction errors abnormal acoustic signals are high.
 reconstruction errors au- toencoder was used “anomaly score”, reconstruction error predetermined threshold indicates novel acoustic event.
 model was trained public dataset containing in-home sound events, evaluated dataset including new anomaly events.
 results demonstrated model per- formed existing methods.
 idea is used practical acoustic example [13, LSTM RNNs are used predict short- term frames.
 [16] Ralf C at.
 employed LSTM-RNN intrusion detection problem supervised manner.
 processed version KDD Cup dataset, is represented time-series, were fed LSTM-RNN.
 network has outputs representing groups attacks normal connections data.
 labeled normal connections labeled attacks were used training model estimating best LSTM-RNN architecture pa- rameters.
 selected model was evaluated corrected dataset measurements confusion matrix accuracy.
 results shown model performed terms accuracy, achieved high performance groups attacks, Probe DoS.
 best knowledge, non previous work using LSTM RNN ad- dresses problem collective anomaly detection.
 aim develop collective anomaly detection using LSTM RNN.
 solution consists stages: (1) LSTM RNN be employed represent relationship previous time-steps current order estimate anomalous (known prediction error) time-step.
 stage is considered developing time series anomaly detection, similar previous work [11]; (2) method be proposed observing sequences single data points based anomalous scores detect collective anomaly.
 second stage makes work original different previous work applied LSRM RNN time series anomaly detection.
 prove efficient example: First, train LSTM RNN normal data order learn normal be- havior.
 trained model be validated normal validation set order estimate model parameters.
 result classifier employ rate anomalous score data time-step.
 anomalous score sequence time steps be aggregated contribution individual one.
 imposing predetermine threshold sequence single time-steps indicate collective anomaly anomalous score is higher threshold.
 More details approach be found Section
 Preliminaries section briefly describe structure Long Short Term Memory nodes, architecture LSTM RNN using LSTM hidden layer.
 LSTM was proposed Hochreiter al.
 [7] has proven be powerful technique addressing problem time series prediction.
 difference initiated LSTM regarding other types RNN resides “smart” nodes presented Hidden layer block Fig.

 cells contains gates, input gate, forget gate output gate, decide react input.
 Depending strength information node receives, decide block pass
 information is filtered set weights associated cells is transferred cells.
 Fig.

 LSTM RNN Architecture LSTM node structure enables phenomenon called backpropagation time.
 calculating hidden layer partial derivatives output, weight input values, system move backwards trace evolving error real output predicted output.
 Afterwards, network uses derivative evolution adapt weights decrease prediction error.
 learning method is named Gradient descent.
 simple LSTM RNN Fig consists layers: Input layer, LSTM hidden layer output layer.
 input output layers are same multi-layered perceptrons (MLPs).
 input nodes are input data, output nodes be sigmoid, tanh other transform functions.
 LSTM hidden layer is formed number “smart” nodes are connected input output nodes.
 common techniques, Gradient descent Back-propagation be used optimizing loss function updating parameters.
 mentioned Long Short-Term Memory has power incorporate behaviour network training normal data.
 system becomes representative variations data.
 other words, prediction is made focusing features: value sample position specific time.
 means same input value different times results different outputs.
 is LSTM RNN is stateful, i.e. has “memory”, changes response inputs.
 Proposed Approach mentioned Related work section, recent research us- ing LSTM RNNs building anomaly detection model time series data is [11].
 model was demonstrated be efficient detect anomalies time series data.
 model, prediction errors data point is com- puted fitting prediction errors vector multivariate Gaussian distribu- tion is used assess likelihood anomalous behavior.
 Hence, is suitable detecting abnormal events happen i.e. short time such electrocardiogram power demand applications model do have ability estimating likelihood anomaly long period time (a sequence data points).
 Consequently, model is suitable collective anomaly detection context network security kinds network attack last significant period time.
 Therefore, paper propose new approach using LSTM RNNs cyber-security attacks collective level.
 use simple LSTM RNN architecture, contrast stacked LSTM [11].
 does change core principle method: given sufficient training, LSTM RNN adapts weights, become characteristic training data.
 network, output represents time-step model predicts.
 example, model is fed data point time-step t-th predicting values next steps ahead, first, second third output nodes represent values time-steps (t + 1)-th, (t + (t + 3)-th respectively.
 fitting mixture Gaussian model, simple compute prediction error individual data point mean prediction errors time-steps.
 show LSTM RNNs ability learn behavior training set, stage acts time series anomaly detection model.
 Following stage, proposed terms (defined below) monitor prediction errors certain number successive data points locate collective anomaly data.
 second stage allows model detect collective anomaly, make model original different previous ones.
 Thus, performance model is compared previous models using LSTM RNN time series anomaly detection, e.g. [11].
 Terms measuring prediction errors data point, monitoring anoma- lous behavior period time-steps are defined below: – Relative Error (RE): relative prediction error real value x prediction value xˆ LSTM RNN time-step is defined eq.1. Note single data be considered collective anomaly.
 However, larger value RE data point has, higher probability single data point belongs collective anomaly.
 RE (x, xˆ) = |x − xˆ| (1) – Prediction Error Threshold (PET): is employed determiner individual query point time-step) be classified normal time-step considered element potential collective anomaly.
 predic- tion error RE threshold PET indicate element collective anomaly.
 Collective Range (CR): minimum number anomalies appearing suc- network flow are considered collective anomaly.
 P ET C R are estimated based best classification performance model normal validation set.
 Experiments Datasets order demonstrate efficient performance proposed model, choose dataset related network security domain, KDD dataset [1, experiments.
 dataset tcpdump format was collected sim- ulated military-like environment period weeks March April
 dataset is composed two-weeks training, weeks week (free attack), week validation, week (labeled attacks), other weeks testing, weeks (both normal anomalous data).
 are main groups attacks dataset, restrict experiments specific attack, Neptune, Denial-of-Service group.
 dataset is converted time series version feeding models.
 More details obtain time series version original tcpdump dataset, choose training, validation testing sets are presented following paragraphs.
 first crucial step is build usable time series dataset tcpdump data, select interested features.
 use terminal commands python program convert original tcpdump data KDD dataset time dependent function.
 method is development proposed transformation [10] acts tcpdump obtain real time statistics data.
 scheme follows step step transition described tcpdump ⇒ pcap ⇒ csv day records be time-filtered input new .pcap file.
 has advantage giving first approach visualizing data using Wireshark functionalities.
 is done, tshark command is adapted select transfer relevant information records csv format.
 note doing is first step towards computation better system efficiency, irrelevant pcap columns be ignored.
 method converting data do suit model performing real-time, is sufficient evaluating proposed model detecting collective anomaly main objective work.
 However, attack data is recorded real-time time series format, method be applied real-time detection.
 experiments, use 6-days normal traffic first third weeks training, ntrain one-day normal traffic (Thursday) week valida- tion, nvalid
 Testing sets include 1-day normal traffic week (Friday), ntest 1-day data containing attacks week (Wednesday), atest
 protocol be following: training network ntrain using nvalid choosing Prediction Error Threshold (PET) Collective Range (CR), evaluating proposed models ntest atest
 Table
 Parameter Settings LSTM RNN Parameters Input Size Hidden LSTM Layer Output Sigmoid Layer Learning Rate Number Epochs Momentum Batch Size Collective Thresholds Prediction Error Threshold (PET) Collective Range (CR) Experimental Settings experiments are aim demonstrate ability detecting collective anoma- lies proposed model.
 However, is anomalous instances available training validation stages, is harder binary classification problem estimating hyper-parameters, optimizing network architectures setting thresholds.
 Thus, briefly discus issues next paragraph, design experiments, preliminary experiment choosing thresholds main experiment evaluating proposed model.
 investigate network architectures.
 difference ar- chitectures is size output layer, one, outputs predicting 1-step, 2-step 3-step respectively.
 means three-output network predict values steps cur- rent input.
 number hidden nodes learning rate strongly influ- ence performance LSTM RNN.
 synapse network is weighted differently, be considered unique interpretation input data.
 node hidden layer is storage space interpretations.
 Theo- retically, higher number hidden more information network contain.
 means more computation, lead over-fitting.
 be trade detection rate computa- tional expenses constructing models querying new data.
 paper, choose size LSTM hidden layer equal
 learning rate is an- other factor linked speed LSTM RNN improve predictions.
 time step training synapse weights neural network are updated.
 learning rate defines much wish weight be modified instant.
 choose common used value learning rate,
 preliminary experiment aim tune Prediction Error Threshold (PET) Collective Range (CR) using normal validation set, nvalid
 means parameters are model classify most instances nvalid (say belonging normal class.
 paper, set threshold keep examples nvalid normal data.
 choice CR depends long single data point represents for.
 data, single point represents period minutes, CR.
 choose CR equal is equivalent period minutes.
 CR chosen, compute detection rate model different values PET ranging nvalid
 smallest value PET enables model classify examples nvalid is chosen.
 More details network architectures, parameters settings are presented Table
 main experiment is shown ability LSTM-RNNs detecting disproportionate durable change time series anomaly.
 preliminary experiment is complete, trained models collective thresholds, PET CR is employed detect anomalous region data, nvalid ntest atest
 experimental results include prediction error single data point, specific anomalous regions normal validation set test sets models.
 training error is plotted Fig
 Fig illustrate prediction errors validation set (vn ), testing sets (ntest atest ).
 Specific regions predicted collective anomaly proportion anomalous regions are presented Table
 Fig.

 training errors proposed model Fig.

 prediction error 3-step model validation set.
 Results Discussion Table shows collective anomaly prediction proposed model datasets, ntest atest
 collective anomaly prediction includes specific regions data percentage data instances regions.
 is anomalous region found normal validation set, thresholds, P ET C R have been tuned classify nvalid Fig.

 prediction error 3-step model normal test.
 Fig.

 prediction error 3-step model anomaly test.
 Table
 prediction collective anomalies validation test sets 1-Step Ahead Anomaly 2-Step Ahead Anomaly 3-Step Ahead Dataset nvalid atest Anomaly region ratio Anomaly region ratio Anomaly region Anomaly ratio belonging normal class.
 regions normal test, ntest is found third classifier (3-step ahead), is False Negative.
 anomaly test, many regions are detected collective anomalies classifiers.
 seems be more steps model predict, more regions be found.
 are regions (8.79%), regions (12.13%) regions (14.23%) found first, second third classifiers respectively.
 fig illustrates training errors classifiers 1-step, 2- step 3-step prediction ahead.
 errors tend converge epochs, error curve 3-step classifier levels earlier others.
 fig present prediction errors third classifiers nvalid ntest atest
 fig prediction errors are fluctuated few individual errors have large values.
 Thus, datasets are considered collective anomaly.
 However, error patterns fig are different.
 errors time steps is high, higher rest error regions.
 regions are detected collective anomalies presented Table
 training more prediction time-steps model has, training error model produces (see fig.

 suggests models more prediction time-steps tend learn normal behaviors network traffic ones less prediction time-steps.
 However, number prediction time-steps enrich model’s ability detect anomaly regions anomaly testing set
 imply three-steps model is robust learn normal behaviors identify collective anomalies others.
 Conclusion Further work paper, have proposed model collective anomaly detection based Long Short-Term Memory Recurrent Neural Network.
 have motivated method investigating LSTM RNN problem time series, adapted detect collective anomalies proposing measurements Section
 investigated hyper-parameters, suitable number inputs thresholds using validation set.
 proposed model is evaluated using time series version KDD dataset.
 results suggest proposed model is capable detecting collective anomalies dataset.
 However, be used caution.
 training data fed network be organized coherent manner guarantee stability system.
 future work, focus improve classification accuracy model.
 observed implementing variations LSTM RNNs number inputs trigger different output reactions.
 References
 DARPA intrusion detection evaluation.
 (n.d.).
 (Retrieved June http:
 Ahmed, M., Mahmood, A.N., Hu, J.: A survey network anomaly detection tech- niques.
 Journal Network Computer Applications (2016)
 Bhattacharyya, D.K., Kalita, J.K.: Network anomaly A machine learning perspective.
 CRC Press (2013)
 Chandola, V., Banerjee, A., Kumar, V.: Anomaly detection: survey.
 ACM com- puting surveys (CSUR) (2009)
 Chmielewski, A., Wierzchon, S.T.: V-detector algorithm tree-based struc- tures.
 In: Proc.
 International Multiconference Computer Science Information Technology, Wis/la (Poland).
 pp.

 Citeseer (2006)
 Hawkins, S., He, H., Williams, G., Baxter, R.: Outlier detection using replicator neural networks.
 In: International Conference Data Warehousing Knowl- edge Discovery.
 pp.

 Springer (2002)
 Hochreiter, S., Schmidhuber, J.: Long short-term memory.
 Neural computation (1997)
 KDD Cup Dataset (1999), available following website http://kdd.ics.uci.

 Lee, W., Stolfo, S.J.: A framework constructing features models in- trusion detection systems.
 ACM transactions Information system security (TiSSEC) (2000)
 Lu, W., Ghorbani, A.A.: Network detection based wavelet analysis.
 EURASIP Journal Advances Signal Processing (2009)
 Malhotra, P., Vig, L., Shroff, G., Agarwal, P.: Long short term memory networks anomaly detection time series.
 In: Proceedings.
 p.

 Presses universitaires Louvain (2015)
 Marchi, E., Vesperini, F., Eyben, F., Squartini, S., Schuller, B.: A novel approach automatic acoustic novelty detection using denoising autoencoder bidi- rectional lstm neural networks.
 In: IEEE International Conference Acous- tics, Speech Signal Processing (ICASSP).
 pp.

 IEEE (2015)
 Marchi, E., Vesperini, F., Weninger, F., Eyben, F., Squartini, S., Schuller, B.: Non-linear prediction lstm recurrent neural networks acoustic novelty detection.
 In: International Joint Conference Neural Networks (IJCNN).
 pp.

 IEEE (2015)
 Olsson, T., Holst, A.: A probabilistic approach aggregating anomalies unsu- pervised anomaly detection industrial applications.
 In: FLAIRS Conference.
 pp.

 Salama, M.A., Eid, H.F., Ramadan, R.A., Darwish, A., Hassanien, A.E.: Hybrid intelligent intrusion detection scheme.
 In: Soft computing industrial applications, pp.

 Springer (2011)
 Staudemeyer, R.C., Omlin, C.W.: Evaluating performance long short-term mem- ory recurrent neural networks intrusion detection data.
 In: Proceedings South African Institute Computer Scientists Information Technologists Conference.
 pp.

 ACM (2013)
 Lots biological experiments theoretical analysis have demonstrated speed scale processing infor- mation biological neural networks are much larger manual methods [1] [2].
 Inspired animals’ central nervous systems particular brain, many kinds Jie Yang · Pingping Zhang School Mathematical Sciences, Dalian University Technology, Dalian China.
 E-mail: yangjiee@dlut.edu.cn; jssxzhpp@dlut.edu.cn Yan Liu School information Science Engineering, Dalian Polytechnic University, Dalian China.
 E-mail: liuyan.3001@gmail.com artiﬁcial neural networks (ANNs) training methods have presented mimic animals’ behavior characteristics.
 ANNs are distributed mathematical models process in- formation [3] [4].
 have been used solve wide variety tasks are hard solve using or- dinary rule-based programming, including computer vision speech recognition [5] [6] [7] [8].
 networks base characteristics scales data complex- ity systems.
 adjusting weights are connected different nodes adjacent layers, ANNs achieve purpose processing information.
 special class ANNs, spiking neural networks (SNNs) simulate spikes generated animal dendrites axons neurons [4] [9].
 temporal information coding single spikes process information, were proved be type strong anthropomorphic networks.
 However, due many uncontrollable factors, such noising inputs, individual spike decay times, thresholding weights, abilities processing information spiking neural networks be affected [10] [11].
 order get available network architecture, is important do research robustness.
 encoding input vari- ables time differences pulses, spiking neurons are sensitive input signals.
 Recently, has led several explorations computational abili- ties learning performance neuromorphic networks spiking neurons noise [12] [13].
 However, works have considered type perturbations robust- ness classiﬁcation ability SNNs. analyze robustness SNNs’ classiﬁcation abil- ities, paper performed series numerical ex- periments classical XOR problem other benchmark datasets (i.e., Iris dataset, Wisconsin breast can- cer dataset StatLog landsat dataset) SpikeProp algorithm [14].
 Notably, closer perturbed inputs desired more perturbed points are Jie Yang al.
 Fig.
 structure simple spiking neural network.
 input space.
 What’s more, perturbation be periodic practice.
 facts led consider sinusoidal Gaussian perturbations paper.
 summarize, main contributions include: – As know, is ﬁrst work validate ro- bustness classiﬁcation ability SNNs receive perturbed inputs.
 kinds perturbations were considered robust- ness classiﬁcation ability SNN.
 fact, perturba- tions be arbitrary styles performing inputs.
 focus sinusoidal Gaussian perturbations.
 – classical XOR problem other bench- mark datasets, evaluate classiﬁcation ability SNNs show robustness experimentally.
 Spiking neural networks Compared traditional neural networks BP), SNNs have several differences network architectures.
 important is are multiple synaptic termi- nals speciﬁc synaptic delay spiking neurons adjacent layers.
 addition, due fast temporal en- coding is different traditional rate-coded networks, spiking neurons improve com- plex non-linear classiﬁcation performances [4] [14].
 architectures SNNs A simple feed-forward SNN multiple input spiking neu- rons output spiking neuron is shown Fig
 network architecture consists input layer, hidden layer output layer, denoted I, H O respec- tively.
 connection different layers comprises several synapses neuron receives set spikes previous neurons.
 Formally, assuming dur- ing simulation interval neuron generates most spike ﬁres internal state variable reaches threshold, state variable x j neuron j receives out- puts previous neurons weighted sum pre-synaptic contributions: x j(t) = Σi∈D jΣm k=1wk i jyk i (t) (1) D j denotes set pre-synaptic neurons associated neuron j, wi j is weight associated synaptic terminal k, yk i (t) represents delayed pre-synaptic po- tential (PSP) terminal, yk i (t) = ε(t − ti − dk) spike-response function.
 time ti is ﬁring time pre-synaptic neuron i, dk is delay associated synaptic terminal k.
 ﬁring time is determined ﬁrst time state variable reaches threshold.
 spike-response function is described form ε(t) =(cid:26) t τ exp(1− t τ ), t > t ≤ (3) τ models membrane potential decay time constant determines rise decay time PSP.
 Learning algorithm basic SpikeProp algorithm [14] is performed choose least mean square error-function.
 Given desired spike times {t d j actual ﬁring times {t j}, derive form error-function E = Σj(t j − t d j )2.
 (4) error back-propagation, weights update rule is fol- lowed: wk+ i j (t j) = wk i j(t j) + ∆wk i j(t j) Deﬁne δj = t d j − t j ∂yl Σil wl i j i (t j ∂t j (5) (6) output layer, basic weight adaptation function neurons is derived ∆wk i j(t j) = −η yk i (t j)(t d j − t j) ∂yl i (t j ∂t j Σilwl i j −ηyk i (t j)δj.
 (7) hidden layers, weight adaptation function neurons is given ∆wk hi(t j) = −η ∂yk i (t j ∂t j yk h(t j)Σj{δjΣkwk ∂yl n(t j ∂t j Σnlwl ni i j (8) η is learning rate network.
 more details, see [14] [15] [16].
 Robustness classiﬁcation ability spiking neural networks Perturbations neural networks Table Encoded inputs outputs XOR problem Input patterns Output patterns (1)Sinusoidal perturbations Traditional neural network perturbation approaches Traditional neural networks are led suitable Lyapunov- Krasovskii functional introducing appropriate ran- dom variables, such free weights analyze stochastic neural networks associated value parameter uncertainties numerical experiments demon- strate robust global exponential stability asymptotic stability.
 effect variation range dis- tribution time delay were taken account delay- distribution-dependent state estimation.
 stochastic per- turbations are described terms Brownian motion time-varying delay is characterized introducing Bernoulli stochastic variable.
 [18], parameter matrices were used express relationships system variables were perturbed study asymptotic stability delay nonlinear cellular neural network.
 perturbing time variable interval Hopﬁeld neural networks, [19] in- vestigates existence equilibrium points global robust exponential stability
 original input ˜x0 is perturbed sinusoidal perturba- tion term, be expressed ˜x = ˜x0 + A sin(2πy) (10) A is constant (0,1] control perturba- tion amplitude, y is random vector component values belong [0,1].
 numbers components associ- ated ˜x0 are same.
 (2)Gaussian perturbations original input ˜x0 is perturbed Gaussian perturba- tion.
 is different sinusoidal perturbation, be expressed perturbation approach SNNs reacted pulses transmit information.
 input signal enter spiking neural state neuron change (see formulas (1),(2),(3)).
 state variable exceeds threshold value, neuron arouses pulse.
 Hence input signals SNNs have great relevance robustness.
 large number practical examples imply signals random perturbations be different original one, few perturbed signals have large deviations origi- nal data.
 basic facts, are motivated perform different types perturbations input signals, inves- tigate robust classiﬁcation abilities SNNs. sake simplicity, describe perturba- tions XOR problem.
 other op- eration is similar.
 Classical XOR problem requires hidden units transform inputs desired outputs needs classify points (described (0,0),(1,1),(1,0),(0,1) categories.
 be points are perturbed same value, is equivalent translat- ing simultaneously, leading difference original inputs essence.
 convenience, choose point (1,1) target point test robustness spiking neural networks.
 basic perturbation formula is ˜x = ˜x0 + σ (9) ˜x0 being original input, σ noise term.
 formula means ˜x0 = (a, b), perturbed input ˜x be (a + σ, b + σ).
 ˜x = ˜x0 + ˜x0(I − exp(−r2/2) · sgn(l)) (11) is identity matrix, r is random vector component values [0,1] sgn(l) is signal function associated l.
 numbers components associated ˜x0, r l are same.
 Experimental results aim is assess robustness classiﬁcation ability SNNs, don’t design complex SNNs. train simple spiking neural network XOR problem other benchmark datasets.
 XOR problem Firstly, input output signals spiking neural net- works are coded [14].
 Max Min are extremal values variable x (e.g.an input signal), encode spike ﬁred time f (x) = x − Min Max − Min · L (12) length coding interval L.
 better representation spike-time patterns, associate “late” ﬁring time “early” ﬁring time.
 speciﬁc values respective in- put lead encoded XOR [14] [20] showing Table
 table, input numbers repre- sent spike times (i.e. late ﬁring times early ﬁring times) milliseconds.
 actual input patterns contain setting Jie Yang al.
 Table result correct classiﬁcation sinusoidal per- turbations.
 ROS: Rates correct classiﬁcation sinusoidal perturbations, RWS: Rates correct classiﬁcation sinusoidal perturbations.
 Table result correct classiﬁcation Gaussian pertur- bations.
 ROG: Rates correct classiﬁcation Gaussian per- turbations, RWG: Rates correct classiﬁcation Gaussian per- turbations.
 Epoches ROS RWS Epoches threshold adding third input neuron.
 deﬁne difference times equivalent “0” “1” coding interval L = ms.
 use feed-forward net- work connections have delay interval ms, available synaptic delays are 1-16 ms classiﬁcation.
 According formula (3), calculated postsynaptic states selected τ ms.
 Based above settings, numerical experiments were performed spiking neural network composed input layer, hidden layer output layer.
 were input layer neurons (two encoding neurons threshold neuron), ﬁve hidden layer neurons (of inhibitory neuron generating negative signal PSPs) output layer neuron.
 weight corresponding terminals (m were conﬁgurated pair synaptic neurons adjacent layers.
 Using computer simulations, randomly generated perturbed samples different r see distribution perturbed samples shown Fig
 Considering com- putational complexity time, used perturbed samples testing.
 varied r∗ (the least upper bound components random vector r )to control perturbation amplitudes.
 network learned XOR patterns η
 different perturbations, got correct classiﬁcation rate spiking neural network.
 average correct classiﬁcation rates perturbed data are compared Table Table
 results are reported Table Table clas- siﬁcation accuracies associated original data are differ- ent equal (about is input data were reordered epoch.
 larger per- turbation was performed input data, greatly rates correct classiﬁcation sinusoidal disturbances decreased (see Table
 disturbed input data Gaussian perturbations, network did got simi- lar results (see Table
 correct classiﬁcation rates network did fall much.
 reason be most perturbed data Gaussian perturbations were clustered desired value.
 indicates r∗ ROG RWG spiking neural networks have strong anti-interference abili- ties.
 Other benchmarks further validate robustness classiﬁcation ability SNNs practice, consider following benchmarks realistic signiﬁcance.
 XOR problem does, ﬁrst adopt method [14] encode continuous input variables min,...,In spike times.
 Specially, variable n range[In use N neurons Gaussian receptive ﬁelds encode input variable.
 neuron i, center was set min + min}/(N −2) width σ = (2i−3)/2·{In max − min}/(N −
 set β remainder experi- ments.
 output classiﬁcation, encoded patterns according winner-take-all paradigm.
 max− max], (1) Iris dataset Iris ﬂower data set Fisher’s Iris data set [21] is multivariate data set typical test case many clas- siﬁcation techniques machine learning.
 data set con- sists samples species Iris (Iris setosa, Iris virginica Iris versicolor).
 features are measured sample: length width sepals petals centimetres, respectively.
 Based combination features, add perturba- tions samples build perturbed input sets.
 set experiments, implement SNN layers XOR problem.
 feature was encoded neurons Gaussian receptive ﬁelds(yielding encoding neurons threshold neuron).
 SNN consists hidden layer neurons neuron gener- ates negative signal PSPs) output neurons.
 num- ber corresponding synaptic terminals is same last experiment(i.e.m =
 train SNN perturbed input data clear input data distinguish species other.
 results are presented Table Table
 Robustness classiﬁcation ability spiking neural networks r*=3.0,sigma=0.5 r*=3.0,sigma=1.0 r*=3.0,sigma=1.5 r*=4.0,sigma=1.0 r*=2.0,sigma=1.0 r*=8.0,sigma=1.0 Fig.
 perturbed samples different r∗.
 desired point is (1,1) closer desired more perturbed points are.
 perturbed points become scattered r∗ increases.
 Table result correct classiﬁcation sinusoidal pertur- bations Iris dataset.
 ROS: Rates correct classiﬁcation sinusoidal perturbations, RWS: Rates correct classiﬁcation sinusoidal perturbations.
 Table result correct classiﬁcation Gaussian pertur- bations Iris dataset.
 ROG: Rates correct classiﬁcation Gaussian perturbations, RWG: Rates correct classiﬁcation Gaussian perturbations.
 Epoches ROS RWS Epoches output layer neurons.
 results are presented Ta- ble Table
 (2) Wisconsin breast cancer (Original) dataset breast cancer (Original) dataset [22] is Uni- versity Wisconsin Hospitals contains case en- tries, divided benign malignant cases.
 case has measurements measurement is assigned integer larger numbers indicating greater likelihood malignancy.
 experiments, encoded measurement spaced neurons covering input range.
 set hidden layer neurons r∗ ROG RWG Jie Yang al.
 Table result correct classiﬁcation sinusoidal pertur- bations Wisconsin breast cancer (Original) dataset.
 ROS: Rates correct classiﬁcation sinusoidal perturbations, RWS: Rates correct classiﬁcation sinusoidal perturbations.
 Table result correct classiﬁcation sinusoidal pertur- bations StatLog landsat dataset.
 ROS: Rates correct classi- ﬁcation sinusoidal perturbations, RWS: Rates correct classiﬁcation sinusoidal perturbations.
 Epoches ROS RWS Epoches A ROS RWS Table result correct classiﬁcation Gaussian pertur- bations Wisconsin breast cancer (Original) dataset.
 ROG: Rates correct classiﬁcation Gaussian perturbations, RWG: Rates correct classiﬁcation Gaussian perturbations.
 Table result correct classiﬁcation Gaussian pertur- bations StatLog landsat dataset.
 ROG: Rates correct clas- siﬁcation Gaussian perturbations, RWG: Rates correct classiﬁcation Gaussian perturbations.
 Epoches r∗ ROG RWG Epoches r∗ ROG RWG (3) StatLog landsat dataset test robustness SNNs larger dataset, investigated Landsat dataset described StatLog survey machine learning algorithms [23].
 dataset consists training set cases test set cases contains ground cover types (classes).
 sam- ple contains values pixel patch pixel is described spectral bands.
 classiﬁcation single pixel, case contains values pixel patch, pixel described spectral bands, totaling inputs case.
 band, used average value cor- responding bands pixels new band pixel.
 case was represented average pixel separate band was encoded neurons.
 set experments results obtained Statlog survey are summarized Table Table
 above results, conclude ap- plication SNNs SpikeProp algorithm encoded versions benchmark problems yields satisfac- tory robustness sinusoidal Gaussian perturbations.
 Conclusions work, robustness classiﬁcation ability SNNs has been investigated disturbing input signals different perturbation methods classical XOR problem other complicated realistic prob- lems.
 experiments results, be concluded nevertheless perturbations affect classiﬁcation abil- ity SNNs, classiﬁcation ability does decrease dra- networks have certain anti-interference capability.
 References
 Rochester, N., J.H. Holland, L.H. Habit, W.L, Duda,Tests cell theory action brain, using large digital computer.
 IRE Transactions Information Theory,2,80-93(1956)
 Thorpe S, Fize D, Marlot C, Speed processing human vi- sual system.
 Nature,381,520-522 (1996)
 Werbos, P.J,Beyond Regression: New Tools Prediction Analysis Behavioral Sciences.
 PhD thesis, Harvard University, (1974)
 Wolfgang Maass,Networks spiking third gen- eration neural network models.Neural Networks,10,1659-
 M Riesenhuber, T Poggio,Hierarchical models object recogni- tion cortex.
 Nature neuroscience,2,1019-1025(1999)
 Fukushima, K.,Neocognitron: A self-organizing neural network model mechanism pattern recognition unaffected shift position.
 Biological
 A.
 Graves, M.
 Liwicki, S.
 Fernandez, R.
 Bertolami, H.
 Bunke, Improved J.
 Schmidhuber,A Novel Connectionist System Robustness classiﬁcation ability spiking neural networks Unconstrained Handwriting Recognition.
 IEEE Transactions PAMI,31,855-868(2009)
 D.V. Buonomano, M.
 Merzenich.
 neural network model tem- poral code generation position-invariant pattern recognition.
 Neural Comput,11,103-116(1999)
 A model neocortex.Network:Computation Neural Systems,6,179-124(1995)
 Vogl, T.P., Mangis, J.K., Rigler, A.K., Zink, W.T., Alkon, back-propagation D.L., Accelerating convergence method,Biol.Cybern.,59,257-263(1988)
 Ghosh-Dastidar, networks detection,Integr.Comput.-Aid E.,14,187-212(2007) neural EEG classiﬁcation epilepsy seizure S., Adeli, H.,Improved spiking
 Maass,W., Noisy spiking neurons temporal coding have more computational power sigmoidal neurons,In: Advances Neural Information Processing Systems,MIT Press,Cambridge,USA,9,211-
 Wolfgang Maass.Noise Resource Computation Learning Networks Spiking Neurons.Proceedings IEEE,102,860-880(2014)
 Sander M.Bothe, backpropagation encoded networks neurons.
 Neurocomputing 48,17-37(2002) Joost N.Kok, Han La Poutre.
 Error- spiking
 Gerstner,W., Kistler,W., Spiking Neuron Models.Cambridge Uni- versity Press,England (2002)
 Jie Yang, Wenyu Yang, Wei Wu. A remark error- backpropagation learning algorithm spiking.
 Applied Mathemat- ics Letters,25,1118-1120(2012)
 Haibo Bao, Jinde Cao.
 Delay-distribution-dependent state estima- tion discrete-time stochastic neural networks random delay.
 Neural
 Mo Y Z, Ding M Z,Yu J M.
 Stability analysis nonlinear cellular neural networks time-varying delay.
 J Chongqing Univ Nat Sci Ed,22,817-822(2010)
 Li Y B, Wang R L.
 Stability Reaction-diffusion Hopﬁeld Neural Networks S-type Distributed Delays.
 J Harbin Univ Nat Sci Ed,
 Jie Yang, Wenyu Yang, Wei Wu. A novel spiking perceptron solve XOR problem.
 Neural Network world,21,45-50(2011)
 Fisher, R.A. Use Multiple Measurements Taxonomic Problems.
 Annals Eugenics,7,179-188(1936)
 W.H. Wolberg, Cancer dataset obtained Williams H.
 Wolberg University Wisconsin Hospitals, Madison,

 D.
 Michie, D.J. Spiegelhalter, C.C. Taylor (Eds.), Machine Learn- ing, Neural Statistical Classiﬁcation, Ellis Horwood, West Sus- sex,
 wheat is main source nutrients world population, most produc- tion is converted ﬂour human consumption.
 Southern Brazil, domestic wheat is produced, Fusarium head blight fungal disease, is ma- jor concern.
 yield loss, causal agent Fusarium graminearum cause mycotoxin contamination wheat products, creating health problems.
 Therefore, avoid potential health risks, Fusarium affected grains be iden- tiﬁed segregated, processing, avoid incorporation food humans animal feed.
 detection Fusarium head blight (FHB) is carried human experts using process be lengthy tiresome.
 Moreover, effectiveness kind detection drop factors such fatigue, external distractions optical illusions [2].
 Thus, improving detection Fusarium Head Blight (FHB) wheat kernels has been major goal, due health risks associated ingestion mycotoxin, deoxynivalenol (DON).
 Methods capable performing disease detection automatic way are demanded productive wheat chain, segregate lots.
 Most automatic meth- ods proposed date image processing perform tasks [2,
 Barbedo al.
 [2] used Hyperspectral imaging (HSI) detecting Fusarium head blight (FHB) wheat kernels using algorithm.
 outcome was Fusarium index (FI), ranging be interpreted likelihood kernel be damaged FHB.
 According authors, hyperspectral imagery is sensi- tive estimate DON content directly.
 However, indirect estimation Fusarium damaged kernels was achieved, accuracy [2].
 Other study investigated use hyperspectral imaging (HSI) deoxyni- valenol (DON) screening wheat kernels.
 developed algorithm achieved accuracies three- two-class classiﬁcation schemes, respectively.
 results, accurate provide conclusive screening, indicating algorithm be used initial screening detect wheat batches warrant further analysis regarding DON content [3].
 Min Cho [9] presented review nondestructive detection fungi mycotoxins grains, focusing spectroscopic techniques chemometrics.
 spec- troscopy has advantages conventional methods including rapidness nonde- structive nature approach.
 However, limitations expensive complex setup equipment’s low accuracy due external interferents exist, be overcome widespread adoption techniques.
 application computer vision digital images offers high-throughput non-invasive alternative analytical immunological methods.
 paper presents automated method detect Fusarium Damaged Kernels, uses application computer vision digital images.
 main goal work is use machine learning algorithms computer vision techniques detect Fusarium Damaged Kernels wheat, based digital images.

 Material Methods Digital images Fusarium Head Blight symptomatic non-symptomatic wheat ker- nels were available National Research Centre Wheat (Embrapa Wheat), located Passo Fundo, Rio Grande do Sul State, Brazil.
 images were obtained recording video minutes using Olym- pus SP-810UZ digital camera optical wide-angle view, 14- megapixel resolution, HD video Olympus Lens Wide Optical Zoom ED 1:2.9-5.
 tasks run iMac workstation conﬁgured RAM DDR3 quad-core Intel Core i7 processor, NVIDIA GeForce GTX GPU GDDR5 memory.
 TensorFlow built source CUDA Toolkit cuDNN v5.1 enable GPU support.
 scripts were developed using Python Methodology classify digital images predeﬁned classes, use several methods developed recent decades [10,
 Methodologies solve kind problem was developed Artiﬁcial Intelligence (AI), research area Computer Science, Statistics.
 main differences Statistics AI approach are size task, statistics point view, algorithms techniques are limited input size image dataset are greater tens thousand pictures large number classiﬁcation sets.
 training stage system classify images objects based information content embedded single digital ﬁle, is necessary system detect possible contexts object occur.
 Small differences color, luminosity, angle other be misinterpreted proposed system results wrong low-level prediction classiﬁcation.
 key advantage using AI strategy image classiﬁcation task are re- lated knowing combine layers manage relationship levels information, needing strong assumption related type dependency relational structure input information.
 cases, improvements accuracy precision gain are archive using ﬁne tune setting, most effort is made input infor- mation, other words, processes are designed take most self-learning way.
 Deep Convolution Neural Network (DNN) used architecture neural network image classiﬁcation task is called convolutional neural network (CNN).
 convolution operation or, called convolution layer is related operation process respond “stimuli” limited region known receptive ﬁeld.
 receptive ﬁeld neuron contains partial overlap information input layer (raw image) and, way, preprocessing further operations occur minimal amount effort.
 Other advantages technique are related possibility using distributed algorithms GPU versions) calculate, ﬁlters processing small pieces information, time aggregated results necessary [7].
 deep portion CNN come stacking combination several layers output preceding layer is used input next one.
 common layers employed DNN are convolution, ReLU (Rectiﬁed Linear Units), tanh (Hyper- bolic Tangent Function), max pooling, average pooling, connected, concat, dropout softmax.
 better understand relationship see CNN example Figure
 Transfer Learning Transfer learning is machine learning method utilizes pre-trained neural net- work, technique allows detachment lasts outer layer (classiﬁcation layer) Figure
 example relationship layers CNN convolution, max pooling dense neuron connections annotated illustration.
 Source: [7, Figure uses remains structure retraining get new weights corresponding classes interest – damaged kernel case (Figure
 Figure
 illustration representation Deep Convolutional Neural Net- work, trained top ImageNet detailed information Feature Extrac- tion part Classiﬁcation part.
 small box represents layer types Inception Network: Convolution AvgPool, MaxPool, Concat, Dropout, Fully Connected Softmax.
 Source: Adapted [4, Figure work, use pre-trained neural network output [11].
 Szegedy al.
 [11] developed layer deep convolution neural network top ImageNet classifying leaf-node categories, using images training, validation images testing.
 brief, transfer learning makes possible classify new classes based new set images, reusing feature extraction part re-train classiﬁcation part new picture set.
 feature extraction part network was trained (which is complex part), new neural network be trained less computational resources time.
 Machine Learning Framework Nowadays have options build analyze deep neural networks using ma- chine learning algorithms.
 ﬁnal choice base computational infrastruc- ture available run task, number classes, intended purpose ﬁnal Net be deployed handle.
 work, is necessary framework machine learning run distributed system CPU GPU, possibility deploy ﬁnal network servers, desktops, mobile applications embedded systems easy way.
 needs, is necessary chosen framework im- plement Transfer Learning techniques, described before.
 Based requirements, natural choice is TensorFlow [1].
 Abadi al.
 presented TensorFlow interface expressing executing machine learning be performed little change broad range heterogeneous systems, ranging mobile devices such phones tablets large-scale distributed systems hundreds machines thousands computa- tional devices such GPU cards.
 Image Pre-processing generate normal kernel images was used FFMPEG library1 split minutes movie individual ﬁles (1280x720).
 normal damaged kernel images were arranged separate folder later use.
 using images generating neural network, al- located distinct image sets: training validation set, respec- tively.
 process training Net, other parts wheat plant structure spikes leaves were used composition DNN intending classify better wheat damaged kernels.

 Results Rationale Comparing effort described [11] training whole DNN scratch number pictures necessary get reasonable results.
 case, number pictures available moment (≈ archive scores deﬁnitely, time computational infrastructure necessary training evaluate resultant DNN be larger installed capacity.
 broad range applications are using transfer learning, Devikar [4] describe use image classiﬁcation various dog breeds overall accuracy dog breeds.
 Wang al.
 [13] describes application remote scene classiﬁcation attempt form baseline transferring pre-trained DNN remote sensing images various spatial spectral information.
 Esteva al.
 [5] describes use DNN dermatologist-level classiﬁcation skin cancer, trained end-to-end images directly, using pixels disease labels inputs dataset clinical images.
 library, see more information http://www.ffmpeg.org.
 Tk´aˇc Verner [12] presents many business applications, using artiﬁcial neural related ﬁnancial distress bankruptcy problems, stock price forecasting, decision support, particular attention classiﬁcation tasks.
 other uses CNN DNN, see [10,
 training procedure was carried described workstation took hour ﬁnish, output retrained neural network archive overall average accuracy
 main structure ﬁnal neural network is presented Figure
 validation purposes check correctness classifying new images wheat kernels (with Fusarium damage) choose new dataset validate DNN.
 images were National Research Centre Wheat articles lished last decade content, other open source images Fusarium damaged kernel found Internet.
 point, share positive experience using transfer learning techniques relation time training DNN new set images classes overall accuracy achieved initial image dataset.
 results validation dataset present total misclassiﬁcation, half (10%) new images were classiﬁed damaged leaves class.
 results be related main reasons: (a) small number pictures Fusarium Damaged Kernels (b) prevalence normal wheat kernels present initial dataset (≈

 Conclusions associated use Transfer Learning, TensorFlow, Inception-v3 cut time nec- essary training necessity have large image dataset start classiﬁcation procedure good accuracy level, compared training DNN scratch.
 misclassiﬁcation damaged leaves class be associated char- acteristics damage kernel leaves (in cases) region color lesions was blight standard wheat kernel.
 symptoms leaves present initial dataset was sep- arated diseases.Thus, was possible claim visual characteristics FHB leaves be transferred kernel evaluations context.
 hypothesys needs be investigated adding new images present dataset related Fusarium Damaged Kernel wheat leaves FHB symptoms.
 misclassiﬁcation new dataset, overall average accuracy archived (94.7%) Fusarium Damaged Kernels Deep Neural Network (FDK- DNN).
 Therefore, is potential using methodology classifying Fusarium Damaged Kernel means smartphone camera.
 accuracy proposed methodology is equivalent ones using HSI methodology presented [3].
 interesting future work be related using mixed RGB pictures, layers HSI operational spectra Fusarium Damaged Kernel proposed [2].
 Figure
 illustration Deep Convolutional Neu- ral Network, trained using TensorFlow [1] Transfer Learning techniques pre-trained Inception-v3 [11] image dataset contain- ing Fursarim Damaged Ker- nel, normal kernel, spikes leaves wheat.
 output neural network archive overall average ac- curacy
 kernels using hyperspectral imaging.
 Biosystems Engineering,
 [4] P.
 Devikar.
 Transfer learning image classiﬁcation various dog breeds.
 In- ternational Journal Advanced Research Computer Engineering Technology (IJARCET),
 [5] A.
 Esteva, B.
 Kuprel, R.
 A.
 Novoa, J.
 Ko, S.
 M.
 Swetter, H.
 M.
 Blau, S.
 Thrun.
 Dermatologist-level classiﬁcation skin cancer deep neural networks.
 Nature,
 [6] Y.
 Guo, Y.
 Liu, A.
 Oerlemans, S.
 Lao, S.
 Wu, M.
 S.
 Lew.
 Deep learning visual understanding: review.
 –
 ISSN
 doi: https://doi.org/10.1016/j.neucom.2015.09.116.
 Recent Developments Deep Big Vision.
 [7] A.
 Krizhevsky, I.
 Sutskever, G.
 E.
 Hinton.
 Imagenet classiﬁcation deep convolutional neural networks.
 Advances neural information processing sys- tems, pages
 [8] P.
 V.
 Maloney, S.
 Petersen, R.
 A.
 Navarro, D.
 Marshall, A.
 L.
 McKendry, J.
 M.
 Costa, J.
 P.
 Murphy.
 Digital image analysis method estimation fusarium- damaged kernels wheat.
 Crop Science,
 [9] H.
 Min B.-K.
 Cho.
 Spectroscopic techniques nondestructive detection fungi mycotoxins agricultural materials: A review.
 Journal Biosystems Engineering,
 References [1] M.
 Abadi, A.
 Agarwal, P.
 Barham, E.
 Brevdo, Z.
 Chen, C.
 Citro, G.
 S.
 Corrado, A.
 Davis, J.
 Dean, M.
 Devin, al.
 Tensorﬂow: Large-scale machine learning heterogeneous distributed systems.
 arXiv preprint arXiv:1603.04467,
 [2] J.
 G.
 Barbedo, C.
 S.
 Tibola, J.
 M.
 Fernandes.
 Detecting fusarium head blight wheat kernels using hyperspectral imaging.
 Biosystems Engineering,
 [3] J.
 G.
 A.
 Barbedo, C.
 S.
 Tibola, M.
 I.
 P.
 Lima.
 Deoxynivalenol screening wheat [10] J.
 Schmidhuber.
 Deep learning neural overview.
 Neural Networks, –
 ISSN 0893-6080.
 doi: https://doi.org/10.1016/j.neunet.2014.09.

 [11] C.
 Szegedy, W.
 Liu, Y.
 Jia, P.
 Sermanet, S.
 Reed, D.
 Anguelov, D.
 Erhan, V.
 Van- houcke, A.
 Rabinovich.
 Going deeper convolutions.
 Computer Vision Pattern Recognition (CVPR),
 [12] M.
 Tk´aˇc R.
 Verner.
 Artiﬁcial neural networks business: decades ISSN 1568-4946.
 doi: research.
 Applied Soft Computing, –
 https://doi.org/10.1016/j.asoc.2015.09.040.
 [13] J.
 Wang, C.
 Luo, H.
 Huang, H.
 Zhao, S.
 Wang.
 Transferring pre-trained deep cnns remote scene classiﬁcation general features learned linear pca network.
 Remote Sensing,
 paper, are aiming developing eﬀective optimization algorithm solving linearly constrained nonconvex program: min x∈X f (x) + r(x), s.t. Ax ≤ b, (1) x = [x1,


 ∈ Rd, xi ∈ Rdi Pn smooth nonconvex r(x) ∈ Rp×d, b ∈ Rp X ⊂ Rd is closed convex set.
 Problem (1) abstracts plethora mathematical models arising deep learning [1], distributed optimization coordination [2], network utility maximization [29], resource allocation [33], statistical learning [11] on.
 typical problems used practice constrained concave penalized linear regression (CCPLR) [19] nonconvex support vector machine (NCSVM) [8].
 i=1 di = d.
 objective function f Rd → R is i=1 ri(xi), ri is nonconvex.
 eﬀectiveness, is hard ﬁnd stationary solution problem (1).
 diﬃculty comes aspects.
 perspective problem structure, nonsmoothness objective function prohibits use gradient projection set {x Ax ≤ b} is intractable general.
 perspective computational cost, per-iteration cost is proportional full dimension hence expensive high-dimensional data-driven applications.
 Therefore, eﬃcient algorithm theoretical guarantee is demand.
 However, none existing algorithms meet requirements.
 paper, propose novel incremental path-following splitting denoted IPFS, resolve problem (1).
 key idea approach is construct sequence δ-smoothed problems log-barrier functions solve problem splitting method.
 speciﬁc, introduce slack variable s ≥ transform inequality constraint equality constraint, eliminate non-negative constraint using log-barrier function.
 leads several following δ-smoothed problem, are easier problem (1), f (x) + r(x) − δ min x∈X s.t. Ax + s = b.
 Pi=1 log(si), (2) fact, are able show stationary solution sequence δ-smoothed problems constitute path converges stationary solution problem (1).
 light, proposed IPFS method follow path, return ǫ-approximate stationary solution problem (1).
 Furthermore, cyclic randomized variable selection rule, are amenable high- dimensional optimization, is assigned alleviate issue computational cost.
 Finally, present detailed convergence iteration complexity analysis appendix.
 contributions work are summarized follows: propose construct sequence δ-smoothed problems, path is constituted converges stationary solution problem (1) δ →
 • develop novel incremental path-following splitting denoted IPFS, cyclic randomized variable selection rule, are amenable high-dimensional optimization.
 • evaluate eﬃcacy proposed algorithm constrained concave penalized linear regression nonconvex support vector machine.
 Experimental results demonstrate method outperforms other competing methods.
 Related works: best approach is ﬁrst incremental algorithm developed solving problem (1) theoretical guarantee low per-iteration cost.
 is achieved optimizing sequence smoothed problem decreasing parameter incremental splitting method.
 following brieﬂy discusses related work literature.
 log-barrier function has been used path-following method linear programming [14], generalized self-concordant function [26, convex programming.
 Recent years have witnessed renewed interests path-following method solving Lagrangian decomposition separable convex optimization [6] constrained convex minimization [5], idea behind is relevant continuation method, standard technique training neural network [17].
 recently, new continuation method proposed Hazan al.
 [15] has been proven convergent special class unconstrained non-convex smooth programs.
 However, remains unclear path-following method be extended solve problem (1) theoretical guarantee.
 splitting method is standard tool solving linearly constrained convex programs.
 Much eﬀort has been devoted establishing theoretical guarantee nonconvex splitting meth- ods.
 However, existing analysis is limited fraction problems, requires strong as- sumptions.
 iteration complexity analysis has been established nonconvex smooth consensus problem sharing problem [18], nonconvex problems Kurdyka-Lojasiewicz (KL) condition [32] symmetric nonnegative matrix factorization [24,
 recently, Jiang al.
 presented uniﬁed framework deﬁne ǫ-stationary solution nonconvex problems, presented iteration complexity splitting method terms variational inequality.
 Melo Monteiro [25] analyzed nonconvex Jacobi-type non-Euclidean splitting method elegant iteration complexity analysis, Gon¸calves et al.
 [12] obtained similar complexity result nonconvex proximal splitting method over-relaxation step-size.
 other hand, Combettes Pesquet [4] proposed block-coordinate ﬁxed-point algorithms, achieved low per-iteration cost incorporating random sweeping.
 However, problem (1) does fall class problems dis- cussed before.
 Therefore, remains unclear problem (1) be solved randomized splitting method theoretical guarantee.
 Algorithm make following assumptions paper: Assumption
 set stationary solutions problem (1) is nonempty.
 Assumption
 set X =Qn i=1 Xi is bounded, Ai has full column rank.
 Assumption
 objective f is diﬀerentiable partial derivative ∇if is Lipschitz continuous.
 speciﬁc, exists constant Li > such that, i =


 n, k∇if (x) − ∇if (y)k2 ≤ Li kx − yk2 ∀x, y ∈ Rd. Assumption is standard satisﬁed many loss functions machine learning.
 least square logistic loss, i.e., f (x) = i=1 l(x, ξi) ξi = (ai, bi) is single data sample, l(x, ξi) is deﬁned as: N PN log(cid:16)1 + exp(cid:16)−bi · a⊤ i x(cid:17)(cid:17)
 a⊤ i x − proceed optimality problem (1).
 Deﬁnition
 Let h Rd → R ∪ {+∞} be proper lower semi-continuous function.
 Suppose h(¯x) is ﬁnite given ¯x.
 v ∈ Rd, say • v is regular sub-gradient h ¯x, written v ∈ ∂ˆh(¯x), lim x6=¯x inf x→¯x h(x) − h(¯x) − hv, x − ¯xi kx − ¯xk2 ≥
 • v is general sub-gradient h ¯x, written v ∈ ∂h(¯x), exist sequences {xk} {vk} such xk → ¯x h(xk) → h(¯x), vk ∈ ∂ˆh(xk) vk → v k → +∞.
 following proposition lists facts semi-continuous functions.
 Proposition
 Let h Rd → R ∪ {+∞} g Rd → R ∪ {+∞} be proper lower semi-continuous functions.
 holds
 Fermats rule remains true: is local minimum h, ∈ ∂h(¯x).

 h is diﬀerentiable x, ∂(h + g)(x) = ∂h(x) + ∂g(x).

 h is Lipschitz continuous x, ∂(h + g)(x) ⊂ ∂h(x) + ∂g(x).

 Suppose h(x) is Lipschitz continuous, X is closed convex set, ¯x is local minimum h X
 exists v ∈ ∂h(¯x) such (x − ¯x)⊤v ≥ ∈ X
 Assumption
 set generalized gradient ri, denoted ∂ri, is assumed be bounded.
 proximal mapping ri, deﬁned as: proxαri(x) = argmin (cid:20)ri(y) + ky − xk2 is obtained.
 Remark
 Assumption is standard satisﬁed ℓ1-norm smoothly clipped absolute devia- tion (SCAD) [9], reasonable solution ri(x)+ is unique α > small.
 kxk2 are ready introduce notion ǫ-stationary solution problem (1).
 introducing slack variable s ≥ Lagrangian function is deﬁned as: L (x, s, λ) = f (x) + r(x) + hλ, Ax + s − bi
 Based ﬁrst-order optimality condition, deﬁne ǫ-stationary solution problem (1) follows: Deﬁnition
 call x∗ ∈ Rd be ǫ-stationary solution problem (1) exists s∗ λ∗ ∈ Rp such following holds true, dist(cid:16)−∇if (x∗) − A⊤ i λ∗, ∂ri(x∗ i )(cid:17) ǫ, i =


 n, j(cid:1) λ∗ j ≥ −ǫ, j =


 p, (cid:0)sj − s∗ kAx∗ + s∗ − bk2 ≤ ǫ, Ai is i-th column A, s ≥ dist(x,H) is standard Euclidean distance x closed convex set H.
 solution x∗ is stationary solution problem (1) ǫ = holds true.
 Remark
 Given stationary solution problem (1), has been recognized signiﬁcant reference several ﬁrst-order methods obtain iterates, converge solution.
 stationary solution attained proposed algorithm be local minimizer conditions, such second-order suﬃcient condition [27].
 propose incremental path-following splitting method, denoted IPFS, analyze iteration complexity cyclic randomized variable selection rule.
 proposed algorithm is presented Algorithm
 algorithm is double-looped.
 iteration outer loop, consider δ-smoothed problem, i.e., problem (2), δ > is smoothing parameter.
 optimize problem splitting method variable selection rule, obtain ǫδ-stationary solution problem (2).
 move next iteration outer loop, decrease δ γδ, < γ <
 inner loop is devoted optimize problem (2) splitting method.
 iteration, select set index ⊆ {1,


 n}, update {xi}i∈I based last iterate, i.e., xk.
 speciﬁc, introduce β deﬁne function as: Lk(cid:16){xi}i∈I xk, s, λ(cid:17) = Xi∈I (cid:20)D∇if (xk), xi − xk (cid:13)(cid:13)(cid:13) iE + i + s − b+ + Aixi +Xi /∈I + ri(xi)(cid:21) − δ Aixi +Xi /∈I +*λ,Xi∈I Li + Aixk (3) Given s = sk λ = λk, function is convex {xi}i∈I according boundedness ∂ri.
 end, obtain {xk+1 {xk+1 }i∈I following: }i∈I = argmin (4) Aixk xi − xk i(cid:13)(cid:13)(cid:13) (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) Xi∈I {xi∈Xi}i∈I Lk(cid:16){xi}i∈I xk, sk, λk(cid:17)
 log(si) Xi=1 i + s − b(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) Algorithm Incremental Path-Following Splitting Method (IPFS) Initialize: primal variable ¯x ∈ Rd; slack variable ¯s ≥ dual variable ¯λ ∈ Rp; smoothing parameter δ >
 Set: ratio γ ∈ (0, ﬁnal tolerance ǫ >
 δ > ǫ do Set k ←
 Set β > according δ.
 Set (cid:0)x0, s0, λ0(cid:1) ←(cid:0)¯x, ¯s, ¯λ(cid:1).
 stopping criterion is satisﬁed do
 Pick set index, i.e., I, according variable selection rule.
 Update {xk+1 }i∈I Eq. (4).
 Update {xk+1 }i /∈I ← {xk i }i /∈I.
 Update sk+1 Eq. (5).




 Update λk+1 Eq. (6).
 Update k ← k +

 end δ ← γδ.
 (cid:0)¯x, ¯s, ¯λ(cid:1) ←(cid:0)xk+1, sk+1, λk+1(cid:1).
 end }i /∈I = {xk s∈Rp "−δ Furthermore, set {xk+1 sk+1 = argmin i }i /∈I
 obtain sk+1 λk+1 following: log(si) + Xi=1 (cid:13)(cid:13)(cid:13)(cid:13) s + Axk+1 − b + λk+1 = λk + β(cid:16)Axk+1 + sk+1 − b(cid:17)
 λk(cid:13)(cid:13)(cid:13)(cid:13) (5) (6) Remark
 assume problem (4) be solved theoretical analysis convenience has been shown [13] subproblem admits closed-form solution few special nonconvex regularization function ri, e.g., SCAD [9] Ai is identity matrix Xi = Rdi.
 However, closed-form solution is intractable.
 alternative, iterative methods be used solve it.
 refer interested readers [13] more details.
 follows, discuss variable selection rules: cyclic randomized.

 cyclic rule.
 Let be set index selected k-th iteration inner loop, have k mod n is remainder k divided n.
 = {ik = k mod n},
 randomized rule.
 k-th iteration, index i ∈ is selected random probability pi i.e., Prob (i ∈ I) = pi ≥ pmin >
 design reasonable procedure identify tolerance ǫδ >
 procedure, known stopping criterion, is crucial performance proposed algorithm.
 speciﬁc, is unnecessary obtain accurate local solution δ is large, take lot iterations.
 follows, clarify connection ǫδ used stopping criterion δ.
 Stopping Criterion: repeat inner loop splitting method following statements hold true, C > is set constant is independent δ.
 (cid:13)(cid:13)(cid:13) (cid:13)(cid:13)(cid:13) xk+1 − xk(cid:13)(cid:13)(cid:13)2 ≤ Cδ, Axk+1 + sk+1 − b(cid:13)(cid:13)(cid:13)2 ≤ Cδ, Discussion Firstly, compare method existing methods.

 method is diﬀerent stochastic alternating direction method (SADM) [28].
 SADM draw subset data samples iteration method selects subset decision variables.
 theoretical guarantee SADM is established objective is convex.

 method is related algorithm presented [18].
 However, theoretical guarantee algorithm is established applied solve nonconvex smooth consensus sharing problems.

 method is related variants ADMM analyzed [20], i.e., proximal ADMM-m proximal ADMM-g.
 However, methods has theoretical guarantee applied solve problem (1).
 speciﬁc, problem (1) be reformulated min x∈X f (x) + r(x), s.t. Ax + s = b, s ≥
 objective is respect x s, violates conditions [20].
 computational cost proximal ADMM-m proximal ADMM-g is expensive high-dimensional problems.
 Secondly, remark parameter setting varies practical usage theoretical analysis.
 example, prove next section iterates converge ǫ-stationary solution β remains large constant.
 However, β be adapted accelerate method improve practical performance.
 Main Result section, present convergence iteration complexity analysis proposed algorithm cyclic randomized variable selection rule.
 Convergence deﬁne set stationary solutions problem (2), prove (¯x, ¯s) converges stationary solutions.
 Deﬁnition
 call (cid:0)xδ,∗, sδ,∗(cid:1) ∈ Rd× Rp be ǫδ-stationary solution problem (2) exists λδ,∗ ∈ Rp such following statement holds true, sδ,∗ j λδ,∗ i λδ,∗, ∂ri(xδ,∗ dist(cid:16)−∇if (xδ,∗) − A⊤ (cid:13)(cid:13)(cid:13) (cid:13)(cid:13)(cid:13) Axδ,∗ + sδ,∗ − b(cid:13)(cid:13)(cid:13)2 ≤ ǫδ.
 i )(cid:17) ǫδ, i =


 n, j − δ(cid:13)(cid:13)(cid:13)2 ≤ ǫδ, j =


 p, solution (cid:0)xδ,∗, sδ,∗(cid:1) is stationary solution problem (2) ǫδ = holds true.
 high-level, given δ (¯x, ¯s) approximate(cid:0)xδ,∗, inner loop goes.
 (cid:0)xδ,∗(cid:1) converges x∗ δ characterize limiting behavior algorithm.

 δ → show ¯x → x∗ terms expectation cyclic randomized variable selection rules, x∗ is stationary solution problem (1).
 Iteration Complexity subsection, analyze iteration complexity proposed algorithm.
 Speciﬁcally, use measure optimality terms variational inequality, closeness optimal solution, is intractable nonconvex optimization.
 ǫ-stationary solution problem (1) is regarded reached following statements hold true,
 smoothing parameter is smaller ﬁnal tolerance, i.e., δ < ǫ.

 stopping criterion inner loop is satisﬁed terms expectation cyclic randomized variable selection rules, respectively.
 are ready state main result iteration complexity.
 show iteration number inner loop, O(log( proposed algorithm returns ǫ-stationary solution problem (1) O(cid:0) ǫ )) iterations term iteration number outer loop.

 Suppose cyclic randomized variable selection rule is proposed IPFS algorithm returns ǫ-stationary solution problem (1) terms expectation ǫ(cid:1) iterations term cyclic randomized variable selection rules, respectively, O(cid:0) iteration number inner loop, O(log( ǫ(cid:1) iterations term ǫ )) iterations term iteration number outer loop.
 Table Statistics datasets.
 Dataset Number Samples Dimension ohscal classic mnist a9a w8a SUSY Remark
 highlight iteration complexity order O(cid:0) methods applied solve several non-convex problems.

 practice, variety large-scale artiﬁcial intelligence machine learning applications requires solution low accuracy, i.e., ǫ
 ǫ(cid:1) is theoretical optimal ﬁrst-order Experiments section, evaluate eﬃcacy algorithm linearly constrained nonconvex problem arising machine learning.
 compare method well-known heuristic algorithms existing exact convergence method problem (1) is unknown.
 speciﬁcally, consider inexact augmented Lagrangian method (InexactALM) [16] proximal alternating direction method multipliers (proximal ADMM-m) [20], are referred “heuristic” theoretical guarantees methods are valid convex minimization class nonconvex minimization.
 conduct experiments synthetic data ﬁrst task, named Constrained Concave Penal- ized Linear Regression [10], real data second task, named Nonconvex Support Vector Machine [30].
 objective value is used metric experiments.
 Constrained Concave Penalized Linear Regression Problem: problem Constrained Concave Penalized Linear Regression (CCPLR) has been rec- ognized constrained nonconvex program, covers few interesting applications statistical learning image processing.
 speciﬁc, is aiming recovering sparse signal x∗ ∈ Rd s ≪ d non-zero components observation y ∈ Rn b ∈ Rm, are deﬁned y = Ax∗ + ǫ1 b = Cx∗ + ǫ2.
 ∈ Rn×d C ∈ Rm×d are measurement matrices, ǫ1 ∈ Rm ǫ2 ∈ Rn are white noises.
 Mathematically, is deﬁned as: min x∈Rd − + Pλ(x), s.t. Cx − b ≤ (7) Pλ(w) is deﬁned as: Pλ(w) = λkwk1 − Xi=1(cid:20) w2 i − + λ2 − I(λ < wi ≤ θλ) + (λwi − (θ + )I(wi > θλ)(cid:21)
 Settings: generate ∈ Rn×d independent standard Gaussian entries normalized column; generate C ∈ Rm×d independent standard Gaussian entries; ǫ1 ∼ N (0, σ2Id), ǫ2 contains independent random entries distributed [0, σ], σ = σ = select diﬀerent regularization parameters {1.4, show algorithms are robust.
 implement random initialization, terminate relative change consecutive objective function values is less
 Experimental results: Figure shows objective value function time cost (in seconds) σ = σ = observe IPFS-Cyclic IPFS-Randomized outperform InexactALM proximal ADMM-m datasets, dimension is high.
 con- ﬁrms advantage algorithms InexactALM proximal ADMM-m is solid theoretical guarantee.
 Furthermore, IPFS-Randomized performs best low per-iteration cost, supporting usage randomized algorithms.
 Nonconvex Support Vector Machine Problem: problem NonConvex Support Vector Machine (NCSVM) is powerful binary classiﬁcation tool high accuracy great ﬂexibility.
 Mathematically, is deﬁned as: min x∈Rd α1⊤ξ + Pλ(x), s.t. − ξ − b · A⊤x ≤ ξ ≤ (8) ∈ Rn, Pλ(w) is deﬁned before.
 Settings: set σ vary regularization parameter λ {1.4,
 use datasets123 evaluate proposed algorithm, statistics is presented Table
 remaining setting is same used CCPLR problem.
 Experimental results: Figure shows objective value funtion time cost (in seconds).
 In- deed, algorithms outperform InexactALM proximal ADMM-m consistently, IPFS-Randomized performs best large margin high-dimensional real data.
 Conclusions paper, proposed novel incremental path-following splitting denoted IPFS, solve linearly constrained nonconvex program, abstracts few machine learning applications.
 best knowledge, is ﬁrst incremental method developed 2https://www.shi-zhong.com/software/docdata.zip Table Performance comparison referred algorithms.
 Methods IPFS-R IPFS-C IALM proximal ADMM-m IPFS-R IPFS-C IALM proximal ADMM-m IPFS-R IPFS-C IALM proximal ADMM-m IPFS-R IPFS-C IALM proximal ADMM-m λ = (objective,time) (6265.58, (6323.42, (6345.11, (6381.79, λ (objective,time) (7164.32, (7226.43, (7251.31, (7282.70, λ (objective,time) (8066.46, (8129.12, (8157.26, (8176.47, λ (objective,time) (8932.08, (9031.59, (9063.15, (9053.72, (6296.56, (7167.99, (8051.38, (8961.63, (6367.52, (6388.92, (6394.51, (487.81, (331.24, (1791.67, (965.95, (527.80, (550.85, (907.07, (1183.02, (7276.89, (7301.49, (7288.24, (10.54, (8.13, (1685.02, (1100.06, (32.32, (9.20, (11.24, (8184.76, (8213.84, (8163.68, (9.18, (10.13, (12.13, (1230.22, (10.131, (10.17, (15.21, (1350.04, (1513.66, (9093.21, (9125.88, (9017.17, (10.10, (12.13, (10.13, (1359.44, (11.37, (12.19, (18.21, (1676.46, (n, d) λ = λ = λ = λ = IPFS-C IPFS-R InexactALM ADMM IPFS-C IPFS-R InexactALM ADMM IPFS-C IPFS-R InexactALM ADMM IPFS-C IPFS-R InexactALM ADMM Time Cost Time Cost Time Cost Time Cost IPFS-C IPFS-R InexactALM ADMM IPFS-C IPFS-R InexactALM ADMM IPFS-C IPFS-R InexactALM ADMM IPFS-C IPFS-R InexactALM ADMM Time Cost Time Cost Time Cost Time Cost IPFS-C IPFS-R InexactALM ADMM Time Cost IPFS-C IPFS-R InexactALM ADMM IPFS-C IPFS-R InexactALM ADMM Time Cost IPFS-C IPFS-R InexactALM ADMM IPFS-C IPFS-R InexactALM ADMM IPFS-C IPFS-R InexactALM ADMM Time Cost Time Cost IPFS-C IPFS-R InexactALM ADMM IPFS-C IPFS-R InexactALM ADMM Time Cost Time Cost Time Cost Time Cost Figure Comparison IPFS-C (Cyclic variable selection rule) IPFS-R (Randomized variable selection rule) InexactALM ADMM (proximal ADMM-m) Constrained Concave Penalized Linear Regression.
 Datasets λ = λ = λ = λ = ×105 ×105 IPFS-C IPFS-R ADMM InexactALM Time Cost ×105 IPFS-C IPFS-R ADMM InexactALM ×105 IPFS-C IPFS-R ADMM InexactALM IPFS-C IPFS-R ADMM InexactALM ×105 Time Cost Time Cost IPFS-C IPFS-R ADMM InexactALM IPFS-C IPFS-R ADMM InexactALM ×105 IPFS-C IPFS-R ADMM InexactALM Time Cost IPFS-C IPFS-R ADMM InexactALM ×105 ×105 Time Cost Time Cost Time Cost IPFS-C IPFS-R ADMM InexactALM IPFS-C IPFS-R ADMM InexactALM Time Cost ×104 IPFS-C IPFS-R ADMM InexactALM Time Cost ×104 IPFS-C IPFS-R ADMM InexactALM IPFS-C IPFS-R ADMM InexactALM Time Cost ×104 IPFS-C IPFS-R ADMM InexactALM Time Cost IPFS-C IPFS-R ADMM InexactALM Time Cost IPFS-C IPFS-R ADMM InexactALM ×104 Time Cost Time Cost Time Cost Time Cost IPFS-C IPFS-R ADMM InexactALM IPFS-C IPFS-R ADMM InexactALM ×104 IPFS-C IPFS-R ADMM InexactALM ×104 Time Cost Time Cost Time Cost ×104 IPFS-C IPFS-R ADMM InexactALM Time Cost ×106 IPFS-C IPFS-R ADMM InexactALM Time Cost ×106 IPFS-C IPFS-R ADMM InexactALM Time Cost ×104 IPFS-C IPFS-R ADMM InexactALM Time Cost ×104 IPFS-C IPFS-R ADMM InexactALM Time Cost ×106 ×104 IPFS-C IPFS-R ADMM InexactALM Time Cost IPFS-C IPFS-R ADMM InexactALM Time Cost ×106 ×104 IPFS-C IPFS-R ADMM InexactALM Time Cost IPFS-C IPFS-R ADMM InexactALM Time Cost Figure Comparison IPFS-C (Cyclic variable selection rule) IPFS-R (Randomized variable selection rule) InexactALM ADMM (proximal ADMM-m) Nonconvex Support Vector Ma- chine.
 solving problem (1) theoretical guarantee.
 cyclic randomized block variable selection rules improve eﬃciency proposed algorithm high-dimensional data, conﬁrmed experiments nonconvex penalized linear regression support vector machine tasks.
 References [1] Y.
 Bengio.
 Learning deep architectures ai.
 Foundations trends R(cid:13) Machine Learning,
 [2] D.
 P.
 Bertsekas J.
 N.
 Tsitsiklis.
 Parallel Distributed Computation: Numerical Methods, volume
 Prentice hall Englewood Cliﬀs, NJ,
 [3] C.
 Cartis, N.
 I.
 M.
 Gould, P.
 L.
 Toint.
 complexity steepest descent, newton’s regularized newton’s methods nonconvex unconstrained optimization problems.
 SIAM journal optimization,
 [4] P.
 L.
 Combettes J-C.
 Pesquet.
 Stochastic quasi-fej´er block-coordinate ﬁxed point iterations random sweeping.
 SIAM Journal Optimization,
 [5] Q.
 T.
 Dinh, A.
 Kyrillidis, V.
 Cevher.
 inexact proximal path-following algorithm con- strained minimization.
 SIAM Journal Optimization,
 [6] Q.
 T.
 Dinh, I.
 Necoara, C.
 Savorgnan, M.
 Diehl.
 inexact perturbed path-following method lagrangian decomposition large-scale separable convex optimization.
 SIAM Journal Op- timization,
 [7] J.
 Eckstein D.
 P.
 Bertsekas.
 douglas-rachford splitting method proximal point algorithm maximal monotone operators.
 Mathematical Programming,
 [8] S.
 Ertekin, L.
 Bottou, C.
 L.
 Giles.
 Nonconvex online support vector machines.
 IEEE Trans- actions Pattern Analysis Machine Intelligence,
 [9] J.
 Fan R.
 Li. Variable selection nonconcave penalized likelihood oracle properties.
 Journal American statistical Association,
 [10] E.
 X.
 Fang, B.
 He, H.
 Liu, X.
 Yuan.
 Generalized alternating direction method multipliers: New theoretical insights applications.
 Mathematical Programming Computation,
 [11] J.
 Friedman, T.
 Hastie, R.
 Tibshirani.
 Elements Statistical Learning, volume
 Springer series statistics Springer, Berlin,
 [12] M.
 L.
 N.
 Gon¸calves, J.
 G.
 Melo, R.
 D.
 C.
 Monteiro.
 Convergence rate bounds proximal admm over-relaxation stepsize parameter solving nonconvex constrained problems.
 ArXiv Preprint:
 [13] P.
 Gong, C.
 Zhang, Z.
 Lu, J.
 Huang, J.
 Ye. general iterative shrinkage thresholding algorithm non-convex regularized optimization problems.
 ICML, pages
 C.
 C.
 Gonzaga.
 Path-following methods linear programming.
 SIAM review,
 [15] E.
 Hazan, K.
 Y.
 Levy, S.
 Shalev-Shwartz.
 graduated optimization stochastic non-convex problems.
 ICML, pages
 [16] B.
 He, H.-K.
 Xu, X.
 Yuan.
 proximal jacobian decomposition alm multiple- block separable convex minimization problems relationship admm.
 Journal Scientiﬁc Computing,
 [17] G.
 E.
 Hinton, S.
 Osindero, Y-W.
 Teh.
 fast learning algorithm deep belief nets.
 Neural Computation,
 [18] M.
 Hong, Z-Q.
 Luo, M.
 Razaviyayn.
 Convergence analysis alternating direction method multipliers family nonconvex problems.
 SIAM Journal Optimization,
 [19] G.
 M.
 James, C.
 Paulson, P.
 Rusmevichientong.
 constrained lasso.
 Technical report, University Southern California,
 [20] B.
 Jiang, T.
 Lin, S.
 Ma, S.
 Zhang.
 Structured nonconvex optimization: Algo- rithms iteration complexity analysis.
 ArXiv Preprint
 P-L.
 Lions B.
 Mercier.
 Splitting algorithms sum nonlinear operators.
 SIAM Journal Numerical Analysis,
 [22] D.
 C.
 Liu J.
 Nocedal.
 limited memory bfgs method large scale optimization.
 Mathematical programming,
 [23] S.
 Lu, M.
 Hong, Z.
 Wang.
 nonconvex splitting method symmetric nonnegative matrix factorization: Convergence analysis optimality.
 IEEE Transactions Signal Processing,
 [24] S.
 Lu, M.
 Hong, Z.
 Wang.
 stochastic nonconvex splitting method symmetric nonnegative matrix factorization.
 AISTATS, pages
 [25] J.
 G.
 Melo R.
 D.
 C.
 Monteiro.
 Iteration-complexity jacobi-type non-euclidean admm multi-block linearly constrained nonconvex programs.
 ArXiv Preprint:
 [26] Y.
 Nesterov A.
 Nemirovskii.
 Interior-point polynomial algorithms convex programming.

 [27] J.
 Nocedal S.
 Wright.
 Numerical Optimization.
 Springer Science Business Media,
 [28] H.
 Ouyang, N.
 He, L.
 Tran, A.
 Gray.
 Stochastic alternating direction method multipliers.
 ICML, pages
 [29] D.
 P.
 Palomar M.
 Chiang.
 tutorial decomposition methods network utility maximiza- tion.
 Selected Areas Communications, IEEE Journal on,
 [30] B.
 Peng.
 Methodologies Algorithms Non-convex Penalized Models Ultra High Dimensional Data.
 Thesis, University Minnesota,
 J.
 Renegar.
 mathematical view interior-point methods convex optimization.

 [32] Y.
 Wang, W.
 Yin, J.
 Zeng.
 Global convergence admm nonconvex optimization.
 ArXiv Preprint
 [33] L.
 Xiao, M.
 Johansson, S.
 P.
 Boyd.
 Simultaneous routing resource allocation dual decomposition.
 Communications, IEEE Transactions on,
 Proof Theorem construct potential function Φ(x, s, λ) present key technical lemma guarantees k=0 is non- increasing lower bounded conditions penalty parameter β >
 potential function Φ(x, s, λ) is deﬁned that, optimizing δ-smoothed version, i.e., problem (2), sequence (cid:8)Φ(xk, sk, λk)(cid:9)+∞ Φ(x, s, λ) = f (x) + r(x) − δ Xj=1 log(sj) + hλ, Ax + s − bi + kAx + s − bk2 δ is given smoothing parameter, β > is penalty parameter chosen according δ.
 A.1 Proof Technical Lemmas Lemma
 optimizing δ-smoothed version, exists s >
 following statement holds true, √2δ s2 β > k=0 is non-increasing lower bounded.
 conclude sequence (cid:8)Φ(xk, sk, λk)(cid:9)+∞ Φ(xk, sk, λk) → Φ∗, (cid:13)(cid:13)Axk+1 + sk+1 − b(cid:13)(cid:13)2 → (cid:13)(cid:13)xk+1 − xk(cid:13)(cid:13)2 → (cid:13)(cid:13)sk+1 − sk(cid:13)(cid:13)2 k → ∞, cyclic randomized variable selection rules, respectively.
 Furthermore, sequence (cid:8)(cid:0)xk, sk, λk(cid:1)(cid:9)+∞ Proof.
 follows update {xk+1 k=0 remains bounded.
 }i∈I {xk+1 }i /∈I (9) (10) Φ(xk, sk, λk) − Φ(xk+1, sk, λk) ≥ Furthermore, Φ(xk+1, s, λk) is convex respect s, using update sk+1, have Φ(xk+1, sk, λk) − Φ(xk+1, sk+1, λk) ≥ Finally, using update λk+1, have Φ(xk+1, sk+1, λk) − Φ(xk+1, sk+1, λk+1) = − (11) xk i − (cid:13)(cid:13)(cid:13) (cid:13)(cid:13)(cid:13) (cid:13)(cid:13)(cid:13) sk − sk+1(cid:13)(cid:13)(cid:13) β (cid:13)(cid:13)(cid:13) λk − λk+1(cid:13)(cid:13)(cid:13) follows, try show exists s such sk {sk}+∞ sk+1 j λk+1 exists ¯λ such λk j ≥ s i =


 p.
 is clear j=1 log(sj) is convex.
 obtain = δ combining update sk+1 λk+1.
 suﬃce show k=1 is bounded sequence −Pp j ≤ ¯λ.
 speciﬁc, have ∂ri(xk+1


 xk N − A⊤ = −∇if (xk ∋ −∇if (xk i  i(cid:17) − βA⊤ Xj≤i i (cid:16)sk+1 − sk(cid:17) i(cid:17) − βA⊤ N − A⊤ i =


 obtain A⊤ k=1 is bounded sequence, Xi set generalized gradient ri are bounded sets.
 loss generality, A = [A1 A2


 AN ] j ≤ ¯λ ¯λ >
 Therefore, have is assumed be full row rank, leads fact λk (12) i λk − (Li + i − xk i λk+1 − (Li + i − xk i λk is bounded {sk}+∞ j +Xj>i


 Ajxk+1 Ajxk j + sk − b Finally, combine (10), (11) (12) obtain (cid:13)(cid:13)(cid:13) λk − λk+1(cid:13)(cid:13)(cid:13)2 =(cid:13)(cid:13)(cid:13)(cid:13) sk − sk+1(cid:13)(cid:13)(cid:13)(cid:13)2 (cid:13)(cid:13)(cid:13) s2 (cid:13)(cid:13)(cid:13) sk − sk+1(cid:13)(cid:13)(cid:13)2 +(cid:18) β (cid:13)(cid:13)(cid:13) − Φ(xk, sk, λk) − Φ(xk+1, sk+1, λk+1) ≥ i − xk+1 xk µ2 βs4(cid:19)(cid:13)(cid:13)(cid:13) sk − sk+1(cid:13)(cid:13)(cid:13) (13) Furthermore, function Φ(cid:0)xk, sk, λk(cid:1) has been shown bounded [20], i.e., exists Φ∗ ∈ R such Φ(xk, sk, λk) ≥ Φ∗.
 cyclic variable selection rule, holds that, (cid:13)(cid:13)(cid:13) xk − xk+1(cid:13)(cid:13)(cid:13)2 =sXi∈I (cid:13)(cid:13)(cid:13) (cid:13)(cid:13)(cid:13) i − xk+1 xk k → +∞.
 randomized variable selection rule, take conditional expectation sides (13) obtain E"Xi∈I (cid:13)(cid:13)(cid:13) Xi=1(cid:20)(cid:13)(cid:13)(cid:13) i − xk+1 xk
 ≥ (cid:13)(cid:13)(cid:13) Φ(xk, sk, λk) − EhΦ(xk+1, sk+1, λk+1) | (xk, sk, λk)i ≥ pi ≥ pmin i =


 N have Φ(xk, sk, λk) − EhΦ(xk+1, sk+1, λk+1) | (xk, sk, λk)i ≥ pmin i − ˜xk+1 xk that, k=0 is super-martingale respect natural history; super-martingale convergence k=0 converges surely.
 Therefore, holds true ˜xk+1 is “virtual” iterate assuming variables are updated
 Thus(cid:8)Φ(xk, sk, λk)(cid:9)+∞ theorem, sequence (cid:8)Φ(xk, sk, λk)(cid:9)+∞ Similarly, obtain (cid:13)(cid:13)sk+1 − sk(cid:13)(cid:13)2 → (cid:13)(cid:13)Axk+1 + sk+1 − b(cid:13)(cid:13)2 = +∞ cyclic randomized variable selection rules, respectively.
 completes proof.
 (cid:13)(cid:13)(cid:13) xk − xk+1(cid:13)(cid:13)(cid:13)2 → k → +∞.
 β (cid:13)(cid:13)λk − λk+1(cid:13)(cid:13)2 → k → Lemma is easy see stopping criterion be satisﬁed.
 present lemma guarantees that, ¯x ∈ Rd approaches local solution problem (2), Lagrangian function is deﬁned Lδ (x, s, λ) = f (x) + r(x) − δ Xi=1 log(si) + hλ, Ax + s − bi

 Let (¯x, ¯s) satisﬁes stopping criterion cyclic randomized variable selection rules, respectively.
 (¯x, ¯s) is δ-stationary solution problem (2).
 is say, exists ¯λ ∈ Rp such following statement holds true, dist(cid:16)−∇if (¯x) − A⊤ ¯λ, ∂ri(¯xi)(cid:17) ≤ δ, i =


 n, (cid:13)(cid:13)¯sj ¯λj − δ(cid:13)(cid:13)2 ≤ δ, j =


 p, kA¯x + ¯s − bk2 ≤ δ, cyclic variable selection rule Ehdist(cid:16)−∇if (¯x) − A⊤ ¯λ, ∂ri(¯xi)(cid:17)i ≤ δ, i =


 n, E(cid:2)(cid:13)(cid:13)¯sj ¯λj − δ(cid:13)(cid:13)2(cid:3) ≤ δ, j =


 p, E [kA¯x + ¯s − bk2] ≤ δ, randomized variable selection rule.
 Proof.
 optimizing δ-smoothed version, ﬁrst-order optimality condition (xk+1, sk+1, λk+1) is i − xk+1 xk (cid:13)(cid:13)(cid:13)2  j λk+1 = δ ≤ j ≤ p.
 j − xk+1 xk i λk+1, ∂ri(xk+1 Xj=1
 )(cid:17) ≤ D dist(cid:16)−∇if (xk+1) − A⊤ i ∈ D = max (cid:13)(cid:13)(cid:13)2 kAjk2(cid:13)(cid:13)(cid:13)  + (2Li + β (cid:13)(cid:13)λk − λk+1(cid:13)(cid:13)2 sk+1 Furthermore, have (cid:13)(cid:13)Axk+1 + sk+1 − b(cid:13)(cid:13)2 = follows Lemma exists large ¯K such that, k ≥ ¯K, have (cid:13)(cid:13)(cid:13)2 (cid:13)(cid:13)(cid:13)2  + (Li + xk j − xk+1 xk i − xk+1  ≤ δ, β (cid:13)(cid:13)(cid:13) λk − λk+1(cid:13)(cid:13)(cid:13)2 ≤ δ, (cid:13)(cid:13)(cid:13)2  + (Li +  ≤ δ, E(cid:13)(cid:13)(cid:13) λk − λk+1(cid:13)(cid:13)(cid:13)2 ≤ δ, cyclic variable selection rule, E D kAjk2(cid:13)(cid:13)(cid:13) kAjk2(cid:13)(cid:13)(cid:13) Xj=1 Xj=1 (cid:13)(cid:13)(cid:13)2 j − xk+1 xk i − xk+1 xk D randomized variable selection rule.
 case, stopping criterion is satisﬁed, conclusion, obtain above inequality holds (cid:0)¯x, ¯s, ¯λ(cid:1) optimizing δ-smoothed version.
 ¯λ, ∂ri(¯xi)(cid:17) ≤ δ, i =


 n, ¯sj ¯λj − δ = j =


 p, kA¯x + ¯s − bk2 ≤ δ, dist(cid:16)−∇if (¯x) − A⊤ cyclic variable selection rule, Ehdist(cid:16)−∇if (¯x) − A⊤ ¯λ, ∂ri(¯xi)(cid:17)i ≤ δ, i =


 n, E(cid:2)¯sj ¯λj − δ(cid:3) = j =


 p, E [kA¯x + ¯s − bk2] ≤ δ, randomized variable selection rule.
 A.2 Proof Theorem sequence (¯x, ¯s) remains bounded δ set limiting points is non-empty.
 consider sub-sequence (¯x, ¯s) indexed {kl}+∞ l=1 converges (x∗, s∗).
 using Lemma conclude exists ¯λ ∈ Rp such (¯x, ¯s) indexed kl satisﬁes dist(cid:16)−∇if (¯x) − A⊤ ¯λ, ∂ri(¯xi)(cid:17) ≤ (γ)klδ0, i =


 n, ¯sj ¯λj − (γ)klδ0(cid:13)(cid:13)(cid:13)2 ≤ (γ)klδ0, j =


 p, kA¯x + ¯s − bk2 ≤ (γ)klδ0, (cid:13)(cid:13)(cid:13) cyclic variable selection rule Ehdist(cid:16)−∇if (¯x) − A⊤ ¯λ, ∂ri(¯xi)(cid:17)i ≤ (γ)klδ0, i =


 n, ¯sj ¯λj − (γ)klδ0(cid:13)(cid:13)(cid:13)2i ≤ (γ)klδ0, j =


 p, E [kA¯x + ¯s − bk2] ≤ (γ)klδ0, Eh(cid:13)(cid:13)(cid:13) randomized variable selection rule.
 is initial smoothing parameter.
 know (γ)kl → l → +∞ γ ∈ (0,
 conclude x∗ is stationary solution problem (1) cyclic variable selection rule terms expectation randomized variable selection rule.
 completes proof.
 B Proof Theorem B.1 Proof Technical Lemma present technical lemma show number iterations required reach stopping criterion optimizing δ-smoothed version problem (1), i.e., problem (2).
 Speciﬁcally, number is disproportionate value δ, makes lot sense problem (2) becomes harder δ

 Suppose cyclic randomized variable selection rule is employed optimizing δ-smoothed version, i.e., problem (2), stopping criterion is satisﬁed terms expectation, respectively, O( δ iterations.
 Proof.
 Lemma suﬃces show that, exists C > such that, k ≥ C statement holds true, δ following D Xj=1 kAjk2(cid:13)(cid:13)(cid:13) j − xk+1 xk cyclic variable selection rule, E D Xj=1 kAjk2(cid:13)(cid:13)(cid:13) j − xk+1 xk i − xk+1 xk (cid:13)(cid:13)(cid:13)2 (cid:13)(cid:13)(cid:13)2  + (Li +  ≤ δ, β (cid:13)(cid:13)(cid:13) λk − λk+1(cid:13)(cid:13)(cid:13)2 ≤ δ, (cid:13)(cid:13)(cid:13)2  + (Li + E(cid:13)(cid:13)(cid:13)  ≤ δ, λk − λk+1(cid:13)(cid:13)(cid:13)2 ≤ δ, (cid:13)(cid:13)(cid:13)2 i − xk+1 xk randomized variable selection rule.
 follows Lemma that, ∀K ≥ have Φ(cid:0)x0, s0, λ0(cid:1) − Φ∗ ≥ Xk=0 Xi∈Ik+1(cid:13)(cid:13)(cid:13) xk i − xk+1 +(cid:18) βs2 − (cid:13)(cid:13)(cid:13) βs2(cid:19)(cid:13)(cid:13)(cid:13) λk − λk+1(cid:13)(cid:13)(cid:13)  (14) Ik is denoted active set chosen k-th iteration inner loop optimizing δ-smoothed version.
 Combining fact yields iteration complexity is O( δ ).
 using similar technique, obtain same (cid:13)(cid:13)(cid:13) xk − xk+1(cid:13)(cid:13)(cid:13) = Xi∈Ik+1(cid:13)(cid:13)(cid:13) i − xk+1 xk (cid:13)(cid:13)(cid:13) complexity randomized variable selection rule, holds true j − xk+1 xk kAjk2(cid:13)(cid:13)(cid:13) D E Xj=1 completes proof.
 B.2 Proof Theorem i − xk+1 xk (cid:13)(cid:13)(cid:13)2  + (Li + E(cid:13)(cid:13)(cid:13) (cid:13)(cid:13)(cid:13)2  ≤ δ, λk − λk+1(cid:13)(cid:13)(cid:13)2 ≤ δ.
 hand, is clear derive δ ← γδ δ ≤ ǫ is satisﬁed O(log( ǫ )) iterations term iteration number outer loop.
 other hand, using Lemma obtain iterations required term iteration number inner loop is T = log( ǫ Xk=0 γkδ0 δ0 ǫ − γ − ≤ Cγ δ0 − γδ0 implies iterations required term iteration number inner loop is O( completes proof.
 ǫ ).

 Pioneered Deep Q-network [Mnih et al., followed various extensions advancements [Mnih et al., Lillicrap Schulman et Schulman et Deep Reinforcement Learning (DRL) algorithms show great potential solving high-dimensional real-world robotics sensory control tasks.
 However, DRL methods require several millions training sam- ples, making infeasible train real robotic systems.
 result, DRL algorithms are trained simulated environments, transferred deployed real scenes.
 However, reality gap, referred ∗indicates equal contribution.
 domain shift, noise pattern, texture, lighting con- dition discrepancies, etc., synthetic renderings real sensory readings, imposes major challenges gener- alizing sensory control policies trained simulation reality.
 paper, focus visual control tasks, au- tonomous agents perceive environment onboard cameras, execute commands based color image read- ing streams.
 natural way typical choice recent literature dealing reality gap visual con- trol, is increasing visual ﬁdelity simulated im- ages [Bousmalis et al., matching distribution synthetic images real ones [Tobin et al., adapting learned features represen- tations simulated domain real-world domain [Rusu
 sim-to-real methods, however, have add preprocessing steps individ- ual training frame expensive learning pipeline DRL control policies; complete policy training phase has be conducted different real-world scene.
 Attempts have been made computer graphics increase quality simulators, make rendered images realis- tic; however, rendering detailed realistic texture modality adds computational burden.
 paper attempts tackle reality gap visual control domain novel perspective, aim adding minimal extra computational burden learning pipeline.
 cope reality gap ac- tual deployment phase agents real-world scenarios, adapting real camera streams synthetic modality, translate unfamiliar unseen features real images simulated style, agents have learned deal training simulators.
 Compared other sim-to-real methods bridging reality gap, proposed real-to-sim approach, refer VR Goggles, has several appealing properties: • First approach is lightweight.
 does add extra processing burden training phase DRL policies.
 • Secondly, proposed method is ﬂexible efﬁcient.
 decouple policy training adaptation preparations transferring polices simulation real world be con- ducted parallel training DRL control poli- cies.
 different real-world environ- ment expect deploy agent in, need collect several order im- ages, train model VR Goggles them.
 importantly, do need retrain ﬁnetune visual control policy new environments.
 additional contribution, propose new shift loss, enables generate consistent synthetic image streams information impose temporal constraints, sequential training data.
 show shift loss is promising much cheaper alternative constraints im- posed optical ﬂow, demonstrate effectiveness artistic style transfer videos domain adaptation.
 Related Works Domain Adaptation Domain adaptation, referred image-to-image trans- lation, targets translating images source domain target domain.
 focus general unsu- pervised methods require least manual effort are applicable robotics control tasks.
 CycleGAN [Zhu et introduced cycle- consistent loss enforce inverse mapping tar- get domain source domain top source target mapping.
 does require paired data domains interest, shows convincing results simple data distributions containing few semantic types.
 However, terms translating com- plex data distributions containing many semantic types, results are satisfactory, permutations se- mantics occur.
 CyCADA [Hoffman et added semantic constraint top, enforce match semantic map translated image input.
 However, semantic loss was added experiments large datasets due memory limitations.
 Following observation several recent ad- vanced robotics simulators do provide semantic ground truth, semantic segmentation literature is mature (e.g., [Chen et adopt semantic constraint CyCADA method.
 are able include seman- tic loss calculation special conﬁgurations (Sec.

 Domain Adaptation DRL DRL approaches have been adopted robotics control tasks such manipulation navigation.
 review recent literature emphasis works taking re- ality gap consideration.
 manipulation, [Bousmalis bridged re- ality gap adapting synthetic images realistic do- main training phase.
 However, addition adaptation step training iteration slow whole learning pipeline.
 [Tobin al., proposed ramdomise texture objects, lighting condi- tions, camera positions training, hope learned model generalize real-world sce- narios.
 However, such randomization cannot be satisﬁed low cost most popular robotic simu- lators.
 Moreover, is guarantee randomized simulations cover visual modality random real- world scene.
 [Rusu al., deals reality gap adapting learned features represen- tations model trained simulation realistic domain.
 method, however, needs go expensive control policy training phase dif- ferent real-world scenario.
 Artistic style transfer works video sequences individual frames.
 targets generating consistent stylizations sequential frames.
 [Ruder al., provides key observation that: trained stylization network total downsampling factor s (e.g., s = network convolutional layers stride is shift invariant shifts equal multiples s pixels, output different stylizations otherwise.
 un- navigation, autonomous agents are expected encounter sensor readings environments larger scale manipulation, reality gap has been dealt literature learning-based visual control best knowledge.
 works, however, chose spe- cial setups circumvent reality gap.
 Lidar [Tai et Zhang et al., depth images [Zhang et al., Tai al., are chosen sensor modality transferring navigation policies real world, discrepancies simu- lated domain real-world domain are smaller color images.
 [Zhu al., conducted real-world experiments visual inputs.
 se- real-world scene is similar simulated environment, is condition be met practice.
 paper, consider domain adaptation visual navigation tasks using DRL, has been considered literature.
 believe adaptation navigation is challenging manipula- tion, navigation agents work environments much larger scales more complexities con- ﬁned workspace manipulators.
 believe proposed real-to-sim method be adopted manipulation.
 important aspect domain adaptation, con- text dealing reality gap DRL, is consis- tency subsequent frames, has been considered aforementioned adaptation meth- ods.
 method solving sequential decision making, consistency subsequent input frames DRL agents be critical successful fulﬁllment ﬁ- nal goals.
 solutions solving reality gap DRL, general domain adaptation literature lacks works considering sequential frames single frames.
 Therefore, look borrow techniques other re- search ﬁelds extend single-frame algorithms video domain, applicable meth- ods are artistic style transfer literature.
 Artistic Style Transfer Videos Artistic style transfer is technique transfering artistic style artworks photographs [Johnson
 Figure VR Goggles pipeline.
 depict computation losses LGANX, LcycY LsemY LshiftX.
 show outdoor indoor scenarios demonstration, adaptation outdoor scene is trained semantic loss Lsem (since simulated domain CARLA has ground semantic labels train segmentation network fX), indoor (since simulated domain Gazebo does provide semantic ground truth).
 components marked red are involved ﬁnal deployment phase: real sensor reading is captured (y ∼ preal), passed Goggles module (generator GX) be translated simulated domain X, DRL agents were trained; translated image ˆx is fed DRL agents, output control commands autonomous vehicles.
 clarity, skip counterpart losses LGANY LcycX, LsemX LshiftY
 desired property (of being shift invariant) causes out- put trained network change small changes input, leads temporal in- consistency (under assumption lim- ited changes appear subsequent frames in- coming sequential data).
 However, solution adding temporal constraints generated subsequent frames, is expensive, requires optical ﬂow input deployment.
 [Huang al., incorporated temporal constraint single-frame artistic style transfer pipeline is cheap solution.
 However, believe constraining optical ﬂow single input images is well- deﬁned.
 suspect improved temporal consistency [Huang al., is due im- posed consistency constraints regional shifts optical ﬂow.
 validate suspicion experiments (Sec.

 believe fundamental problem causing in- consistency (the shift variance) be solved additional constraint shift loss, introduce Sec.
 show shift loss enables constrain consis- tency generated subsequent frames, need expensive optical ﬂow constraint.
 argue network has been trained learn smooth function small changes input result small changes output.
 Methods Problem formulation consider visual data sources domains: X, con- taining sequential frames {x0, x1, x2,···} (e.g., synthetic images output simulator; x ∼ psim, psim de- notes simulated data distribution), Y, containing se- quential frames {y0, y1, y2,···} (e.g., real camera readings onboard camera mobile robot; y ∼ preal, preal denotes distrbution real sensory readings).
 emphasize that, require method generate consistent outputs sequential inputs, do need training data be sequential; formalize way baseline methods have requirement.
 have discussed, DRL agents are trained simulated domain X, are expected perform tasks real-world domain Y.
 have discussed choose tackle problem translating images real domain images synthetic domain dur- ing deployment.
 following introduce details approach performing domain adaptation.
 cope sequential nature incoming data streams, introduce technique constraining consistency translated subsequent frames.
 CycleGAN Loss achieve this, ﬁrst build top CycleGAN [Zhu et learns generative models map domains: GY X → Y, discriminator DY, GX Y → X, discriminator DX, training GANs simultaneously: LGANY (GY, DY; X, Y) = Ey [log DY(y)] LGANX (GX, DX; Y, X) = Ex [log DX(x)] (1) + Ex [log(1 − DY(GY(x)))] (2) + Ey [log(1 − DX(GX(y)))] GY learns generate images GY(x) matching domain Y, GX tries translating y im- ages domain X.
 Following CycleGAN, add cycle consistency loss constrain mappings: LcycY (GX, GY; Y) = Ey [||GY(GX(y)) − y||1] LcycX (GY, GX; X) = Ex [||GX(GY(x)) − x||1]
 (3) Semantic Loss translation domains interest are syn- thetic images real-world sensor images, take advan- tage fact many recent robotic simulators provide ground truth semantic labels add semantic constraint inspired CyCADA [Hoffman et
 X∼psimGXGYˆYLcycYLsemYLshiftXfXfXˆX[x→·,y→·]GXY[x→·,y→·]LGANXDXtrainedinXY∼prealindoorˆXoutdooroutdoorindoorindooroutdoor Assuming images domain ground truth semantic information SX is semantic segmenta- tion network fX be obtained minimizing cross-entropy loss, denoted CrossEnt(SX, fX(X)).
 further assume ground truth semantic do- main Y is lacking (which is case most real scenarios), meaning fY is accessible.
 case, provide ”semi” semantic supervision training agents.
 fX semantic segmentaion domain X is ob- tained, ”semi” semantic supervision generators be incorporated, imposing consistency seman- tic map input generated output.
 consistent image translation be achieved minimizing following losses (we fX generate ”semi” semantic labels domain Y): LsemY (GY; X, fX) = CrossEnt(fX(X), fX(GY(X))) (5) LsemX (GX; Y, fX) = CrossEnt(fX(Y), fX(GX(Y))) Shift Loss Consistent Generation literature image-to-image Different current translation domain model is expected output consistent images sequential input data.
 adding Lsem, semantics con- secutive outputs are constrained, inconsistences artifacts occur often.
 Moreover, cases ground truth semantics are unavailable domain, sequential outputs are less constrained, lead inconsistent DRL policy outputs.
 constrain consistency situations, following discussion Sec.
 introduce shift loss below.
 input image x, use x[x→i,y→j] denote re- sult shift shifting x X axis i pixels, j pixels Y axis.
 omit y x → subscript image is shifted X Y axis.
 According [Ruder trained stylization net- work is shift invariant shifts multiples s pixels (s represents total downsampling factor network), output different stylizations otherwise.
 causes output trained network change small changes input.
 propose add simple direct effective shift LshiftY (GY;X) = E (cid:104)(cid:12)(cid:12)(cid:12)(cid:12)GY(x)[x→i,y→j] − GY(x[x→i,y→j]) (cid:12)(cid:12)(cid:12)(cid:12)2 (cid:104)(cid:12)(cid:12)(cid:12)(cid:12)GX(y)[x→i,y→j] − GX(y[x→i,y→j])(cid:12)(cid:12)(cid:12)(cid:12)2 LshiftX (GX;Y) = E y, i,j∼u(1,s−1) x, i,j∼u(1,s−1) (cid:105) (cid:105) (7) (8) u denotes uniform distribution.
 Shift loss constrains shifted output match output shifted input, regarding shifts image-scale move- ments.
 assumption limited regional move- ment appear subsequent input frames, shift loss ef- smoothes mapping function small regional movements, restricting changes outputs subse- quent inputs frames.
 be regarded cheap alternative imposing consistency constraints small movements, eliminating need expensive optical ﬂow information, is crucial meeting requirement real-time control robotics.
 Full Objective full objective learning VR Goggles (Fig.
 is: L(GY, GX, DY, DX; X, Y, fX) = (9) LGANY (GY, DY; X, Y) + LGANX(GX, DX; Y, X) LcycY (GX, GY; Y) + LcycX (GY, GX; X)(cid:1) + λcyc + λsem (LsemY (GY; X, fX) + LsemX (GX; Y, fX)) + λshift (LshiftY (GY; X) + LshiftX (GX; Y)) λcyc, λsem λshift controls weighting loss.
 corresponds solving following optimization: DY,DX L(GY, GX, DY, DX).
 (10) G∗Y, G∗X = arg GY,GX Experiments Artistic Style Transfer Videos evaluate method, conduct experiments artistic style transfer video sequences, validate ef- fectiveness shift loss constraining consistency se- quential frames.
 collect training dataset HD video footage sequences (from VIDEVO1, containing frames total); Sintel2 sequences are used testing, ground-truth optical ﬂow is available.
 compare per- formance models trained following setups: • FF [Johnson et Conanical feed style transfer trained single frames; • FF+ﬂow [Huang et FF trained sequential images, optical ﬂow added imposing temporal constraints subsequent frames.
 shift loss, discussed Sec.
 • Ours: FF trained single frames, additional do compare method [Ruder et require optical ﬂow input deploy- ment.
 is expensive target application real-time control.
 Implementation-wise, use pretrained VGG-19 loss network, relu2 content layer, relu1 relu2 relu3 relu4 style layers.
 set weight loss follows: content, style, 1e-7 spa- tial regularization, optical ﬂow, shift.
 downsampling factor s transformer network is use same transformer network architecture style im- ages [Johnson
 Shifts are uniformly sam- pled [1, s training frame.
 proof concept, begin evaluation compar- ing setups ability generate shift invariant styl- izations.
 particular, image x testing dataset, generate more test images shifting original image X axis pixels respectively, pass frames (x, x[x→1], x[x→2], x[x→3], x[x→4]) trained network examine consistency generated images (Fig.

 Figure Temporal error maps generated stylizations subsequence input frames.
 row: input frames; ∼ row: temporal error maps (with corresponding stylizations shown top) outputs FF, FF+ﬂow, Ours.
 choose challenging style (mosaic) temporal consistency, contains many ﬁne details, tiny tiles laid original image ﬁnal stylizations.
 Yet, Ours achieves high consistency.
 lower temporal loss shift loss constraint.
 Domain Adaptation Outdoor Scenarios Next validate shift loss ﬁeld domain adapta- tion, outdoor urban street scenarios col- lect synthetic domain images X ∼ psim CARLA sim- ulator [Dosovitskiy al., realistic domain images Y ∼ preal RobotCar dataset [Maddern
 compare following setups: • CyCADA [Hoffman et CycleGAN se- mantic constraints, trained single frames; • CyCADA+ﬂow: CyCADA temporal constraint [Huang al., trained sequential frames; • Ours: CyCADA shift loss, trained single frames; refer VR Goggles.
 Table Temporal loss comparison FF, FF+ﬂow Ours.
 metric is part optimization objective FF+ﬂow, optical ﬂow is provided Ours; method is able achieve lower temporal loss evaluated Sintel sequences.
 Figure Shift-invariance evaluation, comparing FF, FF+ﬂow Ours.
 shift input image x X axis pixels feed frames net- works trained FF, FF+ﬂow Ours, show generated stylizations.
 mark visible differences small circles dim rest generated images.
 is discussed [Ruder FF generates identical stylizations x x[x→4] (because is multiple total downsampling factor trained network), x[x→1],x[x→2],x[x→3] differ sig- niﬁcantly.
 FF+ﬂow improves shift-invariance, suspect improvement is due inexplicit consistency constraint regional shifts imposed optical ﬂow.
 Ours, is able generate shift-invariant stylizations proposed shift loss.
 results shown Fig.
 validate discussion [Ruder et stylizations x x[x→4] FF are identical (s = trained network), differ otherwise.
 FF-ﬂow improves in- variance limited amount; Ours method is capable gen- erating consistent stylizations shifted input frames, shift loss reducing shift variance.
 continue evaluating consistency stylized Fig.
 show temporal sequential input frames.
 error maps, same metric [Huang al., stylized consecutive frames method.
 Ours (bottom row) achieves highest temporal consistency.
 Furthermore, evaluate temporal loss computed us- ing ground optical ﬂow Sintel sequences (Ta- ble
 temporal loss is part optimization objective FF-ﬂow, method does have access optical ﬂow information, Ours is able achieve x[x→1]xx[x→2]x[x→3]x[x→4]FFABOursABFF+ﬂowABmosaiclamuse0.1190.0930.0860.1320.1080.1100.1270.1040.0980.1150.0890.0830.1240.0950.087FFOursFF+ﬂow0.1220.0960.0900.1130.0910.0890.1130.0850.0780.1520.1300.1270.1250.0990.0920.1540.1310.1300.1230.0970.0900.1350.1120.1120.1380.1080.1070.1210.1000.0920.1320.1060.0940.1290.1040.0940.1140.0900.0910.1270.0960.0830.1320.1020.101FFOursFF+ﬂowambush5bamboo1market6temple2sleeping2shaman3alley2bamboo2alley1sleeping1 Figure Real-world visual control experiment.
 DRL agent is trained simulated ofﬁce environment, is able navigate chairs based visual input.
 retraining ﬁnetuning DRL policy, proposed VR Goggles enables mobile robot deploy policy real ofﬁce environments, achieving success rate set real-world experiments.
 refer attached video details real-world experiments.
 Speciﬁcally, begin training DRL agent simu- lated ofﬁce environment (A3C [Mnih et roll- outs, parallel CPU threads), accomplish task navi- gating chairs based front-facing color camera readings; agent obtained reward −0.005 step cost, −0.05 collision, reaching target.
 deploy trained DRL policy real robot real- world ofﬁce, compare following adaptations: • NoGoggles: Feed sensor readings trained DRL policy; • CycleGAN [Zhu et Use CycleGAN trans- real sensory inputs synthetic domain be- fore feeding DRL policy; synthetic do- main (Gazebo) does provide ground se- mantics, drop semantic constraint Lsem; VR Goggles translate input images.
 • Ours: Use models trained CycleGAN + shift loss use same network conﬁguration Sec.
 input images are size ×
 show attached video that, domain adap- tation, deploying DRL policy fails real-world proposed method achieves high- est success rate NoGoggles, Cycle- GAN Ours respectively) due quality consis- tency translated streams.
 control cycle runs real- time Nvidia Jetson TX2.
 video, show VR Goggles train new model new type chair adjustment trained control policy.
 limited velocity robot due camera exposure time, motion blur in- ﬂuence adaptation quality.
 leave future work evaluate compatible platforms.
 Conclusions conclude, tackle reality gap deploying DRL visual control policies trained simulation real world, translating real image streams synthetic domain deployment phase.
 Due sequential nature incoming sensor streams control tasks, propose shift loss increase consistency translated subsequent frames.
 validate shift loss artistic Figure Comparison translated images sequential input frames different approaches.
 row: subsequent input frames realistic domain, several representative images simulated domain shown between; ∼ row: out- puts CyCADA, CyCADA+ﬂow Ours.
 method is able output consistent subsequent frames eliminate artifacts.
 adjust brightness zoom-ins visualization purposes.
 pretrain segmentation network fX using Deeplab [Chen et
 is worth mentioning original CyCADA paper did use semantic constraint experiments due memory issues.
 are able incorpo- rate semantic loss calculation, cropping input images.
 naive random crop likely lead se- mantic permutations; crop inputs domains same training iteration same random position, empirical results show stabilizes adaptation.
 input images are size × train network × crops.
 use same network architecture CycleGAN, train epochs learning rate − observe performance gain training longer iterations.
 show comparison subsequent frames generated approaches.
 method achieves highest consistency eliminates artifacts due smoothness learned model.
 Fig.
 Domain Adaptation Indoor Scenarios Real-world Robotic Experiments Finally, conduct domain adaptation indoor ofﬁce en- vironments (X ∼ psim rendered self-built Gazebo [Koenig et world Y ∼ preal captured real ofﬁce, using RealSense R200 camera mounted Turtlebot3 Wafﬂe).
 validate proposed method us- ing VR Goggles facilitate transfer polices trained simulated domain realistic domain, set real-world robotic experiments.
 CycleGANNoGogglesVRGogglesDRLpolicytrainedinGazebo0%60%100%Successrate style transfer videos, domain adaptation.
 end, verify domain adaptation method vi- sual control set real-world robot experiments.
 Several future works be conducted based method.
 example, training DRL agents more com- plicated tasks, complicated simulated environ- ments provide ground truth semantic labels, such released MINOS [Savva et
 Also, paper have focused learning based visual navigation, applying method manipulation tasks be interesting direction.
 References [Bousmalis et al., Konstantinos Bousmalis, Alex Ir- pan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey, Mri- nal Kalakrishnan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt Konolige, al.
 Using simulation domain adapta- tion improve efﬁciency deep robotic grasping.
 arXiv preprint arXiv:1709.07857,
 [Chen et LC Chen, G Papandreou, Kokkinos, K Murphy, AL Yuille.
 Deeplab: Semantic image seg- mentation deep convolutional nets, atrous convolu- tion, connected crfs.
 IEEE transactions pat- tern analysis machine intelligence,
 [Dosovitskiy al., Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, Vladlen Koltun.
 Conference Carla: open urban driving simulator.
 Robot Learning, pages
 [Hoffman et Judy Hoffman, Eric Tzeng, Tae- sung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei A Efros, Trevor Darrell.
 Cycada: Cycle- consistent adversarial domain adaptation.
 arXiv preprint arXiv:1711.03213,
 [Huang al., Haozhi Huang, Hao Wang, Wenhan Luo, Lin Ma, Wenhao Jiang, Xiaolong Zhu, Zhifeng Li, Wei Liu.
 Real-time neural style transfer videos.
 Proceedings IEEE Conference Computer Vision Pattern Recognition, pages
 [Johnson al., Justin Johnson, Alexandre Alahi, Li Fei-Fei.
 Perceptual losses real-time style transfer European Conference Com- super-resolution.
 puter Vision, pages

 [Koenig al., Nathan Koenig, B A, Andrew Howard.
 Design use paradigms gazebo, open- Intelligent Robots source multi-robot simulator.
 Systems,
 Proceedings.
 IEEE/RSJ International Conference on, volume pages
 IEEE,
 [Lillicrap al., Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, Continuous con- David Silver, Daan Wierstra.
 arXiv preprint trol deep reinforcement learning.

 [Maddern al., Will Maddern, Geoff Pascoe, Chris Linegar, Paul Newman.
 Oxford RobotCar Dataset.
 International Journal Robotics Research (IJRR),
 [Mnih al., Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle- mare, Alex Graves, Martin Riedmiller, Andreas K Fidje- land, Georg Ostrovski, al.
 Human-level control deep reinforcement learning.
 Nature,
 [Mnih al., Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu.
 Asyn- chronous methods deep reinforcement learning.
 International Conference Machine Learning, pages
 [Ruder al., Manuel Ruder, Alexey Dosovitskiy, Thomas Brox.
 Artistic style transfer videos spher- ical images.
 arXiv preprint arXiv:1708.04538,
 [Rusu al., Andrei A Rusu, Matej Veˇcer´ık, Thomas Roth¨orl, Nicolas Heess, Razvan Pascanu, Raia Had- sell.
 Sim-to-real robot learning pixels progres- sive nets.
 Conference Robot Learning, pages
 [Savva al., Manolis Savva, Angel X Chang, Alexey Dosovitskiy, Thomas Funkhouser, Vladlen Koltun.
 Minos: Multimodal indoor simulator navigation complex environments.
 arXiv preprint arXiv:1712.03931,
 [Schulman et al., John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, Philipp Moritz.
 Trust International Conference region policy optimization.
 Machine Learning, pages
 [Schulman et al., John Schulman, Filip Wolski, Pra- fulla Dhariwal, Alec Radford, Oleg Klimov.
 Prox- arXiv preprint imal policy optimization algorithms.

 [Tai al., Lei Tai, Giuseppe Paolo, Ming Liu.
 Virtual-to-real deep reinforcement Continuous control mobile robots mapless navigation.
 IEEE/RSJ International Conference Intelligent Robots Systems (IROS), pages Sept
 [Tai al., Lei Tai, Jingwei Zhang, Ming Liu, Wolfram Burgard.
 Socially-compliant navigation raw depth inputs generative adversarial imitation learning.
 Robotics Automation (ICRA), IEEE International Conference on, May
 [Tobin al., Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, Pieter Abbeel.
 Domain randomization transferring deep neural net- Intelligent works simulation real world.
 Robots Systems (IROS), IEEE/RSJ International Conference on, pages
 IEEE,
 [Zhang al., Jingwei Zhang, Jost Tobias Springen- berg, Joschka Boedecker, Wolfram Burgard.
 Deep reinforcement learning successor features navi- gation similar environments.
 IEEE/RSJ In- ternational Conference Intelligent Robots Systems (IROS), pages Sept
 [Zhang al., Jingwei Zhang, Lei Tai, Joschka Boedecker, Wolfram Burgard, Ming Liu.
 Neural slam.
 arxiv preprint.
 arXiv preprint arXiv:1706.09520,
 [Zhu al., Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A Efros.
 Unpaired image-to-image translation using cycle-consistent adversarial networks.
 Proceed- ings IEEE Conference Computer Vision Pat- tern Recognition, pages
 [Zhu al., Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, Ali Farhadi.
 Target-driven visual navigation indoor scenes Robotics Au- using deep reinforcement learning.
 tomation (ICRA), IEEE International Conference on, pages
 IEEE,
 Gaussian Processes (GPs) [Rasmussen, are powerful tools regression classiﬁcation problems mod- els are able learn complex representation data expressive covariance kernels.
 However, application GP real-world is limited due poor scalability.
 training data size n, GPs requires O(n3) computation O(n2) storage training O(n2) computation storage inference single test point.
 popular approach scale GPs larger [Seeger et al., dataset is inducing points methods Titsias, Lawrence et
 methods, enables GPs be applied larger datasets, result degradation performance due requirement choos- ing m (cid:28) inducing points [Wilson et
 way scaling GPs is structure exploita- tion [Saatc¸i, Wilson et
 methods uti- lizes existing algebraic structure covariance matrix ∗Equal contribution achieve fast exact learning inference.
 However, methods require input data have grid structure do hold real-world data
 Recently, structured kernel interpolation (SKI) frame- work KISS-GP [Wilson Nickisch, further im- scalability GPs unifying inducing points methods structure exploitation.
 SKI extends structure exploiting approaches located data interpo- lating covariance matrix multidimensional grid (the in- ducing points).
 SKI framework KISS-GP, train- ing cost reduce O(n) computation storage, al- constant time prediction [Wilson
 KISS-GP be scalable, number inducing points grid grows dimensions requires kernel be separable stationary order exploit structure.
 other hand, have seen trend deploying machine learning systems servers various resource- limited devices, such mobile phone, robotics etc et
 Modern machine learning models such deep neural networks consist millions parameters cannot be deployed devices due com- putational storage limitations.
 Many works have stud- ied compress large neural networks smaller deployment complex models becomes possible resource-limited devices [Buciluˇa et al., Hinton Chen Han et
 GPs have shown great applicability tasks resource-limited devices, such control robotics [Deisen- roth al., modeling ﬂexibility uncer- tainty measurements.
 previous solutions scal- ing GPs focus training GPs scratch.
 methods be applied deployment GPs resource-limited devices, potential beneﬁts transferring knowledge trained model were dis- cussed.
 wish investigate possibility compressing trained exact GP model smaller approximate GP model preserve predictive power exact model.
 paper, propose, kernel distillation, framework approximate trained GP model.
 Kernel distillation ex- tends inducing point methods insights SKI frame- work utilizes knowledges trained model.
 particular, contributions are: approximate exact kernel matrix sparse low-rank structured matrix.
 formulate kernel matrix approximation problem constrained F -norm minimization problem, leading accurate kernel approximation compared previous approximation ap- proaches.
 • method is general purpose kernel approximation method.
 do require kernel function be sepa- rable stationary do assume input data have special structure.
 • KISS-GP suffers curse dimensionality due grid structure inducing points.
 Instead, select in- ducing points using clustering algorithm input data, allow handle high-dimensional case worrying explosion inducing points.
 show application kernel distillation is fast accurate prediction GP.
 evaluate approach various real-world datasets, empirical results ev- idence kernel distillation better preserving predictive power trained GP model im- proving speed compared other al- ternatives.
 Background Gaussian Processes provide brief introduction Gaussian Processes regression problem paper.
 denote dataset D consists input feature vectors X = {x1,


 xn} real-value targets y = {y1,


 yn}.
 Gaussian Process (GP) is collection random vari- ables such ﬁnite subset such random variables have joint Gaussian distribution.
 Using GP, model distribution functions f (x) ∼ GP(µ, kγ), set function values forms joint Gaussian distribution charac- terized mean function µ(·) covariance mapping func- tion kγ(·,·) γ is set hyper-parameters be trained.
 common kernel function is RBF kernel deﬁned as: kRBF(x, z) = exp(−0.5||x − z||2/l2) l is hyper-parameter.
 GP, model function values evaluated training data that: [f (x1),


 f (xn)](cid:62) ∼ N (µµµ, KXX µµµi = µ(xi) i, j entry covariance (KXX )i,j = kγ(xi, xj).
 use kγ(X, X) denote calculation KXX simplicity paper.
 Based Gaussian Identity, arrive posterior pre- dictive distribution inference Williams, f(cid:63)|X, X(cid:63), y ∼ N (KX(cid:63)X (KXX + σ2I)−1y, KX(cid:63)X(cid:63) − KX(cid:63)X (KXX + σ2I)−1KXX(cid:63) ).
 matrix KX(cid:63)X = kγ(X(cid:63), X) is covariance measured X(cid:63) X.
 prediction mean variance cost O(n) time O(n2) storage test point.
 Inducing Point Methods Training GPs involves marginalizing log likelihood data is conditioned hyper-parameters γ, costs O(n3) time O(n2) storage
 purpose paper, do focus training procedure GPs refer readers previous literature [Ras- mussen Nickisch, details training.
 common approach approximate GP is inducing point methods.
 Examples inducing points methods are Subset Regressors (SoR) [Silverman, Deterministic Training Conditional (DTC) Fully Independent Train- ing Conditional (FITC) Approximation [Snelson Ghahra- mani,
 methods be viewed use ˜k(x, z) approximate true kernel function z): ˜kSoR(x, z) = KxU K−1 ˜kFITC(x, z) = ˜kSoR(x, z) + δx,z U U KU z (cid:16) k(x, z) − ˜kSoR (cid:17) set m inducing points U = [ui], i =


 m.
 SoR, approximated kernel matrix KXU K−1 U U KU X has rank most m.
 FITC achieves full rank approximation diagonal correction, improves performance signif- practice.
 low rank approximations reduce mean variance prediction time O(n) O(n2) O(m) O(m2).
 storage cost mean variance prediction is reduced O(n) O(n2) O(n) O(nm).
 meth- ods, however, suffer severe deterioration predictive performance n is large requires m (cid:28) n achieve efﬁciency gain [Wilson
 Structure Exploiting Methods family approaches scalable GP learning inference is exploit existing algebraic structure KXX.
 However, structures are exploitable certain kinds kernel function.
 example, Toeplitz method [Cun- ningham et al., requires stationary kernel Kro- necker methods [Saatc¸i, Wilson et requires product stationary kernel.
 exact inference approaches requires x be rectilinear grid constraints application limited kinds data.
 Structured Kernel Interpolation (SKI) [Wilson Nick- isch, uniﬁes inducing points methods structure ex- ploiting approaches.
 SKI starts extending SoR method approximating KXU ≈ W KU U
 SKI further scales GPs placing U grid structure structure exploiting is possible.
 overall approximation KXX is: KXX ≈ KXU K−1 = W KU U W (cid:62) = ˜KSKI U U KU X ≈ W KU U K−1 U U KU U W (cid:62) values W are interpolation weights X U W be sparse row has non- values.
 special structure SKI approximation en- ables.
 inference cost GPs SKI reduce O(n) time storage.
 is worth noticing SKI framework, m be larger n due structure exploitation.
 However, enforcing inducing points be grid structured limits applica- tion SKI low-dimensional data (d number grid point increases dimension increases.
 Knowledge Distillation Deep Learning Knowledge distillation [Hinton et al., is used trans- ferring generalization ability ensemble large neural networks light-weight model is suit- able deployment.
 core idea knowledge distil- lation is train neural network model large ca- pacity, ensemble neural networks, teacher model.
 teacher model has been use out- put produced teacher model “soft targets” train small model student model.
 training student model scratch, compress large neural network smaller [Chen et Han et
 works ply techniques such hashing, pruning quantization weight parameters neural networks.
 result, compressed neural network has fewer parameters much drop predictive performance.
 see next section, kernel distillation framework inherits ideas knowledge distillation model compression.
 compress exact ker- nel matrix scalable structure compression process involving optimization teacher model, be viewed form knowledge distillation.
 Kernel Distillation section, detail method kernel distillation.
 assume have access trained exact GP full kernel matrix KXX teacher model training data {X, y}.
 assume perform kernel distilla- tion machine enough computational power store KXX.
 kernel apply distilled stu- dent model resource-limited device inference other applications.
 Sparse Low-rank Kernel Approximation Algorithm outlines distillation approach.
 Formulation propose use student kernel matrix sparse low-rank structure, ˜KXX = W KU U W (cid:62) ap- proximate trained kernel matrix KXX.
 W is sparse matrix KU U is covariance evaluated set induc- ing points U.
 Similar KISS-GP [Wilson Nickisch, ap- proximate KXU ˜KXU = W KU U
 KISS-GP, W is calculated using cubic interpolation grid-structured induc- ing points.
 number inducing points grows exponen- dimension input data grows, limiting KISS-GP applicable low-dimensional data.
 enforcing inducing points U be grid, causes curse dimensionality, choose m centroids in- ducing points using K-means clustering X.
 addi- tion, store U KD-tree T nearest neighbor search be used optimization.
 Sparse Low-rank Kernel Approximation Input: A trained kernel function kγ, training feature vectors X targets y, step size η, number iterations T sparsity b.
 J ← indices b nearest neighbors xi U Wi(J) ← argminβ||βKU U (J) − (KXU )i||2 Output: Approximated kernel matrix W KU,U W (cid:62) U ← K-MEANS(X) KXX ← kγ(X, X) KU U ← kγ(U, U KXU ← kγ(X, U Initialization W ← ∈ Rn×m xi X do end Step Gradient Descent t T do end E ← W KU U W (cid:62) − KXX Ei,i ← ≤ i ≤ n ∇W ← E(cid:62)W KU U Project row ∇W b-sparse space Update W ← W − η∇W kernel distillation, ﬁnd optimal W con- strained optimization problem.
 constrain row W have most b non-zero entries.
 set objective func- tion be F -norm error teacher kernel stu- dent kernel: ||KXX − W KU U W (cid:62)||F min subject ||Wi||0 ≤ b ∀i ||Wi||0 denotes number non-zero entries row i W
 Initialization initial values W are crucial later optimization.
 initialize W optimal solution ||KXU − W KU U||F sparsity constraint.
 speciﬁcally, xi X, ﬁnd b nearest points U querying T
 denote indices b neighbors J.
 initialize row Wi W solving following linear least square problem: minWi(J) ||Wi(J)KU U (J) − (KXU )i||2 Wi(J) denotes entries row Wi indexed J KU U (J) denotes rows KU U indexed J.
 entries Wi index J are set zero.
 Optimization W is initialized, solve F -norm minimization problem using standard gradient descent.
 De- tails gradient calculation is given Algorithm
 satisfy sparsity constraint, iteration, project row gradient ∇W b-sparse space according indices J, update W accordingly.
 Fast Prediction One direct application kernel distillation is fast predic- tion approximated kernel matrix.
 Given test point x(cid:63), Methods FITC [Qui˜nonero-Candela Rasmussen, KISS-GP [Wilson et Kernel distillation Storage Mean Prediction Variance Prediction O(nm) O(m2) O(1) O(n + O(m2) O(b log m + b3) O(m) O(1) O(b log m + b3) Table Time storage complexity prediction FITC, KISS Distillation.
 m is number reducing points, d is dimension data b is non elements choose Algorithm
 (a) |KXX − Kdistill| (b) |KXX − KKISS| (c) |KXX − KSoR| (d) Error v.s. b Figure Kernel Reconstruction Experiments.
 (c) Absolute error matrix reconstructing KXX kernel KISS-GP SoR respectively.
 (d) F -norm error reconstructing KXX distillation different setting b (sparsity constraint) W
 follow similar approximation scheme distillation test time try approximate Kx(cid:63)X: Kx(cid:63)X ≈ ˜Kx(cid:63)X = W(cid:63) ˜KU X = W(cid:63)KU U W (cid:62) W(cid:63) is forced be sparse efﬁciency.
 mean variance prediction be approximated by: E[f(cid:63)] ≈ ˜Kx(cid:63)X ˜KXX + σ2I)−1y ≈ W(cid:63)KU U W (cid:62)( ˜KXX + σ2I)−1y = W(cid:63) ˜ααα V ar[f(cid:63)] ≈ Kx(cid:63)x(cid:63) − ˜Kx(cid:63)X [ ˜KXX + σ2I]−1 ˜KXx(cid:63) ≈ Kx(cid:63)x(cid:63) − W(cid:63)KU U W (cid:62)[ ˜KXX + σ2I]−1W KU U W (cid:62) = Kx(cid:63)x(cid:63) − W(cid:63)V W (cid:62) (cid:63) (cid:63) ˜ααα =KU U W (cid:62)( ˜KXX + σ2I)−1y V =KU U W (cid:62)[ ˜KXX + σ2I]−1W KU U be precomputed distillation.
 key question is estimate W(cid:63) efﬁciently.
 use same procedure initialization W compute W(cid:63).
 start ﬁnds b nearest neighbors x(cid:63) U mark indices J(cid:63) set elements W(cid:63) indices are J(cid:63)
 entries indices J(cid:63), solve following least square problem get optimal values W(cid:63)(J(cid:63)): ||W(cid:63)(J(cid:63))KU U (J(cid:63)) − Kx(cid:63)U (J(cid:63))||2.
 min see takes O(b log m) query nearest neighbors, O(b3) get W(cid:63) O(b) O(b2) mean variance prediction respectively.
 Therefore, total, predic- tion time complexity is O(b log m + b3).
 storage com- plexity, need store precomputed vector mean pre- diction diagonal matrix variance prediction cost O(m2).
 Table provides comparison time storage complex- ity different GP approximation approaches.
 stor- age complexity KISS-GP grows di- mension input data.
 practice, avoid exponential growth, KISS-GP learns mapping P project input data low dimension training [Wilson
 assumption input data are embedded low- dimensional space hurt predictive perfor- mance demonstrate experiment section.
 Kernel distillation, other hand, makes assumption dimensionality input data achieve rea- accurate prediction reduce storage cost.
 Other Applications Apart fast prediction GPs, kernel distillation be used other applications, such online update GPs [McIntire et
 distilled model is de- ployed mobile devices robotics, want adjust model seeing new data points.
 Online update beneﬁt sparse low-rank structure distilled kernel.
 leave future work explore integrate kernel distillation online update GPs. Experiments evaluate kernel distillation ability approximate exact kernel, predictive power speed infer- ence time.
 particular, compare approach FITC KISS-GP are popular approaches are related kernel distillation.
 Kernel Reconstruction ﬁrst study kernel distillation reconstruct full teacher kernel matrix.
 generate × kernel matrix KXX RBF kernel evaluated (sorted) inputs X Dataset Boston Housing Abalone PUMADYM32N KIN40K Dim train test Exact FITC KISS-GP Distill Table SMSE Results Comparison.
 Dim is dimension input data.
 Number inducing points grid) KISS-GP are number inducing points FITC kernel distillation are datasets respectively.
 sparsity b is set Boston Housing other datasets.
 predictions kernel distillation are indistinguishable exact GP KISS-GP.
 variance, kernel distillation’s predictions are variance outputs exact GP, variance outputs predicted KISS-GP are exact solution.
 experiment shows potential problem KISS-GP, sacriﬁces ability provide uncertainty measure- ments, is crucial property Bayesian modeling, exchanging massive scalability.
 other hand, kernel distillation honestly provide uncertainty prediction exact GP model.
 Empirical Study further evaluate performance kernel distillation several benchmark regression data sets.
 summary datasets is given Table
 Setup compare kernel distillation teacher kernel (exact GP), FITC KISS-GP.
 use same in- ducing points selected K-Means FITC ker- nel distillation.
 KISS-GP, datasets do lie lower dimension, project input con- struct grid data inducing points.
 Number in- ducing points grid) KISS-GP are set (70 grid dimension) Boston Housing, Abalone, PUMADYM32N, KIN40K.
 number inducing points FITC kernel distillation are Boston Housing, Abalone, PUMADYM32N KINK40K.
 sparsity b kernel distillation is set Boston Housing other datasets.
 meth- ods, choose ARD kernel kernel function, is deﬁned as: kARD(x, z) = exp(−0.5 (xi − zi)2/σ2 i d(cid:88) i=1 d is dimension input data σi’s are hyper-parameters learn.
 experiments were conducted PC laptop Intel Core(TM) i7-6700HQ CPU @ GB RAM.
 Predictive performance comparison start evaluating kernel distillation preserve predictive per- formance teacher kernel.
 metrics use eval- uation is standardized mean square error (SMSE) deﬁned SMSE(y, ˆy) = (yi − ˆyi)2/V ar(y) n(cid:88) i=1 (a) Mean (b) Variance Figure Mean (a) variance (b) prediction comparison KISS-GP Kernel Distillation example.
 sampled N
 compare kernel dis- tillation KISS-GP SoR (FITC is SoR diagonal correction mentioned Section
 set number grid points KISS-GP number in- ducing points SoR is set kernel distillation
 set sparsity b kernel distillation.
 F -norm errors are × × KISS-GP, SoR kernel distillation respec- tively.
 Kernel distillation achieves lowest F -norm error com- pared FITC KISS-GP number inducing points is much fewer kernel distillation.
 Moreover, absolute error matrices (Figure a-c), see errors are more distributed kernel distillation, seems exist strong error pattern other two.
 show sparsity parameter b affect ap- proximation quality.
 evaluate error different choices b shown Figure (d).
 observe er- ror converges sparsity b is example.
 shows structured student kernel approximate full teacher kernel W is sparse.
 Toy Example evaluate distilled model’s predictive ability, set following experiment.
 sample n = data points X [-10,
 set response y(x) = sin(x) exp(− x2 +   ∼ N (0,
 train exact GP RBF kernel teacher ﬁrst apply ker- nel distillation number inducing points set sparsity set
 compare mean variance predic- tion kernel distillation KISS-GP trained grid inducing points.
 results are showed Figure
 see, mean (a) Boston Mean (b) Boston Variance (c) Abalone Mean (d) Abalone Variance Figure Test error variance comparison Boston Housing (a-b) Abalone (c-d) different choices sparsity constraint b W
 variance prediction comparison, calculate root square mean error variance exact GPs approximate GPs (KISS-GP kernel distillation).
 true labels y model predictions ˆy.
 Table summarizes results.
 see exact GPs achieve lowest errors datasets.
 FITC gets second lowest error datasets Boston Hous- ing.
 Errors kernel distillation are FITC KISS-GP has largest errors dataset.
 poor performance KISS-GP be resulted loss information projection input data low di- mension.
 Effects sparsity further study effects sparsity b predictive performance.
 choose b be range [5,


 compare test error variance predic- tion KISS-GP kernel distillation Boston Housing Abalone datasets.
 results are shown Figure
 error kernel distillation decreases sparsity increases need b be outperform KISS-GP.
 variance prediction, plot error outputs exact GPs approximate GPs. see kernel dis- tillation provides reliable variance output KISS-GP level sparsity.
 Speed comparison evaluate speed prediction kernel distillation.
 compare speed FITC KISS-GP.
 setup approximate models is same predictive performance comparison exper- iment.
 dataset, run test prediction points report average prediction time seconds.
 Table summarizes results speed.
 shows KISS-GP kernel distillation are much predic- tion time compared FITC datasets.
 Figure shows detailed comparison prediction time KISS-GP kernel distillation.
 kernel distillation is slower KISS-GP, considering improvement accu- racy reliable uncertainty measurements, cost prediction time is acceptable.
 Also, KISS-GP claims have constant prediction time complexity theory [Wilson actual implementation is data-dependent speed varies different datasets.
 general, ker- nel distillation provides better trade-off predictive power scalability alternatives.
 FITC KISS-GP Dataset Boston Housing Abalone PUMADYM32N KIN40K Distill Table Average prediction time seconds test data point dataset.
 Setup models is same Table
 Figure Prediction time comparison kernel distillation KISS-GP.
 vertical black line shows standard deviation rounds experiments.
 Conclusion have proposed general framework, kernel distillation, compressing trained exact GPs kernel student ker- nel low-rank sparse structure.
 framework does assume special structure input data kernel func- tion, be applied ”out-of-box” datasets.
 Kernel distillation framework formulates approximation constrained F -norm minimization exact kernel approximate student kernel.
 distilled kernel matrix reduces storage cost O(m2) compared O(mn) other inducing point meth- ods.
 show application kernel distilla- tion is fast accurate GP prediction.
 Kernel distillation produce more accurate results KISS-GP pre- diction time is FITC.
 Overall, method provide better balance speed predictive perfor- mance other approximate GP approaches.
 [Rasmussen Williams, Carl Edward Rasmussen Christopher KI Williams.
 Gaussian processes ma- chine learning.

 Carl Edward Rasmussen.
 Gaussian pro- cesses machine learning.
 Advanced lectures ma- chine learning, pages

 Yunus Saatc¸i.
 Scalable inference struc- tured Gaussian process models.
 PhD thesis, University Cambridge,
 [Seeger al., Matthias Seeger, Christopher Williams, Neil Lawrence.
 Fast forward selection speed Artiﬁcial Intel- sparse gaussian process regression.
 ligence Statistics number EPFL-CONF-161318,
 Bernhard W Silverman.
 aspects spline smoothing approach non-parametric regres- sion curve ﬁtting.
 Journal Royal Statistical Society.
 Series B (Methodological), pages
 [Snelson Ghahramani, Edward Zoubin Ghahramani.
 Sparse gaussian processes using Advances neural information pseudo-inputs.
 processing systems, pages
 Snelson [Titsias, Michalis K Titsias.
 Variational learning in- ducing variables sparse gaussian processes.
 AISTATS, volume pages
 [Wilson Nickisch, Andrew Wilson Hannes Nickisch.
 Kernel interpolation scalable structured gaussian processes (kiss-gp).
 Proceedings International Conference Machine Learning, pages
 [Wilson al., Andrew Wilson, Elad Gilboa, John P Cunningham, Arye Nehorai.
 Fast kernel learning Advances multidimensional pattern extrapolation.
 Neural Information Processing Systems, pages
 [Wilson al., Andrew Gordon Wilson, Christoph Dann, Hannes Nickisch.
 Thoughts scal- able gaussian processes.
 arXiv preprint arXiv:1511.01870,
 References [Buciluˇa et al., Cristian Buciluˇa, Rich Caruana, Alexandru Niculescu-Mizil.
 Model compression.
 Pro- ceedings ACM SIGKDD international con- ference Knowledge discovery data mining, pages

 [Chen et Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, Yixin Chen.
 Compress- Inter- ing neural networks hashing trick.
 national Conference Machine Learning, pages
 [Cunningham al., John P Cunningham, Krishna V Shenoy, Maneesh Sahani.
 Fast gaussian process meth- ods point process intensity estimation.
 Proceedings 25th international conference Machine learning, pages

 et al., Marc Peter Deisenroth, Dieter Fox, Carl Edward Rasmussen.
 Gaussian processes data- IEEE Trans- efﬁcient learning robotics control.
 actions Pattern Analysis Machine Intelligence,
 [Han al., Song Han, Huizi Mao, William J Dally.
 Deep compression: Compressing deep neural net- works pruning, trained quantization huffman cod- ing.
 arXiv preprint arXiv:1510.00149,
 [Hinton et Geoffrey Hinton, Oriol Vinyals, Jeff Dean.
 Distilling knowledge neural network.
 arXiv preprint arXiv:1503.02531,
 et al., Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam.
 Mo- bilenets: Efﬁcient convolutional neural networks mo- bile vision applications.
 arXiv preprint arXiv:1704.04861,
 [Lawrence al., Neil Lawrence, Matthias Seeger, Ralf Herbrich.
 Fast sparse gaussian process methods: Proceedings informative vector machine.
 Annual Conference Neural Information Process- ing Systems, number EPFL-CONF-161319, pages
 [McIntire al., Mitchell McIntire, Daniel Ratner, Stefano Ermon.
 Sparse gaussian processes bayesian Proceedings Thirty-Second Con- optimization.
 ference Uncertainty Artiﬁcial Intelligence, UAI’16, pages Arlington, Virginia, United States,
 AUAI Press.
 [Qui˜nonero-Candela Rasmussen, Joaquin Qui˜nonero-Candela Carl Edward Rasmussen.
 unifying view sparse approximate gaussian process Journal Machine Learning Research, regression.

 [Rasmussen Nickisch, Carl Edward Rasmussen Hannes Nickisch.
 gpml toolbox version
 Compressive Sensing (CS) is method signal processing aims reconstruct signals small number measurements.
 exploiting fact unknown signal is sparse, reconstruction is achieved sampling rates less Nyquist rate [1], [2].
 paper, focus Binary Compressive Sensing (BCS) conﬁnes signals interest binary {0, 1}-valued binary signals.
 is motivated wide use binary signals engineering, example, fault detection black-and-white image reconstruction [4], digital com- munications [5], name few.
 Related works are follows.
 Nakarmi al.
 [6] designed novel sensing matrix tailored binary signal reconstruction.
 Wang al.
 [7] combined ℓ1 norm ℓ∞ norm reconstruct sparse binary signals.
 Nagahara [8] exploited sum weighted ℓ1 norms reconstruct signals entries are integer-valued and, particular, binary signals bitonal images.
 Keiper et al.
 analyzed phase transition binary Basis Pursuit.
 best knowledge, most BCS algorithms developed are based convex optimization methods.
 family methods enjoys appealing theoretical properties such reconstruction guarantees (e.g. [9], were found be slow large-scale applications [11].
 purpose work is ﬁll gap propos- ing fast BCS method attains accurate reconstruction results.
 proposed method leverages gradient descent approach smoothed ℓ0 norm [12] introducing ad- ditional probability distribution binary signals penalty function.
 compare proposed method state-of-the-art convex-optimization-based BCS algorithms demonstrate advantage reconstruction speed, more impressively, reconstruction accuracy.
 paper is organized follows.
 present short review CS BCS algorithms Section II.
 present proposed algorithm Section III.
 is main contribution paper.
 Emperical experiments results are presented Section IV demonstrate competitiveness proposed algorithm.
 given conclusion Section V.
 Notations: vector v = [v1, · · · vN ]⊤, use kvkp denote ℓp norm v, ≤ p ≤ ∞.
 use kvk0 denote number non-zero entries vector v.
 P(E) denotes probability event E.
 integer n, use [n] short-hand notation set {1, · · · n}.
 write N -dimensional column vector entries equal
 II.
 BINARY COMPRESSIVE SENSING (BCS) standard CS scheme, aims recover sparse signal linear measurements.
 constraints posed measurements be formulated Φz = y, (1) Φ ∈ Rm×N m ≪ N is measurement matrix, z ∈ RN = Φx is measurement vector sparse signal x ∈ RN
 CS algorithms exploit fact x is sparse seek sparse solution z Equation
 BCS scheme, signals interest are conﬁned binary signals.
 Notice binary signal x is sparse measurement matrix Φ is known observer, (1) recover dense binary signals sparse ones.
 complementary signalex := − x is supported.
 be converted Φez = ey, ez − z ey := Φex = Φ1N − y, vice versa.
 observation allows types models binary signals have been consid- ered literature (e.g., [13], [7], [8]).
 (i) Sparseness prior: x is deterministic vector is binary sparse, i.e., most entries are few are (ii) Probability distribution prior: x is random vector entries are independent distributed (i.i.d.) probability distribution P(xj = = p ﬁxed ≤ p ≤
 p is small, realization x is likely sparse binary signal.
 work, consider second model covers dense binary signals large support sparse binary signals.
 give short review CS/BCS methods are related work.
 A.
 ℓ0 minimization (L0) A naive approach ﬁnding sparse solutions is ℓ0 minimization, min z∈RN kzk0 subject Φz = y.
 (P0) method works continuously-valued signals i.e., signals x ∈ RN entries are are zero.
 However, solving ℓ0 minimization requires combinatorial search is NP-hard [14].
 B.
 Smoothed ℓ0 minimization (SL0) z2 z2 Smoothed ℓ0 minimization (SL0) [12] replaces ℓ0 norm (P0) non-convex relaxation: Φz = y Φz = y min z∈RN is motivated observation implies z = (z1,


 )⊤ ∈ RN lim σ→0 NXi=1(cid:18)1 − exp(cid:18) subject Φz = y.
 =(1 exp(cid:20) −t2 NXi=1(cid:18)1 − exp(cid:20) −z2 i=1(cid:16)1 − exph = kzk0.
 is smooth func- t = t lim σ→0 (2) Noticing z tion ﬁxed σ > Mohimani al.
 [12] proposed algorithm based gradient descent method.
 algorithm obtains approximate solution decreasing σ.
 Mohammadi al.
 adapted SL0 algorithm par- non-negative signals.
 algorithm, called Constrained Smoothed ℓ0 method (CSL0), incorporates non-negativity constraints introducing appropriate weight functions cost function.
 Empirically, showed CSL0 achieves better performance SL0 recon- struction non-negative signals.
 C.
 Basis Pursuit (BP) well-known standard relaxation (P0) is ℓ1-minimization, known Basis Pursuit (BP) [16]: min z∈RN kzk1 subject Φz = y.
 continuously-valued signals x ∈ RN are sparse.
 method works (P0), D.
 Boxed Basis Pursuit (Boxed BP) Donoho al.
 [13] proposed Boxed Basis Pursuit (Boxed BP) reconstruction k-simple bounded signals: min z∈[0,1]N kzk1 subject Φz = y.
 intuition Boxed BP is straightforward: ℓ1 norm minimization promotes sparsity solution restriction z ∈ [0, reduces set feasible solutions.
 Recently, Keiper al.
 analyzed performance Boxed BP reconstruction binary signals.
 E.
 Sum Norms (SN) Wang al.
 [7] introduced following optimization prob- lem combines ℓ1 ℓ∞ min z∈RN kzk1 + λkz − · k∞ subject Φz = y.
 before, minimizing term kzk1 promotes sparsity z.
 other hand, minimizing term kz · k∞ forces entries |zi − | be small equal magnitude.
 facts are illustrated Fig.

 norms are balanced tuning parameter λ >
 z1 z1 (1, Fig.

 Left: minimization kzk1 ﬁnds sparse solutions, Right: minimization kz − | be small equal magnitude.
 · k∞ forces entries |zi (0, p) (1, − p) zi Fig.

 contribution zi function (1 − p)kzk1 + pkz − k1.
 F.
 Sum Absolute Values (SAV) Nagahara [8] proposed following method reconstruc- tion discrete signals entries are chosen set ﬁnite alphabets α = {α1, α2,


 αL} priori known probability distribution.
 special case α = {0, binary signals, SAV is formulated as, min z∈RN (1 − p)kzk1 + pkz − k1 subject Φz = p = P(xj = j ∈ [N ], is probability distribution entries x.
 Here, contribution entry zi function (1 − p)kzk1 + pkz − k1 is given (Fig.
 zi < −zi + p (1 − + p ≤ zi < zi − p zi ≥
  p ≈ i.e., x is sparse, (1−p)kzk1+pkz−1N k1 ≈ kzk1 corresponds BP.
 III.
 BOX-CONSTRAINED SUM OF SMOOTHED ℓ0 Note L0 SL0 utilize ℓ0 norm smoothed version respectively, however, do exploit fact x is binary.
 other hand, BP, Boxed BP, SN, SAV utilize ℓ1 norm way methods BP are adjusted binary setting.
 natural question arises adjust L0 SL0 binary setting achieve better recovery performance binary signals.
 note Boxed BP takes account binary prior x imposing restriction x ∈ [0,
 is straightforward apply same trick L0 SL0, are called Boxed L0 Boxed SL0 respectively.
 Note Boxed L0 is NP-hard is applicable.
 Fig.
 shows Boxed SL0 achieves improvement recovery performance SL0 requiring similar run time.
 However, error rate appears be worse Boxed BP SN.
 paper, aim adapt formulation SAV restriction x ∈ [0, SL0, order achieve better performance existing algorithms.
 straightforward adaptation yields following problem.
 σ > small, min z∈[0,1]N Fσ(z) subject Φz = y, (3) Fσ(z) (1 − p) NXi=1(cid:16)1 − e−z2 i /(2σ2)(cid:17) NXi=1(cid:16)1 − e−(zi−1)2/(2σ2)(cid:17) + p NXi=1(cid:16)1 − (1 − e−z2 i /(2σ2) − p e−(zi−1)2/(2σ2)(cid:17) p = P(xj = ∀j ∈ [N ].
 Note (2), have lim σ→0 Fσ(z) = (1 − p)kzk0 + pkz − k0 F0(z) be approximated Fσ(z) small σ >
 Next, use weight function incorporate restriction z ∈ [0, function Fσ(z).
 integers k let wk(t) ,(1 ≤ t ≤ k otherwise.
 (5) σ > integers k ≥ deﬁne F boxed σ,k (z) NXi=1 wk(zi)(cid:16)1 − (1 − e−z2 i /(2σ2) − p e−(zi−1)2/(2σ2)(cid:17)
 Note − (1 − p)e−t2/(2σ2) − p e−(t−1)2/(2σ2) > t ∈ R, minimizing F boxed σ,k (z) forces wk(zi) be small zi’s lie
 way, restriction z ∈ [0, is merged penalty function.
 optimization problem reads follows.
 σ > small k ∈ N large, z∈RN F boxed σ,k (z) subject Φz = y.
 algorithm proposed solve problem is based gradient descent method implementation is similar algorithms [12], [15].
 major differ- ence algorithm theirs is cost function σ,k (z) is designed binary valued signal, adapting formulation SAV [8].
 SL0 is replaced F boxed i=1(cid:16)1 − exp(cid:16) −z2 PN proposed algorithm is comprised nested loops.
 outer loop, slowly decrease σ search optimal solution coarse ﬁne scale decreasing σ factor < d <
 σ decreases, increase k larger penalty is put solutions Algorithm Box-Constrained Sum Smoothed ℓ0 (BSSL0) Data: Measurement matrix Φ ∈ Rm×N observation y ∈ Rm, probability distribution p = P(xj = Parameters: minimal σ: σmin, inner-loop iteration gradient descent factor: µ, σ L, decreasing factor: d; Initialization: ˆx = Φ⊤(ΦΦ⊤)−1 Iters do Iters = logd(cid:0) σmin σ (cid:1), k = + N p L, do Iters σ = max |ˆx|, ˆx ← ˆx − σ2µ∇F boxed σ,k (z) % Gradient descent ˆx ← ˆx − Φ⊤(ΦΦ⊤)−1(Φˆx − % Projection end σ = σ × d k = k + N p Iters (4) end ˆx ← round(ˆx) % Round binary vector.
 have entries range [0,
 inner loop performs gradient descent L iterations function F boxed σ,k (z), σ k are given outer loop.
 iteration, obtained solution is projected set feasible solutions {z Φz = y}.
 numerical experiments Section IV indicate binary signals proposed algorithm outperforms SL0, achieving better recovery performance other CS/BCS algorithms (BP, Boxed BP, SN, SAV).
 mentioned, algorithm is implemented ogous SL0 [12], [15] penalty function.
 parameters used algorithm are same [12] k p.
 justiﬁed [12, Section IV-B], use minimum ℓ2 norm solution Φz = y initial estimate, is, initialize ˆx = Φ⊤(ΦΦ⊤)−1 y.
 initialization value σ is discussed [12, Remark Section III].
 choice step-size σ2µ gradient descent is justiﬁed [12, Remark Section III], choice k is explained [15, Lemma
 gradient F boxed σ,k (z), appearing Algorithm is obtained σ,k (z) = ∂F boxed ∇F boxed


 boxed σ,k (z) σ,k (z) ∂z1 ∂zN !⊤ σ2 (cid:18)(1 − p)zi exp(cid:20) −z2 (cid:21)(cid:19) a.e., +p (zi − exp(cid:20) −(zi − wk(zi) ∂F boxed σ,k (z) ∂zi used fact w′ k(t) = t t =
 IV.
 NUMERICAL EXPERIMENTS section, investigate proposed algorithm BSSL0.
 performance BSSL0 is compared state-of-the-art CS BCS algorithms described Section II.
 experiment have been carried reconstruction accuracy.
 same time, BSSL0 is several orders faster compared baseline methods.
 B.
 Experiment Bitonal Image Reconstruction repeat bitonal image reconstruction experiment designed [8].
 Consider × 37-pixel bitonal image shown Fig.
 (left).
 Random Gaussian noise mean standard deviation is added pixel, resulted blurred image is shown Fig.
 (right).
 Denote disturbed image X.
 Fig.

 Original image (left) noise-added image (right).
 apply discrete Fourier transform (DFT) real valued matrix X.
 be written linear equation: (W ⊗ W )vec(X) = vec( ˆX), W is DFT matrix.
 Let ∈ C685 be half-size randomly downsampled vector vec( ˆX).
 Let sensing matrix Φ ∈ C685×1369 be matrix generated corresponding down- sampling row vectors W ⊗W
 present Φ optimization methods reconstruct image.
 SN, tuning parameter λ is tested stepsize λ = is chosen performed best.
 SAV BSSL0, have chosen p = P(xj = rough estimate sparsity bitonal image (this estimate p = was used [8]).
 set σmin d = µ = L = parameters BSSL0.
 MATLAB R2015a environment Intel Core i7 Macintosh notebook computer GB RAM.
 codes reproducing experiments are available https://github.com/liutianlin0121/BSSL0.
 A.
 Experiment Binary Sparse Signal Reconstruction experiment, reconstruct binary signal using BSSL0 other baseline methods.
 Let measurement ma- trix Φ ∈ R40×100 be random Gaussian, is, component Φ is drawn standard normal distribution.
 p, varied step-size binary signal x ∈ R100 is generated entries drawn P(xi = = p P(xi = = − p.
 Φ x generated, compute measurement vector y = Φx apply respective algorithms (BP, Boxed BP, SN, SAV, SL0, Boxed SL0, BSSL0) get reconstructed signal z, is estimate x.
 Note Boxed SL0 is special case CSL0 constrains signals boxed range [0,1].
 following measures are considered evaluation reconstruction result: (i) Failure Perfect Reconstruction z x (failed reconstruct) (succeed reconstruct); (ii) Noise Signal Ratio (NSR): NSR = kx−zk2 (as deﬁned [8]).
 (iii) Run time kxk2 measured tic/toc command MATLAB coarse estimation complexities algorithms.
 ﬁxed p, process point is repeated times evaluation results measurements are averaged.
 SN, set parameter λ be ﬁne-tuned [7].
 BSSL0, let σmin = d = µ = L =
 − p BP Boxed BP SN SAV SL0 Boxed SL0 BSSL0 Fig.

 Upper ﬁgure: Failure Perfect Recovery; middle ﬁgure: NSR; lower ﬁgure: Run time From Fig.
 see that, compared baseline meth- ods, BSSL0 yield better reconstruction results metrics Fig.

 Reconstructed images BP (upper left), SN (upper right), SAV (lower left), proposed BSSL0 (lower right).
 TABLE THE RUN TIME COMPARISON Optimization Method Basis Pursuit SN SAV BSSL0 (proposed) Run Time seconds seconds seconds seconds [14] B.
 K.
 Natarajan, “Sparse approximate solutions linear systems,” SIAM Journal Computing, vol.
 no.
 pp.

 [15] M.
 Mohammadi, E.
 Fatemizadeh, M.
 H.
 Mahoor, “Non-negative sparse decomposition based constrained smoothed ℓ0 norm,” Signal Processing, vol.
 pp.

 [16] S.
 S.
 Chen, D.
 L.
 Donoho, M.
 A.
 Saunders, “Atomic decomposition basis pursuit,” SIAM Review, vol.
 no.
 pp.

 reconstructed ﬁgures BP, SN, SAV, BSSL0 is shown Fig.

 time consumption is shown Tab.
 I.
 ﬁgures table show proposed BSSL0 enhances reconstruction accuracy speed.
 V.
 CONCLUSION paper, proposed novel algorithm BSSL0 reconstructing binary signals reduced measurements.
 algorithm is derived based smooth relaxation techniques gradient descent methods.
 experiments, show proposed algorithm outperforms state-of-the-art convex optimization methods reconstruction accuracy speed.
 ACKNOWLEDGMENT T.
 Liu D.
 G.
 Lee acknowledge support DFG Grant PF 450/6-1.
 authors are grateful Robert Fischer G¨otz E.
 Pfander helpful suggestions.
 REFERENCES [1] D.
 L.
 Donoho, “Compressed sensing,” IEEE Transactions Informa- tion Theory, vol.
 no.
 pp.

 [2] S.
 Foucart H.
 Rauhut, Mathematical Introduction Compressive Sensing.
 Basel: Birkh¨auser,
 [3] D.
 Bickson, D.
 Baron, A.
 Ihler, H.
 Avissar, D.
 Dolev, “Fault identiﬁcation nonparametric belief propagation,” IEEE Transactions Signal Processing, vol.
 no.
 pp.

 [4] M.
 F.
 Duarte, M.
 A.
 Davenport, D.
 Takbar, J.
 N.
 Laska, T.
 Sun, K.
 F.
 Kelly, R.
 G.
 Baraniuk, “Single-pixel imaging compressive sampling,” IEEE signal Processing Magazine, vol.
 no.
 pp.

 [5] K.
 Wu X.
 Guo, “Compressive sensing digital sparse signals,” Wireless Communications Networking Conference (WCNC), IEEE.
 IEEE, pp.

 [6] U.
 Nakarmi N.
 Rahnavard, “Bcs: Compressive sensing binary IEEE, sparse signals,” Military Communications Conference
 pp.

 [7] S.
 Wang N.
 Rahnavard, “Binary compressive sensing sum ℓ1-norm ℓ∞-norm regularization,” Military Communications Conference IEEE.
 IEEE, pp.

 [8] M.
 Nagahara, “Discrete signal reconstruction sum absolute values,” IEEE Signal Processing Letters, vol.
 no.
 pp.

 [9] S.
 Keiper, G.
 Kutyniok, D.
 G.
 Lee, G.
 E.
 Pfander, “Compressed sensing ﬁnite-valued signals,” Linear Algebra Applications,
 J.-H.
 Lange, M.
 E.
 Pfetsch, B.
 M.
 Seib, A.
 M.
 Tillmann, “Sparse recovery integrality constraints,” arXiv preprint arXiv:1608.08678,
 [11] D.
 L.
 Donoho Y.
 Tsaig, “Fast solution ell1-norm minimization problems solution be sparse,” IEEE Transactions Information Theory, vol.
 no.
 pp.

 [12] H.
 Mohimani, M.
 Babaie-Zadeh, C.
 Jutten, “A approach overcomplete sparse decomposition based smoothed ℓ0 norm,” IEEE Transactions Signal Processing, vol.
 no.
 pp.

 [13] D.
 L.
 Donoho J.
 Tanner, “Precise undersampling theorems,” Pro- ceedings IEEE, vol.
 no.
 pp.

 response susceptibility neural networks adver- sarial examples (Szegedy et has been signif- icant interest constructing defenses increase robustness neural networks.
 progress has been made understanding defending adversarial ex- amples, complete solution has been found.
 best knowledge, defenses adversarial ex- amples published peer-reviewed venues date (Papernot Hendrik Metzen et Hendrycks Gim- pel, Meng Chen, Zantedeschi al., are vulnerable powerful optimization-based attacks (Carlini Wagner,
 benchmarking iterative optimization-based at- tacks such BIM (Kurakin et PGD (Madry et al., Carlini Wagner’s attack (Carlini Wagner, has become standard practice evaluating potential defenses, new defenses have arisen appear withstand powerful optimization-based attacks.
 *Equal contribution Institute Technol- ogy California, Berkeley.
 Correspondence to: Anish Athalye <aathalye@mit.edu>, Nicholas Carlini <npc@berkeley.edu>.
 identify common reason many defenses provide apparent robustness iterative attacks: obfuscated gra- dients.
 good gradient signal, optimization-based methods cannot succeed.
 identify types obfus- cated gradients.
 defenses cause shattered gradients, non-differentiable operations numerical instability, resulting nonexistent incorrect gradient signal.
 defenses are randomized, causing stochastic gradients depend test-time entropy unavailable attacker.
 Other defenses cause vanishing/exploding gradients (Bengio et resulting unusable gradient signal.
 propose new techniques overcome obfuscated gradi- ents caused phenomenon.
 address gra- dient shattering due non-differentiable operations new attack technique call Backward Pass Differentiable Approximation.
 compute gradients randomized de- fenses applying Expectation Transformation (Atha- lye et al.,
 solve vanishing/exploding gradients reparameterization optimize space vanishing/exploding gradients are issue.
 investigate prevalence obfuscated gradients understand applicability attack techniques, use ICLR defenses case study.
 ﬁnd obfuscated gradients are common occurrence, accepted defenses relying phenomenon.
 Applying new attack techniques develop, overcome obfus- cated gradients circumvent them.
 this, offer analysis evaluations performed papers.
 hope provide researchers common baseline knowledge, description attack techniques, common evaluation pitfalls, future defenses avoid falling vulnerable same attacks.
 promote reproducible research, release re- implementation defenses, imple- mentations attacks each.
 https://github.com/anishathalye/obfuscated-gradients Obfuscated Gradients Give False Sense Security: Circumventing Defenses Adversarial Examples
 Preliminaries Notation consider neural network f (·) used classiﬁcation f (x)i represents probability image x corresponds label i.
 classify represented x ∈ [0, 3-color image width w height h.
 use f j(·) refer layer j neural network, application layers j.
 denote classiﬁca- tion network c(x) = arg maxif (x)i true label image x is written c∗(x).
 Adversarial Examples Given image x classiﬁer f (·), adversarial example (Szegedy et x(cid:48) satisﬁes properties: D(x, x(cid:48)) is small distance metric D, c(x(cid:48)) (cid:54)= c∗(x).
 is, images, x x(cid:48) appear similar x(cid:48) is classiﬁed incorrectly.
 paper use (cid:96)∞ (cid:96)2 distortion metrics measure similarity.
 images have small dis- tortion metrics appear identical.
 report (cid:96)∞ distance normalized [0, space, distortion corresponds (cid:96)2 distance total root-mean-square distortion nor- malized total number pixels.
 Datasets Models evaluate defenses same datasets claim robustness: MNIST (LeCun, Saman- gouei et al.
 (2018), CIFAR-10 (Krizhevsky Hinton, Madry al.
 (2018); Song et al.
 (2018); Ma et al.
 (2018); Buckman et al.
 (2018); Dhillon et al.
 (2018), ImageNet (Krizhevsky et Guo al.
 (2018); Xie et al.
 (2018).
 defense argues security MNIST other dataset, circumvent defense larger dataset.
 MNIST CIFAR-10, evaluate defenses entire test set generate untargeted adversarial examples.
 ImageNet, evaluate selected images test set, construct targeted ad- versarial examples selected target classes.
 Generating targeted adversarial examples is harder problem believe is meaningful metric, dataset.
 use standard models dataset.
 MNIST use standard convolutional neural network reaches accuracy.
 CIFAR-10 train wide resnet (Zagoruyko Komodakis, et accuracy.
 ImageNet use InceptionV3 (Szegedy is meaningful metric dataset, misclassiﬁcation related classes, e.g. German shepherd classiﬁed Doberman, be meaningful.
 network reaches top-1 top-5 accuracy.
 Attack Methods construct adversarial examples iterative optimization-based methods.
 high level, given instance optimization attacks attempt search δ such c(x + δ) (cid:54)= c∗(x) minimizing (cid:107)δ(cid:107), maximizing loss f (x + δ).
 generate (cid:96)∞ bounded adversarial examples use Projected Gradient Descent (PGD); (cid:96)2, use Lagrangian relaxation: Carlini Wagner’s formulation (Carlini Wagner,
 speciﬁc choice optimizer (e.g., gradient descent Adam) regularization (e.g., (cid:96)∞-regularized (cid:96)2-regularized) is important using optimization-based methods (Madry et al.,

 Obfuscated Gradients high level, defense obfuscates gradients travel- ing direction suggested gradient is useful direction travel construct adversarial example.
 discover ways defenses cause obfuscated gradients.
 brieﬂy deﬁne discuss them.
 Shattered Gradients are caused defense is non- differentiable, introduces numeric instability, otherwise causes true gradient signal be incorrect.
 Defenses cause gradient shattering do unintentionally, introducing operations are differentiable gradient does point direction maximizes classiﬁcation loss.
 Stochastic Gradients are caused randomized defenses, network is randomized input is randomized being fed classiﬁer.
 Evaluat- ing gradient multiple times gives different results time.
 cause single-step methods opti- mization methods using single sample randomness estimate true gradient direction fail converge minima randomized classiﬁer.
 Exploding Vanishing Gradients are caused defenses consist multiple iterations neural network evaluation.
 be viewed deep neural network evaluation, is easy see gradients either vanish explode.
 Identifying Obfuscated Gradients cases be obvious defense contains non-differentiable operation.
 However, other cases, be clear.
 discuss several symptoms diagnostic tests help detect obfuscated gradients.
 do claim tests Obfuscated Gradients Give False Sense Security: Circumventing Defenses Adversarial Examples cover cases, ﬁnd defense are aware obfuscates gradients fails least tests.
 Check iterative attacks are better single step.
 Iterative optimization-based attacks applied white-box setting are stronger single-step attacks give superior performance.
 single-step methods give performance superior multi-step methods, is likely multi-step attack is becoming stuck optimization search local minimum.
 Verify white-box attacks work black-box.
 black-box threat model is strict subset white- box threat model, attacks white-box setting perform better; however, defense is obfuscating gradients, times black-box attacks do require gradient signals perform better white-box attacks.
 Ensure unbounded attacks reach success.
 unbounded distortion, classiﬁer have robust- ness adversarial examples classiﬁer is constant function).
 attacks do reach success, indicates defense is defeating attack subtle manner be increasing robustness.
 Perform brute-force random sampling.
 ﬁnal simple method identify obfuscated gradients is perform brute- force search adversarial examples (105 more) clean image.
 random search ﬁnds adversarial examples found optimization methods, defense is likely be obfuscating gradients.

 Attack Techniques Generating adversarial examples optimization- based methods requires useful gradients obtained backpropagation (Rumelhart et
 Many defenses therefore cause gradi- ent descent fail obfuscated gradients caused gradient shattering, stochastic gradients, vanish- ing/exploding gradients.
 discuss number techniques develop overcome obfuscated gradients.
 Backward Pass Differentiable Approximation Shattered gradients be caused unintentionally, e.g. numerical instability, intentionally, e.g. us- ing non-differentiable operations.
 attack defenses gradients are available, introduce tech- nique call Backward Pass Differentiable Approximation (BPDA).
 BPDA approach be used arbitrary network, is differentable, obtain useful gradient.
 SPECIAL CASE Many non-differentiable defenses be expressed fol- lows: given pre-trained classiﬁer f (·), construct prepro- cessor g(·) let secured classiﬁer ˆf (x) = f (g(x)) preprocessor g(·) satisﬁes g(x) ≈ x (e.g., g(·) perform image denoising remove adversar- ial perturbation).
 g(·) is smooth differentiable, computing gradients combined network ˆf is sufﬁcient circumvent defense (Carlini Wag- ner,
 However, recent work has constructed various functions g(·) are smooth differentiable, be backpropigated generate adversarial examples white-box attack.
 introduce new attack call Backward Pass Differ- entiable Approximation.
 g is constructed property g(x) ≈ approximate derivative derivative identity function: ∇xg(x) ≈ ∇xx =
 approximate derivative f (g(x)) point ˆx as: ∇xf (g(x))|x=ˆx ≈ ∇xf (x)|x=g(ˆx) allows compute gradients mount white-box optimization attack.
 Conceptually, attack is simple.
 perform forward propagation neural network usual, backward pass, replace g(·) identity function.
 implementation be expressed simpler way: approximate ∇xf (g(x)) evaluating ∇xf (x) point g(x).
 gives approximation true gradient, perfect, is useful averaged many iterations gradient descent generates adversarial example.
 GENERALIZED ATTACK above attack is effective simple class networks expressable f (g(x)) g(x) ≈ is general.
 generalize above approach.
 Let be smooth, differentable function, f (x) ≈ h(x).
 approximate ∇xf (x) perform forward pass y = f (x) is done typically.
 However, perform backward pass, backpropagate value function h(x).
 functions are similar, ﬁnd inaccurate gradients prove useful constructing adversarial example.
 practice, do use general construc- tion above.
 Instead, functions form f (g(x)), g(x) ≈ h(x) h(x) is differentiable, approx- imate ∇xf (g(x)) replacing g(·) h(·) back- ward pass.
 have found applying BPDA is necessary: replacing Obfuscated Gradients Give False Sense Security: Circumventing Defenses Adversarial Examples g(·) h(·) passes is ineffective (e.g., Song al.
 (2018)) many times effective (e.g. Buckman et al.
 (2018)).
 Differentiating Randomness Stochastic gradients arise using randomized transfor- mations input feeding classiﬁer using stochastic classiﬁer.
 using optimization- based attacks defenses employ techniques, is necessary estimate gradient stochastic function.
 Expectation Transformation.
 defenses em- ploy randomized transformations input, apply Ex- pectation Transformation (EOT) (Athalye et compute gradient expected trans- formation input.
 attacking classiﬁer f (·) ﬁrst trans- forms input according function t(·) sampled distribution transformations T EOT proposes optimizing expectation transformation Et∼T f (t(x)).
 optimization problem be solved gradient descent, not- ing ∇Et∼T f (t(x)) = Et∼T∇f (t(x)), differentiating classiﬁer transformation, approximat- ing expectation samples gradient descent step.
 Stochastic classiﬁers.
 defenses use stochastic classiﬁers, compute gradient computing gradients expectation random parameters.
 Reparameterization solve vanishing/exploding gradients reparameteriza- tion.
 are given classiﬁer f (g(x)) g(·) performs optimization loop transform input x new input ˆx.
 times, optimization loop means differentiating g(·), possible, yields ex- ploding vanishing gradients.
 resolve this, make change-of-variable x = h(z) function h(·) such g(h(z)) = h(z) z, h(·) is differentable.
 example, g(·) projects samples manifold speciﬁc manner, construct h(z) return points manifold.
 allows compute gradients f (h(z)) circumvent defense.

 Case Study: ICLR Defenses case study evaluating prevalence obfuscated gradients, study ICLR defenses argue robustness white-box threat model.
 ﬁnd defenses relies phenomenon argue security, demonstrate techniques Defense Buckman et al.
 (2018) Ma et al.
 (2018) Guo et al.
 (2018) Dhillon et al.
 (2018) Xie et al.
 (2018) Song et al.
 (2018) Samangouei (2018) Madry al.
 (2018) al.
 Dataset CIFAR CIFAR ImageNet CIFAR ImageNet CIFAR MNIST Distance ((cid:96)∞) ((cid:96)∞) ((cid:96)2) ((cid:96)∞) ((cid:96)∞) ((cid:96)∞) ((cid:96)2) Accuracy CIFAR ((cid:96)∞) Table
 Summary Results: Seven defense techniques accepted ICLR cause obfuscated gradients are vul- nerable attacks.
 (Defenses denoted ∗ propose combining adversarial training; report defense alone, see §5 full numbers.) Figure
 Illustration different distortion levels.
 Row Clean images.
 Row Adversarial examples, distortion  =
 Row Adversarial examples, distortion  =
 circumvent rely obfuscated gradients.
 omit defenses provable security claims argues black-box security.
 include paper, Ma et al.
 (2018), was proposed defense se, suggests method detect adversarial exampls.
 is asymmetry attacking defenses con- structing robust defenses: show defense be by- passed, is necessary demonstrate way do so; contrast, defender show attack succeeds.
 therefore give way evade defense be done.
 Table summarize results.
 accepted papers, be bypassed techniques rely obfuscated gradients.
 defenses argue robustness ImageNet, task CIFAR- argues robustnes MNIST, easier task CIFAR-10.
 such, comparing defenses datasets is difﬁcult.
 section use (cid:96)∞ distortion CIFAR-10 ImageNet; show Figure distortion looks comparison.
 Obfuscated Gradients Give False Sense Security: Circumventing Defenses Adversarial Examples A Secured Classiﬁer ADVERSARIAL TRAINING τ (xi,j,c)k ﬁrst paper consider (Madry et al., trains high- capacity neural network classify adversarial examples generated optimization methods.
 ﬁnd approach does cause gradient descent fail artiﬁcial ways.
 Defense Details.
 proposed Szegedy al.
 (2013), adversarial training is simple pro- cess.
 Given training data X loss function (cid:96)(·), standard training chooses network weights θ θ∗ = arg Ex∈X (cid:96)(x; Fθ).
 Adversarial training chooses -ball solves min-max formulation θ∗ = arg (cid:96)(x + δ; Fθ) Ex∈X (cid:20) (cid:21) max δ∈[−,]N solve formulation, Madry et al.
 (2018) solve inner maximization problem generating adver- sarial examples training data.
 Discussion.
 evaluation authors perform defense pass sanity checks.
 ﬁnd approach does cause obfuscated gradients are unable invalidate claims made.
 However, make important observations defense: (1) note adversarial retraining has been shown be difﬁcult ImageNet scale (Kurakin al., (2) training l∞ adversarial examples provides limited robustness adversarial examples other distortion metrics.
 Gradient Shattering THERMOMETER ENCODING Thermometer encoding (Buckman al., is encod- ing scheme designed break local linearity neural networks.
 ﬁnd consequence causes gradient shattering causes traditional optimization-based attacks fail.
 Defense Details.
 contrast prior work (Szegedy et viewed adversarial examples “blind spots” neural networks, Goodfellow et al.
 (2014b) argue reason adversarial examples exist is neural networks be- have linear manner.
 purpose thermometer encoding is break linearity.
 Given image x, pixel color l-level thermometer encoding τ (xi,j,c) is l-dimensional vector (cid:40) xi,j,c > k/l example, 10-level thermometer encoding, have τ (0.66)
 training process is identical normal thermometer networks.
 Due discrete nature thermometer encoded val- ues, is possible perform gradient descent thermometer encoded neural network.
 authors therefore construct Logit-Space Projected Gradient Ascent (LS-PGA) attack discrete thermometer en- coded inputs.
 Using attack, authors perform adversarial training Madry al.
 (2018) thermometer encoded networks.
 CIFAR-10, performing thermometer encoding was found give accuracy  = (cid:96)∞ distortion.
 performing adversarial training steps LS-PGA, robustness increased
 Discussion.
 intention defense is break local linearity neural networks, ﬁnd defense fact causes gradient shattering.
 be observed black-box attack ad- versarial examples generated standard trained model transfer thermometer encoded model re- ducing accuracy robustness white-box iterative attack.
 Evaluation.
 use BPDA approach Sec- tion let g(x) = τ (x).
 Observe deﬁne ˆτ (xi,j,c)k = min(max(xi,j,c − k/l, τ (xi,j,c)k = ﬂoor(ˆτ (xi,j,c)k) trained adversarial let h(x) = ˆτ (x) replace backwards pass function h(·).
 LS-PGA reduces model accuracy thermometer- encoded model training (bounded  =
 constrast, achieve model accuracy lower  = (and 
 shows measurable improvement standard models, trained thermometer encoding.
 attack thermometer-encoded trained model are able reproduce accu- racy  = claim LS-PGA.
 However, attack reduces model accuracy
 is weaker rate success original Madry et al.
 (2018) model.
 model is trained is, thermometer encoded model is trained using approach (Madry et al.,
 Obfuscated Gradients Give False Sense Security: Circumventing Defenses Adversarial Examples k is defense hyperparameter controls num- ber nearest neighbors consider.
 authors use distance function dj(x, y) =(cid:13)(cid:13)f − f measure distance jth activation layers.
 authors compute vector LID values sample: −−→ LID(x) = {LIDdj (x)}n j=1.
 −−→ Finally, compute LID(x) training data adversarial examples generated training data, train logistic regression classiﬁer detect adversarial examples.
 are grateful authors releasing complete source code.
 Discussion.
 LID is defense itself, authors assess ability LID detect different types attacks.
 solving formulation min.
 |x − x(cid:48)|2 + α ((cid:96)(x(cid:48)) + LID-loss(x(cid:48))) authors attempt determine LID metric is good metric detecting adversarial examples.
 Here, LID-loss(·) is function be minimized reduce LID score.
 However, authors report modiﬁed attack achieves success.
 Carlini Wagner’s (cid:96)2 attack is unbounded, time attack does reach success indicates attack became stuck local minima.
 happens, is possible modify loss function return attack success (Carlini Wagner,
 case, observe reason performing type adaptive attack fails is LID computation is, differentiable, differentiable useful maner.
 Computing LID term involves computing k-nearest neighbors computing ri(x).
 Minimizing gradient distance current k-nearest neighbors is representative true direction travel optimal set k-nearest neighbors.
 consequence, ﬁnd adversarial examples generated gradient methods penalizing high LID (a) are adversarial; are detected adversarial, penalizing LID loss.
 Evaluation.
 evaluate happen defense apply LID detect adversarial ex- amples.
 performing gradient descent term is difﬁcult differentiate through, have found generating high conﬁdence adversarial examples (Carlini Wagner, oblivious detector) is sufﬁcient fool detector.
 obtain authors detector trained Carlini Wagner’s (cid:96)2 Figure
 Model accuracy versus distortion (under l∞).
 Adversar- ial training increases robustness  = thermometer encoding provides limited value, coupled adversarial training performs worse adversarial training alone.
 (comparatively weak) LS-PGA attack, is unable adapt stronger attack present above.
 LOCAL INTRINSIC DIMENSIONALITY (LID) next paper studies properties adversarial exam- ples (Ma et al.,
 examine metric measures distance input compared neigh- bors, suggest LID be useful detecting adversarial examples.
 present evidence LID is larger adversarial examples generated existing attacks normal images, construct classiﬁer distinguish adversarial images normal images.
 authors emphasize classiﬁer is intended defense adversarial examples
 However, be natural wonder be effective defense, study robustness; results conﬁrm is adequate defense.
 method used compute LID relies ﬁnding k nearest non-differentiable operation, rendering gradient descent based methods ineffective.
 Defense Details.
 Local Intrinsic Dimensionality (Amsaleg et “assesses space-ﬁlling capability region surrounding reference example, based distance distribution example neighbors” (Ma et al.,
 Let S be mini-batch N clean exam- ples.
 Let ri(x) denote distance (under metric d(x, y)) sample x i-th nearest neighbor S (under metric d).
 LID be approximated (cid:32) LIDd(x) = − (cid:33)−1 k(cid:88) i=1 log ri(x) rk(x) communication authors.
 Magnitude0.00.20.40.60.81.0Model AccuracyBaselineThermometerAdv.
 TrainAdv.
 Therm Obfuscated Gradients Give False Sense Security: Circumventing Defenses Adversarial Examples attack train own Fast Gradient Sign were found be effective detecting adver- sarial examples generated other methods.
 generating high-conﬁdence adversarial examples minimizing (cid:96)∞ dis- tortion, are able reduce model accuracy success  =
 LID reports adversarial examples are benign rate (unmodiﬁed test data is ﬂagged benign rate).
 evaluation demonstrates LID metric be circumvented, future work evaluate building detector relying LID is robust adversarial examples targeting such detector.
 work raises questions large LID is fundamental characteristic adversarial examples, is by-product certain attacks.
 INPUT TRANSFORMATIONS Guo al.
 (2018) defend adversarial examples input transformations.
 explore number functions modifying input is fed classiﬁer.
 ﬁnd transformations is non- differentiable causes gradient shattering.
 Defense Details.
 Guo al.
 (2018) propose ﬁve input transformations counter adversarial examples: aging multiple runs.
 • Perform random image cropping rescaling, aver- • Quantize images bit-depth reduction.
 Apply JPEG compression remove perturbations.
 • Randomly drop pixels, restore performing total variance minimization.
 • Image quilting: Reconstruct images replacing × patches patches “clean” images, using minimum graph cuts overlapping boundary regions remove edge artifacts.
 authors explore different combinations input trans- formations different underlying ImageNet clas- siﬁers, including trained models.
 ﬁnd input transformations provide protection vanilla classiﬁer, providing varying degrees robustness varying transformations normalized (cid:96)2 perturbation budgets.
 Discussion.
 authors ﬁnd ResNet-50 classiﬁer provides varying degree accuracy ﬁve proposed input transformations strongest attack normalized (cid:96)2 dissimilarity strongest defenses achieving top-1 accuracy.
 observe similar results evaluating InceptionV3 classiﬁer.
 authors do succeed white-box attacks, credit- ing lack access test-time randomness “particularly crucial developing strong defenses” (Guo
 Evaluation.
 is possible bypass defense inde- pendently.
 circumvent image cropping rescaling direct application Expectation Transforma- tion (Athalye
 circumvent bit-depth reduc- tion, JPEG compression, total variance minimization, image quilting, use BPDA approximate backward pass identity function.
 Using attack, classi- ﬁcation accuracy drops strongest defense perturbation budget times smaller small- est perturbation budget considered Guo al.
 (2018), root-mean-square perturbation (corresponding “normalized” (cid:96)2 perturbation deﬁned Guo al.
 (2018)
 Stochastic Gradients STOCHASTIC ACTIVATION PRUNING (SAP) SAP drops neurons layer network proportional absolute magnitude (Dhillon
 using sampling compute gradients expec- tation values randomness, generate adversarial examples attack defense.
 Defense Details.
 SAP randomly drops neurons layer f i according probability proportional absolute value.
 hidden activation vector hi = f layer i, SAP deﬁnes probability distribution (cid:32) m(cid:88) (cid:33)−1 j = |hi j| · pi hi k=1 j is proportional magnitude hi is, pi j compared magnitude other neurons layer.
 SAP computes modiﬁed distribution j = − (1 − pi j)r qi r is defense hyperparameter (discussed below).
 Then, hi probability qi j is updated new value ˆhi j keeping j dropping  hi qi ˆhi j = probability qi probability − qi authors apply image cropping/rescaling, bit-depth reduc- tion, JPEG compression baselines (Personal communication authors).
 defense be stronger threat model adversary does have complete information exact quilting process used (Personal communication authors).
 Obfuscated Gradients Give False Sense Security: Circumventing Defenses Adversarial Examples values likely be sampled are scaled accord- ingly.
 value r is chosen keep large enough fraction neurons accuracy remains high, large neurons are kept.
 follow authors advice choose r test accuracy drops
 Discussion.
 authors evaluate SAP taking single step gradient direction (Dhillon
 taking single step direction gradient is effective non-randomized neural networks, ran- domization is used, computing gradient respect sample randomness is ineffective.
 direction Evaluation.
 resolve difﬁcult, estimate gra- dients computing expectation instantiations randomness.
 iteration gradient descent, taking step direction ∇xf (x) move i=1 ∇xf (x) invocation is ran- domized SAP.
 have found choosing k = provides useful gradients.
 had resolve numerical instability computing gradients: de- fense caused computing backward pass cause exploding gradients due division numbers
 re- solve clipping gradients stable numerical techniques.
 are able reduce SAP model accuracy  = .015, 
 consider attack successful example is classiﬁed times (and consider classiﬁed is classiﬁed correct label), model accuracy is 
 MITIGATING THROUGH RANDOMIZATION Xie al.
 (2018) defend adversarial examples rescaling padding images natural im- ages retain classiﬁcation adversarial examples are perturbed lose adversarial nature.
 circumvent defense using EOT synthesize robust adversarial examples (Athalye et al.,
 Defense Details.
 (Xie al., propose defend adversarial examples adding randomization layer input classiﬁer.
 classiﬁer takes × input, defense ﬁrst rescales image r × r image, r ∈ [299, randomly zero-pads image result is
 output is fed classiﬁer.
 Discussion.
 authors consider attack scenarios: vanilla attack (an attack original classiﬁer), single- pattern attack (an attack assuming ﬁxed randomization pattern), ensemble-pattern attack (an attack small ensemble ﬁxed randomization patterns).
 authors strongest attack reduces InceptionV3 model accuracy top-1 accuracy (over images were classiﬁed correctly).
 authors dismiss stronger attack larger choices randomness, stating be impossible” (emphasis ours) attack “may converge” (Xie
 Evaluation.
 ﬁnd authors ensemble-pattern attack overﬁts ensemble ﬁxed randomization.
 by- pass defense applying Expectation Transforma- tion (Athalye optimizing (in case, discrete) distribution transformations T minimizing Et∼T f (t(x)).
 approximate gradient above sampling differentiating transformation.
 Using attack, consider attack successful example is classiﬁed times reduce accuracy classiﬁer maximum (cid:96)∞ perturbation  =
 Vanishing Exploding Gradients PIXELDEFEND Song al.
 (2018) propose using PixelCNN generative model project potential adversarial example data manifold feeding classiﬁer.
 bypass defense using BPDA.
 Defense Details.
 Song al.
 (2018) argue adversarial examples lie low-probability region training distribution.
 PixelDefend “puriﬁes” perturbed images projecting data manifold use PixelCNN generative model, feeds resulting image unmodiﬁed classiﬁer.
 PixelDefend uses greedy decoding procedure approximate ﬁnding highest probability example -ball input image.
 Discussion.
 authors evaluate PixelDefend CIFAR- number underlying classiﬁers, perturbation budgets, attack algorithms.
 maximum (cid:96)∞ perturbation  = CIFAR- PixelDefend claims accuracy (with vanilla ResNet classiﬁer).
 authors dismiss possibility end-to-end attacks PixelDefend due difﬁculty differentiating unrolled version PixelDefend due vanishing gradients computation cost.
 Evaluation.
 sidestep problem computing gradi- ents unrolled version PixelDefend approxi- mating gradients BPDA, approximating backward Obfuscated Gradients Give False Sense Security: Circumventing Defenses Adversarial Examples pass identity function, mount end-to-end attack using technique
 attack, reduce accuracy trained classiﬁer achieves accuracy maximum (cid:96)∞ perturbation  =
 Using trained classiﬁer Madry al.
 (2018), using PixelDefend provides additional robustness using trained classiﬁer.
 DEFENSE-GAN Defense-GAN (Samangouei et uses Generative Adversarial Network (Goodfellow et project samples manifold generator classi- fying them.
 use reparameterization circumvent multi-step projection process construct adversarial ex- amples.
 Defense Details.
 defender ﬁrst trains Generative Adversarial Network generator G(z) maps sam- ples latent space (typically z ∼ N (0, images look training data.
 Defense-GAN takes trained classiﬁer f (·), classify input x, return- ing f (x), returns f (arg minz |G(z) − x|).
 perform projection manifold, authors take many steps gradient descent starting different random initializa- tions.
 Defense-GAN was shown be effective CIFAR-10.
 therefore evaluate MNIST was argued be secure).
 Discussion.
 Samangouei al.
 authors con- struct white-box attack unrolling gradient descent used classiﬁcation.
 unbounded (cid:96)2 pertur- bation size, Carlini Wagner’s attack reaches misclassiﬁcation rate vulnerable modele strongest.
 leads believe unrolling gradient descent breaks gradients.
 Evaluation.
 Performing manifold projection is non- trivial inner optimization step generating adver- sarial examples.
 sidestep difﬁculty, show adversarial examples exist data manifold.
 is, construct adversarial example x(cid:48) = G(z∗) |x − x(cid:48)| is small c(x) (cid:54)= c(x(cid:48)).
 do solve re-parameterized formulation min.
 (cid:107)G(z) − x(cid:107)2 + c · (cid:96)(G(z)).
 place PixelCNN, due availability pre-trained model, use PixelCNN++ (Salimans et al., discretize mixture logistics produce 256-way softmax possible pixel values.
 Due compute limitations, evaluate attack random sample images test set.
 Figure
 Images MNIST test set.
 Row Clean images.
 Row Adversarial examples unsecured classiﬁer.
 Row Adversarial examples Defense-GAN.
 initialize z = arg minz |G(z)− x| (also found gradi- ent descent).
 train WGAN using code authors provide (Gulrajani al., MNIST CNN accuracy.
 run iterations gradient descent generating adversarial example; takes minute instance.
 unsecured classiﬁer requires mean (cid:96)2 distor- tion (per-pixel normalized, un-normalized) fool.
 mount attack Defense-GAN, require mean distortion increase distortion see Figure examples adversarial examples.
 reason attacks succeed success suffering vanishing exploding gradients is gradient computation needs differentiate generator G(·) once.
 Concurrent work, Ilyas et al.
 (2017) develop identical approach Defense-GAN; ﬁnd is vulnerable attack outline increase robustness adversarial training.
 do evaluate extended approach.

 Discussion Having demonstrated attacks papers, take step describe believe be complete method evaluating defense adversarial examples.
 Much advice give has been given prior work (Carlini Wagner, Madry repeat offer own perspective com- pleteness.
 hope future work use guide performing evaluation.
 Deﬁne (realistic) threat model constructing defense, is critical deﬁne threat model limits adversary.
 Prior work used words white-box, grey-box, black-box, no-box describe different threat models, overloading same word.
 attempting to, again, redeﬁne vocabulary, outline various aspects defense be Obfuscated Gradients Give False Sense Security: Circumventing Defenses Adversarial Examples revealed adversary held secret defender: Evaluate adaptive attacks • Model architecture Model weights • Training algorithm Training data • defenses involve randomness, adversary knows exact sequence random values be chosen, distribution.
 • assuming adversary does know model architecture weights, query access is allowed.
 so, model output is logits, probability vector, predicted label (i.e., arg max).
 are aspects defense be held secret, threat models contain unrealistic constraints.
 believe compelling threat model least grant knowledge model architecture, training algorithm, allow query access.
 do believe is meaningful restrict compu- tational power adversary.
 defenses are robust generating adversarial examples takes second takes ten robustness has increased.
 adversary’s computational effort be shown be increased pre- diction runtime be acceptable use runtime security parameter.
 However, increasing attack time few seconds (as occurs defenses attack paper) is meaningful.
 Make speciﬁc, testable claims deﬁning clear threat model, defense make make speciﬁc, testable claims.
 defense claim robustness (cid:96)∞ adversarial examples distortion most  = claim mean (cid:96)2 distortion adversarial examples increases factor baseline model secured model (in case, baseline be deﬁned).
 Unfortunately, many defenses evaluate claim are robust giving speciﬁc bounds.
 biggest violation advice is defense claim complete robustness unbounded attacks: unlimited distortion image be converted other, yielding “success”.
 order allow claims be defense be speciﬁed completely, hyperparameters given.
 Releasing source code pre-trained model paper is useful method making ex- plicit claims.
 defenses study made complete source code available (Madry et al., Ma Guo Xie
 Claiming increased robustness existing attacks, speciﬁc testable, is useful claim.
 is important evaluate own defense new defense-aware attacks justify claims security.
 particular, defense has been speciﬁed, is important attempt circumvent concrete defense, assuming adversary is restricted threat model.
 be circumvented, is important give ways prevent speciﬁc attack (i.e., tweaking hyperparameter).
 evaluation, is acceptable modify defense, new attack be built target modiﬁed defense.
 way, concluding evaluation ﬁnal adaptive attack be seen analogous evaluating model test data.

 Conclusion Constructing defenses adversarial examples requires de- fending existing attacks future attacks be developed.
 paper, identify obfuscated gradients, phenomenon exhibited certain defenses makes standard gradient-based methods fail generate adversarial examples.
 develop attack techniques bypass different types obfuscated gra- dients.
 evaluate applicability techniques, use ICLR defenses case study, circumventing accepted defenses.
 generally, hope future work be able avoid relying obfuscated gradients perceived robust- ness use evaluation approach detect occurs.
 Defending adversarial examples is impor- tant area research believe performing thorough evaluation is critical step be overlooked.
 Acknowledgements are grateful Aleksander Madry Andrew Ilyas helpful comments early draft paper.
 thank Bo Li, Xingjun Ma, Laurens van der Maaten, Aurko Roy, Yang Song, Cihang Xie useful discussion insights defenses.
 work was supported National Science Foundation CNS-1514457, Qualcomm, Hewlett Foundation Center Long-Term Cybersecurity.
 References Amsaleg, Laurent, Chelly, Oussama, Furon, Teddy, Girard, St´ephane, Houle, Michael E, Kawarabayashi, Ken-ichi, Nett, Michael.
 Estimating local intrinsic dimension- Obfuscated Gradients Give False Sense Security: Circumventing Defenses Adversarial Examples ality.
 Proceedings ACM SIGKDD Inter- national Conference Knowledge Discovery Data Mining, pp.


 Athalye, Anish, Engstrom, Logan, Ilyas, Andrew, Kwok, Kevin.
 Synthesizing robust adversarial examples.
 arXiv preprint arXiv:1707.07397,
 Bengio, Y., Simard, P., Frasconi, P.
 Learning long-term dependencies gradient descent is difﬁcult.
 IEEE Transactions Neural Networks, Mar
 ISSN 1045-9227.
 doi:
 Buckman, Jacob, Roy, Aurko, Raffel, Colin, Good- fellow, Ian.
 Thermometer encoding: hot way International Conference resist adversarial examples.
 Learning Representations,
 URL https:// openreview.net/forum?id=S18Su--CW.
 cepted poster.
 Carlini, Nicholas Wagner, David.
 Adversarial examples are detected: Bypassing ten detection methods.

 Carlini, Nicholas Wagner, David.
 Magnet “efﬁcient defenses adversarial attacks” are robust adversarial examples.
 arXiv preprint arXiv:1711.08478,
 Carlini, Nicholas Wagner, David.
 Towards evaluating robustness neural networks.
 IEEE Symposium Security Privacy,
 Dhillon, Guneet S., Azizzadenesheli, Kamyar, Bernstein, Jeremy D., Kossaiﬁ, Jean, Khanna, Aran, Lipton, Zachary C., Anandkumar, Animashree.
 Stochastic In- activation pruning robust adversarial defense.
 ternational Conference Learning Representations,
 URL https://openreview.net/forum?
 id=H1uR4GZRZ.
 accepted poster.
 Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley, David, Ozair, Sherjil, Courville, Aaron, Bengio, Yoshua.
 Generative adversarial nets.
 Advances neural information processing systems, pp.

 Goodfellow, Ian J, Shlens, Jonathon, Szegedy, Christian.
 Explaining harnessing adversarial examples.
 arXiv preprint arXiv:1412.6572,
 Guo, Chuan, Rana, Mayank, Cisse, Moustapha, van der Maaten, Laurens.
 Countering adversarial images us- International Conference ing input transformations.
 Learning Representations,
 URL //openreview.net/forum?id=SyJ7ClWCb. cepted poster.
 He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, Sun, Jian.
 Deep residual learning image recognition.
 Proceedings IEEE conference computer vision pattern recognition, pp.

 Hendrik Metzen, Jan, Genewein, Tim, Fischer, Volker, Bischoff, Bastian.
 detecting adversarial perturbations.
 International Conference Learning Representations,
 Hendrycks, Dan Gimpel, Kevin.
 Early methods detecting adversarial images.
 International Conference Learning Representations (Workshop Track),
 Ilyas, Andrew, Jalal, Ajil, Asteri, Eirini, Daskalakis, Con- stantinos, Dimakis, Alexandros G.
 robust mani- fold defense: Adversarial training using generative mod- els.
 arXiv preprint arXiv:1712.09196,
 Krizhevsky, Alex Hinton, Geoffrey.
 Learning multiple layers features tiny images.

 Krizhevsky, Alex, Sutskever, Ilya, Hinton, Geoffrey E.
 ImageNet classiﬁcation deep convolutional neural networks.
 Advances neural information processing systems, pp.

 Kurakin, Alexey, Goodfellow, Ian, Bengio, Samy.
 Ad- versarial examples physical world.
 arXiv preprint arXiv:1607.02533,
 Kurakin, Alexey, Goodfellow, Ian J., Bengio, Samy.
 Adversarial machine learning scale.
 arXiv preprint arXiv:1611.01236,
 LeCun, Yann.
 mnist database handwritten digits.
 http://yann.lecun.com/exdb/mnist/,
 Ma, Xingjun, Li, Bo, Wang, Yisen, Erfani, Sarah M., Wijewickrema, Sudanthi, Schoenebeck, Grant, Houle, Michael E., Song, Dawn, Bailey, James.
 Characteriz- ing adversarial subspaces using local intrinsic dimension- ality.
 International Conference Learning Represen- tations,
 URL https://openreview.net/ forum?id=B1gJ1L2aW.
 accepted oral presenta- tion.
 Gulrajani, Ishaan, Ahmed, Faruk, Arjovsky, Martin, Du- moulin, Vincent, Courville, Aaron.
 Improved training wasserstein gans.
 arXiv preprint arXiv:1704.00028,
 Madry, Aleksander, Makelov, Aleksandar, Schmidt, Lud- wig, Tsipras, Dimitris, Vladu, Adrian.
 Towards deep learning models resistant adversarial attacks.
 International Conference Learning Representations, Obfuscated Gradients Give False Sense Security: Circumventing Defenses Adversarial Examples Zantedeschi, Valentina, Nicolae, Maria-Irina, Rawat, Ambrish.
 Efﬁcient defenses adversarial attacks.
 arXiv preprint arXiv:1707.06728,

 URL https://openreview.net/forum?
 accepted poster.
 Meng, Dongyu Chen, Hao.
 MagNet: two-pronged de- fense adversarial examples.
 ACM Conference Computer Communications Security (CCS),
 arXiv preprint arXiv:1705.09064.
 Papernot, Nicolas, McDaniel, Patrick, Wu, Xi, Jha, Somesh, Swami, Ananthram.
 Distillation defense ad- versarial perturbations deep neural networks.
 Security Privacy (SP), IEEE Symposium on, pp.

 IEEE,
 Rumelhart, David E., Hinton, Geoffrey E., Williams, Ronald J.
 Learning representations back-propagating errors.
 Nature,
 Salimans, Tim, Karpathy, Andrej, Chen, Xi, Kingma, Diederik P.
 Pixelcnn++: pixelcnn implementation discretized logistic mixture likelihood other modiﬁ- cations.
 ICLR,
 Samangouei, Pouya, Kabkab, Maya, Chellappa, Rama.
 Defense-gan: Protecting classiﬁers adversarial at- tacks using generative models.
 International Conference Learning Representations,
 URL https:// openreview.net/forum?id=BkJ3ibb0-.
 cepted poster.
 Song, Yang, Kim, Taesup, Nowozin, Sebastian, Ermon, Stefano, Kushman, Nate.
 Pixeldefend: Leveraging generative models understand defend adver- sarial examples.
 International Conference Learning Representations,
 URL https://openreview.
 net/forum?id=rJUYGxbCW.
 accepted poster.
 Szegedy, Christian, Zaremba, Wojciech, Sutskever, Ilya, Bruna, Joan, Erhan, Dumitru, Goodfellow, Ian, Fer- gus, Rob.
 Intriguing properties neural networks.
 ICLR,
 Szegedy, Christian, Vanhoucke, Vincent, Ioffe, Sergey, Shlens, Jon, Wojna, Zbigniew.
 Rethinking in- ception architecture computer vision.
 Proceedings IEEE Conference Computer Vision Pattern Recognition, pp.

 Xie, Cihang, Wang, Jianyu, Zhang, Zhishuai, Ren, Zhou, Yuille, Alan.
 Mitigating adversarial effects randomization.
 International Conference Learning Representations,
 URL https://openreview.
 net/forum?id=Sk9yuql0Z.
 accepted poster.
 Zagoruyko, Sergey Komodakis, Nikos.
 Wide residual networks.
 arXiv preprint arXiv:1605.07146,
 D EEP learning (DL) networks take form heuristic rich architectures develop unique intermediate data representation.
 complexity architectures is re- ﬂected sizes layers and, large number data sets reported literature, processing.
 architectural complexity excessive number weights units are built DL data representation design are deliberate [1–5].
 deep architectures are capable learning complex mappings, are difﬁcult train, is hard interpret layer has learnt.
 Moreover, gradient- based optimization random initialization used training is susceptible converging local minima [6], [7].
 addition, is believed humans analyze complex interactions breaking isolated understandable hierarchical concepts.
 emergence part- based representation human cognition be tied nonnegativity constraints [8].
 way enable easier human understandability concepts neural networks is constrain network’s weights be nonnegative.
 Note such representation nonnegative weights multilayer network perceptron implement shattering B.
 O.
 Ayinde is Department Electrical Computer En- gineering, University Louisville, Louisville, KY, USA (e-mail: babajide.ayinde@louisville.edu).
 J.
 M.
 Zurada is Department Electrical Computer Engineer- ing, University Louisville, Louisville, KY, USA, Information Technology Institute, University Social Science,Ł´odz Poland (Corresponding author, e-mail: jacek.zurada@louisville.edu).
 work was supported part NSF grant
 points provided suitable negative bias values are used [9].
 Drawing inspiration idea Nonnegative Matrix Factorization (NMF) sparse coding [8], [10], hidden structure data be unfolded learning features have capabilities model data parts.
 NMF enforces encoding data features be nonnegative thereby resulting additive data representation, however, incorporating sparse coding NMF purpose encoding data is expensive, AEs, incorporation is learning-based fast.
 performance deep network be enhanced using Nonnegativity Constrained Sparse Autoencoder (NCAE) part-based data representation capability [11], [12].
 is remarked weight regularization is concept has been employed understandability generaliza- tion context.
 is used suppress magnitudes weights reducing sum squares.
 Enhancement sparsity be achieved penalizing sum absolute values weights sum squares [13–17].
 paper, work proposed [11] is extended modifying cost function extract more sparse features, encouraging nonnegativity network weights, enhancing understandability data.
 Other related model is Nonnegative Sparse Autoencoder (NNSAE) trained online algorithm tied weights linear output activation function mitigate training hassle [18].
 [18] uses piecewise linear decay function enforce nonnegativity focuses shallow proposed uses composite norm focus deep architectures.
 Dropout is introduced used heuristic sparsify AEs prevent overﬁtting dropping units connections neural network training [20].
 recently, different paradigm AEs constrain output encoder follow chosen prior distribution have been proposed [21–23].
 variational decoder is trained reconstruct input samples follow chosen using variational inference [21].
 Realistic data points be reconstructed original data space feeding decoder samples chosen prior distribution.
 other hand, adversarial AE matches encoders output distribution arbitrary prior distribution using adversarial training discriminator generator [22].
 adversarial training, encoder learns map data distribution prior distribution.
 problem addressed is three-fold: inter- pretability AE-based deep layer architecture fostered en- forcing high degree weight’s nonnegativity network.
 improves NCAEs show negative weights imposing nonnegativity constraints network’s weights [11].
 (ii) is demonstrated proposed architecture be utilized extract meaningful representations unearth hidden structure high-dimensional data.
 (iii) is shown resulting nonnegative AEs do deteriorate classiﬁcation performance.
 paper ex- pands scope AE model ﬁrst introduced [24] by: (i) introducing smoothing function L1 regularization numerical stability, (ii) illustrating connection proposed regularization weights’ nonnegativity, (iii) drawing more insight variety dataset, (iv) comparing proposed recent AE architectures, (v) sup- porting interpretability claim new experiments text categorization data.
 paper is structured follows: Section II introduces network conﬁguration notation nonnegative sparse feature extraction.
 Section III discusses experimental designs Section IV presents results.
 Finally, conclusions are drawn Section V.
 II.
 NONNEGATIVE SPARSE FEATURE EXTRACTION USING CONSTRAINED AUTOENCODERS shown [8], way representing data is shattering various distinct pieces manner additive merging pieces reconstruct original data.
 Mapping intuition AEs, idea is disintegrate data parts encoding layer subse- parts recombine original data decoding layer.
 disintegration be achieved imposing nonnegativity constraint network’s weights [11], [26].
 A.
 L1/L2-Nonnegativity Constrained Sparse Autoencoder (L1/L2-NCSAE) order encourage higher degree nonnegativity network’s weights, composite penalty term (1) is added objective function resulting cost function expression L1/L2-NCSAE: n(cid:48)(cid:88) (cid:0)W, b(cid:1) = JAE + β sl(cid:88) r=1 (cid:13)(cid:13)(cid:13)(cid:13) (cid:18) sl+1(cid:88) fL1/L2 m(cid:88) (cid:0)w(l) k=1 ij (cid:1) DKL hr(x(k)) JL1/L2-NCSAE l=1 i=1 j=1 (1) W = {W(1), W(2)} b = {bx, bh} represent weights biases encoding decoding layers respec- tively; sl is number neurons layer l.
 w(l) represents connection jth neuron layer l−1 ith neuron ij layer l given input x, (cid:13)(cid:13)(cid:13)σ(W(2)σ(W(1)x(k) + bx) + bh) − x(k)(cid:13)(cid:13)(cid:13)2 (2) || (cid:5) ||2 is m is number training Euclidean norm.
 DKL((cid:5)) is Kullback-Leibler (KL) divergence sparsity control [27] p denoting desired m(cid:88) JAE = activation average activations hidden units, n(cid:48) is number hidden units, hj(x(k)) = σ(W(1) j x(k) + bx,j) denotes activation hidden unit j due input x(k), σ((cid:5)) is element-wise application logistic sigmoid, σ(x) = + exp(−x)), β controls sparsity penalty term, (cid:40) fL1/L2 (wij) = α1Γ(wij, κ) + ||wij||2 α2 wij wij ≥ (3) α1 α2 are L1 L2 nonnegativity-constraint weight penalty factors, respectively.
 p, β, α1, α2 are set using sampled images training set held-out validation set hyperparameter tuning network is retrained entire dataset.
 weights are updated using error backpropagation: w(l) ij = w(l) ij − ξ ∂w(l) ij JL1/L2-NCSAE(W, b) b(l) i = b(l) i − ξ ∂b(l) JL1/L2-NCSAE(W, b) (4) (5) ξ > is learning rate gradient L1/L2- NCSAE loss function is computed (6).
 ∂w(l) ij JL1/L2-NCSAE(W, b) = (cid:0)W, b(cid:1) (cid:18) DKL JAE ∂w(l) ij + β + g(cid:0)w(l) ∂w(l) ij (cid:1) ij (cid:19) hj(x(k)) m(cid:88) k=1 (6) g(wij) is composite function denoting derivative fL1/L2(wij) (3) respect wij (7).
 (cid:26) α1∇w (cid:107)wij(cid:107) + α2wij wij wij ≥ (7) g(wij) = (cid:19) penalty function (1) is extension NCAE (obtained setting α1 zero), close scrutiny weight distribution encoding decoding layer NCAE reveals many weights are nonnegative imposing nonnegativity constraints.
 reason is original L2 norm used NCAE penalizes negative weights big magnitudes stronger smaller magnitudes.
 forces good number weights take small negative values.
 paper uses additional L1 occurrence, is, L1 penalty forces most negative weights become nonnegative.
 B.
 Implication imposing nonnegative parameters com- posite decay function graphical illustration relation weight distribution composite decay function is shown (a) (b) (c) (d) Fig.
 (a) Symmetric (G3) skewed (G1 G2) weight distributions.
 Decay function values α1 α2 weight distribution (b) G3 (c) G1 (d) G2.
 Fig.

 Ideally, addition Frobenius norm weight matrix (α||W||2 F reconstruction error (2) imposes Gaussian prior weight distribution shown curve G3 Fig.

 However, using composite function (3) results imposition positively-skewed deformed Gaussian distribution curves G1 G2.
 degree nonnegativity be adjusted using parameters α1 α2.
 parameters have be chosen enforce nonnegativity ensuring good supervised learning outcomes.
 effect L1 (α2 = L2 (α1 = L1/L2 (α1 (cid:54)= α2 (cid:54)= nonnegativity penalty terms weight updates weight distributions G1, G2 G3 are shown Fig.
 b.
 be observed distributions L1/L2 regularization enforces stronger weight decay individual L1 L2 regulariza- tion.
 Other observation Fig.
 is more positively- skewed weight distribution lesser weight decay function.
 consequences minimizing (1) are that: average reconstruction error is reduced sparsity hidden layer activations is increased negative weights are forced leading sparsity enhancement, (iii) number nonnegative weights is increased.
 resultant effect penalizing weights L1 L2 norm is large positive connections are preserved magnitudes are shrunk.
 However, L1 norm (3) is non-differentiable origin, lead numerical instability simulations.
 circumvent drawback, well known smoothing function approximates L1 norm (3) is utilized.
 Given ﬁnite dimensional vector z positive constant following smoothing function approximates L1 norm: ||z|| > κ ||z|| ≤ κ ||z|| > κ ||z|| ≤ κ (8) (9) (cid:40) ||z|| Γ(z, κ) = ||z||2 gradient ∇zΓ(z, κ) = (cid:40) z ||z|| convenience, adopt (8) smoothen L1 penalty function κ is set III.
 dataset experiments, data sets are used, namely: MNIST [28], NORB normalized-uniform [29], Reuters- text categorization dataset.
 Reuters-21578 text categorization dataset comprises documents featured Reuters newswire.
 ModApte split was em- frequent classes.
 ployed limit ModApte split was utilized limit categories frequent categories.
 bag-of-words format has been stemmed stop-word removed was used; see http://people.kyb.tuebingen.mpg.de/pgehler/rap/ further clariﬁcation.
 dataset contains documents dimensions.
 techniques were used reduce dimensionality document order preserve informative correlated words [30].
 reduce dimensionality document contain informa- tive correlated words, words were sorted based frequency occurrence dataset.
 Words frequency were eliminated.
 informative words do occur topic were selected based information gain class attribute.
 remaining words (or features) dataset were sorted using method, important features were removed based desired dimension documents.
 paper, length feature vector documents was reduced
 preliminary experiment, subset MNIST handwritten digits extracted purpose understanding deep network constructed using L1/L2-NCSAE processes classiﬁes input.
 easy in- terpretation, small deep network was constructed trained stacking AEs hidden neurons softmax neurons.
 number hidden neurons was chosen obtain good classiﬁcation accuracy keeping network small.
 network is kept small full MNIST data require larger hidden layer size limit network interpretability.
 image digit is ﬁltered network, be observed Fig.
 sparsiﬁcation weights layers is aftermath nonnegativity −1−0.500.5100.20.40.60.81weight wProbability Distribution P(w) G1G2G3−1−0.500.51−2−1.5−1−0.50wg(w) α1= α1= α1= Fig.
 Filtering signal L1/L2-NCSAE trained using reduced MNIST data set class labels
 test image is 28×28 pixels image unrolled vector values.
 input test sample receptive ﬁelds ﬁrst autoencoding layer are presented images.
 weights output layer are plotted diagram row output column hidden neuron (L − layer.
 architecture is 784-10-10-3.
 range weights are scaled [-1,1] mapped graycolor map.
 w = −1 is assigned black, w grey, w = is assigned white color.
 is, black pixels indicate negative, grey pixels indicate zero-valued weights white pixels indicate positive weights.
 constraints imposed network.
 observation is most weights network have been conﬁned nonnegative domain, removes opaqueness deep learning process.
 be seen fourth seventh receptive ﬁelds ﬁrst AE layer have dominant activations (with activation values respectively) capture most information test input.
 are able ﬁlter distinct part input digit.
 outputs ﬁrst layer sigmoid constitute higher level features extracted test image emphasis fourth seventh features.
 second layer second, sixth, eight, tenth neurons have dominant activations (with activation values respectively) have stronger connections dominant neurons ﬁrst layer rest.
 softmax layer, second neuron was activated has strongest connections dominant neurons second layer classifying test image ”2”.
 fostering interpretability is demonstrated using subset NORB normalized-uniform dataset [29] class labels ”four-legged animals”, ”human ﬁgures”, ”airplanes”.
 1024-10-5-3 network conﬁguration was trained subset NORB data using stacked L1/L2-NCSAEs Softmax layer.
 Fig.
 shows sampled test patterns weights activations ﬁrst second AE layer are shown Fig.

 bar charts indicate activations hidden units sample input patterns.
 features learned units layer are localized, sparse allow easy interpretation isolated data parts.
 features show nonnegative weights making easier visualize input object patterns respond.
 be seen units network discriminate objects images react input patterns.
 Third, sixth, eight, ninth hidden units layer capture features are common objects class ”2” react shown ﬁrst layer activations.
 features captured second layer activations reveal second ﬁfth hidden units are stimulated objects class ”2”.
 outputs Softmax layer represent posteriori class probabilities given sample are denoted Softmax scores.
 important observation Fig.
 c is hidden units layers did capture signiﬁcant representative features class ”1” white color-coded test sample.
 is reasons is misclassiﬁed class ”3” probability
 argument goes class dark-grey color-coded test sample misclassiﬁed class ”3” probability
 contrast, hidden units layers capture signiﬁcant representative features class ”2” test samples color codes.
 is class ”2” test samples are classiﬁed high probabilities shown Fig.

 Lastly, network contains good number representative features class ”3” test samples was able classify given Fig.

 IV.
 RESULTS DISCUSSION A.
 Unsupervised Feature Learning Image Data ﬁrst set experiments, three-layer L1/L2-NCSAE, NCAE [11], DpAE [19], conventional SAE network hidden neurons were trained using MNIST dataset handwritten digits ability discover patterns Test sample (Image) Weights biases hidden neurons Layer image is formed weights single neuron
 dot-products input Neuron weights Layer
 bias is added, sigmoid is applied
 bias is added, sigmoid is applied
 dot-product classification layer weights.
 Biases are added
 softmax nonlinearity is applied get probabilities -5.881 -3.329 -3.169 -2.919 -3.163 -3.173 -3.098 -27.69 -3.567 -3.344 = = = = = = = = = = -3.917 -4.142 -3.550 -3.381 -3.699 -3.969 -3.410 -3.987 -3.899 -3.793 = = = = = = = = = = = = = “1” “2” “6” Weights biases hidden neurons Layer
 row is vector weights single neuron Matrix classification weights row represents output Fig.
 weights were trained using stacked L1/L2-NCSAEs. RFs learned reduced NORB dataset are plotted images bottom part (a).
 intensity pixel is proportional magnitude weight connected pixel input image negative value indicating black, positive values white, value corresponding gray.
 biases are shown.
 activations ﬁrst layer hidden units NORB objects presented (b) are depicted bar chart top RFs. weights second layer AE are plotted diagram topmost part (a).
 row plot corresponds weight hidden unit second AE column weight hidden unit ﬁrst layer AE.
 magnitude weight corresponds area square; white indicates positive, grey indicates zero, black negative sign.
 activations second layer hidden units are shown bar chart right-hand side second layer weight diagram.
 column shows activations hidden unit ﬁve color-coded examples same object.
 outputs Softmax layer color-coded test objects class labels (c) ”fourlegged tagged class (d) ”human ﬁgures” class (e) ”airplanes” class
 high dimensional data are compared.
 experiments were run time recorded.
 encoding weights W(1), known receptive ﬁelds ﬁlters case image data, are reshaped, scaled, centered × pixel box visualized.
 ﬁlters learned L1/L2-NCSAE are compared learned counterparts, NCAE SAE.
 be observed results Fig.
 L1/L2-NCSAE learned receptive ﬁelds are more sparse localized SAE, DpAE, NCAE.
 is remarked black pixels SAE DpAE features are results negative weights values numbers are reduced NCAE nonnegativity constraints, are reduced imposing additional L1 penalty term L1/L2-NCSAE shown histograms located right side ﬁgure.
 case L1/L2-NCSAE, tiny strokes dots constitute basic part handwritten digits, are unearthed compared SAE, DpAE, NCAE.
 Most features learned SAE are major parts digits blurred version digits, are sparse learned L1/L2-NCSAE.
 features learned DpAE are fuzzy compared L1/L2-NCSAE are sparse distinct.
 achieved sparsity encoding be traced ability L1 L2 regularization enforcing high degree weights’ nonnegativity network.
 Fig.
 L1/L2-NCSAE other AEs are compared terms reconstruction error, varying number hidden nodes.
 expected, be observed L1/L2-NCSAE yields lower reconstruction error MNIST training set compared SAE, DpAE, NCAE.
 Although, close scrutiny result reveals reconstruction error L1/L2-NCSAE deteriorates compared NCAE hidden size grows
 average, L1/L2-NCSAE reconstructs other AEs considered.
 be observed DpAE dropout has high reconstruction error hidden layer size is small (100 less).
 is few neurons left are unable capture dy- namics data, results underﬁtting data.
 However, reconstruction error improves hidden layer size is increased.
 Lower reconstruction error case L1/L2-NCSAE NCAE is indication nonnegativity constraint facilitates learning parts digits are essential reconstructing digits.
 KL-divergence sparsity measure reveals L1/L2-NCSAE has more sparse hidden activations SAE, DpAE NCAE different hidden layer size shown Fig.

 Again, averaging training examples, L1/L2-NCSAE yields activated hidden neurons compared counterparts.
 Also, using t-distributed stochastic neighbor (a) (b) (e) Softmax scores (c) (d) Weights hidden units Layer Weights hidden units Layer Activations Layer hidden units Activations Layer hidden units Class images Class images Class images (a) SAE (b) DpAE (c) NCAE (d) L1/L2-NCSAE Fig.
 receptive ﬁelds (W(1)) weight histograms learned MNIST digit data set using (a) SAE, (b) DpAE (c) NCAE, (d) L1/L2-NCSAE.
 Black pixels indicate negative, white pixels indicate positive weights.
 range weights are scaled [-1,1] mapped graycolor map.
 w = −1 is assigned black, w grey, w = is assigned white color.
 (a) (b) Fig.
 (a) Reconstruction error (b) Sparsity hidden units measured KL-divergence using MNIST train dataset p =
 embedding (t-SNE) project 196-D representation MNIST handwritten digits distribution features encoded encoding ﬁlters DpAE, NCAE, L1/L2-NCSAE are visualized Figs.
 b, c.
 careful look Fig.
 reveals digits ”4” ”9” are overlapping DpAE, increase chance misclassifying digits.
 be observed Fig.
 corresponding NCAE digit ”2” is projected different landmarks.
 manifolds digits L1/L2-NCSAE are separable counterpart shown Fig.
 aiding classiﬁer map separating boundaries digits easily.
 second experiment, SAE, NCAE, L1/L2-NCSAE, DpAE hidden nodes were trained using NORB normalized-uniform dataset.
 NORB normalized- uniform dataset, is second dataset, contains training images test images toys generic categories: four-legged animals, human ﬁgures, airplanes, trucks, cars.
 training testing sets consist instances category.
 image consists channels, size pixels.
 inner pixels channels cropped resized using bicubic interpolation × pixels form vector entries input.
 selected weights neurons are plotted Fig.

 be seen L1/L2- NCSAE learned sparse features compared features learned other AEs considered.
 receptive ﬁelds hidden nodesReconstruction error SAENCAEL1/L2−NCSAEDpAE4955001.922.12.22.3 hidden nodesKL−Divergence SAENCAEL1/L2−NCSAEDpAE4804905002468x Fig.
 t-SNE projection [31] representations MNIST handwritten digits using (a) DpAE (b) NCAE (c) L1/L2- NCSAE.
 (a) SAE (b) DpAE (c) NCAE Fig.
 Weights selected receptive ﬁlters (a) SAE (b) DpAE (c) NCAE, (d) L1/L2-NCSAE using NORB dataset.
 range weights are scaled [-1,1] mapped graycolor map.
 w <= −1 is assigned black, w grey, w >= is assigned white color.
 (d) L1/L2-NCSAE (a) (b) (c) Fig.
 distribution encoding (W(1)) decoding ﬁlters (W(2)) weights learned NORB dataset using (a) DpAE (b) NCAE (c) L1/L2-NCSAE.
 learned L1/L2-NCSAE captured real actual edges toys edges captured NCAE are fuzzy, -0.15-0.1-0.0500.050.10.15010002000numberµ=-0.0027**-0.15-0.1-0.0500.050.10.15010002000numberµ=-0.0024**-0.6-0.4-0.200.20.40.60.801000200030004000numberAvg(W -0.0026*-0.6-0.4-0.200.20.40.60.8020004000numberAvg(W 2(i,j))=0.0826**-0.500.510500010000numberAvg(W 0.0017-0.500.510500010000numberAvg(W Fig.
 Visualizing representations subset Reuters Documents data using (a) DpAE, (b) NCAE, (c) L1/L2- NCSAE.
 (a) (b) Fig.
 Deep network trained Reuters-21578 data using (a) DpAE, (b) L1/L2-NCSAE.
 area square is proportional weight’s magnitude.
 range weights are scaled [-1,1] mapped graycolor map.
 w = −1 is assigned black, w grey, w = is assigned white color.
 learned DpAE SAE are holistic.
 shown weight distribution depicted Fig.
 L1/L2-NCSAE has encoding decoding weights centered zero most weights positive compared DpAE NCAE have weights distributed sides origin.
 B.
 Unsupervised Semantic Feature Learning Text Data experiment DpAE, NCAE, L1/L2-NCSAE are evaluated compared based ability extract semantic features text data, are able discover underlined structure text data.
 purpose, Reuters-21578 text categorization dataset features is utilized train types AEs hidden
 subset examples belonging categories ”grain”, ”crude”, ”money-fx” was extracted test set.
 experiments were run times, averaged recorded.
 Fig.
 20-dimensional representations Reuters data subset using DpAE, NCAE, L1/L2-NCSAE are visualized.
 be observed L1/L2-NCSAE is able disentangle documents distinct categories more linear manifolds NCAE.
 addition, L1/L2- NCSAE is able group documents are semantic space same categories DpAE ﬁnds difﬁcult group documents distinct categories less overlap.
 TABLE I: Classiﬁcation accuracy MNIST NORB dataset Dataset MNIST NORB SAE NCAE NNSAE L1/L2-NCSAE DAE (50% input dropout) DpAE (50% hidden dropout) AAE SAE NCAE NNSAE L1/L2-NCSAE DAE (50% input dropout) DpAE (50% hidden dropout) ﬁne-tuning Mean (± SD) p-value ± <0.001 (±0.0085) (±0.027) <0.0001 (±0.0077) (±0.011) (±0.0021) <0.0001 ± <0.0001 (±0.021) (±0.025) <0.0001 (±0.0084) (±0.0019) (±0.0013) <0.0001 AAE ﬁne-tuning Mean (± SD) p-value ± <0.001 (±0.0012) (±0.001) <0.0001 (±0.0087) (±0.0021) (±0.0017) <0.0001 (±0.0016) <0.0001 ± (±0.0095) (± <0.001 (±0.0001) (±0.0015) (±0.0038) <0.0001 (±0.041) <0.0001 Epochs C.
 Supervised Learning last set deep network was con- structed using stacked L1/L2-NCSAE softmax layer classiﬁcation test enhanced ability network shatter data parts lead improved classiﬁcation.
 Eventually, entire deep network is ﬁne-tuned improve accuracy classiﬁcation.
 set experiments, performance pre-training deep network L1/L2- NCSAE is compared pre-trained recent AE architectures.
 MNIST NORB data sets were utilized, run experiments is repeated ten times averaged combat effect random initialization.
 classiﬁcation accuracy deep network pre-trained NNSAE [18], DpAE [19], DAE [32], AAE [22], NCAE, L1/L2-NCSAE using MNIST NORB data are detailed Table I.
 network architectures are 784- 196-20-10 1024-200-20-5 MNIST NORB dataset respectively.
 is remarked training AAE layers hidden units encoder, decoder, discriminator, other hyperparameters tuned described [22], accuracy was
 AAE reported Table used encoder, decoder, discriminator layers hidden units trained epochs.
 classiﬁcation accuracy speed convergence are ﬁgures merit used benchmark L1/L2-NCSAE other AEs. is observed result L1/L2-NCSAE-based deep network gives improved accuracy ﬁne-tuning compared methods such NNSAE, NCAE, DpAE, NCAE.
 However, performance terms classiﬁcation accuracy ﬁne-tuning is competitive.
 fact, be inferred p-value experiments conducted MNIST NORB Table is signiﬁcant difference accuracy ﬁne-tuning NCAE L1/L2-NCSAE most weights L1/L2-NCSAE are nonnegativity constrained.
 is remarked interpretability deep network has been fostered constraining most weights be nonnegative sparse, nothing signiﬁcant has been lost terms accuracy.
 addition, network trained L1/L2-NCSAE was observed converge faster counterparts.
 other hand, NNSAE has nonnegative weights deterioration accuracy, is con- spicuous ﬁne-tuning stage.
 improved accuracy ﬁne-tuning L1/L2-NCSAE based network be traced ability decompose data distinguishable parts.
 performance L1/L2- NCSAE ﬁne-tuning is similar DAE NCAE better NNSAE, DpAE, AAE, L1/L2-NCSAE constrains most weights be nonnegative sparse foster transparency other AEs. However, DpAE NCAE performed accurate L1/L2-NCSAE NORB network ﬁne-tuning.
 light constructing interpretable deep network, L1/L2-NCSAE pre-trained deep network hidden neurons ﬁrst AE layer, hidden neurons second AE, output neurons (one category) softmax layer was constructed.
 was trained Reuters data, compared pre-trained using DpAE.
 interpretation encoding layer ﬁrst AE is provided listing words associated strongest weights, interpretation encoding layer second AE is portrayed images characterized magnitude sign weights.
 Compared AE weights signs shown Fig.
 Fig.
 allows much better insight categorization topics.
 Topic earn output weight matrix resonates 5th hidden neuron most, lesser
 resonance happen hidden neuron reacts input words columns addition, lesser degree, hidden neuron reacts input words column words.
 So, tandem, dominant columns are sets words trigger category earn.
 Analysis term words topic acq leads similar conclusion.
 topic resonates dominant hidden neurons neuron
 neurons are driven columns words
 difference categories is lesser degree, category acq is inﬂuenced column words.
 interesting point is contribution column words.
 column connects hidden neuron weights neuron output layer are smaller signiﬁcant other ﬁve neurons (or rows) output weight matrix.
 column is least relevance topical categorization.
 D.
 Experiment Running Times training time networks non- negativity constraints was compared.
 constrained network converges requires lesser number training epochs.
 unconstrained network requires more time epoch constrained one.
 running time experiments were performed using full MNIST benchmark dataset In- tel(r) Core(TM) i7-6700 CPU @ RAM running 64-bit Windows Enterprise edition.
 software implementation has been MATLAB batch Gradient Descent method, LBFGS minFunc [33]) is used minimize objective function.
 usage times constrained unconstrained networks were compared.
 consider usage time milliseconds time elapsed ms trained deep network requires classify test samples.
 unconstrained network took ms epoch training phase constrained counterpart took ms.
 unconstrained network required ms usage time, network nonnegative weights took ms.
 above observations, is remarked nonnegativity constraint simpliﬁes resulting network.
 V.
 CONCLUSION paper addresses concept properties special regularization DL AE takes advantage non-negative encodings same time special regularization.
 has been shown using L1 L2 penalize negative weights, most are forced be nonnegative sparse, network interpretability is enhanced.
 fact, is observed most weights Softmax layer become nonnegative sparse.
 sum, has been observed encouraging nonnegativity NCAE-based deep architecture forces layers learn part-based repre- sentation input leads comparable classiﬁcation accuracy ﬁne-tuning entire deep network not- so-signiﬁcant accuracy deterioration ﬁne-tuning.
 has been shown select examples concurrent L1 L2 regularization improve network interpretability.
 performance proposed method was compared terms sparsity, reconstruction error, classiﬁcation accuracy conventional SAE NCAE, utilized MNIST handwritten digits, Reuters documents, NORB dataset illustrate proposed concepts.
 REFERENCES [1] Y.
 Bengio Y.
 LeCun, “Scaling learning algorithms towards Large-Scale Kernel Machines, vol.
 no.
 pp.

 [2] Y.
 Bengio, “Learning deep architectures ai,” Foundations trends® Machine Learning, vol.
 no.
 pp.

 [3] G.
 Hinton R.
 Salakhutdinov, “Reducing dimensionality data neural networks,” Science, vol.
 no.
 pp.

 [4] L.
 Deng, “A tutorial survey architectures, algorithms, applications deep learning,” APSIPA Transactions Signal Information Processing, vol.
 p.

 [5] S.
 Bengio, L.
 Deng, H.
 Larochelle, H.
 Lee, R.
 Salakhutdinov, “Guest editors introduction: Special section learning deep architec- tures,” IEEE Transactions Pattern Analysis Machine Intelligence, vol.
 no.
 pp.

 [6] Y.
 Bengio, P.
 Lamblin, D.
 Popovici, H.
 Larochelle, “Greedy layer- wise training deep networks,” Advances Neural Information Processing Systems, vol.
 p.

 [7] B.
 Ayinde J.
 Zurada, “Clustering receptive ﬁelds autoencoders,” Neural Networks (IJCNN), International Joint Conference on.
 IEEE, pp.

 [8] D.
 D.
 Lee H.
 S.
 Seung, “Learning parts objects non- negative matrix factorization,” Nature, vol.
 no.
 pp.

 [9] J.
 Chorowski J.
 M.
 Zurada, “Learning understandable neural networks nonnegative weight constraints,” Neural Networks Learning Systems, IEEE Transactions vol.
 no.
 pp.

 [10] B.
 A.
 Olshausen D.
 J.
 Field, “Emergence simple-cell receptive ﬁeld properties learning sparse code natural images,” Nature, vol.
 no.
 pp.

 [11] E.
 Hosseini-Asl, J.
 M.
 Zurada, O.
 Nasraoui, “Deep learning part-based representation data using sparse autoencoders non- negativity constraints,” Neural Networks Learning Systems, IEEE Transactions vol.
 no.
 pp.

 [12] M.
 Ranzato, Y.
 Boureau, Y.
 LeCun, “Sparse feature learning deep belief networks,” Advances Neural Information Processing Systems, vol.
 pp.

 [13] M.
 Ishikawa, “Structural learning forgetting,” Neural Networks, vol.
 no.
 pp.

 [14] P.
 L.
 Bartlett, “The sample complexity pattern classiﬁcation neural size weights is important size network,” Information Theory, IEEE Transactions vol.
 no.
 pp.

 [15] G.
 Gnecco M.
 Sanguineti, “Regularization techniques subopti- mal solutions optimization problems learning data,” Neural Computation, vol.
 no.
 pp.

 [16] J.
 Moody, S.
 Hanson, A.
 Krogh, J.
 A.
 Hertz, “A simple weight decay improve generalization,” Advances Neural Information Processing Systems, vol.
 pp.

 [17] O.
 E.
 Ogundijo, A.
 Elmas, X.
 Wang, “Reverse engineering gene regulatory networks measurement missing values,” EURASIP Journal Bioinformatics Systems Biology, vol.
 no.
 p.

 [18] A.
 Lemme, R.
 Reinhart, J.
 Steil, “Online learning generalization parts-based image representations non-negative sparse autoen- coders,” Neural Networks, vol.
 pp.

 [19] G.
 E.
 Hinton, N.
 Srivastava, A.
 Krizhevsky, I.
 Sutskever, R.
 R.
 Salakhutdinov, “Improving neural networks preventing co-adaptation feature detectors,” arXiv preprint arXiv:1207.0580,
 [20] N.
 Srivastava, G.
 Hinton, A.
 Krizhevsky, I.
 Sutskever, R.
 Salakhut- dinov, “Dropout: A simple way prevent neural networks over- Journal Machine Learning Research, vol.
 no.
 pp.

 [21] D.
 P.
 Kingma M.
 Welling, “Auto-encoding variational bayes,” arXiv preprint arXiv:1312.6114,
 [22] A.
 Makhzani, J.
 Shlens, N.
 Jaitly, I.
 Goodfellow, “Adversarial autoencoders,” arXiv preprint arXiv:1511.05644,
 [23] Y.
 Burda, R.
 Grosse, R.
 Salakhutdinov, “Importance weighted autoencoders,” arXiv preprint arXiv:1509.00519,
 [24] B.
 O.
 Ayinde, E.
 Hosseini-Asl, J.
 M.
 Zurada, “Visualizing understanding nonnegativity constrained sparse autoencoder deep learning,” Rutkowski L., Korytkowski M., Scherer R., Tadeusiewicz R., Zadeh L., Zurada J.
 (eds) Artiﬁcial Intelligence Soft Computing.
 ICAISC
 Lecture Notes Computer Science, vol
 Springer, pp.

 [25] S.
 J.
 Wright J.
 Nocedal, Numerical optimization.
 vol.

 Springer New [26] T.
 D.
 Nguyen, T.
 Tran, D.
 Phung, S.
 Venkatesh, “Learning parts- based representations nonnegative restricted boltzmann machine,” Asian Conference Machine Learning, pp.

 [27] A.
 Ng, “Sparse autoencoder,” CS294A Lecture notes.
 URL https: //web.stanford.edu/class/cs294a/sparseAutoencoder Stan- ford University,
 [28] Y.
 LeCun, L.
 Bottou, Y.
 Bengio, P.
 Haffner, “Gradient-based learning applied document recognition,” Proceedings IEEE, vol.
 no.
 pp.

 [29] Y.
 LeCun, F.
 J.
 Huang, L.
 Bottou, “Learning methods generic object recognition invariance pose lighting,” Computer Vision Pattern Recognition,
 CVPR
 Proceedings IEEE Computer Society Conference on, vol.

 IEEE, pp.
 II–97.
 P.-N.
 Tan, M.
 Steinbach, V.
 Kumar al., Introduction data mining.
 Pearson Addison Wesley Boston, vol.

 [31] L.
 V.
 der Maaten G.
 Hinton, “Visualizing data using t-sne,” Journal Machine Learning Research, vol.
 no.

 [32] P.
 Vincent, H.
 Larochelle, Y.
 Bengio, P.
 Manzagol, “Extracting composing robust features denoising autoencoders,” International Conference Machine Learning.
 pp.

 [33] R.
 H.
 Byrd, P.
 Lu, J.
 Nocedal, C.
 Zhu, “A limited memory algo- rithm bound constrained optimization,” SIAM Journal Scientiﬁc Computing, vol.
 no.
 pp.

 Babajide Ayinde (S’09) received M.Sc. de- gree Engineering Systems Control King Fahd University Petroleum Minerals, Dhahran, Saudi Arabia.
 is Ph.D. student University Louisville, Kentucky, USA recipient University Louisville fellowship.
 current research interests include unsupervised feature learning deep learning techniques applications.
 Jacek M.
 Zurada (M’82-SM’83-F’96-LF’14) Ph.D., has received degrees Gdansk In- stitute Technology, Poland.
 serves Professor Electrical Computer Engineering University Louisville, KY.
 authored co- authored several books papers com- putational intelligence, neural networks, machine learning, logic rule extraction, bioinformatics, delivered presentations world.
 served IEEE V-President, Technical Activities (TAB Chair).
 chaired IEEE TAB Periodicals Committee, TAB Periodicals Review Advisory Committee was Editor- in-Chief IEEE Transactions Neural Networks (1997-03), Associate Editor IEEE Transactions Circuits Systems, Neural Networks Proceedings IEEE.
 2004-05, was President IEEE Computational Intelligence Society.
 Dr. Zurada is Associate Editor Neurocomputing, several other journals.
 has been awarded numerous distinctions, including Joe Desch Innovation Award, Distinguished Service Award, ﬁve honorary professorships.
 has been Board Member IEEE, IEEE CIS IJCNN.
 important problem database research is determining combine multiple data sources are described diﬀerent (heterogeneous) schemata [6].
 outcome such process is expected be uniform integrated view data sources.
 Relational data sources are popular ways store enterprise Web data [20].
 However, relational schema lacks well-deﬁned semantic description.
 deﬁne semantics data, introduce ontology [20].
 goal is map attributes relational data sources classes properties ontology.
 refer problem semantic labeling.
 Semantic labeling plays important role data integration [6, augmenting ex- isting knowledge bases [9, mapping relational sources ontologies
 Various approaches automate semantic labeling have been developed, including DSL [14] T2K [17].
 automated semantic labeling techniques several prob- lems.
 be naming conﬂicts [15], including cases users represent ∗ Work accomplished Data61, CSIRO.
 licensed Creative Commons License CC-BY Leibniz International Proceedings Informatics Schloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, Germany XX:2 Semantic labeling same data diﬀerent ways.
 Secondly, diﬀerent attributes have syn- similar content, example, birth date versus date death.
 Thirdly, are considerable number attributes do have corresponding property ontology, accident purpose.
 majority existing systems focus ﬁrst problems, do consider third problem evaluation
 address challenges automated semantic labeling, formulate task supervised classiﬁcation problem.
 set semantic labels known classiﬁer is speciﬁed training time, e.g., provided domain ontology.
 introduce special class attributes, called unknown.
 purpose unknown class is capture attributes be mapped ontology.
 training data classiﬁer consist source attributes (name content) semantic labels provided user, including unknown labels.
 assigning labels attributes is costly operation, lack training data is common problem semantic labeling systems.
 Existing systems [14, use knowledge transfer techniques overcome issue.
 Instead, introduce sampling method similar bagging ensemble models [3].
 bagging technique allows generate multiple training instances user- labeled attributes, overcoming lack labeled training data.
 allows overcome common issue class imbalance, semantic labels have more support others attributes.
 achieve re-balancing training data preferential bagging minority class attributes.
 main contributions paper are:
 introduce bagging approach handle class imbalance lack training data drawing random subsamples values attribute.
 approach achieve meaningful diversity training data increase number training instances under-represented semantic labels.

 address issue “unwanted” attributes, i.e., attributes do get mapped element ontology.
 cases have suﬃcient amount training models achieve Mean Reciprocal Rank (MRR) sets data sources benchmark.

 construct classiﬁcation model DINT hand-engineered semantic labeling fea- tures implement above.
 addition, design deep learning models CNN MLP use simple features, such normalized character frequencies padded character sequences extracted raw values data attributes.

 construct benchmark common evaluation strategy compare diﬀerent approaches supervised semantic labeling.
 benchmark includes such models DINT, CNN, MLP state-of-the-art DSL [14], sets data sources diﬀerent domains.
 show approach has strengths shortcomings, choosing particular semantic labeling system depends use case.
 have released implementation benchmark open source license
 benchmark be extended include other models datasets, be used choose appropriate model given use case.
 Problem illustrate semantic labeling problem using simple domain ontology shown Fig.

 have data sources “personal-info”, “businessInfo” “Employees” (see http://github.com/NICTA/serene-benchmark N.
 Rümmele, Y.
 Tyshetskiy, A.
 Collins XX:3 Figure Example ontology.
 Tab.
 attributes choose label according example ontology (Fig.

 deﬁne semantic label tuple consisting domain class property.
 example, attribute name source “personal-info” (see Tab.
 is labeled (Person,name).
 Note semantic labels are ﬁxed ontology.
 task semantic labeling is deﬁned assigning semantic labels attributes data source.
 case supervised semantic labeling, use existing known semantic labels data sources improve performance assigning semantic labels new sources.
 example, assume are given sources “personal-info” “businessInfo” correct semantic system assign labels attributes source “Employees”.
 build system, cannot names columns.
 example, columns name (1a), ceo (1c) employee (1b) refer same property (Person,name).
 Using values columns is problematic.
 example, (1a) acronyms are used states, (1c) state names are written.
 Furthermore, values overlap heterogeneous columns founded (1c) birthDate (1a).
 have attributes are mapped property ontology.
 be reasons existence: are interested content attribute want discard future analysis; have overlooked attribute designing ontology accurately.
 do diﬀerentiate cases mark such attributes unknown class, example, founded (1c).
 presence unknown class makes task semantic labeling more complicated.
 Estab- lishing approaches handle such attributes is crucial many real-world scenarios relational data sources HTML tables [17] domain speciﬁc data contain considerable number such attributes.
 Machine learning techniques proved be eﬃcient building predictive models noisy messy data.
 apply techniques need represent source attributes feature vectors, semantic labels (classes) attached vectors.
 Table show such representation source Employees.
 have shown possible features, simplicity.
 However, actual size feature vector be arbitrary long, process designing components is known feature engineering.
 next CityPlacesubclassStatestatepostalCodenamesubclassPersonlivesInbornInOrganizationworksForbirthDatenameoperatesInceoemailphonename XX:4 Semantic labeling Table Example relational data sources semantic labels.
 name Neil Mary Henry (Person, name) city birthDate 21-05-1916 Waterloo Eveleigh 07-12-1990 Redfern (Person, (City, birthDate) state NSW NSW NSW (State, name) workplace CSIRO CSIRO Data61 (Organization, Semantic labels (a) personal-info employer employee DOB CSIRO Data61 NICTA (Organization, Neil Mary Henry (Person, name) (Person, birthDate) (b) Employees company CSIRO Data61 NICTA ceo state Larry Marshall Australian Capital Territory Adrian Turner Hugh Durrant New South Wales New South Wales Semantic labels (Organization, (c) businessInfo (Person,name) (State,name) founded 21-05-1916 12-07-2016 15-03-2002 unknown section discuss features used semantic labeling system.
 Approaches section describe classiﬁers semantic labeling problem used evaluation.
 discuss approaches problem unknown attributes lack training data.
 have set labeled data sources, construct feature vectors attributes set mark representatives class corresponding semantic labels.
 constructed set (feature vector, class label) pairs is used train classiﬁer.
 consider several approaches, divided major groups: DINT, Deep Learning state-of-the-art DSL.
 approach trains multi-class classiﬁcation model produces, prediction stage, list class probabilities attribute new source.
 class highest predicted probability is assigned attribute decision stage.
 DINT ﬁrst approach DINT (Data INTegrator) hand-engineer features, in- clude characteristics such number whitespaces other special characters, statistics values column (e.g, mean/ max/ min string length numeric statistics) many more.
 complete list features is available open source benchmark repos- N.
 Rümmele, Y.
 Tyshetskiy, A.
 Collins XX:5 Table Feature vectors data source Employees.
 feature vector attribute entropy mean string length employer employee DOB ratio alpha chars ratio unique values class (Organization, name) (Person, (Person, birthDate)











 entropy [11]) string X is deﬁned H(X) = −P itory
 important features characterising information content attribute is Shannon’s entropy attribute’s concatenated rows.
 Shannon’s entropy (or information i pi log2 pi, pi is probability character, index character vocabulary is i, appear X, summation ranges characters vocabulary.
 evaluate pi Shannon’s entropy, evalu- ate normalized character frequency distribution chardist attribute, character counts concatenated rows attribute, normalized total length concatenated rows.
 vocabulary characters consists printable characters (including \n).
 Finally, add 100-dimensional vector pi attribute feature vector.
 addition above features, be calculated attribute val- ues, compute mean cosine similarity attribute character distribution character distributions class instances.
 adds many additional scalar features full attribute feature vector are classes training data.
 expect names attributes contain useful information determine semantic types, addition information provided attribute values.
 extract features attribute names, compute string similarity metrics: minimum edit distance, WordNet based similarity measures such JCN [7] LIN [10], k-nearest neighbors using Needle-Wunsch distance [13].
 minimum edit distance strings s1 s2 is minimum number edit operations, such insertion, deletion, substitution, are required transform string [11].
 compute similarity attribute name class instances training data.
 number extracted features depends number semantic labels training data.
 choose train Random Forest [4] (RF) set features.
 RF is robust noisy data, works correlated features, captures complex nonlinear relationships features target.
 Additionally, RF classiﬁers require little hyperparameter tuning, hence work straight “out box”, makes convenient versatile classiﬁer use.
 Deep Learning Deep learning has gained much popularity due tremendous impact such areas speech recognition, object recognition, machine translation [8].
 biggest advantages deep learning is ability process data raw form discover representation needed classiﬁcation, assisting feature engineering step.
 speaking, deep learning is overarching term artiﬁcial neural networks, word “deep” refers depth network.
 basic level neural networks are composed perceptrons, neural nodes.
 be several layers interconnected serene-benchmark XX:6 Semantic labeling neural ﬁrst layer is input layer last one is output layer.
 layers are called hidden.
 Neural nodes layer take input output nodes previous layer, perform computation nonlinear activation function (e.g., tanh RELU) pass result next layer.
 are connections nodes same layer.
 Overall, deep learning models improve performance more data are trained
 exact architecture deep learning models, i.e., number layers, number nodes layer, activation functions neurons interconnectedness layers, inﬂuence performance trained models.
 choose diﬀerent architectures deep learning classiﬁers: (i) Multi-Layer Perceptron (MLP) [19] (ii) Convolutional Neural Network (CNN) [8].
 have experi- mented diﬀerent designs MLP CNN varying hyperparameters control number hidden layers, numbers nodes/ﬁlters layer, dropout probability, etc., found designs, described brieﬂy below, work datasets benchmark.
 input layer MLP architecture takes 101-dimensional feature vector character frequencies pi (chardist) Shannon entropy.
 Following input layer, MLP has connected hidden layers nodes layer, tanh activations.
 hidden layer, introduced stochastic dropout layer dropout probability prevent overﬁtting.
 output layer MLP (the actual classiﬁer) is softmax layer number nodes equal number semantic types (including ‘unknown’ type).
 CNN model takes input one-hot representation attribute’s concatenated rows character space, embeds dense 64-bit embedding, passes embedded "image" attribute consecutive convolution layers ﬁlters layers, followed 1-d max-pooling layer, ﬂattening layer, dropout layer probability dropout connected layer nodes, connected softmax output layer (the classiﬁer) number nodes equal number semantic types (including ‘unknown’ type).
 cannot be sure ﬁnal choice architectures is optimal, seems be good trade-oﬀ complexity models, required computational resources training, overall performance semantic labeling task.
 have implemented models using Keras library GPU-based TensorFlow backend [1].
 DSL Domain-independent Semantic Labeler (DSL) has been proposed Pham al [14], feature groups based similarity metrics are constructed.
 metrics measure attribute names values are similar characteristics other attributes.
 means given attributes training data (i.e., labeled instances) distinct semantic labels, new attribute be compared representatives semantic label features be calculated total.
 considered similarity metrics are: attribute name similarity, standard Jaccard similarity textual data modiﬁed version numerical data, TF-IDF cosine similarity, distribution histogram similarity.
 building multi-class classiﬁer, authors train binary classiﬁers separ- semantic label.
 binary classiﬁer particular semantic label is Logistic Regression model trained set similarity metrics representatives label.
 predicting semantic labels new attribute, combine predictions classiﬁer produce ﬁnal vector probabilities.
 distinctive properties N.
 Rümmele, Y.
 Tyshetskiy, A.
 Collins XX:7 approach is ability transfer classiﬁcation model trained domain predict- ing semantic labels attributes domain.
 denote enhanced approach DSL+.
 Bagging train classiﬁer semantic labeling, need data sources have many labeled at- tributes.
 However, costly operation assigning labels attributes, relative small number columns compared data set size, implies lack training data is common problem semantic labeling systems.
 Existing systems [14, use knowledge transfer techniques overcome issue.
 introduce method increasing training sample size based machine learning approach known bagging [3].
 Breiman [3] introduced concept bootstrap aggregating, known bagging, construct ensembles models improve prediction accuracy.
 method consists training diﬀerent classiﬁers bootstrapped replicas original dataset.
 Hence, diversity is obtained resampling procedure usage diﬀerent data subsets.
 prediction stage individual classiﬁer estimates unknown instance, majority weighted vote is used infer class.
 modify idea bagging problem.
 is clear semantics columns table “Employees” (Table change have rows.
 create several training instances attribute, instance (called bag) contain random sample (with replacement) content.
 procedure is governed parameters numBags bagSize: ﬁrst parameter controls many bags are generated attribute, latter indicates many rows are sampled bag.
 way address issue noise increasing diversity training data issue insuﬃcient training data.
 common problem encountered wide range data mining machine learning initiatives is class imbalance.
 Class imbalance occurs class instances dataset are represented.
 such situation, building standard machine learning models lead poor results, favor classes large populations classes small populations.
 address issue, have tried several resampling strategies equalize number instances class.
 Unknown class mentioned attributes are mapped property ontology.
 handle issue, introduce more class called unknown.
 example, attributes get discarded integration process be marked unknown.
 way help classiﬁer recognize such attributes new sources.
 addition, is advantage having unknown class deﬁned explicitly.
 Consider new attribute unseen semantic label, is, label is present training data.
 picking closest match known semantic labels, classiﬁer mark unknown.
 user need validate attributes are classiﬁed unknown.
 ensure unknown class consists unwanted attributes.
 do introduce class diﬀerentiate unwanted attributes unseen labels cannot guarantee is overlap them.
 DINT Deep Learning approaches support unknown class.
 XX:8 Semantic labeling Experiments have run experiments Dell server GiB memory, CPUs cores each, Titan GPU GeForce Ti GPU.
 deep learning models have been optimized GPUs using Tensorﬂow.
 benchmark semantic labeling system is implemented Python is available open source license
 Datasets use diﬀerent sets data sources evaluation, labeled as: museum, city, weather, soccer [14] weapons [21].
 set corresponds domain speciﬁc set semantic labels.
 Descriptive statistics domain set are shown Table
 see, museum soccer domains are only domains have unknown attributes.
 city domain has many semantic labels attributes museum domain contains more data sources.
 Table Description data sources.
 Domain weather weapons museum soccer city sources semantic labels attributes unknown attributes rows source avg attributes source estimate class imbalance domain, plot class distribution Figure
 museum domain has highest imbalance classes, soccer weapons domains have imbalanced classes, weather city domains have represented classes.
 Experimental setting establish common evaluation framework approaches described Section
 performance metric use Mean Reciprocal Rank (MRR) [5].
 derive comprehens- ive estimate performance domains, implement cross-validation techniques: leave out repeated holdout.
 leave out strategy is deﬁned using source testing sample rest sources domain training samples.
 procedure is repeated many times are sources domain.
 calculate MRR testing sample report average MRR ﬁnal performance metric iteration.
 example, domain museum obtain models total model is trained diﬀerent sources, MRR is calculated prediction outcome single source.
 strategy allows estimate performance diﬀerent models given are enough instances semantic label.
 http://github.com/NICTA/serene-benchmark N.
 Rümmele, Y.
 Tyshetskiy, A.
 Collins XX:9 numeric identiﬁers semantic labels numeric identiﬁers semantic labels numeric identiﬁers semantic labels (a) weather (b) weapons (c) soccer numeric identiﬁers semantic labels numeric identiﬁers semantic labels (d) museum (e) city Figure Distribution attributes according semantic labels, including unknown class, diﬀerent domains.
 see class imbalance museum, soccer weapons domains.
 x-axis have semantic labels sorted number attributes class.
 y-axis shows number attributes.
 repeated holdout strategy, randomly sample ratio p sources place training sample use remaining sources testing sample, procedure is repeated n times.
 ﬁnal MRR score is average MRR scores iteration.
 use technique simulate scenario is shortage labeled sources.
 set ratio p number iterations n
 Results section report results experiments.
 total evaluate models, report run required train considered models.
 train MLP CNN models, need many training instances, use bagging (presented Section parameters numBags=150 bagSize=100 increase size initial training set.
 train semantic labeling system DINT diﬀerent sampling strategies.
 particular, report results apply resampling bagging parameters bagSize=100 numBags=100.
 experiment various class imbalance resampling strategies, including resampling mean maximum instance counts class.
 brevity loss generality report results resampling mean strategy denoted ResampleToMean.
 design DSL DSL+ use resampling.
 mentioned Section DINT model is built set engineered features.
 MLP model, other hand, uses chardist entropy.
 compare performance MLP DINT, create new model DINT base reduce XX:10 Semantic labeling Table MRR scores leave strategy unknown attributes are considered.
 Model DSL city DINT DINT base DINT base+ MLP CNN DINT DINT base DINT base+ DINT base DINT base+ museum soccer weapons weather Resample DINT Mean Table Model training times (s) leave out.
 Sampling None Bagging Sampling None Bagging Model DSL city museum soccer weapons weather DINT DINT base DINT base+ MLP CNN DINT DINT base DINT base+ DINT base DINT base+ Resample DINT Mean number features chardist entropy.
 addition, create model DINT base+ using chardist entropy add feature minimum edit distance.
 choose feature feature importance scores produced random forest algorithm rank edit distance higher other features extracted names.
 Table reports MRR scores leave strategy.
 Surprisingly, models built normalized character distributions attribute values perform many cases well.
 Deep learning models MLP CNN are comparable DINT models, come higher computational cost.
 Run times training model are shown Table
 see, DINT models use bagging sample more training instances achieve best results domains.
 Remarkably, are domains higher class imbalance variety data sources terms number rows number columns.
 Data sources city domains have same number attributes.
 have discovered bagging needs be performed training prediction stages achieve best performance.
 have observed setting makes noticeable N.
 Rümmele, Y.
 Tyshetskiy, A.
 Collins XX:11 diﬀerence domains number rows varies data sources.
 example, museum domain number rows ranges soccer domain range is
 terms computation best performing model DINT museum domain requires lot time training.
 expensive features are diﬀerent edit distances: minimum edit distance, JCN, LIN k-nearest neighbors.
 suggests DINT model possible features does scale increasing number attributes training set.
 Considering similarity metrics used other approaches DSL T2K [17], computing TF-IDF Jaccard’s scores help resolve runtime issue DINT all.
 class imbalance, ResampleToMean strategy improves performance DINT models sampling domains highest class imbalance (i.e., museum soccer), appears ResampleToMean strategy leads decreased performance domains prominent imbalance (i.e., weapons weather).
 leads idea class resampling strategy needs be improved.
 potential strategy be combining bagging resampling strategies.
 ﬁxing numBags attributes, parameter be changed be mean maximum instance counts class.
 way perform resampling strategy does produce replicas attributes.
 city weapons domains, designed models have similar performance DSL.
 However, computational complexity models varies.
 museum domain DINT base+ has higher MRR DSL, DINT base+ needs time less training.
 appears attributes contain mixture textual numeric are bottleneck DSL data sources city weapons domains have multiple mixed data columns.
 Table MRR scores repeated holdout strategy unknown attributes are considered.
 Sampling None Bagging Resample DINT Mean city Model DSL DSL+ DINT DINT base DINT base+ MLP CNN DINT DINT base DINT base+ DINT base DINT base+ museum soccer weapons weather cases are few labeled instances (repeated holdout strategy Table observe DSL performs well, DSL+, leverages labeled instances other domains.
 be aware scenario are many unseen labels, makes MRR ill-deﬁned.
 compare DINT models scenario, suggests bagging is advantageous situations are few labeled attributes.
 enhancing DINT model, uses simple features bagging, DSL+ know- XX:12 Semantic labeling Table Performance leave strategy unknown class is considered.
 MRR scores Sampling Model None Bagging DSL DINT DINT base DINT base+ MLP CNN DINT DINT base DINT base+ Resample DINT Mean DINT base DINT base+ Train time (s) museum soccer museum soccer ledge transfer capability result stable semantic labeling system.
 enhancement be introduce resampling strategies DSL system.
 addition, perform experiments domains museum soccer, unmapped attributes cause skewed class distributions.
 want establish diﬀerent approaches recognize such attributes.
 Tables see performance semantic labeling systems changes considerably.
 DSL DSL+ performance is aﬀected inability diﬀerentiate "unwanted" attributes.
 performing bagging attributes training data, introduce diversity drawing many samples attribute values.
 However, do apply perturbation technique names attributes use exact replicas.
 Table observe DINT base performs DINT base+ bagging is used.
 datasets scarce labeled instances DINT models tend overﬁt attribute names are present training data.
 suggests introducing technique similar bagging column headers lead better performance.
 other hand, results are consistent observations work Ritze al.[17].
 results indicate comparing attribute values is crucial task attribute names introduce additional noise.
 Clearly, performance approach DINT varies depending chosen bagging parameters numBags bagSize.
 explore dependence, evaluate performance DINT only chardist entropy features varying bagging parameters ﬁxing other one.
 report results evaluation Figure
 do consider unknown attributes choose repeated holdout strategy analyze behavior bagging is shortage training data.
 Interestingly, increasing values bagging parameters does lead improved performance, computational time required training prediction stages increases.
 city domain is sensitive bagging parameters.
 assume is city domain is only domain equal distribution semantic labels, equal numbers columns rows data sources.
 appears other domains, bagging makes models robust towards variance characteristics.
 N.
 Rümmele, Y.
 Tyshetskiy, A.
 Collins XX:13 Table Performance repeated holdout strategy unknown class is considered.
 MRR scores Sampling Model None Bagging DSL DSL+ DINT DINT base DINT base+ MLP CNN DINT DINT base DINT base+ Resample DINT Mean DINT base DINT base+ Train time (s) museum soccer museum soccer numBags weather weapons soccer museum city bagSize (a) ﬁxing bagSize=100 (b) ﬁxing numBags=50 Figure Dependence MRR scores DINT base bagging parameters using repeated holdout strategy.
 attributes are considered.
 Related Work problem semantic labeling, addressed work, be regarded problem schema matching ﬁeld data integration [2].
 schema matching problem match elements source target schemata.
 case elements source schema are attributes, want map attributes properties ontology.
 semantic labeling problem is known literature attribute-to-property matching [18,
 Indicating semantic correspondences be appropriate few data sources need be integrated, however, becomes tedious growing number heterogeneous schemata.
 Hence, automatic semi-automatic approaches schema matching are being developed.
 machine learning categorize approaches unsupervised techniques compute various similarity metrics supervised techniques build multi-class classiﬁcation model.
 Unsupervised approaches are used SemanticTyper [16], XX:14 Semantic labeling T2K [18] extended version [17].
 approaches authors design similarity metrics attribute names attribute values, substantial diﬀerence is additional knowledge is used computation.
 example, authors [18] [17] leverage contextual information DBpedia.
 supervised approaches, are probabilistic graphical models used work Limaye al.
 [9] annotate web tables entities cell values, types attributes relationships binary combinations attributes.
 Mulwad al.
 [12] extend approach leveraging information Wikitology Knowledge Base (KB).
 problem probabilistic graphical models is do scale number semantic labels domain.
 Also, Mulwad al.
 Venetis al.
 [23], used isA database KB, extract additional data knowledge bases assign semantic label attribute.
 Hence, approaches are limited domains represented knowledge bases.
 approach, other hand, is domain speciﬁc allows model be trained data.
 However, cannot apply model learnt domain another, is possible DSL approach [14].
 best knowledge, DSL introduced Pham al.[14] is top semantic labeling systems.
 Pham al.
 compare DSL previous approach Semantic- Typer [16] T2K system [18], achieve higher MRR scores variety datasets.
 Therefore, use DSL state-of-the art model benchmark evaluate new approaches.
 Ritze al.
 [17] Pham et al.
 [14] mention problem unknown class.
 ﬁrst work authors discuss "unwanted" attributes second work authors reﬂect handle "unseen" attributes.
 work do diﬀerentiate cases show identify such attributes suﬃcient training data is available.
 Conclusion paper have studied problem supervised semantic labeling have conduc- ted experiments evaluate diﬀerent approaches perform task.
 main ﬁnding is bagging sampling technique provide meaningful diversity training data improve performance.
 Additionally, technique overcome lack labeled attrib- utes domain increase number instances under-represented semantic labels.
 ﬁnd given scarce training data, bagging leads noticeable improvement performance, state-of-the-art system DSL [14] achieves better precision leveraging information labeled instances other domains.
 However, are consider unwanted attributes unseen semantic new system DINT demon- strates best performance.
 semantic labeling systems benchmark have observed performance results are dependent use case.
 have shown deep learning models, such CNN MLP, be applied solve problem.
 models do excel performance majority cases, advantage is simplicity features extracted attributes.
 example, CNN is built raw sequences attribute values.
 Surprisingly, have discovered random forests constructed character distributions values entropy attributes provide remarkable results many cases.
 supports observations literature attribute values are crucial semantic labeling task [18,
 Future work involve exploring combination bagging class imbalance res- ampling strategies.
 have observed domain data has high imbalance N.
 Rümmele, Y.
 Tyshetskiy, A.
 Collins XX:15 representatives diﬀerent semantic labels, resampling lead improved perform- ance sophisticated approach is required domains do exhibit characteristics.
 possible direction improvement is introduce equivalent bagging attribute names.
 addition, experiments indicate performance systems is aﬀected variance sizes data sources semantic label is represented training data.
 end, consider including T2KMatch [17] benchmark domain sets RODI benchmark [15].
 References Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, al.
 Tensorﬂow: system large-scale machine learning.
 Proc.
 OSDI, pages
 Zohra Bellahsene, Angela Bonifati, Erhard Rahm, editors.
 Schema Matching Mapping.
 Data-Centric Systems Applications.

 Leo Breiman.
 Bagging predictors.
 Machine learning,
 Leo Breiman.
 Random forests.
 Machine learning,
 Nick Craswell.
 Mean reciprocal rank.
 Encyclopedia Database Systems, pages AnHai Doan, Alon Y.
 Halevy, Zachary G.
 Ives.
 Principles Data Integration.
 Morgan Kaufmann,
 Jay J Jiang David W Conrath.
 Semantic similarity based corpus statistics lexical taxonomy.
 arXiv preprint cmp-lg/9709008,
 Yann LeCun, Yoshua Bengio, Geoﬀrey Hinton.
 Deep learning.
 Nature,


 Girija Limaye, Sunita Sarawagi, Soumen Chakrabarti.
 Annotating searching web tables using entities, types relationships.
 Proc.
 VLDB Endowment, 3(1-2):1338–
 Dekang Lin al.
 information-theoretic deﬁnition similarity.
 Proc.
 ICML, Christopher D Manning, Prabhakar Raghavan, Hinrich Schütze, al.
 Introduction volume pages
 information retrieval, volume

 Varish Mulwad, Tim Finin, Anupam Joshi.
 Semantic message passing generating linked data tables.
 Proc.
 ISWC, pages
 Saul B Needleman Christian D Wunsch.
 general method applicable search similarities amino acid sequence proteins.
 Journal molecular biology,
 Minh Pham, Suresh Alse, Craig A Knoblock, Pedro Szekely.
 Semantic labeling: domain-independent approach.
 Proc.
 ISWC, pages

 Christoph Pinkel, Carsten Binnig, Ernesto Jiménez-Ruiz, Evgeny Kharlamov, Wolfgang May, al.
 RODI: Benchmarking relational-to-ontology mapping generation quality.
 Se- mantic Web, (Preprint):1–28,
 SK Ramnandan, Amol Mittal, Craig A Knoblock, Pedro Szekely.
 Assigning semantic labels data sources.
 Proc.
 ESWC, pages
 Dominique Ritze Christian Bizer.
 Matching web tables dbpedia feature utility study.
 Proc.
 EDBT, pages
 Dominique Ritze, Oliver Lehmberg, Christian Bizer.
 Matching HTML tables DB- pedia.
 Proc.
 WIMS, page
 David E Rumelhart, Geoﬀrey E Hinton, Ronald J Williams.
 Learning internal repres- entations error propagation.
 Technical report, DTIC Document,
 XX:16 Semantic labeling Dimitrios-Emmanuel Spanos, Periklis Stavrou, Nikolas Mitrou.
 Bringing relational databases semantic web: survey.
 Semantic Web,
 Mohsen Taheriyan, Craig A Knoblock, Pedro Szekely, José Luis Ambite.
 Leveraging linked data discover semantic relations data sources.
 Proc.
 ISWC, pages

 Mohsen Taheriyan, Craig A.
 Knoblock, Pedro A.
 Szekely, José Luis Ambite.
 Learning semantics structured data sources.
 J.
 Web Sem.,
 Petros Venetis, Alon Halevy, Jayant Madhavan, Marius Paşca, Warren Shen, Fei Wu, Gengxin Miao, Chung Wu. Recovering semantics tables web.
 Proc.
 VLDB Endowment,
 Recent studies have highlighted lack robustness state-of-the-art neural network models, e.g., imperceptible adversarial image be crafted mislead well-trained network (Szegedy Goodfellow et Chen
 researchers have identiﬁed adversarial examples are valid digital space plausible physical world (Kurakin Evtimov
 vulnerability adversarial examples calls question safety-critical applications services deployed neural networks, including autonomous driving systems malware detection protocols, others.
 literature, studying adversarial examples neural networks has twofold purposes: (i) se- curity implications: devising effective attack algorithms crafting adversarial examples, (ii) robustness analysis: evaluating intrinsic model robustness adversarial perturbations normal examples.
 principle means tackling problems are expected be inde- pendent, is, evaluation neural network’s intrinsic robustness be agnostic attack methods, vice versa, existing approaches use different attack results measure robustness target neural network.
 Speciﬁcally, given set normal examples, attack success rate distortion corresponding adversarial examples crafted particular at- tack algorithm are treated robustness metrics.
 Consequently, network robustness is entangled attack algorithms used evaluation analysis is limited attack capabilities.
 importantly, dependency robustness evaluation attack approaches cause ∗Tsui-Wei Weng Huan Zhang contributed Published conference paper ICLR biased analysis.
 example, adversarial training is used technique improving robustness neural network, accomplished generating adversarial examples retraining network corrected labels.
 However, such trained network is made robust attacks used craft adversarial examples training, be vulnerable unseen attacks.
 Motivated evaluation criterion assessing quality text image generation is independent underlying generative processes, such BLEU score texts (Papineni al., INCEPTION score images (Salimans et aim propose comprehensive attack-agnostic robustness metric neural networks.
 Stemming perturbation analysis arbitrary neural network classiﬁer, derive universal lower bound minimal distortion required craft adversarial example original one, lower bound applies attack algorithm (cid:96)p norm p
 show lower bound associates maximum norm local gradients respect original ex- ample, robustness evaluation becomes local Lipschitz constant estimation problem.
 estimate local Lipschitz constant, propose use extreme value theory (De Haan Ferreira, robustness evaluation.
 context, extreme value corresponds local Lipschitz constant interest, be inferred set inde- pendently sampled local gradients.With aid extreme value theory, propose robustness metric called CLEVER, is short Cross Lipschitz Extreme Value nEtwork Robustness.
 note CLEVER is attack-independent robustness metric applies neural network classiﬁer.
 contrast, robustness metric proposed Hein Andriushchenko (2017), attack-agnostic, applies neural network classiﬁer hidden layer.
 highlight main contributions paper follows: propose novel robustness metric called CLEVER, is short Cross Lipschitz Extreme Value nEtwork Robustness.
 best knowledge, CLEVER is ﬁrst robustness metric is attack-independent be applied arbitrary neural network classiﬁer scales large networks ImageNet.
 proposed CLEVER score is supported theoretical analysis formal ro- bustness guarantees use extreme value theory.
 robustness analysis extends results Hein Andriushchenko (2017) differentiable functions special class non-differentiable functions – neural+ networks ReLU activations.
 • corroborate effectiveness CLEVER conducting experiments state-of-the- art models ImageNet, including ResNet (He et al., Inception-v3 (Szegedy et al., MobileNet (Howard et
 use CLEVER investigate defended networks adversarial examples, including use defensive distillation (Papernot al., bounded ReLU (Zantedeschi et
 Experimental results show CLEVER aligns attack-speciﬁc robustness indicated (cid:96)2 (cid:96)∞ distortions adversarial examples.
 BACKGROUND AND RELATED WORK ATTACKING NEURAL NETWORKS USING ADVERSARIAL EXAMPLES popular formulations found literature crafting adversarial examples mis- lead neural network is formulate minimization problem, variable δ ∈ Rd be optimized refers perturbation original example, objective function takes account unsuccessful adversarial perturbations speciﬁc norm δ assuring similar- ity.
 success adversarial examples be evaluated cross-entropy loss (Szegedy et Goodfellow et al., model prediction (Carlini Wagner,
 norm constraint δ be implemented clipping manner (Kurakin al., treated i=1 |δi|p)1/p particular, p = ∞, p ≥ is used crafting adversarial examples.
 (cid:107)δ(cid:107)∞ = maxi∈{1,...,d} |δi| measures maximal variation dimensions δ.
 i=1 |δi| measures total variation δ.
 state-of-the-art attack methods (cid:96)∞, (cid:96)2 (cid:96)1 norms are iterative fast gradient sign (I-FGSM) (Goodfellow et al., Kurakin Carlini Wagner’s attack (CW attack) (Carlini Wagner, elastic-net attacks deep neural net- works (EAD) (Chen respectively.
 attacks fall category white-box attacks network model is assumed be transparent attacker.
 Adversarial examples penalty function (Carlini Wagner,
 (cid:96)p norm δ, deﬁned (cid:107)δ(cid:107)p = ((cid:80)d p = (cid:107)δ(cid:107)2 becomes Euclidean norm δ.
 p = (cid:107)δ(cid:107)1 = (cid:80)p Published conference paper ICLR be crafted black-box network model using ensemble approach (Liu et al., training substitute model (Papernot al., employing optimization based attacks (Chen
 EXISTING DEFENSE METHODS discovery vulnerability adversarial examples (Szegedy et various defense methods have been proposed improve robustness neural networks.
 rationale defense is make neural network resilient adversarial perturbations, ensuring resulting defended model attains similar test accuracy original undefended network.
 Papernot al.
 proposed defensive distillation (Papernot uses distillation technique (Hinton et al., modiﬁed softmax function ﬁnal layer retrain network parameters prediction probabilities (i.e., soft labels) original network.
 Zantedeschi al.
 (2017) showed changing ReLU function bounded ReLU function, neural network be made resilient.
 popular defense approach is adversarial training, generates augments adversarial examples original training data network training stage.
 MNIST, trained model proposed Madry al.
 (2017) defend majority adversarial examples price increased network capacity.
 Model ensemble has been discussed increase robustness adversarial examples (Tram`er et al., Liu
 addition, detection methods such feature squeezing (Xu al., example reforming (Meng Chen, be used identify adversarial examples.
 However, CW attack is shown be able bypass different detection methods (Carlini Wagner,
 paper, focus evaluating intrinsic robustness neural network model adversarial examples.
 effect detection methods is scope.
 THEORETICAL ROBUSTNESS GUARANTEES FOR NEURAL NETWORKS Szegedy al.
 (2013) compute global Lipschitz constant layer use product explain robustness issue neural networks, global Lipschitz constant gives loose bound.
 Hein Andriushchenko (2017) gave robustness lower bound using local Lipschitz continuous condition derived closed-form bound multi-layer perceptron (MLP) single hidden layer softplus activation.
 Nevertheless, closed-form bound is hard derive neural network more hidden layer.
 Wang al.
 (2016) utilized terminologies topology study robustness.
 However, robustness bounds estimates were provided neural networks.
 other hand, works done Ehlers (2017); Katz al.
 (2017a;b); Huang et al.
 (2017) focus verifying viability certain properties neural networks possible input, transform formal veriﬁcation problem satisﬁability modulo theory (SMT) large-scale linear programming (LP) problems.
 SMT LP based approaches have high computational complexity are plausible small networks.
 use distortion adversarial examples found certain attack algorithm robustness metric.
 example, Bastani et al.
 (2016) proposed linear programming (LP) formula- tion ﬁnd adversarial examples use distortions robustness metric.
 observe LP formulation ﬁnd adversarial examples smaller distortions other gradient-based attacks L-BFGS (Szegedy et
 However, distortion found algorithms is upper bound true minimum distortion depends speciﬁc attack algorithms.
 methods differ proposed robustness measure CLEVER, CLEVER is estimation lower bound minimum distortion is independent attack algorithms.
 Additionally, LP-based approaches are impractical large networks, CLEVER is feasible large networks Inception-v3.
 concept minimum distortion upper/lower bound be deﬁned Section
 ANALYSIS OF FORMAL ROBUSTNESS GUARANTEES A CLASSIFIER section, provide formal robustness guarantees classiﬁer Theorem robust- ness guarantees are general require mild assumption Lipschitz continuity classiﬁcation function.
 differentiable classiﬁcation functions, results are consistent main theorem (Hein Andriushchenko, are obtained much simpler more Published conference paper ICLR Table Table Notation Notation Notation Deﬁnition dimensionality input vector ∆p,min number output classes f Rd → RK neural network classiﬁer x0 ∈ Rd xa ∈ Rd δ ∈ Rd (cid:107)δ(cid:107)p original input vector adversarial example distortion := xa − x0 (cid:96)p norm distortion, p ≥ Deﬁnition minimum (cid:96)p distortion x0 lower bound minimum distortion upper bound minimum distortion Lipschitz constant local Lipschitz constant βL βU Lj Lj Bp(x0, R) hyper-ball center x0 radius R CDF cumulative distribution function intuitive manner1.
 Furthermore, robustness analysis be extended non-differentiable classiﬁcation functions (e.g. neural networks ReLU) Lemma analysis Hein Andriushchenko (2017) is restricted differentiable functions.
 Speciﬁcally, Corollary shows robustness analysis (Hein Andriushchenko, is fact special case analysis.
 start analysis deﬁning notion adversarial examples, minimum (cid:96)p distortions, lower/upper bounds.
 notations are summarized Table
 Deﬁnition (perturbed example adversarial example).
 Let x0 ∈ Rd be input vector K-class classiﬁcation function f Rd → RK prediction is given c(x0) = argmax1≤i≤K fi(x0).
 Given say is perturbed example x0 noise δ ∈ Rd (cid:96)p-distortion ∆p xa = x0 + δ ∆p = (cid:107)δ(cid:107)p.
 adversarial example is perturbed exam- ple xa changes c(x0).
 successful untargeted attack is ﬁnd xa such c(xa) (cid:54)= c(x0) successful targeted attack is ﬁnd xa such c(xa) = t given target class t (cid:54)= c(x0).
 Deﬁnition (minimum adversarial distortion ∆p,min).
 Given input vector x0 classiﬁer f, minimum (cid:96)p adversarial distortion x0, denoted ∆p,min, is deﬁned smallest ∆p adversarial examples x0.
 Deﬁnition (lower bound ∆p,min).
 Suppose ∆p,min is minimum adversarial distortion x0.
 lower bound ∆p,min, denoted βL βL ≤ ∆p,min, is deﬁned such perturbed examples x0 (cid:107)δ(cid:107)p ≤ βL are adversarial examples.
 Deﬁnition (upper bound ∆p,min).
 Suppose ∆p,min is minimum adversarial distortion x0.
 upper bound ∆p,min, denoted βU βU ≥ ∆p,min, is deﬁned such exists adversarial example x0 (cid:107)δ(cid:107)p ≥ βU
 lower upper bounds are instance-speciﬁc depend input x0.
 βU be given ﬁnding adversarial example x0 using attack method, βL is easy ﬁnd.
 βL guarantees classiﬁer is robust perturbations (cid:107)δ(cid:107)p ≤ βL, certifying robustness classiﬁer.
 show derive formal robustness guarantee classiﬁer Lipschitz continuity assumption.
 Speciﬁcally, analysis obtains lower bound (cid:96)p minimum adversarial distortion βL = minj(cid:54)=c Lemma (Lipschitz continuity relationship gradient norm (Paulaviˇcius ˇZilinskas,
 Let S ⊂ Rd be convex bounded closed set let h(x) S → R be differentiable function open set containing S.
 Then, h(x) is Lipschitz function Lipschitz constant Lq following inequality holds x, y ∈ S: fc(x0)−fj (x0) Lj (1) )(cid:62) is gradient h(x), |h(x) − h(y)| ≤ Lq(cid:107)x − y(cid:107)p, Lq = max{(cid:107)∇h(x)(cid:107)q x ∈ S},∇h(x) = ∂h(x) ∂x1 p + q = ≤ p, q ≤ ∞.
 ,··· ∂h(x) ∂xd Given Lemma provide formal guarantee lower bound βL.
 (Formal guarantee lower bound βL untargeted attack).
 Let x0 ∈ Rd f Rd → RK be multi-class classiﬁer differentiable components fi let c = argmax1≤i≤K be class f predicts x0.
 δ ∈ Rd (cid:107)δ(cid:107)p ≤ min j(cid:54)=c fc(x0) − fj(x0) Lj (2) authors Hein Andriushchenko (2017) assume Lipschitz continuity use Mean Value Theorem H¨older’s Inequality prove main theorem.
 provide simple direct proof Lipschitz continuity assumption involving Mean Value Theorem H¨older’s Inequality.
 Published conference paper ICLR argmax1≤i≤K fi(x0 + δ) = c holds q = ≤ p, q ≤ ∞ Lj function fc(x) − fj(x) (cid:96)p norm.
 other words, βL = minj(cid:54)=c bound minimum distortion.
 p + q is Lipschitz constant fc(x0)−fj (x0) is lower Lj intuitions Theorem is shown Figure one-dimensional example.
 function value g(x) = fc(x) − fj(x) point x0 is double cone formed lines passing (x0, g(x0)) slopes equal ±Lq, Lq is (local) Lipschitz constant g(x) x0.
 other words, function value g(x) x0, g(x0 δ) be bounded g(x0), δ Lipschitz constant Lq. g(x0 + δ) is decreased adversarial exam- ple is found minimal change δ is g(x0) Lq complete proof is deferred Appendix A.
 Remark
 Lj call cross Lipschitz constant following (Hein Andriushchenko,
 q is Lipschitz constant function involving cross terms: fc(x) − fj(x), hence Figure Intuitions Theorem distinguish analysis (Hein Andriushchenko, show Corollary obtain same result (Hein Andriushchenko, Theorem fact, analysis (Hein Andriushchenko, is special case analysis authors assume Lipschitz continuity fi(x) requiring fi(x) be differen- tiable.
 use local Lipschitz constant (Lq,x0) global Lipschitz constant (Lq) obtain tighter bound adversarial perturbation δ.
 Corollary (Formal guarantee βL untargeted attack).
 Let Lj be local Lipschitz constant function fc(x)−fj(x) x0 ﬁxed ball Bp(x0, R) := {x ∈ Rd | (cid:107)x−x0(cid:107)p ≤ R} let δ ∈ Bp(0, R).
 Theorem obtain bound (Hein Andriushchenko, (3) fc(x0) − fj(x0) (cid:26) (cid:27) R q,x0 (cid:107)δ(cid:107)p ≤ min min j(cid:54)=c Lj q,x0 important use case Theorem Corollary is bound targeted attack: Corollary (Formal guarantee βL targeted attack).
 Assume same notation speciﬁed target class j, have (cid:107)δ(cid:107)p ≤ Theorem Corollary min(cid:8) fc(x0)−fj (x0) R(cid:9).
 Lj q,x0 addition, further extend Theorem special case non-differentiable functions – neural networks ReLU activations.
 case Lipchitz constant used Lemma be replaced maximum norm directional derivative, analysis go through.
 Lemma (Formal guarantee βL ReLU networks).
 Let h(·) be l-layer ReLU neural network Wi weights layer i.
 ignore bias terms don’t contribute gradient.
 h(x) = σ(Wlσ(Wl−1


 σ(W1x))) σ(u) = max(0, u).
 Let S ⊂ Rd be convex bounded closed set, equation (1) holds is Lq = supx∈S{| sup(cid:107)d(cid:107)p=1 D+h(x; d)|} D+h(x; d) := limt→0+ one-sided directional direvative, Theorem Corollary Corollary hold.
 h(x+td)−h(x) THE CLEVER ROBUSTNESS METRIC VIA EXTREME VALUE THEORY section, provide algorithm compute robustness metric CLEVER aid extreme value theory, CLEVER be viewed efﬁcient estimator lower bound βL is ﬁrst attack-agnostic score applies neural network classiﬁers.
 Recall Section proof deferred Appendix B proof deferred Appendix C Published conference paper ICLR show lower bound network robustness is associated g(x0) cross Lipschitz q,x0, g(x0) = fc(x0) − fj(x0) is available output classiﬁer constant Lj q,x0 is deﬁned maxx∈Bp(x0,R) (cid:107)∇g(x)(cid:107)q.
 ∇g(x) be calculated back Lj q,x0 is involved requires obtain maximum value propagation, computing Lj (cid:107)∇g(x)(cid:107)q ball.
 Exhaustive search low dimensional x Bp(x0, R) seems infeasible, mention image classiﬁers large feature dimensions interest.
 feature dimension d = MNIST, CIFAR ImageNet respectively.
 q,x0 is sampling set points x(i) ball Bp(x0, R) approach compute Lj x0 taking maximum value (cid:107)∇g(x(i))(cid:107)q.
 However, signiﬁcant amount samples be needed obtain good estimate max(cid:107)∇g(x)(cid:107)q is unknown good estimate is compared true maximum.
 Fortunately, Extreme Value Theory ensures maximum value random variables follow extreme value distributions, is useful estimate max(cid:107)∇g(x)(cid:107)q tractable number samples.
 is worth noting Wood Zhang (1996) applied extreme value theory estimate Lipschitz constant.
 However, are main differences work paper.
 First all, sampling methodology is different.
 Wood Zhang (1996) calculates slopes pairs sample points take samples norm gradient Lemma Secondly, functions considered Wood Zhang (1996) are one-dimensional opposed high-dimensional classiﬁcation functions considered paper.
 compari- son, show experiment approach Wood Zhang (1996), denoted SLOPE Table Figure perform high-dimensional classiﬁers such deep neural networks.
 ESTIMATE Lj q,x0 VIA EXTREME VALUE THEORY sampling point x Bp(x0, R), (cid:107)∇g(x)(cid:107)q be viewed random vari- able characterized cumulative distribution function (CDF).
 purpose illustration, derived CDF 2-layer neural network Theorem D.1.4 neural networks, suppose have n samples denote sequence independent distributed (iid) random variables Y1, Y2,··· Yn, CDF FY (y).
 CDF max{Y1,··· Yn}, denoted F Y (y), is called limit distribution FY (y).
 Fisher-Tippett- Gnedenko theorem says F Y (y), exists, be family extreme value distributions – Gumbel class, Fr´echet class reverse Weibull class.
 (Fisher-Tippett-Gnedenko Theorem).
 exists sequence pairs real num- Y (any + bn) = G(y), G is non-degenerate bers (an, bn) such > limn→∞ F distribution function, G belongs either Gumbel class (Type I), Fr´echet class (Type II) Reverse Weibull class (Type III) CDFs follows: y − aW (cid:3)(cid:9), Gumbel class (Type I): G(y) = exp(cid:8) − exp(cid:2) (cid:110) (cid:0) y−aW (cid:0) aW −y (cid:110) exp{− Fr´echet class (Type II): G(y) = Reverse Weibull class (Type III): G(y) = bW (cid:1)−cW}, (cid:1)cW}, y ∈ R, y < aW y ≥ aW y < aW y ≥ aW bW bW aW ∈ R, bW > cW > are location, scale shape parameters, respectively.
 implies maximum values samples follow families distributions.
 g(x) has bounded Lipschitz constant, (cid:107)∇g(x(i))(cid:107)q is bounded, limit distribution have ﬁnite right end-point.
 are interested reverse Weibull class, CDF has ﬁnite right end-point (denoted aW ).
 right end-point reveals upper limit distribution, known extreme value.
 extreme value is unknown local cross Lipschitz constant Lj q,x0, ﬁrst generate Ns samples x(i) ﬁxed ball Bp(x0, R) batch total Nb batches.
 compute (cid:107)∇g(x(i))(cid:107)q store maximum values batch set S.
 Next, samples S, perform maximum likelihood estimation reverse Weibull distribution parameters, location estimate ˆaW is used estimate Lj theorem proof are deferred Appendix D.
 q,x0 like estimate paper.
 estimate Lj q,x0.
 Published conference paper ICLR COMPUTE CLEVER: A ROBUSTNESS SCORE OF NEURAL NETWORK CLASSIFIERS Given instance x0, classiﬁer f (x0) target class j, targeted CLEVER score classiﬁer’s robustness be computed g(x0) Lj q,x0.
 Similarly, untargeted CLEVER scores be computed.
 proposed procedure estimating Lj q,x0 described Section summarize ﬂow computing CLEVER score targeted attacks un-targeted attacks Algorithm respectively.
 Nb, number samples batch Ns, perturbation norm p, maximum perturbation R Algorithm CLEVER-t, compute CLEVER score targeted attack Input: K-class classiﬁer f (x), data example x0 predicted class c, target class j, batch size Result: CLEVER Score µ ∈ R+ target class j S ← {∅}, g(x) ← fc(x) − fj(x), q ← p p−1.
 i ← Nb do k ← Ns do select point x(i,k) ∈ Bp(x0, R) compute bik ← (cid:107)∇g(x(i,k))(cid:107)q back propagation end S ← S ∪ {maxk{bik}} end ˆaW ← MLE location parameter reverse Weibull distribution S µ ← min( g(x0) R) ˆa Algorithm CLEVER-u, compute CLEVER score un-targeted attack Input: Same Algorithm target class j Result: CLEVER score ν ∈ R+ un-targeted attack j K, j (cid:54)= c do end ν ← minj{µj} µj ← CLEVER-t(f, x0, c, j, Nb, Ns, p, R) EXPERIMENTAL RESULTS NETWORKS AND PARAMETER SETUP conduct experiments CIFAR-10 (CIFAR short), MNIST, ImageNet data sets.
 former smaller datasets CIFAR MNIST, evaluate CLEVER scores small networks: single hidden layer MLP softplus activation same number hidden units (Hein Andriushchenko, 7-layer AlexNet-like CNN (with same structure (Carlini Wagner, 7-layer CNN defensive distillation (Papernot (DD) bounded ReLU (Zantedeschi et (BReLU) defense techniques employed.
 ImageNet data set, use popular deep network architectures: 50-layer Residual Net- work (He et al., (ResNet-50), Inception-v3 (Szegedy et al., MobileNet (Howard et
 were chosen following reasons: yield (close to) state-of-the- art performance equal-sized networks; (ii) architectures are different unique building blocks, i.e., residual block ResNet, inception module Inception net, depthwise separable convolution MobileNet.
 Therefore, diversity network architectures is appropriate test robustness metric.
 MobileNet, set width multiplier achiev- ing accuracy ImageNet.
 used public pretrained weights ImageNet models5.
 experiments, set sampling parameters Nb = Ns = R =
 targeted attacks, use test-set images CIFAR MNIST use test-set images ImageNet; image, evaluate targeted CLEVER score targets: random target class, likely class (the class lowest probability predicting original example), Pretrained models be downloaded https://github.com/tensorﬂow/models/tree/master/research/slim Published conference paper ICLR top-2 class (the class largest probability true class, is easiest target attack).
 conduct untargeted attacks MNIST CIFAR test-set images, evaluate untargeted CLEVER scores.
 experiment code is available6.
 FITTING GRADIENT NORM SAMPLES WITH REVERSE WEIBULL DISTRIBUTIONS ﬁt cross Lipschitz constant samples S (see Algorithm reverse Weibull class dis- tribution obtain maximum likelihood estimate location parameter ˆaW scale parameter ˆbW shape parameter ˆcW introduced Theorem validate reverse Weibull distri- bution is good ﬁt empirical distribution cross Lipschitz constant samples, conduct Kolmogorov-Smirnov goodness-of-ﬁt test K-S test) calculate K-S test statistics D corresponding p-values.
 null hypothesis is samples S follow reverse Weibull distribution.
 Figure plots probability distribution function cross Lipschitz constant samples ﬁtted Reverse Weibull distribution images various data sets network architectures.
 estimated MLE parameters, p-values, K-S test statistics D are shown.
 calculate percentage examples estimation have p-values greater illustrated Figure
 p-value is greater null hypothesis cannot be rejected, meaning underlying data samples ﬁt reverse Weibull distribution well.
 Figure shows numbers are close validating use reverse Weibull distribution underlying distribution gradient norm samples empirically.
 ﬁtted location parameter reverse Weibull distribution (i.e., extreme value), ˆaW be used good estimation local cross Lipschitz constant calculate CLEVER score.
 exact numbers are shown Table Appendix E.
 (a) CIFAR-MLP (b) MNIST-CNN (c) ImageNet-MobileNet Figure cross Lipschitz constant samples images CIFAR, MNIST ImageNet datasets, ﬁtted Reverse Weibull distributions corresponding MLE estimates location, scale shape parameters (aW bW cW shown top plot.
 D-statistics K-S test p-values are denoted ks pval.
 small ks high p-value, hypothesized reverse Weibull distribution ﬁts empirical distribution cross Lipschitz constant samples well.
 (a) Least likely target (b) Random target (c) Top target Figure percentage examples null hypothesis (the samples S follow reverse Weibull distribution) cannot be rejected K-S test signiﬁcance level p = p = ∞.
 numbers model are close indicating S ﬁts reverse Weibull distributions well.
 Source code is available https://github.com/huanzhang12/CLEVER 80859095100percentage(%)MobileNetResnetInceptionCIFAR-BReLUCIFAR-DDCIFAR-CNNCIFAR-MLPMNIST-BReLUMNIST-DDMNIST-CNNMNIST-MLPp=∞p=280859095100percentage(%)MobileNetResnetInceptionCIFAR-BReLUCIFAR-DDCIFAR-CNNCIFAR-MLPMNIST-BReLUMNIST-DDMNIST-CNNMNIST-MLPp=∞p=280859095100percentage(%)MobileNetResnetInceptionCIFAR-BReLUCIFAR-DDCIFAR-CNNCIFAR-MLPMNIST-BReLUMNIST-DDMNIST-CNNMNIST-MLPp=∞p=2 Published conference paper ICLR COMPARING CLEVER SCORE WITH ATTACK-SPECIFIC NETWORK ROBUSTNESS apply state-of-the-art white-box attack methods, iterative fast gradient sign (I- FGSM) (Goodfellow et Kurakin al., Carlini Wagner’s attack (CW) (Carlini Wagner, ﬁnd adversarial examples networks, including networks trained CIFAR, networks trained MNIST, networks trained ImageNet.
 CW attack, run iterations ImageNet CIFAR, iterations MNIST, MNIST has shown be difﬁcult attack (Chen
 Attack learning rate is tuned model: Inception-v3 ResNet-50, Mo- bileNet other networks.
 I-FGSM, run iterations choose optimal  ∈ {0.01, achieve smallest (cid:96)∞ distortion individ- ual image.
 distilled (DD) iterations I-FGSM are use iterations CIFAR-DD iterations MNIST-DD achieve success rate.
 problem be non-trivial, images are classiﬁed are skipped.
 report attack success rates networks, average distortion adversarial examples indicate attack-speciﬁc robustness network.
 comparison, compute CLEVER scores same set images attack targets.
 best knowledge, CLEVER is ﬁrst attack-independent robustness score is capable handling large networks studied paper, compare attack-induced distortion metrics study.
 evaluate effectiveness CLEVER score comparing upper bound βU (found attacks) CLEVER score, CLEVER serves estimated lower bound, βL.
 Table compares average (cid:96)2 (cid:96)∞ distortions adversarial examples found targeted CW I-FGSM attacks corresponding average targeted CLEVER scores (cid:96)2 (cid:96)∞ norms, Figure visualizes results (cid:96)∞ norm.
 Similarly, Table compares untargeted CW I-FGSM attacks untargeted CLEVER scores.
 expected, CLEVER is smaller dis- tortions adversarial images most cases.
 importantly, CLEVER is independent attack reported CLEVER scores indicate distortion best pos- sible attack terms speciﬁc (cid:96)p distortion.
 average (cid:96)2 distortion found CW attack is (cid:96)2 CLEVER score, indicating CW is strong (cid:96)2 attack.
 addition, defense mechanism (Defensive Distillation Bounded ReLU) is used, corresponding CLEVER scores are increased (except CIFAR-BReLU), indicating network is made resilient adversarial perturbations.
 CIFAR-BReLU, CLEVER scores (cid:96)p norm adversarial examples found CW attack decrease, implying bound ReLU is ineffective defense CIFAR.
 CLEVER scores be seen security checkpoint unseen attacks.
 example, is substantial gap distortion CLEVER score considered attack suggest existence effective attack close gap.
 CLEVER score is derived estimation robustness lower bound, further verify viability CLEVER example, i.e., is smaller upper bound found attacks.
 Table shows percentage inaccurate estimations CLEVER score is larger distortion adversarial examples found CW I-FGSM attacks ImageNet networks.
 found CLEVER score provides accurate estimation most examples.
 MobileNet Resnet-50, CLEVER score is strict lower bound attacks more tested examples.
 Inception-v3, condition strict lower bound Table Comparison average untargeted CLEVER score distortion found CW I-FGSM untargeted attacks.
 DD BReLU represent Defensive Distillation Bounded ReLU defending methods applied baseline CNN network.
 CW I-FGSM (cid:96)2 (cid:96)2 MNIST-MLP MNIST-CNN MNIST-DD MNIST-BReLU CIFAR-MLP CIFAR-CNN CIFAR-DD CIFAR-BReLU (cid:96)∞ (cid:96)∞ CLEVER (cid:96)∞ (cid:96)2 Published conference paper ICLR Table Comparison average targeted CLEVER scores average (cid:96)∞ (cid:96)2 distortions found CW, I-FSGM attacks, average scores calculated using algorithm Wood Zhang (1996) (denoted SLOPE) estimate Lipschitz constant.
 DD BReLU denote Defensive Distillation Bounded ReLU defending methods applied CNN network.
 did include SLOPE ImageNet networks has been shown be ineffective smaller networks.
 (a) avergage (cid:96)∞ distortion CW I-FGSM targeted attacks, CLEVER SLOPE estimation.
 large SLOPE estimates (in parentheses) exceeding maximum possible (cid:96)∞ distortion are reported
 Least Likely Target CW I-FGSM CLEVER MNIST-MLP MNIST-CNN MNIST-DD MNIST-BReLU CIFAR-MLP CIFAR-CNN CIFAR-DD CIFAR-BReLU Inception-v3 Resnet-50 MobileNet SLOPE (1.090) (5.327) Random Target CW I-FGSM CLEVER SLOPE Top-2 Target CW I-FGSM CLEVER SLOPE (2.470) (b) average (cid:96)2 distortion CW I-FGSM targeted attacks, CLEVER SLOPE estimation.
 large SLOPE estimates (in parentheses) exceeding sampling radius R = are reported
 Least Likely Target CW I-FGSM CLEVER MNIST-MLP MNIST-CNN MNIST-DD MNIST-BReLU CIFAR-MLP CIFAR-CNN CIFAR-DD CIFAR-BReLU Inception-v3 Resnet-50 MobileNet SLOPE (8.028) (9.947) (10.628) (52.058) (5.013) Random Target CW I-FGSM CLEVER SLOPE (8.102) (9.619) (9.493) (37.531) Top-2 Target CW I-FGSM CLEVER SLOPE (7.853) (7.921) (9.646) (23.548) (a) MNIST: Least likely target (b) MNIST: Random target (c) MNIST: Top target (d) CIFAR: Least likely target (e) CIFAR: Random target (f) CIFAR: Top target Figure Comparison (cid:96)∞ distortion obtained CW I-FGSM attacks, CLEVER score slope based Lipschitz constant estimation (SLOPE) Wood Zhang (1996).
 SLOPE exceeds distortions found attacks, is inappropriate estimation lower bound βL.
 is worse more found cases attack distortion differs CLEVER score small amount.
 Figure show empirical CDF gap CLEVER score (cid:96)2 norm adversarial distortion generated CW attack same set images Table
 Figure plot (cid:96)2 distortion CLEVER scores Published conference paper ICLR Table Percentage images ImageNet CLEVER score image is greater adversarial distortion found different attacks.
 Least Likely Target CW I-FGSM Random Target CW I-FGSM Top-2 Target CW I-FGSM MobileNet Resnet-50 Inception-v3 (a) MobileNet L∞ L2 L∞ L2 L∞ L2 L∞ L2 (c) Inception-v3 L∞ L2 L∞ L2 (b) ResNet-50 Figure empirical CDF gap CLEVER score (cid:96)2 norm adversarial distortion generated CW attack random targets images ImageNet networks.
 (a) MobileNet (b) ResNet-50 (c) Inception-v3 Figure Comparison CLEVER scores (circle) (cid:96)2 norm adversarial distortion generated CW attack (triangle) random targets images.
 x-axis is image ID y-axis is (cid:96)2 distortion metric.
 (a) Least likely target (b) Random target (c) Top-2 target Figure Comparison CLEVER score calculated Nb = {50, (cid:96)2 norm adversarial distortion found CW attack (CW) ImageNet models target types.
 individual image.
 positive gap indicates CLEVER (estimated lower bound) is less upper bound found CW attack.
 Most images have small positive gap, signiﬁes near-optimality CW attack terms (cid:96)2 distortion, CLEVER sufﬁces estimated capacity best possible attack.
 TIME V.S. ESTIMATION ACCURACY Figure vary number samples (Nb = compute (cid:96)2 CLEVER scores large ImageNet models, Inception-v3, ResNet-50 MobileNet.
 observe Published conference paper ICLR samples are sufﬁcient obtain accurate robustness estimation using smaller number samples.
 single GTX Ti GPU, cost sample (with Ns = is measured s MobileNet, s ResNet-50 s Inception-v3, computational cost CLEVER is feasible state-of-the-art large-scale deep neural networks.
 Additional ﬁgures MNIST CIFAR datasets are given Appendix E.
 CONCLUSION paper, propose CLEVER score, novel generic metric evaluate robustness target neural network classiﬁer adversarial examples.
 Compared existing robustness evaluation metric has following advantages: (i) attack-agnostic; (ii) applicable neural network classiﬁer; (iii) comes strong theoretical guarantees; (iv) is computa- feasible large neural networks.
 extensive experiments show CLEVER score matches practical robustness indication wide range natural defended networks.
 Acknowledgment.
 Luca Daniel Tsui-Wei Weng are supported MIT-Skoltech pro- gram MIT-IBM Watson AI Lab.
 Cho-Jui Hsieh Huan Zhang acknowledge support NSF IIS-1719097.
 REFERENCES Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis, Aditya Nori, An- tonio Criminisi.
 Measuring neural net robustness constraints.
 Advances Neural Infor- mation Processing Systems, pp.

 Nicholas Carlini David Wagner.
 Adversarial examples are detected: Bypassing ten detection methods.
 arXiv preprint arXiv:1705.07263,
 Nicholas Carlini David Wagner.
 Towards evaluating robustness neural networks.
 IEEE Symposium Security Privacy (SP), pp.

 Hongge Chen, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Cho-Jui Hsieh.
 Show-and-fool: Crafting adversarial examples neural image captioning.
 CoRR, abs/1712.02051,
 Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh.
 Ead: Elastic-net attacks deep neural networks adversarial examples.
 arXiv preprint arXiv:1709.04114,
 Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, Cho-Jui Hsieh.
 Zoo: Zeroth order op- timization based black-box attacks deep neural networks training substitute models.
 arXiv preprint arXiv:1708.03999,
 Laurens De Haan Ana Ferreira.
 Extreme value theory: introduction.
 Springer Science Business Media,
 Ruediger Ehlers.
 Formal veriﬁcation piece-wise linear feed-forward neural networks.
 arXiv preprint arXiv:1705.01320,
 Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul Prakash, Amir Rahmati, Dawn Song.
 Robust physical-world attacks machine learning models.
 arXiv preprint arXiv:1707.08945,
 Ian J Goodfellow, Jonathon Shlens, Christian Szegedy.
 Explaining harnessing adversarial examples.
 ICLR’15; preprint
 Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.
 Deep residual learning image recog- nition.
 Proceedings IEEE conference computer vision pattern recognition, pp.

 Matthias Hein Maksym Andriushchenko.
 Formal guarantees robustness classiﬁer adversarial manipulation.
 arXiv preprint arXiv:1705.08475,
 Published conference paper ICLR Geoffrey Hinton, Oriol Vinyals, Jeff Dean.
 Distilling knowledge neural network.
 arXiv preprint arXiv:1503.02531,
 Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam.
 Mobilenets: Efﬁcient convolutional neural networks mobile vision applications.
 arXiv preprint arXiv:1704.04861,
 Xiaowei Huang, Marta Kwiatkowska, Sen Wang, Min Wu. Safety veriﬁcation deep neural networks.
 International Conference Computer Aided Veriﬁcation, pp.


 Guy Katz, Clark Barrett, David Dill, Kyle Julian, Mykel Kochenderfer.
 Reluplex: efﬁcient smt solver verifying deep neural networks.
 arXiv preprint arXiv:1702.01135,
 Guy Katz, Clark Barrett, David L Dill, Kyle Julian, Mykel J Kochenderfer.
 Towards proving adversarial robustness deep neural networks.
 arXiv preprint arXiv:1709.02802,
 Alexey Kurakin, Ian Goodfellow, Samy Bengio.
 Adversarial examples physical world.
 arXiv preprint arXiv:1607.02533,
 Alexey Kurakin, Ian Goodfellow, Samy Bengio.
 Adversarial machine learning scale.
 ICLR’17; preprint
 Xuanqing Liu, Minhao Cheng, Huan Zhang, Cho-Jui Hsieh.
 Towards robust neural networks random self-ensemble.
 arXiv preprint arXiv:1712.00673,
 Yanpei Liu, Xinyun Chen, Chang Liu, Dawn Song.
 Delving transferable adversarial exam- ples black-box attacks.
 arXiv preprint arXiv:1611.02770,
 Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu.
 Towards deep learning models resistant adversarial attacks.
 arXiv preprint arXiv:1706.06083,
 Dongyu Meng Hao Chen.
 Magnet: two-pronged defense adversarial examples.
 arXiv preprint arXiv:1705.09064,
 Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, Ananthram Swami.
 Distillation IEEE Symposium defense adversarial perturbations deep neural networks.
 Security Privacy (SP), pp.

 Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, Ananthram Swami.
 Practical black-box attacks machine learning.
 ACM Asia Conference Com- puter Communications Security, pp.

 Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu.
 method automatic evaluation machine translation.
 Proceedings 40th annual meeting association computational linguistics, pp.

 Association Computational Linguistics,
 Remigijus Paulaviˇcius Julius ˇZilinskas.
 Analysis different norms corresponding lipschitz constants global optimization.
 Technological Economic Development Economy,
 Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen.
 Improved techniques training gans.
 Advances Neural Information Processing Systems, pp.

 Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus.
 Intriguing properties neural networks.
 arXiv preprint arXiv:1312.6199,
 Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna.
 Rethink- ing inception architecture computer vision.
 Proceedings IEEE Conference Computer Vision Pattern Recognition, pp.

 Florian Tram`er, Alexey Kurakin, Nicolas Papernot, Dan Boneh, Patrick McDaniel.
 Ensemble adversarial training: Attacks defenses.
 arXiv preprint arXiv:1705.07204,
 Published conference paper ICLR Beilun Wang, Ji Gao, Yanjun Qi. A theoretical framework robustness (deep) classiﬁers adversarial noise.
 arXiv preprint arXiv:1612.00334,
 GR Wood BP Zhang.
 Estimation lipschitz constant function.
 Journal Global Optimization,
 Weilin Xu, David Evans, Yanjun Qi. Feature squeezing: Detecting adversarial examples deep neural networks.
 arXiv preprint arXiv:1704.01155,
 Valentina Zantedeschi, Maria-Irina Nicolae, Ambrish Rawat.
 Efﬁcient defenses adver- sarial attacks.
 arXiv preprint arXiv:1707.06728,
 Published conference paper ICLR APPENDIX A PROOF OF THEOREM Proof.
 According Lemma assumption g(x) := fc(x)−fj(x) is Lipschitz continuous Lipschitz constant Lj (4) q gives Let x = x0 + δ y = x0 (4), get be rearranged following form q(cid:107)x − y(cid:107)p.
 |g(x) − g(y)| ≤ Lj |g(x0 + δ) − g(x0)| ≤ Lj q(cid:107)δ(cid:107)p ≤ g(x0 + δ) ≤ g(x0) + Lj q(cid:107)δ(cid:107)p, q(cid:107)δ(cid:107)p.
 g(x0) − Lj (5) g(x0 + δ) = adversarial example is found.
 indicated (5), g(x0 + δ) is lower q(cid:107)δ(cid:107)p ≥ adversarial bounded g(x0)−Lj examples be found: q(cid:107)δ(cid:107)p.
 (cid:107)δ(cid:107)p is small such g(x0)−Lj g(x0) − Lj q(cid:107)δ(cid:107)p ⇒ (cid:107)δ(cid:107)p ≤ g(x0) q ⇒ (cid:107)δ(cid:107)p ≤ Lj fc(x0) − fj(x0) Lj Finally, achieve argmax1≤i≤K fi(x0 + δ) = c, take minimum bound (cid:107)δ(cid:107)p (A) j (cid:54)= c.
 I.e. (cid:107)δ(cid:107)p ≤ min j(cid:54)=c fc(x0) − fj(x0) Lj classiﬁer decision be changed attack succeed.
 B PROOF OF COROLLARY Proof.
 Lemma let g = fc − fj, get Lj = maxy∈Bp(x0,R) (cid:107)∇g(y)(cid:107)q = maxy∈Bp(x0,R) (cid:107)∇fj(y) − ∇fc(y)(cid:107)q, gives bound Theorem (Hein An- driushchenko,
 q,x0 C PROOF OF LEMMA Proof.
 x, y, let d = be unit vector pointing x y r = (cid:107)y − x(cid:107)p.
 (cid:107)y−x(cid:107)p Deﬁne uni-variate function u(z) = h(x + zd), u(0) = h(x) u(r) = h(y) observe D+h(x + zd; d) D+h(x + zd;−d) are right-hand left-hand derivatives u(z), have (cid:26)D+h(x + zd; d) ≤ Lq undeﬁned u(cid:48)(z) = D+h(x + zd; d) = D+h(x + zd;−d) D+h(x + zd; d) (cid:54)= D+h(x + zd;−d) ReLU be most ﬁnite number points z ∈ (0, r) such g(cid:48)(z) does exist.
 be shown discontinuous z is caused ReLU activation, are ﬁnite combinations.
 Let = z0 < z1 < ··· < zk−1 < zk = be points.
 Then, using fundamental theorem calculus interval separately, exists ¯zi ∈ (zi, zi−1) i such u(r) − u(0) ≤ |u(zi) − u(zi−1)| |u(cid:48)(¯zi)(zi − zi−1)| i=1 k(cid:88) k(cid:88) k(cid:88) i=1 (Mean value theorem) (zi are line (x, y)) Lq|zi − zi−1|p i=1 = Lq(cid:107)x − y(cid:107)p.
 Theorem corollaries remain valid replacing Lemma Lemma Published conference paper ICLR D THEOREM D.1 AND ITS PROOF Theorem D.1 (FY (y) one-hidden-layer neural network).
 Consider neural network f Rd → (cid:1) pieces, RK input x0 ∈ Rd, hidden layer U hidden neurons, rectiﬁed linear unit (ReLU) activation function.
 sample ball Bp(x0, R), cumulative distribution function (cid:107)∇g(x)(cid:107)q, denoted FY (y), is piece-wise linear most M =(cid:80)d (cid:0)U i=0 g(x) = fc(x) − fj(x) given c j, (cid:33) Proof.
 jth output one-hidden-layer neural network be written q = ≤ p, q ≤ ∞.
 U(cid:88) (cid:32) d(cid:88) U(cid:88) p + fj(x) = Vjr · σ r=1 i=1 Wri · xi + br Vjr · σ (wrx + br) r=1 σ(z) = max(z, is ReLU activation function, W V are weight matrices ﬁrst second layer respectively, wr is rth row W
 compute g(x) (cid:107)∇g(x)(cid:107)q below: r=1 U(cid:88) U(cid:88) (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) U(cid:88) r=1 r=1 g(x) = fc(x) − fj(x) = (cid:107)∇g(x)(cid:107)q = I(z) is univariate indicator function: Vcr · σ (wrx + br) − Vjr · σ (wrx + br) U(cid:88) r=1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)q (Vcr − Vjr) · σ (wrx + br) I(wrx + br)(Vcr − Vjr)w(cid:62) (cid:110) z > z ≤
 I(z) = Figure Illustration Theorem D.1 d q = U =
 hyperplanes wix + bi = space regions (with different colors).
 red dash line encloses ball B2(x0, R1) blue dash line encloses larger ball B2(x0, R2).
 draw samples balls, probability (cid:107)∇g(x)(cid:107)2 = y is propor- tional intersected volumes ball regions (cid:107)∇g(x)(cid:107)2 = y.
 illustrated Figure hyperplanes wrx + br = r ∈ {1,


 U} divide d dimensional spaces Rd different regions, interior region satisfying different set inequal- ity constraints, e.g. wr+x + br+ > wr− x + br−
 Given identify region belongs checking sign wrx + br r.
 Notice gradient norm is same points same region, i.e. x1, x2 satisfying I(wrx1 + br) = I(wrx2 + br) Published conference paper ICLR have (cid:107)∇g(x1)(cid:107)q = (cid:107)∇g(x2)(cid:107)q.
 be most M =(cid:80)d i=0 (cid:0)U (cid:1) different regions d-dimensional space U hyperplanes, (cid:107)∇g(x)(cid:107)q take most M different values.
 perform uniform sampling ball Bp(x0, R) centered x0 radius R denote (cid:107)∇g(x)(cid:107)q random variable Y probability distribution Y is discrete CDF is piece-wise constant most M pieces.
 loss generality, assume are M0 ≤ M distinct values Y denote m(1), m(2),


 m(M0) increasing CDF Y denoted FY (y), is following: FY (m(i)) = FY (m(i−1)) + Vd({x | (cid:107)∇g(x)(cid:107)q = m(i)}) ∩ Vd(Bp(x0, R))) Vd(Bp(x0, R)) i =


 M0, FY (m(0)) = m(0) < m(1), Vd(E) is volume E d dimensional space.
 E ADDITIONAL EXPERIMENTAL RESULTS E.1 PERCENTAGE OF EXAMPLES HAVING P VALUE > Table shows percentage examples null hypothesis cannot be rejected K-S test, indicating maximum gradient norm samples ﬁt reverse Weibull distribution well.
 Table Percentage estimations null hypothesis cannot be rejected K-S test signiﬁcance level
 bar plots table are illustrated Figure
 Least Likely L∞ L2 MNIST-MLP MNIST-CNN MNIST-DD MNIST-BReLU CIFAR-MLP CIFAR-CNN CIFAR-DD CIFAR-BReLU Inception-v3 Resnet-50 MobileNet Random L2 L∞ Top-2 L2 L∞ E.2 CLEVER V.S. NUMBER OF SAMPLES Figure shows (cid:96)2 CLEVER score different number samples (Nb = MNIST CIFAR models.
 most models MNIST-BReLU, reducing number samples change CLEVER scores slightly.
 MNIST-BReLU, increasing number samples improves estimated lower bound, suggesting larger number samples is preferred.
 practice, start small Nb = a, try samples see CLEVER scores change signiﬁcantly.
 CLEVER scores stay same increasing Nb, conclude using Nb = is sufﬁcient.
 Published conference paper ICLR (a) MNIST, Least likely target (b) MNIST, Random target (c) MNIST, Top-2 target (d) CIFAR, Least likely target (e) CIFAR, Random target (f) CIFAR, Top2 target Figure Comparison CLEVER score calculated Nb = {50, (cid:96)2 norm adversarial distortion found CW attack (CW) MNIST CIFAR models target types.

 fast development sophisticated machine learning algorithms, artiﬁcial intelligence has been penetrat- ing number brand new ﬁelds unprecedented speed.
 outstanding problems hampering further progress is interpretability challenge.
 challenge arises models build machine learning algorithms are be used humans decision making, such decisions are subject legal consequences administrative audits.
 human decision makers operating circumstances, accept professional legal responsibility ensuing decisions assisted machine comprehend models.
 is learning, true areas criminal justice, health care, terrorism detection, education system ﬁnancial markets.
 trust model, decision makers need ﬁrst under- stand model’s behavior, evaluate reﬁne model using domain knowledge.
 areas book movie recommendations [1] automated aids [2], is critical explanations recommendation error made increase trust reliance systems.
 European General Data Protection Regulation, forthcoming June, stipulates explainability made decisions concerning individuals, includes decisions made assisted machine learning models.
 Hence, is growing demand interpretability machine learning algorithms.
 paper, deﬁne interpretability model ability provide visual textual presentation connections input features output predictions.
 realize goal interpretability, are approaches.
 is design algorithm is interpretable, achieving competitive accuracy com- plex model.
 examples are Decision Trees [3], Decision Lists [4], Decision Sets [5], etc.
 disadvantage approach is is trade interpretability accuracy: is easy learn interpretable (so simple) model expressing complex process high accuracy.
 other approach does sacriﬁce accuracy takes opposite approach: ﬁrst builds accurate model worrying interpretabilty, uses separate set re-representation techniques assist user understanding behavior algorithm.
 techniques be use aforementioned simple interpretable algorithms explain behavior complex model reasons given classiﬁer, treated black box, classiﬁes given instance particular way, e.g. LIME [6], BETA [7], TREPAN [11].
 Deep learning methods have been successful image processing natural language processing.
 be categorized representation learning approach [12], learns reﬁned features improve model’s generalization ability.
 Deep learning, however, is non- interpretable.
 paper are reporting work progress try interpret inner mechanisms deep learning.
 method: CNN-INTE is inspired [8].
 design implement tool helps user understand hidden layers deep CNN model work classify examples.
 results are expressed graphs indicates sequential separations true class hypothesis.
 main contributions method is follows: • Compared LIME [6] provides local in- terpretation speciﬁc regions feature space, method provides global interpretation test instances whole feature space.
 • Compared models apply interpretable algorithms, e.g. [5], method has advantage compromising accuracy model be interpreted.
 produces reliable interpretation.
 • contrary [6] [7] treat model be in- terpreted black box, interpret inner mechanisms deep CNN models.
 experiments are implemented TensorFlow [9] platform, makes model scalable big datasets easily.
 Scalability is issue pointed future work [6] [7] realized yet.
 II.
 RELATED WORK resolve problems “trusting prediction” “trusting model”, methods are proposed [6] explain individual predictions understand model’s behavior respectively: Local Interpretable Model-agnostic Explanations (LIME) Submodular Pick LIME (SP-LIME).
 main idea LIME is use interpretable models g interpret complex models f locally.
 designed objective function minimize unfaithfulness (when g is approximating f local area) complexity g.
 was stated paper objective function g be interpretable models, set g sparse linear models paper.
 Based individual explanations generated LIME, design submodular pick algorithm: SP-LIME explain model whole picking number representative non-redundant instances.
 was suggested [10] coverage, precision effort be used evaluate results model interpretation.
 LIME achieves high precision low effort, coverage is clear.
 other words, LIME is able explain speciﬁc prediction is made using weights local model g, can’t indicate local region explanation is faithful.
 solve problem, Anchor Local Interpretable Model-Agnostic Explanations method (aLIME) was introduced [10].
 if-then rules are used using weights linear model explain speciﬁc prediction (as was executed LIME).
 idea is based Decision Sets algorithm [5].
 if-then rules are easy comprehend has good coverage.
 was pointed is trade inter- pretability accuracy machine learning algorithms [5].
 terms interpretable models, rule-based models, e.g. Decision Trees Decision Lists are preferred, ﬁnd balance factors.
 Decision lists are considered interpretable decision trees, use if-then-else statements hierar- chy structure.
 structure reduces extent interpretability, interpret additional rule previous rules be reasoned
 new rules list are applied much narrow feature spaces, makes multi-class classiﬁcation difﬁcult minority classes deserves good rules.
 motivates proposal Decision Sets algorithm [5], produces isolated if- rules, rule be independent prediction.
 realize objective function takes account interpretability (expressed precision recall rules) accuracy (expressed size, length, cover overlap).
 showed solving objective function is NP- hard problem, ﬁnds near-optimal solutions it.
 However, Decision set’ accuracy approaches random forest, expressive power catches decision tree.
 model agnostic explanation approach is Black Box Explanations Transparent Approximations (BETA), introduced [7].
 Different LIME aims local interpretation, BETA is framework attempts produce global interpretation classiﬁer are treated black box classiﬁers.
 Based previous work Decision Sets, authors designed framework level decision sets taking account ﬁdelity (faithfulness black box model), unambiguity (single deterministic explanations instance), interpretability (complexity minimized) interactivity (user speciﬁed explorations feature’s subspace).
 level structure, outer if-then rules are “neighborhood descriptors” inner if-then rules are “decision logic rules” (how black box model labels instance outer if-then rules).
 Similar [5], objective function is built near-optimal solutions are found.
 III.
 METHODOLOGY methodology be classiﬁed post-hoc interpre- tation [14], trained model is given main task is interpret it.
 method is second approach mentioned fourth paragraph introduction section, is different many ways.
 model be interpreted is treated black box interpret hidden layers deep CNN.
 Second, compared LIME [6] has local method achieves global interpretability.
 Similar LIME, provide qualitative interpretation graphs visualize results.
 method interprets deep CNN Meta-learning, ﬁrst brieﬂy introduces deep CNN, meta-learning discuss framework details.
 A.
 Deep Convolutional Neural Network section introduces deep CNN model are going interpret.
 implement program using TensorFlow, use TensorBoard function draw structure deep CNN construct Fig.

 Deep CNN is advanced machine learning algorithm image classiﬁcation.
 takes advantage two-dimensional structure input images.
 uses set ﬁlters ﬁlter pixels raw input images generate higher level representations be learnt model order improve performance.
 are major components deep CNN: convolu- tional layer, pooling layer connected layer (same regular neural networks).
 deep CNN model is stack layers.
 convolutional layer, ﬁlter is used compute dot products pixels input image speciﬁc position values ﬁlter, producing single value output feature map.
 convolution operation is completed ﬁlter is slided width height input image.
 Following convolutional layer, activation function, rectiﬁed linear unit (ReLU) [15], is applied inject nonlinearities model speed training process.
 Following ReLU is pooling layer is non-linear down-sampling layer.
 common algorithm pooling is max pooling algorithm.
 algorithm, sub-region previous feature map is turned single maximum value region.
 Max pooling reduces computation controls overﬁtting.
 order calculate predicted class, performing max pooling, feature map needs be ﬂattened feed connected layer.
 last layer: output layer, softmax classiﬁer is applied prediction.
 structure deep CNN model designed is illustrated Fig.
 “Placeholder” represents interface input training data.
 “Reshape” is needed convert input one-dimensional image data dimensional data.
 experiment, use MNIST dataset [16].
 input features are converted two-dimensional × image.
 model has series convolutional layer followed pooling layer: “conv1”-“pool1”-“conv2”- “pool2”, are followed connected layer “fc1”.
 connected network is susceptible suffer “dropout” operation [17] applied “fc1” aims reduce it.
 operation, probability parameter p is set keep speciﬁc neuron probability p (or drop probability 1-p).
 “Adam [18], standard Stochastic Gradient Descent optimizer is used train model modifying variables reducing loss.
 “fc2” is output layer neurons: represents class 0-9.
 B.
 Meta-learning Meta-learning is ensemble learning method learns results base classiﬁers.
 has two-level structure, algorithms used ﬁrst level are called base-learners algorithm second level is called meta-learner.
 base-learners are trained original training data.
 meta-learner is trained predictions base classiﬁers true class original training data.
 training meta-learner, “Class-combiner” strategy [13] is applied here, predictions includes predicted class (instead classes, “Binary-class-combiner”).
 Fig.

 Structure deep CNN model generated TensorFlow’s Tensor- Board
 understand meta-learning algorithm intuitively, Fig.
 illustrates simpliﬁed training process meta-learning [19].
 numbers represents steps training.
 step, base learning algorithms m are trained training data.
 step, validation dataset is used test trained classiﬁers m.
 step, predictions generated step true labels validation dataset are used train meta-learner.
 Finally, step, meta-classiﬁer is produced whole meta- learning training process is completed.
 training process is test process is easier execute.
 Fig.
 presents simpliﬁed test process [19].
 step, test data is applied base classiﬁers generate predictions combined true labels test data comprises meta-level test data step.
 step, ﬁnal predictions are generated testing meta-level classiﬁer predictions 2nd step accuracy be calculated.
 Fig.

 Meta-learning training process.
 Fig.

 CNN-INTE training process.
 Fig.

 Meta-learning test process.
 C.
 Framework framework is named CNN-INTE stands Convolutional Neural Network Interpretation.
 is similar meta-learning, different few ways.
 work, interpret ﬁrst connected layer “fc1” deep CNN model illustrated Fig.

 training process is shown Fig.

 step, original training data is used train CNN model.
 step, parameters generated 1st step are used calculate values activations ﬁrst connected layer: fc1.
 step, clustering algorithm is used cluster data generated step number groups deﬁne factors henceforth.
 step, data belonging factors are clustered generating number clusters assigned unique ID.
 5th step, IDs are grouped training features meta-level, using labels original training data label meta-learner.
 step, features original training data IDs (set labels) step are used train number random forests [21].
 Assume training data T has N numbers instances layer “fc1” has H neurons.
 labels training data are Ty = {l1, l2, lN}.
 deep CNN model is trained, training instance ti, calculate activations hidden neuron layer.
 Hence, obtain matrix S size H × N.
 construct meta-level training data, use clustering algorithm cluster matrix hidden layer axis several factors F = {f1, f2, ..., fk}.
 factors, cluster data again, time axis instances.
 clustering results are IDs instance belongs to.
 instance, clusters, second level clustering instance have ID 0-9.
 IDs combined true labels training data builds meta-level training data.
 present technical details CNN-INTE training process, provide pseudo code Algorithm
 Line is initialization algorithm.
 line activations S are clustered K factors, K is number clusters set clustering algorithm C l.
 lines same clustering algorithm C l is applied factors generate K sets ID numbers.
 Lines 8-9 uses generated ID numbers true labels original training data train meta-learner: C m.
 Till now, training process is done yet.
 need generate base models be used test process.
 Lines 10-12 uses features original training data ID numbers train K base models.
 output training process be meta-lever classiﬁer: ˜M K base models: B = {M1, M2,··· MK}.
 CNN-INTE Training Process Input: activations: S; training data: T Meta learning algorithm: C m; Clustering algorithm: C l; Base learning algorithms: {C1, C2,··· CK} E = ∅ Scv = ∅ {f1, f2,··· fK} = C l(S) k = K do IDsk = C l(fk) end Scv = {IDs1, IDs2,··· IDsK, Ty} ˜M = C m(Scv) k = K do end E = ({M1, M2,··· MK} ˜M Output: Ensemble E Mk = Ck(Tx, IDsk) Fig.
 is toy example illustrates above process.
 example, are hidden neurons training instances.
 set number clusters ﬁrst second level clustering
 Hence, matrix S size × is ﬁrst clustered factors {f1, f2, f3} horizontally.
 factor, activations are clustered clusters vertically, e.g. F1 is clustered {C11, C12, C13}.
 set ID numbers cluster {0, corresponding ID numbers t1 t6 factor f1 according Fig.
 are {0,
 Hence, meta-level training features are expressed as0  data combined corresponding training labels original training data is used train meta-learner.
 meta-learner used is Decision Tree [3], interpretable algorithm.
 tree structure provides excellent visual explanation predictions.
 Fig.

 Toy example generation Meta-level training data.
 test process meta-model is same meta-learning test process, is shown Fig.

 test process, use original test data test base classiﬁers generated meta-level training process obtain meta-level test data’s features.
 base-learner applied is random forest.
 number base models is equal number factors.
 Hence, have K base models: B = {M1, M2, ..., MK}.
 toy example, are factors lead base models.
 training data ﬁrst base model corresponding F be (t1 − l1) (t2 − l2) (t3 − l3) (t4 − l4) (t5 − l5) (t6 − l6) ti−li represents features original training instance.
 obtain K base models, use original test data test produce meta-level test data.
 data are feed trained decision tree model interpret individual test predictions.
 Fig.

 CNN-INTE test process.
 IV.
 dataset use is MNIST database handwritten digits [16].
 extracted examples (the original dataset has examples training) training data examples test data.
 examples represents images pixels ﬂattened features.
 experiments are performed TensorFlow platform.
 A.
 Experimental Setup First all, need train nice deep CNN model.
 ﬁrst reshape input training data images size ×
 Training data epoch is expensive, requires lots resources computer lead termination program.
 apply stochastic training: ﬁrst epoch, select mini- batch training data perform optimization batch; loop batches, randomize training data start new epoch.
 experiment, set epoch e = batch size b =
 Stochastic training is cheap achieves similar performance using whole training data epoch.
 mini-batch, ﬁrst convolutional layer, apply ﬁlters (or kernels) size generates feature maps.
 ﬁrst pooling layer apply ﬁlters size ×
 stride size is set
 second convolutional layer use ﬁlters same size ﬁrst convolutional layer.
 second pooling layer has same parameters previous one.
 pooling layer is ﬁrst connected layer.
 set number neurons layer
 reduce overﬁtting set dropout [17] parameter d = means neuron’s output has probability be dropped.
 last layer is second connected layer (or “readout layer”), has neurons neuron outputs probability corresponding digits 0-9.
 test accuracy trained CNN model test data is
 comes key part setting interpretation.
 deﬁne interpretability model ability provide visual textual presentation connections input features output predictions.
 ﬁrst feed trained connected layer f c1 original training data, produce data S size ×
 cluster S several factors.
 clustering algorithm applied is k-means algorithm [20].
 number factors is equal number clusters set level.
 Hence, S is turned list F = {f1, f2, ..., f8} size × having row representing data belonging factor.
 second level clustering, factor F use k-means algorithm cluster number clusters.
 set number experiment number classes original training data is
 Hence cluster be assigned unique ID number
 use IDs belonging training instances true labels original training data train decision tree algorithm.
 Due limitation space, are unable show structure trained decision tree here.
 set maximum depth decision tree
 deeper decision tree generate better accuracy, makes harder interpret many tree levels.
 obtain test data decision tree, ﬁrst use original training data’s feature features IDs factor F labels train corresponding random forest algorithm [21], generating base models.
 random forest, set number trees maximum nodes
 use original test data test trained base models.
 generated predictions become features meta-level test data sizes ×
 Using meta-level test data trained decision tree produces accuracy tree depth=5.
 value is comparable test accuracy trained deep CNN model:
 be noted decision tree’s accuracy be improved increasing depth tree tuning other related parameters.
 B.
 Experimental Results interpret deep CNN model’s behavior test data, intend use diagrams generated tool: CNN-INTE examine individual predictions test data.
 Hence, provide qualitative interpretations visually.
 selected test instances were classiﬁed decision tree test instance was classiﬁed.
 be noted tool be used test instances limited cases provide.
 details selected test instances are shown Table I.
 “f0-f7” represents features meta- level test data, “label” is test label original test data, “pred” is prediction generated decision tree meta-level test data.
 “True1” “True2” represents classiﬁed instances “Wrong1” is classiﬁed instance.
 INSTANCES SELECTED THE META-LEVEL TEST DATA TABLE Features labels True1 True2 Wrong1 f0 f1 f2 f3 f4 f5 f6 f7 label pred order examine classiﬁcation process check feature values according trained structure decision tree plot graphs activations corresponding true label hypothesis.
 in- terpretation result instance “True1” is shown Fig.

 true label instance is other classes be regarded hypothesis is are graphs Fig.

 row represents examination feature values corresponding different factors different levels trained decision tree, e.g. ﬁrst row represents root level decision tree.
 set depth decision tree are rows all.
 column stands query test instance belongs corresponding hypothesis nodes visited.
 Take column “Hypothesis:0” example, goal is ﬁnd label test instance is
 row extract activations corresponding “f6” satisﬁes condition f6 (this is determined trained decision tree) draw graph activations belongs label=0 (hypothesis) label=3 (true).
 check graph evaluate data corresponding true class be separated hypothesis.
 answer is hypothesis represented blue points overlaps true class shown red points.
 Hence, need query trained decision tree further.
 values factors need check is: f6 (cid:54) row; f6 (cid:54) row; f7 (cid:54) row; f1 (cid:54) row.
 process, noticed row true class hypothesis class are separated red points corresponding true label are left.
 don’t examine further that’s graph 5th row is displayed.
 highlight graph green rectangles ﬁnal results are separable red vice versa.
 same idea is applied other hypothesis.
 draw graphs instances “True2” “Wrong1” Fig.
 Fig.
 respectively.
 V.
 CONCLUSION AND FUTURE WORK work, present interpretation tool CNN-INTE, interprets hidden layer deep CNN model: ﬁnd learned hidden layer classiﬁes new test instances.
 show results ﬁrst connected layer read-out layer, approach be applied hidden layers.
 interpretation is realized ﬁnding relationships original training data trained hidden layer “fc1” meta- learning.
 used two-level k-means clustering algorithm ﬁnd meta-level training data random forests base models generating meta-level test data.
 visual results generated program indicate test instance is classiﬁed checking are overlaps corresponding activations.
 future work, plan initiate quantiﬁcation interpreted results.
 experiments, things ﬁnd tricky is setting number clusters k-means algorithm.
 future, plan replace k-means algorithm DBSCAN [22] Fig.

 Example classiﬁed test instance: True1.
 doesn’t need specifying number clusters.
 stated [5], “decision sets” seems be better option decision tree interpretable algorithm, plan replace decision tree decision sets.
 Last least, be meaningful apply tool real world applications interpretations are demanded training data hidden layer hidden layer predictions.
 authors acknowledge support Province Nova Scotia, Dalhousie University, Natural Sciences Engineering Research Council Canada CREATE program grant.
 REFERENCES [1] J.
 L.
 Herlocker, J.
 A.
 Konstan, J.
 Riedl, “Explaining collaborative ﬁltering recommendations,” Proceedings ACM conference Computer supported cooperative work, pp.

 [2] M.
 T.
 Dzindolet, S.
 A.
 Peterson, R.
 A.
 Pomranky, L.
 G.
 Pierce, H.
 P.
 Beck.
 “The role trust automation reliance,” Int.
 J.
 Hum.-Comput.
 Stud., vol.58, no.6, pp.697–718,
 [3] J.
 Ross Quinlan, C4.
 programs machine learning, Elsevier,
 [4] R.
 L.
 Rivest, “Learning decision Machine learning, vol.2, no.3, pp.229–246,
 [5] H.
 Lakkaraju, S.
 H.
 Bach, J.
 Leskovec, “interpretable decision sets: joint framework description prediction,” Proceedings ACM SIGKDD International Conference Knowledge Discovery Data Mining, pp.
 ACM, August,
 [6] M.
 T.Ribeiro, S.Singh C.Guestrin, “Why i trust you?
 Explaining predictions classiﬁer,” Proceedings ACM SIGKDD International Conference Knowledge Discovery Data Mining, pp.
 ACM, August,
 [7] H.
 Lakkaraju, E.Kamar, R.Caruana J.Leskovec, “Interpretable Explorable Approximations Black Box Models,” KDD’17 workshop,
 [8] J.
 J.Thiagarajan, B.Kailkhura, P.
 Sattigeri K.
 N.Ramamurthy, “TreeView: Peeking Deep Neural Networks Via Feature-Space Par- titioning,” Conference Neural Information Processing Systems (NIPS),
 [9] M.Abadi,et al.
 “TensorFlow: System Large-Scale Machine Learn- ing,” OSDI, Vol.
 pp.

 [10] M.
 T.Ribeiro, S.Singh C.Guestrin, “Nothing Else Matters: Model- Agnostic Explanations Identifying Prediction Invariance,” Con- ference Neural Information Processing Systems (NIPS),
 [11] M.
 Craven J.
 W.
 Shavlik, “Extracting tree-structured representations trained networks,” Advances neural information processing systems, pp.

 [12] I.
 Goodfellow, Y.
 Bengio A.
 Courville, Deep learning, MIT press,
 [13] P.
 K.
 Chan S.
 J.
 Stolfo, “Experiments multistrategy learning meta-learning,” Proceedings second international conference information knowledge management, pp.
 ACM, De- cember,
 [14] G.
 Montavon, W.
 Samek, K.R. M¨uller, “Methods interpreting understanding deep neural networks,” Digital Signal Processing,
 [15] V.
 Nair G.
 E.
 Hinton, “Rectiﬁed linear units improve restricted boltz- mann machines,” Proceedings 27th international conference machine learning (ICML-10), pp.

 [16] Y.
 LeCun, L.
 Bottou, Y.
 Bengio, P.
 Haffner, “Gradient-based learning applied document recognition,” Proceedings IEEE, pp.
 November,
 [17] N.
 Srivastava, G.
 Hinton, A.
 Krizhevsky, I.
 Sutskever, R.
 Salakhut- dinov, “Dropout: A simple way prevent neural networks overﬁt- Journal Machine Learning Research, pp.

 [18] D.
 P.
 Kingma J.
 Ba, “Adam: A method stochastic optimization,” arXiv preprint arXiv:1412.6980,
 [19] X.
 Liu, X.
 Wang, S.
 Matwin, N.
 Japkowicz, “Meta-learning large scale machine learning MapReduce,” Big Data, IEEE International Conference pp.
 IEEE, October,
 Fig.

 Example classiﬁed test instance: True2.
 [20] J.
 A.
 Hartigan M.
 A.
 Wong, “Algorithm AS A k-means clustering algorithm,” Journal Royal Statistical Society, Series C (Applied Statistics), pp.
 100-108,
 [21] A.
 Liaw M.
 Wiener, “Classiﬁcation regression randomFor- est,” R pp.

 [22] M.
 Ester, H.
 P.
 Kriegel, J.
 Sander, X.
 Xu, “A density-based algorithm discovering clusters large spatial databases noise,” Kdd, Vol.
 No. pp.
 226-231, August,
 Fig.

 Example classiﬁed test instance: Wrong1.
 computations deep neural network are equal importance.
 conventional deep learning pipeline, expert crafts neural architecture trains prepared dataset.
 success training deep model requires trial error, such loop has little control prioritizing computations happening neural network.
 researchers started develop model-simpliﬁcation methods convolutional neural networks (CNNs), bearing mind computations are non-critical redundant hence be removed trained model degrading model’s performance.
 Such methods accelerate computational efﬁciency alleviate model’s overﬁtting effects.
 Discovering subsets computations trained CNN are reasonable prune, however, is nontrivial.
 Existing methods be categorized learning perspective computational perspective.
 learning perspective, methods use data-independent approach training data does assist determining part trained CNN be pruned, e.g. et al.
 (2017) Zhang et al.
 (2016), others use data-dependent approach joint optimization generating pruning decisions, e.g., Han et al.
 (2015) An- war et al.
 (2017).
 computational perspective, most approaches focused setting ∗This work was done Jianbo Ye interned Adobe summer.
 Published conference paper ICLR dense weights convolutions linear maps be structured sparse, propose method adopting new conception achieve effect same goal.
 regarding computations CNN collection separate computations sitting different layers, view network ﬂow delivers information input output different channels different layers.
 believe saving computations CNN is reducing are calculated individual layer, understanding channel is contributing entire information ﬂow underlying passing graph removing channels are responsible such process.
 Inspired new conception, propose design “gate” channel CNN, controlling received information is sent other channels processing.
 channel “gate” closes, output be constant.
 fact, designed “gate” have prior intention close, has “strong” duty sending received information input follow-up layers.
 ﬁnd implementing idea pruning CNNs is unsophisticated, be detailed Sec
 method introduces extra parameters existing CNN, changes computa- tion graph.
 fact, introduces marginal overheads existing gradient training CNN.
 possess attractive feature build multiple compact models different inference performances single round resource-intensive training (as were done exper- iments).
 eases process choose balanced model deploy production.
 Probably, only applicability constraint method is convolutional layers fully-connected layer last layer) CNN be batch normalized (Ioffe Szegedy,
 Given batch normalization has becomes adopted ingredient designing state-of-the-art deep learning models, many successful CNN models are using believe approach has wide scope potential impacts.1 paper, start rethinking basic assumption explored existing channel prun- ing work.
 point several issues gaps realizing assumption successfully.
 propose alternative approach, work several numerical difﬁculties.
 experiment method different benchmarks validate usefulness strengths.
 RELATED WORK Reducing size neural network speeding computational performance inference time has been long studied topic community neural network deep learning.
 Pioneer works include Optimal Brain Damage (LeCun et al., Optimal Brain Surgeon (Hassibi Stork,
 recent developments focused reducing structural complexity provided network training compact simpliﬁed network scratch.
 work be categorized former type.
 literature review revolves reducing structural complexity.
 reduce structural complexity deep learning models, previous work have focused sparsifying weights convolutional kernels feature maps multiple layers network (Anwar et al., Han et
 recently, work proposes impose structured sparsity vector components motivated implementation perspective specialized hardware (Wen et Zhou Alvarez Salzmann, Lebedev Lempitsky,
 argued authors (Molchanov al., regularization-based pruning techniques require layer sensitivity analysis adds extra computations.
 Molchanov al.
 (2017) relies global rescaling criteria layers does require sensitivity esti- mation, beneﬁcial feature approach has.
 knowledge, is much useful are work deep learning.
 Section discuss details potential issues regularization-based pruning techniques hurting being applicable, regularize high-dimensional tensor parameters use magnitude-based pruning methods.
 approach works mentioned issues constraining anticipated pruning operations batch normalized convolutional layers.
 posing structured convolution layer is trained batch convert “near equivalent” convolution layer batch normalization removing bias term b setting σ + , β = b + µ, σ µ are estimated outputs convolution training γ = samples.
 Published conference paper ICLR sparsity kernels feature maps, enforce sparsity scaling parameter γ batch normal- ization operator.
 blocks sample-wise information passing part channels convolution layer, effect implies remove channels.
 recent work (Huang Wang, used similar technique ours remove unimportant residual modules ResNet introducing extra scaling factors original network, optimiza- tion subtleties pointed paper were explained.
 recent work called Network-Slimming (Liu et aims sparsify scaling parameters batch normal- ization.
 using off-the-shelf gradient learning theirs, propose new algorithmic approach based ISTA rescaling trick, improving robustness speed undergoing opti- mization.
 work Liu al.
 (2017) was able prune VGG-A model ImageNet.
 is unclear work deal γ-W rescaling effect approach be adopted large pre-trained models, such ResNets Inceptions.
 experimented pre-trained ResNet-101 compared recent work were shown work large CNNs. experimented image segmentation model has inception-like module (pre-trained ImageNet) locate foreground objects.
 RETHINKING SMALLER-NORM-LESS-INFORMATIVE ASSUMPTION most regularized linear regressions, large-norm coefﬁcient is strong indicator informative feature.
 has been perceived statistics machine learning community, removing features have small coefﬁcient does affect regression er- rors.
 Therefore, has been established practice use tractable norm regularize parameters optimizing model pick important ones comparing norms training.
 How- ever, assumption is unconditional.
 using Lasso ridge regression select important predictors linear models, has ﬁrst normalize predictor variable.
 result be explanatory.
 example, ridge regression penalizes more predictors has low variance, Lasso regression enforces sparsity coefﬁcients are small OLS.
 Such normalization condition right use regularization is unsatisﬁed nonconvex learning.
 example, has consider issues outlined below.
 provides cases exemplify regularization fail be limited usage.
 exist ways avoid speciﬁc failures.
 Model Reparameterization.
 ﬁrst case, show is easy have ﬁne-grained control weights’ norms different layers.
 has choose uniform penalty layers struggle reparameterization patterns.
 Consider ﬁnd deep linear (convolutional) network subject least square Lasso: λ > n(cid:88) i=1 E(x,y)∼D(cid:107)W2n ∗


 ∗ W2 ∗ W1 ∗ x − y(cid:107)2 + λ min {Wi}2n i=1 (cid:107)W2i(cid:107)1
 above formulation is well-deﬁned problem parameter set {Wi}2n ﬁnd parameter set {W (cid:48) keeping corresponding l0 norm unchanged setting i=1, i}2n i=1 such achieves smaller total loss i = αWi, i =


 − W (cid:48) W (cid:48) i = Wi/α, i =


 α >
 word,  ﬁnd parameter set {Wi}2n i=1 (which is non-sparse) minimizes ﬁrst least square loss having second Lasso term less .
 note gradient-based learning is inefﬁcient exploring such model reparameteriza- tion patterns.
 fact, are recent discussions (Dinh
 adopts pre-trained model, augments original objective new norm-based parameter regu- larization, new gradient updates increase take long time variables traveling model’s reparameterization trajectory.
 highlights theoretical gap questioning existing sparsity inducing formulation actual computational algorithms achieve satisfactory parameter sparsiﬁcation deep learning models.
 Published conference paper ICLR Transform Invariance.
 second case, show batch normalization is compatible weight regularization.
 example is penalizing l1- l2-norms ﬁlters convolution layer is followed batch normalization: l-th layer, let xl+1 = max{γ · BNµ,σ,(W l ∗ xl) + β, γ β are vectors length is number channels.
 Likewise, see uniform scaling W l changes l1- l2-norms have ef- fects output xl+1.
 speaking, is interested minimizing weight norms multiple layers together, becomes unclear choose proper penalty layer.
 Theoretically, exists optimizer change weight inﬁnitesimal magnitude hurting inference performance.
 pointed reviewers, avoid issue projecting weights surface unit ball.
 has deal non-convex feasible set parameters, causing extra difﬁculties developing optimization data-dependent pruning method.
 is worth noting existing work used such strategy layer-by-layer greedy way (He et Zhang et
 Based discussion, many existing works claim use Lasso, group Lasso (e.g. Wen al.
 (2016); Anwar et al.
 (2017)), thresholding (e.g. Molchanov et al.
 (2017)) enforce param- eter sparsity have theoretical gaps bridge.
 fact, many heuristic algorithms neural net pruning do generate sparse parameterized solution.
 often, thresholding is used set certain subset parameters network zeros, be problem- atic.
 reason is essence questions.
 First, setting parameters threshold zeros, functionality neural net be preserved guarantees?
 yes, conditions?
 Second, set thresholds weights different layers?
 layer contributes neural net.
 is expected layers act performance use small computation memory budget, other layers help performance consume lot resources.
 is desirable prune calculations latter kind layers former.
 contrast existing approaches, focus enforcing sparsity tiny set parameters CNN — scale parameter γs batch normalization.
 placing sparse constraints γ is simpler easier monitor, more importantly, have strong reasons:
 γ multiplies normalized random variable, channel importance be- comes comparable different layers measuring magnitude values
 reparameterization effect different layers is avoided follow-up convolution layer is batch normalized.
 other words, impacts scale changes γ parameter are independent different layers.
 Nevertheless, current work falls short strong theoretical guarantee.
 believe working normalized feature inputs regularized coefﬁcients together, is closer more robust meaningful approach.
 Sparsity is goal, goal is ﬁnd important channels using sparsity inducing formulation.
 CHANNEL PRUNING OF BATCH-NORMALIZED CNN describe basic principle algorithm channel pruning technique.
 PRELIMINARIES Pruning constant channels.
 Consider convolution batch normalization: xl+1 = max(cid:8)γl · BNµl,σl,l (W l ∗ xl) + βl,
 ease notation, let γ = γl.
 Note element γ is set zero, say, γ[k] = output image xl+1 :,:,:,k becomes constant βk, convolution constant image channel is everywhere constant (except padding regions, issue be discussed later).
 show constant image channels be pruned same functionality network is kept: Published conference paper ICLR • follow-up convolution layer does have batch normalization, xl+2 = max(cid:8)W l+1 ∗ xl+1 + bl+1, values (a.k.a. elements β) is absorbed bias term following equation such new := bl+1 + I(γ = · ReLU(β)T sum reduced(W l+1 bl+1 xl+2 ≈ max(cid:8)W l+1 ∗γ xl+1 + bl+1 :,:,·,·) ∗γ denotes convolution operator is calculated channels indexed non-zeros γ.
 Remark W ∗ = sum reduced(W:,:,·,·) W ∗ i,j Wi,j,a,b.
 a,b =(cid:80) (cid:0)W l+1 ∗ xl+1(cid:1) + βl+1, • follow-up convolution layer has batch xl+2 = max(cid:8)γl+1 · BNµl+1,σl+1,l+1 moving average is updated new := µl+1 − I(γ = · ReLU(β)T sum reduced(W l+1 µl+1 :,:,·,·) (cid:110) γl+1 · BNµl+1 (cid:0)W l+1 ∗γ xl+1(cid:1) + βl+1, (cid:111) such xl+2 ≈ max Remark approximation (≈) is equivalence (=) padding is used convolu- tion operator ∗, feature parallel work Liu et al.
 (2017) does possess.
 original model uses padding computing convolution layers, network function is preserved pruning.
 practice, ﬁne-tune pruned network ﬁx such performance degradation last.
 short, formulate network pruning problem simple set more elements γ zero.
 is easier deploy pruned model, extra parameters layers are introduced original model.
 understand works entire CNN, imagine channel-to-channel computation graph formed connections layers.
 graph, channel is node, infer- ence dependencies are represented directed edges.
 γ parameter serves “dam” node, deciding let received information “ﬂood” other nodes following graph.
 end-to-end training channel pruning is ﬂood control system.
 suppose be rich information input distribution, ways, original input information is lost way CNN inference, useful part — is supposed be preserved network inference — be label sensitive.
 Conventional CNN has way reduce information: transforming feature maps (non-invertible) forward propagation.
 approach introduces other way: block information channel forcing output being constant using ISTA.
 ISTA.
 gap Lasso sparsity non-convex settings, found ISTA (Beck Teboulle, is useful sparse promoting method.
 need use carefully.
 Speciﬁcally, adopt ISTA updates γs.
 basic idea is project parameter step gradient descent more sparse subject proxy problem: let l denote training loss interest, (t + 1)-th step, set γt+1 = min (1) ∇γlt is derivative respect γ computed step t, µt is learning rate, λ is penalty.
 stochastic learning, ∇γlt is estimated mini-batch step.
 Eq. (1) has closed form solution (cid:107)γ − γt + µt∇γlt(cid:107)2 + λ(cid:107)γ(cid:107)1 µt γt+1 = proxµtλ(γt − µt∇γlt) proxη(x) = max{|x|− η, sgn(x).
 ISTA method serves “ﬂood control system” end-to-end learning, functionality γ is dam.
 γ is information ﬂood is blocked, γ same amount information is passed form geometric quantities magnitudes are proportional γ.
 Published conference paper ICLR Scaling effect.
 see γ is scaled α W l+1 is scaled is, γ := αγ, W l+1 := W l+1 output xl+2 is unchanged same input xl.
 changing output, scaling γ W l+1 scales gradients ∇γl ∇W l+1l α, respectively.
 observed, parameter dynamics gradient learning ISTA depends scaling factor decides choose other Intuitively, α is large, optimization W l+1 is progressed slower γ.
 THE ALGORITHM describe algorithm below.
 following method applies training scratch re-training pre-trained model.
 Given training loss l, convolutional neural net N hyper-parameters ρ, α, µ0, method proceeds follows:
 Computation sparse penalty layer.
 Compute memory cost channel layer denoted λl set ISTA penalty layer l ρλl.
 kl λl = w · i i (cid:88) l(cid:48)∈T (l) w · kl h · cl−1 + w · kl(cid:48) kl(cid:48) h · cl(cid:48) + l w · l (2)  w i w · kl size follow-up convolution layer l(cid:48).
 h is size input image neural network.
 h is kernel size convolution layer l.
 Likewise, kl(cid:48) • i w · kl(cid:48) • kl • T (l) represents set follow-up convolutional layers layer l • cl−1 denotes channel size previous layer, l-th convolution operates • l denotes channel size follow-up layer l(cid:48).
 h is image size feature map layer l.
 over; cl(cid:48) w · l h is kernel
 γ-W rescaling trick.
 layers channels are going get reduced, scale γls batch normalizations α scale weights follow-up convolutions

 End-to-End training ISTA γ.
 Train N regular SGD, exception γls are updated ISTA, initial learning rate is µ0.
 Train N loss l total sparsity γls converges, Lasso ρ(cid:80) γl are output pruned model (cid:101)N absorbing constant channels follow-
 γ-W rescaling trick.
 γls weights (cid:101)N were scaled Step training,
 Fine-tune (cid:101)N using regular stochastic gradient learning.

 Post-process remove constant channels.
 Prune channels layer l elements scale α (scaling back).
 layers (as described earlier section.).
 l converges.
 Remark choosing proper α used Steps is necessary using large µt · ρ ISTA, makes sparsiﬁcation progress γls faster.
 GUIDELINES FOR TUNING HYPER-PARAMETERS summarize sensitivity hyper-parameters impacts optimization below: • µ (learning rate): larger µ leads fewer iterations convergence faster progress • ρ (sparse penalty): larger ρ leads sparse model convergence.
 trained sparsity.
 µ large, SGD approach wouldn’t converge.
 large ρ, channels be pruned.
 Published conference paper ICLR • α (rescaling): use other
 pretrained models, choose α {0.001, smaller α warms progress sparsity.
 recommend following parameter tuning strategy.
 First, check cross-entropy loss regularization loss, select ρ such quantities are comparable beginning.
 Second, choose reasonable learning rate.
 Third, model is pretrained, check average magnitude γs network, choose α such magnitude rescaled γl is
 found choose parameters right range magnitudes, optimization progress is enough robust.
 monitor mentioned quantities training terminate iterations quantities plateaus.
 are several patterns found experiments suggest parameter tuning has been successful.
 ﬁrst few epochs Lasso-based regularization loss keeps decreas- ing sparsity γs stays decrease α restart.
 ﬁrst few epochs sparsity γs raise decrease ρ restart.
 ﬁrst few epochs cross-entropy loss keeps increases non-informative level, decrease µ ρ restart.
 EXPERIMENTS CIFAR-10 EXPERIMENT experiment standard image classiﬁcation CIFAR-10 different net- work architectures: ConvNet ResNet-20 (He et
 resize images × zero-pad ×
 pre-process padded images randomly cropping size × ﬂipping, adjusting brightness contrast, standardizing such pixel values have zero mean variance.
 ConvNet reducing channels ConvNet, are interested studying convert over-parameterized network compact one.
 start standard 4-layer convolutional neural network network attributes are speciﬁed Table
 use ﬁxed learning rate µt = scaling parameter α = set batch size
 Model is trained scratch using base model initial warm-up ρ = steps, is trained raising ρ
 termination criterion are met, prune channels base model generate smaller network called model A.
 evaluate classiﬁcation performance model A running exponential average parameters.
 is found test accuracy model A is better base model.
 Next, start pre-trained model create model B raising ρ
 end smaller network called model B, is worse model A, saves third parameters.
 Likewise, start pre-trained model B create model C.
 detailed statistics pruned channel size are reported Table
 train reference ConvNet scratch channel sizes are 32-64-64-128 parameters test accuracy being
 referenced model is good Model B, has smaller number parameters higher accuracy.
 have major observations experiment: (1) base network is over- approach reduce number channels base model improves generalization performance test set.
 (2) Performance degradation seems unavoidable channels network are saturated, approach gives satisfactory trade-off test accuracy model efﬁciency.
 ResNet-20 want verify second observation state-of-art models.
 choose popular ResNet-20 base model CIFAR-10 benchmark, test accuracy is
 focus pruning channels residual modules ResNet-20, has convolutions total.
 detailed Table model is trained scratch using ResNet-20’s network structure base model.
 use warm-up ρ = steps train ρ
 are able remove parameters ResNet-20 percent accuracy loss.
 Likewise, Model B is created model A higher penalty ρ =
 Published conference paper ICLR layer conv1 pool1 conv2 pool2 conv3 pool4 fc param.
 size test accuracy (%) output × × × × × × × kernel × × × × × × × base channel model A model B model C channel channel channel Table Comparisons different pruned networks base network.
 group block 1-1 1-2 1-3 2-1 2-2 2-3 3-1 3-2 3-3 ResNet-20 model A model B channels param size.
 test accuracy (%) = channels param size.
 test accuracy (%) = channels param size.
 test accuracy (%) = Table Comparisons ResNet-20 pruned versions.
 last columns are number channels residual modules pruning.
 ILSVRC2012 EXPERIMENT experiment approach pre-trained ResNet-101 ILSVRC2012 image classiﬁcation dataset (He
 ResNet-101 is state-of-the-art network architecture ImageNet Challenge.
 follow standard pipeline pre-process images training ResNets.
 adopt pre-trained TensorFlow ResNet-101 model single crop error rate is × parameters.
 set scaling parameter α = initial learning rate µt = sparsity penalty ρ batch size GPUs).
 learning rate is decayed epochs rate
 create pruned models different iterations training ResNet-101: one has × parameters other has × parameters.
 ﬁne-tune models using standard way training ResNet-101, report error rates.
 Top-5 error rate increases models are less
 Top-1 error rates are summarized Table
 best knowledge, few work has reported performances large-scale benchmark w.r.t. Top-1 errors.
 compare approach recent work terms models’ parameter size, ﬂops, error rates.
 shown Table model v2 has achieved compression ratio more maintains more lower error rates other state art models comparable size parameters.
 ﬁrst experiment (CIFAR-10), train network scratch allocate enough steps γ W adjusting own scales.
 Thus, initialization improper scale γ-W is issue given optimize enough steps.
 pre-trained models were optimized constraints γ, γs scales are unanticipated.
 takes many steps training scratch γ warm up.
 adopting rescaling trick setting α smaller value, are able skip warm-up stage quick start sparsify γs.
 example, take more hundred epoch train ResNet-101, takes 5-10 epochs complete pruning few more epochs ﬁne-tune.
 Published conference paper ICLR network resnet-101 pruned (v2, ours) resnet-34 pruned (Li param size.
 resnet-50 pruned (Huang Wang, ∼ × × × × × × × resnet-101 pruned (v1, ours) resnet-50 resnet-101 resnet-34 ﬂops × × × × × × × error (%) ∼ ratio Table Attributes different versions ResNet single crop errors ILSVRC2012 benchmark.
 last column means parameter size pruned model
 base model.
 IMAGE FOREGROUND-BACKGROUND SEGMENTATION EXPERIMENT discussed major observations Section appealing scenario is apply approach pruning channels over-parameterized model.
 happens adopts pre-trained network large task (such ImageNet classiﬁcation) ﬁne-tunes model different smaller task (Molchanov
 case, expect channels were useful ﬁrst pre-training task are contributing outputs second task.
 describe image segmentation experiment neural network model is composed inception-like network branch densenet network branch.
 entire network takes image outputs binary mask same size.
 inception branch is used locating foreground objects densenet network branch is used reﬁne boundaries segmented objects.
 model is trained multiple datasets.
 experiment, attempt prune channels inception branch densenet branch.
 set α ρ = µt = batch size
 train pre-trained base model termination criterion are met, build pruned model ﬁne-tuning.
 pruned model saves parameters ﬂops base model.
 compare ﬁne-tuned pruned model pre-trained base model different test benchmark.
 Mean IOU is used evaluation metric3.
 shows pruned improves base model ﬁve test datasets ∼ performs worse base model challenged dataset DUT-Omron, foregrounds contain multiple objects.
 test dataset (#images) MSRA10K (Liu et (2,500) DUT-Omron (Yang et (1,292) Adobe Flickr-portrait (Shen et (150) Adobe Flickr-hp (Shen et (300) COCO-person (Lin et (50) param.
 size ﬂops mIOU mIOU base model pruned × × × × Table mIOU reported different test datasets base model pruned model.
 CONCLUSIONS proposed model pruning technique focuses simplifying computation graph deep convolutional neural networks.
 approach adopts ISTA update γ parameter batch normalization operator embedded convolution.
 accelerate progress model pruning, use γ-W rescaling trick stochastic training.
 method avoids possible numerical difﬁculties such mentioned other regularization based related work, hence Published conference paper ICLR is easier apply practitioners.
 validate method several benchmarks show usefulness competitiveness building compact CNN models.
 REFERENCES Jose M Alvarez Mathieu Salzmann.
 Learning number neurons deep networks.
 Advances Neural Information Processing Systems, pp.

 Sajid Anwar, Kyuyeon Hwang, Wonyong Sung.
 Structured pruning deep convolutional neural networks.
 ACM Journal Emerging Technologies Computing Systems (JETC),
 Amir Beck Marc Teboulle.
 fast iterative shrinkage-thresholding algorithm linear inverse problems.
 SIAM Journal Imaging Sciences,
 Laurent Dinh, Razvan Pascanu, Samy Bengio, Yoshua Bengio.
 Sharp minima generalize deep nets.
 Proceedings International Conference Machine Learning,
 Song Han, Jeff Pool, John Tran, William Dally.
 Learning weights connections efﬁcient neural network.
 Advances Neural Information Processing Systems, pp.

 Babak Hassibi David G Stork.
 Second order derivatives network pruning: Optimal brain surgeon.
 Advances Neural Information Processing Systems, pp.

 Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.
 Deep residual learning image recognition.
 Proceedings IEEE Conference Computer Vision Pattern Recognition, pp.

 Yihui He, Xiangyu Zhang, Jian Sun.
 Channel pruning accelerating deep neural networks.
 Proceedings International Conference Computer Vision,
 Zehao Huang Naiyan Wang.
 Data-driven sparse structure selection deep neural networks.
 arXiv preprint arXiv:1707.01213,
 Sergey Ioffe Christian Szegedy.
 Batch normalization: Accelerating deep network training reducing internal covariate shift.
 International Conference Machine Learning, pp.

 Vadim Lebedev Victor Lempitsky.
 Fast convnets using group-wise brain damage.
 Proceedings IEEE Conference Computer Vision Pattern Recognition, pp.

 Yann LeCun, John S Denker, Sara A Solla.
 Optimal brain damage.
 Advances Neural Information Processing Systems, pp.

 Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, Hans Peter Graf.
 Pruning ﬁlters efﬁcient convnets.
 International Conference Learning Representations,
 Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, C Lawrence Zitnick.
 Microsoft coco: Common objects context.
 European conference computer vision, pp.


 Tie Liu, Zejian Yuan, Jian Sun, Jingdong Wang, Nanning Zheng, Xiaoou Tang, Heung-Yeung Shum.
 Learning detect salient object.
 IEEE Transactions Pattern analysis machine intelligence,
 Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang.
 Learning efﬁcient convolutional networks network slimming.
 arXiv preprint arXiv:1708.06519,
 Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz.
 Pruning convolutional neural net- International Conference Learning Representations, works resource efﬁcient transfer learning.

 Shen, Aaron Hertzmann, Jiaya Jia, Sylvain Paris, Brian Price, Eli Shechtman, Ian Sachs.
 Au- tomatic portrait segmentation image stylization.
 Computer Graphics Forum, volume pp.

 Wiley Online Library,
 Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, Hai Li. Learning structured sparsity deep neural networks.
 Advances Neural Information Processing Systems, pp.

 Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, Ming-Hsuan Yang.
 Saliency detection graph- based ranking.
 Proceedings IEEE conference computer vision pattern recognition, pp.

 Published conference paper ICLR Xiangyu Zhang, Jianhua Zou, Kaiming He, Jian Sun.
 Accelerating deep convolutional networks classiﬁcation detection.
 IEEE Transactions Pattern Analysis Machine Intelligence,
 Hao Zhou, Jose M Alvarez, Fatih Porikli.
 Less is more: Towards compact cnns.
 European Conference Computer Vision, pp.


 Figure Visualization number pruned channels convolution inception branch.
 Colored regions represents number channels kept.
 height bar represents size feature map, width bar represents size channels.
 is observed most channels bottom layers are kept most channels top layers are pruned.

 Systematic reviews are essential various domains summarize evidence multiple sources.
 are used formulation public policies contribute medicine development clinical guidance [6,
 involve searching, screening synthesis research ev- idence multiple sources available form textual documents.
 is critical systematic reviews identify studies relevant review order minimise bias.
 is easy see process be time- consuming demand signiﬁcant human resources [12].
 Researchers have exploited active learning text classiﬁca- tion make process systematic review eﬃcient.
 Active learning is iterative process starts small set labelled studies learns diﬀer- entiate relevant irrelevant studies [10,
 Feature extraction models are essential success active learning.
 [15] proposed multi-view approach encoded documents terms diﬀerent feature vectors such as, words appear title abstract, keywords MeSH terms.
 feature space is used train dif- ferent classiﬁer, ensemble classiﬁers predicts ﬁnal outcome based majority voting.
 Most previous approaches worked studies clinical domain, was shown [10] task identifying relevant studies public health domain resulted poorer performance.
 was argued task iden- tifying relevant studies is challenging domain compared clinical domain due presence doc- uments wide ranging disciplines (e.g., social science, occupational health, education, etc.) public health com- pared speciﬁc studies included clinical systematic reviews [1].
 authors proposed using topic modelling extract topic based features using Latent Dirichlet Alloca- tion [2].
 addition LDA, are variety diﬀerent topic models available machine learning probabilistic latent semantic indexing (pLSI).
 Hashimoto al.
 presented topic detection model improve performance active learning classiﬁer.
 uses neural network model, referred para- graph vectors [8] extract ﬁxed length feature representa- tions documents.
 is argued paragraph vectors encode information contained sequence words.
 opposed topic modelling techniques LDA treat documents bag-of-words [9].
 was shown [8] paragraph vectors compute semantic relatedness diﬀerent textual contents varying lengths.
 LDA other hand treats documents bag-of-words does utilise information stored sequence context words.
 [7] models topics using clustered represen- tation paragraph vectors represents document terms normalized distance cluster centroids.
 work, perform experiments comprehensive dataset reviews derived wide ranging diverse domains.
 perform experiments diﬀerent feature ex- traction techniques including bag-of-words, paragraph vec- tors topic modelling using LDA.
 observe dif- ferent feature extraction methods work diﬀerent do- mains, isolated feature extraction technique works reviews.
 previous works [15, naive active learning algorithm is used.
 naive active learning algo- rithm suﬀers problem retrieving studies are similar retrieved studies.
 happens be- cause active learning classiﬁer does receive enough diverse samples classify correctly.
 [13], authors propose active learning rationals, where, rationals are groups words (phrases) describe label.
 ra- tionals are asked labeler providing label lead extra manual eﬀort.
 addition, such ap- proach work bag words representation documents, paragraph vector based repre- sentation.
 many cases, (including case), get additional data labels extra manual eﬀort involved.
 [14] proposed active learning strategy uses non-linear SVM, training such classiﬁers is time consuming suited active learning clinical text classiﬁcation, system keep labeler waiting documents labelled.
 propose novel active learning algorithm clinical text classiﬁcation reduces bias including novelty addition relevance documents, derive mean- ingful insights results choose feature extraction model particular review.
 contributions be summarized follows: perform experiments use diﬀerent feature extraction models comprehensive set reviews, contrary published results [7] observe larger dataset paragraph vector based mod- elling does work compared bag-of-words and/or simple paragraph vectors based approach active learning systematic reviews.
 • propose novel active learning algorithm re- moves inevitable bias naive algorithms based relevance, develop algorithm early phases uses novelty documents, addition relevance.
 • derive insights experiments per- formance diﬀerent feature extraction models dif- ferent domains, propose eﬀective way choose correct feature extraction model requires prior knowledge domain and/or review.

 METHODS Feature Extraction Bag-of-Words basic oldest feature extraction models text documents is referred bag-of-words.
 bag-of-words model, possible terms entire corpus documents is used construct vocabulary.
 addition simple bag-of-words, terms be weighted tf-idf, measures relative popularity diﬀerent words given document reduces popularity used words corpus.
 Latent Dirichlet Analysis Topic modelling techniques, LDA [2], have been used area systematic reviews extract topical features studies [11].
 Such topical representations extracted documents are used training active learning classiﬁer.
 Paragraph Vectors Recently, neural network model has been proposed learns word vectors paragraph vectors (PV) joint manner [8].
 is contrast approach learn- ing separately.
 Originally, word vectors represented words, paragraph vectors are able represent sequence words form phrases, sentences paragraphs.
 Paragraph vectors have been reported work success previous works [7], are expected encode natural language other topic mod- els.
 have been used measure similarities Wikipedia article research papers [5].
 have been used extract topics studies used systematic reviews [7].
 Topic Modelling using PV [7], technique topic modelling using paragraph vec- tors was proposed.
 is argued paragraph vectors lead better feature extraction learning vector representation words documents.
 Hence, clustering such paragraph vectors lead better topic modelling compared topic models are based bag-of-words representation, LDA [2].
 Clustering Bag-of-Words cluster tf-idf based bag-of-words representa- tion documents obtain topics.
 documents be represented terms cluster-distance matrix.
 is basic diﬀerent topic extraction models was used baseline [7].
 Active Learning active learning process begins small number labelled documents.
 documents are used train classiﬁer be used diﬀerentiate relevant irrelevant studies rest stud- ies.
 studies are ordered decreasing probability relevance top-k studies are reviewed expert reviewer.
 reviewed k studies are used retrain active learning classiﬁer labelled studies.
 is obvious problem naive active learning algorithm.
 classiﬁer get biased to- wards studies are chosen beginning pro- cess.
 Such classiﬁer continue look similar studies based current knowledge.
 lead biased sample studies training set classiﬁer.
 hypothesized including novelty addition rele- vance choosing documents active learning lead overall improvement performance.
 other ap- proaches [13] require additional information labels, work bag-of-words represen- tation documentation.
 comparison, method work bag-of-words distributed represen- tations paragraph vectors).
 Note, best knowledge, naive active learning is only algorithm used success systematic reviews [7,
 Proposed Algorithm solve above mentioned extract topics documents using LDA topic model.
 gives output topic vectors document corpus v(d) ∈ Rk×1.
 topic vectors diﬀerent documents form matrix V ∈ Rn×k, n is number studies cor- pus k is number topics.
 create separate matrix topic vectors documents d have been labelled.
 refer set la- belled documents H, matrix topic vectors documents d ∈ H is referred S ∈ R|H|×k.
 denote set unlabelled documents G, set documents D.
 use principal component analysis compute top-t principal eigen vectors ST S.
 deﬁne probability document being novel as: p(n|d) = − (cid:107)U U T v(d)(cid:107)2 (cid:107)v(d)(cid:107)2 (1) U ∈ Rk×t contains t principal eigen vectors, U U T ∈ Rk×k is subspace formed top-t principal eigen vectors ST S.
 measures novelty doc- ument projecting topic vector principal sub- space formed topic vectors documents training set.
 projection be small document be considered novel vice versa, novel.
 obtain probability given document d being rele- vant p(r|d) classiﬁer.
 compute probability document being relevant novel as: p(r, n|d) = p(r|d) ∗ p(n|d) (2) order documents mentioned p(r, n|d), select top-k documents iterative step active learning algorithm review expert re- viewer.
 continue using novelty document ac- tive learning process have discovered certain ﬁxed number topics.
 assign document d topic i i = arg max vk(d) vk(d) is value kth index topic vector v(d).
 stop incorporating novelty active learning process continue based relevance.
 assume given topic has been discovered doc- ument labelled set H is assigned topic.
 provide formal description above mentioned process Algorithm
 Evaluation Evaluation Method Firstly, evaluate performance active learning classiﬁer using diﬀerent feature extraction methods.
 ex- periment linear-SVM (as used [7, logistic regression classiﬁer observe performance classiﬁers is similar.
 Therefore, use logistic regression classiﬁer models posterior probability study being eligible p(y|d).
 Secondly, evaluate proposed active learning algo- rithm compare naive active learning ap- proach.
 active learning process starts small set labelled studies.
 Features are extracted labelled set using diﬀerent feature extraction models tioned above.
 step iterative process ﬁxed set studies are reviewed expert reviewer.
 extract sample top-k studies iterative step manual labelling ordered list candidate studies [4,
 derive insights experiments help choose correct feature extraction model prior knowledge reviews.
 datasets are labelled expert reviewers.
 simulated human feedback active learning strategy [7,
 evaluation strategy is similar published work [7].
 Parameter Tuning tune diﬀerent parameters PV using cross vali- dation.
 keep number topics LDA [7].
 experimented dimensionality paragraph vec- tors found using dimensional document vectors performed general diﬀerent reviews.
 ex- perimented higher value dimension such (as clf = Classiﬁer() clf = trainClassifier(clf, H) R = getRelevanceScores(clf, G) (cid:46) p(r|d) N = getNoveltyScores(V H, G, t) (cid:46) p(n|d) ∀d ∈ G do scores(d) = R(d) ∗ N (d) Get initial labelled set H = getInitialLabelledSet G = D − H t = max topics = s n Get topical representation docs using LDA V = LDA(D, topic=300) n <max topics do Systematic Reviews Learning procedure Active Learning end procedure function getNoveltyScores(V H, G, t) clf = Classiﬁer() clf = trainClassifier(clf, H) R = getRelevanceScores(clf, G) ∀d ∈ G do end Get top s documents scores G(cid:48) = getTopkDocuments(scores, s) H = H ∪ G(cid:48) G = G − G(cid:48) end Get top s documents scores G(cid:48) = getTopkDocuments(scores, s) H = H ∪ G(cid:48) G = G − G(cid:48) Topics discovered labelled set n = getNumberOfTopicsDiscovered(V, H) N = {} Get top t Eigen Vectors U = getEigenVectors(H, ncomps = t) ∀d ∈ G do (cid:46) p(r|d) end |G| > do scores(d) = R(d) end N (d) = − (cid:107)U U T v(d)(cid:107)2 (cid:107)v(d)(cid:107)2 end return N end function function getNumberOfTopicsDiscovered(V, H) end return |T| end function T = {} ∀d ∈ H do t = arg maxk vk(d) T = T ∪ t [7]), results were better.
 tuned reg- ularization parameter linear classiﬁer, observed results are stable diﬀerent values regularization parameter used value gave (a) Cooking Skills (b) Youth Development (c) Tobacco Packaging (d) FABC (e) CAFO (f) NPA (g) ASCD (h) DPCAD (i) STCS (j) FVC (k) SPCHD (l) LHVS Figure X-axis represents number documents have been annotated Y-axis represents number relevant documents have been discovered.
 observe diﬀerent feature extraction methods work diﬀerent reviews.
 BoW works public health documents PV performs clinical studies.
 infer feature extraction method is superior others documents/domains.
 Dataset Domain Num.
 citations Fraction relevant studies LHVS ASCD FABC DPCAD STCS FVC SPCHD NPA CAFO Cooking Skills Youth Development Tobacco Packaging Clinical Clinical Clinical Clinical Clinical Clinical Clinical Animal Studies Animal Studies Public Health Public Health Public Health Table Statistics systematic review datasets used experiment.
 best results.
 number principal eigen vectors use compute p(n|d) are i.e. t = (line Algo- rithm
 value parameter max topics was set line Algorithm
 denotes number topics be explored stop using novelty criteria active learning algorithm.
 used value s = algorithm.
 small values s implies clas- siﬁer have be learned often, larger value lead selecting few relevant documents beginning, result decrease overall performance.
 Evaluation Metric evaluate performance active learning process using metric called WSS@95.
 stands Work Saved Sampling yield.
 be expressed as: W SS@95 = (1 − burden) | yield ≥ yield = T P M + T P A T P M + T P A + F N A burden = T P M + T N M + T P A + F P A (3) (4) (5) N are studies superscript M A de- note manual automatic screening decisions.
 TP, FP, TN FN stand True Positives, False Positives, True Negatives False Negatives respectively.
 no- tations/deﬁnitions are similar mentioned [7].
 Datasets used number public health, animal study clinical review datasets completed systematic reviews.
 datasets have been used Miwa al.
 [7].
 summarize characteristics diﬀerent datasets used experiments Table
 give short description diﬀerent datasets used: [10] Hashimoto et al.
 • LHVS: Leukodepletion patients undergoing heart valve surgery • ASCD: Amiodarone versus other pharmacological in- terventions prevention sudden cardiac death • FABC: Altering availability proximity products changing selection consumption food, alco- hol tobacco • NPA: Animal studies neuropathic pain • CAFO: Concentrated animal feeding operations • DPCAD: Psychological pharmacological interven- tions depression patients coronary artery disease • STCS: Preoperative statin therapy patients under- going cardiac surgery • FVC: Interventions increasing fruit vegetable consumption children aged years • SPCHD: Internet-based interventions secondary prevention coronary heart disease • Youth Development, Cooking Skills Plain Tobacco Packaging described [7,
 RESULTS investigate performance diﬀerent feature extrac- tion techniques terms relevant studies discovered amount manual annotations required.
 plot num- ber relevant studies identiﬁed given amount man- ual annotation diﬀerent reviews range public health clinical science.
 quantities converge maximum value last iteration ac- tive learning process.
 plot performance diﬀer- ent feature extraction models Figure
 denote modelling based paragraph vectors PV-TM, BoW stands bag-of-words, BoW-TM denotes topic modelling based BoW model LDA refers Latent Dirichlet Allocation.
 see Figure diﬀerent feature extraction methods work diﬀerent studies.
 ob- serve majority cases, paragraph vectors BoW models perform better rest.
 documents pertaining public health, BoW performs well, whereas clinical reviews PV performs well.
 results are con- trary previous results [7], topic modelling based PVs seemed outperform other models disciplines.
 observe BoW PV perform better topic modelling based paragraph vectors most datasets.
 show is possible ﬁnd single feature extraction method performs superior others domains, need identify review.
 use active learning algorithm BoW model, refer IG-BOW (Information Gain- BOW), whereas use paragraph vector model, refer IG-PV (Information Gain-PV).
 observe performance proposed active learning algo- rithm compared naive active learning algorithm screening progresses Figure
 notice pro- posed algorithm explores initial phases screen- ing process, process continues performance improves.
 outperforms naive active learning algorithm end screening process.
 plot WSS@95 (i.e. WSS yield) Figure proposed active learn- ing algorithm.
 addition naive active learning algo- rithm, compare method additional baseline algorithm selects samples classiﬁer is least conﬁdent initial (set using cross val- idation independent dataset) screening i.e. randomly (a) Cooking Skills (b) Youth Development (c) Tobacco Packaging (d) FABC (e) CAFO (f) NPA (g) ASCD (h) DPCAD (i) STCS (j) FVC (k) SPCHD (l) LHVS Figure X-axis represents number documents have been annotated Y-axis represents number relevant documents have been discovered.
 observe proposed active learning algorithm, is IG-PV IG-BOW explore initial phases screening performance improves process continues.
 show results proposed active learning algorithm compared naive active learning algorithm terms WSS@95 Figure
 selects k studies have ≤ p(y = ≤ re- fer baseline LC, consequently, LC-BoW uses bag-of-words features LC-PV uses paragraph vectors features.
 observe two, i.e. IG- BoW IG-PV, performs compared naive active learning algorithm using BoW PV.
 validates hypothesis using novelty explore initial phases active learning lead better results overall, terms WSS@95.
 present BoW PV (as mentioned previous paragraph) observed feature extraction methods per- form most studies.
 mention point used active learning algorithm feature extraction model presented [7], did perform comparable PV BoW.
 did present results PV-TM keep analysis simple sequential.
 has been shown results PV BoW work speciﬁc systematic reviews.
 is feasible estimate certainty advance work review.
 Meanwhile, obtained recall feature extraction methods screening initial data.
 see comparison Table
 report recall initial data denote bold approach scores higher terms WSS@95 end process.
 observe approach has higher recall initial data works terms WSS@95 end screening process.
 result, infer approaches IG- BOW IG-PV be used beginning one performs better be continued.
 mention experimented diﬀerent ensembles feature extraction models, results were impressive compared individual models, omit results.
 Dataset LHVS ASCD FABC NPA CAFO DPCAD STCS FVC SPCHD Cooking Skills Youth Development Tobacco Packaging IG-BOW IG-PV Table Recall screening studies review.
 use bold notation mark overall winner terms WSS@95 end screening process.

 DISCUSSION mention point experi- mented learning paragraph vectors using additional ex- ternal data case public health animal studies.
 expected additional documents lead im- proved paragraph vectors.
 external data consisted studies related review question, most were relevant review.
 surprise, did see improvement results using such external data learning paragraph vectors.
 contrary, performance decreased used ex- ternal data learning paragraph vectors.
 mention conducted experiments more reviews, obtained results similar presented paper.
 observed active learning be eﬀective strat- egy semi-automate manual annotations reduce workload.
 current active learning approaches do estimate proportion relevant studies have been annotated.
 many cases, is necessary extract more relevant studies order avoid bias systematic review.
 was eﬀective way estimate recall active learner screening process move towards complete automation.
 future, work towards accurate recall esti- mation using eﬃcient active learning strategy.

 CONCLUSION evaluated diﬀerent feature extraction models comprehensive dataset reviews varied domains.
 observed BoW PV outperform other ap- proaches certain reviews domains.
 recognized naive active learning algorithm suf- fers bias.
 tries select documents are similar documents training data.
 Initially, training data are small lead reduced performance.
 propose novelty based active learning algorithm works exploring diﬀerent topics initial phases active learning process proceeds based relevance later phases.
 leads exploration diﬀerent topics beginning better performance terms WSS@95 end.
 evaluate approach naive active learning algorithm, observe proposed algorithm works equal to, better naive algorithm instances.
 develop insights regarding choice feature extraction methods diﬀerent reviews.
 observe based experiments large number reviews, feature extraction methods be used screening initial studies.
 better per- forming approach be continued.
 leads extra manual annotations beginning, overall gain terms WSS@95 compensates disadvan- tage.

 acknowledge thanks Annette O’Connor (Iowa State University), Cochrane Heart Group EPPI- Centre (University College London) review team sup- plied data conducting study.
 work was sup- ported grant awarded UK Medical Research Council: Identifying relevant studies systematic reviews health technology assessments using text mining [Grant No. MR/J005037/1].
 James Thomas was (in part) sup- ported National Institute Health Research (NIHR) Collaboration Leadership Applied Health Research Care (CLAHRC) North Thames Bart’s Health NHS Trust.
 views expressed are author(s) Figure ﬁgure plots performance proposed active learning algorithm naive active learning algorithm terms WSS@95.
 use best performing feature extraction models (i.e. BoW PV) comparing proposed active learning algorithm naive algorithm baseline algorithm (LC-Bow/LC-PV).
 observe datasets proposed approach (IG-PV/IG-BoW) is best performing terms WSS@95.
 winners have signiﬁcant lead losers using t-test p
 NHS, NIHR Depart- ment Health.

 REFERENCES [1] C.
 C.
 Beahler, J.
 J.
 Sundheim, N.
 I.
 Trapp.
 Information retrieval systematic reviews: challenges public health arena.
 American journal preventive medicine,
 [2] D.
 M.
 Blei, A.
 Y.
 Ng, M.
 I.
 Jordan.
 Latent dirichlet allocation.
 Journal machine Learning research,
 [3] I.
 Chalmers, L.
 V.
 Hedges, H.
 Cooper.
 brief history research synthesis.
 Evaluation health professions,
 [4] A.
 M.
 Cohen, W.
 R.
 Hersh, K.
 Peterson, P.-Y.
 Yen.
 Reducing workload systematic review preparation using automated citation classiﬁcation.
 Journal American Medical Informatics Association,
 [5] A.
 M.
 Dai, C.
 Olah, Q.
 V.
 Le. Document embedding paragraph vectors.
 arXiv preprint arXiv:1507.07998,
 [6] D.
 Gough, S.
 Oliver, J.
 Thomas.
 introduction systematic reviews.

 [7] K.
 Hashimoto, G.
 Kontonatsios, M.
 Miwa, S.
 Ananiadou.
 Topic detection using paragraph vectors support active learning systematic reviews.
 Journal Biomedical Informatics,
 [8] Q.
 V.
 Le T.
 Mikolov.
 Distributed representations sentences documents.
 ICML, volume pages
 [9] T.
 Mikolov, I.
 Sutskever, K.
 Chen, G.
 S.
 Corrado, J.
 Dean.
 Distributed representations words phrases compositionality.
 Advances neural information processing systems, pages
 [10] M.
 Miwa, J.
 Thomas, A.
 O’Mara-Eves, S.
 Ananiadou.
 Reducing systematic review workload certainty-based screening.
 Journal biomedical informatics,
 [11] Y.
 Mo, G.
 Kontonatsios, S.
 Ananiadou.
 Supporting systematic reviews using lda-based document representations.
 Systematic reviews,
 [12] A.
 O’Mara-Eves, J.
 Thomas, J.
 McNaught, M.
 Miwa, S.
 Ananiadou.
 Using text mining study identiﬁcation systematic reviews: systematic review current approaches.
 Systematic reviews,
 [13] M.
 Sharma, D.
 Zhuang, M.
 Bilgic.
 Active learning rationales text classiﬁcation.

 [14] S.
 Tong D.
 Koller.
 Support vector machine active learning applications text classiﬁcation.
 Journal machine learning research,
 [15] B.
 C.
 Wallace, T.
 A.
 Trikalinos, J.
 Lau, C.
 Brodley, C.
 H.
 Schmid.
 Semi-automated screening biomedical citations systematic reviews.
 BMC bioinformatics,
 0.000.250.500.75ASCDCAFOCooking SkillsDPCADFABCFVCLHVSNPASPCHDSTCSTobacco PackagingYouth DevelopmentDatasetWSS@95method: BOWLC−BoWIG−BOWPVLC−PVIG−PV
 last decade, deep learning has achieved remarkable results computer vision, speech recognition natural language processing, obtaining tasks human-like [1] super-human [2] performance.
 roots recent successes deep learning be found in: (i) increase data available training neural networks, (ii) rising commodity computational power needed crunch data, (iii) development new techniques, archi- tectures, activation functions improve convergence training deeper networks, overcoming obstacle vanishing/exploding gradient [3], [4].
 many years, neural networks have employed logistic sigmoid activation functions.
 acti- vation is affected saturation issues.
 problem reduces effectiveness and, usage feedforward networks is discouraged [5].
 overcome such weakness improve accuracy results, active area research has been devoted design novel activation functions.
 training procedure architectures in- volve optimization weights layers only, non-linearities are pre-speciﬁed (possible) parameters are considered hyper-parameters be tuned manually.
 paper, introduce approaches able learn combinations base activation functions (such identity function, ReLU, tanh) training; aim is identify search space activation functions means convex combination afﬁne combination base functions.
 best knowledge, is ﬁrst attempts combine optimize activation functions training phase.
 tested different well-known networks employing cus- tomary activation functions standard datasets compared results obtained applying novel approaches.
 techniques proposed paper outper- formed baselines experiments, using deep architectures: found percentage points increase top-1 accuracy AlexNet ILSVRC-2012.
 paper is organized follows: Section II related works are summarized; Section III proposed methods are described; Section IV experimental results are pre- sented; ﬁnally, conclusions future works are summarized Section V.
 II.
 RELATED WORK Designing activation functions enable fast training accurate deep neural networks is active area research.
 rectiﬁed linear activation function, introduced [6] argued [7] be better biological model logistic sigmoid activation function, has eased training deep neural networks alleviating problems related weight initialization vanishing gradient.
 Slight variations ReLU have been proposed years, such leaky ReLU (LReLU), [8], addresses dead neuron issues ReLU networks, thresholded ReLU [9], tackles problem large negative biases autoencoders, parametric ReLU (PReLU) [10], treats leakage parameter LReLU per-ﬁlter learnable weight.
 smooth version ReLU, called softplus, has been proposed [11].
 theoretical advantages ReLU (it is differentiable has less saturation issues), activation function does achieve better results [7] compared basic version.
 recently, maxout has been introduced [12] activation function aimed enhancing dropouts abilities model averaging technique.
 extensions, is worth mentioning probabilistic maxout [13], Lp norm pooling activation [14] is able recover maxout activation p → ∞.
 Considering last developments activation functions neural networks, is important mention exponential linear unit function (ELU) [15] scaled exponential linear unit function (SELU) [16].
 ReLU, LReLU, PReLU, ELU reduces vanishing gradient problem.
 Fur- thermore, ELU has negative values, allowing push mean unit activations batch normalization, speeding learning.
 SELU extends property ensuring activations mean unit variance are propagated many network layers converge towards mean unit presence noise perturbations.
 exception PReLU, previous activations are pre-speciﬁed non-learnable).
 ﬁrst attempt learn activations neural network be found [17], authors propose randomly add remove logistic Gaussian activation functions using evolutionary program- ming method.
 other hand, [18]–[21] authors proposed novel approaches learn best activation function neuron pool allowed activations means genetic evolutionary algorithms.
 different method has been proposed [22], is able learn hardness parameter sigmoid function, approach employed PReLU learn leakage parameter.
 However, previous learning techniques are limited fact family functions learning takes place is ﬁnite simple parameterization customary activation functions.
 Recently, [23] authors tackle problem different angle using piecewise linear activation functions are learned neuron using gradient descent.
 However, (i) number linear pieces is treated hyper-parameter optimization problem; (ii) number learned parameters increases amount hidden units; (iii) learned piecewise linear activation functions are ReLU(x) x large exists u ∈ R such g(x) = ReLU(x) x ≥ u), reducing expressivity learned activation functions design.
 is worth mentioning that, [24] authors propose network network approach replace activation functions convolutional layers small multi-layer per- ceptrons.
 paper try overcome limitations aforementioned approaches.
 techniques explained Section III: (i) increase expressivity power learned activation functions enlarging hypothesis space explored training respect [17]–[22]; (ii) restrict hypothesis space respect [23], [24], order allow training need careful initialization network weights (see Proposition following lines).
 III.
 METHODS A neural network Nd made d hidden layers be seen functional composition d functions Li followed ﬁnal mapping ¯L depends task hand (e.g. classi- ﬁcation, regression): Nd = ¯L◦ Ld◦

 .◦ L1.
 particular, hidden layer function Li be written composition functions, gi followed σi, former being suitable remapping layer input latter being activation function layer: Li = σi ◦ gi.
 general case, σi gi are parameterized belongs hypothesis spaces Hσi Hgi, respectively.
 Hence, learning procedure Li amounts optimization problem layer hypothesis space Hi = Hσi × Hgi.
 Usually, σi is taken non-learnable function; therefore, common scenario Hσi is singleton: Hi = {σi} × Hgi.
 example, fully-connected layer Rni Rmi ReLU activation have Hgi is set afﬁne transformations Rni Rmi, Hi = {ReLU}× Lin(Rni, Rmi) × K(Rmi), Lin(A, B) K(B) are sets linear maps A B, set translations B, respectively.
 paper, introduce techniques deﬁne learn- able activation functions be plugged hidden layers neural network architecture.
 approaches differ deﬁne hypothesis space Hσi.
 are based following idea: (i) select ﬁnite set activation functions F := {f1,


 fN}, elements be used base elements; (ii) deﬁne learnable activation function σi linear combination elements F; (iii) identify suitable hypothesis space Hσi; (iv) whole network, hypothesis space hidden layer is Hi = Hσi × Hgi.
 give basic deﬁnitions used paper.
 Note that, hereinafter, activation functions R R extend functions Rn Rn means entrywise application.
 Given vector space V ﬁnite subset A ⊆ V deﬁne following subsets V (i) convex hull A, conv(A) := {(cid:80) aﬀ(A) := {(cid:80) i ciai |(cid:80) i ciai |(cid:80) (ii) afﬁne hull A, namely: i ci ci ≥ ai ∈ A}; i ci ai ∈ A}.
 remark that, conv(A) aﬀ(A) are vector sub- spaces V
 Indeed, conv(A) is generic convex subset V reducing (|A| − 1)-dimensional simplex elements are linearly independent.
 other hand, aﬀ(A) is afﬁne subspace V dimension |A| − i.e. arbitrary ¯a ∈ aﬀ(A) set {a − ¯a | ∈ aﬀ(A)} is linear subspace V dimension |A| −
 Clearly, conv(A) ⊂ aﬀ(A).
 Let F := {f0, f1,


 be ﬁnite collection activa- tion functions fi R R.
 deﬁne vector space i cifi.
 Note that, F is (by deﬁnition) spanning set F is basis; |F| ≥ dim F
 (almost everywhere) differentiability is property preserved ﬁnite linear combinations, conv(F ⊂ aﬀ(F ⊆ F assuming F contains differentiable activation functions, conv(F aﬀ(F are made (almost differentiable functions, i.e. valid F F taking linear combinations (cid:80) implies ¯f =(cid:80) i cifi with(cid:80) (⇐) hypothesis, ¯f = (cid:80) ¯f (0) =(cid:80) i cifi (cid:80) i cifi(0) ¯f(cid:48)(0) =(cid:80) i ci ¯f ∈ aﬀ(F ).
 i ci
 Hence, i ci i (0) =(cid:80) i cif(cid:48) ¯f approximates identity origin.
 conv(F ⊂ aﬀ(F ), conv(F enjoys same property.
 is important underline that, activation functions approximating identity origin, have been argued be desirable train deep networks initialized weights [26].
 Indeed, such activation function initial gradients be large, speeding training phase.
 Note that, such behavior is enjoyed ReLU, approximates identity side: i.e. x →
 Furthermore, training deep networks activation functions approximating identity origin requires careful initialization, order properly layers input [27].
 aforementioned reasons: (i) preserved differen- tiability; (ii) absence requirement monotonicity; (iii) approximation identity origin; aﬀ(F conv(F are good candidates Hσi.
 Thanks previous deﬁnitions aforementioned properties, formalize techniques build learnable activation functions follows: (i) choose ﬁnite set F = {f1,


 fN}, fi is (almost everywhere) differentiable activation function approximating identity origin (at least side); fi ∈ F; conv(F ).
 (ii) deﬁne new activation function ¯f linear combination sets aﬀ(F (iii) select hypothesis space H ¯f Section IV, present results using following choices F: F := {id, tanh}, conv(F ⊂ aﬀ(F ), F := {id, ReLU}, F := {ReLU, tanh}, F := {id, ReLU, tanh}, (1) id is identity function.
 Clearly, other choices F be provided requirements (i) are satisﬁed.
 convex hull-based tech- nique be understood regularized version afﬁne hull-based one, corresponding hypothesis space has been constrained be compact.
 Such regularization, addition restrict complexity hypothesis space, guarantees ﬁnal activation function is monotonic (provided fi ∈ F are monotonic well).
 Moreover, convex hull-based technique F := {id, ReLU,} recovers learnable LReLU activation func- tion, i.e. LReLUα(x) = x x ≥ LReLUα(x) = αx otherwise, ≤ α (cid:28) α 10-2).
 Indeed, conv(F ¯f := p · id +(1 − p) · ReLU ≤ p ≤ ReLU = id x ≥ ReLU = otherwise, have ¯f = p · id +(1 − p) · id = id x ≥ ¯f = p · id +(1 − p) · = p · id otherwise, LReLUp. is worth mentioning that, shown Figure layers using convex hull-based afﬁne hull-based activations Fig.

 ﬁgure shows relationship afﬁne hull, convex hull, convex cone set F made linearly independent elements: F := {f1, f2, f3}.
 dim F = |F| = conv(F is 2-simplex (the gray triangle ﬁgure), aﬀ(F is plane dimensional vector space F (the plane identiﬁed dashed circle ﬁgure).
 cone(F is dimensional manifold delimited incident straight lines {x ∈ F | x = αfi, ∀0 ≤ α ∈ R} i = i.e. cone extremal rays.
 be seen conv(F corresponds intersection cone(F aﬀ(F ).
 activation functions neural network be learned means gradient descent.
 activations used real world scenarios are monotonic increasing functions.
 monotonic- ity is ensured arbitrary linear combination, meaning fi ∈ F are non-decreasing arbitrary element ¯f ∈ F be non-decreasing non-increasing function.
 matter fact, considering i ≥ ∀fi ∈ F), non-decreasing differentiable functions (f(cid:48) non-decreasing differentiable functions F lie convex cone cone(F ⊂ F i.e.: cone(F := {(cid:80) i cifi | ci ≥ fi ∈ F}.
 Indeed, ∀g ∈ cone(F have g(cid:48)
 Thanks deﬁnition aﬀ(F ), cone(F ), conv(F conclude conv(F = cone(F ∩ aﬀ(F ), implies monotonicity elements F is preserved elements conv(F aﬀ(F (see
 Nevertheless, [25] is shown non-monotonic activation functions approximate complex func- tions large neural networks.
 Indeed, [5] authors trained feedforward network using cosine activation functions MNIST dataset obtaining error rate smaller
 Therefore, is proper candidate Hσi.
 afﬁne subspace aﬀ(F enjoys following property.
 Proposition
 Let fi ∈ F be linearly independent approximate identity function origin (i.e. fi(0) = i (0) then, ¯f ∈ F approximates identity f(cid:48) ¯f ∈ aﬀ(F ).
 Proof.
 proof is immediate.
 Let expand F (cid:51) ¯f ¯f = i cifi (by hypothesis fi form basis) prove two-way implication.
 (⇒) hypothesis, ¯f (0) = ¯f(cid:48)(0) =
 relation i ci
 turns, (cid:80) derivative reads(cid:80) i (0) =(cid:80) i cif(cid:48) TABLE KE R A SNE T ARCHITECTURE Id conv_1 conv_2 conv_3 conv_4 fc_5 fc_6 Layers convolution Activation convolution Activation Max pooling Dropout convolution Activation convolution Activation Max pooling Dropout Fully connected Activation Dropout Fully connected Activation Properties × (3, × (3, (2, × (3, × (3, (2, softmax pixel value augmented resulting dataset training phase means random horizontal ﬂip image shifting; ILSVRC-2012: classes classiﬁcation task training examples validation examples.
 examples are colour images various sizes [30].
 resized image pixels, subtracted pixel mean values, augmented dataset training phase cropping pixels ﬂipping images.
 Note that, did train network relighting data-augmentation proposed [31].
 considered architectures are following: LeNet-5: convolutional network made convolu- tional layers followed connected layers [32].
 convolutional layers have ﬁlters size hidden connected layer is made neurons.
 Max pooling size is used convolutional layer, dropout.
 assessed LeNet-5 Fashion-MNIST CIFAR-10 datasets, resulting networks parameters, respectively; KerasNet: convolutional neural network included Keras framework [33].
 is made convolutional layers connected layers, employs max pooling dropout.
 architecture is presented Ta- ble I.
 tested KerasNet Fashion-MNIST CIFAR-10 datasets, resulting networks parameters, respectively; ResNet-56: residual network made total layers, employing pooling skip connection [34].
 performance has been evaluated CIFAR-10, corresponding network parameters; AlexNet: convolutional network made convolutional connected layers [31].
 tested ILSVRC-2012 dataset, resulting network pa- rameters.
 Note that, shown [31], ReLU-based activation functions outperform networks based other activations.
 Therefore, context ReLU-based networks Fig.

 ﬁgure shows neural network layer convex/afﬁne hull- based activation be seen two-stage pipeline.
 k-th layer is neurons layer, set F composed base functions.
 ﬁrst stage pipeline is made stacked featuring activation belonging F.
 weights stacked layers are shared.
 second stage pipeline is 1D-convolution kernel size stacked layers, play role n channels convolution.
 weights convolution are constrained sum one, be positive using convex hull-based technique.
 n base functions be seen following two-stage pipeline: (i) n stacked connected layers, featuring base functions, sharing weights; 1D-convolution kernel size n stacked layers (which are treated convolution n separate channels), weights are constrained sum (and be positive case convex hull-based technique)1.
 IV.
 RESULTS tested convex hull-based afﬁne hull-based approaches evaluating effectiveness pub- licly available datasets used image classiﬁcation, differing number input features examples.
 Moreover, dataset was evaluated using different network architectures.
 networks were trained tested using activation functions (for hidden layers) learned convex hull-based afﬁne hull-based approaches combining base activations reported Equation (1).
 base activation functions LReLU were employed order compare overall performance.
 Speciﬁcally, datasets used Fashion-MNIST: is dataset Zalando’s article im- ages, composed training test sets examples, respectively.
 example is 28x28 grayscale image, associated label belongs classes [28].
 divided pixel value augmented resulting dataset training phase means random horizontal ﬂip image shifting; CIFAR-10: is balanced dataset colour images belonging classes.
 are training images test images [29].
 case divided 1D-convolution kernel size be seen weighted average stacked connected layers (with negative weights case afﬁne hull-based technique).
 Fig.

 ﬁgure shows hidden afﬁne hull-based convex hull-based activation functions learned training KerasNet architecture CIFAR-10 dataset.
 Activations learned aﬀ({id, ReLU, tanh}) are represented solid lines, learned conv({id, ReLU, tanh}) are dashed.
 have been built tested comparison.
 networks were implemented scratch using Keras framework [33] top TensorFlow [35].
 Notice that, AlexNet implementation is porting Keras Caffe architecture2.
 trained LeNet-5, KerasNet, ResNet-56 using RMSprop learning rate 10-4 learning rate decay mini-batch update 10-6.
 AlexNet used SGD starting learning rate 10-2, step- wise learning rate decay, weight-decay hyper-parameter 5·10-4, Table II shows top-1 accuracy run exper- iments.
 best conﬁgurations (shaded cells table) are achieved using techniques, experiments afﬁne hull-based approach outperformed convex hull-based ones.
 uplift top-1 accuracy using approaches compared customary activations goes percentage points (pp) LeNet-5 Fashion-MNIST pp KerasNet CIFAR-10.
 is worth mentioning that, deep neural networks, such AlexNet, substantial increase is observed, i.e. pp ILSVRC-2012.
 Moreover, proposed techniques achieve bet- ter results corresponding base activation functions (boldface table).
 Note novel activations work deep networks various architectures involving different types layer (e.g. residual unit, dropout, pooling, convolutional, connected).
 Furthermore, experiments show learning leakage parameter achieved activation based conv({id, ReLU}) allows outperform achieve same results LReLU.
 Figure shows activations learned KerasNet CIFAR-10 using convex hull-based afﬁne hull- based activations F = {id, ReLU, tanh}.
 is possible notice proved Section III, convex combinations preserved monotonicity base ac- tivation functions afﬁne ones did
 https://github.com/BVLC/caffe/tree/master/models/bvlc alexnet.
 V.
 CONCLUSION paper introduced novel techniques able learn new activations starting ﬁnite collection F base functions.
 ideas are based building arbitrary linear combination elements F deﬁning suitable hypothesis space learning procedure linear combination takes place.
 hypothesis spaces techniques are conv(F ).
 showed that, provided elements F approximate identity origin, aﬀ(F is set is possible ﬁnd combined activations approximate id origin.
 Moreover, aﬀ(F allows explore non-monotonic activation functions, conv({id, ReLU}) be seen learnable LReLU activation function.
 tested techniques various architec- tures (LeNet-5, KerasNet, ResNet-56, AlexNet) datasets (Fashion-MNIST, CIFAR-10, ILSVRC-2012), comparing results LReLU single base activation functions.
 experiments, techniques proposed paper achieved best performance combined activation functions learned using approaches outperform corresponding base components.
 effectiveness proposed techniques is further proved increase performance achieved using networks different depths architectures.
 be analyze other sets base functions.
 opinion, interesting extension work REFERENCES [1] W.
 Xiong, J.
 Droppo, X.
 Huang, F.
 Seide, M.
 Seltzer, A.
 Stolcke, D.
 Yu, G.
 Zweig, “Achieving human parity conversational speech recognition,” arXiv preprint arXiv:1610.05256,
 [2] D.
 C.
 Ciresan, U.
 Meier, J.
 Masci, J.
 Schmidhuber, “A committee IEEE, neural networks trafﬁc sign classiﬁcation.” IJCNN.
 pp.

 [3] Y.
 Bengio, P.
 Simard, P.
 Frasconi, “Learning long-term dependencies gradient descent is difﬁcult,” IEEE transactions neural networks, vol.
 no.
 pp.

 [4] X.
 Glorot Y.
 Bengio, “Understanding difﬁculty training deep feedforward neural networks,” Proceedings Thirteenth International Conference Artiﬁcial Intelligence Statistics, pp.

 [5] I.
 Goodfellow, Y.
 Bengio, A.
 Courville, Deep Learning.
 MIT Press,
 EXPERIMENT RESULTS.
 TABLE SHOWS THE TOP-1 ACCURACY RESULTS FOR ALL THE ANALYZED NETWORKS ON FA S H O N-MNIST TEST SET, CIFAR-10 TEST SET, AND ILSVRC-2012 VALIDATION SET.
 CONVEX HULL-BASED AND AFFINE HULL-BASED ACTIVATIONS ACHIEVING TOP-1 ACCURACY RESULTS GREATER THAN THEIR CORRESPONDING BASE ACTIVATION FUNCTIONS ARE HIGHLIGHTED IN BOLDFACE.
 BEST RESULT FOR EACH NETWORK/DATASET IS SHADED.
 TABLE II Activation id ReLU tanh LReLU conv({id, ReLU}) conv({id, tanh}) conv({tanh, ReLU}) conv({id, ReLU, tanh}) aﬀ({id, ReLU}) aﬀ({id, tanh}) aﬀ({tanh, ReLU}) aﬀ({id, ReLU, tanh}) Fashion-MNIST CIFAR-10 ILSVRC-2012 LeNet-5 KerasNet LeNet-5 KerasNet ResNet-56 AlexNet [6] K.
 Jarrett, K.
 Kavukcuoglu, Y.
 LeCun al., is best multi- stage architecture object recognition?” Computer Vision, IEEE International Conference on.
 IEEE, pp.

 [7] X.
 Glorot, A.
 Bordes, Y.
 Bengio, “Deep sparse rectiﬁer neural networks,” Proceedings Fourteenth International Conference Artiﬁcial Intelligence Statistics, pp.

 [8] A.
 L.
 Maas, A.
 Y.
 Hannun, A.
 Y.
 Ng, “Rectiﬁer Nonlinearities Improve Neural Network Acoustic Models,” ICML Workshop Deep Learning Audio, Speech Language Processing,
 [9] K.
 Konda, R.
 Memisevic, D.
 Krueger, “Zero-bias autoencoders beneﬁts co-adapting features,” arXiv preprint arXiv:1402.3337,
 [10] K.
 He, X.
 Zhang, S.
 Ren, J.
 Sun, “Delving rectiﬁers: Surpassing human-level performance imagenet classiﬁcation,” Proceedings IEEE International Conference Computer Vision (ICCV), ser.
 ICCV ’15.
 Washington, DC, USA: IEEE Computer Society, pp.

 [Online].
 Available: http://dx.doi.org/10.1109/ICCV.2015.123 [11] C.
 Dugas, Y.
 Bengio, F.
 B´elisle, C.
 Nadeau, R.
 Garcia, “Incorpo- rating second-order functional knowledge better option pricing,” Advances neural information processing systems, pp.

 [12] I.
 J.
 Goodfellow, D.
 Warde-Farley, M.
 Mirza, A.
 Courville, Y.
 Ben- gio, “Maxout networks,” Proceedings 30th International Conference International Conference Machine Learning-Volume
 JMLR.
 pp.
 III–1319.
 [13] W.
 Sun, F.
 Su, L.
 Wang, “Improving deep neural networks multilayer maxout networks,” IEEE Visual Communications Image Processing Conference, VCIP pp.
 dec
 [14] C.
 Gulcehre, K.
 Cho, R.
 Pascanu, Y.
 Bengio, “Learned-norm pooling deep feedforward recurrent neural networks,” Joint European Conference Machine Learning Knowledge Discovery Databases.
 Springer, pp.

 D.-A.
 Clevert, T.
 Unterthiner, S.
 Hochreiter, “Fast Accurate Deep Network Learning Exponential Linear Units (ELUs),” Pro- ceedings ICLR nov
 [16] G.
 Klambauer, T.
 Unterthiner, A.
 Mayr, S.
 Hochreiter, “Self- normalizing neural networks,” Advances neural information pro- cessing systems,
 [17] Y.
 Liu X.
 Yao, “Evolutionary design artiﬁcial neural networks different nodes,” Evolutionary Computation, Proceedings IEEE International Conference on.
 IEEE, pp.

 [18] R.
 Poli, “Parallel distributed genetic programming, new ideas opti- mization,”
 [19] D.
 Weingaertner, V.
 K.
 Tatai, R.
 R.
 Gudwin, F.
 J.
 Von Zuben, “Hierarchical evolution heterogeneous neural networks,” Evolu- tionary Computation,
 CEC’02.
 Proceedings Congress on, vol.

 IEEE, pp.

 [20] A.
 J.
 Turner J.
 F.
 Miller, “Cartesian genetic programming encoded artiﬁcial neural comparison using benchmarks,” Proceedings 15th annual conference Genetic evolutionary computation.
 pp.

 [21] M.
 M.
 Khan, A.
 M.
 Ahmad, G.
 M.
 Khan, J.
 F.
 Miller, “Fast learning neural networks using cartesian genetic programming,” Neurocomputing, vol.
 pp.

 [22] A.
 J.
 Turner J.
 F.
 Miller, “Neuroevolution: evolving heterogeneous artiﬁcial neural networks,” Evolutionary Intelligence, vol.
 no.
 pp.

 [23] F.
 Agostinelli, M.
 Hoffman, P.
 Sadowski, P.
 Baldi, “Learning acti- vation functions improve deep neural networks,” ICLR Workshop,
 [24] M.
 Lin, Q.
 Chen, S.
 Yan, “Network network,” arXiv preprint arXiv:1312.4400,
 [25] G.
 Cybenko, “Approximation superpositions sigmoidal function,” Mathematics Control, Signals, Systems (MCSS), vol.
 no.
 pp.

 [26] H.
 H.
 Aghdam E.
 J.
 Heravi, Guide Convolutional Neural Networks.

 [27] D.
 Sussillo L.
 Abbott, “Random walk initialization training deep feedforward networks,” arXiv preprint arXiv:1412.6558,
 [28] H.
 Xiao, K.
 Rasul, R.
 Vollgraf.
 (2017) Fashion-MNIST: novel image dataset benchmarking machine learning algorithms.
 [29] A.
 Krizhevsky G.
 Hinton, “Learning multiple layers features tiny images,” Technical report, University Toronto, Tech.

 [30] O.
 Russakovsky, J.
 Deng, H.
 Su, J.
 Krause, S.
 Satheesh, S.
 Ma, Z.
 Huang, A.
 Karpathy, A.
 Khosla, M.
 Bernstein, A.
 C.
 Berg, L.
 Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,” International Journal Computer Vision (IJCV), vol.
 no.
 pp.

 [31] A.
 Krizhevsky, I.
 Sutskever, G.
 E.
 Hinton, “Imagenet classiﬁcation deep convolutional neural networks,” Advances neural infor- mation processing systems, pp.

 [32] Y.
 LeCun, L.
 Bottou, Y.
 Bengio, P.
 Haffner, “Gradient-based learning applied document recognition,” Proceedings IEEE, vol.
 no.
 pp.

 [33] F.
 Chollet al., “Keras,” https://github.com/fchollet/keras,
 [34] K.
 He, X.
 Zhang, S.
 Ren, J.
 Sun, “Deep residual learning image recognition,” Proceedings IEEE conference computer vision pattern recognition, pp.

 Irving, M.
 [35] M.
 Abadi, A.
 Agarwal, P.
 Barham, E.
 Brevdo, Z.
 Chen, C.
 Citro, G.
 S.
 Corrado, A.
 Davis, J.
 Dean, M.
 Devin, S.
 Ghemawat, I.
 Goodfellow, A.
 Harp, G.
 Isard, Y.
 Jia, R.
 Jozefowicz, L.
 Kaiser, M.
 Kudlur, J.
 Levenberg, D.
 Man´e, R.
 Monga, S.
 Moore, D.
 Murray, C.
 Olah, M.
 Schuster, J.
 Shlens, B.
 Steiner, I.
 Sutskever, K.
 Talwar, P.
 Tucker, V.
 Vanhoucke, V.
 Vasudevan, F.
 Vi´egas, O.
 Vinyals, P.
 Warden, M.
 Wattenberg, M.
 Wicke, Y.
 Yu, X.
 Zheng, “TensorFlow: Large-scale machine learning heterogeneous systems,” software available tensorﬂow.org.
 [Online].
 Available: https://www.tensorﬂow.org/
 Clustering is subjective [Caruana von Luxburg et single dataset be clus- tered multiple ways, different users prefer differ- ent clusterings.
 subjectivity is motivations constraint-based (or semi-supervised) clustering [Wagstaff et al., Bilenko
 Methods setting ex- ploit background knowledge obtain clusterings are aligned user’s preferences.
 knowl- edge is given form pairwise constraints indicate instances be same cluster (a must- link constraint) (a cannot-link constraint) [Wagstaff et
 traditional constraint-based clustering systems set constraints is assumed be given priori, practice, pairs are queried are selected ran- domly.
 contrast, active clustering [Basu et Mallapragada et Xiong al., is method decides pairs query.
 Typically, active methods are informative random ones, improves clustering quality.
 work introduces active constraint-based clustering method named Constraint-Based Repeated Aggregation BRA).
 differs existing approaches several ways.
 aims exploit constraint transitivity entailment [Wagstaff properties allow deriving additional constraints given set constraints.
 doing actual number pairwise constraints COBRA works is much larger num- ber pairwise constraints are queried user.
 Secondly, COBRA introduces assumption ex- ist small local regions data are grouped potential clusterings.
 clarify this, consider exam- ple clustering images people taking different poses (e.g. facing left right).
 are least natural cluster- ing targets data: want cluster based identity pose.
 appropriate feature space, expects images agree criteria (i.e. single person, tak- ing single pose) be close.
 is need consider instances individually, end same cluster targets user be interested in.
 COBRA aims group such instances super-instance, such be treated clustering process.
 Doing reduces num- ber pairwise queries.
 Thirdly, COBRA is ac- tive method: constraints are selected execution algorithm itself, constraint selection algorithm execution are intertwined.
 contrast, existing approaches consist component selects constraints one uses clustering.
 experiments show COBRA outperforms state-of- the-art active clustering methods terms clustering quality runtime.
 Furthermore, has distinct advan- tage does require knowing number clusters beforehand, competitors do.
 many realistic clus- tering scenarios number is known, running algorithm wrong number clusters results signiﬁcant decrease clustering quality.
 discuss related work (active) constraint-based clus- tering section
 section elaborate key ideas COBRA describe method detail.
 present experimental evaluation section conclude sec- tion
 Background Related Work Most existing constraint-based methods are extensions well-known unsupervised clustering algorithms.
 use constraints adapted clustering procedure [Wagstaff et Rangapuram Hein, Wang et learn similarity metric [Xing et Davis al., [Bilenko Basu
 Constraint-based extensions have been developed most clustering algorithms, including K-means [Wagstaff et Bilenko spectral clustering [Rangapu- ram Hein, Wang et DBSCAN [Lelis Sander, Campello al., EM [Shental et
 Basu al.
 [2004a] introduce strategy select informative constraints, performing single run constraint-based clustering algorithm.
 show ac- tive constraint selection improve clustering performance.
 Several selection strategies have been proposed [Mal- lapragada et al., Xu Xiong al., most are based classic approach uncer- tainty sampling.
 COBRA chooses pairs query, consider be active method, ex- periments compare other methods setting.
 Note, however, COBRA is different existing meth- ods active constrained clustering active learning general.
 COBRA, selecting pairs query is inher- ent clustering procedure, whereas most other meth- ods selection strategy is optional considered be separate component.
 core, COBRA is related hierarchical clustering follows same procedure trying merge closest clusters.
 Constraints have been used hier- archical clustering different ways.
 Davidson al.
 [2009], example, present algorithm ﬁnd cluster- ing hierarchy is consistent given set constraints.
 Nogueira al.
 [2012] propose active semi-supervised hi- erarchical clustering algorithm is based merge con- ﬁdence.
 related ours is work Campello al.
 [2013], have developed framework extract given hierarchy ﬂat clustering is consistent given set constraints.
 key difference is COBRA starts super-instances, i.e. small clusters produced K- means, merging decision is settled pairwise constraint.
 idea working small number representatives (in case super-instance medoids, be discussed section individual instances has been used before, different purposes.
 example, Yan et al.
 [2009] use speed unsupervised spectral clustering, use reduce number pairwise queries.
 Constraint-Based Repeated Aggregation Constraint-based clustering algorithms aim produce clus- tering dataset resembles unknown target cluster- ing Y possible.
 algorithm cannot query cluster labels Y directly, query relation pairs instances.
 must-link constraint is obtained instances have same cluster label Y cannot-link con- straint otherwise.
 aim is produce clustering is target clustering Y using few pairwise queries possible.
 Several strategies be used exploit constraints clus- tering.
 Figure illustrates them.
 naive strategy is query pairwise relations, construct clus- ters sets instances are connected must-link con- straint (Figure
 is good strategy scenario, allows formulate baseline further improvements.
 results perfect clustering, high cost: dataset N instances,(cid:0)N (cid:1) questions are asked.
 previous strategy be improved exploiting con- straint transitivity entailment, known prop- erties constraint-based clustering [Wagstaff et Bilenko
 Must-link constraints are known be transitive: must-link(A, B) ∧ must-link(B, C) ⇒ must-link(A, C), cannot-link constraints have entailment property: must-link(A, B) ∧ cannot-link(B, C) ⇒ cannot-link(A, C).
 Thus, time constraint is queried added set constraints, transitivity entailment be applied ex- pand set.
 strategy is illustrated Figure
 Exploit- ing transitivity entailment reduces num- ber pairwise queries needed obtain clustering, loss clustering quality.
 order constraints are queried inﬂu- ences number constraints be derived.
 gen- eral, is obtain must-link constraints early on.
 way, future query involving instances nected must-link applies others.
 suggests querying closest pairs are likely be- same cluster be connected must- link constraint.
 is strategy is illustrated Figure
 previous strategies obtain perfect clustering, require high number queries makes inap- plicable sized datasets.
 reduce number queries, COBRA groups similar instances super-instances clusters representatives, i.e. medoids.
 assumes instances super-instance are connected must-link constraint.
 clustering medoids, COBRA uses discussed strategies querying closest pairs exploiting transitivity entailment.
 strategy, illustrated Figure results substantial reduction number queries.
 does result perfect clustering is possible instances particular super-instance be grouped target clustering.
 Table illustrates extent improvements described reduces number queries.
 perform extensive evaluation quality clusterings COBRA produces section
 Algorithmic Description presenting main motivations step CO- BRA, give more detailed description Algorithm (a) (b) (c) (d) Figure Illustration different querying strategies.
 Colors indicate desired clustering.
 Solid green lines indicate must-link, red ones cannot-link constraints.
 Dashed lines indicate derived constraints same color code.
 number next solid line indicates ordering queried constraints, number next dashed line indicates constraint number constraint was derived.
 (a) Querying constraints (b) Exploiting entailment transitivity results querying constraints.
 (c) Querying closest pairs ﬁrst results constraints.
 (d) Introducing super-instances (dashed ellipses) results queries.
 dataset instances total pairs queries queries queries iris wine dermatology hepatitis ecoli transitivity −−−−−→ entailment closest pairs −−−−−−→ ﬁrst super−−−−→ instances Table table shows total number pairwise relations several datasets (column number pairwise queries is required exploiting transitivity entailment (column numbers are averages runs random orderings pairwise queries), querying closest pairs (column
 last column shows number pairwise queries (averaged runs) COBRA is run super-instances.
 (cid:83) disjoint subsets {C1,


 ,CNC} S ((cid:83) i=1, xi ∈ Rm instances be clus-
 Let X = {xi}N tered.
 set instances X is ﬁrst over-clustered NS disjoint subsets, super-instances {Si}NS i=1 such i Si = X
 over-clustering is obtained running K- means K be larger ac- tual number clusters.
 super-instance Si is represented medoid si, forming set super-instance representa- tives S = {s1,


 sNS}.
 pairwise queries are per- formed are super-instance representatives.
 goal COBRA is cluster representatives cluster Ci is set super-instance representatives, contains points corresponding super-instances.
 number clusters NC is unknown priori be determined clustering procedure.
 Initially, are NS clusters, containing single super- instance representative, shown line Algorithm
 clusters are merged (if necessary) subsequent loop.
 i Ci = S).
 iteration, COBRA ﬁrst sorts pairs clusters is cannot-link constraint (line
 distance clusters, pairs are sorted, is deﬁned follows (as single-linkage clustering): d(C1,C2) = min s1∈C1,s2∈C2 (cid:107)s1 − s2(cid:107)2 (1) Next, COBRA loops pairs clusters checks be merged.
 starts selecting clos- est pair super-instance representatives clusters (line asks be same cluster (line
 is case, clusters are merged (lines while-loop is restarted set clusters is changed).
 is case, pair representatives is added set cannot-link constraints (line inner loop continues inspecting next pair clusters.
 execution stops clusters are complete merge is be done anymore.
 Number Super-instances Number Queries exact number queries COBRA needs depends extent querying closest pairs leads must- link constraints actual number clusters.
 is difﬁcult determine execution.
 However, estimate terms lower upper bound be posed: NS number super-instances, NC number clusters target clustering.
 worst case, CO- BRA need query pairwise relations super- NS − NC +(cid:0)NC instances, requires(cid:0)NS tice, COBRA needs much less than(cid:0)NS BRA needs NS−NC +(cid:0)NC NC clusters, followed additional(cid:0)NC (cid:1) (cid:46) queries ≤(cid:0)NS queries.
 happens (cid:1) queries.
 (cid:1) queries.
 NS−NC must- (cid:1) queries (i) super-instances are homogeneous tar- get clustering (ii) distances must-link pairs are smaller distances cannot-link pairs, CO- link constraints are needed merge NS super-instances is cannot-link pair super-instances.
 prac- ensure nothing be merged anymore.
 formula is in- applicable practice number clusters is known (cid:1) Algorithm COBRA Require: X dataset, NS: number super-instances, i=1 = K-means(X NS) Ensure: C, clustering D {Si}NS ∀i =


 NS Ci = {si} CL = ∅ mergeHappened = T rue mergeHappened do P = {C1,C2 (cid:64)x ∈ C1, y ∈ C2 (x, y) ∈ CL}, ordered d(C1,C2) C1,C2 P do sa, sb = arg mins1∈C1,s2∈C2(cid:107)s1 − s2(cid:107)2 must-link(sa, sb) mergeHappened = F alse end end end return C C1 = C1 ∪ C2 C = C \ {C2} mergeHappened = T rue break CL = CL ∪ {(sa, sb}) beforehand, serves means understand number queries be needed.
 Figure compares lower bound actual number queries was needed COBRA clustering tasks (these be de- scribed more detail section
 most tasks, actual number pairwise queries is close lower bound.
 is possible get smaller number queries suggested lower bound: happens single super-instance contains instances actual clusters, (cid:1) factor inaccurate.
 rendering the(cid:0)NC Figure estimated lower bound number queries
 number queries needed COBRA (averaged CV folds).
 dot corresponds clustering tasks.
 Experimental Evaluation section, discuss experimental evaluation CO- BRA.
 Existing Constraint-based Algorithms compare COBRA following state-of-the-art constraint-based clustering algorithms: • MPCKMeans [Bilenko et al., is hybrid constraint-based extension K-means: uses metric learning adapted clustering procedure combining within-cluster sum squares cost violat- ing constraints objective.
 use implementa- tion is available WekaUT package1.
 • Constrained Spectral Clustering (COSC) [Rangapuram Hein, is based spectral clustering, opti- mizes modiﬁed objective takes constraint violation account.
 use code provided authors web page2.
 is important note that, contrast COBRA, COSC MPCKMeans require number clusters input parameter.
 experiments, true number clusters is provided algorithms.
 many clustering applica- tions, however, number is known before- hand.
 Thus, COSC MPCKMeans are advantage.
 Active Selection Strategies algorithms is combined following active selection strategies: • MinMax [Mallapragada et starts ex- ploration phase K (the number clusters, assumed be known advance) neighborhoods cannot-links are sought.
 subse- quent consolidation neighborhoods are ex- panded selecting uncertain instances de- termining neighborhood membership means pairwise constraints.
 set width parameter Gaussian kernel 20th percentile distribution pairwise Euclidean distances, [Mallapragada
 • NPU [Xiong et al., is based concept neighborhoods, contrast MinMax is it- erative method: data is clustered multiple times, clustering is used determine next set pairs query.
 al.
 [2014] show NPU outperforms MinMax terms clustering quality, cost increased runtime.
 Datasets experiment UCI datasets: iris, wine, der- matology, hepatitis, glass, ionosphere, optdigits389, ecoli, breast-cancer-wisconsin, segmentation, column parkin- sons, spambase, sonar yeast.
 Most datasets have been used earlier work constraint-based cluster- ing [Bilenko Xiong al.,
 Optdigits389 contains digits UCI handwritten digits data 2http://www.ml.uni-saarland.de/code/cosc/cosc.htm lower bound queries050100150200250300# queries25 super-instances50 super-instances100 super-instances Table Wins losses aggregated clustering tasks.
 win (loss) count, report average margin COBRA wins (loses).
 win counts marked asterisk, differences are signiﬁcant according Wilcoxon test p
 COBRA vs.
 MPCKM-MinMax COBRA vs.
 MPCKM-NPU COBRA vs.
 COSC-MinMax COBRA vs.
 COSC-NPU super-instances win loss super-instances loss win super-instances loss win (0.03) [Bilenko et Mallapragada et
 Duplicate instances were removed datasets, data was normalized
 perform exper- iments CMU faces dataset, contains im- ages persons taking different poses, different ex- pressions, sunglasses.
 Hence, dataset has target clusterings: identity, pose, expression sun- glasses.
 extract 2048-value feature vector im- age running pre-trained Inception-V3 net- work [Szegedy et al., storing output second last layer.
 Finally, cluster news- groups data.
 dataset, consider tasks: clustering documents newsgroups related topics (the target clusters are comp.graphics, comp.os.ms-windows comp.windows.x, [Basu al., Mallapra- gada et al., clustering documents news- groups different topics (alt.atheism, rec.sport.baseball sci.space, [Basu Mallapragada et
 ﬁrst extract tf-idf features, next apply latent semantic indexing (as [Mallapragada re- duce dimensionality
 brings total datasets, clustering tasks are deﬁned (15 UCI datasets single target, CMU faces targets, subsets newsgroups data).
 Experimental Methodology use cross-validation procedure is similar ones used e.g. [Basu al., [Mallapragada et al.,
 folds, instances are set test set.
 clustering algorithm is run entire dataset, query pairwise constraints instances are training set.
 evaluate quality resulting clustering, compute Ad- justed Rand index (ARI, [Hubert Arabie, instances test set.
 ARI measures similarity clusterings, case pro- duced constraint-based clustering algorithm indicated class labels.
 ARI means clustering is random, indicates perfect clustering.
 ﬁnal score algorithm particular dataset is computed average ARI folds.
 exact number pairwise queries is known before- hand COBRA, more super-instances results more queries.
 evaluate COBRA varying amounts user input, run super-instances.
 fold, execute following steps: • Run COBRA count many constraints needs.
 • Run competitors same number con- • Evaluate resulting clusterings computing ARI straints.
 test set.
 make sure COBRA queries pairs instances are training set, medoid super- instance is calculated based training instances super-instance (and such, test instances are queried clustering).
 rare event super- instance contains test instances, is merged nearest super-instance does contain training instances.
 MinMax NPU selection strategies, pairs involv- ing instance test set are excluded selection.
 Results results clustering tasks are summarized Ta- bles
 Table reports wins losses competitors.
 shows COBRA tends pro- duce clusterings competitors.
 difference COSC is signiﬁcant according Wilcoxon test p difference MPCKMeans is not.
 Table shows average ranks COBRA competi- tors.
 Friedman aligned rank test [Hodges Lehmann, has more power Friedman test number algorithms comparison is low [Garca et al., indicates super-instances, differences rank COBRA competitors are signiﬁcant, using posthoc Holm test p
 Table dataset, algorithms are ranked (best) (worst).
 table shows average ranks super-instances.
 Algorithms difference COBRA is signiﬁcant according Friedman aligned rank test post- hoc Holm test p < are marked asterisk.
 super-instances super-instances super-instances COBRA MPCK-NPU MPCK-MM COSC-MM* COSC-NPU* COBRA MPCK-MM* COSC-NPU* COSC-MM* MPCK-NPU* COBRA COSC-NPU* MPCK-NPU* MPCK-MM* COSC-MM* Running Competitors Different Numbers Queries previous experiments, competitors are run same number queries COBRA required, COBRA cannot be ﬁxed beforehand.
 won- der constitutes advantage COBRA, above conclusions hold competitors (a) (b) (c) (d) Figure Comparing clustering qualities COBRA competitors wider range numbers constraints.
 COBRA, black marker shows average number questions COBRA required particular number super-instances, average ARI 5-fold CV).
 show results MinMax selection strategy, conclusions are drawn hold NPU.
 number super-instances COBRA is (as experiments before),


 be run different numbers constraints.
 answer question, run COBRA wider range super- instances, competitors more numbers con- straints.
 Figure shows results datasets, con- clusions are drawn hold others.
 ﬁrst conclusion is datasets COBRA outper- forms competitors experiments discussed does larger numbers constraints (e.g. Figures
 results discussed previous sec- tion are representative.
 Secondly, clustering quality plateaus many datasets (e.g. Figure
 is espe- true MPCKMeans, be explained strong spherical bias.
 contrast, several datasets COBRA COSC produce better clusterings more constraints are given (e.g. Figures
 Selecting Right Number Clusters COBRA does require specifying number clusters K beforehand.
 Most often, produces correct K (i.e. one indicated class labels).
 does K ﬁnds is correct one.
 contrast, COSC MPCKMeans do require specifying K.
 experiments discussed were given correct K.
 have found that, majority cases, run- ning different K reduces clustering quality, signiﬁcant amount.
 different K improves results, was case was small margin.
 results are omitted paper due lack space.
 Runtime Figure compares runtimes COBRA competitors.
 COBRA consists steps: constructing super-instances, grouping form clusters.
 are fast, K-means is used ﬁrst step sec- ond step is applied small set super-instances.
 be seen Figure runtimes MPCKMeans- MinMax are comparable COBRA, is surprising is built K-means.
 contrast, COSC- MinMax is more expensive, is built spec- tral clustering.
 used NPU selection strategy, MPCKMeans COSC become much slower, NPU requires several runs clustering algorithm.
 Figure dot shows runtime (averaged CV folds) method datasets.
 ﬁgure shows results COBRA super-instances, ﬁgures super- instances are comparable.
 COBRA MPCKMeans-MinMax are scalable ﬁnish seconds.
 Conclusion have introduced COBRA, active constraint-based clus- tering method.
 other methods, is built ex- tension existing unsupervised algorithm.
 Instead, CO- BRA is constraint-based.
 selection strat- egy, aims exploit constraint transitivity entailment.
 experiments show COBRA outperforms state art terms clustering quality runtime, other methods have advantage being given right number clusters.
 future work, investigate appropriate number super-instances be determined automatically, e.g. reﬁn- ing necessary.
 Acknowledgements Toon Van Craenendonck is supported Agency In- novation Science Technology Flanders (IWT).
 Se- bastijan Dumanˇci´c is supported Research Fund KU Leu- ven (GOA/13/010).
 References [Basu et al., Sugato Basu, Arindam Banerjee, Raymond J.
 Mooney.
 Active semi-supervision pair- Proceedings wise constrained clustering.
 queries0.00.20.40.60.81.0ARIsegmentationMPCKMeans-MinMaxCOSC-MinMaxCOBRA50100150200250300350400# queries0.00.20.40.60.81.0ARIglassMPCKMeans-MinMaxCOSC-MinMaxCOBRA50100150200250300350400# queries0.00.20.40.60.81.0ARIfaces eyesMPCKMeans-MinMaxCOSC-MinMaxCOBRA50100150200250300# queries0.00.20.40.60.81.0ARIbreast-cancer-wisconsinMPCKMeans-MinMaxCOSC-MinMaxCOBRAdatasets10-1100101102103104105runtime (s)MPCKMeans-MinMaxMPCKMeans-NPUCOSC-MinMaxCOSC-NPUCOBRA SIAM International Conference Data Mining (SDM- April
 cal Clustering Using Conﬁdence-Based Active Learning.

 [Rangapuram Hein, Syama S.
 Rangapuram Matthias Hein.
 Constrained 1-spectral clustering.
 Proc.
 15th International Conference Artiﬁcial Intelli- gence Statistics,
 [Shental et al., Noam Shental, Aharon Bar-Hillel, Tomer Hertz, Daphna Weinshall.
 Computing Gaussian mixture models EM using equivalence constraints.
 Advances Neural Information Processing Systems
 [Szegedy al., Christian Szegedy, Vincent Van- houcke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna.
 Rethinking inception architecture computer vision.
 CoRR, abs/1512.00567,
 [von Luxburg Ulrike von Luxburg, Robert C.
 Williamson, Isabelle Guyon.
 Clustering: Science Art?
 Workshop Unsupervised Learning Trans- fer Learning, JMLR Workshop Conference Proceed- ings
 [Wagstaff al., Kiri Wagstaff, Claire Cardie, Seth Rogers, Stefan Schroedl.
 Constrained K-means Clus- tering Background Knowledge.
 Proc.
 Eigh- teenth International Conference Machine Learning, pages
 [Wang al., Xiang Wang, Buyue Qian, Ian Davidson.
 constrained spectral clustering appli- cations.
 Data Mining Knowledge Discovery,
 [Xing al., Eric P.
 Xing, Andrew Y.
 Ng, Michael I.
 Jordan, Stuart Russell.
 Distance metric learning, application clustering side-information.
 Advances Neural Information Processing Systems pages
 [Xiong et al., Sicheng Xiong, Javad Azimi, Xi- aoli Z.
 Fern.
 Active learning constraints semi- supervised clustering.
 IEEE Transactions Knowledge Data Engineering,
 [Xu al., Qianjun Xu, Marie desJardins, Kiri L.
 Wagstaff.
 Active Constrained Clustering Examining Spectral Eigenvectors, pages
 Springer Berlin Heidelberg, Berlin, Heidelberg,
 [Yan al., Donghui Yan, Ling Huang, Michael I.
 Pro- Jordan.
 Fast approximate spectral clustering.
 ceedings ACM SIGKDD International Con- ference Knowledge Discovery Data Mining, KDD ’09, pages New York, NY, USA,
 ACM.
 [Basu al., Sugato Basu, Misha Bilenko, Ray- mond J.
 Mooney.
 probabilistic framework semi- Proceedings 10th ACM supervised clustering.
 SIGKDD International Conference Knowledge Discov- ery Data Mining (KDD-2004), page January
 [Bilenko al., Mikhail Bilenko, Sugato Basu, Raymond J.
 Mooney.
 Integrating constraints metric learning semi-supervised clustering.
 Proc.
 In- ternational Conference Machine Learning, pages July
 [Campello al., Ricardo J.
 G.
 B.
 Campello, Davoud Moulavi, Arthur Zimek, J¨org Sander.
 framework semi-supervised unsupervised optimal extraction clusters hierarchies.
 Data Mining Knowledge Discovery,
 [Caruana al., Rich Caruana, Mohamed Elhawary, Proc.
 In- Nam Nguyen.
 Meta clustering.
 ternational Conference Data Mining,
 [Davidson Ravi, Davidson SS Ravi.
 Us- ing instance-level constraints agglomerative hierarchi- cal clustering: theoretical empirical results.
 Data min- ing knowledge discovery, pages
 [Davis al., Jason V.
 Davis, Brian Kulis, Prateek Jain, Suvrit Sra, Inderjit S.
 Dhillon.
 Information- theoretic metric learning.
 Proceedings In- ternational Conference Machine Learning, ICML ’07, pages New York, NY, USA,
 ACM.
 [Garca al., Salvador Garca, Alberto Fernndez, Julin Luengo, Francisco Herrera.
 Advanced nonparamet- ric tests multiple comparisons design ex- periments computational intelligence data mining: Information Sciences, Experimental analysis power.
 –
 Special Issue Intelligent Distributed Information Systems.
 [Hodges Lehmann, J.
 L.
 Hodges E.
 L.
 Lehmann.
 Rank methods combination independent experiments analysis variance.
 Annals Math- ematical Statistics,
 [Hubert Arabie, Lawrence Hubert Phipps Arabie.
 Comparing partitions.
 Journal Classiﬁcation,
 [Lelis Sander, Levi Lelis J¨org Sander.
 Semi- supervised density-based clustering.
 Ninth IEEE International Conference Data Mining, pages Dec
 [Mallapragada al., Pavan K.
 Mallapragada, Rong Jin, Anil K.
 Jain.
 Active query selection semi- supervised clustering.
 Proc.
 International Conference Pattern Recognition,
 [Nogueira al., Bruno M Nogueira, M Jorge, Solange O Rezende.
 HCAC Semi-supervised Hierarchi-
 results US Presidential Election were ﬁnalized, was clear majority pro- fessional polling groups, many had pre- dicted probability Clinton victory be were had overestimated predictions ([1,
 be argued underlying models were correct par- ticular result was rare event, post-mortem analyses have revealed ﬂaws led large pre- diction biases.
 According multiple post-election analyses, was concluded leading cause er- ror majority election forecasting models was lack correlation individual states pre- dictions ([4,
 Uncorrelated models, much simpler build train, cannot capture complex behavior fully-connected system.
 capture higher-order relationships, fully- connected graphical model be ideal.
 models are more powerful, practical roadblocks have prevented widespread adoption due dif- ﬁculties implementation using classical computa- tion.
 However, recent studies have shown quan- tum computing is competitive alternative generating such networks ([8,
 Quantum machine learning (QML) is blossom- ing ﬁeld.
 summarized comprehensive re- view QML [13], machine learning applications support vector machines principal compo- nent analysis are being reimagined various quan- tum devices.
 exciting research ar- eas QML is deep quantum learning, fo- cuses impact quantum devices algorithms have classical deep neural networks (DNNs) graphical models.
 particular class DNNs is Boltzmann machine (BM), is incred- powerful fully-connected graphical model be trained learn arbitrary probability distri- butions.
 downside networks is BMs are costly train, fact has limited practical application.
 large computational training cost has drawn attention implemen- tation quantum computation help train such networks.
 BMs realized quantum devices (partic- ularly adiabatic quantum devices such pro- duced D-Wave Systems ([9]) possess inherent beneﬁts compared realized classical de- vices.
 Research groups have realized various forms BMs (fully-connected BM, restricted Boltzmann machines (RBMs), Helmholtz machines) trained using quantum computation, research has shown quantum computation be used eﬀec- train neural networks image recognition tasks ([9,
 work, leverage power adia- batic quantum computation train fully- connected BMs novel purpose election modeling.
 have ex- plored number assumptions underlying approach using adiabatic quantum computers (AQC) model BMs, have demonstrated most systems interest (such one) approach does appear be valid.
 believe methods proposed paper bring in- teresting new factor conversation election forecasting large, quantum compu- tation play future role.
 Methodology Modeling Boltzmann Machines AQC work, be generating fully-connected BMs trained using D-Wave qubit quan- tum device using general method described [9].
 methodology training RBMs BMs using D-Wave machine have been laid previous papers ([9, brieﬂy re- view logic methodology here.
 BM is fully-connected graph (N binary units (neurons).
 neurons be “visi- ble” model aspect data distribu- tion) “hidden” (not tied particular aspect data distribution used captur- ing features data distribution).
 net- work has possible states, probability sampling particular state s = (s1, ..., sN model is p(s) = (1) e−E(s) wherein Z well-known partition function E is energy function deﬁned E(s) = −(cid:88) bisi − (cid:88) si∈s si,sj∈s Wijsisj, (2) wherein b represent linear “bias” unit W represents “weight” coupling be- tween units (b W be referred “model parameters”).
 train net- work, need adjust model parameters model distribution produced sampling model is possible un- derlying data precisely, want maximize log-likelihood, L, data dis- tribution.
 calculate model parameters maximizing L, use familiar gradient descent method learning rate η get model parameter update equations (3) ∆Wij = (cid:17) (cid:16)(cid:104)sisj(cid:105)D − (cid:104)sisj(cid:105)M (cid:16)(cid:104)si(cid:105)D − (cid:104)si(cid:105)M (cid:17) ∆bi = (4) equations values (cid:104)∗(cid:105) represent expectation values data (D) model (M distributions.
 model be trained ﬁrst ((cid:104)si(cid:105)) second ((cid:104)sisj(cid:105)) order moments were identical data model distribution.
 adjust model parameters need calculate expectation values model it- self.
 Getting “true” values require calculation possible states model, is intractable system size in- creases.
 particular calculations are use quantum computation is ideal, see potential speedup overall training algo- rithm.
 quantum devices produced D-Wave Sys- tems perform quantum annealing algorithm.
 theory, algorithm leverage quantum eﬀects take initial quantum system is well- known ground state transform ﬁnal Hamiltonian system be ground state (assuming annealing pro- cess was slow enough, many other factors discussed [14]).
 original use case algorithm lies fact prop- map diﬃcult problem in- terest ﬁnal Hamiltonian, measuring ground state ﬁnal Hamiltonian produce optimal solution original prob- lem.
 However, use case has been elusive scale; shown research [15], focuses fundamental limitations quantum devices ﬁ- nite temperatures.
 taking optimistic as- sumptions (such perfect, instant thermalization), system (problem) size grows, probabil- ity measuring optimal (ground) state system decreases
 return- ing ground state solution, measuring device returns Boltzmann distribution energies.
 results prove challenging using such hardware optimization, presents ideal opportunity training BMs. high level, in- stead trying calculate (cid:104)si(cid:105)M (cid:104)sisj(cid:105)M di- rectly, map network D- Wave quantum device.
 obtaining ﬁnite num- ber samples hardware device, goal is generate better approximations (cid:104)si(cid:105)M (cid:104)sisj(cid:105)M classical heuristics.
 method seems more natural form Hamiltonian H D-Wave device is H(S) = − (cid:88) hiSi − (cid:88) JijSiSj, (5) Si∈S Si,Sj∈S is same functional form BM en- ergy equation
 equation, S is vector qubit spin states, hi are bias terms qubit, Jij are (anti)ferromagnetic couplings qubits.
 mapping model param- eters BM hardware parameters D- Wave device making set measurements device, use measurements con- struct approximations (cid:104)si(cid:105)M (cid:104)sisj(cid:105)M
 Ad- vantages have been shown using fully-connected BM QC devices using methods [9], eﬀective temperature device does have be taken account.
 Equation is spe- cial case general representation; raising exponential −E(s), more gen- eral expression raises −E(s)β, β is “eﬀective” temperature system re- lated temperature system).
 β = arrive equation general using quantum device know eﬀective temperature beforehand, experience large ﬂuctuations measurements.
 be problematic training RBMs using quantum annealers, requires diﬀerent techniques esti- mate parameter ([10, connected BMs do require additional calculations eﬀective training ([9]).
 structure BM graph embed device is connected, are practice limited graph structure realized hardware.
 adiabatic quantum device used research was D-Wave has qubits connected Chimera graph archi- tecture consisting qubit cells arranged K4,4 bipartite graphs.
 qubits cells are cross connected, cell is connected adjacent cells (with exception cells boundaries) shown Figure
 map BM energy function (2) device, graph minor-embedding problem be solved; need hardware embedding uses chain multiple physical qubits realize single logical qubit problem Hamiltonian (5).
 Using same method [9], ﬁnd embeddings using embedding heuristic provided D-Wave’s API resolve discrepancies qubit chains using ma- jority vote (a post-processing step measure- ments).
 Figure bipartite cells Chimera graph architecture showing cells interconnect.
 cell are horizontal verti- cal qubits, colored blue burnt orange.
 cells, qubits are cou- pled means Josephson junction, indicated green circles.
 qubit be coupled ad- dtional qubits adjacent cells, means Josephson junctions, indicated light green circles.
 Quantum Boltzmann Machines Forecasting Elections methodology outlined section lays approach training fully-connected BM using D-Wave quantum device.
 section detail procedure implementing networks forecast elections.
 research, study US Presidential election, binary unit BM represents single US state.
 winner particular election simulation is determined candidate electoral college votes.
 US state has particular number electoral college votes award candidate (2 + integer scales function state’s population), votes are awarded candi- date (winner-take-all).
 assert sample returned fully-connected BM eﬀect be simulation US presidential election.
 sample BM returns binary vector, entry vector corresponds vot- ing results particular US state.
 individ- ual state voting results are mapped particular candidate/party (i.e., = Democrat, = Republi- can).
 determine election simulation weight US state outcomes accord- ing net weight national vote (each state’s electoral votes).
 winner simu- lation (sample) is determined sum party’s overall national vote, is calculated us- ing binary results sample) national weight (electoral votes) state.
 goal is train BM is being sam- pled ﬁrst second order mo- ment terms model distribution approach data distribution.
 training process has been discussed section section expand determined ﬁrst second order moment terms data distribution election model.
 ﬁrst order moment terms represent probability state vote particular candidate.
 example, believe is chance Democratic candidate wins Colorado, ﬁrst order moment binary variable as- signed represent Colorado be equal determine ﬁrst order moments state model, use current time-averaged polling results made available FiveThir- tyEight ([16]).
 obtain projected vote share candidates day data is avail- able (6 months before, including, November
 projected vote shares are used input sigmoidal model (same model used FiveThirtyEight [17]) assumes elections are stochastic, result state/country follows probabilistic de- terministic curve based popular-vote / pro- jected vote share margin.
 method convert- ing popular-vote margin probability victory is shown particular state Figure
 Figure Model interpreting projected vote share probabilities.
 A.
 Plot Maine’s polling projec- tions time, solid lines are time- averaged projected vote share candidates, dashed lines are resulting probabilities victory candidate, calculated using best ﬁt function shown B.
 Given underlying projected vote shares state best ﬁt function shown Fig- ure calculating ﬁrst order moment terms state is straightforward.
 Calculating second order terms, eﬀective “correlations” be- tween states, is diﬃcult.
 correla- tions express likelihood states end same (or diﬀerent) voting result election.
 States vote same are cor- related (higher second order moment), states don’t have lower correlation (lower second or- der moment).
 correlations are inﬂuenced plethora demographic (race, age, education), geo- graphic, additional factors.
 Professional model- ers (such FiveThirtyEight) have com- plex methodologies determining correla- tions; however, rigorous analysis corre- lations is scope particular work.
 used data obtained publicly, is suﬃcient validate general approach model.
 calculate second order moment terms, use source data make particular ansatz.
 data source use is presidential elec- tion results last US Presidential elec- tions.
 data contains date results state election.
 ﬁrst ansatz is consider states, states have higher correlations (second order terms) had voted previous elections.
 correlation is agnostic towards candidate was voted previous elections; only important factor states question is vote was same candidate diﬀerent one.
 second ansatz is terms weighting previ- ous election results, recent elections are more relevant.
 means recent elections increase corre- lations states hap- pened ago.
 assume linear relationship time importance.
 raw correlations (cid:104)sisj(cid:105)Draw states i j are calculated follows: n=1:11 n(cid:0)2injn − − jn + (cid:80) (cid:104)sisj(cid:105)Draw = (cid:80) n (6) n refers particular election year set [1968,
 (higher n is re- cent) jn are results election n respective states.
 enforce hard con- straint second order correlations contradict ﬁrst order moments are calculated current polling data).
 is accom- plished calculating second order mo- ments states i j (cid:104)sisj(cid:105)D = (cid:104)sisj(cid:105)Draw min((cid:104)si(cid:105)D,(cid:104)sj(cid:105)D).
 (7) have methodology mapping election forecasting models, US Pres- idential election, BM deﬁning mathematical models calculating ﬁrst second order data distribution terms.
 following section, validate approach holds true small, nonexistent countries attempt simulate “real time” forecast Presidential election using quantum-trained BMs. Caveats limitations section reviewed methodology training fully-connected BM D-Wave ma- chine describe approach mapping election forecast models (to be trained) BM.
 work uses approach described, few caveats limitations deserve additional attention here.
 Hardware constraints hardware size limitations D-Wave does allow embed state model DC province, are fundamen- tal voting blocks US Presidential election.
 Using virtual full-yield Chimera capability fered D-Wave, uses combination quantum device hardware tandem classical software simulating missing qubits couplers, were able embed states omitted DC Maryland.
 were omitted were ranked “deﬁnite” model stan- dards were approaching likelihood vote Democrat), adjacent.
 Assert states are winner-take- US Presidential election is winner-take- state level, states are exceptions rule: Maine Nebraska.
 winner- take-all, states award delegates district.
 simplify model ﬁt hardware con- straints, treat states winner-take-all re- gardless.
 decision was made reasons.
 primary purpose paper is val- idate overall election methodology model- ing such elections using QC-trained neural networks; such state speciﬁc rules fall scope work.
 Second, states have small weight (electoral college votes) broader election, treating winner-take-all has reduced ef- fect compared larger state same voting system.
 Third, future treat provinces individual states themselves, awarding electoral college votes winner-take- system.
 However, due limitation expressed previous issue, experiment be left future studies larger quantum device.
 Inability model national errors same model strength models correlations de- scribed is simple; account form error is inaccessible independent models.
 However, are other primary types error want ﬁnal model consider: national state-speciﬁc errors.
 er- ror arise fact polling is perfect; are voting blocks are over- represented based types people are polled respond poll.
 National error arises fact states have system- missed particular type voting block similar, characteristic manner.
 leads errors aﬀect state similar way.
 State-speciﬁc error is same concept, state-by-state level.
 latter (as discussed future results sec- tion) be addressed nature QC-training algorithm; former cannot.
 wish emulate best possible model, in- corporating state-speciﬁc, state, national errors, choose create meta-model aggregates results several diﬀerent models build as- sumption diﬀerent national error.
 case, take equally-spaced samples t-distribution degrees freedom; is same distri- bution degrees freedom used national state-speciﬁc error used FiveThirtyEight ([16]).
 points deﬁne national errors use train diﬀerent models.
 instance, national model have national error favoring Clinton point, favor Trump
 national errors are incorpo- rated ﬁrst order moment terms state, leading models are biased towards candidate relative degree.
 average models is calculated simu- lating model independently, weighing results probability occurrence national error.
 Limited time windows D-Wave ac- cess production environment, be ideal produce updates forecasts (or several times day) high-proﬁle elec- tions.
 updates occur new polls come in, changing particular predictions state, national results.
 Applying proposed methodology be used purposes, limiting factor simu- lating daily forecast months is access D-Wave quantum device.
 Due limited access time run experiments D-Wave device, have simulate multiple error models (as explained section choose model weeks data daily.
 allowed generate appropriate number simulations days national error models.
 Results Eﬀect Chain Length As mentioned section using well posed Hamiltonian right environmental AQC be capable ﬁnding ground state Hamiltonian.
 practice, thermal ﬂuctuations, environmental interactions, in- short annealing times, plethora other physical engineering challenges result low probability measuring ground state, other low energy near- optimal) state.
 is true larger Hamiltonian systems, shown [15]; ﬁnite- temperature AQCs, system size increases, probability measuring non-optimal low energy state approaches
 contrast, wish use AQC sampling engine sampling BMs, face diﬀerent set ob- stacles using small embeddings (system sizes).
 terms using AQC machine learning pur- poses, returning distribution low energy solu- tions optimal conﬁguration drives learning process, ﬁrst second order statistics measurements determines up- date terms model.
 small physical em- bedding size, probability measuring op- timal state increases signiﬁcantly, con- cise embedding sizes probability measuring ground state energy approach
 be- havior is Hopﬁeld network, is BM T
 BM, Hopﬁeld net- work return ground state energy solutions.
 imply training algorithm de- scribed section work such sys- tem.
 time model updates equations are made, energy function equation changes well.
 new energy function lead new ground state solutions, turn have diﬀerent model parameters.
 train- ing BM leads model updates “smoothly” guid- ing model parameters ((cid:104)si(cid:105)M (cid:104)sisj(cid:105)M data distribution ((cid:104)si(cid:105)D, (cid:104)sisj(cid:105)D), slight changes model parameters Hopﬁeld network change ground state solutions, lead- ing chaotic model parameter updates.
 Figure Training results arbitrary Boltzmann machines realized D-Wave device using (A) embedding qubit embedding chains.
 subplot, horizontal red lines are respec- tive target values.
 potential way mitigate eﬀects is increase size qubit chains embedding problem.
 optimization purposes, goal be ﬁnd minimum chain length embedding problem Hamiltonian physical device.
 keeping embedding chains minimal, system size is small possible increases chance measuring optimal ground state.
 opposite be true well: more increase chain lengths embedding logical qubits hardware, more low energy states become available system, increasing probability system transition ground state annealing process.
 enable train BM number nodes, given qubit chain lengths are long enough.
 validating assertion, argue approach using AQC realizing BMs election modeling be ap- plied sized system, validate particular experiments are regime proper learning is possible.
 test hypothesis, performed experiments connected graphs size em- bedded diﬀerent embeddings various chain lengths, studied train systems reproduce activation probability dis- tributions deﬁned graphs arbitrary ﬁrst second order terms.
 run, activation (hi) correlation (Ji,j) probabilities were selected ran- domly such node i activation probability hi ∈ (0, nodes i j, correlation probability Ji,j = ci,jhihj ci,j is correla- tion strength ci,j ∈ (0,
 embeddings were used graph: concise embedding, embedding de- rived concise graph many nodes (denoted “2x”), embedding derived concise graph times many nodes (denoted “3x”).
 de- cision approach problem way was done D-Wave API has been set op- timization problems, such hardware em- bedding functions general attempt return max- concise embeddings.
 embed- dings returned API were graphs size problem graph, were reduced correct size joining physical qubits representing pairs (in case triples (in case logical qubits represented chains physical qubits) single logical qubits chains physical qubits times original embedding.
 ex- ample training shortest (1x) medium (2x) chain lengths are shown Figure
 subgraphs Figure x-axis graph is number completed iterations training algorithm y-axis is acti- vation probability sampling graph multi- ple times.
 graphs diagonal are single node activation probabilities(ﬁrst order moments) oﬀ-diagonal graphs are node cor- relations (second order moments).
 Figure activation probabilities fail converge desired values, indicating qubit chains are allowing suﬃcient degrees freedom sys- tem model Boltzmann machine.
 However, using same network embedding qubit chains, network was able converge time target ﬁrst second order moment values.
 Table show root mean squared error (RMSE) training iterations dif- ferent QC-trained networks diﬀerent embedding chain lengths.
 Nodes Chain RMSE ± ± ± ± ± ± Table RMSE QC-trained networks diﬀer- ent embedding chain lengths.
 networks grow chain length diﬀerences grows more negli- gible chains are getting longer satisfy embedding.
 Given current D-Wave qubit connectivity graph, problem size grows larger, average embedding chain length grows.
 most studies embed large problem possible device, has led chain lengths previous research.
 future hardware improvements are made shorter qubit chains are feasible increased connectivity), be- come important validate individual log- ical qubits are learning respective tar- get terms.
 lengthening technique shown provide simple eﬃcient tool ensuring Boltzmann-like behavior nodes logical graph having perturb individ- ual energy scalings.
 Modeling Presidential Elec- tion primary experiment conducted was at- tempt simulate “real-time election model fore- cast” using QC-trained Boltzmann machines.
 Start- ing date 2016-06-08 continuing elec- tion day trained multiple con- nected Boltzmann machines using D-Wave adi- abatic device.
 Figure Summed error function training iterations national error model.
 small spikes error occur training process are artifact updates ﬁrst or- der moments happen week (25 iteration) intervals.
 mentioned section due limitations retrained network weeks daily, used diﬀerent networks model diﬀerent national errors (derived t- distribution degrees freedom).
 net- works starting 2016-06-08 were initialized small, random coeﬃcients trained iterations each.
 Then, week interval, ﬁrst order moment terms were updated trained additional iterations.
 changes ﬁrst order moments were small, fewer training iterations were necessary converge stable summed error (sum squared ﬁrst second moment errors) networks.
 led total training iterations national er- ror (150 2016-06-08 + next ten Figure Comparing Presidential election forecasting results QC-trained methodology FiveThirtyEight.
 QC-trained networks had national bias towards Clinton (CB), Trump (TB), candidate.
 two-week updates).
 example training er- ror particular national error model is shown Figure
 Knowing section qubit chains are learn properly, training error results Figure are be expected.
 Similar plots were observed national error models, translates nothing more scaling ﬁrst order moment terms.
 take samples networks diﬀerent iterations election forecasting simulation re- sults.
 choose take samples last iterations forecasting date (this be it- erations 2016-06-08 16-25 next forecasting dates).
 allows sam- ple network has reached general steady-state terms summed training error.
 discussed section logical qubit is mapped particular state sample is equivalent election forecast.
 determine candidate “won” particular sample, map qubit results particular state represents, add state’s number electoral votes can- didate state voted sample.
 democratic candidate was heavy favorite most election models, choose express forecast- ing results terms probability Clinton victory.
 way, sample results partic- ular candidate winning (270 electoral votes more) losing (we combined ties category simplicity, exact tie is unlikely).
 experiments, took samples D-Wave device iteration na- tional error model.
 gave samples national error model forecasting date (10 training iterations, samples training iteration).
 probability Clinton victory national error model was sum individual samples were won Clinton divided number total samples (10,000 case, national error model time step).
 Finally, get average election forecast func- tion time (shown Figure calculated weighted arithmetic mean national error models forecasting date.
 weights national error were deﬁned t-distribution probability density function national error (t-distribution degrees freedom).
 evidenced Figure QC-trained network results followed trends similar trends professional FiveThirtyEight forecasts.
 overall probabilities diﬀerent national error networks follows naturally; networks had national error favor Clinton increased probability Clinton victory, networks national error favor Trump decreased probability Clinton victory.
 largest apparent diﬀer- ence QC-trained models was overall probability Clinton victory.
 Aver- age result line QC-trained networks follows similar pattern predictions FiveThir- QC-trained results are lower.
 result way says quan- tum methodology is “better”, highlights diﬀerences overall approach.
 is likely results are dependent un- derlying diﬀerences calculated sec- ond order moments terms states.
 interesting future study be replicate quantum-training protocol described using second order moments driven demographic data individual state inhabitants.
 important factor forecasters desire forecasting model is know states are important predicting particular outcome.
 straightforward approach is generate vector state (1 = state voted Democrat, = state voted Republican) similar vector out- come (1 = Democratic victory, = Republican vic- tory) simulation date November
 calculate Pearson correla- tion coeﬃcient vectors take absolute value correlations.
 Table shows states highest lowest correlation coeﬃcients.
 expected, states leaned Democratic Republican had low correlation coeﬃcients; regardless outcome election, states Illinois Nebraska were virtual locks Democratic Republican candidates, respec- tively.
 Similarly, states highest corre- lation coeﬃcients contained many con- tested states election.
 FiveThirtyEight’s fore- casts have similar “tipping-point chance” metric deﬁne “the probability state provide decisive vote Electoral College” ([16]).
 election day, states ranked highest tipping-point chance states were list most correlated states Table (the diﬀerences: FiveThirtyEight included Virginia, Minnesota, Wisconsin, ours in- cluded New Hampshire, Iowa, Arizona).
 take consideration individual state errors observed QC-trained models.
 mentioned previously, modelers (such FiveThir- tyEight) apply degree noise individ- State Ohio Florida Nevada New Hampshire Pennsylvania Iowa Michigan North Carolina Colorado Arizona Illinois Nebraska Alabama Oklahoma California West Virginia Delaware Oregon Idaho Arkansas Correlation coeﬃcients Table Pearson correlation coeﬃcients states (top) least (bottom) correlated election forecasting results.
 ual states, such adding state-speciﬁc error sampling.
 be useful know natu- ral sampling quantum device training lends state-speciﬁc error.
 iteration used determining national averages, calculated diﬀerence target (cid:104)s(cid:105)D current model output (cid:104)s(cid:105)M
 diﬀerence is negative, be state-speciﬁc error fa- vor Democratic candidate, vice versa positive value translates error beneﬁting Re- publican candidate.
 taking errors state, form diﬀerent state-speciﬁc error dis- tributions state.
 distributions vary con- siderably, depending underlying target (cid:104)s(cid:105)D value, evidenced Figure
 extremes, see error distribu- tions states leaning Democratic Repub- lican are asymmetrical.
 occurs due (cid:104)s(cid:105)M being bound
 (cid:104)s(cid:105)D ≈ (state leaning Republican), error be biased negative direction; similarly, states (cid:104)s(cid:105)D ≈ (state leaning Democrat) have positively-biased error distributions.
 swing states, see uniform spread error, shows absence bounds ((cid:104)s(cid:105)D ≈ error tends be distributed.
 Figure Example distributions state-speciﬁc error states leaning Republican (top), Democratic (bottom), swing states (middle).
 interesting ﬁnding was heavily-learning Democratic states seemed have longer error dis- tribution tails compared heavily-leaning Re- publican states.
 seen Figure probability mass Alabama’s error distribution is contained range -5 substantial amount California’s error distribution falls out- side bounds.
 phenomena introduce amount bias favor particular candi- date.
 potential mitigation technique deal- ing issue is taking average multiple gauges ([11]), “ﬂip” mea- surement value (ﬂip Republican = Democrat =
 Additionally, interesting new techniques using “shimming” ([18]) have been shown reduce overall qubit error.
 future work, be in- teresting topic explore evolution individual logical qubit error distributions QC-trained Boltz- mann machines using shimming techniques (re- ducing error) introducing random (increas- ing error) per-qubit basis.
 Conclusions work, have showed initial implementa- tion QC-trained Boltzmann machines, be employed diﬃcult task sampling correlated essential problem heart many applications such election mod- eling.
 validated approach learned various data distributions based state polling results US Presidential cam- paign, QC-trained models generated fore- casts had similar structural properties out- comes compared best class election modeling group.
 quantum computers samplers are emerging technology, believe application area be near-term interest.
 methodol- ogy be interesting technique bring broader conversation modeling future election forecasts.
 References [1] Survey ﬁnds Hillary Clinton has ‘more chance’ winning election Donald Trump, http://www.independent.co.uk/,
 Huﬃngton Post Presidential Forecast, http://elections.huﬃngtonpost.com/2016/fore- cast/president,
 [3] Key model predicts big election win Clinton, http://money.cnn.com/2016/11/01/news/econ- omy/hillary-clinton-win-forecast-moodys- analytics/index.html,
 [4] polls, including ours, missed trump’s http://www.reuters.com/article/us- victory, usa-election-polls-idUSKBN1343O6,
 [5] Analysis: Early thoughts went wrong http://www.wbur.org/politicker/2016/11/09/ pollster-early-thoughts,
 election polls, [6] devil polling data, https://www.quantamagazine.org/why-nate- silver-sam-wang-and-everyone-else-were-wrong- part-2-20161111/,
 [7] Epic fail, http://www.economist.com/news/ united-states/21710024-how-mid-sized-error- led-rash-bad-forecasts-epic-fail,
 [8] J.
 E.
 Dorband, ArXiv e-prints (2016),
 [9] M.
 Benedetti, J.
 Realpe-G´omez, R.
 Biswas, A.
 Perdomo-Ortiz, Phys.
 Rev.
 (2017).
 [10] M.
 Benedetti, J.
 Realpe-G´omez, R.
 Biswas, A.
 Perdomo-Ortiz, Phys.
 Rev.
 (2016).
 [11] S.
 H.
 Adachi M.
 P.
 Henderson, ArXiv e- prints
 [12] M.
 Benedetti, A.
 Perdomo-Ortiz,
 J.
 Realpe-G´omez, ArXiv e-prints (2017), [13] J.
 Biamonte al., Nature (2017).
 [14] E.
 Farhi al., Science (2001).
 [15] T.
 Albash, V.
 Martin-Mayor, I.
 Hen, Phys.
 Rev.
 Lett.
 (2017).
 [16] user’s guide election ﬁvethirtyeight’s forecast, general https://ﬁvethirtyeight.com/features/a-users- guide-to-ﬁvethirtyeights-2016-general-election- forecast/,
 [17] FiveThirtyEight, Election Clin- forecast, ton’s big lead means steadier https://ﬁvethirtyeight.com/features/election- update-clintons-big-lead-means-a-steadier- forecast/,
 [18] S.
 Adachi, Qubit bias measurement correc- tion, D-Wave Users Conference,

 impressive (and superhuman) accuracies machine learning diverse tasks such object recognition (He speech recognition (Xiong al., play- ing Go (Silver al., classiﬁers fail presence small imperceptible adversarial perturbations (Szegedy et al., Goodfellow et Kurakin
 addition being intriguing phenonemon, existence such “adversarial examples” exposes serious vulnerability current ML systems (Evtimov et Sharif Carlini
 deﬁning “imperceptible” perturbation is difﬁcult, commonly-used proxy is perturbations are bounded (cid:96)∞-norm (Goodfellow et al., Madry Tramèr focus attack model paper, proxy is known construct high-performing image classiﬁers are robust perturbations.
 proposed defense (classiﬁer) is shown be successful set attacks known time, new stronger attacks are discovered render defense useless.
 example, defensive distillation (Papernot al., adversarial training Fast Gradient Sign Method (Goodfellow et were defenses were shown be ineffective stronger attacks (Carlini Wagner, Tramèr
 order break arms race attackers defenders, need come defenses are successful attacks certain class.
 However, computing worst-case error given network adversarial pertur- bations (cid:96)∞-ball is intractable.
 common approximation is replace worst-case loss loss given heuristic attack strategy, such Fast Gradient Sign Method (Goodfellow et al., more powerful iterative methods (Carlini Wagner, Madry
 Adversarial training minimizes loss respect heuristics.
 How- ever, minimizes lower bound worst-case loss, is problematic points bound is loose have lower objective values, lure mislead optimizer.
 Indeed, adversarial training provides robustness speciﬁc attack, fails generalize new attacks, described above.
 approach is compute worst-case perturbation using discrete optimization (Katz Carlini review conference paper ICLR (a) (b) Figure Illustration margin function f (x) simple two-layer network.
 (a) Contours f (x) (cid:96)∞ ball x.
 Sharp curvature x renders linear approximation inaccu- rate, f (Afgsm(x)) obtained maximising approximation is smaller f (Aopt(x)).
 (b) Vector ﬁeld ∇f (x) length arrows proportional (cid:107)∇f (x)(cid:107)1.
 approach, bound f (Aopt(x)) bounding maximum (cid:107)∇f (˜x)(cid:107)1 neighborhood (green arrow).
 general, be different (cid:107)∇f (x)(cid:107)1 point x (red arrow).
 al.,
 Currently, approaches take several hours longer compute loss single example small networks few hidden units.
 Training network require performing computation inner loop, is infeasible.
 paper, introduce approach avoids inaccuracy lower bounds intractability exact computation, computing upper bound worst-case loss neural networks hidden layer, based semideﬁnite relaxation be computed efﬁciently.
 upper bound serves certiﬁcate robustness attacks given network input.
 Minimizing upper bound is safer minimizing lower bound, points bound is loose have objective values, optimizer tend avoid.
 Furthermore, certiﬁcate robustness, virtue being differentiable, is trainable—it be optimized training time network, acting regularizer encourages robustness (cid:96)∞ attacks.
 summary, are ﬁrst (along concurrent work Kolter Wong (2017)) demon- strate certiﬁable, trainable, scalable method defending adversarial examples two-layer networks.
 train network MNIST test error clean data is comes certiﬁcate attack misclassify more test examples using (cid:96)∞ perturbations size  = Notation.
 vector z ∈ Rn, use zi denote ith coordinate z.
 matrix Z ∈ Rm×n, Zi denotes ith row.
 activation function σ R → R (e.g., sigmoid, ReLU) vector z ∈ Rn, σ(z) is vector Rn σ(z)i = σ(zi) (non-linearity is applied element-wise).
 use B(z) denote (cid:96)∞ ball radius  z ∈ Rd: B(z) = {˜z | |˜zi − zi| ≤  i =


 d}.
 denote vector zeros vector ones
 SETUP Score-based classiﬁers.
 goal is learn mapping C X → Y, X = Rd is input space (e.g., images) Y = {1,


 is set k class labels (e.g., object categories).
 Assume C is driven scoring function f i X → R classes i ∈ Y, classiﬁer chooses class highest score: C(x) = arg f i(x).
 Also, deﬁne pairwise margin f ij(x) def= f i(x) − f j(x) pair classes (i, j).
 Note classiﬁer outputs C(x) = i iff f ij(x) > alternative classes j i.
 Normally, classiﬁer is evaluated 0-1 loss (cid:96)(x, y) = I[C(x) (cid:54)= y].
 paper focuses linear classiﬁers neural networks hidden layer.
 lin- i x, Wi is ith row parameter matrix W ∈ Rk×d.
 ear classiﬁers, f i(x) def= W (cid:62) review conference paper ICLR neural networks hidden layer consisting m hidden scoring function is i σ(W x), W ∈ Rm×d V ∈ Rk×m are parameters ﬁrst second f i(x) = V (cid:62) layer, respectively, σ is non-linear activation function applied elementwise (e.g., ReLUs, σ(z) = max(z,
 assume gradients σ are bounded: σ(cid:48)(z) ∈ [0, ∈ R; is true ReLUs, sigmoids (with stronger bound σ(cid:48)(z) ∈ [0, ]).
 Attack model.
 are interested classiﬁcation presence attacker A X → X takes (test) input x returns perturbation ˜x.
 consider attackers A perturb feature xi most  ≥ formally, A(x) is required lie (cid:96)∞ ball B(x) def= {˜x | (cid:107)˜x − x(cid:107)∞ ≤ }, is standard constraint ﬁrst proposed Szegedy al.
 (2014).
 Deﬁne adversarial loss respect A (cid:96)A(x, y) = I[C(A(x)) (cid:54)= y].
 assume white-box setting, attacker A has full knowledge C.
 optimal (untargeted) attack chooses input maximizes pairwise margin incorrect class i correct class y: Aopt(x) = arg maxi f iy(˜x).
 neural network, computing Aopt is non-convex optimization problem; heuristics are such Fast Gradient Sign Method (FGSM) (Goodfellow et perturbs x based gradient, Carlini-Wagner attack, performs iterative optimization (Carlini Wagner,
 CERTIFICATE ON THE ADVERSARIAL LOSS ease exposition, ﬁrst consider binary classiﬁcation classes Y = {1, multiclass extension is discussed end Section Without loss generality, assume correct label x is y
 Simplifying notation, let f (x) = f − f be margin incorrect class correct class.
 Aopt(x) = arg f (˜x) is optimal attack, is successful f (Aopt(x)) >
 f (Aopt(x)) is intractable compute, try upper tractable relaxation.
 rest section, ﬁrst review classic result simple case linear networks tight upper bound is based (cid:96)1-norm weights (Section
 extend general classiﬁers, f (Aopt(x)) be upper bounded using maximum (cid:96)1-norm gradient point ˜x ∈ B(x) (Section
 two-layer networks, quantity is upper bounded optimal value fQP(x) non-convex quadratic program (QP) (Section turn is upper bounded optimal value fSDP(x) semideﬁnite program (SDP).
 SDP is convex be computed (which is important obtainining actual certiﬁcates).
 summarize, have following chain inequalities: f (A(x)) ≤ f (Aopt(x)) (3.2)≤ f (x) +  max ˜x∈B(x) (cid:107)∇f (˜x)(cid:107)1 (3.3)≤ fQP(x) (3.3)≤ fSDP(x), (1) implies adversarial loss (cid:96)A(x) = I[f (A(x)) > respect attacker A is upper bounded I[fSDP(x) >
 Note certain non-linearities such ReLUs, ∇f (˜x) does exist everywhere, analysis below holds f is differentiable almost-everywhere.
 LINEAR CLASSIFIERS (binary) linear classiﬁers, have f (x) = (W1 − W2)(cid:62)x, W1, W2 ∈ Rd are weight vectors classes.
 input ˜x ∈ B(x), Hölder’s inequality (cid:107)x − ˜x(cid:107)∞ ≤  gives: (2) f (˜x) = f (x) + (W1 − W2)(cid:62)(˜x − x) ≤ f (x) + (cid:107)W1 − W2(cid:107)1.
 Note bound is tight, obtained taking Aopt(x)i = xi +  sign(W1i − W2i).
 GENERAL CLASSIFIERS more general classiﬁers, cannot compute f (Aopt(x)) exactly, motivated above, use gradient obtain linear approximation g: f (˜x) ≈ g(˜x) def= f (x) + ∇f (x)(cid:62)(cid:0)˜x − x(cid:1) ≤ f (x) + (cid:107)∇f (x)(cid:107)1.
 (3) review conference paper ICLR Using linear approximation generate A(x) corresponds Fast Gradient Sign Method (FGSM) (Goodfellow et
 However, f is g ˜x is close x, people have observed gradient masking phenomenon (Tramèr Papernot several proposed defenses train approximations g, such saturat- ing networks (Nayebi Ganguli, distillation (Papernot al., adversarial training (Goodfellow et al.,
 Speciﬁcally, defenses try minimize (cid:107)∇f (x)(cid:107)1 train- ing points result loss surfaces exhibit sharp curvature points, rendering linear approximation g(˜x) meaningless.
 attacks (Carlini Wagner, Tramèr evade defenses witness large f (Aopt(x)).
 Figure provides simple illustration.
 propose alternative approach: use integration obtain exact expression f (˜x) terms gradients line x ˜x: (cid:90) ∇f(cid:0)t˜x + (1 − t)x(cid:1)(cid:62)(cid:0)˜x − x(cid:1)dt f (˜x) = f (x) + ≤ f (x) + max ˜x∈B(x) (cid:107)∇f (˜x)(cid:107)1, (4) inequality follows fact t˜x + (1 − t)x ∈ B(x) t ∈ [0,
 key difference (4) (3) is consider gradients entire ball B(x) x (Figure
 However, computing RHS (4) is intractable general.
 two-layer neural networks, optimization has additional structure exploit next section.
 TWO-LAYER NEURAL NETWORKS unpack upper bound (4) two-layer neural networks.
 Recall Section f (x) = f − f = v(cid:62)σ(W x), v def= V1 − V2 ∈ Rm is difference second-layer weights classes.
 Let try bound norm gradient (cid:107)∇f (˜x)(cid:107)1 ˜x ∈ B(x).
 apply chain rule, see only dependence ˜x is σ(cid:48)(W activation derivatives.
 leverage assumption σ(cid:48)(z) ∈ [0, vectors z ∈ Rm, optimize possible activation derivatives s ∈ independent x (note is potential looseness such s need be obtainable ˜x ∈ B(x)).
 Therefore: (cid:107)∇f (˜x)(cid:107)1 (i) = (cid:107)W (cid:62) diag(v)σ(cid:48)(W ˜x)(cid:107)1 (ii)≤ max (cid:107)W (cid:62) diag(v)s(cid:107)1 s∈[0,1]m (iii) (5) (i) follows chain rule, (ii) uses fact σ has bounded derivatives σ(cid:48)(z) ∈ (iii) follows identity (cid:107)z(cid:107)1 = maxt∈[−1,1]d t(cid:62)z.
 (Note sigmoid networks, σ(cid:48)(z) ∈ [0, Substituting bound (5) (4), obtain upper bound adversarial loss call fQP: strengthen above bound corresponding factor max s∈[0,1]m,t∈[−1,1]d t(cid:62)W (cid:62) diag(v)s, f (Aopt(x)) ≤ f (x) +  max ˜x∈B(x) (cid:107)∇f (˜x)(cid:107)1 ≤ f (x) +  max s∈[0,1]m,t∈[−1,1]d t(cid:62)W (cid:62) diag(v)s def= fQP(x).
 (6) Unfortunately, (6) involves non-convex optimization problem (since W (cid:62) diag(v) is neces- negative semideﬁnite).
 fact, is similar NP-hard MAXCUT problem, requires maximizing x(cid:62)Lx x ∈ [−1, graph Laplacian matrix L.
 MAXCUT is NP-hard, be approximated, shown celebrated semidef- inite programming relaxation MAXCUT Goemans Williamson (1995).
 follow similar approach obtain upper bound fQP(x).
 First, make variables lie [0, reparametrize s produce: max s∈[−1,1]m,t∈[−1,1]d t(cid:62)W (cid:62) diag(v)(1 + s).
 (7) review conference paper ICLR Next pack variables vector y ∈ Rm+d+1 parameters matrix M: (cid:62) diag(v) W (cid:62) diag(v) M (v, W def= y def= diag(v)(cid:62)W diag(v)(cid:62)W (cid:34) (cid:35)  
 (8) terms new objective takes form: max y∈[−1,1](m+d+1) (9) Note valid vector y ∈ [−1, +1]m+d+1 satisﬁes constraints yy(cid:62) (cid:23) (yy(cid:62))jj =
 Deﬁning P = obtain following convex semideﬁnite relaxation problem: y∈[−1,1](m+d+1) max y(cid:62)M (v, W )y = (cid:104)M (v, W ), yy(cid:62)(cid:105).
 fQP(x) ≤ fSDP(x) def= f (x) + max P(cid:23)0,diag(P )≤1 (cid:104)M (v, W ), P(cid:105)
 (10) Note optimization semideﬁnite program depends weights v W does depend inputs x, needs be computed model (v, W ).
 Semideﬁnite programs be solved off-the-shelf optimizers, optimizers are slow large instances.
 Section propose fast stochastic method training, requires computing top eigenvalue matrix.
 Generalization multiple classes.
 preceding arguments generalize pairwise margins f ij, give: f ij(A(x)) ≤ f ij SDP(x) def= f ij(x) + (11) M ij(V, W is deﬁned (9) v = Vi − Vj. adversarial loss attacker, (cid:96)A(x, y) = SDP(x) ≥ f iy(A(x)).
 particular, I[maxi(cid:54)=y f iy(A(x)) > be bounded using fact f iy (12) (cid:96)A(x, y) maxi(cid:54)=y f iy P(cid:23)0,diag(P )≤1 max SDP(x) <
 (cid:10)M ij(V, W ), P(cid:11) TRAINING THE CERTIFICATE previous section, proposed upper bound (12) loss (cid:96)A(x, y) attack A, based bound (11).
 Normal training classiﬁcation loss (cid:96)cls(V, W yn) hinge loss cross-entropy encourage pairwise margin f ij(x) be large magnitude, won’t cause second term (11) involving M ij be small.
 natural strategy is use following regularized objective given training examples (xn, pushes terms: (cid:10)M ij(V, W ), P(cid:11) (13) (W (cid:63), V (cid:63)) = arg (cid:96)cls(V, W xn, + λij max P(cid:23)0,diag(P )≤1 (cid:88) W,V (cid:88) i(cid:54)=j i(cid:54)=j λij > are regularization hyperparameters.
 However, computing gradients above objective involves ﬁnding optimal solution semideﬁnite program, is slow.
 Duality rescue.
 computational burden is lifted beautiful theory duality, provides following equivalence primal maximization problem P dual minimization problem new variables c (see Section A details): (cid:0)M ij(V, W − diag(cij)(cid:1) max(c, (cid:10)M ij(V, W ), P(cid:11) = min D · λ+ max P(cid:23)0,diag(P )≤1 cij∈RD max (14) max(B) is maximum eigenvalue B, eigenvalues are D = (d + m + λ+ negative.
 dual formulation allows introduce additional dual variables cij ∈ RD are optimized same time parameters V W resulting objective be trained using stochastic gradient methods.
 ﬁnal objective.
 Using (14), end optimizing following training objective: (W (cid:63), V (cid:63), c(cid:63)) = arg W,V,c λij ·(cid:2)D · λ+ max(M ij(V, W − diag(cij)) max(cij,
 (cid:96)cls(V, W xn, + (cid:88) (cid:88) (15) review conference paper ICLR objective (15) be optimized efﬁciently.
 expensive operation is λ+ max, requires computing maximum eigenvector matrix M ij − diag(cij) order take gradi- ents.
 be done using standard implementations iterative methods Lanczos.
 Further implementation details (including tuning λij) are presented Section Dual certiﬁcate robustness.
 dual formulation is useful value dual is upper bound optimal value primal.
 Speciﬁcally, (W [t], V [t], c[t]) are parameters iteration t training, (cid:0)M ij(V [t], W [t]) − diag(c[t]ij)(cid:1) max(c[t]ij, (cid:2)D · λ+ (16) f ij(A(x)) ≤ f (x) + max attack A.
 train network, obtain quick upper bound worst-case adver- sarial loss regularization loss, having optimize SDP time.
 OTHER UPPER BOUNDS Section described function f ij SDP yields efﬁcient upper bound adversarial loss, obtained using convex relaxations.
 consider other simple ways upper loss; describe common ones based spectral Frobenius norms.
 Spectral bound: Note v(cid:62)(σ(W ˜x) − σ(W x)) ≤ (cid:107)v(cid:107)2(cid:107)σ(W ˜x) − σ(W x)(cid:107)2 Cauchy- Schwarz.
 Moreover, σ is contractive, (cid:107)σ(W ˜x) − σ(W x)(cid:107)2 ≤ (cid:107)W (˜x − x)(cid:107)2 ≤ (cid:107)W(cid:107)2(cid:107)˜x − x(cid:107)2 ≤  d(cid:107)W(cid:107)2, (cid:107)W(cid:107)2 is spectral norm (maximum singular value) W
 yields following upper bound denote fspectral: f ij(A(x)) ≤ f ij spectral(x) def= f ij(x) +  d(cid:107)W(cid:107)2(cid:107)Vi − Vj(cid:107)2.
 (17) measure vulnerability adversarial examples based spectral norms weights layer is considered Szegedy al.
 (2014) Cisse et al.
 (2017).
 Frobenius bound: ease training, Frobenius norm is regularized (weight decay) spectral norm.
 (cid:107)W(cid:107)F ≥ (cid:107)W(cid:107)2, get corresponding upper bound ffrobenius: (18) f ij(A(x)) ≤ f ij d(cid:107)W(cid:107)F(cid:107)Vi − Vj(cid:107)2.
 frobenius(x) = f ij(x) +  Section compare proposed bound using f ij SDP upper bounds.
 EXPERIMENTS evaluated method MNIST dataset handwritten digits, task is classify images ten classes.
 results be summarized follows: First, Section show certiﬁcates robustness are tighter based simpler methods such Frobenius spectral bounds (Section bounds are high be meaningful general networks.
 Section show training certiﬁcates, obtain net- works much better bounds meaningful robustness.
 reﬂects important point: analyzing robustness arbitrary network is hard, training certiﬁcate leads network is robust so.
 Section present implementation details, design choices, empirical observations made implementing method.
 Networks.
 work, focus layer networks.
 experiments, used neural networks m hidden units, TensorFlow’s implementation Adam (Kingma Ba, optimizer; considered networks more hidden units, did substan- improve accuracy.
 experimented multiclass hinge loss cross-entropy.
 hyperparameters (including choice loss function) were tuned based error Pro- jected Gradient Descent (PGD) attack (Madry  report hyperparameter settings below.
 considered following training objectives providing different
 Normal training (NT-NN).
 Cross-entropy loss explicit regularization.

 Frobenius norm regularization (Fro-NN).
 Hinge loss regularizer λ((cid:107)W(cid:107)F +(cid:107)v(cid:107)2) λ
 review conference paper ICLR
 Spectral regularization (Spe-NN).
 Hinge loss regularizer λ((cid:107)W(cid:107)2 + (cid:107)v(cid:107)2) λ

 Adversarial training (AT-NN).
 Cross-entropy adversarial loss PGD regularizer, regularization parameter set found regularized loss works optimizing adversarial loss, is defense proposed Madry al.
 (2017).
 set step size PGD adversary number iterations perturbation size
 Proposed training objective (SDP-NN).
 Dual SDP objective described Equation Section
 Implementation details hyperparameter values are detailed Section Evaluating upper bounds.
 consider various upper bounds adversarial loss (cid:96)Aopt (based method, Frobenius spectral bounds described Section
 compare ground-truth adversarial loss (cid:96)Aopt, computing is difﬁcult.
 compare upper bounds adversarial loss lower bound (cid:96)Aopt instead.
 loss attack provides valid lower bound consider strong Projected Gradient Descent (PGD) attack run cross-entropy loss, starting random point B(x), random restarts.
 observed PGD hinge loss did work used cross-entropy attacking networks trained hinge loss.
 QUALITY THE UPPER BOUND ﬁve networks described computed upper bounds 0-1 loss based certiﬁcate refer “SDP bound” section), Frobenius spectral bounds described Section
 Section provides procedure obtaining SDP bound result training, networks trained method need solve SDP end training obtain certiﬁcates.
 Fortunately, only needs be done pair classes.
 experiments, use modeling toolbox YALMIP (Löfberg, Sedumi (Sturm, backend solve SDPs, using dual form (14); took minutes SDP (around hours total given model).
 Figure display average values different upper bounds test examples, corresponding lower bound PGD.
 ﬁnd bound is tighter Frobenius spectral bounds networks considered, tightness relative PGD lower bound varies networks.
 instance, bound is tight Fro-NN, Fro-NN is robust adversarial examples (the PGD attack exhibits large error).
 contrast, trained network AT-NN does appear be robust attacks, certiﬁcate, being much tighter Frobenius spectral bounds, is PGD lower bound.
 only network is robust has tight upper bounds is SDP-NN, was trained be robust certiﬁable described Section examine network effects training more detail next subsection.
 EVALUATING PROPOSED TRAINING OBJECTIVE.
 previous section, saw SDP bound, being tighter simpler upper bounds, be loose arbitrary networks.
 However, optimizing SDP certiﬁcate seemed make certiﬁcate tighter.
 section, explore effect different optimization objectives more detail.
 plot single axis best upper bound (i.e., SDP bound) lower bound PGD) adversarial loss obtained ﬁve training objectives discussed above.
 is given Figure
 spectral Frobenius norm regularization seems be helpful encouraging adversarial robustness—the actual performance networks PGD attack is worse upper bound SDP-NN attacks.
 shows SDP certiﬁcate provides useful training objective encouraging robustness compared other regularizers.
 ask SDP-NN is robust actual attacks.
 explore robustness network Figure plot performance SDP-NN attacks—the PGD attack Carlini-Wagner attack (Carlini Wagner, (another strong attack), weaker Fast Gradient Sign Method (FGSM) baseline.
 see substantial robustness attacks, method was trained mind.
 review conference paper ICLR (a) NT-NN (b) Fro-NN (c) Spe-NN (d) AT-NN (e) SDP-NN Figure Upper bounds adversarial error different networks MNIST.
 (a) (b) Figure (a) Upper bound (SDP) lower bound (PGD) adversarial error different networks.
 (b) Error SDP-NN different attacks.
 review conference paper ICLR Network SDP-NN LP-NN PGD error SDP bound LP bound Table Comparison bound (LP bound) training approach Kolter Wong (2017).
 Numbers are reported  = LP-NN has certiﬁcate (provided LP bound) attack misclassify more examples.
 Next, compare other bounds reported literature.
 rough ceiling is given network Madry al.
 (2017), is large four-layer convolutional network trained PGD.
 network has accompanying certiﬁcate robustness, was eval- uated number attack strategies had worst-case error  = set numbers comes Carlini al.
 (2017), use formal veriﬁcation methods compute Aopt input examples small (72-node) variant Madry et al.
 network.
 authors reported network misclassiﬁes examples  = (we note were misclassiﬁed start with, be ﬂipped different wrong class  <
 value  = was tuned, SDP-NN has error PGD attack, upper bound error attack.
 is better small 72-node network, much worse full Madry et al.
 network.
 much latter looseness comes conservatism method, fact network has layers?
 get idea considering AT-NN network, was trained Madry al., uses same architecture SDP-NN.
 Figure see error SDP-NN PGD (16%) is much worse AT-NN (11%), AT-NN was trained PGD attack.
 suggests most gap comes smaller network depth, conservatism SDP bound.
 are process extending approach deeper networks, optimistic obtaining improved bounds such networks.
 compare approach proposed Kolter Wong (2017) work appeared initial version paper.
 provide upper bound adversarial loss using linear programs (LP) followed method train networks minimize upper bound.
 order compare SDP-NN, authors provided network same architecture SDP-NN, trained using LP based objective.
 call network LP-NN.
 Table shows LP-NN SDP-NN are comparable terms robustness PGD, robustness guarantees come with.
 Interestingly, SDP LP approaches provide vacuous bounds networks trained min- imize respective upper bounds networks are robust).
 suggests approaches are comparable, complementary.
 note contrast work, approach Kolter Wong (2017) extends deeper networks, allows train four-layer CNN provable upper bound adversarial error error.
 IMPLEMENTATION DETAILS implemented training objective TensorFlow, implemented λ+ max custom operator using SciPy’s implementation Lanczos algorithm fast top eigenvector computation; oc- Lanczos fails converge due small eigen-gap, case back full SVD.
 used hinge loss classiﬁcation loss, decayed learning rate steps decreasing factor epochs.
 gradient step involves computing top eigenvectors different matrices, pair classes (i, j).
 order speed com- putation, update, randomly pick compute gradients pairs (it, j), j (cid:54)= it, requiring top eigenvector computations step.
 regularization parameters λij, simplest idea is set equal same value; leads unweighted regularization scheme λij = λ pairs (i, j).
 tuned λ led good bounds.
 However, observed certain pairs classes tended have larger margins ij(x) other classes, meant certain label pairs appeared maximum (12) often.
 led consider weighted regularization scheme λij = wijλ, wij is fraction training points label i (or j) appears review conference paper ICLR (a) (b) Figure (a) Weighted unweighted regularization schemes.
 network produced weighting has better certiﬁcate lower error PGD attack.
 (b) dual certiﬁcate robustness (SDP dual), obtained training, is good certiﬁcate produced solving SDP.
 maximizing term (12).
 updated values weights epochs.
 Figure compares PGD lower bound SDP upper bound unweighted weighted networks.
 weighted network is unweighted network lower upper bounds.
 saw Equation Section dual variables cij provide quick-to-compute certiﬁcate robustness.
 Figure shows certiﬁcates provided dual variables are obtain optimizing semideﬁnite programs.
 dual certiﬁcates made easy track robustness epochs training tune hyperparameters.
 DISCUSSION work, proposed method producing certiﬁcates robustness neural networks, training certiﬁcates obtain networks are robust adversaries.
 Related work.
 parallel independent work, Kolter Wong (2017) provide robust networks (cid:96)∞ perturbations using convex relaxations.
 approach uses single semideﬁnite program compute upper bound adversarial loss, Kolter Wong (2017) use separate linear programs data point, apply method networks depth four.
 theory, bound is tighter other, experiments (Table suggest bounds are complementary.
 Combining approaches seems be promising future direction.
 Katz al.
 (2017a) follow-up Carlini et al.
 (2017) provide certiﬁcates robustness neural networks (cid:96)∞ perturbations.
 work uses SMT solvers, are tool formal veriﬁcation literature.
 SMT solver answer binary question “Is adversarial example distance  input x?”, is correct terminates.
 main drawback SMT similar formal veriﬁcation methods is are slow—they have worst-case exponential-time scaling size network; moreover, use training require separate search gradient step.
 Huang al.
 (2017) use SMT solvers are able analyze state-of-the-art networks MNIST, make various approximations such numbers are true upper bounds.
 Bastani al.
 (2016) provide tractable certiﬁcates require  be small ensure entire (cid:96)∞ ball input lies same linear region.
 networks values  consider paper, found condition did hold.
 Recently, Hein An- driushchenko (2017) proposed bound guaranteeing robustness (cid:96)p-norm perturbations, based maximum p p−1-norm gradient -ball inputs.
 Hein Andriushchenko (2017) show compute bound p opposed work focuses (cid:96)∞ requires different techniques achieve scalability.
 review conference paper ICLR Madry al.
 (2017) perform adversarial training PGD MNIST CIFAR-10 datasets, obtaining networks suggest are “secure ﬁrst-order adversaries”.
 However, is based empirical observation PGD is nearly-optimal gradient-based attacks, does correspond formal robustness guarantee.
 notion certiﬁcate appears theory convex optimization, means something different context; speciﬁcally, corresponds proof point is optimum convex function, certiﬁcates provide upper bounds non-convex functions.
 Addi- tionally, robust optimization (Bertsimas al., provides tool optimizing objectives robustness constraints, applying involve same intractable optimization Aopt deal here.
 Other approaches veriﬁcation.
 have been explored context neural networks, are approaches control theory literature verifying robustness dynamical systems, based Lyapunov functions (Lyapunov,
 think activations neural network evolution time-varying dynamical system, attempt prove stability trajectory system (Tedrake et al., Tobenkin
 Such methods use sum-of-squares veriﬁcation (Papachristodoulou Prajna, Parrilo, are restricted low-dimensional dynamical systems, scale larger settings.
 approach is construct families networks are robust priori, remove need verify robustness learned model; knowledge has been done expressive model families.
 Adversarial examples secure ML.
 has been great deal recent work security ML systems; provide sampling here, refer reader Barreno al.
 (2010), Biggio et al.
 (2014a), Papernot et al.
 (2016b), Gardiner Nagaraja (2016) recent surveys.
 Adversarial examples neural networks were discovered Szegedy al.
 (2014), number attacks defenses have been proposed.
 have discussed gradient- based methods defenses based adversarial training.
 are other attacks based e.g., saliency maps (Papernot et al., KL divergence (Miyato al., elastic net optimization (Chen et many attacks are collated cleverhans repository (Goodfellow et al.,
 defense, making networks robust adversaries, work has focused detecting adversarial examples.
 However, Carlini Wagner (2017a) showed known detection methods be subverted strong attacks.
 explained Barreno al.
 (2010), are number different attack models test- time attacks considered here, based different attacker goals capabilities.
 instance, consider data poisoning attacks, attacker modiﬁes training set effort affect test-time performance.
 Newsome al.
 (2006), Laskov Šrndi`c (2014), Biggio et al.
 (2014b) have demonstrated poisoning attacks real-world systems.
 Other types certiﬁcates.
 Certiﬁcates performance machine learning systems are desirable number settings.
 includes verifying safety properties air trafﬁc control systems (Katz et al., self-driving cars et al., security applications such robustness training time attacks (Steinhardt
 broadly, certiﬁcates performance are likely necessary deploying machine learning systems critical infrastructure such internet packet routing (Winstein Balakrishnan, Sivaraman et
 robotics, certiﬁcates stability are used safety veriﬁcation (Lygeros Mitchell al., controller synthesis (Ba¸sar Bernhard, Tedrake
 traditional veriﬁcation work, Rice’s theorem (Rice, is strong impossibility result essen- stating most properties most programs are undecidable.
 Similarly, expect verifying robustness arbitrary neural networks is hard.
 However, results work suggest is possible learn neural networks are amenable veriﬁcation, same way is possible write programs be veriﬁed.
 Optimistically, given expressive enough certiﬁcation methods model families, strong enough speciﬁcations robust- ness, one hope train vector representations natural images strong robustness properties, closing chapter adversarial vulnerabilities visual domain.
 review conference paper ICLR REFERENCES M.
 Barreno, B.
 Nelson, A.
 D.
 Joseph, J.
 D.
 Tygar.
 security machine learning.
 Machine Learning,
 T.
 Ba¸sar P.
 Bernhard.
 H-inﬁnity optimal control related minimax design problems: dy- namic game approach.
 Springer Science Business Media,
 O.
 Bastani, Y.
 Ioannou, L.
 Lampropoulos, D.
 Vytiniotis, A.
 Nori, A.
 Criminisi.
 Measuring neural net robustness constraints.
 Advances Neural Information Processing Systems (NIPS), pp.

 D.
 Bertsimas, D.
 B.
 Brown, C.
 Caramanis.
 Theory applications robust optimization.
 SIAM review,
 B.
 Biggio, G.
 Fumera, F.
 Roli.
 Security evaluation pattern classiﬁers attack.
 IEEE Transactions Knowledge Data Engineering,
 B.
 Biggio, K.
 Rieck, D.
 Ariu, C.
 Wressnegger, I.
 Corona, G.
 Giacinto, F.
 Roli.
 Poisoning be- havioral malware clustering.
 Workshop Artiﬁcial Intelligence Security (AISec),
 N.
 Carlini D.
 Wagner.
 Defensive distillation is robust adversarial examples.

 N.
 Carlini D.
 Wagner.
 Adversarial examples are detected: Bypassing ten detection methods.

 N.
 Carlini D.
 Wagner.
 Towards evaluating robustness neural networks.
 IEEE Sympo- sium Security Privacy, pp.

 N.
 Carlini, P.
 Mishra, T.
 Vaidya, Y.
 Zhang, M.
 Sherr, C.
 Shields, D.
 Wagner, W.
 Zhou.
 Hidden voice commands.
 USENIX Security,
 N.
 Carlini, G.
 Katz, C.
 Barrett, D.
 L.
 Dill.
 Ground-truth adversarial examples.

 P.
 Chen, Y.
 Sharma, H.
 Zhang, J.
 Yi, C.
 Hsieh.
 EAD: Elastic-net attacks deep neural networks adversarial examples.

 M.
 Cisse, P.
 Bojanowski, E.
 Grave, Y.
 Dauphin, N.
 Usunier.
 Parseval Improving robustness adversarial examples.
 International Conference Machine Learning (ICML), pp.

 I.
 Evtimov, K.
 Eykholt, E.
 Fernandes, T.
 Kohno, B.
 Li, A.
 Prakash, A.
 Rahmati, D.
 Song.
 Robust physical-world attacks machine learning models.

 J.
 Gardiner S.
 Nagaraja.
 security machine learning malware c&c detection: survey.
 ACM Computing Surveys (CSUR),
 M.
 Goemans D.
 Williamson.
 Improved approximation algorithms maximum cut satisﬁa- bility problems using semideﬁnite programming.
 Journal ACM (JACM),
 I.
 Goodfellow, N.
 Papernot, P.
 McDaniel.
 cleverhans v2.0.0: adversarial machine learning library.

 I.
 J.
 Goodfellow, J.
 Shlens, C.
 Szegedy.
 Explaining harnessing adversarial examples.
 International Conference Learning Representations (ICLR),
 K.
 He, X.
 Zhang, S.
 Ren, J.
 Sun.
 Delving deep rectiﬁers: Surpassing human-level perfor- mance imagenet classiﬁcation.
 arXiv preprint arXiv:1502.01852,
 M.
 Hein M.
 Andriushchenko.
 Formal guarantees robustness classiﬁer adver- sarial manipulation.
 arXiv preprint arXiv:1705.08475,
 X.
 Huang, M.
 Kwiatkowska, S.
 Wang, M.
 Wu. Safety veriﬁcation deep neural networks.
 Computer Aided Veriﬁcation (CAV), pp.

 review conference paper ICLR G.
 Katz, C.
 Barrett, D.
 Dill, K.
 Julian, M.
 Kochenderfer.
 Reluplex: efﬁcient SMT solver verifying deep neural networks.
 arXiv preprint arXiv:1702.01135,
 G.
 Katz, C.
 Barrett, D.
 L.
 Dill, K.
 Julian, M.
 J.
 Kochenderfer.
 Towards proving adversarial robustness deep neural networks.

 D.
 Kingma J.
 Ba. Adam: A method stochastic optimization.
 arXiv preprint arXiv:1412.6980,
 J.
 Z.
 Kolter E.
 Wong.
 Provable defenses adversarial examples convex outer adversarial polytope.
 arXiv preprint arXiv:1711.00851,
 A.
 Kurakin, I.
 Goodfellow, S.
 Bengio.
 Adversarial examples physical world.

 P.
 Laskov N.
 Šrndi`c.
 Practical evasion learning-based classiﬁer: case study.
 Symposium Security Privacy,
 J.
 Löfberg.
 YALMIP: toolbox modeling optimization MATLAB.
 CACSD,
 A.
 M.
 Lyapunov.
 general problem stability motion (in Russian).
 PhD thesis, Kharkov Mathematical Society,
 A.
 M.
 Lyapunov.
 general problem stability motion.
 International Journal Control,
 J.
 Lygeros, C.
 Tomlin, S.
 Sastry.
 Controllers reachability speciﬁcations hybrid systems.
 Automatica,
 A.
 Madry, A.
 Makelov, L.
 Schmidt, D.
 Tsipras, A.
 Vladu.
 Towards deep learning models resistant adversarial attacks.

 I.
 M.
 Mitchell, A.
 M.
 Bayen, C.
 J.
 Tomlin.
 time-dependent Hamilton-Jacobi formulation reachable sets continuous dynamic games.
 IEEE Transactions Automatic Control,
 T.
 Miyato, S.
 Maeda, M.
 Koyama, K.
 Nakae, S.
 Ishii.
 Distributional smoothing virtual adversarial training.

 A.
 Nayebi S.
 Ganguli.
 inspired protection deep networks adversarial at- tacks.
 arXiv preprint arXiv:1703.09202,
 J.
 Newsome, B.
 Karp, D.
 Song.
 Paragraph: Thwarting signature learning training mali- ciously.
 International Workshop Recent Advances Intrusion Detection,
 M.
 O’Kelly, H.
 Abbas, S.
 Gao, S.
 Shiraishi, S.
 Kato, R.
 Mangharam.
 APEX: Autonomous vehicle plan veriﬁcation execution.
 Technical report, University Pennsylvania,
 M.
 O’Kelly, H.
 Abbas, R.
 Mangharam.
 Computer-aided design safe autonomous vehicles.
 Technical report, University Pennsylvania,
 A.
 Papachristodoulou S.
 Prajna.
 construction lyapunov functions using sum squares decomposition.
 IEEE Conference Decision Control,
 A.
 Papachristodoulou S.
 Prajna.
 Analysis non-polynomial systems using sum squares decomposition.
 Positive polynomials control,
 N.
 Papernot, P.
 McDaniel, S.
 Jha, M.
 Fredrikson, Z.
 B.
 Celik, A.
 Swami.
 limitations deep learning adversarial settings.
 Security Privacy (EuroS&P), IEEE European Symposium on, pp.

 N.
 Papernot, P.
 McDaniel, A.
 Sinha, M.
 Wellman.
 science security privacy machine learning.

 N.
 Papernot, P.
 McDaniel, X.
 Wu, S.
 Jha, A.
 Swami.
 Distillation defense adversarial perturbations deep neural networks.
 IEEE Symposium Security Privacy, pp.

 review conference paper ICLR P.
 A.
 Parrilo.
 Semideﬁnite programming relaxations semialgebraic problems.
 Mathematical programming,
 H.
 G.
 Rice.
 Classes enumerable sets decision problems.
 Transactions American Mathematical Society,
 M.
 Sharif, S.
 Bhagavatula, L.
 Bauer, M.
 K.
 Reiter.
 Accessorize crime: Real stealthy attacks state-of-the-art face recognition.
 ACM SIGSAC Conference Computer Com- munications Security, pp.

 D.
 Silver, A.
 Huang, C.
 J.
 Maddison, A.
 Guez, L.
 Sifre, G.
 V.
 D.
 Driessche, J.
 Schrittwieser, I.
 Antonoglou, V.
 Panneershelvam, M.
 Lanctot, al.
 Mastering game go deep neural networks tree search.
 Nature,
 A.
 Sivaraman, K.
 Winstein, P.
 Thaker, H.
 Balakrishnan.
 experimental study learnabil- ity congestion control.
 SIGCOMM,
 J.
 Steinhardt, P.
 W.
 Koh, P.
 Liang.
 Certiﬁed defenses data poisoning attacks.
 Advances Neural Information Processing Systems (NIPS),
 J.
 F.
 Sturm.
 Using SeDuMi MATLAB toolbox optimization symmetric cones.
 Optimization Methods Software,
 C.
 Szegedy, W.
 Zaremba, I.
 Sutskever, J.
 Bruna, D.
 Erhan, I.
 Goodfellow, R.
 Fergus.
 Intriguing properties neural networks.
 International Conference Learning Representations (ICLR),
 R.
 Tedrake, I.
 R.
 Manchester, M.
 M.
 Tobenkin, J.
 W.
 Roberts.
 LQR-trees: Feedback motion planning sums squares veriﬁcation.
 International Journal Robotics Research,
 M.
 M.
 Tobenkin, I.
 R.
 Manchester, R.
 Tedrake.
 Invariant funnels trajectories using sum-of-squares programming.
 IFAC Proceedings Volumes,
 F.
 Tramèr, A.
 Kurakin, N.
 Papernot, D.
 Boneh, P.
 McDaniel.
 Ensemble adversarial training: Attacks defenses.
 arXiv preprint arXiv:1705.07204,
 K.
 Winstein H.
 Balakrishnan.
 TCP ex Computer-generated congestion control.
 SIGCOMM,
 W.
 Xiong, J.
 Droppo, X.
 Huang, F.
 Seide, M.
 Seltzer, A.
 Stolcke, D.
 Yu, G.
 Zweig.
 Achieving human parity conversational speech recognition.

 DUALITY section justify duality relation (14).
 Recall primal program is maximize (cid:104)M, P(cid:105) subject P (cid:23) diag(P ≤
 (19) Rather taking dual directly, ﬁrst add redundant constraint tr(P ≤ d + m + (it is redundant SDP is d + m + dimensions diag(P ≤
 yields maximize (cid:104)M, P(cid:105) subject P (cid:23) diag(P ≤ tr(P ≤ d + m +
 form Lagrangian constraints diag(P ≤ leaving other constraints as-is.
 yields equivalent optimization problem (cid:104)M, P(cid:105) + c(cid:62)(1 − diag(P )) maximize min c≥0 subject P (cid:23) tr(P ≤ d + m +
 review conference paper ICLR Now, apply minimax duality swap order min max; value (21) is equal (cid:104)M, P(cid:105) + c(cid:62)(1 − diag(P )) (22) minimize max P(cid:23)0, tr(P )≤d+m+1 subject c
 inner maximum be simpliﬁed (cid:104)M−diag(c), P(cid:105)(cid:17) max P(cid:23)0,tr(P )≤1 = max(M−diag(c)).
 (23) Therefore, (22) simpliﬁes minimize + (d + m subject c
 max(M − diag(c)) (24) is form given (14), c is constrained be non-negative have max(c,
 However, note λ+ max term, is c be larger; therefore, replacing c max(c, means optimal value c be non- negative, allowing drop c ≥ constraint optimize c unconstrained manner.
 ﬁnally yields claimed duality relation (14).

 Recurrent neural networks (RNNs) are speciﬁc type neural networks are designed model sequence data.
 last decades, various RNN architectures have been proposed, such Long- Short-Term Memory (LSTM) (Hochreiter Schmidhuber, Gated Recurrent Units Cho al.
 (2014).
 have enabled RNNs achieve state-of-art performance many applications, e.g., language models (Mikolov et al., neural machine translation (Sutskever et Wu automatic speech recognition (Graves al., image captions (Vinyals et etc.
 However, models build high dimensional input/output,e.g., large vocabulary language models, deep inner recurrent networks, making models have many parameters deploy portable devices limited resources.
 addition, RNNs be executed dependence current hidden states.
 causes large latency inference.
 applications server large scale concurrent requests, e.g., on-line machine translation speech recognition, large latency leads limited requests processed machine meet stringent response time requirements.
 costly computing resources are demand RNN based models.
 alleviate above problems, several techniques be employed, i.e., low rank approximation (Sainath et Jaderberg Lebedev Tai sparsity (Liu ∗Work performed interning Alibaba search algorithm team.
 †Corresponding author.
 Published conference paper ICLR et al., Han et Wen al., quantization.
 are build redundancy current networks be combined.
 work, focus quantization based methods.
 precisely, are quantize parameters multiple binary codes {−1, +1}.
 idea quantizing weights activations is proposed (Hubara
 has shown 1-bit binarization achieve good performance visual classiﬁcation tasks.
 Compared full precision counterpart, binary weights reduce memory factor
 costly arithmetic operations weights activations be replaced cheap XNOR bitcount operations(Hubara et al., leads much acceleration.
 Rastegari al.
 (2016) further incorporate real coefﬁcient compensate binarization error.
 apply method challenging ImageNet dataset achieve better performance pure binarization (Hubara
 However, is large gap compared full precision networks.
 bridge gap, recent works (Hubara et al., Zhou al., further employ quantization bits achieve plausible performance.
 Meanwhile, amount works, e.g., (Courbariaux Li Zhu Guo quantize weights only.
 much memory saving be achieved, acceleration is limited modern computing devices (Rastegari et al.,
 existing quantization works, most focus convolutional neural networks (CNNs) pay less attention RNNs. As mentioned latter is demanding.
 Recently, (Hou et showed binarized LSTM preconditioned coefﬁcients achieve promising performance easy tasks such predicting next character.
 However, RNNs large input/output, e.g., large vocabulary language models, is challenging quantization.
 works Hubara al.
 (2016b) Zhou et al.
 (2017) test effectiveness multi-bit quantized RNNs predict next word.
 using results quantization have noticeable gap full precision.
 motivates ﬁnd better method quantize RNNs. main contribution work is follows: formulate multi-bit quantization optimization problem.
 binary codes {−1, +1} are learned rule-based.
 ﬁrst time, observe codes be derived binary search tree coefﬁcients are knowns advance, see, e.g., Algorithm
 whole optimization is eased removing discrete unknowns, are handle.
 (b) propose use alternating minimization tackle quantization problem.
 separating binary codes real coefﬁcients solve subproblem part is ﬁxed.
 proper initialization, need alternating cycles get high precision approximation, is effective quantize activations on-line.
 (c) evaluate effectiveness alternating quantization language models.
 well-known RNN structures, i.e., LSTM GRU, are tested different quantization bits.
 Compared full-precision counterpart, 2-bit quantization achieve ∼16× memory saving ∼6× real inference acceleration CPUs, reasonable loss accuracy.
 3-bit quantization, achieve loss accuracy surpass original model ∼10.5× memory saving ∼3× real inference acceleration.
 results beat exiting quantization works large margins.
 illustrate alternating quantization is general extend, apply image classiﬁcation tasks.
 RNNs neural technique achieves plausible performance.
 EXISTING MULTI-BIT QUANTIZATION METHODS introducing proposed multi-bit quantization, ﬁrst summarize existing works follows: (a) Uniform quantization method (Rastegari Hubara scales value range x ∈ [−1,
 adopts following k-bit quantization: (cid:32) (cid:33) − qk(x) = round[(2k − x+1 )] − (1) method scales original range.
 Such quantization is rule based is easy implement.
 intrinsic beneﬁt is computing inner product Published conference paper ICLR Figure Illustration optimal 2-bit quantization α1 α2 (α1 ≥ α2) are known advance.
 values are quantized −α1 − α2, −α1 + α2, α1 − α2, α1 + α2, respectively.
 partition intervals are separated middle points adjacent quantization codes, i.e., α1, correspondingly.
 quantized employ cheap bit shift count operations replace costly multiplications additions operations.
 However, method be optimum quantizing non-uniform data, is believed be trained weights activations deep neural network (Zhou
 (b) Balanced quantization (Zhou alleviates drawbacks uniform quantization equalizing data.
 method constructs intervals contain same percentage data.
 maps center interval corresponding quantization code (1).
 sounding reasonable uniform one, afﬁne transform centers be suboptimal.
 addition, is guarantee spaced partition is suitable compared non-evenly spaced partition speciﬁc data distribution.
 (c) Greedy approximation (Guo tries learn quantization tackling following problem: bi ∈ {−1, +1}n.
 (2) min {αi,bi}k i=1 αibi (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)w − k(cid:88) (cid:107)ri−1 − αibi(cid:107)2 ri−1 = w − i−1(cid:88) i=1 min αi,bi k = above problem has closed-form solution (Rastegari
 Greedy approximation extends k-bit (k > quantization minimizing residue.
 is optimal solution is given αi = (cid:107)ri−1(cid:107)1 bi = sign(ri−1).
 j=1 αjbj.
 (3) (4) Greedy approximation is efﬁcient implement modern computing devices.
 able reach high precision solution, formulation minimizing quantization error is promising.
 (d) Reﬁned greedy approximation (Guo extends further decrease quantization error.
 j-th iteration minimizing problem (3), method adds extra step reﬁne computed {αi}j i=1 least squares solution: [α1,


 =(cid:0)(BT j w(cid:1)T j Bj)−1BT (5) experiments quantizing weights CNN, reﬁned approximation is veriﬁed be better original greedy one.
 However, show later, reﬁned method is satisfactory quantization accuracy.
 Bj = [b1,


 bj], general multi-bit quantization summarized above, Li et al.
 (2016) propose ternary quantization extending 1-bit binarization more feasible state,
 does quantization tackling minα,t (cid:107)w − αt(cid:107)2 t ∈ {−1, +1}n.
 However, efﬁcient algorithm is proposed (Li
 set entries w absolute scales binarize left entries (4).
 fact, ternary quantization is special case 2-bit quantization (2), additional constraint α1 α2.
 binary codes are optimal coefﬁcient α1 (or α2) be derived least squares solution similar (5).
 Quantize to-α1α10Quantize toQuantize toQuantize toα1-α2-α1+α2α1+α2-α1-α2 Published conference paper ICLR Figure Illustration binary search tree determine optimal quantization.
 Binary Search Tree (BST) determine optimal code BST(w, v) {w is real value be quantized} {v is vector quantization codes ascending order} m = length(v) m == return v1 end w ≥ (vm/2 + vm/2+1)/2 BST(w, vm/2+1:m) end BST( w, v1:m/2) parallel binarized quantization discussed here, vector quantization is applied compress weights feedforward neural networks et al., Han et
 Different ours weights are constraint {−1, +1}, vector quantization learns small codebook applying k-means clustering weights conducting product quantization.
 weights are reconstructed indexing codebook.
 has been shown technique, number parameters be reduced order magnitude limited accuracy loss (Gong
 is possible multi-bit quantized binary weight be compressed using product quantization.
 OUR ALTERNATING MULTI-BIT QUANTIZATION ascending order, i.e., v = {−(cid:80)k i=1 αi,


 introduce quantization method.
 tackle same minimization problem (2).
 simplicity, consider problem k
 Suppose α1 α2 are known advance α1 ≥ α2 ≥ quantization codes are restricted v = {−α1 − α2,−α1 + α2, α1 − α2, α1 + α2}.
 entry w w problem (2), quantization code is determined least distance codes.
 partition number axis intervals.
 interval corresponds particular quantization code.
 common point adjacent intervals becomes middle point quantization codes, i.e., α1.
 Fig.
 gives illustration.
 general k-bit quantization, suppose {αi}k i=1 are known have possible codes i=1 αi}.
 partition number axis intervals, boundaries are determined centers adjacent codes v, i.e., {(vi + vi+1)/2}2k−1 i=1
 However, comparing entry boundaries needs comparisons, is inefﬁcient.
 make use ascending property v.
 Hierarchically, partition codes v ordered sub-sets, i.e., v1:m/2 vm/2+1:m m deﬁned length v.
 w < (vm/2 + vm/2+1)/2, feasible codes are restricted v1:m/2.
 w ≥ (vm/2 +vm/2+1)/2 feasible codes become vm/2+1:m.
 evenly partition ordered feasible codes, determine -α1-α2 -α1+α2 α1-α2 α1+α2 -α1 α1 Published conference paper ICLR Algorithm Alternating Multi-bit Quantization Require :Full precision weight ∈ Rn, number bits k, total iterations T Ensure :{αi, bi}k Greedy Initialize {αi, bi}k iter T do (4) i=1 i=1 (5) Update {αi}k Construct v feasible codes accending order Update {bi}k i=1 Algorithm
 end optimal code entry k comparisons.
 whole procedure is fact binary search tree.
 summarize Algorithm
 Note getting quantization code, is straightforward map binary code b.
 Also, maintaining mask vector same size w indicate partitions, operate BST entries simultaneously.
 give better illustration, give binary tree example k = Fig.

 Note k = derive optimal codes closed form solution, i.e., b1 = sign(w) b2 = sign(w − α1b1) α1 ≥ α2 ≥
 above observation, let reconsider reﬁned greedy approximation (Guo introduced Section
 modiﬁcation computed {αi}j i=2 are optimal method keeps ﬁxed.
 improve reﬁned greedy approximation, i=1 becomes natural choice.
 getting {bi}k i=1 {bi}k alternating minimizing {αi}k i=1 described optimize {αi}k i=1 (5).
 real experiments, ﬁnd greedy initialization (4), alternating cycles is good ﬁnd high precision quantization.
 better illustration, summarize alternating minimization Algorithm
 updating {αi}k i=1, need binary operations kn non-binary operations.
 Combining kn non-binary operations determine binary code, total T alternating cycles, need k2n binary operations + non-binary operations quantize w ∈ Rn k-bit, extra corresponding greedy initialization.
 i=1 (5), {bi}j APPLY ALTERNATING MULTI-BIT QUANTIZATION TO RNNS Implementation.
 introduce implementation details quantizing RNN.
 simplicity, consider layer LSTM language model.
 goal is predict next word indexed t sequence one-hot word tokens (y∗ N follows:


 e y∗ t−1, xt = WT it, ft, ot, gt = σ(Wixt + bi + Whht−1 + bh), ct = ft (cid:12) ct−1 + (cid:12) gt, ht = ot (cid:12) tanh(ct), yt = softmax(Wsht + bs).
 (6) σ represents activation function.
 above formulation, multiplication weight matrices vectors xt ht occupy most computation.
 is apply quantization to.
 weight matrices, do apply quantization full row row.
 matrix vector execute binary multiplication.
 multiply obtained binary vector high precision scaling coefﬁcients.
 little extra computation results more freedom is brought approximate weights.
 give illustration left part Fig.

 Due one-hot word tokens, xt corresponds speciﬁc row quantized We. needs quantization.
 Different weight matrices, ht depends input, needs be quantized on-line inference.
 consistent notation existing work, e.g., (Hubara Zhou call quantizing ht quantizing activation.
 W ∈ Rm×n ht ∈ Rn, standard matrix-vector product needs operations.
 quantized product kw-bit W kh-bit ht, have + hn binary operations + non-binary operations, corresponds cost alternating approximation (T = corresponds ﬁnal product coefﬁcients.
 binary Published conference paper ICLR Figure Illustration quantized matrix vector multiplication (left part).
 matrix is quantized row row, provides more freedom approximate adds little extra computation.
 reformulating right part, make full use intrinsic parallel binary matrix vector multiplication further acceleration.
 (2kwkhmn+4k2 multiplication operates bit, full precision multiplication operates bits, feasible implementations, acceleration be theory.
 alternating quantization nn)+6khn+2kwkhm.
 here, overall theoretical acceleration is computed γ = Suppose LSTM has hidden states n have Wh ∈ R4096×1024.
 acceleration ratio becomes (kh, kw) = (2, (kh, kw) = (3,
 addition binary acceleration real implementations be affected size matrix, much memory reduce result better utilizing limited faster cache.
 implement binary multiplication kernel CPUs. Compared optimized Intel Math Kernel Library (MKL) full precision matrix vector achieve (kh, kw) = (2, (kh, kw) = (3,
 more details, please refer Appendix A.
 indicated left part Fig.
 binary multiplication be conducted associativity.
 operation is suitable parallel computing conducting multiplication, needs extra effort parallelization.
 concatenate binary codes shown right part Fig.

 such modiﬁcation, are able make full use much optimized inner parallel matrix multiplication, gives possibility further acceleration.
 ﬁnal result is obtained adding partitioned vectors together, has little extra computation.
 Training.
 proposed Courbariaux al.
 (2015), training quantized neural network, adding small gradients quantized weights result change it.
 maintain full precision weight accumulate gradients apply quantization mini-batch.
 whole procedure be formulated bi-level optimization (Colson problem: min w,{αi,bi}k i=1 αibi (cid:32) k(cid:88) i=1 (cid:33) (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)w − k(cid:88) i=1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2 α(cid:48) ib(cid:48) (7) s.t. {αi, bi}k Denote quantized weight ˆw =(cid:80)k i=1 = arg i}k i,b(cid:48) {α(cid:48) i=1 i=1 αibi.
 forward propagation, derive full precision w lower-level problem apply upper-level function f (·), i.e., RNN paper.
 backward propagation, derivative ∂f ∂ ˆw is propagated w lower-level function.
 Due discreteness bi, is hard model implicit dependence ˆw w.
 adopt “straight-through estimate” (Courbariaux et al., i.e., ∂f ∂ ˆw
 compute derivative quantized hidden state ht, same trick is applied.
 training, ﬁnd same phenomenon Hubara al.
 (2016b) ∂w = ∂f … * … … -1 -1 … … -1 … -1 -1 … -1 -1 … -1 -1 * + … … … -1 -1 … … -1 … … -1 -1 … -1 -1 -1 -1 … -1 … * Standard Matrix Vector Product Multi-bit Binary Product Modified Multi-bit Binary Product Published conference paper ICLR Table Measurement approximation different quantization methods, e.g., Uniform (Hubara et Balanced (Zhou et Greedy (Guo et Reﬁned (Guo al., Alternating method, see Section
 apply methods quantize full precision pre-trained weight LSTM PTB dataset.
 best values are bold.
 W-bits represents number weight bits FP denotes full precision.
 W-Bits Uniform Balanced Greedy Reﬁned Alternating (ours) Relative MSE Testing PPW FP Table Quantization full precision pre-trained weight GRU PTB dataset.
 W-Bits Uniform Balanced Greedy Reﬁned Alternating (ours) Relative MSE Testing PPW FP entries w grow large, become outliers harm quantization.
 clip w range
 EXPERIMENTS LANGUAGE MODELS section, conduct quantization experiments language models.
 most well-known recurrent neural networks, i.e., LSTM (Hochreiter Schmidhuber, GRU (Cho et al., are evaluated.
 are predict next word, performance is measured perplexity word (PPW) metric.
 experiments, initialize pre-trained model using vanilla SGD.
 initial learning rate is set
 epoch evaluate validation dataset record best value.
 validation error exceeds best record, decrease learning rate factor Training is terminated learning rate less reaching maximum epochs, i.e.,
 gradient norm is clipped range [−0.25,
 unroll network time steps regularize standard dropout (probability dropping units equals (Zaremba et
 simplicity notation, denote methods using uniform, balanced, greedy, reﬁned greedy, alternating quantization Uniform, Balanced, Greedy, Reﬁned, Alternating, respectively.
 Peen Tree Bank.
 ﬁrst conduct experiments Peen Tree Bank (PTB) corpus (Marcus et using standard preprocessed splits size vocabulary (Mikolov,
 PTB dataset contains training tokens, validation tokens, test tokens.
 fair comparison existing works, use LSTM GRU hidden layer size
 have glance approximation ability different quantization methods detailed Section conduct experiments quantizing trained full precision weight (neither quantization activation retraining).
 Results LSTM GRU are shown Table Table respectively.
 left parts record relative mean squared error quantized weight matrices full precision one.
 see proposed Alternating get much lower error Published conference paper ICLR Table Testing PPW multi-bit quantized LSTM GRU PTB dataset.
 W-Bits A-Bits represent number weight activation bits, respectively.
 W-Bits / A-Bits Uniform Balanced Reﬁned Alternating (ours) LSTM FP/FP GRU FP/FP Table Testing PPW multi-bit quantized LSTM GRU WikiText-2 dataset.
 LSTM GRU W-Bits / A-Bits FP/FP FP/FP Reﬁned Alternating (ours) Table Testing PPW multi-bit quantized LSTM GRU Text8 dataset.
 LSTM GRU W-Bits / A-Bits FP/FP FP/FP Reﬁned Alternating (ours) varying bit.
 measure testing PPW quantized weight shown right parts Table
 results are consistent left part, less errors result lower testing PPW.
 Note Uniform Balanced quantization are rule-based aim minimizing error.
 have much worse result direct approximation.
 repeat experiment other datasets.
 LSTM GRU, results are similar here.
 conduct experiments quantizing weights activations.
 train batch size
 ﬁnal result is shown Table
 comparing existing works, conduct experiment Reﬁned competitive baseline.
 do include Greedy is shown be much inferior reﬁned one, see, e.g., Table
 Table full precision model attain lower PPW existing works.
 However, considering gap quantized model full precision alternating quantized neural network is better existing works, i.e., Uniform (Hubara et al., Balanced (Zhou et
 Compared Reﬁned, Alternating quantization achieve compatible performance using 1-bit less quantization weights activations.
 other words, same tolerance accuracy drop, Alternating executes uses less memory Reﬁned.
 see weights/activations quantized LSTM achieve better performance full precision one.
 possible explanation is due regularization introduced quantization (Hubara
 WikiText-2 (Merity et al., is dataset released alternative PTB.
 contains training, validation, test tokens, has vocabulary words, is times larger dataset size, times larger vocabulary PTB.
 train layer’s hidden state size set batch size
 result is shown Table
 Similar PTB, Alternating use 1-bit less quantization attain compatible lower PPW Reﬁned.
 Published conference paper ICLR Text8.
 order determine Alternating remains effective larger dataset, perform experiments Text8 corpus (Mikolov
 follow same setting (Xie et al.,
 ﬁrst characters are used training, next validation, ﬁnal testing, resulting training tokens, validation tokens, test tokens.
 preprocess data mapping words appear fewer times unknown token, resulting size vocabulary.
 train LSTM GRU hidden layer size set batch size
 result is shown Table
 LSTM left part, Alternating achieves excellent performance.
 2-bit quantization weights activations, exceeds Reﬁned 3-bit.
 2-bit result is better reported (Xie LSTM adding noising schemes regularization attain testing PPW.
 GRU right part, Alternating is better Reﬁned, 3-bit quantization has gap full precision one.
 attribute uniﬁed setting hyper-parameters experiments.
 tuned hyper-parameters dataset, make gap.
 Note alternating quantization is general technique.
 is suitable language models here.
 comprehensive veriﬁcation, apply image classiﬁcation tasks.
 RNNs neural alternating quantization achieves lowest testing error compared methods.
 Due space limitation, deter results Appendix B.
 CONCLUSIONS work, address limitations RNNs, large memory high latency, quantization.
 formulate quantization minimizing approximation error.
 key observation parameters be singled others ﬁxed, simple effective alternating method is proposed.
 apply quantize LSTM GRU language models.
 2-bit weights activations, achieve accuracy loss compared full precision one, ∼16× reduction memory ∼6× real acceleration CPUs. 3-bit quantization, attain compatible result full precision one, ∼10.5× reduction memory ∼3× real acceleration.
 beat existing works large margin.
 apply alternating quantization image classiﬁcation tasks.
 RNNs neural method achieve plausible performance.
 ACKNOWLEDGEMENTS like thank reviewers suggestions manuscript.
 Zhouchen Lin is supported National Basic Research Program China (973 Program) (grant no.
 National Natural Science Foundation (NSF) China (grant nos.
 Qual- comm, Microsoft Research Asia.
 Hongbin Zha is supported Natural Science Foundation (NSF) China (No.
 REFERENCES Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio.
 Learning phrase representations using RNN encoder-decoder statistical machine translation.

 Benoît Colson, Patrice Marcotte, Gilles Savard.
 overview bilevel optimization.
 Annals Operations Research,
 Tim Cooijmans, Nicolas Ballas, César Laurent, Ça˘glar Gülçehre, Aaron Courville.
 Recurrent batch normalization.
 ICLR,
 Matthieu Courbariaux, Yoshua Bengio, Jean-Pierre David.
 Binaryconnect: Training deep neural networks binary weights propagations.
 NIPS, pp.

 Yunchao Gong, Liu Liu, Ming Yang, Lubomir Bourdev.
 Compressing deep convolutional networks using vector quantization.

 Alex Graves, Abdel-rahman Mohamed, Geoffrey Hinton.
 Speech recognition deep recurrent neural networks.
 ICASSP, pp.

 IEEE,
 Published conference paper ICLR Yiwen Guo, Anbang Yao, Hao Zhao, Yurong Chen.
 Network sketching: Exploiting binary structure deep cnns.
 CVPR,
 Song Han, Jeff Pool, John Tran, William Dally.
 Learning weights connections efﬁcient neural network.
 NIPS, pp.

 Song Han, Huizi Mao, William J Dally.
 Deep compression: Compressing deep neural networks pruning, trained quantization huffman coding.
 ICLR,
 Sepp Hochreiter Jürgen Schmidhuber.
 short-term memory.
 Neural Computation,
 Lu Hou, Quanming Yao, James T Kwok.
 Loss-aware binarization deep networks.
 ICLR,
 Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, Yoshua Bengio.
 Binarized neural networks.
 NIPS, pp.


 Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, Yoshua Bengio.
 Quan- tized neural networks: Training neural networks low precision weights activations.

 Sergey Ioffe Christian Szegedy.
 Batch normalization: Accelerating deep network training reducing internal covariate shift.
 ICML, pp.

 Max Jaderberg, Andrea Vedaldi, Andrew Zisserman.
 Speeding convolutional neural networks low rank expansions.

 Diederik Kingma Jimmy Ba. Adam: A method stochastic optimization.
 ICLR,
 Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, Victor Lempitsky.
 Speeding- convolutional neural networks using ﬁne-tuned cp-decomposition.

 Fengfu Li, Bo Zhang, Bin Liu.
 Ternary weight networks.

 Zefan Li, Bingbing Ni, Wenjun Zhang, Xiaokang Yang, Wen Gao.
 Performance guaranteed network acceleration high-order residual quantization.
 ICCV, pp.

 Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, Marianna Pensky.
 Sparse convolu- tional neural networks.
 CVPR, pp.

 Mitchell P Marcus, Mary Ann Marcinkiewicz, Beatrice Santorini.
 Building large annotated corpus english: penn treebank.
 Computational Linguistics,
 Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher.
 Pointer sentinel mixture models.
 ICLR,
 Tomáš Mikolov.
 Statistical Language Models Based Neural Networks.
 PhD thesis, Brno University Technology,
 Tomáš Mikolov, Martin Karaﬁát, Lukáš Burget, Jan ˇCernocký, Sanjeev Khudanpur.
 Recurrent neural network based language model.
 INTERSPEECH, pp.

 Tomáš Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu, Marc’Aurelio Ranzato.
 Learning longer memory recurrent neural networks.

 Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, Ali Farhadi.
 XNOR-Net: Imagenet classiﬁcation using binary convolutional neural networks.
 ECCV, pp.


 Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, Bhuvana Ramabhadran.
 Low- rank matrix factorization deep neural network training high-dimensional output targets.
 ICASSP, pp.

 IEEE,
 Karen Simonyan Andrew Zisserman.
 deep convolutional networks large-scale image recognition.
 ICLR,
 Published conference paper ICLR Ilya Sutskever, Oriol Vinyals, Quoc V Le. Sequence sequence learning neural networks.
 NIPS, pp.

 Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, Weinan E.
 Convolutional neural networks low-rank regularization.
 ICLR,
 Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan.
 Show tell: A neural image caption generator.
 CVPR, pp.

 Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, Hai Li. Learning structured sparsity deep neural networks.
 NIPS, pp.

 Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, al.
 Google’s neural machine translation system: Bridging gap human machine translation.

 Ziang Xie, Sida Wang, Jiwei Li, Daniel Lévy, Aiming Nie, Dan Jurafsky, Andrew Y Ng. Data noising smoothing neural network language models.
 ICLR,
 Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals.
 Recurrent neural network regularization.

 Shu-Chang Zhou, Yu-Zhi Wang, Wen, Qin-Yao He, Yu-Heng Zou.
 Balanced quantization: effective efﬁcient approach quantized neural networks.
 Journal Computer Science Technology,
 Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, Wen, Yuheng Zou.
 Dorefa-net: Training low bitwidth convolutional neural networks low bitwidth gradients.

 Chenzhuo Zhu, Song Han, Huizi Mao, William J Dally.
 Trained ternary quantization.
 ICLR,
 Published conference paper ICLR APPENDIX A BINARY MATRIX VECTOR MULTIPLICATION IN CPUS Table Computing time binary matrix vector multiplication CPUs, Quant represents cost execute alternating quantization on-line.
 Weight Size W-Bits / A-Bits Total (ms) Quant (ms) Quant / Total Acceleration × × FP/FP FP/FP section, discuss implementation binary multiplication kernel CPUs. binary multiplication is divided steps: Entry-wise XNOR operation (corresponding entry-wise product full precision multiplication) bit count operation accumulation (corresponding compute sum multiplied entries full precision multiplication).
 test Intel Xeon E5-2682 v4 @ GHz CPU.
 XNOR operation, use Single instruction, multiple data (SIMD) _mm256 _xor_ps, execute bit simultaneously.
 bit count operation, use function _popcnt64 (Note step be accelerated up-coming instruction _mm512 _popcnt_epi64 execute bits simultaneously.
 Similarly, XNOR operation be accelerated up-coming _mm512 _xor_ps instruction execute bits simultaneously).
 compare optimized Intel Math Kernel Library (MKL) full precision matrix vector multiplication execute codes single-thread mode.
 conduct scales experiments: matrix size multiplying vector size matrix size × multiplying vector size correspond hidden state product Whht−1 softmax layer Wsht Text8 dataset inference batch size (See Eq. (6)).
 results are shown Table
 see alternating quantization step accounts small portion total executing time, larger scale matrix vector multiplication.
 Compared full precision binary multiplication achieve acceleration 2-bit quantization acceleration 3-bit quantization.
 Note is simple test CPU.
 alternating quantization method be extended GPU, ASIC, FPGA.
 B IMAGE CLASSIFICATION Sequential MNIST.
 simple illustration show alternating quantization is limited texts, conduct experiments sequential MNIST classiﬁcation task (Cooijmans et
 dataset consists training set test set × gray-scale images.
 divide last training images validation.
 time, use row image input (28×1), results total time steps.
 use hidden layer’s LSTM size same optimization hyper-parameters language models.
 weights activations, inputs are quantized.
 testing error rates 1-bit input, 2-bit weight, 2-bit activation are shown alternating quantized method achieves plausible performance task.
 MLP MNIST.
 alternating quantization proposed work is general technique.
 is suitable RNNs, feed-forward neural networks.
 example, conduct classiﬁcation task MNIST compare existing work (Li et
 method proposed (Li al., is greedy multi-bit quantization method.
 fair comparison, follow same setting.
 use MLP consisting hidden layers units L2-SVM output layer.
 convolution, preprocessing, data augmentation pre-training is Published conference paper ICLR Table Testing error rate LSTM MNIST 1-bit input, 2-bit weight, 2-bit activation.
 Methods Full Precision Reﬁned (Guo Alternating (ours) Testing Error Rate % % % Table Testing error rate MLP MNIST 2-bit input, 2-bit weight, 1-bit activation.
 Methods Full Precision Greedy (reported (Li Reﬁned (Guo Alternating (ours) Testing Error Rate % % % % Table Testing error rate CNN CIFAR-10 2-bit weight 1-bit activation.
 Methods Full Precision (reported (Hou al., XNOR-Net (1-bit weight activation, reported (Hou Reﬁned (Guo Alternating (ours) Testing Error Rate % % % % used.
 use ADAM (Kingma Ba, decaying learning rate Batch Normalization (Ioffe Szegedy, batch size
 testing error rates 2-bit input, 2-bit weight, 1-bit activation are shown Table
 compared multi-bit quantization alternating achieves lowest testing error.
 CNN CIFAR-10.
 conduct experiments CIFAR-10 follow same setting (Hou et al.,
 is, use images training, validation, remaining testing.
 images are preprocessed global contrast normalization ZCA whitening.
 use VGG-like architecture (Simonyan Zisserman, (2 × C3)−MP2−(2 × C3)−MP2−(2 × C3)−MP2−(2 × FC)−10 SVM C3 is × convolution layer, MP2 is × max-pooling layer.
 Batch Normalization, mini-batch size ADAM are used.
 maximum number epochs is
 learning rate starts decays factor epochs.
 testing error rates 2-bit weight 1-bit activation are shown Table alternating method achieves lowest test error rate compared quantization methods.

 Deep Neural Network (DNN) architectures are promising solutions achieving remarkable results wide range machine learning applications, including, limited computer vision, speech recognition, language modeling autonomous cars.
 Currently, is major growing trend introducing advanced DNN architectures employing end-user ap- plications.
 considerable improvements DNNs are achieved increasing complexity requires more compu- tational resources training inference.
 Recent research di- rections make progress sustainable are: development Graphical Processing Units (GPUs) vital hardware compo- nent servers mobile devices [27], design efficient algorithms large-scale distributed training [7] efficient in- ference [33], compression approximation models [38], introducing collaborative computation cloud fog known dew computing [36].
 Using cloud servers computation storage is becoming favorable due technical advancements improved accessibility.
 Scalability, low cost, satisfactory Quality Service (QoS), made offloading cloud typical choice computing intensive tasks.
 other side, mobile-device are being equipped powerful general purpose CPUs GPUs. Very is new trend hardware companies design dedicated chips better tackle machine-learning tasks.
 example, Apple’s A11 Bionic chip [26] used iPhone X uses neural engine GPU speed DNN queries applications such face identification facial motion capture [23].
 are methods DNN inference: mobile cloud only.
 simple models, mobile device is responsible performing computation.
 case complex models, raw input data (image, video stream, voice, etc.) is uploaded computed cloud.
 results task are downloaded device.
 improvements mobiles devices mentioned ear- computational power mobile devices are considered weaker cloud ones.
 Therefore, mobile-only approach cause large inference latency failure meeting QoS.
 Moreover, embedded devices undergo major energy consump- tion constraints due battery capacity limits.
 other hand, cloud-only suffers communication overhead uploading raw data downloading outputs.
 Moreover, slowdowns caused service congestions, subscription costs, network dependency be considered downsides approach.
 superiority persistent improvement DNNs is dependent providing huge amount training data.
 Typically, data is collected different resources fed network training.
 final model be delivered dif- ferent devices inference functions.
 However, is trend appearance applications requiring adaptive learning on- line environments, such self driving cars security drones [30][25].
 Model parameters smart devices are being changed based continuous interaction surround- ings.
 Complexity architectures extended number parameters current cloud-only methods DNN training, im- plies constant communication cost burden increased power consumption mobile device.
 Automatic partitioning extensive tasks cloud optimization performance energy consumption has been studied [2].
 Most recently, scalable distributed hierarchy structures end-user device, edge, cloud have been suggested are specialized DNN applications.
 However, exploiting layer granularity DNN architectures run time partitioning has been studied throughly yet.
 Figure Different computation partitioning methods.
 (a) Mobile only: computation is done mobile de- vice.
 (b) Cloud raw input data is sent cloud, compu- tations is done cloud results are sent mobile device.
 (c) JointDNN: DNN architecture is partitioned granularity layers, layer be computed cloud mobile.
 work, are investigating inference training DNNs joint platform mobile cloud alternatives current single-platform methods illustrated Figure
 Con- sidering DNN architectures ordered sequence layers, possibility computation layer mobile cloud, model DNN structure directed acyclic graph (DAG).
 parameters real-time adaptive model are dependent following factors: mobile/cloud hardware software re- sources, battery capacity, network specifications, QoS.
 Based modeling, show problem finding optimal computation schedule different scenarios, i.e. best performance energy consumption, be reduced polynomial time shortest path problem.
 present realistic results, made experiments real hard- wares mobile device cloud.
 model communication platform, used different network technologies recent reports specifications U.S. DNN architectures be categorized based functionality.
 differences enforce specific type order layers archi- tecture, affecting partitioning result collaborative method.
 discriminative models, used recognition applica- tions, layer size gradual decrease proceeding input output
 sequence suggests computation first few lay- ers mobile device avoid excessive communication cost uploading large raw input data.
 other hand, growth layer size input output generative models used synthesizing new data, implies possibility uploading small input cloud downloading computing last layers mobile device better efficiency.
 Interesting mo- bile applications image image translation are implemented autoencoder architectures, consisting middle layers smaller sizes compared input output.
 expect first last layers be computed mobile de- vice collaborative approach.
 examined well-known DNN benchmarks selected categories illustrate differences collaborative computation approach.
 see Section ??, communication mobile cloud is main bottleneck performance energy collaborative approach.
 investigated specific characteristics CNN layer outputs introduced lossless com- pression method reduce communication costs.
 Figure Typical layer size architecture (a) Discriminative (b) Autoencoder (c) Generative models.
 State-of-the-art work collaborative computation DNNs [21] considers offloading point, assigning computation previous layers next layers mobile cloud platforms, respectively.
 show approach is non-generic fails be optimal, introduced new method granting possibility computation platforms layer independent other layers.
 evaluations show JointDNN improves latency energy com- pared status-quo single platform approaches compression.
 main contributions paper be listed as: • Introducing novel model collaborative computation • Formulating problem optimal computation scheduling DNNs layer granularity mobile cloud computing environment shortest path problem integer linear programming (ILP) • Examining compressibility DNN layers developing lossless compression method improve communication costs • Demonstrating significant improvements performance, mobile energy consumption, cloud workload achieved using JointDNN mobile cloud PROBLEM DEFINITION AND MODELING section, explain general architecture DNN layers profiling method.
 Moreover, elaborate cost optimization be reduced shortest path problem introducing JointDNN graph model.
 show constrained problem is formulated setting ILP.
 DNN Building Blocks DNNs are networks composed several layers stacked other.
 briefly explain functionality layers used state-of-the-art architectures: Convolution Layer (conv) consists set filters di- mensions smaller input.
 filter traverses input predefined step size com- putes dot product it’s parameters correspond- ing part input.
 process creates different feature maps (referred channels) different filters same input data.
 aspect preserving locality input features has made Convolutional Neural Network (CNN) architectures horse power state-of-the-art image classification models.
 dot product basis conv, be formulated General Matrix

 dataOutput data Multiplication (GEMM), capable gaining performance improvement using parallel computing devices GPUs).
 Connected Layer (fc) is main component most regular neural networks neuron is connected neurons previous layer.
 pairwise connection architecture comprises large portion computation whole network.
 conv, fc layer is formulated GEMM.
 Pooling Layer (pool) performs non-linear sampling function non-overlapping local parts input.
 Max- pooling is common function used type layer other functions such average L2-norm pooling.
 Activation Layer increases non-linearity property neu- ral network architectures.
 layer applies non-linear activation function single data points input generate output same size.
 various non-linear functions, such sigmoid hyperbolic tangent, Rectified Linear Unit (relu) is favorable choice DNN architectures is simple speeds tedious training process [10].
 Local Response Normalization (lrn) performs local normal- ization imposing local competition big activities adjacent features channel, features same spatial location different channels.
 lrn are inspired in- hibition schemes observed brain helps intention generalization.
 are different formulations suggested lrn, shown lead slight improvements.
 Dropout Layer (drop) mentioned earlier, fc occupies most parameters DNN models vulnerable overfitting.
 regularization methods are used prevent overfitting reducing high dependency network individual neurons training.
 dropout [37] technique, training iteration neurons be removed (droped out) network predetermined probability p kept probability − p training is done remaining network.
 dropped nodes have previous weight next training iteration.
 Deconvolution Layer (deconv) known transposed con- volution is used generative autoencoder models applications such building high-resolutions picture low- resolution pictures high-level descriptions.
 goal decon- volution is find f convolution equation form f ∗ д = h.
 case DNNs, д is filter f is input convolu- tion [42].
 Short-Term Memory Layer (lstm) is building unit layers recurrent neural network (RNN) is used due promising results speech recognition applications.
 typical LSTM unit is composed cell, input gate, output gate forget gate, is responsible remembering forgetting specific values arbitrary time intervals.
 whole LSTM unit be thought typical artificial neuron, feed-forward neural network.
 Softmax (soft) is last layer multi-class architectures, usu- ally connected one-to-one correspondence way fc layer.
 Softmax establishes probability distribution representing class probability single neuron.
 Figure Latency grouped separated execution con- volution operator.
 Energy Latency Profiling are methods measuring latency energy con- sumption layer neural networks: Statistical Modeling: method, regression model configurable parameters operators (e.g. filter size convo- lution) be used estimate associated latency energy.
 method is prone large error inter-layer opti- mizations performed DNN software packages.
 Therefore, is necessary consider execution several consecutive operators grouped other profiling.
 Many software packages are proprietary, making access inter-layer optimization techniques impossible.
 order illustrate issue, designed experiments consecutive convolutions NVIDIA Pascal™ GPU using cuDNN® library [1].
 first experiment, measure latency convolution operator set total latency sum them.
 second experiment, group convolutions measure total latency.
 parameters are located GPU’s memory experiments, avoiding data transfer main memory make sure results are representing actual computation latency.
 see Figure is large error gap separated grouped execution experiments grows number convolutions is increased.
 observation confirms need profile grouped operators have more accurate estimations.
 Considering various consecutive combination operators different input sizes, method requires large number measurements, mention need complex regression model.
 Analytical Modeling: derive analytical approach es- timation latency energy consumption, is required obtain exact hardware software specifications.
 However, state-of-the-art work latency modeling DNNs [31] fails estimate layer-level delay acceptable error bound, in- stance, underestimating latency connected layer neurons
 Industrial developers do reveal detailed hardware architecture specifications propri- etary parallel computing architectures such CUDA®, therefore, analytical approach be challenging [15].
 Application-specific Profiling: method, DNN ar- chitecture application being used is profiled run-time.
 number applications mobile device using neural networks Number050100150Latency (ms)Separate executionGrouped execution Figure Computation model linear topology.
 Figure Graph representation mobile cloud computing optimal scheduling problem linear topology.
 are limited.
 conclusion, method is feasi- ble, promising higher accuracy estimations.
 have chosen method estimation energies latencies experiments paper.
 k =1 ωk.
 (energy) querying DNN isn JointDNN Graph Model First, assume DNN is presented sequence distinct layers linear topology depicted Figure
 Layers are executed sequentially, output data generated layer feeds input next one.
 denote input output data sizes kth layer αk βk, respectively.
 Denoting latency (energy) layer k ωk, k = ..., n, total latency mobile cloud computing optimal scheduling problem be reduced shortest path problem, node S F, graph Figure
 Mobile Execution cost kth layer (C(MEk)) is cost executing kth layer mobile cloud server is idle.
 Cloud Execution cost kth layer (C(CEk)) is executing cost kth layer cloud server mobile is idle.
 Uploading Input Data cost kth layer is cost uploading output data (k-1)th layer cloud server (U IDk).
 Downloading Input Data cost kth layer is cost downloading output data (k-1)th layer mobile (DIDk).
 costs refer latency energy.
 However, showed Section assumption linear topology DNNs is true need consider consecutive grouping layers network.
 fact suggests replacement linear topology tournament graph depicted Figure
 define parameters new graph, JointDNN graph model, Table
 graph, node Ci:j represents layers i j are com- puted cloud server, node Mi:j represents layers i j are computed mobile device.
 edge adjacent nodes JointDNN graph model is associated possible cases: transition mobile mobile, includes mobile computation cost (MEi, j) transition cloud cloud, includes cloud computa- tion cost (CEi, j) transition mobile cloud, includes mobile computation cost uploading cost inputs next node (EUi, j = MEi, j + U IDj+1) A transition cloud mobile, includes cloud compu- tation cost downloading cost inputs next node (EDi, j = CEi, j +DIDj+1).
 formulation, transform computation scheduling problem finding shortest path S F.
 Residual networks are class powerful easy-to-train ar- chitectures DNNs [14].
 residual networks, depicted Figure (a), output layer is fed layer distance least two.
 need keep track source layer (node Figure know layer is computed mobile cloud.
 standard graph model has memory is previous layer.
 provide method transform computation graph type network standard model, JointDNN graph.
 regard, add additional chains size k − k is number nodes residual block (3 Figure
 chain represents case computing layer mobile other represents case computing layer cloud.
 Figure have shown weights need be modified, D2 U2 are cost downloading uploading output layer respectively.
 solving shortest path problem JointDNN graph model, obtain optimal scheduling inference DNNs. Online training consists inference back-propagation step.
 total number layers is noted N paper are layers modeling training, second N layers are mirrored version first N layers, associated operations are gradients error function respect DNN’s weights.
 main difference mobile cloud computing graph inference online training is need updating model downloading new weights cloud.
 assume cloud server performs whole back-propagation step is scheduled be done mobile, therefore, is need mobile device upload weights are updated order save mobile energy consumption.
 modification JointDNN graph Table Parameter Definition Graph Model Param.
 CEi:j MEi:j EDi, j EUi, j ϕk Ωk Ψk Γk Πm Πc U1 Description Cost Executing layers i j cloud Executing layers i j mobile CEi:j + DIDj MEi:j + U IDj All following edges: ∀i = k − EDi,k−1 following edges: ∀i = k − MEi,k−1 following edges: ∀i = k − EUi,k−1 following edges: ∀i = k − CEi,k−1 following edges: ∀i = n MEi,n following edges: ∀i = n EDi,n Uploading input first layer = α2 α1kβ2 = α3βk-1 = αkn-1nβn-1 = αnαn-1βk = αk+1 βnω1 ω2ωkωn-1ωn C(CE2)n-1nC(CEn-1)C(CEn-2)12C(ME1) C(ME2)n-1nC(MEn-1)C(MEn-2)C(CEn)FSC(ME1) Figure JointDNN graph model.
 size m ∈ {1,


 N}.
 Thus, have N + (N − +
 = N(N + (1) number different profiling values delay energy.
 Consider- ing layer i layer j be computed mobile device cloud server, assign binary variables mi, j ci, j, respec- tively.
 Download upload communication delays needs be added execution time, switching cloud to/from mobile, respectively.
 + ci, j .TcloudLi, j (2) mi, j .cj+1,k .TuploadLj ci, j .mj+1,k .TdownloadLj (3) model is adding costs downloading weights layers are updated cloud EDi, j.
 efficiently.
 shortest path problem be solved polynomial time However, problem shortest path subjected constraints has been shown be NP-Complete [41].
 instance, assuming standard graph is constructed energy need find shortest path subject constraint total latency path being less time deadline (QoS).
 However, is ap- proximation solution problem, "LARAC" algorithm nature application does require solve optimization problem frequently, therefore, aim obtain optimal solution.
 constitute small look-up table optimization results different set parameters (e.g. network bandwidth, cloud server load, etc.).
 provide ILP formulations DNN partitioning following sections.
 ILP Setup Performance Efficient Computation Offloading ILP Setup Inference.
 formulated scheduling inference DNNs ILP tractable number variables.
 method, profile delay energy consumption consecutive layers Figure (a) A residual building block (b) Transformation residual building block shortest path problem.
 n i =1 j=i i =1 n n n n i =1 i =1 Tcomputation = Tcommunication = (mi, j .TmobileLi, j n n n n k =j+1 j=i j=i k =j+1 c1,i .TuploadLi ci,n .TdownloadLn i =1 TmobileLi, j Ttotal = Tcomputation + Tcommunication TcloudLi, j TuploadLi (4) represent execution time ith layer jth layer mobile cloud, respectively.
 represent latency downloading TdownloadLi uploading output ith layer, respectively.
 Consider- ing set consecutive layers, mi, j {cj+1,k}k =j+1:n are equal one, output jth layer is up- loaded cloud.
 same argument applies downloading.
 note last terms Eq. represent condition last layer is computed cloud need download output mobile device, first layer is computed cloud need upload input cloud, respectively.
 support residual architectures, need add pair download upload terms similar first terms Eq. starting ending layers residual block.
 order guarantee layers are computed need add following set constraints: M1:kk=1:nME1:kEU1,kM2:kk=2:nME2:kΩ1Φ1Mn-1:kk=n-1:nMEn-1:kΩn-2Φn-2Mn:kk=n:nMEn:kΩn-1Φn-1C1:kk=1:nCE1:kU1ED1,kC2:kk=2:nCE2:kΓ1ψ1Cn-1:kk=n-1:nCEn-1:kΓn-2ψn-2Cn:kk=n:nCEn:kΓn-1ψn-1ED2,kEDn-1,kEDn,kEU2,kEUn-1,kEUn,kSSFFΠcΠm.




 ..




 .1234534234341234155(a)(b) m i =1 n j=m ∀m ∈ n (mi, j + ci, j) = (5) non-linearity multiplication, additional step is needed transform Eq. standard form ILP.
 define sets new variables: EmobileLi, j EcloudLi, j represent amount energy re- quired compute ith layer jth layer mobile cloud, respectively.
 EdownloadLi represent energy required download upload output ith layer, respectively.
 Similar performance efficient ILP constraints, layer be executed once: EuploadLi ui, j = mi, j
 cj+1,k ∀m ∈ n mi, j ≤ (11) m n i =1 j=m n n k =j+1 k =j+1 di, j = ci, j
 mj+1,k following constraints: (6) (7) ILP problem be solved different set parameters (e.g. different uplink download speeds), scheduling re- sults be stored look-up table mobile device.
 number variables setup is tractable solving ILP is quick.
 instance, solving ILP AlexNet takes seconds Intel(R) Core(TM) i7-3770 CPU MATLAB®’s intlinprog() function using primal simplex algorithm.
 Performance Efficient Computation Offloading ILP Setup Training.
 ILP formulation online training phase is similar inference.
 online training have layers N obtained mirroring DNN, second N layers are backward propagation.
 Moreover, need download weights are updated cloud mobile.
 assume cloud server has updated version weights does require mobile device upload up- dated weights.
 following terms need be added ILP setup training: Tcomputation = + ci, j .TcloudLi, j (12) i =1 j=i n i =1 i =1 i =1 (mi, j .TmobileLi, j k =j+1 j=i j=i k =j+1 c1,i .TuploadLi ci, j .TdownloadWi i =n+1 j=i mi, j .cj+1,k .TuploadLj ci, j .mj+1,k .TdownloadLj Tcommunication = Ttotal = Tcomputation + Tcommunication Energy Efficient Computation Offloading ILP Setup (13) (14) k =j+1 ui, j ≤ mi, j ui, j ≤ n di, j ≤ n mi, j + di, j ≤ ci, j ci, j + k =j +1 k =j+1 cj+1,k cj+1,k − ui, j ≤ mj+1,k mj+1,k − di, j ≤ k =j+1 orn take value binary variables, mi, j first constraints ensure ui, j be mi, j l =j+1 cj+1,l are zero.
 third inequality guarantees ui, j l =j+1 cj+1,l are set one.
 same reasoning works di, j.
 total number variables ILP formulation be +1)/2, N is total number layers network.
 Energy Efficient Computation Offloading ILP Setup Inference.
 nature application, care energy consumption mobile side.
 formulate ILP follows: Ecomputation = Ecommunication = j=i i =2 i =1 n n n n n−1 n + n + n i =1 i =1 j=i j=i i =1 mi, j .EmobileLi, j mi, j .EdownloadLi mi, j .EuploadLj (8) (9) (1 − m1,i) − (n − (1 − mi,n) − (n − Etotal = Ecomputation + Ecommunication Training.
 Ecomputation = (10) i =1 j=i mi, j .EmobileLi, j (15) Ecommunication = mi, j .EdownloadLi mi, j .EuploadLj j =i i =2 i =1 i =1 j=i + (1 − m1,i) − (2n − defined ILP listed below: j=i + i =n+1 (1 − mi, j) − (n − (16) (17) Etotal = Ecomputation + Ecommunication Scenarios.
 be different optimization scenarios • Performance efficient computation: case, is suf- ficient solve ILP formulation performance efficient computation offloading.
 • Energy efficient computation: case, is sufficient solve ILP formulation energy efficient computation offloading.
 • Battery budget limitation: case, based avail- able battery, operating system decide dedicate specific amount energy consumption application.
 adding following constraint performance effi- cient ILP formulation, framework adapt battery limitations: Ecomputation + Ecommunication ≤ Eubound (18) • Cloud limited resources: presence cloud server congestion limitations user’s subscription, ap- ply execution time constraints application alleviate server load: n i =1 j=i ci, j .TcloudLi, j ≤ Tubound (19) • QoS: scenario, minimize required energy con- sumption meeting specified deadline: min{Ecomputation + Ecommunication} Tcomputation + Tcommunication ≤ TQoS (20) constraint be applied energy perfor- mance efficient ILP formulations.
 EVALUATION Deep Architecture Benchmarks architecture neural networks depends type application, have chosen common application types DNNs: (1) Discriminative neural networks are class models machine learning modeling conditional probabil- ity distribution P(y|x).
 class is used clas- sification regression tasks.
 AlexNet[22], OverFeat[34], Algorithm JointDNN engine optimal scheduling DNNs function JointDNN (N Li Di N B, N P); Input :1: N number layers DNN Li|i = N layers DNN Di|i = N data size layer N B: mobile network bandwidth N P: mobile network uplink downlink power consumption Output:Optimal schedule DNN end j j < N j = j + do Latencyi, j Enerдyi, j = ProfileGroupedLayers(i, j); i = i < N i = i + do end G,S,F = ConstructShortestPathGraph(N ,Li,Di,N B,N P) //S F are start finish nodes G is JointDNN graph model schedule = ShortestPath(G,S,F) constraints n i =1n Battery Limited Constraint Ecomm + Ecomp ≤ Eubound schedule = PerformanceEfficientILP(N ,Li,Di,N B,N P) end Cloud Server Contraint j=i ci, j .TcloudLi, j ≤ Tubound schedule = PerformanceEfficientILP(N ,Li,Di,N B,N P) Tcomm + Tcomp ≤ TQoS schedule = EnergyEfficientILP(N ,Li,Di,N B,N P) end QoS end end return schedule; VGG16[35], Deep Speech[13], ResNet[14], NiN[24] are well-known discriminative models use benchmarks experiment.
 Deep Speech, used speech recog- nition, other benchmarks are used image classification tasks.
 Generative neural networks model joint probability distribution P(x, y), allowing generation new samples.
 networks have applications Computer Vision [11] Robotics [9], be deployed mobile device.
 Chair [8] is generative model use benchmark work.
 (3) Autoencoders are class neural networks used learn representation data set.
 applications are image reconstruction, image image translation, denoising name few.
 Mobile robots be equipped autoencoders be used computer vision tasks.
 use Pix2Pix [18], benchmark class.
 Figure Latency energy improvements different batch sizes inference.
 Table Benchmark Specifications Type Discriminative Generative Autoencoder Model AlexNet OverFeat Deep Speech ResNet VGG16 NiN Chair Pix2Pix Layers Table Mobile networks specifications U.S. Param.
 Download speed (Mpbs) Upload speed (Mbps) αu (mW/Mpbs) αd (mW/Mpbs) β (mW) Wi-Fi Mobile Server Setup used Jetson TX2 module developed NVIDIA® [3], fair representative mobile computation power mobile device.
 module enables efficient implementation DNN applications used products such robots, drones, smart cameras.
 is equipped NVIDIA Pascal®GPU CUDA cores shared GB bit LPDDR4 memory GPU CPU.
 measure power consumption mobile platform, used INA226 power sensor [17].
 NVIDIA® Tesla® K40C [4] GB memory serves server GPU.
 computation capability device is more order magnitude compared mobile device.
 Communication Parameters model communication platforms, used av- erage download upload speed mobile Internet [28, different networks (3G, Wi-Fi) shown Table
 communication power download (Pd) upload (Pu) is dependent network throughput (td tu).
 Comprehensive examinations [16] indicates uplink downlink power be modeled linear equations (Eq. accurate less error rate.
 Table shows parameter values equation different networks.
 Pu = αutu + β Pd = αd td + β (21) RESULTS latency energy improvements inference online training engine different benchmarks are shown improvement (%)Pix2Pix, Performance Efficient Inference1234567891040.0%60.0%80.0%Energy improvement (%)Pix2Pix, Energy Efficient Inference12345678910-10.0%0.0%10.0%Deep Speech, Performance Efficient Inference12345678910-10.0%0.0%10.0%Deep Speech, Energy Efficient Inference123456789100.0%25.0%50.0%VGG16, Performance Efficient Inference123456789100.0%25.0%50.0%VGG16, Energy Efficient Inference123456789100.0%20.0%NiN, Performance Efficient Inference12345678910-10.0%0.0%10.0%NiN, Energy Efficient Inference12345678910-10.0%0.0%10.0%Latency improvement (%)Chair, Performance Efficient Inference12345678910Batch size-10.0%0.0%10.0%Energy improvement (%)Chair, Energy Efficient Inference123456789100.0%50.0%Overfeat, Performance Efficient Inference12345678910Batch size0.0%50.0%Overfeat, Energy Efficient Inference3G4GWi-Fi123456789100.0%50.0%AlexNet, Performance Efficient Inference12345678910Batch size0.0%50.0%AlexNet, Energy Efficient Inference123456789100.0%20.0%ResNet, Performance Efficient Inference12345678910Batch size0.0%25.0%50.0%ResNet, Energy Efficient Inference Figure Latency energy improvements different batch sizes training.
 Figures respectively.
 considered best case mobile- baseline.
 JointDNN achieve improvements latency energy consumption, re- inference.
 Communication cost increases batch size is case computation cost grows much lower rate, depicted
 Therefore, key observation is increase batch size, mobile-only approach becomes more preferable.
 online huge communication overhead transmitting updated weights be added total cost.
 Therefore, order avoid downloading large data, few back-propagation steps are computed cloud server.
 per- formed simulation varying percentage updated weight.
 percentage updated weights increases, latency energy consumption becomes constant is shown Figure
 is result fact back-propagations be performed mobile device weights are transfered cloud mobile.
 JointDNN achieve improvements latency energy consumption inference.
 Different patterns scheduling are demonstrated Figure
 represent optimal solution Wi-Fi network opti- mizing latency.
 show computations DNN is divided mobile cloud.
 be seen, dis- criminative models (e.g. AlexNet), inference follows mobile-cloud pattern training follows mobile-cloud-mobile pattern.
 intuition is last layers are intensive (fc) small data sizes, require low communication cost, therefore, last layers tend be computed cloud.
 gen- erative models (e.g. Chair), execution schedule inference is Figure (a) Latency epoch online training using JointDNN algorithm vs percentage updated weights La- tency mobile-only inference vs.
 batch size.
 improvement (%)Pix2Pix, Performance Efficient Training1234567891020.0%40.0%Energy improvement (%)Pix2Pix, Energy Efficient Training123456789100.0%25.0%50.0%Deep Speech, Performance Efficient Training123456789100.0%50.0%Deep Speech, Energy Efficient Training123456789100.0%25.0%50.0%VGG16, Performance Efficient Training123456789100.0%25.0%50.0%VGG16, Energy Efficient Training123456789100.0%20.0%NiN, Performance Efficient Training12345678910-10.0%0.0%10.0%NiN, Energy Efficient Training123456789100.0%50.0%Latency improvement (%)Chair, Performance Efficient Training12345678910Batch size0.0%20.0%Energy improvement (%)Chair, Energy Efficient Training123456789100.0%25.0%50.0%Overfeat, Performance Efficient Training12345678910Batch size0.0%25.0%50.0%Overfeat, Energy Efficient Training3G4GWi-Fi123456789100.0%20.0%40.0%AlexNet, Performance Efficient Training12345678910Batch size0.0%25.0%50.0%AlexNet, Energy Efficient Training123456789100.0%20.0%40.0%ResNet, Performance Efficient Training12345678910Batch size0.0%20.0%40.0%ResNet, Energy Efficient Training020406080100Percentage weights updated backpropagation(a)020004000Latency (ms)12345678910Batch size inference(b)0100020003000Latency (ms)Pix2PixDeep SpeechVGG16NiNChairOverfeatAlexNetResNet Figure Interesting schedules execution types DNN architectures.
 opposite discriminative networks, last layers are huge optimal solution are computed mobile.
 Lastly, autoencoders, input output data sizes are first last layers are computed mobile.
 JointDNN pushes parts computations mobile device.
 result lead less workload cloud server.
 see Table reduce cloud server’s workload average, enables cloud provider service more users, obtaining higher performance lower energy consumptions compared single-platform ap- proaches.
 Table Workload reduction cloud server different mobile networks Optimization Target Latency Energy (%) (%) Wi-Fi (%) Communication Dominance Execution time energy breakdown AlexNet, is noted representative state-of-the-art architectures deployed cloud servers, is depicted Figure
 cloud-only approach is dominated communication costs.
 demonstrated Figure total execution time is used communication case Wi-Fi, respectively.
 relative portion applies energy consumption.
 Comparing latency energy communication mobile- notice mobile-only approach AlexNet is cloud-only approach mobile networks.
 apply lossless compression methods order reduce effect communication, be covered next section.
 Layer Compression preliminary results experiments show more total energy delay cost DNNs are caused communication collaborative approach.
 cost is proportional size layer being downloaded up- loaded mobile device.
 complex feature extraction process DNNs, size intermediate layers are larger network’s input data.
 example, ratio go high VGG16.
 address bottleneck, Figure (a) Execution time AlexNet optimized per- formance (b) Mobile energy consumption AlexNet opti- mized energy (c) Data size layers AlexNet scheduled computation, first layers are computed mobile rest cloud, is optimal solution performance energy.
 investigated compression data communica- tion.
 process be applied different DNN architecture types; however, considered CNNs due specific characteristics explained details.
 CNN architectures are used image video recogni- tion applications.
 local preservation char- acteristics conv assume output first convolution layers are following same structure Figure Layer output passing input image conv, relu lrn.
 Channels are preserving gen- eral structure input image large ratio out- put data is black (zero) due existence relu.
 Tiling is used put channels together.
 AlexNet, Wi-Fi, Latency Efficient, TrainingMobileCloudChair, Wi-Fi, Latency Efficient, TrainingPix2Pix, Wi-Fi, Latency Efficient, TrainingAlexNet, Wi-Fi, Latency Efficient, InferencePix2Pix, Wi-Fi, Latency Efficient, InferenceChair, Wi-Fi, Latency Efficient, InferenceMobile-onlyCloud-only (3G)JointDNN (3G)Cloud-only (4G)JointDNN (4G)Cloud-only (Wi-Fi)JointDNN (Wi-Fi)(a)02004006008001000Latency (ms)Mobile computationCloud computationCommunicationMobile-onlyCloud-only (3G)JointDNN (3G)Cloud-only (4G)JointDNN (4G)Cloud-only (Wi-Fi)JointDNN (Wi-Fi)(b)02004006008001000Energy (mJ)Mobile computationCommunicationdataconv1relu1lrn1pool1conv2relu2lrn2pool2conv3relu3conv4relu4conv5relu5pool5fc6relu6fc7relu7fc8softmax(c)0100K200KData size (bytes)Executed mobileExecuted cloud input image, shown Figure
 Moreover, big ratio layer outputs are expected be due presence relu layer.
 observations shows ratio neurons equal (ZR) varies relu CNNs. characteristics, layers being similar input image, large proportion data being single value, suggest employ existing image compression techniques output.
 are general categories compression techniques, lossy lossless [5].
 lossless techniques is possible recon- struct original information completely.
 contrary, lossless techniques use approximations original data cannot be reconstructed.
 experiments, examined impact com- pression using PNG, lossless technique, based encoding frequent sequences image.
 data type DNN parameters typical im- plementations are 32-bits floating-points, most image formats are based 3-bytes RGB color triples.
 Therefore, compress layer same way floating-point data be quantized 8-bits fixed-point.
 Recent studies show representing parameters DNNs 4-bits affect accuracy [38].
 work, implemented architectures 8-bits fixed-point presented baseline compression.
 layers CNN contain numerous channels matrices, similar image.
 simple method is compress channel separately.
 addition extra overhead file header channel, method take best frequent sequence decoding PNG.
 alternative is locating different channels side side, referred tiling, form large matrix representing layer shown Figure
 be noted fc layers are small did apply compression them.
 Compression Ratio (CR) is defined ratio size layer (8-bit) size compressed matrix PNG.
 Looking results compression different CNN archi- tectures Figure observe high correlation ratio pixels being (ZR) CR.
 PNG compress layer data average.
 results confirm ef- fectiveness proposed compression method.
 replacing compressed layers output adding cost compression pro- cess JointDNN formulations, achieve extra improvements energy latency average, respectively.
 RELATED WORK AND COMPARISON General Task Offloading Frameworks.
 are existing prior arts focusing offloading computation mobile cloud[2,
 However, frameworks share limiting feature makes impractical computation partitioning DNN applications.
 frameworks are programmer annotations dependent make decisions pre-specified functions, whereas JointDNN makes scheduling decisions based model topology mo- bile network specifications run-time.
 Offloading function level, cannot lead efficient partition decisions due layers given type architecture have different compu- tation data characteristics.
 instance, specific convolution Figure Compression Ratio (CR) ratio val- ued neurons (ZR) different layers (a) AlexNet (b) VGG16.
 layer structure be computed mobile cloud different models optimal solution.
 Neurosurgeon is only prior art exploring similar computa- tion offloading idea DNNs mobile device cloud server layer granularity.
 Neurosurgeon assumes is data transfer point execution schedule efficient solution starts mobile switches cloud, performs whole rest computations.
 results show is true online training, opti- mal schedule execution follows mobile-cloud-mobile pattern.
 Moreover, generative autoencoder models follow multi data transfer points pattern.
 execution schedule start cloud case generative models input data size is large.
 Furthermore, inter-layer optimizations performed DNN libraries are considered Neurosurgeon.
 Moreover, Neurosurgeon schedules optimal latency en- ergy, JointDNN adapts different scenarios including battery limitation, cloud server congestion, QoS.
 Lastly, Neurosurgeon targets simple CNN ANN models, JointDNN utilizes graph based approach handle complex DNN architectures ResNet RNNs. CONCLUSIONS paper, demonstrated status-quo approaches, cloud-only mobile-only, are optimal regard latency energy.
 reduced problem partitioning compu- tations DNN shortest path problem graph.
 Adding constraints shortest path problem makes NP-Complete, therefore, provided ILP formulations cover different pos- sible scenarios limitations mobile battery, cloud congestion, QoS.
 solve problem different set parameters beforehand (e.g. network bandwidth, cloud server load, etc.) use look-up table avoid overhead solving conv1relu1lrn1maxpool1conv2relu2lrn2maxpool2conv3relu3conv4relu4conv5relu5maxpool5(a)0123456Compression Ratioconv1_1conv1_2pool1conv2_1conv2_2pool2conv3_1conv3_2conv3_3pool3conv4_1conv4_2conv4_3pool4conv5_1conv5_2conv5_3pool5(b)0246Compression RatioCRZR020406080Zero Portion(%)020406080Zero Portion(%) optimization problem.
 output data size discriminative networks is smaller other layers network, therefore, last layers are expected be computed cloud, first layers are expected be computed mobile.
 reverse reasoning works Generative models.
 Autoencoders have large input output data sizes, implies first last layers are expected be computed mobile.
 insights, execution schedule DNNs have various patterns depending model architecture.
 research was supported grants NSF SHF DARPA MTO.
 REFERENCES [1] Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, others.

 cuDNN: Efficient Primitives Deep Learning.
 CoRR abs/1410.0759 (2014).
 arXiv:1410.0759 http://arxiv.org/abs/1410.0759 [2] Byung-Gon Chun, Sunghwan Ihm, Petros Maniatis, Mayur Naik, Ashwin Patti.

 CloneCloud: Elastic Execution Between Mobile Device Cloud.

 [3] Nvidia Corporation.

 Jetson TX2 Module.
 https://developer.nvidia.com/ embedded/buy/jetson-tx2.
 (2018).
 [Online; accessed 15-January-2018].
 [4] Nvidia Corporation.

 TESLA DATA CENTER GPUS FOR SERVERS.
 http:// www.nvidia.com/object/tesla-servers.html.
 (2018).
 [Online; accessed
 [5] Thomas M.
 Cover Joy A.
 Thomas.

 Elements Information Theory (Wiley Series Telecommunications Signal Processing).
 Wiley-Interscience.
 [6] Eduardo Cuervo, Aruna Balasubramanian, Dae-ki Cho, others.

 MAUI: Making Smartphones Last Longer Code Offload.

 https: //doi.org/10.1145/1814433.1814441 [7] Jeffrey Dean, Greg S.
 Corrado, Rajat Monga, others.

 Large Scale Distributed Deep Networks.
 Proceedings 25th International Conference Neural Information Processing Systems Volume (NIPS’12).
 Curran Associates Inc., USA,
 http://dl.acm.org/citation.cfm?id=2999134.2999271 [8] Alexey Dosovitskiy, Jost Tobias Springenberg, Thomas Brox.

 Learning Generate Chairs Convolutional Neural Networks.
 CoRR abs/1411.5928 (2014).
 arXiv:1411.5928 http://arxiv.org/abs/1411.5928 [9] Chelsea Finn Sergey Levine.

 Deep Visual Foresight Planning Robot Motion.
 CoRR abs/1610.00696 (2016).
 arXiv:1610.00696 [10] Xavier Glorot, Antoine Bordes, Yoshua Bengio.

 Deep Sparse Rectifier Neural Networks.
 Proceedings Fourteenth International Conference Artificial Intelligence Statistics (Proceedings Machine Learning Research), Vol.

 PMLR, Fort Lauderdale, FL, USA,
 http://proceedings.mlr.press/ v15/glorot11a.html [11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, others.

 Gener- ative Adversarial Nets.
 Advances Neural Information Processing Systems Z.
 Ghahramani, M.
 Welling, C.
 Cortes, N.
 D.
 Lawrence, K.
 Q.
 Wein- berger (Eds.).
 Curran Associates, Inc.,
 http://papers.nips.cc/paper/ 5423-generative-adversarial-nets.pdf [12] Mark S.
 Gordon, D.
 Anoushe Jamshidi, Scott Mahlke, Z.
 Morley Mao, Xu Chen.

 COMET: Code Offload Migrating Execution Transparently.

 http://dl.acm.org/citation.cfm?id=2387880.2387890 [13] Awni Y.
 Hannun, Carl Case, Jared Casper, others.

 Deep Speech: Scaling end-to-end speech recognition.
 CoRR abs/1412.5567 (2014).
 arXiv:1412.5567 http://arxiv.org/abs/1412.5567 [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.

 Deep Residual Learning Image Recognition.
 CoRR abs/1512.03385 (2015).
 arXiv:1512.03385 http://arxiv.org/abs/1512.03385 [15] Sunpyo Hong Hyesoon Kim.

 Integrated GPU Power Per- formance Model.
 SIGARCH Comput.
 Archit.
 (June
 https://doi.org/10.1145/1816038.1815998 [16] Junxian Huang, Feng Qian, Alexandre Gerber, others.

 Close Ex- amination Performance Power Characteristics LTE Networks.
 Proceedings 10th International Conference Mobile Systems, Applications, Services (MobiSys ’12).
 ACM, New York, NY, USA,
 [17] Texas Instruments Incorporated.

 INA Current/Power Monitor.
 http://www.
 ti.com/product/INA226.
 (2018).
 [Online; accessed 15-January-2018].
 [18] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A.
 Efros.

 Image-to- Image Translation Conditional Adversarial Networks.
 CoRR abs/1611.07004 (2016).
 arXiv:1611.07004 http://arxiv.org/abs/1611.07004 [19] K.
 Jarrett, K.
 Kavukcuoglu, M.
 Ranzato, Y.
 LeCun.

 is best multi-stage architecture object recognition?.
 IEEE International Conference Computer Vision.

 [20] A.
 Juttner, B.
 Szviatovski, I.
 Mecs, Z.
 Rajko.

 Lagrange relaxation based method QoS routing problem.
 (2001), vol.2. [21] Yiping Kang, Johann Hauswald, Cao Gao, others.

 Neurosurgeon: Col- laborative Intelligence Between Cloud Mobile Edge.
 Proceedings Twenty-Second International Conference Architectural Support Program- ming Languages Operating Systems (ASPLOS ’17).
 ACM, New York, NY, USA,
 https://doi.org/10.1145/3037697.3037698 [22] Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton.

 ImageNet Classifi- cation Deep Convolutional Neural Networks.
 Advances Neural Infor- mation Processing Systems F.
 Pereira, C.
 J.
 C.
 Burges, L.
 Bottou, K.
 Q.
 Wein- berger (Eds.).
 Curran Associates, Inc.,
 http://papers.nips.cc/paper/ 4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf abs/1312.4400 (2013).
 arXiv:1312.4400 http://arxiv.org/abs/1312.4400 [23] Hao Li, Jihun Yu, Yuting Ye, Chris Bregler.

 Realtime Facial Animation On-the-fly Correctives.
 ACM Trans.
 Graph.
 Article pages.
 [24] Min Lin, Qiang Chen, Shuicheng Yan.

 Network Network.
 CoRR [25] Mahdi Nazemi, Amir Erfan Eshratifar, Massoud Pedram.

 Hardware- Friendly Algorithm Scalable Training Deployment Dimensionality Reduction Models FPGA.
 Proceedings IEEE International Sympo- sium Quality Electronic Design.
 [26] Apple Newsroom.

 future is here: iPhone X.
 https://www.apple.com/ newsroom/2017/09/the-future-is-here-iphone-x/.
 (2017).
 [Online; accessed 15-January-2018].
 [27] Kyoung-Su Oh Keechul Jung.

 GPU implementation neural networks.
 (06
 [28] OpenSignal.com.

 State Mobile Networks: USA.
 https://opensignal.com/ [Online; accessed reports/2017/08/usa/state-of-the-mobile-network.
 (2017).
 15-January-2018].
 [29] OpenSignal.com.

 United States Speedtest Market Report.
 http://www.
 speedtest.net/reports/united-states/.
 (2017).
 [Online; accessed 15-January-2018].
 [30] Yunpeng Pan, Ching-An Cheng, Kamil Saigol, others.

 Off- Road Autonomous Driving Using End-to-End Deep Imitation Learning.
 CoRR abs/1709.07174 (2017).
 arXiv:1709.07174 http://arxiv.org/abs/1709.07174 [31] Hang Qi, Evan R.
 Sparks, Ameet Talwalkar.

 Paleo: Performance Model Deep Neural Networks.
 (2017).
 [32] Moo-Ryong Ra, Anmol Sheth, Lily Mummert, others.

 Odessa: Enabling Interactive Perception Applications Mobile Devices.

 https: [33] M.
 S.
 Razlighi, M.
 Imani, F.
 Koushanfar, T.
 Rosing.

 LookNN: Neural network multiplication.
 Design, Automation Test Europe Conference Exhibition (DATE),

 https://doi.org/10.23919/DATE.2017.7927280 [34] Pierre Sermanet, David Eigen, Xiang Zhang, others.

 OverFeat: Inte- grated Recognition, Localization Detection using Convolutional Networks.
 CoRR abs/1312.6229 (2013).
 arXiv:1312.6229 http://arxiv.org/abs/1312.6229 [35] Karen Simonyan Andrew Zisserman.

 Deep Convolutional Networks Large-Scale Image Recognition.
 CoRR abs/1409.1556 (2014).
 arXiv:1409.1556 http://arxiv.org/abs/1409.1556 [36] Karolj Skala, Davor Davidovic, Enis Afgan, Ivan Sovic, Zorislav Sojat.

 Scalable Distributed Computing Hierarchy: Cloud, Fog Dew Computing.
 Open Journal Cloud Computing (OJCC) (2015),
 http://nbn-resolving.
 de/urn:nbn:de:101:1-201705194519 [37] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov.

 Dropout: A Simple Way Prevent Neural Networks Overfitting.
 J.
 Mach.
 Learn.
 Res.
 (Jan.

 http://dl.acm.org/ citation.cfm?id=2627435.2670313 [38] Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, Joel S.
 Emer.

 Efficient Processing Deep Neural Networks: A Tutorial Survey.
 CoRR abs/1703.09039 (2017).
 arXiv:1703.09039 http://arxiv.org/abs/1703.09039 [39] Surat Teerapittayanon, Bradley McDanel, H.
 T.
 Kung.

 Distributed Deep Neural Networks Cloud, Edge End Devices.
 CoRR abs/1709.01921 (2017).
 arXiv:1709.01921 http://arxiv.org/abs/1709.01921 [40] Xudong Wang, Xuanzhe Liu, Ying Zhang, Gang Huang.

 Migration Execution JavaScript Applications Between Mobile Devices Cloud.

 https://doi.org/10.1145/2384716.2384750 [41] Zheng Wang J.
 Crowcroft.

 Quality-of-service routing supporting multimedia applications.
 IEEE Journal Selected Areas Communications (Sep
 https://doi.org/10.1109/49.536364 [42] M.
 D.
 Zeiler, D.
 Krishnan, G.
 W.
 Taylor, R.
 Fergus.

 Deconvolutional networks.
 IEEE Computer Society Conference Computer Vision Pattern Recognition.

 https://doi.org/10.1109/CVPR.2010.5539957 [43] Ying Zhang, Gang Huang, Xuanzhe Liu, others.

 Refactoring Android Java Code On-demand Computation Offloading.
 SIGPLAN Not.
 (Oct.

 https://doi.org/10.1145/2398857.2384634
 massive development connected devices precise instruments has introduced world vast volumes high-dimensional data.
 Traditional data analytics cannot cope massive amounts, motivates inves- tigating dimensionality reduction schemes capable gleaning low-dimensional information large-scale datasets.
 Dimensionality reduction is vital ﬁrst step render tractable critical learning tasks, such large-scale regression, classiﬁcation, clustering, allows processing datasets otherwise be tractable.
 [16], [10], Dimensionality reduction methods have been studied signal processing machine learning communities [2], [17].
 Principal component analysis (PCA) [10] is ‘workhorse’ method yielding low-dimensional representations preserve high-dimensional data variance.
 Multi-dimensional scaling (MDS) [12] other hand, maintains pairwise distances data going high- low-dimensional spaces, local linear embedding (LLE) [16] pre- serves relationships neighboring data.
 Information non-neighboring data is lost LLE’s low-dimensional representation, turn inﬂuence performance ensuing tasks such classiﬁcation clustering [6], [21].
 is worth stressing aforementioned approaches cap- ture preserve linear relationships data.
 However, data residing nonlinear manifolds using lin- ear relations produce low-dimensional representations are accurate.
 Generalizing PCA, Kernel PCA [9] capture nonlinear relationships data, preselected Work paper was supported NSF NIH 1R01GM104975-01.
 kernel function.
 addition, Laplacian eigenmaps [2] preserve nonlinear similarities neighboring data.
 aforementioned approaches have been suc- cessful reducing dimensionality various types data, do consider additional information dimensionality reduction process.
 prior information be task speciﬁc, e.g. provided “expert” physics problem, be inferred alternate views data, provide additional insights desired properties low-dimensional representations.
 fMRI signals instance, addition time series collected different brain regions, have access connectivity patterns regions.
 shown [8], [9], [18], [19] PCA, additional information be encoded graph, incorporated dimensionality reduction process graph-aware regularization.
 present paper puts novel framework di- mensionality reduction capture nonlinear relations data, exploit additional information graph regularization.
 framework encompasses afore- mentioned approaches, broadens scope.
 II.
 PRELIMINARIES AND PROBLEM STATEMENT Consider dataset N vectors dimension D col- lected columns matrix X := [x1,


 ].
 loss generality, be assumed sample mean N −1 PN n=1 xn has been removed xi.
 Dimensional- ity reduction looks set d-dimensional vectors {yi}N i=1, d < D, preserve certain properties {xi}.
 MDS instance aims preserve pairwise distances {xi} obtaining corresponding low-dimensional represen- tations {yi}, LLE attempts preserve neighborhoods.
 be shown ensuing subsections, dimen- sionality reduction schemes are special cases kernel-based PCA.
 A.
 Principal component analysis Given X, PCA ﬁnds linear subspace dimension d data lie close (in least- squares sense).
 Speciﬁcally, PCA solves Ud,{yi} i=1 kxi − Udyik2 s.
 U⊤ d Ud = (1) Ud ∈ RD×d is orthonormal matrix.
 optimal solution (1) is yi = U⊤ d xi, Ud is formed eigenvectors XX⊤ = UΣU⊤ corresponding d largest eigenvalues [7].
 future use, consider singular value decomposition (SVD) X = UΣV⊤.
 Given original vectors be recovered xi Udyi.
 PCA thrives data lie d-dimensional hyperplane.
 complexity is eigendecomposing XX⊤, i.e., O(N D2), means PCA is affordable D ≪ N
 contrast, dimensionality reduction small sets high- dimensional vectors (D ≫ N becomes tractable dual PCA.
 B.
 Dual PCA Kernel PCA SVD Y implies Ud = YVΣ−1, d yi = turn yields low-dimensional vectors ψi = U⊤ Σ−1 d Y⊤yi.
 Collecting Ψ := [ψ1,


 have d V⊤ Y = U⊤ d X = ΣdV⊤ (2) Σd ∈ Rd×d is diagonal matrix containing d leading eigenvalues X⊤X, Vd ∈ RN ×d is submatrix V collecting corresponding eigenvectors X⊤X.
 complexity dual PCA is O(DN therefore, is preferable D ≫ N
 be veriﬁed (1), Y (2) is optimal solution following optimization problem min kKx − Y⊤Yk2 s.
 YY⊤ = Λd (3) Kx := X⊤X, Λd denotes d × d diagonal matrix containing d largest eigenvalues Kx. Compared PCA, dual PCA needs inner products {x⊤ i xj order obtain low-dimensional representations, X itself.
 Hence, dual PCA yield low-dimensional vectors general (non-metric) objects are expressed using vectors {xi}, inner products (a.k.a correlations) latter are known.
 Furthermore, expanding cost (3), express tr(YK ⊤).
 (4) min −tr(YKxY ⊤) ⇔ min Y:YY⊤=Λd Y:YY⊤=Λd PCA performs data lie hyper- plane, property be true many datasets [9].
 such cases resort kernel PCA.
 Kernel PCA “lifts” {xi} using nonlinear function φ, higher inﬁnite) dimensional space, data lie linear hyperplane, ﬁnds low-dimensional representations {yi}.
 Kernel PCA is obtained solving (3) [Kx]i,j = κ(xi, xj) = φ⊤(xi)φ(xj), κ(xi, xj) denotes prescribed kernel function [5].
 C.
 Local linear embedding popular method deals data cannot be presumed hyperplane is LLE.
 assumes {xi} lie smooth manifold, be approximated tangential hyperplanes.
 Speciﬁcally, LLE assumes datum be expressed linear combination neighbors; is, xi = Pj∈Ni wij xj + ei, Ni is set containing indices nearest neighbors xi, Euclidean distance sense.
 order solve {wij following optimization problem is considered W = arg ˇW − X ˇWk2 s.
 ˇwij = ∀i /∈ Nj, X ˇwij = (5) ˇwij denotes (i, j)-th entry ˇW.
 obtaining W constrained least-squares solution (5), LLE ﬁnds {yi} best preserve neighborhood relationships encoded W, solving min Y:Y⊤Y=Λd ⇔ min Y:Y⊤Y=Λd kY − YWk2 tr[Y(I − W)(I − W)⊤Y⊤] (6) Λd is diagonal matrix.
 Conventional LLE adopts Λd = I, is subsumed constraint (6).
 Nonethe- less, difference is scaling {yi} I.
 diagonal Λd collects d smallest eigenvalues matrix (I − W)(I − W)⊤, (6) is special case kernel PCA [cf.
 (4)] Kx = [(I − W)(I − W)⊤]† (7) † denotes pseudo inverse.
 Similarly, other popular dimensionality reduction methods such MDS Laplacian eigenmaps be viewed special cases kernel PCA, selecting Kx [4].
 Thus, (4) be viewed encompassing framework nonlinear dimensionality reduction.
 framework is foundation general graph-aware methods develop ensuing section.
 III.
 GRAPH-AWARE NONLINEAR DIMENSIONALITY REDUCTION Matrices Kx dual PCA, kernel PCA LLE depend X.
 However, mentioned Sec.
 I, additional structural information useful dimensionality reduction task be available.
 knowledge be en- coded graph embodied Y graph regularization.
 Speciﬁcally, suppose exists graph G data is smooth; is, vectors {xi} nodes G are other Euclidean distance.
 A denoting adjacency matrix G, have [A]ij = aij node i is connected node j.
 Laplacian G is deﬁned LG := D − A, D is diagonal matrix entries [D]ii = dii = Pj aij
 consider tr(YLG Y⊤) = i=1 j6=i aij(yi − yj)2 (8) is weighted sum distances adjacent yi’s graph.
 minimizing (8) Y, low-dimensional representations corresponding adjacent nodes large edge weights aij be other.
 A.
 Kernel PCA graphs Introducing (8) regularization term original kernel PCA problem (4), arrive min Y:YY⊤=Λd tr(YK−1 x Y⊤) + γtr(YLG Y⊤) (9) γ is positive scalar, Λd collects d smallest x + γLG = ¯VΛ ¯V⊤.
 Combining eigenvalues K−1 Laplacian regularization kernel PCA formulation, (9) is capable ﬁnding {yi} preserve “lifted” covariance captured Kx, same time, promoting Algorithm Local nonlinear embedding graphs Input: X, γ S1.
 Estimate W X.
 S2.
 Obtain kernel matrix Kx (7).
 S3.
 low-dimensional representations Y using (10).
 smoothness low-dimensional representations graph G.
 Problem (9) admits closed-form solution Y = Λ1/2 ¯V⊤ q=1 θqK(q) (10) ¯Vd denotes sub-matrix ¯V containing columns corresponding eigenvalues Λd.
 γ is set obtains solution kernel PCA [cf.
 (2)].
 Remark kernel function needed form Kx is known priori, use Kx = PQ (9), {K(q) x is known dictionary kernel matrices, {θq} are estimated variables (9) [1].
 addition, using LG, family graph kernels r(LG := UGr(Λ)U⊤ G be employed, r(.) is scalar function eigenvectors LG
 selecting r(.), different graph properties be accounted for.
 example, r sets eigenvalues certain threshold acts sort “low pass” ﬁlter graph.
 detailed discussion choose graph kernels be found [14], [15].
 Furthermore, prescribing r(LG ), data-driven dictionary-based approach be employed learn proper graph kernel task hand [15].
 Remark single graph regularizer is introduced (9), method is ﬂexible include multiple graph regularizers based different graphs.
 proposed method offers powerful tool dimensionality reduction so-called multi-layer graphs, encode relationships data multiple graphs [11], [22].
 B.
 Local nonlinear embedding graphs Broadening premise LLE, pursue general dimensionality reduction framework captures nonlinear correlations neighboring data, addition structure induced graph G.
 end, suppose datum be represented neighbors [xi]m = X j∈Ni {hij(·)}N admitting P th-order expansion hij(cid:0)[xj]m(cid:1) + [ei]m, m =


 D (11) i,j=1 are prescribed scalar nonlinear functions hij(z) = p=1 wij [p]zp (12) coefﬁcients are be determined.
 Taylor’s expansion asserts P large, (12) offers accurate approximation memoryless differentiable nonlinear functions.
 Such nonlinear model has been used gragh topology identiﬁcation [20].
 estimated W low-dimensional representations be obtained (10), Kx (7); see Algorithm
 (a) (b) (c) (d) (e) (f) Fig.
 Embedding results manifolds: linear hyperplane trefoil (a) visualization {xi}N i=1 obtained (b) PCA; (c) LLE K = (d) LNEG K = (e) LLE K = (f) LNEG K =
 i=1; {yi}N IV.
 NUMERICAL TESTS performance generalized version LLE was assessed using tests synthetic data.
 Alg.
 is tested using Kx (7) nonlinear embedding (LNE) graph regularization (the abbreviated LNEG), is compared LLE PCA [10].
 experiments, graph G is constructed adjacecy matrix (i, j)th entry aij = xix⊤ j /kxikkxjk.
 types tests are carried order to: a) evaluate embedding performance single manifold; b) assess infor- mative low-dimensional embeddings are distinguishing different manifolds.
 Embedding experiment.
 ﬁrst experiment, em- bedding performance proposed method is assessed.
 3-dimensional Swiss roll manifold is generated, data are sampled manifold shown Fig.
 (a).
 Fig.
 (b) showcases 2-dimensional embeddings obtained PCA, Figs.
 (c) (d) illustrate resulting embeddings LLE LNEG respectively, neighborhoods K = data are considered.
 Figs.
 (e) (f) illustrate embeddings obtained LLE LNEG K =
 regularization parameter LNEG is set P =
 Clearly, exploiting nonlinear relationships γ = polynomial order is set -1 -2 -1 -2 -3 (a) -2.5 -2 -1.5 -1 -0.5 -1 -2 -3 -4 -5 -4 -0.02 -0.04 -0.06 -0.08 -3 -2 -1 -0.1 -0.02 (b) (c) (d) Fig.
 Embedding results linear hyperplane hole trefoil (a) visualization manifolds; {yi}N i=1 obtained (b) LLE K = and, (c) LNEG K = P = (d) PCA.
 Trefoil-knots -1 -2 -4 -2 (a) -2 -1 -0.4 -0.6 -0.8 -1 -1.2 -1.4 -1.6 -1.8 -2 -2.2 -2.4 -1.5 -0.02 -0.04 -0.06 -1 -0.5 -0.08 -0.04 -0.02 (b) (c) (d) Fig.
 Embedding results nonlinear trefoil (a) visualization manifolds; {yi}N obtained (b) LLE K = (c) LNEG K = P = (d) PCA.
 i=1 Plane-hole-trefoil Sphere-trefoil LLE LNE LNEG LLE LNE LNEG PCA TABLE I: Clustering error rate low-dimensional represen- tations obtained from: LLE, LNE, LNEG PCA.
 resulting low-dimensional representations are capable better preserving structure manifold, allowing more accurate visualization.
 Clustering experiment.
 experiment, ability Alg.
 provide meaningful embeddings clustering different manifolds is assessed.
 3-dimensional manifolds, linear hyperplane hole origin trefoil are generated same ambient space [3], data are sampled them, respectively.
 manifold corresponds different cluster.
 Fig.
 illustrates sampled points generated manifolds.
 Z1 ∈ R3×200 Z2 ∈ R3×400 contain data gener- ated linear hyperplane trefoil, respectively.
 manifolds are embedded R100, is Xi = PZi +Ei, P ∈ R100×3 is orthonormal matrix, E is noise matrix entries sampled mean Gaussian distribution variance
 100-dimensional data X := [X1 X2] are embedded 2-dimensional representations Y ∈ R2×600 using LLE, LNEG PCA.
 Figures.
 (c), (d) depict 2- dimensional embeddings Y provided LLE, LNEG, PCA, respectively.
 Similarly, Fig.
 illustrates resulting embeddings Z2 is sampled nonlinear sphere.
 cases, nonlinear methods result embeddings separate manifolds.
 assess performance, K-means is carried resulting Y [13].
 Table shows clustering error running K-means low-dimensional embeddings given PCA, LLE, LNE LNEG, different values K.
 proposed approaches provide embeddings enhance separability man- ifolds, resulting lower clustering error compared LLE PCA.
 addition, greater performance gain is observed manifolds are nonlinear, case Fig.

 V.
 CONCLUSIONS paper introduced general framework nonlinear dimensionality reduction graphs.
 leveraging nonlinear relationships data, low-dimensional representations were obtained preserve nonlinear correlations.
 Graph regularization was employed account additional prior knowledge seeking low-dimensional representations.
 efﬁcient algorithm admits closed-form solution was developed, several tests were conducted simulated data demonstrate effectiveness.
 broaden scope study, several intriguing directions open up: a) extensive numerical tests real data; b) development data-dependent schemes are capable selecting appropriate kernels; c) online implementations handle streaming data; d) generalizations cope large-scale graphs high- dimensional datasets.
 REFERENCES [1] F.
 R.
 Bach, G.
 R.
 Lanckriet, M.
 I.
 Jordan, “Multiple kernel learning, conic duality, smo algorithm,” Proc.intl.
 Conf.
 Machine learning, New York, USA, pp.

 [2] M.
 Belkin P.
 Niyogi, “Laplacian eigenmaps dimensionality reduction data representation,” Neural Computation, vol.
 no.
 pp.
 Mar.

 [3] E.
 Elhamifar R.
 Vidal, “Sparse manifold clustering embedding,” Advances Neural Information Processing Systems, Granada, Spain, pp.

 [4] A.
 Ghodsi, “Dimensionality reduction -A short tutorial,” Department Statistics Actuarial Science, Univ.
 Waterloo, Ontario, Canada, vol.
 p.

 [5] J.
 Ham, D.
 D.
 Lee, S.
 Mika, B.
 Sch¨olkopf, “A kernel view dimensionality reduction manifolds,” Proc.
 Intl.
 Conf.
 Machine Learning.
 Alberta, Canada: ACM, Jul.
 p.

 [6] J.
 A.
 Hartigan M.
 A.
 Wong, “Algorithm AS A K-means clustering algorithm,” Journal Royal Statistical Society, vol.
 no.
 pp.
 Jan.

 [7] T.
 Hastie, R.
 Tibshirani, J.
 Elements Statistical Learning.

 [8] B.
 Jiang, C.
 Ding, J.
 Tang, “Graph-Laplacian PCA: Closed-form solution robustness,” Proc.
 Conf.
 Computer Vision Pattern Recognition, Portland, Oregon, Jun.
 pp.

 [9] T.
 Jin, J.
 Yu, J.
 You, K.
 Zeng, C.
 Li, Z.
 Yu, “Low-rank matrix factorization multiple hypergraph regularizer,” Pattern Recognition, vol.
 no.
 pp.
 Mar.

 [10] I.
 Jolliffe, Principal Component Analysis.
 Wiley Online Library,
 [11] M.
 Kivel¨a, A.
 Arenas, M.
 Barthelemy, J.
 P.
 Gleeson, Y.
 Moreno, M.
 A.
 Porter, “Multilayer networks,” Journal Complex Networks, vol.
 no.
 pp.

 [12] J.
 B.
 Kruskal M.
 Wish, Multidimensional Scaling.
 Sage, vol.

 [13] S.
 Lloyd, “Least-squares quantization PCM,” IEEE Trans.
 Info.
 Theory, vol.
 no.
 pp.

 [14] D.
 Romero, V.
 N.
 Ioannidis, G.
 B.
 Giannakis, “Kernel-based Re- construction Kalman Filtering Space-time Functions Dynamic Graphs,” IEEE Journal Special Topics Signal Processing, vol.
 no.

 [15] D.
 Romero, M.
 Ma, G.
 B.
 Giannakis, “Kernel-based reconstruction graph signals,” IEEE Transactions Signal Processing, vol.
 no.
 pp.
 Feb.

 [16] S.
 T.
 Roweis L.
 K.
 Saul, “Nonlinear dimensionality reduction linear embedding,” Science, vol.
 no.
 pp.
 Dec.

 [17] B.
 Sch¨olkopf, A.
 Smola, K.-R.
 M¨uller, “Kernel principal component analysis,” Proc.
 Intl.
 Conf.
 Artiﬁcial Neural Networks, Lausanne, Switzerland, Oct.
 pp.

 [18] N.
 Shahid, N.
 Perraudin, V.
 Kalofolias, G.
 Puy, P.
 Vandergheynst, “Fast robust PCA graphs,” IEEE Journal Selected Topics Signal Processing, vol.
 no.
 pp.
 Feb.

 [19] F.
 Shang, L.
 Jiao, F.
 Wang, “Graph dual regularization non-negative matrix factorization co-clustering,” Pattern Recognition, vol.
 no.
 pp.
 Jun.

 [20] Y.
 Shen, B.
 Baingana, G.
 B.
 Giannakis, “Kernel-based structural equation models topology identiﬁcation directed networks,” IEEE Trans.
 Sig.
 Proc., vol.
 no.
 pp.
 May
 [21] J.
 A.
 Suykens J.
 Vandewalle, “Least squares support vector machine classiﬁers,” Neural Processing Letters, vol.
 no.
 pp.
 Jun.

 [22] P.
 A.
 Traganitis, Y.
 Shen, G.
 B.
 Giannakis, “Topology inference multilayer networks,” Intl.
 Workshop Network Science Comms., Atlanta, GA, May
 Autonomous driving has attracted considerable interest past decades signiﬁcant progress has been
 According Douges [Donges, autonomous driving tasks be classiﬁed categories: naviga- tion, guidance, stabilization.
 Strategic navigation tasks are responsible generating road-level routes.
 Tactical- level guidance tasks are responsible guiding ego vehi- cle routes complex environments generat- ing tactical maneuver decisions.
 operational-level stabi- lization tasks are responsible translating tactical decisions reference trajectories low-level control signals.
 classes tasks, tactical-level decision making is important due central role has been active ﬁeld research.
 Early successes decision making systems human-designed rules control decision us- ing methods such heuristic rules, decision trees, ﬁnite state machines, fuzzy-logic [Montemerlo et al., Urmson et Miller et
 methods are tailored speciﬁc environments do generalize robustly.
 recently, problem tactical decision making has been cast Partially Observable Markov Decision Pro- cess (POMDP) framework various approximate methods have been proposed solve intractable mod- els tactical decision making [Ulbrich Maurer, Brechtel Galceran
 common problem faced POMDP-based work is strong depen- dency simple environment model, hand-crafted (discrete) observation spaces, transi- tion dynamics, observation mechanisms.
 strong assumptions limit generality methods complex scenarios.
 recent years, success deep learning has revived interest end-to-end driving agent decides low-level control image inputs, using supervised learning [Bojarski al., reinforcement learning (RL) [Sallab et Plessen,
 black-box driving poli- cies learned methods are susceptible inﬂuence un- der drifted inputs.
 efforts have been made iden- tify robust compact subset prediction targets control outputs (e.g. [Chen most prac- tical autonomous driving systems date use deep learning restricted part whole system.
 Deep RL is natural way incorporate deep learning traditional POMDP RL-based decision making meth- ods.
 use function approximators makes possible use high-dimensional raw observations.
 al- leviates strong dependency hand-crafted simple mod- els traditional POMDP RL-based work.
 [Isele et al., [Mukadam et line research, al., deep Q-network (DQN) [Mnih et learn tactical decision policies intersection cross- ing lane changing freeway, respectively.
 Hierarchi- cal RL is combined Monte-Carlo tree search (MCTS) [Paxton learn high- level option policy decision making low-level pol- [Shalev-Shwartz et al., icy option execution.
 Shalev-Shwartz et al., combine high-level RL pol- icy non-learnable low-level policy balance efﬁciency safety.
 However, many commonly-used techniques deep RL are proposed low-level control tasks be efﬁcient and/or robust high-level tactical decision making observation.
 temporal abstrac- tion, frame skipping action repetition cause unstable low-level behavior due discontinuity high-level ac- tion semantics.
 multi-dimensional reward- ing systems, ﬁnd used sparse global goal indicators dense local goal metrics are general redundant harmful, constant value lane penalty is hard induce favorable lane switching behavior multi-lane scenarios.
 Thirdly, decision agents relying learned RL policy issue disastrous action inﬂuence observation noise.
 paper, aim tackle above difﬁculties deep RL.
 main contributions are set practical effective elements deep RL agents towards tactical driv- ing decision making tasks.
 propose non-uniform action skipping stable alternative action repetition.
 counter-based lane penalty is proposed encourage de- sired behavior multi-lane scenarios.
 learned RL agent be enhanced heuristic rules ﬁlter undesirable actions.
 elements are meant make less modiﬁcation existing methods possible sake simplicity, target peculiarity high-level tactical decision making effectiveness.
 proposed elements are equipped hierarchical autonomous driving system effectiveness is demonstrated realis- tic driving scenarios presenting two-way trafﬁc signaled intersections.
 Method consider hierarchical autonomous driving system orchestrates learning-based non-learning modules: tactical decision making module is implemented deep RL agent efﬁciency routing, planning, control modules are realized non-learning methods safety comfort.
 routing module calculates local lane sug- gestions global goal according road map.
 decision module takes consideration rout- ing suggestions other information such status ego vehicle road structure make high-level maneuver directions.
 planning module translates direc- tions trajectories vehicle poses.
 control mod- ule implements planning trajectories low-level control signals.
 Note planning module has certain built-in safety functions avoid hazards such collisions.
 agent-centric perspective, tactical decision agent makes sequential decisions dynamic environ- ment composed non-learning modules rest world.
 time step t, information ego vehicle surrounding environment is compiled observation ot presented agent.
 agent selects tacti- cal decision action = π(ot) according decision pol- icy π(·).
 Downstream modules receive decision control movement ego vehicle accordingly.
 reward- ing module assess movement current time step provide scalar reward rt.
 system evolve for- ward time next time step +
 goal is learn policy maximizes expected total discounted reward (cid:40) T(cid:88) (cid:41) E{Gt} = E γτ−tRτ (1) Note world state is observable de- cision agent due imperfect sensing unpredictable be- τ =t (a) crossing lane (b) crossing lane.
 Figure Illustration discontinuous action semantics lane switching tasks: meaning “switching right lane” changes crossing lane marking.
 havior other agents.
 extend observa- tion vector ot history means frame stacking.
 Other methods, recursive neural networks, be used ﬁll more information observation vector.
 Action Skipping Action-repeated frame skipping [Mnih al., is commonly-used technique speed reinforcement learn- ing algorithms.
 beneﬁts are multi-fold.
 explo- ration, non-trivial high-level maneuvers be explored random perturbation.
 difﬁculty forming series low-level movements correspond high-level maneuver be exponential length series.
 effective horizon semi-MDP re- sulting action repetition is shorter original MDP.
 bootstrapped value estimation meth- ods, such temporal difference learning, receive pro- portional speedup.
 Moreover, reward signal become resilient noises delays thanks extended ef- fective period action.
 However, action repetition be stable high-level decision making tasks due discontinuity action se- mantics.
 Consider driving multi-lane road shown Fig- ure ego vehicle is cross marking current lane (L0) lane right (L1), action switching right lane means switching L0 L1.
 ego car has crossed lane marking, semantics same action is changed switching L1 L2, L2 is lane right L1.
 frame skipping period con- tains such moments non-continuous action semantics, resulting high-level direction is doomed result unfavor- able lower-level behaviors, e.g. frequent overshot correc- tion above scenario.
 propose skip repeat actions.
 Concretely, agent action is meta action consisting actual decision followed several null actions (No-Op).
 de- note operation action skipping.
 agent continue collect observations rewards.
 overall reward meta action is calculated average rewards collected skipping period.
 Note implement action skipping decision module, lower-level modules need continue operation null action.
 is problem trajectory planning module plans horizon longer skip- ping period, is easy implement.
 drawback action skipping is decrease deci- sion frequency delay prevent agent’s reac- tion critical events.
 improve situation, actions take different skipping factors inference.
 instance lane changing tasks, skipping factor lane keeping be kept short allow swift maneuvers skipping factor lane switching be larger agent complete lane changing actions.
 perform- ing non-uniform action skipping, agent observe time phases are skipped training cause domain drift observations.
 solution, uniformly extend skipping factor ﬁrst agent step factor zero agent observe possible time phases training.
 Reward Function Tactical decision making needs balance efﬁciency, safety, comfort.
 scalar reward used tac- tical decision making is composed multiple com- ponents, linear combination.
 speaking, reward signals be classiﬁed sparse global goal indicators, sparse constraint violation alerts, dense heuristic components.
 reward components use is shown Table choices are explained
 Global Goal Indicators Global goal indicators are sparse signals take non-zero values long-term goal achieved missed.
 tactical decision making, true long-term goal is reaching destination given navigation module fast possible.
 common form global goal indicators is signal given end episode, positive ego car reaches destination negative other- wise.
 way, discounted total reward be larger episodes ego car reach destination earlier.
 argue global goal indicators are unnec- essary burdens tactical decision making agent.
 preferences described global goal indicators (i.e. is wanted) be expressed denser re- ciprocal constraint violation alerts (i.e. is unwanted).
 is possible behaviors stop ego ve- hicle reaching destination be deﬁned vio- lating constraints harshly penalized.
 use dense per-step cost devalue behaviors mis- lead ego vehicle dead-end situations.
 result, behavior help achieve global goal result low penalty.
 comparison sparse indicators, denser counterparts result faster credit assignment value estimation is desirable.
 Moreover, global goal indicators emitted end episodes increase absolute value expected accumulated rewards comparison indicator-free counterpart.
 approximators used value functions needs have larger dynamic range, mean learning steps training larger variance inference.
 reasons do use global goal indicator components experiments.
 Constraint Violation Alerts Constraint violation alerts are sparse signals penalize agent being risky situation performing risky ac- tion.
 common situation considered is collision other road other vehicles pedestrian.
 Note re- ward signals fall category be sparse constraint violations events.
 are sparse sense risky situation be rare cautious driving policy.
 consider types risky situations experi- ments: entering intersection red light, collision other vehicles, treading biking opposite lanes ego vehicle has less priority.
 former components termination episode.
 Trafﬁc light: trafﬁc light alert is triggered ego vehicle enters intersection corresponding con- nection road is covered red light.
 Note ren- der longitudinal speed control rule-based planning mod- ule stop ego vehicle most situa- tions, are corner cases grant ego vehicle’s access intersection red lights.
 There- fore trafﬁc light alert be place penalize corner-case behaviors.
 Collision risk: collision risk component is active ego vehicle is crash other vehicles.
 is sum risk components contributed other vehicles region interest.
 component is deﬁned product isotropic distance-based factor di- rectional factor related heading direction cars: (cid:88) rc = u · ri ri d, (2) u is isotropic factor target i ri d is direc- ri tional factor.
 distance-based risk factor is Laplacian distance target car ego car: u(d) = e−d+d0, ri (3) u(d) is distance-based risk target car i dis- ri tance d d0 is normalizing distance.
 directional risk factor is product narrow-band raised-cosine pat- center are aligned heading direc- tion ego car target car, respectively: d(θego, θtarget) = rcos(θ − θego) · rcos(θ − θtarget), ri (4) rcos(·) is narrow-band raised cosine function, θego θtarget are heading angle ego target vehicle, θ is direction vector connecting ego car target car.
 overall effect factors is high risk ego car target car is drive head-to-head Category Goal indicators Constraint violation Name N/A Collision risk Trafﬁc light Dangerous lane Dense heuristics Speed Lane switching Step cost Description N/A Directional risk crashing immediate other cars.
 Entering road section invalidated red light.
 Risk factor linear duration staying undesired lane.
 ego car velocity lane center.
 Unit cost switching adjacent lanes.
 Unit per-step cost.
 Weight N/A −1.0 −1.0 Biking: −0.2 Opposite −0.4 −0.4 −0.1 Table Description reward components weights.
 other.
 risk is low, e.g. ego car target car is driving side-by-side.
 commonly-used measure crashing risk is Time-to-Collision (TTC).
 TTC is propor- tional distance ego target car.
 fore is aggressive risk prediction tend exaggerate crashing risk.
 proposed risk for- mulation indicates risk is happen is conservative.
 foresee upcoming agent use common value estimation methods predict future risk values.
 Dangerous lane: use counter-based risk signals re- ﬂect ever-increasing empirical risk staying biking opposite lanes.
 Speciﬁcally, indicator lane maintains counter (with maximum value cap) keeps track time steps ego vehicle has spent lane.
 risk value is computed linear function corresponding counter value: R = (0.1x + × (x > (5) R is risk value x is counter value.
 lane risk deﬁned such be small ego vehi- cle arrived dangerous lane be- come large ego vehicle remains long time.
 way, switching dangerous lanes, is required overtaking slower trafﬁc single lane roads, be enabled.
 staying dangerous lane be prohibited long run.
 Note reward component deﬁned way becomes stateful, is important aug- ment agent observations history information infer long has stayed particular lane.
 contrast, is difﬁcult design constant- value risk signals has same enabling effect over- take maneuvers: small risk value encourage ego vehi- cle switch dangerous lanes necessary, fail encouraging lane switching small difference risk value be overwhelmed variance approximated value functions.
 agent learn risk dangerous lanes sparse collision events.
 Meanwhile, large risk value effect prohibit switching dangerous lanes, doing is beneﬁcial.
 Dense heuristic metrics Reward signals belonging category are used hard-code designer’s heuristic preference states actions.
 former reward categories, are easier design reﬂect orthogonal aspects desired unwanted outcomes, dense heuristic metrics are harder design heuristic rules conﬂict other or, worse, fail align global goals.
 reason, aim employ minimal set dense heuristic components.
 consider component proportional speed ego vehicle navigation route encourage driving to- wards goal possible.
 speed limits are mon- itored planning module avoid over-speeding.
 second dense component consider is per-step penalty lane changing actions discourage unnecessary lateral maneuvers improve passenger comfort.
 ﬁnal dense component applied is trivial per-step cost prefer short episodes.
 do employ dense penalties related local goals (e.g. local target lane headway other vehi- cles) conﬂict other heuristic metrics global goal.
 Rule-based Action Masking scenarios, undesirable tactical actions be identiﬁed.
 such cases, proposed apply simple rules ﬁlter actions inference hoping agent learn avoid ac- tions.
 reason is that, hand, agent learn avoid inferior be triggered due variance observation learned model.
 other hand, simple rules designed straightforward situations are prone unexpected false positives eas- ier debug happens.
 is contrast conventional rule-based decision policies is comprised complex set rules intended work complex scenarios.
 Implementation shown Figure decision agent observes tilted RGB top-down views ego vehicle’s immediate surround- ing.
 time step, latest frames ren- dered image stream are max-pooled pixel-wise single image frame combat ﬂickering.
 frames latest time steps are further stacked channel-wise form single observation.
 reward components lin- ear combination weights deﬁned Section are used derive scalar reward function.
 comparison, components be removed replaced comparison.
 tered route random starting points cruising speed.
 Ego vehicle start beginning selected route.
 global navigation module provide reference lane (rendered red line observed image) leads route destination.
 episode is ter- minated ego vehicle encounters following con- ditions: reaching destination; receiving crashing risk value > stop moving seconds1; entering in- tersection unpermitted lanes.
 agent conﬁguration is trained scratch simulation runs.
 run ter- minates accumulated number environment steps exceeds
 trained agents are evaluated with- exploration pre-generated test episodes.
 number other vehicles present episode is training test.
 Test results are shown Table organized sec- tions.
 select success rate, longitudinal speed, lateral speed performance metrics reﬂect safety, efﬁciency, comfort.
 agent needs switch correct lane avoid collisions ﬁnish episode.
 Overtak- ing slower trafﬁc is required achieve high longitu- dinal speed.
 Moreover, agent needs avoid unnecessary lateral maneuvers order reduce lateral speed.
 over- metric conﬁguration is calculated median per-step metric simulation runs.
 ﬁrst section presents random rule-based base- lines.
 simulation scenario is complex such random policy achieve success rate
 rule-based decision agent improve success rate metric reasonable number
 longitu- dinal speed rule-based agent is higher random agent thanks overtake maneuvers.
 lateral speed is reduced reasonable decision policy avoid lane changing stay current lane most time.
 second section compares different skipping conﬁgura- tions.
 RL agent skipping operation (ID03) performs poor, achieving success rate
 Action repetition operations (ID04) help im- success rate
 demonstrates impor- tance temporal abstraction high-level tactical decision making tasks.
 success rate dynamic action skipping scheme (ID05) is unsatisfactory action space is super-set action repetition scheme.
 possible explanation be gains broad- ening action space is over-weighed disadvantage extending effective horizon
 expected, action skipping (ID10) result smaller lateral speed action repeti- tion (ID04) thanks elimination overshoot-correction jitters preserving success rate.
 third section compares alternatives rewarding schemes proposed one.
 comparison experiment ID10, ex- periment ID06 superimpose ±1 global goal indicator proposed reward function cause success rate longitudinal speed metrics deteriorate.
 corroborates previous statement global goal indicators are redun- dant presence complete set constraint viola- tion penalties.
 Experiment ID07 ID08 replaces pro- happens ego vehicle drives dead-end lanes.
 (a) (b) Figure (a) Illustration tilted top-down view used agent observation.
 (b) Simulation scenario signaled intersection two-way trafﬁc.
 Red line indicates navigation route.
 agent is composed dueling deep Q-network (DQN) [Wang et al., convolutional layers fol- lowed dense layers linear output layer.
 last dense layer is divided action-wise advantage channel baseline channel.
 Q value action is sum corresponding advantage shared baseline.
 convolutional layers has kernels size stride respectively.
 ﬁrst dense layer has hidden units latter dense layer has hidden units channels.
 convolutional layers apply max pooling hidden layers apply ReLU activation.
 discount factor used is double Q learning [van Hasselt et al., is applied build temporal differ- ence loss.
 use mini-batch samples ADAM optimizer learning rate β1 = β2 = train learning network.
 target network uses syn- chronization rate track learning network.
 exploration factor is annealed steps.
 training process is handled rate-controlled training thread: data point col- lected environment, thread perform up- dates learning network.
 training thread samples memory-mapped on-disk replay buffer maximum size
 data is divided consecutive fragments size fragments are cached memory random sampling time.
 fragment be sampled most times new fragment be swapped replacement.
 Experimental Results shown Fig.2b, experiment different agent con- ﬁgurations simulated driving scenarios two-way traf- ﬁc signaled intersections.
 simulator is wrapped RL environment complying OpenAI Gym API [Brock- man et al., frequency agent-environment in- teraction is regularized terms simulation clock.
 Agents are trained tested episodic simulation.
 episode, route is sampled road map routes.
 route consists road segments con- nected intersection.
 number vehicles are scat- Section Baseline Skipping Reward Proposed Skipping N/A None Repetition Dynamic ID Non-Uniform Uniform Uniform Reward N/A Proposed Global goal lane penalty lane penalty Local goal Proposed Other Random Rule-based N/A / skipping N/A N/A Action mask Action mask Action mask Action mask Success Rate Lon.
 Speed Lat.
 Speed Table Experimental results.
 posed counter-based penalty constant penalty.
 re- sulting agent behavior is sensitive penalty value: larger value bans access dangerous lanes, resulting lower longitudinal speed experiment ID10; smaller value fails reﬂect risk lanes, resulting reduced success rate.
 Experi- ment ID09 adds dense penalty deviating navi- gation lane.
 ego vehicle achieve higher suc- cess rate sticking navigation miss opportunity deviate lane over- take slower trafﬁc.
 result is lower longitu- dinal speed.
 exempliﬁes dense local-goal related rewards conﬂict global goal.
 effectiveness rule-based action masking non- uniform action skipping is illustrated fourth table sec- tion.
 experiment set rules: rule ﬁlters lane switching behavior ego vehicle is moving navigation lane, rule #2 banns ego vehicle treading opposite lane biking lane addi- tion.
 Note rule #2 is stricter rule #1 requires more structural information environment.
 trade- off safety efﬁciency be identiﬁed com- paring Experiment ID11 ID12: stricter rule #2 provide more signiﬁcant improvements success rate rule #1, price reduced longitudinal speed.
 Finally, observe last experiments, non-uniform action skipping increase success rate masking rules giving ego car lane-changing opportunities inference.
 Related Work Temporal abstraction is effective means speeding RL algorithms.
 frame skipping action repe- tition [Mnih al., is simple form temporal abstraction, is effective has been shown be key state-of-art performance many low-level control tasks [Braylan et
 Due discontinuous seman- tics tactical driving decisions, propose replace ac- tion repetition stable action skipping method.
 Dynamic frame skipping [Lakshminarayanan al., is investigated [Isele high-level deci- sion learning, is effective action skipping experiments.
 Other sophisticated form temporal abstractions have been proposed RL-based decision making option framework [Sutton et e.g. [Shalev-Shwartz et al.,
 are tailored speciﬁc scenarios hard generalize.
 appropriate multi-dimensional rewarding system is in- dispensable tactical decision making agents are based RL POMDP.
 Many existing work applies global goal indicators, e.g. [Brechtel Isele Mukadam Paxton
 show is unnecessary use global goal indicators tactical decision making tasks.
 Constraint violation alerts existing work are one-shot constant time [Brech- tel et al., Isele Paxton Li
 propose use counter-based risk signals speed learning multi-lane scenarios.
 Dense local- goal penalties are used previous work reg- ulate driving policy [Ulbrich Maurer, Galceran Paxton
 show such penalties mislead agent sub-optimal policies be avoided possible.
 Conclusion Deep reinforcement learning is promising framework tackle tactical decision making tasks autonomous driv- ing systems.
 paper, propose several practical ingre- dients efﬁcient deep reinforcement learning algorithms to- wards tactical decision making.
 propose action skipping stable alternative commonly used action rep- etition scheme investigate necessary set reward com- ponents guide decision making agent learn effec- complex trafﬁc environments.
 reliable in- ference, heuristic rule-based action masker is combine learned agent ﬁlters unsafe actions.
 proposed ingredients is evaluated realistic driving simu- lator results show outperform various baseline alternative agent conﬁgurations.
 References [Bojarski et al., Mariusz Bojarski, Philip Yeres, Anna Choromanska, Krzysztof Choromanski, Bernhard Firner, Lawrence Jackel, Urs Muller.
 Explaining deep neural network trained end-to-end learning steers car.
 arXiv:1704.07911 [cs], Apr

 [Braylan et Alex Braylan, Mark Hollenbeck, El- liot Meyerson, Risto Miikkulainen.
 Frame skip is powerful parameter learning play atari.
 Space,
 [Brechtel al., S.
 Brechtel, T.
 Gindele, R.
 Dill- mann.
 Probabilistic decision-making uncertainty autonomous driving using continuous POMDPs, pages

 [Brockman et Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba.
 Openai gym,
 [Chen et Chenyi Chen, Ari Seff, Alain Korn- hauser, Jianxiong Xiao.
 DeepDriving: Learning Affor- dance Direct Perception Autonomous Driving, pages

 Edmund Donges.
 conceptual framework active safety road trafﬁc.
 Vehicle System Dynamics, Aug
 [Galceran al., Enric Galceran, Alexander G.
 Cun- ningham, Ryan M.
 Eustice, Edwin Olson.
 Mul- tipolicy Decision-Making Autonomous Driving Changepoint-based Behavior Prediction.

 [Isele al., David Isele, Akansel Cosgun, Kaushik Subramanian, Kikuo Fujimura.
 Navigating intersec- tions autonomous vehicles using deep reinforcement learning.
 arXiv preprint arXiv:1705.01196,
 [Lakshminarayanan al., Aravind Lakshmi- narayanan, Sahil Sharma, Balaraman Ravindran.
 Dynamic Action Repetition Deep Reinforcement Learning., pages

 S.
 [Li al., N.
 Li, D.
 W.
 Oyler, M.
 Zhang, Y.
 Yildiz, I.
 Kolmanovsky, A.
 R.
 Girard.
 Game theoretic mod- eling driver vehicle interactions veriﬁcation validation autonomous vehicle control systems.
 IEEE Transactions Control Systems Technology, PP(99):1–
 [Miller al., Isaac Miller, Mark Campbell, Dan Hut- Frank-Robert Kline, Aaron Nathan, Sergei Lu- pashin, Jason Catlin, Brian Schimpf, Pete Moran, Noah Zych, et al.
 Team cornell’s skynet: Robust perception planning urban environment.
 Journal Field Robotics, Aug
 [Mnih al., Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller.
 Playing atari deep reinforcement learning.
 arXiv:1312.5602 [cs], Dec

 [Montemerlo al., Michael Montemerlo, Jan Becker, Suhrid Bhat, Hendrik Dahlkamp, Dmitri Dolgov, Scott Et- tinger, Dirk Haehnel, Tim Hilden, Gabe Hoffmann, Burkhard Huhnke.
 Junior: stanford entry urban challenge.
 Journal ﬁeld Robotics,
 [Mukadam al., Mustafa Mukadam, Akansel Cos- gun, Alireza Nakhaei, Kikuo Fujimura.
 Tactical de- cision making lane changing deep reinforcement learning.

 [Paxton et Chris Paxton, Vasumathi Raman, Gre- gory D.
 Hager, Marin Kobilarov.
 Combining neural networks tree search task motion planning challenging environments.
 arXiv:1703.07887 [cs], Mar

 Mogens Graf Plessen.
 Automating vehicles deep reinforcement learning using task separation hill climbing.
 arXiv:1711.10785 [cs], Nov

 [Sallab al., Ahmad El Sallab, Mohammed Abdou, Etienne Perot, Senthil Yogamani.
 Deep reinforcement learning framework autonomous driving.
 Electronic Imaging, Jan

 Shalev-Shwartz, Safe, Shaked Shammah, multi-agent, learning autonomous driving.
 arXiv:1610.03295 [cs, stat], Oct

 [Shalev-Shwartz et al., Shai Amnon Shashua.
 reinforcement [Shalev-Shwartz et al., Shai Shalev-Shwartz, Shaked Shammah, Amnon Shashua.
 formal model safe scalable self-driving cars.
 arXiv:1708.06374 [cs, stat], Aug

 [Sutton et Richard S.
 Sutton, Doina Precup, Satinder Singh.
 Between mdps semi-mdps: frame- work temporal abstraction reinforcement learning.
 Artiﬁcial Intelligence, Aug
 [Ulbrich Maurer, Simon Ulbrich Markus Maurer.
 Probabilistic online POMDP decision making lane changes automated driving, pages
 IEEE,
 [Urmson al., Chris Urmson, Joshua Anhalt, Drew Bagnell, Christopher Baker, Robert Bittner, M.
 N.
 Clark, John Dolan, Dave Duggins, Tugrul Galatali, Chris Geyer.
 Autonomous driving urban environments: Boss Journal Field Robotics, urban challenge.

 [van Hasselt Hado van Hasselt, Arthur Guez, David Silver.
 Deep reinforcement learning dou- ble q-learning.
 arXiv:1509.06461 [cs], Sep

 [Wang al., Ziyu Wang, Tom Schaul, Matteo Hes- sel, Hado van Hasselt, Marc Lanctot, Nando Fre- itas.
 Dueling network architectures deep reinforce- ment learning.
 arXiv:1511.06581 [cs], Nov

 Unsupervised discovery common patterns is long standing task artiﬁcial intelligence shown Barlow (1989); Bengio, Courville, Vincent (2012).
 Recent deep learning approaches have offered major breakthroughs classiﬁcation multiple categories millions labeled examples (e.g. Krizhevsky (2009); Szegedy al.
 (2015); et al.
 (2016) many others).
 methods lot annotated data training order perform well.
 Unfortunately, labeling is inefﬁcient expensive progress, learning unlabeled data is desirable many complex tasks.
 same time, human knowledge learning is obtained unsupervised observations Grossberg (1994).
 goal work is show meaningful classes be learned minimal supervision.
 Given set objectness proposals, use activations foreground objects order learn deep features cluster available data learning embedding end-to-end manner.
 speciﬁcally, propose differentiable clustering approach learns better separability classes embedding.
 main idea is store potential cluster means weights neural network higher levels feature representation.
 allows be learned potential feature representation.
 differentiable clustering approach is integrated Deep Neural Networks (e.g. Szegedy al.
 (2015)) learn semantic classes end-to-end fashion manual class labeling.
 idea doing ‘end-to-end’ is gradient descent learn good weights clustering, change embedding allow better clustering use labels.
 see leads feature representation.
 results show different object categories emerge be retrieved test images seen network, resulting clusters meaningful categories, such cars, persons, bicycles.
 Conference Neural Information Processing Systems (NIPS Long Beach, CA, USA.
 work use given segmentation objectness masks, are candidate objects labels.
 be extended using independent objectness-based object generation mechanism Pathak et al.
 (2017); Faktor Irani (2014) using unsupervised motion segmentation videos structure motion Vijayanarasimhan et al.
 (2017).
 Related Work Unsupervised learning (Barlow (1989)) unsupervised deep learning (Bengio, Courville, Vincent (2012), Bengio (2012), Bengio others (2009)) are central topics Machine Learning.
 Unsupervised deep learning has been shown improve results classiﬁcation tasks Erhan al.
 (2010), given small datasets complicated high dimensional data such video.
 has been explored many representations including sequence sequence learning textual representations (Radford, Jozefowicz, Sutskever (2017), Ramachandran, Liu, Le (2016)).
 work focuses unsupervised deep learning discovering visual object categories.
 has been shown improve results such Doersch Zisserman (2017).
 Unsupervised discovery visual objects has been large topic interest computer vision (Sivic et al.
 (2005); Russell et al.
 (2006); Singh, Gupta, Efros (2012); Bach Jordan (2005); Kwak al.
 (2015); Pathak et al.
 (2017)).
 Building specialized, deep embeddings help computer vision tasks is popular approach such Agrawal, Carreira, Malik (2015).
 Transfer learning supervised tasks has proven be successful.
 Further, Agrawal, Carreira, Malik (2015) propose learning lower dimensional embedding unsupervised learning show improved performance transfered other supervised tasks.
 popularity building different embeddings, is little work investigating use clustering modify embedding end-to-end deep learning framework.
 Bottou Bengio (1995) investigate differentiable version kmeans algorithm examine convergence properties.
 work focuses learnable feature representations (instead ﬁxed ones Bottou Bengio (1995)) introduces memory units task.
 Unsupervised clustering unsupervised deep clustering is inspired Bottou Bengio (1995), consider differentiable clustering algorithms.
 differ approach features cluster change backpropogation.
 work, add kmeans-like loss is integrated end-to-end.
 idea is store potential cluster means weights network have be learned.
 proposed clustering is done building embedding.
 Given information potential object vs (binary labels), clustering differentiable way provides better embedding input data.
 show method be used meaningful semantic retrieval related objects.
 Embedding clustering train convolutional neural network (CNN) predict foreground background using oracle labels patches objects background images.
 Concurrently, learn clustering objects imposing constraints force embedding be partitioned multiple coherent clusters objects explicit labels different objects.
 experiments, use random initialization fully-connected layers (the last layers) add differentiable clustering module second last layer.
 Note cluster foreground labels background activations are interest clustering; classiﬁer predict foreground vs background high accuracy (above
 objective function is shown Equation
 Lk = mink[(xn − wk)2] (1) N(cid:88) K(cid:88) K(cid:88) (cid:26)0 N(cid:88) k=0 j=k n=0 LC = N K |countk − countj| countk = argmink[xn] = argmink[xn] = (3) (4) (5) equation, N is number samples, k is number deﬁned clusters, w is “weight” (theoretically mean cluster) k, x is activations connected layer classiﬁcation connected layer.
 is differentiable gradient descent algorithm is shown Equation
 δwk = w(cid:48) k − wk = k = s(xn, w) (2) weights loss L2 =(cid:80) s(xn, w) = argmink[xn] lr is learning rate.
 add L2 regularization j
 Furthermore, use custom clustering regularization loss LC enforces clusters are distributed deﬁned Equation Equation
 j w2 N(cid:88) (cid:26)lr(xn − n=1 ﬁnal loss be optimized is shown (Equation L = Lk + αrL2 + αcLC αr αc are hyperparameters are tuned training.
 method, use αr = αc =
 apply loss point is labeled object ignore background ones clustering.
 way learn foreground vs background learn clustering foreground activations.
 Optimization was performed ‘RMSProp’ optimizer, learning rate momentum decay factor  Experimental evaluation experiment toy example using CIFAR10 challenging example using Cityscapes.
 CIFAR10 dataset ﬁrst test proposed unsupervised clustering approach CIFAR10 Krizhevsky (2009) dataset.
 goal experiment is test clustering uncover separate categories simple toy problem class setting.
 Clusters Automobile Cluster Cluster Dog Table Unsupervised clustering results CIFAR10 discovery classes.
 Per cluster accuracy given classes test set (class labels are unknown training).
 selected example dog automobile classes label foreground.
 train network scratch based Network Network architecture (NiN) Lin, Chen, Yan (2013) scratch experiments.
 other classes CIFAR are considered experiment.
 attaching modiﬁed clustering objective function next last layer, attempt cluster dog automobile labels.
 see simple experimental results classes are clustered majority examples assigned.
 Table shows quantitative results test set.
 seen automobile classes dog examples are assigned separate clusters.
 Note cases, concepts classes dog automobile are unknown training algorithm are looking clustering evaluation.
 Classes Person Rider Car Truck Bus Train Motorcycle Bicycle Cluster Cluster Table Unsupervised clustering objects Cityscapes using method.
 table shows number examples assigned learned cluster K=2).
 Cityscapes dataset Cityscapes dataset (Cordts et al.
 (2016)) is large-scale dataset is used evaluating various classiﬁcation, detection, segmentation related autonomous driving.
 contains training, validation, test images, test set is provided purposes Cityscape competition only.
 work, used training set training validation set testing visualizing results (as test set has annotation results).
 Annotation is provided classes represent moving agents scene, such pedestrian, car, motorcycle, bicycle, bus, truck, rider, train.
 work use foreground/background labels intend discover semantic groups moving objects.
 Weakly supervised discovery classes experiment considered larger, real-life dataset, Cityscapes (Cordts al.
 (2016)), described see important class categories, e.g. moving objects scene be clustered meaningful classes.
 extract locations extents moving objects use weak supervision.
 Note classes are uneven car person dominate.
 show results clustering categories clusters rarity (such bicycle).
 results are presented validation set.
 report results terms number object patches extracted available test images.
 dataset, CNN architecture is based Inception architecture proposed Szegedy al.
 (2015).
 are small number examples, pre-train convolutional layers network.
 Results clustering classes moving objects clusters are presented Table Table learned embedding proposed approach baseline embedding.
 baseline embedding is calculated ﬁne-tuning same architecture same manner, loss (Equation uses same amount information input embedding.
 experiment, apply standard kmeans activations training is completed.
 see method provides clustering dominant classes dataset (car person).
 other hand, baseline embedding clusters class only, similar class case.
 have observed behavior different runs hypothesize is due sparse nature baseline embedding it’s activations.
 Figure visualizes retrieved clusters (color-coded) clustering clusters approach.
 see people (in blue) cars (in green) are retrieved.
 Bikes are more rare be mistaken, example cases portion patch contains part car, bicycle has person riding it.
 is exciting result, given is learned providing single class label training.
 Conclusions propose differentiable clustering objective learns separate classes learning build better embedding.
 key idea is be able learn clusters are stored weights, learn feature representation clustering data.
 results show proposed approach is useful extracting related objects.
 method Baseline Cluster Cluster Cluster Cluster Cluster Cluster Classes Person Rider Car Truck Bus Train Motorcycle Bicycle Table Unsupervised clustering Cityscapes dataset clusters.
 table shows number examples assigned learned cluster.
 method (left) baseline (right).
 method results accuracy.
 Figure Visualization clusters learned method (for K=3).
 ﬁgure, green class is responsible retrieving blue persons, red bicycles.
 see cars persons are discovered bicycles, rarer class, be confused person visible car background.
 References Agrawal, P.; Carreira, J.; Malik, J.

 Learning see moving.
 CVPR.
 Bach, F.
 R., Jordan, M.
 I.

 Learning spectral clustering.
 NIPS.
 Barlow, H.

 Unsupervised learning.
 Neural computation.
 Bengio, Y., al.

 Learning deep architectures ai.
 Foundations trends R(cid:13) Machine Learning
 Bengio, Y.; Courville, A.
 C.; Vincent, P.

 Unsupervised feature learning deep learning: A review new perspectives.
 CoRR, abs/1206.5538.
 Bengio, Y.

 Deep learning representations unsupervised transfer learning.
 Proceedings ICML Workshop Unsupervised Transfer Learning,
 Bottou, L., Bengio, Y.

 Convergence properties k-means algorithms.
 Advances neural information processing systems,
 Cordts, M.; Omran, M.; Ramos, S.; Rehfeld, T.; Enzweiler, M.; Benenson, R.; Franke, U.; Roth, S.; Schiele, B.

 cityscapes dataset semantic urban scene understanding.
 Proceedings IEEE Conference Computer Vision Pattern Recognition,
 Doersch, C., Zisserman, A.
 arXiv:1708.07860.

 Multi-task self-supervised visual learning.
 arXiv preprint Erhan, D.; Bengio, Y.; Courville, A.; Manzagol, P.-A.; Vincent, P.; Bengio, S.

 does unsupervised pre-training help deep learning?
 Journal Machine Learning Research
 Faktor, A., Irani, M.

 Video segmentation non-local consensus voting.
 BMVC.
 Grossberg, S.

 3-d vision ﬁgure-ground separation visual cortex.
 Perception Psychophysics.
 He, K.; Zhang, X.; Ren, S.; Sun, J.

 Deep residual learning image recognition.
 CVPR.
 Krizhevsky, A.

 Learning multiple layers features tiny images.
 Kwak, S.; Cho, M.; Laptev, I.; Ponce2, J.; Schmid, C.

 Unsupervised object discovery tracking video collections.
 ICCV.
 Lin, M.; Chen, Q.; Yan, S.

 Network network.
 arXiv preprint arXiv:1312.4400.
 Pathak, D.; Girshick, R.; Dollar, P.; Darrell, T.; Hariharan, B.

 Learning features watching objects move.
 CVPR.
 Radford, A.; Jozefowicz, R.; Sutskever, I.

 Learning generate reviews discovering sentiment.
 arXiv preprint arXiv:1704.01444.
 Ramachandran, P.; Liu, P.
 J.; Le, Q.
 V.

 Unsupervised pretraining sequence sequence learning.
 arXiv preprint arXiv:1611.02683.
 Russell, B.
 C.; Efros, A.
 A.; Sivic, J.; Freeman, W.
 T.; Zisserman, A.

 Using multiple segmentations discover objects extent image collections.
 CVPR.
 Singh, S.; Gupta, A.; Efros, A.
 A.

 Unsupervised discovery mid-level discriminative patches.
 ECCV.
 Sivic, J.; Russell, B.
 C.; Efros, A.
 A.; Zisserman, A.; Freeman, W.
 T.

 Discovering objects location images.
 ICCV.
 Szegedy, C.; Liu, W.; Jia, Y.; Sermanet, P.; Reed, S.; Anguelov, D.; Erhan, D.; Vanhoucke, V.; Rabinovich, A.

 Going deeper convolutions.
 CVPR.
 Vijayanarasimhan, S.; Ricco, S.; Schmid, C.; Sukthankar, R.; Fragkiadaki, K.

 Sfm-net: Learning structure motion video.
 Emerging Trends.
 Trafﬁc congestion urban areas has become signiﬁcant issue recent years.
 trafﬁc congestion, people United States traveled extra hours purchased extra gallons fuel
 extra time fuel cost were valued dollars [1].
 Congestion is caused accidents, roadwork, special events, adverse weather is called non- recurring congestion [2].
 Compared recurring congestion happens particular times day, weekday peak hours, NRC makes people unprepared has signiﬁcant impact urban mobility.
 example, US, NRC accounts two-thirds overall trafﬁc delay urban areas population [3].
 Driven concepts Internet Things (IoT) smart cities, various trafﬁc sensors have been deployed urban environments large scale.
 number tech- niques have been developed knowledge discovery data mining integrating utilizing sensor data.
 Trafﬁc data is available using static sensors (e.g., loop detectors, radars, cameras, etc.) mobile sensors (e.g., in-vehicle GPS other crowdsensing techniques use mobile phones).
 fast development sensor techniques enables possibility in-depth analysis congestion causes.
 problem ﬁnding anomalous trafﬁc patterns is called trafﬁc anomaly detection.
 Understanding analyzing trafﬁc anomalies, congestion patterns, is critical help- ing city planners make better decisions optimize urban transportation systems reduce congestion conditions.
 identify faulty sensors, many data-driven model-driven methods have been proposed incorporate historical real- time data [4], [5], [6], [7].
 researchers [8], [9], [10], [11] have worked detecting trafﬁc events such car accidents congestion using videos, trafﬁc, vehicular ad hoc data.
 are researchers have explored root causes anomalous trafﬁc [12], [13], [14], [15], [16], [17].
 Most existing work focuses road section small network region identify trafﬁc congestion, few studies explore non-recurring congestion causes large urban area.
 Recently, deep learning techniques have gained great success many research ﬁelds (including image processing, speech recognition, bioinformatics, etc.), provide great opportunity solve NRC identiﬁcation classiﬁcation problem.
 are many open problems: (1) using feature vectors represent trafﬁc conditions loses spatial information road segments, (2) using small unbalanced dataset (trafﬁc data event labels is limited) train neural networks downgrades performance, proper data augmentation mechanism is needed balance training data different class labels, (3) building deep neural networks model trafﬁc conditions recurring non-recurring congestion.
 Contributions.
 paper, propose DxNAT, deep neural network model identify non-recurring trafﬁc conges- tion explain causes.
 best work is ﬁrst efforts utilize deep learning techniques study trafﬁc congestion patterns explain non- recurring congestion using events.
 main contributions research are summarized follows: present algorithm convert trafﬁc data introduce crossover operator data augmentation • A convolutional neural network (CNN) is proposed identify non-recurring trafﬁc anomalies are caused events.
 create scenarios evaluate performance Trafﬁc Message Channel (TMC) format images method training class balancing.
 proposed model using real-world data events types (football games, hockey games, trafﬁc incidents).
 Paper Organization.
 remainder paper is orga- nized follows: Section II compares work related work; Section III presents dataset motivating example explores impact football games trafﬁc conges- tion; Section IV formulates problem; Section V presents solution approach; Section VI evaluates performance model; Section VII gives concluding remarks; II.
 RELATED WORK CHALLENGES section presents overview related work trafﬁc anomaly detection, includes studies faulty trafﬁc sensor detection, trafﬁc event detection, congestion cause indication.
 key research challenges con- tributions detecting NRC are discussed end.
 Faulty Trafﬁc Sensor Detection.
 Robinson al.
 [4] proposed approach used data inductive loop detectors estimate travel time road segments.
 ap- proach included data cleaning method clean collected trafﬁc data.
 Lu al.
 [5] reviewed previous work faulty inductive loops data analysis.
 Widhalm al.
 presented trafﬁc anomaly detection method used Floating-Car Data (FCD) independent information source.
 developed non-linear regression model ﬁt trafﬁc sensor data FCD data.
 Zygouras al.
 [6] proposed method comparing correlations nearby sensors identify faulty sensor readings.
 system was based MapReduce paradigm work crowdsourcing data.
 Ghafouri al.
 presented faulty trafﬁc sensor detection model based Gaussian Processes.
 Particularly, provided effective approach computing parameters detectors minimize loss due false-positive false-negative errors.
 Event Detection Using Trafﬁc Data.
 Monitoring trafﬁc ﬂow intersections is important trafﬁc event detection research.
 Kamijo al.
 [8] developed algorithm based spatiotemporal Markov random ﬁeld (MRF) processing trafﬁc images tracking vehicles intersections.
 Using timeseries observed behaviors vehicles, hidden Markov model accident detection is proposed.
 Veeraraghavan al.
 presented multiple cue-based approach combined switching Kalman ﬁlter detecting vehicle events such turning, stopping slow moving.
 Terroso-Senz et al.
 presented event-driven architecture (EDA) used vehicular ad hoc network external data sources weather conditions detect trafﬁc congestions.
 Yang al.
 [10] proposed coupled Bayesian RPCA (BRPCA) model detecting trafﬁc events used multiple trafﬁc data streams.
 Kong al.
 [11] proposed LoTAD explore anomalous regions long-term poor trafﬁc situations.
 model trafﬁc condition, crowd-sourced bus data is grouped spatiotemporal segments.
 segments high anomaly indexes were combined get anomalous regions.
 Wang al.
 [20] proposed two-stage solution detect road trafﬁc anomalies: (1) Collaborative Path Inference (CPI) model performs path inference incorporating static dynamic features Conditional Random Field (CRF); (2) road Anomaly Test (RAT) model calculates anomalous degree road segment.
 Congestion Cause Indication.
 Liu al.
 [12] studied known (planned) unknown (unplanned) events behaving daily network trafﬁcs anomalies, pro- posed algorithms construct outlier causality trees based temporal spatial properties detected outliers.
 Xu et al.
 [13] introduced approach identify urban congestion patterns based data cube.
 proposed multi- dimensional data analysis method data cube.
 Chow et al.
 presented automatic number plate recognition technol- ogy analyze urban trafﬁc congestions introduced linear regression model indicate causes congestions.
 Kwoczek al.
 [16] proposed Artiﬁcial Neural Network (ANN) based classiﬁer detect road segments affected planned events.
 Mallah al.
 evaluated performance machine learning techniques classifying congestions different causes.
 A.
 Research Challenge Representing Heterogeneous Trafﬁc Data Event Labels Using Multi-Dimensional Images A feature vector is n-dimensional vector is popular representation data objects.
 numerical values, feature vectors represent texts images.
 However, feature vectors be best solutions representing trafﬁc corresponding event labels.
 Trafﬁc conditions are affected different inﬂu- encing factors [21], such incidents, sports games, road work, weather, etc.
 events physical locations are used labels.
 feature vectors have ﬁxed length, is practical encode labels speciﬁc ﬁxed length feature vector.
 importantly, pattern recognition machine learning, features matter most.
 converting image feature convert two-dimensional pixels one- dimensional vector, ﬁrst take histogram image construct feature vector has several comparison metrics, such mean, standard deviation, etc.
 methods lose relative spatial information original images.
 contrast feature vectors, images preserve original spatial relations locating points different pixels integrate multiple data sources adding layers.
 Kwoczek al.
 [16] showed factor representation integrates multiple features event weather different layers data cube.
 However, men- tioned idea possible future did present concrete solution it.
 Ma al.
 [22] proposed CNN- based approach trafﬁc prediction.
 represented trafﬁc speed time using time-space matrix.
 problem time-space matrix is information segments is lost, is important detecting trafﬁc patterns nearby roads show similar related patterns.
 Additionally, model considered trafﬁc data, are many other factors affecting future trafﬁc conditions.
 representing heterogeneous trafﬁc spatial corresponding event labels using images remains research gap.
 key differences proposed approach existing ones is are trying visualize wide area sensor data distribution Trafﬁc Condition Images (TCIs), use CNN other deep learning techniques analyses.
 B.
 Research Challenge Training Deep Learning Models Using Limited Data Instances performance deep learning techniques relies quality training data instances.
 However, collected urban data provide data training data sampling rates.
 proposed model ﬁrst converts trafﬁc data Trafﬁc Condition Images (TCIs) trains different models using images.
 trafﬁc data obtain HERE [23] is requested minute.
 day consists minutes, have trafﬁc images, are few effective training purposes.
 availability issue data instances worse considering is limited label data.
 remains research challenge getting more training data using existing data.
 Traditional ways solving problem are: (1) waiting collecting enough training data is collected, (2) labeling data, (3) adding data sources, e.g., collecting more data social media.
 solution uses idea crossover genetic algorithm.
 assume trafﬁc conditions short time range are associated same events.
 apply crossover operator TCIs generate TCIs same event label.
 C.
 Research Challenge Modeling Trafﬁc Patterns Non- Recurring Events existing work trafﬁc event detection focused analyzing trafﬁc videos trafﬁc sensor data streams detect events are related trafﬁc, such vehicle stopping, car accidents, road congestion.
 few studies explored contextual non-recurrent events impacts are associated certain trafﬁc patterns.
 has been explosion research using deep neural networks.
 still, few have applied deep learning studying trafﬁc patterns.
 Deep learning techniques have gained great success research ﬁelds image processing, speech recognition, bioinformatics, etc.
 Convolutional neural networks are similar original neural networks convo- lutional layers are added front model learn patterns original images.
 trafﬁc label data be converted images, CNN be employed learn labeled patterns.
 is research gap develop effective efﬁcient deep learning network identifying classifying trafﬁc patterns non-recurring events.
 formulate problem identifying speciﬁc trafﬁc patterns associated events Section V-A, present details proposed approach uses con- volutional neural networks Section V-C.
 Trafﬁc Format TMC Source HERE [23] API Sports Game JSON ESPN, hockey- reference.com Update Every Minute Manually Games Size Range GB Present TABLE I.
 DATA Accident JSON Fire Department Manually MB III.
 DATA AND MOTIVATING EXAMPLE section ﬁrst introduces datasets have integrated system, describes motivating example use collected datasets study impact football games trafﬁc congestion city.
 A.
 Datasets October have been collecting storing real-time trafﬁc data HERE API [23] major roads Nashville area.
 order explore impact contextual events urban mobility, collect data incidents sports games.
 cooperate Nashville Fire Department [24] access incident datasets, collect information sports games web.
 illustrated Table I, details datasets have integrated system are follows: trafﬁc dataset provides real-time trafﬁc infor- mation road segments, such speed limit, real-time speed, jam factor (JF), etc.
 dataset contains historical trafﬁc data TMC road segments Nashville area.
 sports game dataset contains operation informa- tion sports games, such game type, start end time, attendance, location, etc.
 incident dataset provides detailed records incidents responding vehicles.
 incident, provides coordinates, time, vehicle arrival departure time, weather condition, etc.
 type, alert incident B.
 Motivating Example motivation research comes brief experi- ment, study impact football games trafﬁc congestion city.
 studied period Sept.
 Jan.
 were football games (as listed Table II) Nissan Stadium downtown Nashville.
 time collected data related trafﬁc (speed limit, real- time speed) football games (date, start time, duration, location)1.
 indicate congestion condition, HERE [23] provides jam factor (JF) ranges dataset is larger.
 However, study are focusing months Fig.

 hours, (c) hours hour, (d) hour hour.
 Impact football games trafﬁc congestion one-hour time windows football games: (a) hours hours, (b) hours TABLE II.
 Date Stadium Attendance Duration THE INFORMATION THE EIGHT FOOTBALL GAMES STUDIED IN THE MOTIVATING EXAMPLE Start Time (CST) PM PM PM PM PM PM PM PM Nissan Stadium Nissan Stadium Nissan Stadium Nissan Stadium Nissan Stadium Nissan Stadium Nissan Stadium Nissan Stadium TMC road segment.
 study compare JF days is football game days is football game, one-hour time window games: [−4,−3], [−3,−2], [−2,−1], relative time game was scheduled2.
 shown Figure results JF difference road segments different time windows are visualized using heat maps.
 ﬁgure, colors ranging green red are used indicate small big JF differences.
 results show impact football games trafﬁc congestion begins increase hours games.
 have observed pattern several game events city.
 hypothesis is event has unique pattern learn pattern time use identify current congestion pattern matches expected pattern.
 pattern does match classify anomaly.
 Most football games are scheduled PM.
 IV.
 PROBLEM FORMULATION section, ﬁrst provide formal deﬁnition problem describe assumptions solving problem.
 A.
 Deﬁnition goal research is model trafﬁc patterns locations non-recurrent events use model identify non-recurring congestion causes.
 trafﬁc pattern use refers spatiotemporal relations trafﬁc speeds many road segments area, be modeled detected classiﬁer.
 deﬁnitions relative notions be found Table III.
 inputs system are data trafﬁc events.
 trafﬁc data collected HERE API is deﬁned using Trafﬁc Message Channel Location Code [25] format (a standard encoding geographic road segments used study are deﬁned using same TMC location codes.
 Event data is categorized labels training validating purposes.
 labels used are follows: Event-related: event indicator levent, time window relative Time-related: time day tday, weekday tweek key differences approach existing ones is are trying visualize wide area sensor data distribution Trafﬁc Condition Images (TCI), use CNN other deep learning techniques analyze model spatiotemporal relations.
 TCI is Iw Iw pixels image.
 pixel p corresponds road segment real world grayscale value pixel represents real-time trafﬁc speed vr road segment r.
 event tevent, Formulation Non-recurring Congestion Identiﬁca- tion Problem.
 Given set trafﬁc data St contains speed limit real-time speed set road segments Sr speciﬁc time tday weekday tweek, set event labels Se, model determine levent indicates given trafﬁc data contains congestion caused subset (cid:48) e event set Se. levent is model TABLE III.
 SYMBOLS USED IN THE FORMULATED PROBLEM tday tweek tevent levent Se Sr St T M Ckey T CI Iw vr T H timestamp time day seconds weekdays encoded using integers (e.g., Sunday, Monday, etc.) time windows relative events 1-hour time window events) indicator current time is time window occurrence event set events city road segment set road segments deﬁned TMC location codes set trafﬁc data contains speed limit real-time speed set road segments Sr string representing road segment Sr trafﬁc Condition Image, gray-scale image represent trafﬁc conditions bounding box width TCI pixel TCI.
 value shows normalized trafﬁc speed road segment real-time trafﬁc speed (miles hour) road segment r threshold classiﬁer determine input trafﬁc data contains recur- ring non-recurring congestion time window tevent relative events tevent be used estimate event occurrence time).
 Figure illustrates example problem.
 Given raw model identify trafﬁc data speciﬁc possible non-recurring congestion provide estimation event occurrence time.
 B.
 Assumptions event information studied area period.
 direction is same segment.
 following assumptions are made design formulate non-recurring trafﬁc congestion identiﬁcation system: assume availability trafﬁc speed data • assume trafﬁc condition short road segment assume event happening urban envi- ronment affect trafﬁc conditions nearby road segments.
 • assume is robust correlation road segments affected event, patterns be identiﬁed image classiﬁcation techniques deep learning.
 V.
 OUR APPROACH order identify speciﬁc trafﬁc patterns associated non-recurring events deﬁned Section IV, present details proposed approach section.
 overall workﬂow system is shown Figure
 are key components system: (1) algorithm converts raw trafﬁc data images, (2) convolutional neural network classiﬁes trafﬁc condition images, (3) ROC analysis tunes classiﬁcation threshold reduce false positive false negative rates.
 A.
 Feature Extraction Mapping Trafﬁc Data Images Research challenge describes problem feature vectors have limitations representing urban data.
 solve issue, ﬁrst step is convert collected trafﬁc data images.
 have been collecting real-time trafﬁc data Nashville area Trafﬁc API [23] Oct.

 trafﬁc data is encoded TMC location codes.
 TMC database is open public, present algorithm convert trafﬁc data speciﬁc time T coded TMC locations trafﬁc images.
 order project trafﬁc conditions pixels images, ﬁrst initialize gridded map re-sample road segments deﬁned TMC location codes grids.
 algorithm’s input, output, step details are follows (for set road segments Sr, step run step run timestamp): Input: Trafﬁc dataset St, road segment set Sr, times- tamp T
 raw trafﬁc data road segments Sr timestamp T is queried Trafﬁc dataset St database.
 A Trafﬁc Condition Image (TCI).
 Step Map grid initialization.
 map area containing road segment set Sr is divided map grid squares.
 length square is meters, grid cell covers square meters map.
 Step Road segment path re-sampling smoothing.
 points road segments are re-sampled centers grid cells points are covered cell.
 distance original points is large are blank cells cells projected points, points be interpolated ﬁll blank cells.
 step, get two-dimensional array, cell contains list TMC keys T M Ckey corresponding points road segments.
 Step Trafﬁc data projection images.
 two- dimensional array acts projecting table original road segments image pixels.
 ﬁll images trafﬁc data querying trafﬁc data using segment keys T M Ckey timestamp T
 use following equation convert trafﬁc speed pixel value: (cid:40) (80−vt p = s)∗255 (1) p denotes pixel value (0-255) vt real-time speed (miles hour).
 s denotes getting initial projected TCI, simple image processing techniques are used resize TCI desired size (Iw Iw).
 Fig.

 Overall workﬂow non-recurring congestion identiﬁcation system B.
 Data Augmentation Crossover Operations C.
 Classifying Non-Recurring Congestion TCI is image representation trafﬁc speeds road segments.
 trafﬁc data is collected minute, data augmentation get (the number minutes day) TCIs day, is training deep learning image processing models.
 address research challenge lack enough trafﬁc data labels), create crossover operator generate more labeled trafﬁc condition images training deep learning models.
 Crossover is genetic operator genetic algorithms vary chromosomes individuals generation another.
 are motivated similar idea present crossover operator system: Getting TCI candidates.
 given timestamp T getting TCI T generate n TCIs time range [T − t, T ] (t denotes time length extend T e.g., minutes).
 TCIs are same image size, differ pixel values correspond trafﬁc speeds different times.
 Generating new labeled TCIs. looping pixels TCI, pixel row is probability pm values mutate randomly select new row same corresponding pixels other TCI candidates.
 second step, get new TCI.
 assume trafﬁc conditions small time range are caused same events, give new TCI same event label.
 crossover operator be executed many times generate many new data instances.
 crossover, have more labeled data, reduce probability over-ﬁtting training phase.
 previous section, have described algorithm converts raw trafﬁc data TCI.
 inputs contain makes sense apply convolutional neural networks.
 section introduces CNN model classify TCI using event labels.
 CNN is class deep feed-forward artiﬁcial neural networks have shown great success image analysis tasks.
 apply CNN problem assigns event congestion labels TCI.
 CNN.
 architecture proposed CNN is shown Figure
 Generally, model consists stack convolu- tional, fully-connected neural, dropout max-pooling layers.
 layers are used model prevent ﬁtting.
 Max pooling layers are used spatial down-sampling.
 middle CNN, feature vectors represent time day day week correspond TCI are concatenated be input CNN help make better decisions.
 Details layer conﬁguration, such dimension, activation function, dropout rate, be found Figure
 use one-hot encoding vectors are categorical format (i.e., dimensional vector is all-zeros index corresponding class sample), categorical cross entropy is used loss function train model.
 One-hot Encoding.
 proposed CNN model, input feature vectors are time day weekday, output labels are (1) congestion input TCI is recurring non-recurring relative time windows TCI belongs is non-recurring congestion.
 use one-hot encoding convert input output vectors binary class matrix.
 input matrix has classes, classes correspond hours classes correspond days week.
 illustrated Figure output matrix has several classes, ﬁrst class represents D.
 Tuning Model Sensitivity ROC Analysis approach uses operating characteristic (ROC) analysis tune sensitivity CNN classiﬁer.
 ROC is statistical plot illustrates diagnostic ability classiﬁer system [26].
 ROC curve is fundamental tool diagnostic test evaluation.
 ROC curve, true positive rate (TPR) is plotted function false positive rate (FPR).
 machine learning, TPR represents sensitivity, recall probability detection, FPR represents fall-out false alarm [27].
 choosing point curve, corresponding classiﬁcation threshold be decided.
 model, non-recurring congestion is considered positive output recurring congestion is negative output.
 use ROC analysis tune classiﬁcation threshold decides trafﬁc congestion input trafﬁc data is recurring congestion non-recurring congestion.
 choose thresholds range corre- sponding FPR TPR training dataset are plotted (an example is shown Figure
 curve’s nearest point point (FPR: TPR: be selected.
 Fig.

 proposed convolutional neural network (CNN).
 Fig.

 example one-hot encoding format.
 Event labels are encoded using classes: (1) ﬁrst digit represents trafﬁc condition belongs recurring congestion non-recurring congestion, (2) it’s non-recurring congestion, next digits represent time windows events.
 trafﬁc condition belongs recurring congestion non-recurring congestion, next classes represent time windows events.
 ﬁrst class is tunable determines input trafﬁc condition contains non-recurring congestion not.
 value ﬁrst class output is higher predeﬁned threshold T H, classiﬁer output input data does contain non-recurring congestion values other classes are higher ﬁrst class).
 details tuning steps are presented following section.
 Fig.

 prediction threshold.
 operating characteristics (ROC) curve analysis VI.
 EXPERIMENTS section, evaluate proposed deep neural net- work’s ability identify non-recurring trafﬁc anomalies using real-world data event types: football games, hockey games, trafﬁc incidents.
 Keras [28] Python deep learning library is used construct models TensorFlow [29] is selected tensor manipulation library.
 A.
 Scenarios illustrated Figure create scenarios test performance proposed model.
 scenario, consider event categories training validating proposed model: • Football Games.
 Between Oct.
 Jan.
 were NFL football games Nissan Stadium Nashville.
 trafﬁc data bounding box (latitude TABLE IV.
 EXPERIMENT RESULTS IN SCENARIO TRAINING DXNAT FPR FOR IDENTIFYING NRC CAUSED BY FOOTBALL GAMES Accuracy FNR Random Forest DxNAT TABLE V.
 EXPERIMENT RESULTS IN SCENARIO TRAINING DXNAT FOR IDENTIFYING NRC CAUSED BY HOCKEY GAMES DxNAT Accuracy FNR FPR people, shows great impact causing non-recurring trafﬁc congestion.
 ﬁrst scenario, use trafﬁc data collected 1-minute intervals Oct.
 Jan.

 Trafﬁc data non-game days game days are used training dataset, non-game day game day are used validating dataset.
 comparison traditional machine learning techniques, build random forest model uses same training validating dataset.
 random forests cannot use images input, ﬁrst convert trafﬁc condition images one-dimensional vectors, concentrate trafﬁc vectors time day day week vectors, use combined feature vector input random forest model.
 accuracy, false positive rate (FPR) false negative rate (FNR) model random forest model are shown Table VI.
 model outperforms random forest model higher accuracy lower FPR FNR.
 C.
 Experiment Identifying NRC Caused Hockey Games Compared NFL football games, NHL hockey games Nashville have less attendance (NHL v.s. NFL
 assume NHL hockey game has less impact trafﬁc conditions be difﬁcult detect NRC related hockey games.
 Scenario use trafﬁc hockey games data Oct.
 Nov.
 training dataset, data Dec.
 (game day) Dec.
 (non-game day) validating dataset.
 accuracy, FPR FNR results are shown Table V.
 Compared results Scenario model has lower accuracy higher FNR.
 assumption is validated NRC associated hockey games less attendance is harder be detected.
 D.
 Experiment Identifying NRC Caused Trafﬁc Accidents scenario explore model’s ability detect NRC caused road accidents.
 selected block area, were trafﬁc accidents different days Oct.
 Dec.

 use days accidents training dataset day accident validating dataset.
 DxNet model archives accuracy FPR FNR Fig.

 Experimental scenarios coverage areas: (1) detecting NRC caused football games, (2) detecting NRC caused hockey games, (3) detecting NRC caused trafﬁc accidents.
 range: [36.1120, longitude range: [-86.8475, is used.
 • Hockey Games.
 Between Oct.
 Jan.
 were NHL hockey games Bridgestone Arena Nashville.
 trafﬁc data bounding box (latitude range: longitude range: -86.7660]) is used.
 • Trafﬁc Accidents.
 Between Oct.
 Mar.
 selected block area.
 trafﬁc data bounding box (latitude range: longitude range: [-86.8126, is used.
 were trafﬁc accidents trafﬁc event datasets are divided subsets training validation.
 event time information is encoded using one-hot encoding described Section V.
 example output model is shown Figure
 Particularly, following metrics are used deﬁne output is positive negative: output is considered be positive determines input TCI contains non- recurring congestion, (2) output is negative determines input TCI contains recurring congestion.
 B.
 Experiment Identifying NRC Caused Football Games shown motivating example Section III-B, selected NFL football games have average attendance TABLE VI.
 EXPERIMENT RESULTS IN SCENARIO TRAINING DXNAT FOR IDENTIFYING NRC CAUSED BY TRAFFIC ACCIDENTS DxNAT Accuracy FNR FPR TIPS grant Vanderbilt University.
 acknowledge support provided partners Nashville Metropoli- tan Transport Authority.
 TABLE VII.
 SUMMARY OF ARCHITECTURAL DECISIONS REFERENCES Challenge Representing Heterogeneous Trafﬁc Data Event Labels Deep Training Learning Models Using Limited Data Instances Modeling Patterns Recurring Events Trafﬁc Non- Approach Using Multi- dimensional Images Section II-A Developing crossover original data operator Employing convolutional neural networks II-B II-C VII.
 CONCLUSION includes: paper, propose deep neural network model identify non-recurring trafﬁc congestion explain causes.
 best knowledge, work is ﬁrst efforts utilize deep learning techniques study trafﬁc congestion patterns explain non-recurring congestion using events.
 main contributions are listed Table VII.
 present algorithm convert trafﬁc data Trafﬁc Message Channel (TMC) format images, crossover operator data augmentation mechanism class balancing.
 convolutional neural network is proposed identify non-recurring trafﬁc anomalies are caused events.
 evaluate proposed model using types events (football games, hockey games, trafﬁc incidents).
 future work extend current proposed model • Integrating contextual features.
 Existing work usu- ally focuses trafﬁc data, are many types urban data available help identify trafﬁc patterns, real-time bus travel time, speed, weather.
 Be- sides events, trafﬁc conditions are affected multiple environmental factors.
 current work considers time day day week environmental training features.
 next step, include various features weather conditions (such humidity, nearest storm distance, visibility, etc.).
 • Identifying sizes block areas length time win- dows.
 event type (e.g., sports games, size impacting block areas number impacting time windows experimental scenarios are selected
 mechanism is needed au- select best impacting area size time windows event type.
 ACKNOWLEDGMENTS work is supported National Science Foundation award numbers CNS-1528799 CNS-1647015 [1] D.
 Schrank, B.
 Eisele, T.
 Lomax, J.
 Bak, “2015 urban mobility scorecard,”
 [2] R.
 W.
 Hall, “Non-recurrent congestion: How big is problem?
 are traveler information systems solution?” Transportation Research Part C: Emerging Technologies, vol.
 no.
 pp.

 [3] S.
 Lockwood, “The 21st century operation oriented state dots, nchrp project Transportation research board, American Association State Highway Transportation Ofﬁcials, Washington, DC,
 [4] S.
 P.
 Robinson, “The development application urban link travel time model using data derived inductive loop detectors,” Ph.D. dissertation, University London,
 X.-Y.
 Lu, P.
 Varaiya, R.
 Horowitz, J.
 Palen, “Faulty loop data analysis/correction loop fault detection,” World Congress Intelligent Transport Systems ITS America’s Annual Meeting,
 [6] N.
 Zygouras, N.
 Panagiotou, N.
 Zacheilas, I.
 Boutsis, V.
 Kalogeraki, I.
 Katakis, D.
 Gunopulos, “Towards detection faulty trafﬁc sensors real-time.” MUD@ ICML, pp.

 [7] A.
 Ghafouri, A.
 Laszka, A.
 Dubey, X.
 Koutsoukos, “Optimal detection faulty trafﬁc sensors used route planning,” Proceedings International Workshop Science Smart City Operations Platforms Engineering.
 pp.

 [8] S.
 Kamijo, Y.
 Matsushita, K.
 Ikeuchi, M.
 Sakauchi, “Trafﬁc monitoring accident detection intersections,” IEEE transactions Intelligent transportation systems, vol.
 no.
 pp.

 [9] H.
 Veeraraghavan, P.
 Schrater, N.
 Papanikolopoulos, “Switching kalman ﬁlter-based approach tracking event detection trafﬁc intersections,” Intelligent Control,
 Proceedings IEEE International Symposium on, Mediterrean Conference Control Automation.
 IEEE, pp.

 [10] S.
 Yang, K.
 Kalpakis, A.
 Biem, “Detecting road trafﬁc events coupling multiple timeseries nonparametric bayesian method,” IEEE Transactions Intelligent Transportation Systems, vol.
 no.
 pp.

 [11] X.
 Kong, X.
 Song, F.
 Xia, H.
 Guo, J.
 Wang, A.
 Tolba, “Lotad: long- term trafﬁc detection based crowdsourced bus trajectory data,” World Wide Web, pp.

 [12] W.
 Liu, Y.
 Zheng, S.
 Chawla, J.
 Yuan, X.
 Xing, “Discovering spatio- temporal causal interactions trafﬁc data streams,” Proceedings ACM SIGKDD international conference Knowledge discovery data mining.
 pp.

 [13] L.
 Xu, Y.
 Yue, Q.
 Li, “Identifying urban trafﬁc congestion pattern historical ﬂoating car data,” Procedia-Social Behavioral Sciences, vol.
 pp.

 [14] A.
 H.
 Chow, A.
 Santacreu, I.
 Tsapakis, G.
 Tanasaranond, T.
 Cheng, “Empirical assessment urban trafﬁc congestion,” Journal advanced transportation, vol.
 no.
 pp.

 [15] S.
 Kwoczek, S.
 Di Martino, W.
 Nejdl, “Predicting visualizing trafﬁc congestion presence planned special events,” Journal Visual Languages Computing, vol.
 no.
 pp.

 [16] ——, “Stuck stadium?
 approach identify road seg- ments affected planned special events,” Intelligent Transportation Systems (ITSC), IEEE International Conference on.
 IEEE, pp.

 [17] R.
 Al Mallah, A.
 Quintero, B.
 Farooq, “Distributed classiﬁcation urban congestion using vanet,” IEEE Transactions Intelligent Transportation Systems,
 [23] [24] “Here trafﬁc api,” https://developer.here.com/rest-apis/documentation/ trafﬁc/topics v6.1/ﬂow.html.

 M.
 G.
 Nashville D.
 County, “Nashville ﬁre department,” [Online].
 Available: http://www.nashville.gov/Fire-Department.aspx [Online; accessed 30-September-2017].
 [25] O.
 Wiki, “Tmc/location code list/location types,” 12-September-2017].
 [Online].
 Available: accessed openstreetmap.org/wiki/TMC/Location Code List/Location Types J.
 A.
 Hanley B.
 J.
 McNeil, “The meaning use area receiver operating characteristic (roc) curve.” Radiology, vol.
 no.
 pp.

 [Online; http://wiki.
 [18] P.
 Widhalm, H.
 Koller, W.
 Ponweiser, “Identifying faulty trafﬁc detectors ﬂoating car data,” Integrated Sustainable Trans- portation System (FISTS), IEEE Forum on.
 IEEE, pp.

 [19] F.
 Terroso-S´aenz, M.
 Vald´es-Vela, C.
 Sotomayor-Mart´ınez, R.
 Toledo- Moreo, A.
 F.
 G´omez-Skarmeta, “A cooperative approach trafﬁc congestion detection complex event processing vanet,” IEEE Transactions Intelligent Transportation Systems, vol.
 no.
 pp.

 [21] [20] H.
 Wang, H.
 Wen, F.
 Yi, H.
 Zhu, L.
 Sun, “Road trafﬁc anomaly detection collaborative path inference gps snippets,” Sensors, vol.
 no.
 p.

 J.
 Kwon, M.
 Mauch, P.
 Varaiya, “Components congestion: Delay incidents, special events, lane closures, weather, potential ramp metering gain, excess demand,” Transportation Research Record: Journal Transportation Research Board, no.
 pp.

 [22] X.
 Ma, Z.
 Dai, Z.
 He, J.
 Ma, Y.
 Wang, Y.
 Wang, “Learning trafﬁc images: deep convolutional neural network large-scale transportation network speed prediction,” Sensors, vol.
 no.
 p.

 [26] [27] MathWorks, “Detector roc [On- curves,” line].
 Available: https://www.mathworks.com/help/phased/examples/ detector-performance-analysis-using-roc-curves.html using 30-September-2017].
 performance accessed analysis [Online; [28] Keras, “Keras: python deep learning library,” [Online; accessed 30-September-2017].
 [Online].
 Available: https://keras.io/ [29] M.
 Abadi, A.
 Agarwal, P.
 Barham, E.
 Brevdo, Z.
 Chen, C.
 Citro, G.
 S.
 Corrado, A.
 Davis, J.
 Dean, M.
 Devin al., “Tensorﬂow: Large-scale machine learning heterogeneous distributed systems,” arXiv preprint arXiv:1603.04467,
 Matrix completion (e.g., [4,5,13]) is fundamental problem signal processing machine learning, studies recovery low-rank matrix observation subset entries.
 has attracted lot attention researchers practitioners are various motivating real-world applications including recommender systems Netﬂix challenge (see recent overview [6]).
 popular approach matrix completion is ﬁnd matrix minimal rank satisfying observation constraints.
 Due non-convexity rank function, popular approaches are convex relaxation (see, e.g., [8]) nuclear norm minimization.
 is rich literature, establishing performance bounds, developing eﬃcient algorithms providing performance guarantees.
 has been new various results non-convex formulations matrix completion problem (see, e.g., [9]).
 Existing conditions ensuring recovery minimal rank matrix are formulated terms missing-at-random entries assumption so-called bounded- coherence (see survey other approaches [6]; do aim give complete overview vast literature).
 results are aimed establishing recovery high probability.
 work [7] studies related problem: uniqueness conditions minimum rank matrix recovery random linear measurements true matrix linear measurements correspond inner product measurement mask matrix true matrix, hence, observations are diﬀerent has matrix completion).
 deterministic pattern observed complete characterization identiﬁable matrix matrix completion remains important open question: conditions pattern, be (at least locally) unique solution?
 Recent work [12] provides insights problem studying so-called completable problems establishing conditions ensuring existence most many rank-r matrices agree observed entries.
 related work studied problem is sparse noise corrupts entries.
 paper, aim answer question diﬀerent point view give geometric perspective.
 particular, consider solution Minimum Rank Matrix Completion (MRMC) formulation, leads non-convex optimization problem.
 address following questions: (i) Given observed entries arranged according (deterministic) pattern, solving MRMC problem, is minimum achievable rank?
 (ii) conditions, be unique matrix is solution MRMC problem?
 give suﬃcient “almost necessary” condition call well-posedness condition) local uniqueness MRMC solutions, illustrate special cases such condition be veriﬁed.
 addition, consider convex relaxation nuclear norm minimization formulations.
 argue given m observation n1 × n2 matrix, minimal rank r∗ is less R(n1, m) := (n1 + − [(n1 + − m]1/2, corresponding solution is unstable sense arbitrary small perturbation observed values make rank unattainable.
 other hand r∗ > R(n1, m), solution is (even locally) unique (cf., [18]).
 indicates rare MRMC problem cannot have properties possessing unique stable solutions.
 Consequently, makes sense is try solve minimum rank problem consider low-rank approximation approaches approach mentioned [6, alternative MRMC formulation.
 propose sequential statistical testing procedure determine rank matrix observed entries.
 Such statistical approach be useful many existing low- rank matrix completion require pre-speciﬁcation matrix rank, such alternating minimization approach solving non-convex problem representing low-rank matrix product low-rank matrix factors (see, e.g., [6]).
 paper is organized follows.
 next section, introduce considered setting basic deﬁnitions.
 Section discuss rank reducibility generic point view.
 Section is devoted studying uniqueness MRMC solutions.
 Low- rank approximations least squares method are discussed Section
 Section consider semideﬁnite relaxations MRMC problem.
 Statistical testing low-rank model is discussed Section Section present numerical results related developed theory.
 Section concludes paper.
 proofs are transferred Appendix.
 Matrix completion problem setup Consider problem recovering n1×n2 data matrix low rank observing small number m entries, are denoted Mij, (i, j) ∈ Ω.
 Ω ⊂ {1, ..., ..., n2} is index set cardinality m.
 goal be written following optimization problem referred Minimum Rank Matrix Completion (MRMC), (2.1) Let introduce necessary deﬁnitions.
 Consider Ωc := {1, ..., × {1, ..., \ Ω, rank(Y subject Yij = Mij, (i, j) ∈ Ω.
 complement index set Ω, deﬁne min Y ∈Rn1×n2 VΩ :=(cid:8)Y ∈ Rn1×n2 Yij = (i, j) ∈ Ωc(cid:9)
 VΩc :=(cid:8)Y ∈ Rn1×n2 Yij = (i, j) ∈ Ω(cid:9) linear space represents set matrices are ﬁlled zeros locations unobserved entries.
 deﬁne represents space matrices cannot be determined speciﬁed observa- tions.
 denote M n1 × n2 matrix speciﬁed entries Mij, (i, j) ∈ Ω, other entries equal zero.
 construction, M + VΩc is aﬃne space matrices satisfy observation constraints.
 Note M ∈ VΩ dimension linear space VΩ is dim(VΩ) = m, dim(VΩc) = n1n2 − m.
 PΩ denote projection space VΩ, i.e., [PΩ(Y )]ij = Yij (i, j) ∈ Ω [PΩ(Y )]ij = (i, j) ∈ Ωc.
 use conventional notations.
 ∈ R denote (cid:100)a(cid:101) least integer is greater equal a, (cid:98)a(cid:99) largest integer less equal a.
 A ⊗ B denote Kronecker product matrices (vectors) B, vec(A) column vector obtained stacking columns matrix A.
 use following matrix identity matrices A, B, C appropriate order vec(ABC) = (C(cid:62) ⊗ A)vec(B).
 (2.2) Sp denote linear space p× p symmetric matrices writing X mean matrix X ∈ Sp is positive semideﬁnite.
 σi(Y denote i-th largest singular value matrix Y ∈ Rn1×n2.
 Ip denote identity matrix dimension p.
 say property holds (a.e) Mij, surely, set matrices Y ∈ VΩ property does hold has Lebesgue measure zero space VΩ.
 Main theoretical results gain insights identiﬁability issue matrix completion, aim answer following questions: (i) is achievable minimum rank (the optimal value problem (2.1)), (ii) minimum rank matrix, i.e., optimal solutions (2.1), is unique given problem set-up.
 Rank reducibility denote r∗ optimal value problem (2.1).
 is, r∗ is minimal rank n1 × n2 matrix prescribed elements Mij, (i, j) ∈ Ω.
 r∗ depends index set Ω values Mij.
 natural question values r∗ attain.
 Note (2.1) is non-convex problem have multiple solutions.
 literature, rank-objective function is approximated nuclear norm function (which is convex), various theoretical properties are derived; discuss approach Section Theorem (Upper bound minimum achievable rank) following upper bound holds (3.1) r∗ ≤(cid:6)√ m(cid:7)
 main idea proof is show aﬃne spaces, representing matrices satisfying observation constraints (2.1), representing matrices have rank less r, have non-empty intersection.
 certain generic sense is possible give lower bound minimal rank r∗.
 Let consider intersection set low-rank matrices aﬃne space matrices satisfying observation constraints.
 is, consider set n1 × n2 matrices rank r Mr :=(cid:8)Y ∈ Rn1×n2 rank(Y = r(cid:9) (3.2) (aﬃne) mapping AM VΩc → Rn1×n2 deﬁned AM (X) := M + X, X ∈ VΩc.
 was pointed image AM (VΩc) = M + VΩc mapping AM deﬁnes space feasible points MRMC problem (2.1).
 is known Mr is smooth, C∞, manifold (3.3) is said mapping AM intersects Mr X ∈ VΩc AM (X) (cid:54)∈ Mr, AM (X) ∈ Mr following condition holds dim(Mr) = r(n1 + − r).
 VΩc + TMr(Y = Rn1×n2, (3.4) Y := AM (X) TMr(Y denotes tangent space Mr Y ∈ Mr. using classical result diﬀerential geometry, is possible show (a.e.) Mij, (i, j) ∈ Ω, mapping AM intersects Mr (this holds manifold Mr) (see [18] discussion result).
 Transversality condition implies following dimensionality condition dim(VΩc) + dim(TMr(Y )) ≥ dim(Rn1×n2).
 turn above condition (3.5) be written r(n1 + − r) ≥ m, r ≥ R(n1, n2, m), R(n1, n2, := (n1 + −(cid:112)(n1 + n2)2/4 − m.
 (3.5) (3.6) (3.7) is, r < R(n1, m), transversality condition (3.4) cannot hold hence a.e. Mij follows rank(M + X) (cid:54)= r X ∈ VΩc.
 AM intersects Mr AM (X) ∈ Mr (i.e., condition (3.4) holds), intersection AM (VΩc)∩Mr forms smooth manifold point Y := AM (X).
 r > R(n1, m), manifold has dimension greater corresponding rank r solution is (locally) unique.
 leads following formal discussion results refer [18]).
 (Lower bound non-uniqueness solutions) index set Ω cardinality m Mij, (i, j) ∈ Ω, following holds: (i) feasible point Y problem (2.1) follows (3.8) (ii) r∗ > R(n1, m), problem (2.1) has multiple one) optimal solutions.
 rank(Y ≥ R(n1, n2, m), particular have square matrix n1 = = n, follows R(n, m) = − n2 − m.
 (3.9) n1 = n2 = n small m/n2 approximate R(n, n, m) = n −(cid:112)1 − m/n2 (cid:17) ≈ m/(2n).
 example, n1 = n2 = m = have m = R(n, n, m) bounds (3.10) become ≤ r∗ ≤
 follows part (i) Theorem r∗ ≥ R(n1, m) a.e. Mij.
 m ≤ min{n1, n2}, (almost surely) minimal rank r∗ is bounds R(n1, m) ≤ r∗ ≤(cid:6)√ m(cid:7) sider data M following form M =(cid:0) M1 (3.10) (2.1) have unique optimal solution r∗ = R(n1, m).
 course such equality happen R(n1, m) is integer number.
 Example shows, integer r∗ satisfying exists index set Ω such corresponding MRMC problem attains minimal rank r∗ a.e. Mij.
 particular shows lower upper bounds (3.10) are tight.
 (Tightness lower upper bounds r∗) r < min{n1, con- M2, M3, respective order r × r, (n1 − r) × r (n1 − r) × (n2 − r), represent observed entry values.
 Cardinality m corresponding index set Ω is r(n1 + n2 − r), i.e., r = R(n1, n2, m).
 Suppose r × r matrix M1 is nonsingular, i.e., rows are linearly independent.
 row matrix M2 be represented (unique) linear combination rows matrix M1.
 follows corresponding MRMC problem has (unique) solution rank r∗ = r.
 other words, rank completed matrix be equal r (the rank sub-matrix M1) be unique matrix achieves rank.
 suppose entries matrices M2 M3 are ob- served, hence cardinality respective index set Ω is less r(n1 +n2−r), r > R(n1, m).
 case respective minimal rank is r, provided matrix M1 is nonsingular, corresponding optimal solutions are unique.
 particular, (cid:1), i.e., entries matrix M1 are observed, m = r2 minimum (cid:1) Here, sub-matrices M1, M2 M3 M =(cid:0) M1 rank is r.
 Uniqueness solutions MRMC problem given matrix M ∈ VΩ corresponding minimal rank r∗ ≤ R(n1, m) question is corresponding solution Y ∗ rank r∗ is unique.
 was discussed previous section, set such matrices M is “thin” sense has Lebesgue measure zero), question uniqueness is important, particular statistical inference (discussed Section entries Mij are observed noise.
 Available results, based so-called Restricted Isometry Property (RIP) low-rank matrix recovery linear observations based coherence property low-rank matrix completion, assert certain probabilistic (Gaussian) models such uniqueness holds high probability.
 given matrix M ∈ VΩ is clear verify solution is unique.
 Let consider following concept local uniqueness solutions.
 Deﬁnition say n1×n2 matrix ¯Y is unique solution problem (2.1) PΩ( ¯Y = M is neighborhood V ⊂ Rn1×n2 ¯Y such rank(Y (cid:54)= rank( ¯Y Y ∈ V, Y (cid:54)= ¯Y
 Note rank is lower semicontinuous function matrix, i.e., {Yk} is sequence matrices converging matrix Y lim inf k→∞ rank(Yk) ≥ rank(Y ).
 local uniqueness ¯Y implies existence neighborhood V such rank(Y > rank( ¯Y Y ∈ V, Y (cid:54)= ¯Y i.e., least problem (2.1) does have optimal solutions diﬀerent ¯Y
 explain main result, introduce constructions associated manifold low-dimensional matrices.
 are several equivalent forms tangent space manifold Mr Y ∈ Mr be represented.
 way be written TMr(Y =(cid:8)Q1Y + Y Q2 Q1 ∈ Rn1×n1, Q2 ∈ Rn2×n2(cid:9)
 (3.11) equivalent form tangent space be written TMr(Y =(cid:8)H ∈ Rn1×n2 F HG = (3.12) F is (n1 − r)× n1 matrix rank n1 − r such F Y = (referred left side complement Y G is n2× (n2− r) matrix rank n2− r such Y G = (referred right side complement Y ).
 use linear space matrix orthogonal (normal) Mr Y ∈ Mr, denoted NMr(Y ).
 matrix Z is orthogonal Mr Y ∈ Mr tr(Z(cid:62)Y (cid:48)) Y (cid:48) ∈ TMr(Y ).
 means tr(cid:2)Z(cid:62)(Q1Y + Y Q2)(cid:3) = tr(Y Z(cid:62)Q1)+tr(Z(cid:62)Y Q2) matrices Q1 Q2 are arbitrary, tr(cid:2)Z(cid:62)(Q1Y + Y Q2)(cid:3) = ∀Q1 ∈ Rn1×n1, ∀Q2 ∈ Rn2×n2.
 NMr(Y =(cid:8)Z ∈ Rn1×n2 Z(cid:62)Y = Y Z(cid:62) =
 follows normal space be written (3.13) (3.14) Deﬁnition (Well-posedness condition) say matrix ¯Y ∈ Mr is well-posed problem (2.1) PΩ( ¯Y = M following condition holds VΩc ∩ TMr( ¯Y = {0}.
 (3.15) Condition (3.15) (illustrated Figure is natural condition having simple geometri- cal interpretation.
 Intuitively, means null space observation operator does have non-trivial matrix lies tangent space low-rank matrix manifold.
 Hence, be local deviation optimal solution satisﬁes measurement constraints.
 formally, suppose condition (3.15) does hold, i.e., exists nonzero matrix H ∈ VΩc ∩TMr( ¯Y ).
 means is curve Z(t) ∈ Mr starting ¯Y tangential H, i.e., Z(0) = ¯Y (cid:107) ¯Y + tH − Z(t)(cid:107) = o(t).
 course moreover PΩ(Z(t)) = M t ∈ R, solution ¯Y is unique.
 is guaranteed, i.e., suﬃcient condition (3.15) be necessary local uniqueness solution ¯Y violation condition implies solution ¯Y is unstable sense matrices Y ∈ Mr ¯Y distance (cid:107)PΩ(Y − M(cid:107) is order o((cid:107)Y − ¯Y (cid:107)).
 motivates introduce well-posedness condition guarantees matrix be unique solution.
 Figure Illustration well-posedness condition.
 give suﬃcient conditions local uniqueness: Theorem (Suﬃcient conditions local uniqueness) Matrix ¯Y ∈ Mr is lo- cally unique solution problem (2.1) ¯Y is well-posed (2.1).
 present equivalent form well-posedness condition.
 Theorem have matrix ¯Y ∈ Mr is well-posed, ¯Y is unique solution problem (2.1).
 Note condition (3.15) implies dim(VΩc) + dim(TMr( ¯Y )) ≤ n1n2.
 is, condition (3.15) implies r(n1 + − r) ≤ m r ≤ R(n1, m).
 Theorem have r∗ > R(n1, m), corresponding optimal solution !ℳ#$%$%ℳ&$%+()* cannot be unique surely.
 Note space VΩ is orthogonal space VΩc, duality arguments condition (3.15) is equivalent following condition VΩ + NMr( ¯Y = Rn1×n2.
 (3.16) using formula (3.12) is possible write condition (3.15) following form {X ∈ VΩc F XG = = {0}, (3.17) F is left side complement ¯Y G is right side complement ¯Y
 Recall vec(F XG) = (G(cid:62)⊗F )vec(X).
 Column vector matrix G(cid:62)⊗F corresponding component j ⊗ fi, fi is i-th column matrix F gj is j-th xij vector vec(X), is g(cid:62) j ⊗ fi, (i, j) ∈ Ωc, are row matrix G.
 Condition (3.17) means column vectors g(cid:62) independent.
 obtain following result be useful checking well-posedness condition.
 (Equivalent condition well-posedness) Matrix ¯Y ∈ Mr is well-posed problem (2.1) left side complement F right side complement G ¯Y column vectors g(cid:62) j ⊗ fi, (i, j) ∈ Ωc, are linearly independent.
 turn implies following necessary condition well-posedness ¯Y ∈ Mr terms pattern index set Ω.
 (Necessary condition well-posedness) matrix ¯Y ∈ Mr is well- posed problem (2.1), row column ¯Y are least r elements index set Ω.
 However, condition index set Ω have row column least r elements is suﬃcient ensure well-posedness shown Theorem below.
 Note deﬁnition matrices F G are full rank.
 Remark (Special case veriﬁable unique solutions) special cases is possible verify global uniqueness minimum rank solutions such conditions work so-called Minimum Rank Factor Analysis discussed Sec- tion
 Let ¯Y ∈ Mr be such PΩ( ¯Y = M
 Consider index (k, l) ∈ Ωc.
 Suppose exist I1 ⊂ {1, ..., \ {k} I2 ⊂ {1, ..., \ {l} such |I1| = |I1| = r I1 × I2 ⊂ Ω.
 Consider r × r submatrix M corresponding rows i ∈ I1 columns j ∈ I2.
 Suppose submatrix is nonsingular.
 follows minimum rank r∗ ≥ r.
 Suppose {k} × I2 ∈ Ω {l} × I1 ∈ Ω.
 matrix Y ∈ Mr such PΩ(Y = M have Ykl = Mkl.
 follows observing (r + × (r + submatrix Y corresponding rows {k} ∪ I1 columns {l} ∪ I2 has rank r hence determinant, applying Shur complement element Ykl.
 holds (k, l) ∈ Ωc, uniqueness solution ¯Y follows.
 Remark Wilson Worcester [21] was constructed example × sym- metric matrices rank same oﬀ-diagonal diﬀerent diagonal elements.
 deﬁne index set Ω := {(i, j) i (cid:54)= j, i, j = ..., be viewed R(6, = example diﬀerent unique solutions rank
 Note m =
 is R(6, > (almost surely) rank cannot be reduced r
 discuss example Section
 Note uniqueness minimum rank solution is invariant respect per- mutations rows columns matrix M
 motivates introduce following deﬁnition.
 Deﬁnition say index set Ω is reducible permutations rows columns, matrix M be represented block diagonal form, i.e., (cid:21) (cid:20) M1 M2 Otherwise say Ω is irreducible.
 M = (3.18) Theorem (Reducible index set) index set Ω is reducible, minimum rank solution ¯Y such ¯Yij (i, j) ∈ Ωc, is (and hence globally) unique.
 was shown Proposition ¯Y is unique, cannot be well- posed.
 index set Ω is reducible, minimum rank solution, diﬀerent M itself, is well-posed.
 course Ω is reducible, happen row column are least r elements index set Ω.
 is, condition discussed Theorem be suﬃcient ensure well-posedness property.
 Uniqueness rank solutions section discuss uniqueness rank solutions MRMC problem (2.1).
 show case minimum rank one, irreducibility Ω is suﬃcient global uniqueness.
 assume Mij (cid:54)= (i, j) ∈ Ω, row column matrix M has least element Mij.
 Let be solution rank problem (2.1), i.e., are nonzero column vectors v w such ¯Y = vw(cid:62) PΩ( ¯Y = M
 Recall permutations components vector v corresponds permutations rows respective rank matrix, permutations components vector w corresponds permutations columns respective rank matrix.
 was shown Corollary index set Ω is reducible, solution ¯Y cannot be unique: Theorem (Global uniqueness rank solution) Suppose Ω is irreducible, Mij (cid:54)= (i, j) ∈ Ω, row column matrix M have least element Mij.
 rank solution is unique.
 Semideﬁnite relaxations nuclear norm minimization section consider alternative versions MRMC problem have been studied literature, including convex relaxation formulation (see, e.g. [8]) nuclear norm minimization, study solution uniqueness.
 make connection existing results Factor Analysis.
 MRMC problem (2.1) be formulated following equivalent form Ξ ∈ Sp, p = + n2, is symmetric matrix form Ξ =(cid:0) M rank(Ξ + X) subject Ξ + X (cid:23) min X∈WSc (3.19) (cid:1).
 Minimization (3.19) is performed matrices X ∈ Sp are complement Ξ sense having zero entries places corresponding speciﬁed values Mij, (i, j) ∈ Ω.
 is, let S ⊂ {1, ..., p} × {1, ..., p} be symmetric index set corresponding index set Ω, i.e., (i, j) ∈ S ≤ i ≤ j ≤ n1, (i, j + ∈ Ω; (i, j) ∈ S, (j, i) ∈ S.
 Sc ⊂ {1, ..., p} × {1, ..., p} denote symmetric index set complement S.
 Deﬁne M(cid:62) WS := {X ∈ Sp Xij = (i, j) ∈ Sc} WSc := {X ∈ Sp Xij = (i, j) ∈ S}.
 consider general minimum rank problem form (3.19) allow index set S be general symmetric subset {1, ..., p} × {1, ..., p}, given matrix Ξ ∈ WS.
 Note WS ∩ WSc = {0} WS + WSc = Sp. heuristic was suggested [8] approximate problem (3.19) following problem tr(X) subject Ξ + X (cid:23)
 min X∈WSc (3.20) minimum rank problem (3.19) consider following SDP problem tr(CX) subject Ξ + X (cid:23) min X∈WSc (3.21) matrix C ∈ WSc. course problem (3.20) is particular case problem (3.21) C := Ip. have following result (cf., [17]).
 (Global uniqueness trace minimization) Suppose matrix C ∈ WSc is positive deﬁnite.
 a.e. Ξ ∈ WS problem (3.21) has unique optimal solution.
 Recall saying property holds a.e. Ξ ∈ WS mean holds Ξ WS subset WS Lebesgue measure zero.
 classical Minimum Rank Factor Analysis (MRFA) be viewed particular case problem (3.19) WSc being space Dp p × p diagonal matrices, given symmetric matrix Ξ oﬀ diagonal elements.
 is possible show (i.e., a.e. reduced rank MRFA problem is bounded (cf., [14]): rank(Ξ + X) ≥ + − + ∀X ∈ Dp. (3.22) Factor Analysis respective minimum trace problem form (3.20) is called Minimum Trace Factor Analysis (MTFA).
 relation MRFA MTFA problems is discussed
 Factor Analysis setting Remark be used show certain generic sense, MRFA solution is unique respective minimal rank is less p/2 (we refer [3], references therein, discussion uniqueness MRFA solutions).
 Consider SDP problem (3.21), assume matrix C ∈ WSc is positive deﬁnite.
 (Lagrangian) dual problem (3.21) is problem tr(CX) − tr[Λ(Ξ + X)].
 (3.23) Λ = C − Θ, Θ ∈ WS, problem (3.23) be written (note tr(CΞ) Ξ ∈ WS) min X∈WSc max Λ(cid:23)0 tr(ΘΞ) subject C − Θ (cid:23)
 max Θ∈WS (3.24) Note problems (3.23) (3.24) Slater condition holds, hence is duality gap problems, problems have nonempty bounded sets optimal solutions.
 Optimality conditions (necessary suﬃcient) problem (3.21) are C = Pτ (Λ), (Ξ + X)Λ = Λ (cid:23) Ξ + X (cid:23) X ∈ WSc. (3.25) (3.26) (3.27) suppose ¯X ∈ WSc is such Ξ + ¯X (cid:23) rank(Ξ + ¯X) = r < p.
 Let E be p × (p − r) matrix rank p − r such (Ξ + ¯X)E =
 optimality conditions (3.25)–(3.27) have ¯X is optimal solution SDP problem (3.21) following condition holds: exists Z ∈ Sp−r + such Pτ (EZE(cid:62)) = C.
 Equations Pτ (EZE(cid:62)) = C be viewed system dim(Wτ equations (p − r)(p − r + unknowns (nonduplicated elements matrix Z ∈ Sp−r).
 r is “small” (p − r)(p − r + > dim(Wτ ), is likely system have solution Z (cid:23) ¯X is optimal solution problem (3.21).
 course is heuristic argument careful analysis is needed.
 view adjusting weight matrix C considered matrix Ξ + ¯X choosing Z deﬁning C := Pτ (EZE(cid:62)).
 such C corresponding SDP problem has ¯X optimal solution.
 Note matrix EZE(cid:62) is positive semideﬁnite Z (cid:23) is guarantee corresponding matrix Pτ (EZE(cid:62)) is positive semideﬁnite.
 consider nuclear norm minimization problem.
 Consider following (con- vex) problem (cid:107) · (cid:107)∗ is nuclear norm (cid:107)Y (cid:107)∗ = (cid:80)min{n1,n2} (cid:107)X + M(cid:107)∗, min X∈VΩc σi(Y ).
 nuclear norm dual norm is given σ1(·) respective unit ball dual norm be written i=1 B =(cid:8)Q ∈ Rn1×n2 λ1(Q(cid:62)Q) ≤ λ1(Q(cid:62)Q) is largest eigenvalue matrix Q(cid:62)Q.
 follows (cid:107)Y (cid:107)∗ is equal optimal value following SDP problem tr(Q(cid:62)Y s.t. max Q∈Rn1×n2 Lagrangian dual problem is Q(cid:62) In2 (cid:20) In1 Q (cid:20) Λ1 − (cid:62) (cid:21) (cid:23)
 (cid:21) − Λ2
 min Λ1∈Sn1 Λ2∈Sn2 tr(Λ1 + Λ2) s.t. index set S being symmetric index set corresponding index set Ω Ξ :=(cid:0) M follows problem (3.28) be written form (3.20) p = n1 + n2, M(cid:62) (the coeﬃcient −1/2 be absorbed X).
 Theorem have (i.e., a.e. Mij) nuclear norm minimization problem has unique optimal solution.
 (cid:1) Low-rank matrix approximation section, discuss formulation MRMC based ﬁnding low-rank ap- proximation noisy incomplete observations matrix entries.
 refer “Low-Rank Matrix Approximation” (LRMA) problem.
 Compared formulation exact low rank recovery, LRMA is realistic presence noise.
 discuss properties LRMA problem introduce statistical test procedure choosing true rank matrix.
 LRMA properties Theorem have minimal rank r∗ is less R(n1, m), corresponding solution is unstable sense arbitrary small perturbation observed values Mij make rank unattainable.
 other hand r∗ > R(n1, m), solution is (even locally) unique.
 indicates rare occasions, problem (2.1) exact rank minimization cannot have properties possessing unique stable solutions.
 Consequently, makes sense is try solve minimum rank problem approximately.
 is consider problem min Y ∈Rn1×n2 X∈VΩc F (X + M, Y subject rank(Y r, (4.1) M ∈ VΩ is given data matrix, F (A, B) is discrepancy matrices A, B ∈ Rn1×n2.
 example F (A, B) := (cid:107)A − B(cid:107)2 i,j Y ij, being Frobenius (4.1) becomes least squares problem F (cid:107)Y (cid:107)2 F = tr(Y (cid:62)Y = (cid:80) (cid:88) min Y ∈Mr (i,j)∈Ω (Mij − Yij)2
 (4.2) least squares approach is natural, is only possible.
 example, statistical approach Factor Analysis discrepancy function is based Maximum Likelihood method is involved.
 discuss least squares approach.
 are several questions related formulation approximation problem (4.2).
 ﬁrst question is solve problem (4.2) numerically.
 general problem (4.2) is convex be diﬃcult solve.
 Proposition (Necessary condition LRMA) following are necessary condi- tions Y ∈ Mr be optimal solution problem (4.2) (PΩ(Y − M )(cid:62)Y = Y (PΩ(Y − M )(cid:62) =
 (4.3) Remark view least squares problem (4.2) following point view.
 Consider function φ(Y, Θ) := Θ)(cid:62)(PΩ(Y − Θ)], (4.4) (4.5) Θ ∈ VΩ viewed parameter.
 Deﬁne (Yij − Mij)2 = f (Y := (cid:88) (i,j)∈Ω M )(cid:62)(PΩ(Y − M )], Hence, problem (4.2) consists minimization f (Y subject Y ∈ Mr. Note Θ = M have f (·) = φ(·, M ), f (·) is deﬁned (4.5).
 Let ¯Y ∈ Mr be such φ( ¯Y Θ0) = Θ0 ∈ VΩ, i.e., PΩ( ¯Y = Θ0.
 suﬃcient condition ¯Y be unique solution problem (2.1), M = Θ0, is tr(cid:2)PΩ(H)(cid:62)PΩ(H)(cid:3) ∀H ∈ TMr( ¯Y \ {0}.
 above condition means H ∈ TMr( ¯Y H (cid:54)= PΩ(H) (cid:54)=
 other words means kernel Ker(PΩ) := {H ∈ TMr( ¯Y PΩ(H) is {0}.
 PΩ(H) = H ∈ VΩc, follows that: condition (4.6) is equivalent suﬃcient condition (3.15) Proposition is, condition (4.6) means matrix ¯Y is well-posed problem (2.1).
 Assuming condition (4.6) (or condition (3.15)) holds, applying Implicit Function Theorem ﬁrst order optimality conditions least squares prob- lem (4.2) have following result.
 Proposition Let ¯Y ∈ Mr be such PΩ( ¯Y = Θ0 Θ0 ∈ VΩ suppose well posedness condition (3.15) holds.
 exist neighborhoods V W ¯Y Θ0, respectively, such M ∈ W ∩ VΩ exists unique Y ∈ V ∩ Mr satisfying optimality conditions (4.3).
 (4.3).
 PΩ( ¯Y is close M (i.e., ﬁt(cid:80) above proposition implies following.
 Suppose run numerical procedure identiﬁes matrix ¯Y ∈ Mr satisfying (necessary) ﬁrst order optimality conditions (i,j)∈Ω (Yij − Mij)2 is small) condition (3.15) holds ¯Y say f (Y > f ¯Y Y (cid:54)= ¯Y neighborhood ¯Y
 is, ¯Y solves least squares problem least locally.
 is clear quantify close” condition, does guarantee global optimality ¯Y ¯Y is unique minimum rank solution.
 Statistical test rank selection second important question LRMA is rank r use considered data matrix M ∈ VΩ.
 above discussion be natural take value r less R(n1, m), otherwise have unique solution.
 ﬁt Y ∈ Mr X + M choice r, be tested statistical sense?
 section discuss statistical testing value “true” minimal rank entries data matrix M are observed noise.
 order proceed assume following model noisy biased obser- vations subset matrix entries.
 is (population) value Y ∗ n1 × matrix ij, (i, j) ∈ Ω, rank r < R(n1, m) Mij are viewed observed (estimated) values Y ∗ based sample size N
 observed values are modeled ij + N−1/2∆ij + εij, (i, j) ∈ Ω, (4.7) Y ∗ ∈ Mr ∆ij are (deterministic) numbers.
 random errors εij are assumed be independent other such N converge distribution ij, (i, j) ∈ Ω.
 additional terms N−1/2∆ij (4.7) normal mean zero variance σ2 represent possible deviation population values “true” model are referred population drift sequence local alternatives (we refer [11] historical overview invention local alternatives setting).
 is realistic model motivated many real applications.
 Deﬁnition say model is identiﬁed (at Y ∗) ¯Y ∈ Rn1×n2 rank( ¯Y ≤ r PΩ( ¯Y = PΩ(Y ∗) imply ¯Y = Y ∗, i.e., Y ∗ is unique solution respective matrix completion problem.
 is said model is identiﬁed holds such ¯Y neighborhood Y ∗, i.e., Y ∗ is unique solution.
 Mij = Y ∗ Consider following weighted least squares problem wij (Mij − Yij)2 (cid:88) min Y ∈Mr (i,j)∈Ω weights wij (i, j) ∈ Ω.
 (Of course, wij = (i, j) ∈ Ω, problem (4.8) coincides least squares problem have following standard result consistency least squares estimates.
 (4.8) Proposition Suppose model is identiﬁed Y ∗ ∈ Mr values Mij, (i, j) ∈ Ω, converge probability respective values Y ∗ ij sample size N tends inﬁnity.
 optimal solution ˆY problem (4.8) converges probability Y ∗ N → ∞.
 Consider following weighted least squares test statistic TN (r) := N min Y ∈Mr wij (Mij − Yij)2 (4.9) (cid:88) (i,j)∈Ω (cid:88) (i,j)∈Ω (cid:88) (i,j)∈Ω (cid:88) (i,j)∈Ω (cid:0)Y ∗ ij ˆσ2 wij := ij converge probability ij N → ∞).
 Recall respective condition form (3.15), (4.6), σ2 is suﬃcient local identiﬁablity Y ∗.
 following asymptotic results be compared similar results analysis covariance structures (cf., [19]).
 ij being consistent estimates σ2 ij (i.e., ˆσ2 Proposition (Asymptotic properties test statistic) Consider noisy obser- vation model (4.7).
 Suppose model is identiﬁed Y ∗ ∈ Mr Y ∗ is well-posed problem (2.1).
 test statistic TN (r) converges distribution non- central chi square degrees freedom df r = m − r(n1 + − r) noncentrality parameter δr = min H∈TMr (Y ∗) ij (∆ij − Hij)2
 σ−2 (4.10) Note optimal (minimal) value weighted least squares problem (4.8) be approximated min H∈TMr (Y ∗) wij (Eij − Hij)2 + RN (4.11) Eij := N−1/2∆ij + εij error term RN = ((cid:107)M − PΩ(Y ∗)(cid:107)2) being stochastic order RN = op(N−1).
 Hence, noncentrality parameter, given (4.10), be approxi- mated δr ≈ N min Y ∈Mr wij ij + N−1/2∆ij − Yij (4.12) (cid:1)2 is, noncentrality parameter is equal N times ﬁt “true” model alternative population values Y ∗ ij + N−1/2∆ij small perturbations order O(N−1/2).
 Remark above asymptotic results are formulated terms “sample size N ” suggesting observed values are estimated data.
 allows formulate precise convergence results.
 take pragmatic point view is “small” random noise observed values, respective test statistics normalized respect magnitude noise have noncentral chi square distribution.
 asymptotics test statistic TN (r) depends r cardinality m index set Ω.
 more observations become available additional entries matrix.
 is are testing model larger index set Ω(cid:48), cardinality m(cid:48), such Ω ⊂ Ω(cid:48).
 order emphasize test statistic depends corresponding index set add index set respective notations.
 Note Y ∗ is solution rank r sets Ω Ω(cid:48) model is (locally) identiﬁed Y ∗ set Ω, model is (locally) identiﬁed Y ∗ set Ω(cid:48).
 Note regularity condition (3.15) holds Y ∗ smaller model (i.e. Ω), holds Y ∗ larger model (i.e. Ω(cid:48)).
 following result be proved same way Theorem (cf., [19]).
 Proposition Consider index sets Ω ⊂ Ω(cid:48) cardinality m = |Ω| m(cid:48) = |Ω(cid:48)|, noisy observation model (4.7).
 Suppose model is identiﬁed Y ∗ ∈ Mr condition (3.15) holds Y ∗ smaller model (and hence models).
 statistic TN (r, Ω(cid:48)) − TN (r, Ω) converges distribution noncentral χ2 df r,Ω(cid:48) − df r,Ω = m(cid:48)−m degrees freedom noncentrality parameter δr,Ω(cid:48)−δr,Ω, TN (r, Ω(cid:48))−TN (r, Ω) is independent TN (r, Ω).
 is possible give asymptotic distribution solutions problem (4.8).
 assumptions Theorem hold ∆ij equation (4.7) being zeros.
 Let be solution problem (4.8), i.e., ˆYN ∈ arg Y ∈Mr (cid:88) wij (i,j)∈Ω (cid:16) Y ∗ ij + εij (cid:124) (cid:123)(cid:122) (cid:125) Mij −Yij (cid:17)2 N ˆYN − Y ∗) converges distribution random matrix A(Z), Z ∈ VΩ is random matrix entries Zij ∼ N (0, σ2 ij), (i, j) ∈ Ω, having normal distribution independent over, A(Z) is optimal solution problem ij (Zij − Hij)2
 σ−2 (4.13) (cid:88) min H∈TMr (Y ∗) (i,j)∈Ω Note A(·) is linear operator hence A(Z) has multivariate normal distribution means.
 Note (compare (7.5)) rank(A(Z)) = dim(cid:0)PΩ(TMr(Y ∗)(cid:1) = r(n1 + − r).
 Numerical experiments section give summary numerical experiments illustrating presented theory.
 extensive discussion results refer supplementary material https://github.com/ruizhang-ray/Matrix-Completion.
 example matrix considered [21] was pointed Remark [21] was given example diﬀerent unique solutions rank r∗ = × matrix index set Ω corresponding oﬀ-diagonal elements.
 matrix M example is  M = rank solutions are given diagonal matrices: D1 = Diag(0.64, D2 = Diag(0.425616,
 numerical procedures soft-thresholded SVD [10] nuclear norm minimiza- tion [2], were applied example.
 procedures didn’t recover solutions; soft-thresholded SVD converged solution matrix nuclear norm minimization produced following diagonal matrix solution D3 = Diag(0.4369, respective rank r smaller nuclear norm above rank solutions solution produced soft-thresholded SVD procedure.
 Note optimal solutions are numerical procedures recover them.
 is clear typical example, diﬀerent optimal solutions, is.
 Recall nuclear norm minimization problem possesses unique optimal solution.
 is clear approximates ‘true’ minimal rank solution is observed noise.
 Comparison LRMA nuclear norm minimization ﬁgure ﬁgure is comparison performances LRMA (using least square) nuclear norm minimization true matrices are observed normal noise.
 generate Y ∗, n1 × n2 matrix rank r∗, generated n1 × r∗ matrix U n2 × r∗ matrix V r∗ × r∗ diagonal matrix D setting Y ∗ = ˜U D ˜V T ˜U ˜V are orthonormalization U V respectively.
 sample Ω, |Ω| = m.
 Observation matrix M is generated Mij = Y ∗ ij + εij, (i, j) ∈ Ω, εij ∼ N (0, σ2N−1).
 Least square approximation is solved soft-threshholded SVD solver setting nuclear norm regularization.
 algorithm stops relative change Frobenius norm successive estimates, (cid:107)Y (t+1) − Y t(cid:107)F /(cid:107)Y (t)(cid:107)F is ij Y less tolerance, denoted tol iteration is larger number, denoted it.
 Nuclear norm minimization is solved TFOCS [2].
 solutions least square approximation nuclear norm minimization are denoted Y ls Y nm respectively.
 following, compare methods looking absolute value ij − Y ∗ Y ls Figure compare solutions situation well-posedness condition is sat- isﬁed.
 experiment, n1 n2 r∗ = m = σ = N = Ω is sampled well-posedness condition is satisﬁed.
 Convergence parameter tol =
 Least square approximation recover true matrix Y ∗ less error nuclear norm minimization.
 ij − Y ∗ ij.
 Figure Absolute errors methods well-posedness condition is sat- isﬁed.
 picture left is structure Ω Ω denoted yellow Ωc denoted blue.
 picture middle is absolute error true matrix completed ij|.
 picture right is absolute error matrix least square approximation, |Y ls true matrix completed matrix nuclear norm minimization, |Y nm ij|.
 True matrix Y ∗ ∈ R40×50, rank(Y ∗) = |Ω| = ∆ij = εij ∼ N (0, observation matrix ij + εij, (i, j) ∈ Ω.
 error, blue denotes small error yellow denotes large error.
 Mij = Y ∗ ij − Y ∗ ij − Y ∗ Figure compare solutions situation necessary condition well- posedness is violated.
 experiment, n1 n2 r∗ = m = σ = N = Ω is sampled necessary condition well-posedness is violated.
 Convergence parameter tol =
 Least square approximation fails recover true matrix rows (the row violating necessary condition well- posedness.
 Nuclear norm minimization recover larger error least square approximation.
 Figure compare solutions situation Ω is reducible.
 experiment, n1 n2 r∗ = m = σ = N = Ω = {(i, j) ∈ {1··· × {1··· ∪ {21··· × {21···
 Convergence parameter tol =
 situation, methods can’t recover true matrix Y ∗ due unidentiﬁability.
 Figure Absolute errors methods necessary condition well- posedness is violated.
 picture left is structure Ω.
 picture middle is absolute error true matrix completed matrix least square ap- proximation, |Y ls ij|.
 picture right is absolute error true matrix ij|.
 True matrix Y ∗ ∈ R70×40, completed matrix nuclear norm minimization, |Y ij + εij, (i, j) ∈ Ω.
 rank(Y ∗) = |Ω| = ε ∼ N (0, necessary condition well-posedness is violated, numbers observations are less row
 Color scheme is same Figure
 observation matrix Mij = Y ∗ ij − Y ∗ ij − Y ∗ Figure Absolute errors methods Ω is reducible.
 picture left is structure Ω.
 picture middle is absolute error true matrix completed matrix least square approximation, |Y ls ij|.
 picture right is absolute error true matrix completed matrix nuclear norm minimization, |Y ij − Y ∗ observation ij + εij, (i, j) ∈ Ω.
 Ω is reducible.
 diagonal block matrices M1 ∈ R20×20 matrix Mij = Y ∗ M2 ∈ R20×30 are observed.
 Note necessary condition well-posedness is satisﬁed situation.
 Color scheme is same ﬁgure
 ij|.
 True matrix Y ∗ ∈ R40×50, rank(Y ∗) = |Ω| = εij ∼ N (0, ij − Y ∗ Numerical veriﬁcation asymptotic theorems N (r)}200 N (r, Ω(cid:48)) − T (k) N (r, Ω)}200 ij (i, j) ∈ Ω, ε(k) Figure is Q-Q plot {T (k) idealized model, “true” solution low rank is observed noise, was in- troduced Section (see equation (4.7)) chi-square test statistics testing “true” minimal rank were suggested.
 are discussed numerical experiments validating asymptotics.
 generate Y ∗, n1 × n2 matrix rank r∗, generated n1 × r∗ matrix U n2 × r∗ matrix V r∗ × r∗ diagonal matrix D setting Y ∗ = ˜U D ˜V T ˜U ˜V are orthonormalization U V respectively.
 sample Ω, |Ω| = m.
 verify asymptotics, observation matrix M is generated ij ∼ N (0, σ2N−1).
 instance corresponding ij = Y ∗ ij + ε(k) M (k) statistic T (k) N (r) is computed using equation (4.9).
 Least square approximation is solved soft-threshholded SVD solver setting nuclear norm regularization algorithm stops relative change Frobenius norm successive estimates, (cid:107)Y (t+1) − Y t(cid:107)F /(cid:107)Y (t)(cid:107)F is less tolerance, denoted tol iteration is larger number, denoted it.
 Figure is Q-Q plot {T (k) k=1 corresponding chi-square distribution.
 experiment, n1 n2 r∗ = m = σ = N = Ω is sampled well-posedness condition is satisﬁed.
 Convergence parameter tol =
 result, see TN (r) follows central chi-square distribution degree freedom df r = m − r(n1 + − r) proved Theorem k=1 corresponding chi- square distribution.
 experiment, n1 n2 r∗ = m = σ = N = m(cid:48) = |Ω(cid:48)| = Ω is sampled well-posedness condition is satisﬁed (Notes Ω(cid:48) satisﬁed well-posedness condition Ω(cid:48)C ⊂ ΩC).
 Convergence parameter tol =
 result, see TN (r, Ω(cid:48))− TN (r, Ω) follows central Chi-square distribution degree freedom df r,Ω(cid:48) − df r,Ω = m(cid:48) − m proved Theorem Figure shows possible procedure determines true rank r∗ sequential chi-square tests.
 experiment, n1 n2 r∗ = m = σ = N = Ω is sampled well-posedness condition is satisﬁed.
 Convergence parameter tol =
 observation matrix M is generated experiment.
 least square approximation are solved speciﬁed r (cid:100)R(n1, n2, m)(cid:101) TN (r), p-value corresponding central chi-square distribution (with degree freedom df r = m − r(n1 + − r)) be computed determine r be accepted.
 shows r < r∗ is rejected ﬁrst r accepted is true r∗, happens high probability experiments showed supplementary material.
 performed numerical results justify asymptotic theorems show least square approximation be solved nu- clear norm minimization, current algorithms give satisfactory statistical inference assumption model (4.7).
 Numerical experiments, shown supplementary material, validate noncentral chi-square asymptotics considered statistics un- der assumption population drift additional terms representing deviation Figure Q-Q plot TN (r) quantiles chi-square distribution.
 Y ∗ ∈ R40×50, rank(Y ∗) = |Ω|
 Observation matrix M is generated ij (i, j) ∈ Ω, times, M (k) ij ∼ N (0, ε(k) N (r) is computed equation Theorem {T (k) N (r)} follows central chi-square df r = m − r(n1 + − r)
 ij = Y ∗ ).
 M (k), T (k) ij +ε(k) Figure Q-Q plot TN (r, Ω(cid:48)) − TN (r, Ω) quantiles chi- Y ∗ ∈ R40×50, square distribution.
 |Ω| rank(Y ∗) = Ω Ω(cid:48).
 Observation M(cid:48) (cid:48)(k) ij = M are generated times, M ij, (i, j) ∈ Y ∗ ij + ε(k) Ω, ε(k) ).
 time, N (r, Ω(cid:48)) T (k) T (k) N (r, Ω) are computed.
 N (r, Ω)} fol- N (r, Ω(cid:48)) − T (k) Theorem {T (k) lows central chi-square df r,Ω(cid:48) − df r,Ω = m(cid:48) − m =
 ij (i, j) ∈ Ω(cid:48) Mij = M(cid:48) |Ω(cid:48)| = ij ∼ N (0, population values “true” model.
 Conclusion paper, have examined non-convex matrix completion geometric view- point established suﬃcient “almost necessary” condition local uniqueness solutions.
 characterization assumes deterministic patterns, results are general.
 study, found minimum rank matrix completion (MRMC) tends lead unstable non-unique solutions alternative low-rank matrix approxi- mation (LRMA) is better stable approach.
 proposed new statistical test rank selection, based observed entries, be useful many practical matrix completion algorithms.
 Figure p-value chi-square rank test.
 experiment is same Figure
 red dash line is p-value =
 plot is equivalent Figure
 Figure Sequential test rank se- Y ∗ ∈ R40×50, rank(Y ∗) = lection.
 |Ω| εij ∼ N (0, observa- ij + εij, (i, j) ∈ Ω.
 Least tion matrix Mij = Y ∗ square approximation are solved speciﬁed r (cid:100)R(n1, n2, m)(cid:101) corresponding TN (r) is computed sequen- tially.
 grey region is conﬁdence in- terval α
 red point indicates corresponding TN (r) is conﬁ- dence interval blue point indicates corresponding TN (r) is conﬁdence interval.
 data is plotted log scale.
 Appendix m is integer.
 Proof Theorem suﬃces verify (3.1) cases m.
 Let V ∈ Rn1×r W ∈ Rn2×r be matrices rank r, respective Consider r := × r row vectors v1, ..., vn1 w1, ..., wn2.
 Consider following system equations j (i, j) ∈ Ω, unknowns given elements matrix Z ∈ Rr×r.
 is viZw(cid:62) linear homogeneous system m = r2 equations r2 unknowns, be written (wj ⊗ vi)vec(Z) = (i, j) ∈ Ω.
 Let matrices V W be chosen such way row vectors wj ⊗ vi, (i, j) ∈ Ω, are linearly independent.
 system linear equations has solution Z =
 Consider linear space matrices L := {V ZW (cid:62) Z ∈ Rr×r}.
 vectors wj ⊗ vi, (i, j) ∈ Ω, are linearly independent have dim(L) = r2, above construction L ∩ VΩc = {0}.
 other hand dim(VΩc) + dim(L) = n1n2 hence L + VΩc = Rn1×n2.
 follows matrix M ∈ VΩ linear (aﬃne) spaces M + VΩc L have nonempty intersection.
 Note M + VΩc represents set matrices satisfy observation constraints.
 note matrix Y ∈ L rank(Y ≤ r.
 follows r2 = m, is possible ﬁnd n1 × n2 matrix rank r ≤ √ m prescribed elements Mij, (i, j) ∈ Ω.
 Proof Theorem argue contradiction.
 Suppose is sequence {Yk} ⊂ Mr converging ¯Y such PΩ(Yk) = M
 Note follows Yk − ¯Y ∈ VΩc.
 passing subsequence necessary assume (Yk − ¯Y )/tk, tk := (cid:107)Yk − ¯Y (cid:107), converges H ∈ VΩc (of course is assumed Yk (cid:54)= ¯Y ).
 have Yk = ¯Y + tkH + o(tk) hence H ∈ TMr( ¯Y ).
 remains note H (cid:54)= obtain required contradiction condition (3.15).
 Proof Theorem Suppose row i ∈ {1, ..., n1} are less r elements Ω.
 means set σi := {j (i, j) ∈ Ωc} has cardinality greater n2 − r.
 Let F be left side complement ¯Y G be right side complement ¯Y
 rows gj G are dimension (n2 − r), have vectors gj, j ∈ σi, are linearly dependent, i.e.,(cid:80) j∈σi λjgj = λj, zero.
 (cid:80) j ⊗ fi) =(cid:0)(cid:80) j∈σi λj(g(cid:62) j∈σi (cid:1)(cid:62) ⊗ fi
 λjgj (7.1) j ⊗ fi, (i, j) ∈ Ωc, be independent.
 contradicts condition vectors g(cid:62) Similar arguments be applied columns matrix ¯Y
 (cid:3) Proof Theorem Suppose Ω is reducible.
 making permutations rows columns necessary, be assumed M has block diagonal form M2 = V2W (cid:62) (3.18).
 Let be respective minimum rank solution.
 is M1 = V1W (cid:62) (cid:1) being n1 × r n2 × r matrices rank r.
 (cid:1) W =(cid:0) W1 are matrices.
 changing V1 αV1 W1 α−1W1 α (cid:54)= change
 ¯Yij (cid:54)= (i, j) ∈ Ωc, cannot happen V1W (cid:62) V2W (cid:62) ¯Y = V W (cid:62) V =(cid:0) V1 (cid:16) M1 V1W (cid:62) (cid:17) (cid:16) M1 Note ¯Y = V2W (cid:62) matrix ¯Y matrix M2 V2
 shows solution ¯Y is unique.
 αV1W (cid:62) (cid:17) W2 α−1V2W (cid:62) M2 Proof Theorem Suppose Ω is irreducible.
 Consider rank solution ¯Y = vw(cid:62) respective vectors v = (v1, ..., vn1)(cid:62) w = (w1, ..., wn2)(cid:62).
 assume v1 is ﬁxed, say v1
 Consider element M1j1, (1, j1) ∈ Ω, ﬁrst row matrix M
 M1j1 = v1wj1, follows component wj1 vector w is deﬁned.
 Next consider element Mi1,j1, (i1, j1) ∈ Ω.
 Mi1j1 = vi1wj1, follows component vi1 vector v is deﬁned.
 proceed iteratively.
 Let ν ⊂ {1, ..., n1} ω ⊂ {1, ..., be index sets respective components vectors v w are deﬁned.
 Let j (cid:54)∈ ω be such is (i, j(cid:48)) ∈ Ω j(cid:48) ∈ ω wj(cid:48) is deﬁned.
 Mij = viwj Mij(cid:48) = viwj(cid:48), follows wj is deﬁned j be added index set ω.
 such column j does exist, take row i (cid:54)∈ ν such is (i(cid:48), j) ∈ Ω i(cid:48) ∈ ν.
 vi is deﬁned i be added ν.
 Ω is irreducible, process be continued components vectors v w are deﬁned.
 Proof Proposition Consider function deﬁned (4.5).
 diﬀerential f (Y be written df (Y = tr[(PΩ(Y − M )(cid:62)dY ].
 Y ∈ Mr is optimal solution least squares problem (4.2), ∇f (Y = PΩ(Y − M is orthogonal tangent space TMr(Y ).
 implies optimality conditions (4.3)
 Proof Proposition Consider function φ deﬁned (4.4), problem mini- mization φ(Y, Θ) subject Y ∈ Mr Θ viewed parameter.
 Y ¯Y ∈ Mr manifold Mr be represented system K = n1n2−dim(Mr) equations gi(Y = i = ..., K, appropriate smooth mapping g = (g1, ..., gK).
 is, above optimization problem be written min φ(y, θ) subject gi(y) = i = ..., K, (7.2) abuse notation write terms vectors y = vec(Y vec(Θ).
 Note mapping g is such gradient vectors ∇g1(¯y), ...,∇gK(¯y) are linearly independent.
 order optimality conditions problem (7.2) are ∇yL(y, λ, θ) = g(y) = (7.3) L(y, λ, θ) := f (y, θ) + λ(cid:62)g(y) is corresponding Lagrangian.
 θ = θ0 system has solution ¯y corresponding vector ¯λ = Lagrange multipliers.
 view (7.3) system (nonlinear) equations z = (y, λ) variables.
 Jacobian matrix (cid:0) H G (cid:1) system (7.3) (y, λ) = (¯y, ¯λ), H := ∇yyφ(¯y, θ0) is like apply Implicit Function Theorem system equations conclude θ0 has unique solution ¯z = (¯y, ¯λ).
 Consider Hessian matrix objective function G := ∇g(¯y) = [∇g1(¯y), ...,∇gK(¯y)].
 need verify Jacobian matrix is nonsingular.
 is implied condition (3.15), is equivalent condition (4.6).
 suppose G(cid:62) (cid:20) H G G(cid:62) (cid:21)(cid:20) v (cid:21) = (7.4) vectors v u appropriate dimensions.
 means Hv + Gu = G(cid:62)v =
 follows v(cid:62)Hv =
 Condition G(cid:62)v = means v is orthogonal tangent space TMr(¯y).
 follows condition (4.6) v
 Gu = hence, G has full column rank, follows u
 equations (7.4) have solution, follows Jacobian matrix is nonsingular.
 implying Implicit Function Theorem system (7.3) obtain required result.
 completes proof.
 Proof Proposition Note speciﬁed assumptions, Mij − Y ∗ ij are stochastic order Op(N−1/2).
 have Proposition optimal solution problem (4.8) converges probability Y ∗.
 standard theory least squares (e.g., [16, Lemma write following local approximation Y ∗ (4.11).
 follows limiting distribution TN (r) is same limiting distribution N times ﬁrst term right hand side (4.11).
 Note N ij Eij converges distribution normal mean σ−1 ij ∆ij variance one.
 follows limiting distribution N times ﬁrst term right hand side (4.11), limiting distribution TN (r), is noncentral chi-square degrees freedom ν = m − dim (PΩ(L)) noncentrality parameter δr.
 Recall dimension linear space L is equal sum dimension image PΩ (L) dimension kernel Ker(PΩ).
 remains note condition (3.15) means Ker(PΩ) = {0} (see Remark dim (PΩ(L)) = dim (L) = r(n1 + − r).
 (7.5) completes proof.
 References [1] Morteza Ashraphijuo, Vaneet Aggarwal, Xiaodong Wang.
 deterministic sam- pling patterns robust low-rank matrix completion.
 IEEE Signal Processing Letter, accepted,
 [2] Stephen R Becker, Emmanuel J Cand`es, Michael C Grant.
 Templates convex cone problems applications sparse signal recovery.
 Mathematical programming computation,
 [3] P.A. Bekker J.M.F. Ten Berge.
 Generic global indentiﬁcation factor analysis.
 Linear Algebra Applications,
 [4] Emmanuel J Cand`es Benjamin Recht.
 Exact matrix completion convex opti- mization.
 Foundations Computational Mathematics (FOCS),
 [5] Emmanuel J Cand`es Terence Tao.
 power convex relaxation: Near-optimal matrix completion.
 IEEE Trans.
 Info.
 Theory,
 [6] M.
 Davenport J.
 Romberg.
 overview low-rank matrix recovery incom- plete observations.
 IEEE Journal Selected Topics Signal Processing,
 Y.
 C.
 Eldar, D.
 Needell, Y.
 Plan.
 Uniqueness conditions low-rank matrix recov- ery.
 Applied Computational Harmonic Analysis, Sept.

 [8] M.
 Fazel.
 Matrix rank minimization applications.
 Ph.D. thesis, Stanford Univer- sity,
 Y.-P.
 Hsieh, Y.-C.
 Kao, R.
 Mahabadi, Y.
 Alp, A.
 Kyrillidis.
 non-euclidean gradient descent framework non-convex matrix factorization.
 submitted,
 [10] Rahul Mazumder, Trevor Hastie, Robert Tibshirani.
 Spectral regularization algo- rithms learning large incomplete matrices.
 J.
 Machine Learning Research,
 [11] D.A. McManus.
 invented local power analysis?
 Econometric Theory,
 [12] D.
 Pimentel-Alarcon, N.
 Boston, R.
 D.
 Nowak.
 characterization deterministic sampling patterns low-rank matrix completion.
 IEEE Journal Selected Topics Signal Processing,
 [13] Benjamin Recht, Maryam Fazel, Pablo A Parrilo.
 Guaranteed minimum-rank solutions linear matrix equations nuclear norm minimization.
 SIAM Review,
 [14] A.
 Shapiro.
 Rank reducibility symmetric matrix sampling theory minimum trace factor analysis.
 Psychometrika,
 [15] A.
 Shapiro.
 Weighted Minimum Trace Factor Analysis.
 Psychometrika,
 [16] A.
 Shapiro.
 Asymptotic distribution test statistics analysis moment struc- tures inequality constraints.
 Biometrika,
 [17] A.
 Shapiro.
 Extremal Problems Set Nonnegative Deﬁnite Matrices.
 Linear Algebra Applications,
 [18] A.
 Shapiro.
 Statistical inference semideﬁnite programming.
 Technical report, Georgia Institute Technology,
 Preprint posted Optimization Online, http://www.optimization-online.org/DB HTML/2017/01/5842.html.
 [19] J.H. Steiger, A.
 Shapiro, M.W. Browne.
 multivariate asymptotic distribution sequential chi-square statistics.
 Psychometrika,
 [20] J.
 A.
 Tropp, A.
 Yurtsever, M.
 Udell, V.
 Cevher.
 Practical sketching algorithms low-rank matrix approximation.
 SIAM J.
 Matrix Anal.
 Dec.

 [21] E.B. Wilson J.
 Worcester.
 resolution tests general factors.
 Proc.
 Nat.
 Acad.
 Sci.
 U.S.A.,

 Spatio-temporal datasets are large difﬁcult analyse [1].
 Traditional centralised data management mining techniques are do consider issues data-driven applications such scalability response time accuracy solutions, distribution heterogeneity [2].
 addition, transferring huge amount data network is efﬁcient strategy be possible security protection reasons.
 Distributed data mining (DDM) techniques constitute better alternative are scalable deal data heterogeneity.
 Many DDM methods such dis- tributed association rules distributed classiﬁcation have been proposed last decade most are based performing partial analysis local data individual sites followed generation global models aggregating local results [3], [4], [5], [6].
 However, few concern distributed clustering.
 many parallel clustering algorithms have been proposed [7], [8], [9].
 are classiﬁed categories.
 ﬁrst consists methods requiring multiple rounds message passing signiﬁcant amount synchronisations.
 second category consists methods build local clustering models aggregate build global models [10].
 Most parallel approaches need multiple synchronisation constraints processes global view dataset, [12].
 distributed aggregation phase is costly, therefore, needs be optimised.
 paper, present approach reduces complexity aggregation phase.
 reduces amount information exchanged aggregation phase, generates correct number clusters.
 case study, was shown data exchanged is reduced more original datasets.
 II.
 DYNAMIC DISTRIBUTED CLUSTERING DDC approach includes main steps.
 ﬁrst step, usual, cluster datasets located node select good local representatives local cluster.
 phase is executed parallel communications nodes.
 phase reach super speed- up.
 next phase consists aggregating local clusters.
 operation is executed certain nodes system, called leaders.
 leaders are elected according nodes’ characteristics such capacity, processing power, connectivity, etc.
 leaders are responsible merging regenerating data objects based local cluster representatives.
 However, nodes need communicate order send local clusters leaders.
 process is shown Figure
 Figure overview DDC Approach.
 A.
 Local Models node use technique clustering local dataset.
 is key features approach deal data heterogeneity.
 node choses technique is suitable data.
 approach relaxes data pre-processing.
 instance need deal data consistency nodes’ datasets.
 needed is consistency local cluster’s representation.
 extracted local clusters be used inputs next phase generate global clusters.
 exchanging local clusters network create signiﬁcant overheads slow- process.
 is major problems majority distributed clustering techniques.
 improve performance propose exchange minimum number points/objects.
 sending clusters datapoints, exchange representative points, constitute total size dataset.
 best way represent spatial cluster is shape density.
 shape cluster is represented boundary points (called contour) [11], [13], [14] (see Figure
 Many algorithms extracting boundaries cluster be found literature [15], [16].
 Recently, developed new algorithm detecting extracting boundary cluster [10].
 main concepts technique are given below.
 Neighbourhood: Given cluster C ⊆ (cid:60)n ≡ {p1, p2, ..., pn} neighbourhood N C(p) point p cluster C is deﬁned set points pi ∈ C distance pi p is equal ε: N C(p) = {pi ∈ C | dist(p, pi) ≤ ε} (1) order determine boundary points cluster, need introduce following concepts: Displacement vector: displacement vector pi ∈ N C(p) point p is deﬁned as: −→ V = (cid:88) pi∈N C (p) (p − pi) vector points area lowest density neighbourhood p.
 Balance vector: balance vector relative point p is deﬁned follows: −→ b p = −→ V p (cid:107)−→ V p(cid:107) > Otherwise (3) (cid:107)−→ V p(cid:107) −→ (cid:40) Note balance vector p points least dense area neighbourhood p, length vector does hold relevant information.
 Figure shows example balance vector.
 neighbourhoods p are circle, balance vector is represented blue colour.
 Boundary points: point is boundary point, be points direction balance vector.
 property allows separate boundary points internal points.
 point p, checks empty area shape is intersection hyper-cone inﬁnite height, vertex, axis aperture ρ, ρ is given angle.
 shown area checked is highlighted green.
 Formally, boundary point is described Boolean predicate.
 Figure Example balance vector.
 Figure Boundary point check (cid:40) Boundary(p) = (∀q ∈ N C ε (p), (q − p) true f alse Otherwise −→ b b < cos(v) deﬁne boundary BC cluster C set boundary points C: (2) BC = {p ∈ C Boundary(p) is true} algorithm selecting boundary points is de- scribed Algorithm complexity is O(n log n).
 Boundary Detection Algorithm −→ b pi, input Cluster C, set balance vectors parameter ν.
 Boundary points BC cluster C BC ← C; point p ∈ BC do point q ∈ N C(p) do −→ b ≥ cos(v) (q − Discard p BC; Break; return BC; Let Ci be set clusters node i Bcj be boundary cluster cj ∈ Ci. local model Li is deﬁned by: n(cid:91) Li = Bcj ∪ Pi (6) j=1 Where Pi is set internal representatives.
 B.
 DDC DBSCAN B.
 Global Models global clusters are generated second phase.
 phase consists main steps: leader collects local clusters neighbours, leaders merge local clusters using overlay technique.
 process merging clusters continue reach root node.
 root node contain global clusters (see
 phase exchange boundaries clusters.
 merging process consists steps: boundary merging regeneration.
 merging is performed boundary-based method.
 Let Li be local model received site i Bi be set boundaries Li. global model G is deﬁned by: G = Γ(∪Bi), Bi ∈ Li (7) Γ is merging function.
 III.
 DDC EVALUATION AND VALIDATION DDC is evaluated using different clustering algorithms.
 paper use centroid-based algorithm (K-Means) density-based Algorithm (DBSCAN).
 A.
 DDC-K-Means DDC K-Means (DDC-K-Means) is characterised fact ﬁrst phase, called parallel phase, node Ni system executes K-Means algorithm local dataset produce Li local clusters calculate contours.
 rest process is same described above.
 was shown [17] DDC-K-Means determines number clusters priori knowl- edge data estimation process number clusters.
 DDC-K-Means was compared well- known clustering algorithms: BIRCH [18] CURE [19].
 results showed generates better clusters.
 Also, expected, approach runs other algorithms; BIRCH CURE.
 DDC-K-Means does need number global clusters advance.
 is calculated dynamically.
 Moreover, local clustering Li needs Ki. Let ˜Ki be exact number local clusters node Ni, is required is set Ki such Ki > ˜Ki. is simpler giving Ki, is enough knowledge local dataset characteristics.
 Nevertheless, is better set Ki possible ˜Ki order reduce processing time calculating contours merging procedure.
 However, DDC-K-Means fails ﬁnd good clusters datasets Non-Covex shapes datasets noises, is due fact K-Means algorithm tends work convex shape only, is based centroid principle generate clusters.
 notice results DDC-K-Means are worse dataset contains big amount noises (T3, T4).
 fact returns whole dataset noise ﬁnal cluster dataset (see Figure
 is K-Means does deal noise.
 DBSCAN (Density-Based spatial Clustering Applica- tions Noise) is well-known density based clustering algorithm capable discovering clusters arbitrary shapes eliminating noisy data [20].
 DBSCAN Complexity: DBSCAN visits point dataset, multiple times, depending size neighbourhood.
 performs neighbourhood operations O(log overall average complexity O(n log n) is obtained parameter Eps is chosen meaningful way.
 worst case execution time complexity remains O(n2).
 distance matrix size O((n2 − n/2)) be materialised avoid distance re-computations, needs O(n2) mem- ory, non-matrix based implementation DBSCAN needs O(n) memory space.
 DDC-DBSCAN Algorithm: approach remains same; using K-Means processing local clusters, use DBSCAN.
 node (ni) executes DBSCAN local dataset produce Ki local clusters.
 local clusters are calculate contours.
 second phase is same pseudo code algorithm is given Algorithm
 DDC DBSCAN.
 input Xi: Dataset Fragment, Epsi: Distance Epsi N M inP tsi: minimum points contain clusters generated N D: tree degree, Li: Local clusters generated N output: Kg: Global Clusters (global results) level = treeheight; DBSCAN(Xi. Epsi, M inP tsi); // N executes DBSCAN locally.
 Contour(Li); // N odei executes Contour algorithm generate boundary local cluster.
 N odei joins group G D // N odei joins neighbourhood Compare cluster N odei other node’s clusters same group; // look overlapping clusters.
 j = ElectLeaderNode(); // Elect node merge overlapping clusters.
 i <> j Send (contour i, j); level > level Repeat level=0; return (Kg: N clusters); Figure illustrates example DDC-DBSCAN.
 Assume distributed computing platform contains ﬁve Nodes (N =
 Node executes DBSCAN algorithm local parameters (Epsi, M inP tsi) local dataset.
 be seen Figure new approach returned right number clusters shapes.
 approach is insensitive way original data was distributed nodes.
 is insensitive noise outliers.
 see, node executed DBSCAN different parameters.
 global ﬁnal clusters were correct noisy dataset (T3) (See Figure
 Figure Example DDC-DBSCAN execution.
 IV.
 EXPERIMENTAL RESULTS section, study performance DDC- DBSCAN approach demonstrate effectiveness com- pared BIRCH, CURE DDC-K-Means.
 choose algorithms are same category such BIRCH belongs hierarchical clustering category, have efﬁcient optimisation approach, such CURE.
 used BIRCH implementation provided [18].
 performs pre-clustering uses centroid- based hierarchical clustering algorithm.
 Note time space complexity approach is quadratic number points pre-clustering.
 set parameters default values suggested [18].
 CURE: used implementation CURE provided [19].
 algorithm uses representative points shrinking towards mean.
 described [19], clusters are merged step algorithm, representative points new merged cluster are selected ones original clusters points merged clusters.
 A.
 Experiments run experiments different datasets.
 used types datasets (T1, T2, T3 T4) different shapes sizes.
 datasets are well-known benchmarks evaluate density-based clustering algorithms.
 datasets are summarised Table I.
 number points clusters dataset is given.
 datasets contain set shapes patterns are easy extract traditional techniques.
 Table I: datasets used test algorithms.
 Type Noise Noise Dataset T1 T2 T3 T4 Description Round Oval shape Different shapes including noise Different shapes Noises Different shapes, clusters surrounded others #Points #Clusters B.
 Quality Clustering run algorithms Four datasets order evaluate quality ﬁnal clusters.
 Figure shows clusters returned algorithms datasets noise (T1) datasets noise (T2, T3, T4).
 use different colours show clusters returned algorithm.
 expected, BIRCH ﬁnd correct clusters; tends work datasets (T1), BIRCH does deal noise.
 results CURE are worse is able extract clusters non-convex shapes.
 see CURE does deal noise.
 DDC-K-Means fails ﬁnd correct ﬁnal results.
 fact returns whole original dataset ﬁnal cluster dataset (T3 T4) (Which contain signiﬁcant amount noise).
 conﬁrms DDC technique is sensitive type algorithm chosen ﬁrst phase.
 second phase deals merging local clusters are correct not.
 issue is corrected DDC-DBSCAN, is suited spatial datasets noise.
 datasets noise eliminates noise outliers.
 fact, generates good ﬁnal clusters datasets have signiﬁcant amount noise.
 ﬁnal observation, results prove DDC framework is efﬁcient regard accuracy results.
 only issue is choose good clustering algorithm ﬁrst phase.
 be done exploring initial datasets question be answered choose appropriate clustering algorithm accordingly.
 Moreover, DDC-K-Means, DDC-DBSCAN is dynamic (the correct number clusters is returned automati- cally) efﬁcient (the approach is distributed minimises communications).
 C.
 Speed-up goal is study execution time algorithms demonstrate impact using parallel distributed architecture deal limited capacity centralised system.
 mentioned Section III-B1, execution time DDC-DBSCAN algorithm be generated cases.
 ﬁrst case is include time required generate distance matrix calculation.
 second case is suppose distance matrix has been generated.
 reason is distance matrix is calculated once.
 Table II illustrates execution times tech- niques different datasets.
 Note execution times do BIRCH CURE DDC-K-Means DDC-DBSCAN T1 T2 T3 T4 Figure Comparing clusters generated different datasets Table II: execution times (ms) BIRCH, CURE, DDC- K-Means DDC-DBSCAN (w) (w/o) distance matrix computation.
 SIZE BIRCH T1 T2 T3 T4 Execution Time (ms) DDC-K-Means CURE DDC-DBSCAN W/O include time post-processing are same algorithms.
 mentioned Section III-B1, Table II conﬁrmed fact distance matrix calculation DBSCAN is sig- niﬁcant.
 Moreover, DDC-DBSCAN’s execution time is lower CUREs execution times datasets.
 Table II shows DDC-K-Means is quick is line polynomial computational complexity.
 BIRCH is fast, however, quality results are good, failed ﬁnding correct clusters datasets.
 DDC-DBSCAN is bit slower DDC-K-Means, returns high quality results tested benchmarks, DDC-K-Means, has good results convex cluster shapes bad results non-convex cluster shapes.
 overall results conﬁrm DDC-DBSCAN clustering techniques compares tested algorithms combined performance measures (quality results response time).
 D.
 Computation Complexity Let be number data objects dataset.
 complexity approach is sum complexities components: local mining, local reduction, global aggregation.
 a) Phase1: Local clustering: Assume local clustering algorithm is DBSCAN nodes.
 cost phase is given by: Max i=1 (DBSCANi) + TP hase1 = Max i=1 (Reductioni) [5] N.-A.
 Le-Khac, L.
 Aouad, M.-T.
 Kechadi, “Performance study distributed apriori-like frequent itemsets mining,” Knowledge Information Systems, vol.
 no.
 pp.

 [6] N.
 Le-Khac, L.Aouad., M.-T.
 Kechadi, Emergent Web Intelligence: Advanced Semantic Technologies.
 Springer London, ch.
 Distributed Knowledge Discovery onGrid Systems, pp.

 [7] L.
 Aouad, N.-A.
 Le-Khac, M.-T.
 Kechadi., “Image analysis plat- meteorological domain,” form data management Industrial Conference, ICDM Leipzig, Germany, July 14-18,
 Proceedings, vol.

 Springer Berlin Heidelberg, pp.

 I.
 Dhillon D.
 Modha, “A data-clustering algorithm distributed memory multiprocessor,” large-Scale Parallel Data Mining, Work- shop Large-Scale Parallel KDD Systems, SIGKDD.
 Springer-Verlag London, UK, pp.

 [8] [9] M.
 Ester, H.-P.
 Kriegel, J.
 Sander, X.
 Xu, “A density-based algorithm discovering clusters large spatial databases noise.” Kdd, vol.
 no.
 pp.

 J.-F.
 Laloux, N.-A.
 Le-Khac, M.-T.
 Kechadi, “Efﬁcient distributed approach density-based clustering,” Enabling Technologies: Infras- tructure Collaborative Enterprises (WETICE), IEEE Interna- tional Workshops, pp.

 [10] [11] N.
 Le-Khac, L.Aouad., M.-T.
 Kechadi, Data Management.
 Data, Data Everywhere: British National Conference Databases.
 Springer Berlin Heidelberg, ch.
 New Approach Distributed Density Based Clustering Grid Platform, pp.

 [12] L.
 Aouad, N.-A.
 L.
 Khac, M.-T.
 Kechadi, Advances Data Mining.
 Theoretical Aspects Applications: Industrial Conference (ICDM Leipzig, Germany, July 14-18,
 Proceedings.
 Springer Berlin Heidelberg, ch.
 Lightweight Clustering Technique Distributed Data Mining Applications, pp.

 [13] L.Aouad, N.-A.
 Le-Khac, M-T.Kechadi, “Variance-based clustering technique distributed data mining applications.” DMIN, pp.

 N.-A.
 Le-Khac, M.
 Bue, M.
 Whelan, M-T.Kechadi, “A knowl- edge based data reduction large spatio-temporal datasets,” International Conference Advanced Data Mining Applications, (ADMA2010),
 [15] A.
 Chaudhuri, B.
 Chaudhuri, S.
 Parui, “A novel approach computation shape dot pattern extraction perceptual border,” Computer vision Image Understranding, vol.
 pp.

 [16] M.
 Melkemi M.
 Djebali, “Computing shape planar points set,” Elsevier Science, vol.
 p.

 [17] M.
 Bendechache M.-T.
 Kechadi, “Distributed clustering algorithm spatial data mining,” Spatial Data Mining Geographical Knowledge Services (ICSDM), IEEE International Conference on, pp.

 [18] T.
 Zhang, R.
 Ramakrishnan, M.
 Livny, “Birch: efﬁcient data clustering method large databases,” SIGMOD-96 Proceed- ings ACM SIGMOD international conference Manage- ment data, vol.

 ACM New York, USA, pp.

 [19] S.
 Guha, R.
 Rastogi, K.
 Shim, “Cure: efﬁcient clustering algorithm large databases,” Information Systems, vol.

 Elsevier Science Ltd.
 Oxford, UK, pp.

 [20] M.Ester, H.
 Kriegel, J.
 Sander, X.
 Xu, “A density-based algorithm discovering clusters large spatial databases noise,” Int.
 Conf., Knowledge Discovery Data Mining (KDD
 N is number nodes system.
 complexity DBSCAN best case scenario is O(n log n) parameter Eps is chosen meaningful way exclude distance matrix computation.
 complexity local reduction algorithm is O(n log n).
 b) Phase2: Aggregation: global aggregation de- pends hierarchical combination contours local clusters.
 combination is based intersection edges contours, complexity phase is O(v log v + p).
 v is number vertices p is number intersections edges different contours (polygons).
 c) Total complexity: total complexity ap- proach is TT otal = O(n log n) + O(n log n) + O(v log v + p), is: TT otal (cid:39) O(n log n) V.
 CONCLUSION paper, proposed efﬁcient ﬂexible dis- tributed clustering framework work existing data mining algorithms.
 framework has been tested spatial datasets using K-Means DBSCAN algorithms.
 proposed approach is dynamic, solves major shortcomings K-Means DBSCAN.
 proposed efﬁcient aggregation phase, reduces data exchange leaders system nodes.
 size data exchange is reduced
 DDC approach was tested using various benchmarks.
 benchmarks were chosen such way reﬂect difﬁculties clusters extraction.
 difﬁculties include shapes clusters (convex non-convex), data vol- ume, computational complexity.
 Experimental results showed approach is efﬁcient deal various situations (various shapes, densities, size, etc.).
 future try extend framework non-spatial datasets.
 look problem data communications reduction phase two.
 research work is conducted Insight Centre Data Analytics, is supported Science Foundation Ireland Grant Number SFI/12/RC/2289.
 REFERENCES [1] M.
 H.
 Dunham D.
 Ming, “Introductory advanced topics,”
 [2] M.
 Bertolotto, S.
 Di Martino, F.
 Ferrucci, M.-T.
 Kechadi, “Towards framework mining analysing spatio-temporal datasets,” Inter- national Journal Geographical Information Science, vol.
 no.
 pp.

 [3] L.
 Aouad, N.-A.
 Le-Khac, M.-T.
 Kechadi, “weight clustering technique distributed data mining applications,” LNCS advances data mining – theoretical aspects applications, vol.
 pp.

 [4] L.Aouad, N.-A.
 Le-Khac, M.-T.
 Kechadi, “Grid-based approaches distributed data mining applications,” Journal Algorithms Computational Technology, vol.
 no.
 pp.

 Artiﬁcial Neural Network (ANN), inspired biological neural networks, is based collection connected units nodes called artiﬁcial neurons.
 systems are used learning algorithm tries mimic brain works.
 ANNs are consider universal function approximators, is, approximate function data sent it.
 is based mul- tilayer perceptron [3] model is class feedforward artiﬁcial neural networks, consisting least layers models.
 Learning occurs perceptron changing connection weights piece data is processed, based amount error output compared expected result.
 is example supervised learning, is carried back- propagation, generalization least mean squares algorithm linear perceptron.
 multilayer perceptron model coupled backpropagation algorithm gave rise Artiﬁcial Neural Network, be used learning algorithm.
 Backpropagation [1] is method used artiﬁcial neural networks calcu- late error contribution neuron batch data is processed.
 is used gradient descent optimization algorithm adjust weight neurons calculating gradient loss function.
 tech- nique is called backward propagation errors, error is calculated output distributed network layers.
 requires known, desired output input value — is considered be supervised learning method.
 Gradient Descent [4] is iterative approach takes small steps reach local minima function.
 is used update weights biases neuron neural network.
 Gradient descent is based observation multivariable function F (x) is deﬁned diﬀerentiable neighborhood point a, F (x) decreases fastest goes direction negative gradient F a, (a).
 follows that, an+1 = – γ∆F (an) γ small enough, F (an) >= F (an+1).
 other words, term γ∆F (a) is subtracted want move gradient, minimum.
 observation mind, starts guess x0 local minimum F considers sequence x0, x1, x2,
 such xn+1 = xn – γn∆F (xn), n
 have F (x0) >= F (x1) >= F (x2) >= ..., sequence xn converges desired local minimum.
 method works general, has few limitations.
 Firstly, due iterative nature algorithm, takes lot time converge local minima function.
 Secondly, gradient descent is slow minimum: technically, asymptotic rate convergence is inferior many other methods.
 gradient methods are ill-deﬁned non-diﬀerentiable functions.
 paper be referring Moore-Penrose Pseudo Inverse [8].
 mathematics, particular linear pseudoinverse A+ matrix A is generalization inverse matrix.
 known type matrix pseudoinverse is Moore–Penrose inverse, was described E.
 H.
 Arne Bjerhammar Roger Pen- rose
 common use pseudoinverse is compute ‘best ﬁt’ (least squares) solution system linear equations lacks unique solution.
 use is ﬁnd minimum (Euclidean) norm solution system linear equations multiple solutions.
 pseudoinverse facilitates state- ment proof results linear algebra.
 pseudoinverse is deﬁned unique matrices entries are real complex numbers.
 be computed using singular value decomposition.
 paper, formulate method ﬁnding errors weights biases neurons neural network.
 like present few assumptions made model neural network, make method feasible.
 Modiﬁcations neuron structure have made change structure artiﬁcial neuron.
 assume is weight bias associated input, is, element input vector is multiplied weight bias is added it.
 is slight alteration traditional artiﬁcial neuron is common bias applied overall output neural network.
 change alter goal end result neural network.
 proof statement is shown below: input vector size ‘n’: c1w1 + b1 + c2w2 + b2 + c3w3 + b3...cnwn + bn = c1w1 + c2w2 + c3w3...cnwn + b b = b1 + b2 + b3..bn (1) (2) (3) Therefore, having separate bias input element make diﬀerence end result.
 Figure Neuron New Backpropagation Algorithm Calculating new weights biases neuron Taking neuron time, is input entering neuron, is multiplied weight bias is added product.
 value is sent activation function, output activation func- tion is taken output neuron.
 Let C be input neuron, original weight applied input is w original bias applied input is b.
 Let be output given input C passes neuron.
 Let be output require.
 Based required output, require diﬀerent weight bias value, say wn bn respectively.
 original output is calculated as, But, required output.
 Therefore, Cw + b = x Let, Cwn + bn = xn wn = w − ∆w bn = b − ∆b Where, ∆w is error weight and, ∆b is error bias.
 Cwn + bn = xn C(w − ∆w) + (b − ∆b) = xn C(w − ∆w) + (b − ∆b) = xn (Cw + b) − (C∆w + ∆b) = xn x − xn = (C∆w + ∆b) Therefore, C∆w + ∆b = (x − xn) (4) (5) (6) (7) (8) (9) (10) (11) (12) (13) Now, [ C ] × [ ∆w ∆b ] = [ (x − xn) ] ∆w ∆b ] = [ C ]−1 × [ (x − xn) ] (14) (15) But, [ C ] is square matrix.
 Therefore, have ﬁnd Moore-Penrose Pseudo-Inverse matrix [ C ].
 ∆w ∆b ] = [ C ]+ × [ (x − xn) ] (16) obtaining ∆w ∆b, change original weight bias new weight bias accordance to, wn = w − (∆w ∗ α) bn = b − (∆b ∗ α) (17) (18) α is learning rate.
 Tackling multiple inputs above mentioned method changing weights biases neuron be extended vector input length n.
 Let input vector C belong nth dimension.
 case, element input vector be multiplied respective weight neuron, bias be added products.
 There- fore, be n input n corresponding weights biases, n outputs weight-bias block.
 outputs are added give single output passed activation function.
 backpropagation stage, desired output is distributed weight-bias pairs, such that, block weight bias i (wi, required output block be required output.
 is, weight-bias blocks (wi, bi) xni = xn/n (19) weights biases are initialized random values beginning, is, absolute weightage given element input vector is randomized.
 Figure Neuron vector length ‘n’ relative weightage given element input vector be same.
 weight-bias block give same output, cumulative output give required answer.
 Therefore, method dividing weights work.
 Activation Function non-linearity achieve non-linearity, general approach taken is pass summation output weight-bias pairs non-linear activation func- tion [6].
 backpropagation phase, correct weights biases values neuron, cannot pass actual output vector required.
 do so, change weights biases is acti- vation function, forward propagation same vector neuron outputs go activation function, give wrong result.
 pass output vector required inverse activation function.
 need make sure have choose activation function such domain range are same, avoid math errors avoid loss data.
 new vector applying inverse activation function is actual vector sent layers network backpropagation phase.
 Network Architecture Figure shows representation neural network.
 neuron outputs value.
 output neuron layer is sent input Figure Neural Network Representation neuron next layer.
 Therefore, layer be associated buﬀer list, output neuron layer be stored passed next layer input.
 help implementation neural network simplifying forward propagation task.
 Figure Neural Network Implementation input propagates network last (output) layer gives output vector.
 Now, last layer, required output is known.
 weights biases neurons last layer be changed.
 do know required output vectors previous layers.
 make calculated guess.
 Using simple intuition asking question, be input (which is output vector previous layer) last layer such output be correct?”, arrive conclusion input, be correct required output previous layer, is vector have given error output last layer.
 be illustrated following equations.
 C ∗ w + b = x Cn vector satisfy equation Cn ∗ w + b = x Cn = (xn − b)/w (20) (21) approach be extended previous layers.
 issue arises many neurons give own ‘required’ input, outputs be correct.
 happen multiclass classi- ﬁcation problem, wherein output vector required is one-hot encoded vector (where element vector position required class is other elements vector are
 take average vectors.
 give equal weightage feedbacks neuron.
 Pass averaged required input vector previous layers required output layer.
 concludes complete working neural network devised backpropagation algorithm.
 Diﬀerences Extreme Learning Machines Extreme learning machines [7] are feedforward neural network classiﬁcation, regression, clustering, sparse approximation, compression feature learning single layer multilayers hidden nodes, parameters hid- den nodes weights connecting inputs hidden nodes) be tuned.
 hidden nodes be assigned updated are random projection nonlinear transforms), be inherited ancestors being changed.
 most cases, output weights hidden nodes are learned single step, amounts learning linear model.
 method Moore-Penrose Pseudo Inverse, are few signiﬁcant diﬀerences ELM proposed backpropagtion method explained paper.
 ELM is feedforward network is aims replacing traditional artiﬁcial neural network, paper provides alternative backpropagation algorithm used traditional artiﬁcial neural networks.
 ELM algorithm provides prop- agation technique change weights bias neurons last hidden layer, have provided method backpropagation change weights biases neurons layer.
 Results Telling-Two-Spirals-Apart Problem Alexis P.
 Wieland proposed useful benchmark task neural networks: distin- guishing intertwined spirals.
 task is easy visualize, is hard network learn due extreme non-linearity.
 report exhibit network architecture facilitates learning spiral task, compare learning speed several variants backpropagation algorithm.
 experiment, are using spiral dataset contains data points class.
 have decided model network 16-32-64-32- conﬁguration, ‘Softplus’ activation function neurons network.
 trained model epochs, learning rate
 Figure Training data points Two-Spirals problem above see doesn’t distinguish spirals are able get accuracy
 is due fact Softplus activation function is rec- Figure Testing data points Two-Spirals problem ommended activation function particular problem.
 recommended activation function is ‘Tanh’ but, due fact domain inverse Tanh function lies (−∞, +∞), cannot be used backpropagation method causing loss data.
 Looking ﬁgure observe non-linearity classiﬁcation sets spirals, proves backpropagation method is working.
 Separating-Concentric-Circles Problem type natural patterns is concentric rings.
 test, use sklearn.dataset.make circles function create concentric circles data points, were assigned classes.
 used artiﬁcial neural network model conﬁgurations using ‘Softplus’ activation function neurons network.
 trained model epochs learning rate
 Observing ﬁgure see is slight non-linearity classiﬁcation points.
 observe accuracy rate
 low accuracy be justiﬁed fact softplus activation function is suitable such types data.
 XOR Problem Continuing tests alternate algorithm, create dataset data points data sample containing numbers, class number.
 numbers are positive negative, class is class number is
 XOR function is applied sign number.
 Figure Training data points Concentric-Circles problem Figure Testing data points Concentric-Circles problem model was conﬁguration ‘Softplus’ activation func- tion is applied neurons.
 learning rate was set network was trained epochs.
 validation accuracy was achieved.
 Figure Training data points XOR problem Figure Testing data points XOR problem Wisconsin Breast Cancer Dataset further test neural network model, used real-world dataset test- ing neural network.
 dataset contains samples, sample has attributes features, class attribute.
 dataset is taken UCI Machine Learning Repository, samples arrive Dr. Wolberg reports clinical cases.
 model had conﬁguration 16-2, ‘Softplus’ activation function is applied neurons.
 trained model epochs learning rate
 observe validation accuracy reached upto epoch.
 values validation error train- ing error are erratic start, seem reach constant value number epochs.
 Figure Validation Accuracy training Wisconsin Breast Cancer Dataset Figure Training Error training Wisconsin Breast Cancer Dataset above experiments, conclude Softplus activation function is suited Wisconsin Breast Cancer Dataset proposed backpropagation algorithm works.
 Figure Validation Error training Wisconsin Breast Cancer Dataset Discussions Conclusion above stated facts results, observe few properties method.
 proposed method backpropagation be used activation functions domain activation function matches range inverse.
 property eases requirement activation function be diﬀerentiable.
 Therefore, ReLU-like activation functions such LeakyReLU, Softplus, S-shaped rectiﬁed linear activation unit (SReLU), etc.
 be good match method.
 Further optimizations be made method, be eﬃ- used.
 requirement diﬀerent type activation function accelerate discovery many more activation functions ﬁt var- ious diﬀerent models.
 believe backpropagation method suits ReLU-like [2] ac- tivation be enhanced be used ﬁelds biomedical engineering, due asymmetric behaviour data collected such ﬁelds number data points diﬀerent classes are balanced.
 future, suitable replacement activation functions, such Sigmoid Tanh, are created, method be used more frequently.
 References [1] Rumelhart, David E.; Hinton, Geoﬀrey E.; Williams, Ronald J.
 (8 October
 ”Learning representations back-propagating errors”.
 Nature.

 doi:10.1038/323533a0 [2] Prajit Ramachandran, Barret Zoph, Quoc V.
 Le Search- ing Activation Functions [3] Rosenblatt, Frank (1958), Perceptron: A Probabilistic Model Infor- mation Storage Organization Brain, Cornell Aeronautical Labo- ratory, Psychological Review, v65, No. pp.

 doi:10.1037/h0042519 [4] Snyman, Jan (3 March
 Practical Mathematical Optimization: In- troduction Basic Optimization Theory Classical New Gradient- Based Algorithms.
 Springer Science Business Media.
 ISBN 24348-1 [5] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W.
 Hoﬀman, David Pfau, Tom Schaul, Brendan Shillingford, Nando Freitas Learning learn gradient descent gradient de- scent [6] arXiv:1602.05980v2 Bing Xu, Ruitong Huang, Mu Li Revise Saturated Activation Functions [7] Guang-Bin Huang, Qin-Yu Zhu, Chee-Kheong Siew Extreme learning ma- chine: new learning scheme feedforward neural networks.
 ISBN: 0- 7803-8359-1 [8] Weisstein, Eric W.
 ”Moore-Penrose Matrix Inverse.” From MathWorld– http://mathworld.wolfram.com/Moore- A Wolfram Web Resource.
 PenroseMatrixInverse.html [9] https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)
 Model interpretability is long-standing problem machine learning has become acute accelerating pace widespread adoption complex predictive algorithms.
 are multiple approaches interpreting models predictions ranging variety visualization techniques [1–3] explanations example
 approach consider paper thinks explanations models approximate decision boundary original predictor belong class is simpler (e.g., local linear approximations).
 Explanations be generated post-hoc alongside predictions.
 popular method, called LIME [6], takes ﬁrst approach attempts explain predictions arbitrary model searching linear local approximations decision boundary.
 other hand, proposed contextual explanation networks (CENs) [7] incorporate similar mechanism deep neural networks arbitrary architecture learn predict explain jointly.
 Here, focus analyzing few properties explanations generated LIME, variations, CEN.
 particular, seek answers following questions:
 Explanations are good features use explain predictions.
 ask feature selection feature noise affect consistency explanations.

 explanation is part learning prediction does affect perfor- mance predictive model?

 Finally, kind insight gain visualizing inspecting explanations?
 Methods start brief overview methods compared paper: LIME [6] CENs [7].
 Given dataset inputs, x ∈ X targets, y ∈ Y, goal is learn predictive model, f X (cid:55)→ Y.
 explain prediction, have access set features, z ∈ Z, construct explanations, gx Z (cid:55)→ Y, such are consistent original model, gx(z) = f (x).
 additional features, z, are assumed be interpretable x, are called interpretable representation [6] attributes [7].
 Interpretable ML Symposium, Conference Neural Information Processing Systems (NIPS Long Beach, CA, USA.
 LIME Variations Given trained model, f, instance features (x, z), LIME constructs explanation, gx, follows: gx = argmin g∈G L(f, g, πx) + Ω(g) (1) L(f, g, πx) is loss measures g approximates f neighborhood deﬁned similarity kernel, πx Z (cid:55)→ R+, space additional features, Z, Ω(g) is penalty complexity explanation.
 speciﬁcally, Ribeiro al.
 [6] assume G is class linear models: (2) deﬁne loss similarity kernel follows: gx(z) := bx + wx · z πx(z(cid:48)) := exp(cid:8) −D(z, z(cid:48))2/σ2(cid:9) (3) L(f, g, πx) := πx(z(cid:48)) (f (x(cid:48)) − g(z(cid:48)))2 data instance is represented (x, z), z(cid:48) corresponding x(cid:48) are perturbed features, D(z, z(cid:48)) is distance function, σ is scale parameter kernel.
 Ω(g) is further chosen favor sparsity explanations.
 (cid:88) z(cid:48)∈Z Contextual Explanation Networks K(cid:88) k=1 LIME is post-hoc model explanation method.
 means justiﬁes model predictions producing explanations which, correct, are used make predictions ﬁrst place.
 Contrary that, CENs use explanations integral part learning process make predictions applying generated explanations.
 formally, CENs construct predictive model f X ×Z (cid:55)→ Y composition: given encoder, eθ X (cid:55)→ G, produces explanation g is applied z make prediction.
 other words: (4) f (x, z) := gx(z), gx := eθ(x) [7] introduced general probabilistic framework allows combine different deterministic probabilistic encoders explanations represented arbitrary graphical models.
 keep discussion simple concrete, assume explanations take same linear form (2) LIME encoder maps x (bx, wx) follows: bx := αθ(x)(cid:62)B, wx := αθ(x)(cid:62)W, α(k) θ (x) = α(k) θ (x) ≥ (5) other words, explanation (bx, wx) is constrained be convex combination K components global learnable dictionary, D := (B, W ), combination weights, αθ(x), called attention, are produced deep network.
 Encoder such form is called constrained deterministic map [7] model is trained w.r.t. (θ, B, W minimize prediction error.
 Analysis Both LIME CEN produce explanations form linear models be used prediction diagnostics.
 goal is understand different conditions affect explanations generated methods, see lead erroneous conclusions, understand learning predict explain affects performance.
 use following tasks analysis: MNIST image classiﬁcation1, sentiment classiﬁcation IMDB reviews [8], poverty prediction households Uganda satellite imagery survey data [9].
 details setup are omitted interest space be found [7], follow same setup.
 Consistency Explanations Linear explanation assign weights interpretable features, z, depend quality way select them.
 consider cases (a) features are corrupted additive noise, (b) selected features are incomplete.
 analysis, use MNIST IMDB data.
 (a) (b) Fig.
 effect feature quality explanations.
 (a) Explanation test error
 level noise added interpretable features.
 (b) Explanation test error
 total number interpretable features.
 train baseline deep architectures (CNN MNIST LSTM IMDB) CEN variants.
 MNIST, z is pixels scaled image (pxl) HOG features (hog).
 IMDB, z is bag words (bow) topic vector (tpc) produced pre-trained topic model.
 effect noisy features.
 experiment, inject features z ask LIME CEN ﬁt explanations noisy features.
 predictive performance produced explanations noisy features is given Fig.

 Note injecting noise, data point has noiseless representation x noisy ˜z.
 baselines take inputs, performance stays same and, regardless noise level, LIME “successfully” explanations—it is able approximate decision boundary baselines using noisy features.
 other hand, performance CEN gets worse increasing noise level indicating model fails learn selected interpretable representation is low quality.
 effect feature selection.
 Here, use same setup, injecting noise z, construct ˜z subsampling set dimensions.
 Fig.
 demonstrates result.
 performance CENs degrades size ˜z, see that, again, LIME is able ﬁt explanations decision boundary original models loss information.
 experiments indicate major drawback explaining predictions post-hoc: con- structed poor, noisy, incomplete features, such explanations overﬁt decision boundary predictor are likely be misleading.
 example, predictions valid model end getting absurd explanations is unacceptable decision support point view.
 Explanations Regularizer part, compare CENs baselines terms performance.
 task, CENs are trained generate predictions construct explanations.
 CENs show competitive performance are able approach surpass baselines number cases, IMDB data (see Table
 suggests forcing model produce explanations predictions does limit capacity.
 (a) (b) Fig.
 (a) Training error vs.
 iteration (epoch batch) baselines CENs. (b) Validation error models trained random subsets data different sizes.
 “explanation layer” CENs affects geometry optimization problem causes better convergence (Fig.

 train models subsets data (the size varied MNIST IMDB) notice explanations play role regularizer improves sample complexity (Fig.

 use Gaussian noise zero mean select variance signal-to-noise ratio level appropriately.
 −30−20−10010SNR,dB141664Testerror(%)MNISTCNNLIME-pxlCEN-pxlCEN-hog−20−10010SNR,dB8163264IMDBLSTMLIME-bowCEN-bowCEN-tpc050100Featuresubsetsize(%)050100Testerror(%)MNISTCNNLIME-pxlCEN-pxlCEN-hog050100Featuresubsetsize(%)2040IMDBLSTMLIME-bowCEN-bowCEN-tpc01020Epochnumber0.250.51.0Trainerror(%)MNISTCNNCEN-pxlCEN-hog05001000Batchnumber204060IMDBLSTMCEN-tpc51015Trainingsetsize(%)510Validationerror(%)MNISTCNNCEN-pxlCEN-hog2040Trainingsetsize(%)10203040IMDBLSTMCEN-bowCEN-tpc Table Performance models classiﬁcation tasks (averaged runs; std.
 are order least signiﬁcant digit).
 subscripts denote features linear models are built: pixels (pxl), HOG (hog), bag-or-words (bow), topics (tpc), embeddings (emb), discrete attributes (att).
 MNIST IMDB Satellite Model Err (%) Model Err (%) Model Acc (%) AUC (%) LRpxl LRhog CNN MoEpxl MoEhog CENpxl CENhog LRbow LRtpc LSTM MoEbow MoEtpc CENbow CENtpc LRemb LRatt MLP MoE CEN (cid:63)6.9 VCEN (cid:63)7.8 previous results similar LSTMs: (supervised) (semi-supervised) [10].
 Visualizing Explanations Finally, showcase insights get explanations produced predictions.
 Particularly, consider problem poverty prediction household clusters Uganda satellite imagery survey data.
 x representation household cluster is collection × satellite images; z is represented vector categorical features living standards measurement survey (LSMS).
 goal is binary classiﬁcation households Uganda poor poor.
 methodology, follow original study Jean al.
 [9] use pretrained VGG-F network embedding images 4096-dimensional space top build contextual models.
 Note datasets is small (642 points), hence keep VGG-F frozen avoid overﬁtting.
 note quantitatively, conditioning VGG features satellite imagery, CENs are able improve sparse linear models survey features (known gold standard remote sensing techniques).
 training CEN dictionary size discover encoder tends select explanations (M1 M2) different household clusters Uganda (see Fig.
 Fig.
 appendix).
 survey data, household cluster is marked urban rural; notice that, conditional satellite image, CEN tends pick M1 urban areas M2 rural (Fig.

 Notice explanations weigh different categorical features, such reliability water source proportion houses walls made unburnt brick, differently.
 visualized map, see CEN selects M1 major city areas, correlates high nightlight intensity areas (Fig.

 High performance model makes conﬁdent produced explanations (contrary LIME discussed Sec.
 allows draw conclusions causes model classify certain households different neighborhoods poor.
 (a) (b) (c) (d) Fig.
 Qualitative results Satellite dataset: (a) Weights given subset features models (M1 M2) discovered CEN.
 (b) M1 M2 are selected areas marked rural urban (top) average proportion Tenement-type households urban/rural area M1 M2 was selected.
 (c) M1 M2 models selected different areas Uganda map.
 M1 tends be selected more urbanized areas M2 is picked rest.
 (d) Nightlight intensity different areas Uganda.
 M1M2Water:UnreliableWatersrc:PublictapWalls:UnburntbricksRoof:Thatch,StrawIswaterpayedVegetationHaselectricityNightlightintensity0.9-0.4-0.6-1.20.3-0.20.50.2-0.3-0.3-0.10.4-0.2-0.8-0.7-0.7−0.8−0.40.00.40.80.30.40.50.60.7Timesmodelselected(%)M1M2RuralUrban0.10.20.30.4HHtype:Tenement(%)M1M2AruaGuluKampala(capital)IgangaMasakaKaseseUganda:ContextualModelsM1M2AruaGuluKampala(capital)IgangaMasakaKaseseUganda:NightlightIntensity0%100% References [1] Karen Simonyan, Andrea Vedaldi, Andrew Zisserman.
 Deep convolutional networks: Visualising image classiﬁcation models saliency maps.
 arXiv preprint arXiv:1312.6034,
 [2] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, Hod Lipson.
 Understanding neural networks deep visualization.
 arXiv preprint arXiv:1506.06579,
 [3] Aravindh Mahendran Andrea Vedaldi.
 Understanding deep image representations invert- ing them.
 Proceedings IEEE conference computer vision pattern recognition, pages
 [4] Rich Caruana, Hooshang Kangarloo, JD Dionisio, Usha Sinha, David Johnson.
 Case-based explanation non-case-based learning methods.
 Proceedings AMIA Symposium, page
 Been Kim, Cynthia Rudin, Julie A Shah.
 bayesian case model: A generative approach case-based reasoning prototype classiﬁcation.
 Advances Neural Information Processing Systems, pages
 [6] Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin.
 Trust You?: Explaining predictions classiﬁer.
 Proceedings ACM SIGKDD International Conference Knowledge Discovery Data Mining, pages

 [7] Maruan Al-Shedivat, Avinava Dubey, Eric P Xing.
 Contextual explanation networks.
 arXiv preprint arXiv:1705.10301,
 [8] Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, Christopher Potts.
 Learning word vectors sentiment analysis.
 Proceedings Annual Meeting Association Computational Linguistics: Human Language Technologies-Volume pages
 Association Computational Linguistics,
 [9] Neal Jean, Marshall Burke, Michael Xie, W Matthew Davis, David B Lobell, Stefano Ermon.
 Combining satellite imagery machine learning predict poverty.
 Science, (6301):790–794,
 [10] Rie Johnson Tong Zhang.
 Supervised semi-supervised text categorization using lstm region embeddings.
 Proceedings International Conference Machine Learning, pages
 Appendix (a) Full visualization explanations M1 M2 learned CEN poverty prediction task.
 (b) Correlation selected explanation value particular survey variable.
 Fig.
 Additional visualizations poverty prediction task.
 B Details Consistency Explanations provide detailed description experimental setup used analysis Section M1M216HHtype:BQ15Iswaterpayed14Waterusagep/day13Dist.towatersrc.12Num.ofrooms11Avg.dist.toroad10Avg.dist.tomarket09Avg.vegetationdec.08Avg.vegetationinc.07Vegetation06Avg.percipitation05Avg.temperature04Hasgenerator03Haselectricity02Isurban01Nightlightintensity-0.9-0.7-0.3-0.3-0.10.10.30.4-0.3-0.10.40.40.10.00.10.2-0.4-0.2-0.10.4-0.20.3-0.0-0.1-0.10.1-0.2-0.8-0.0-0.6-0.7-0.7M1M232Roof:Wood,Planks31Roof:Tin30Roof:Tiles29Roof:Thatch,Straw28Roof:Other27Roof:Mud26Roof:Ironsheets25Roof:Concrete24Roof:Asbestos23HHtype:Uniport22HHtype:Tenement21HHtype:Sharedhouse20HHtype:Other19HHtype:Privatehouse18HHtype:Privateapt17HHtype:Hut-0.7-0.4-0.20.0-0.6-0.30.50.20.30.50.60.5-0.5-0.4-0.5-0.3-0.1-0.3-0.0-0.0-0.6-0.6-0.6-0.7-0.5-0.6-0.2-0.3-0.4-0.70.30.5M1M248Floor:Stone47Floor:Other46Floor:Mosaic/tiles45Floor:Cowdung44Floor:Earth43Floor:Cement42Floor:Bricks41Walls:Stone40Walls:Unburntbricks39Walls:Timber38Walls:Thatch,Straw37Walls:Other36Walls:Mud,poles35Walls:Cementblocks34Walls:Brickw/mud33Walls:Brickw/cement-0.9-1.1-0.5-0.5-0.9-0.90.30.40.3-0.1-0.8-0.8-0.00.0-0.30.60.3-0.20.0-0.9-0.20.4-0.10.10.30.4-0.7-1.0-0.3-0.3-0.6-0.7M1M264Water:Unreliable63Water:Contribution62Water:Badtaste61Water:Unprotect.OK60Water:Longqueues59Water:Faraway58Watersrc:Vendortruck57Watersrc:Unprotectedwell56Watersrc:River/lake/pond55Watersrc:Rainwater54Watersrc:Publictap53Watersrc:Protectedwell52Watersrc:Privatetap51Watersrc:Other50Watersrc:Gravityﬂow49Watersrc:Bore-hole0.9-0.40.3-0.9-0.9-0.4-0.50.2-0.6-0.30.00.2-0.9-0.30.10.3-0.70.2-0.8-0.9-0.6-1.2-0.1-0.1-1.1-1.0-0.8-0.80.70.80.40.2−0.8−0.40.00.40.80123456789101112131415161718192021222324252627282930310.00.1Correlation323334353637383940414243444546474849505152535455565758596061626364Featurenumber−0.050.000.050.10Correlation-0.10.1
 language model (LM) computes likelihood given sentence is used improve accuracy automatic speech recogni- tion (ASR) system.
 Recent research has focused neural network (NN) based LMs [1] outstanding performances generalizing sparse data, traditional n-gram based LMs do.
 particular, recurrent neural network based LMs (RNNLMs) [2] do require Markov assumptions model word histories variable-length, virtues have helped improve performance many ASR systems [3,
 However, knowledge, are adopted real- time ASR systems due high computational complexities.
 Several attempts have been made utilize RNNLMs on- line decoding real-time ASR systems [5, However, ei- simulate aspects RNNLMs traditional ar- chitectures [5, perform 2-pass decoding [7] be applied end utterance was reached.
 have been attempts apply RNNLM on- line ASR approximation eliminating redundant compu- tations [8,
 previous research [9], were successful applying moderate size RNNLMs CPU-GPU hybrid on- line ASR systems cache strategy [10].
 However, order apply complex task bigger RNNLMs, needed ﬁnd way accelerate further.
 Recent studies indicate reduce number distinct RNN computations treating similar past hidden layer outputs, referred history vectors, same [11], RNNLMs be accelerated GPU parallelization [12].
 paper, attempt different approaches order achieve real- time performance large RNNLM based ASR system.
 Firstly, lossy compression is applied cache history vector.
 precision vectors be controlled rounding smaller number signiﬁcant digits extreme, storing sign element.
 Next, propose GPU paralleliza- tion RNNLM computations, selected layers.
 performing RNNLM computations same platform, compute-intensive parts model are computed GPUs, parts need utilize large memory are calculated CPUs. method increases overhead data transfer be- tween CPUs GPUs. is handled coordinating batch transfer method reduces number communications size data blocks same time hybrid ASR systems.
 paper is organized follows.
 architecture base- line ASR system is explained Section
 lossy compression method history vectors is explained Section
 Section explains RNNLM rescoring is accelerated CPU-GPU par- allelization.
 Section evaluates performance improvements proposed methods, followed conclusion Section

 ARCHITECTURE OF OUR BASELINE CPU-GPU HYBRID RNNLM RESCORING CPU-GPU hybrid ASR system [13], weighted ﬁnite state transducer (WFST) is composed layers representing acoustic model (AM), context model, pronunciation model, LM.
 WFSTs output word hypotheses reach word boundaries frame-synchronous Viterbi searches hy- potheses be rescored stored RNNLM.
 However, order speed on-the-ﬂy rescoring based RNNLMs, needed reduce redundant computations possible.
 section, brieﬂy outline architecture baseline CPU- GPU hybrid RNNLM rescoring proposed [9].
 main high- lights baseline architecture are use gated recurrent unit (GRU) [14] based RNNLM, contrastive estimation (NCE) [15] output layer, n-gram based maximum entropy (MaxEnt) by- pass [16] input output layers, cache based on-the-ﬂy rescoring.
 GRU based RNN employed GRU is type gated RNNs [14].
 GRU is mechanism designed prevent vanishing gradient problems re- lated long-term dependencies time using reset gates update gates.
 calculating output vector GRU hidden layer, total weight matrices bias vectors need be loaded memory gate candidate activation, Fig.

 On-the-ﬂy rescoring LM query cache (baseline).
 weight matrices bias vector are required.
 memory usage go several megabytes weights are stored single precision ﬂoating-point format.
 computational com- plexities GRU computations are O(H × H) hidden layer size H.
 is compute-intensive task considering number unique LM queries decoding utterance reach several hundreds thousands.
 Noise contrastive estimation order guarantee scores calculated output layer RNNLM are valid need be normalized different word sequences.
 normalization is com- intensive task considering vocabulary size V reach millions.
 order address this, employ NCE output layer [15].
 NCE is sampling-based approximation method treats partition functions separate parameters learns non-linear logistic regression.
 variances partition functions estimated NCE are limited small values [17], allowing use unnormalized scores signiﬁcant re- duction recognition accuracy.
 required computations are inner products GRU outputs NCE weights corresponding current word, NCE weight matrix size H × V be loaded memory.
 Maximum Entropy second strategy reduce computation GRU based RNNLM is use n-gram based MaxEnt bypass connections input output layers [16].
 MaxEnt scheme helps main- taining small size hidden layer signiﬁcant reduction recognition accuracy.
 types parallel models, main network consisting GRUs NCE, other MaxEnt bypass connections, operate ensemble model improve overall recognition accuracy.
 order reduce computational overhead bypass connections, implemented hash-based MaxEnt.
 method requires load- ing large hash table proportional number n-grams, retrieve probability given n-gram constant time.
 Fig.

 Proposed on-the-ﬂy rescoring cache quantized history vectors.
 On-the-ﬂy rescoring cache process ﬂow diagram baseline CPU-GPU hybrid RNNLM rescoring is shown Figure
 LM queries same history following words are deduplicated applying cache strategy start rescoring procedure [9].
 dedupli- cation, embedding vectors corresponding indices are retrieved using “Index Table”.
 RNNLM computations are performed appropriate values CPU memory.
 results calculations are converted indices, cached, returned graph traversals.

 QUANTIZATION OF HISTORY VECTORS cache-based strategy deduplicating LM queries accelerated baseline RNNLM rescoring cache hit ratio more times reduction computation time [9].
 However, is room improvement extending caching strategy outputs GRU hidden layers.
 current GRU hidden layer outputs computed based previous GRU hidden layer outputs (history vectors) be shared similar LM queries.
 Therefore, order reuse pre- computed history vectors, created cache vectors computing RNNLMs shown Figure
 key cache is GRU input is pair word embedding history vector, value cache is GRU hidden layer output corresponding input.
 number unique computa- tions is reduced assuming close history vectors result similar GRU hidden layer outputs, negligible effect overall ASR results.
 Euclidean distance be easy way measure similarity [11], require lot compu- tations slow whole rescoring process.
 Instead, propose quantize history vectors controlling precision history vector rounding speciﬁed decimal point.
 consider extreme case, store signs element, capture latent meanings hidden layers represent.
 Table shows possible reduction computations Table
 Redundancy rates quantized history vectors.
 Redundancy rate Precision round-2 round-1 sign Count % % % % four-second utterance.
 (Note element history vector ranges -1 term “Precision” refers quantization history vectors speciﬁed decimal place.
 initial deduplication, baseline system, have unique LM queries be observed ﬁrst row Table
 “round- row shows computations be reduced caching history vectors rounded second decimal place.
 Rounding history vectors ﬁrst decimal place shows is further redundancy.
 However, shown last row Table extreme case quantization signs element are stored, were able reduce computations.
 huge reduction affect accu- racy RNNLM results extent extreme sign quantization are possible unique history vectors hidden layer size H = is worthy evaluate effect ASR systems.

 CPU-GPU HYBRID DEPLOYMENT OF RNNLM COMPUTATION described Section proposed RNNLM model cannot be deployed GPU processor due large memory re- quirement.
 word embedding step input layer requires space proportional size vocabulary, MaxEnt step output maintain large hash table store n-grams corresponding scores.
 NCE step output layer requires loading NCE weight matrix proportional size vocabulary.
 other hand, hidden layer oc- cupies ﬁxed amount memory needs large number computations instead.
 Table
 Operation times RNNLM computation step seconds.
 Processor CPU GPU GPU Data transfer Count Unit LM Query Frame Hidden Output Layer Layer Time ﬁrst row Table shows proﬁling result RNNLM computation single layer GRU nodes based three- second utterance.
 is expected, hidden layer takes overall computation, aim reduce section.
 high computational rates neural networks are accelerated utilization GPUs, high memory requirements word embeddings MaxEnts prevent doing so.
 deploy hidden layer part computation GPUs keep input embedding output layer computations CPU side, shown Figure
 be observed second row Table hybrid deployment reduces computation time hidden layer one-third was done CPU alone.
 Fig.

 Proposed GPU based RNNLM rescoring frame-wise batch data transfer.
 However, method introduces setback.
 middle layer RNNLM computations was deployed GPU side, surrounding layers are computed CPUs, information needs be shared heterogeneous pro- cessor units frequently.
 number data exchanges increases, decoding speed hybrid ASR system decreases.
 second row Table shows have been more hundred data exchanges utterance, delayed overall computation seconds, is original utterance.
 frequency data transfers CPUs GPUs affects decoding speed data size transfer.
 propose method reduce number data copies CPUs GPUs concatenating needed information block frame.
 batching step, history vectors next word embeddings are emitted frame are stored consecutive CPU memory block, whole data block is transferred GPU memory once.
 GRU outputs GPU are copied output layer com- putation data block.
 effect be observed last row Table data transfer time is reduced original.
 addition, approach works multi-GPU en- vironments additional operations distributing block GPUs hidden layer calculations segment CPU memory block are related other.

 EXPERIMENTS Experimental setup LMs experiments were trained training corpus LibriSpeech [18].
 compare performance n-grams, “4- gram full LM” LibriSpeech was used.
 vanilla-RNNLMs GRU-RNNLMs consisted single hidden layer 4-gram based MaxEnt connections.
 vocabularies used RNNLMs were same “4-gram full LM” (V =
 bi-directional re- current deep neural network (RDNN) based AM hidden Table
 Performances LibriSpeech’s test sets; evaluations were performed same decoding options.
 LM dev-clean test-clean dev-other test-other Processor Rescoring threads Precision 4-gram full GRU-RNNLM (H = CPU CPU GPU (baseline) round-2 round-1 sign WER RTF WER RTF WER RTF WER RTF long short term memory (LSTM) layers (500 nodes layer), softmax output layer was trained using hours transcribed in-house English speech data consist- ing voice commands dialogs.
 WFSTs were compiled 2-gram LMs, epsilon transitions were removed computations GPUs be optimized.
 hardware speciﬁcation evaluations was Intel Xeon E5-2680 physical CPU cores Nvidia Tesla K80 GPUs equipped GB memory.
 used CUDA GPU par- allelization.
 CUBLAS, is linear algebra library CUDA, was used matrix multiplications kernel functions were im- plemented simple operations such element-wise op- erations.
 RNNLM computations CPUs such output layer computations, used EIGEN is C++ based linear algebra library.
 Results perplexity 4-gram LM.
 tasks “dev-other,” GRU-RNNLM size showed lowest LM perplexities.
 Table shows word error rate (WER) real-time factor (RTF) proposed methods accelerating online RNNLM rescoring.
 decoding options mentioned Table are same methods being compared.
 meanings values column “Precision” are same Table
 Regarding recog- nition average WER baseline system was im- proved 4-gram LM based system be observed ﬁrst rows Table
 expected Section caching quantized history vectors rounded ﬁrst second decimal points did show noticeable improvement recognition speed compared baseline system.
 However, proposed quantization strategy caching only signs history vectors was times compared baseline system with- accuracy degradations.
 shown ﬁfth sixth rows Table pro- posed GPU parallelization method, thread was times average fastest CPU based system (sign).
 recognition speed improves use multiple GPUs. particular, number GPUs increased two, speed was improved, was times sin- gle GPU-based system.
 GPUs were utilized, attained real-time speech recognition test cases.
 RNNLM-based ASR system GPUs has shown fastest average recognition speed RTF.
 was times fastest CPU-based system times baseline system.
 Fig.

 Perplexities depending LM types.

 CONCLUSION experiments, LibriSpeech’s development test cases were used evaluations.
 performance different LMs mea- sured terms perplexity is shown Figure
 term “other” evaluation cases means speech data sets were recorded noisy environments.
 be seen Figure vanilla- RNNLM size showed worst accuracies sets was worse 4-gram LM.
 accuracy vanilla-RNNLM improved hidden layer size showed lowest perplexities, worse 128- size GRU-RNNLM.
 Perplexities GRU-RNNLMs were dropped absolute (averaged cases) model sizes respectively, compared devised RNNLM based on-the-ﬂy rescoring CPU GPU platforms introducing lossy compression strat- egy history vectors novel hybrid parallelization method.
 cache hit ratios got higher lowering decimal precisions vectors, speech recognition was speeded times.
 was signiﬁcant improvement, fact recognition rates were affected dimension history vectors was stored bit representing sign seemed provide clue efﬁcient compression way embedding vectors minimizing loss information.
 Finally, CPU-GPU hybrid par- allelization method, decoding speed cases has fallen real-time.
 [16] T.
 Mikolov, A.
 Deoras, D.
 Povey, L.
 Burget, J.
 Cernock´y, “Strategies training large scale neural network language models,” Proc.
 pp.

 [17] X.
 Chen, X.
 Liu, M.
 Gales, P.
 Woodland, “Recurrent neu- ral network language model training noise contrastive es- timation speech recognition,” Proc.
 ICASSP, pp.

 [18] V.
 Panayotov, G.
 Chen, D.
 Povey, S.
 Khudanpur, “Lib- rispeech: ASR corpus based public domain audio books,” Proc.
 ICASSP, pp.


 REFERENCES [1] Y.
 Bengio, R.
 Ducharme, P.
 Vincent, C.
 Jauvin, “A neural probabilistic language model,” Journal Machine Learning Research, vol.
 pp.

 [2] T.
 Mikolov, M.
 Karaﬁat, L.
 Burget, J.
 Cernocky, S.
 Khu- danpur, “Recurrent neural network based language model,” Proc.
 Interspeech, pp.

 [3] S.
 Kombrink, T.
 Mikolov, M.
 Karaﬁat, L.
 Burget, “Re- current neural network based language modeling meeting recognition,” Proc.
 Interspeech, pp.

 [4] O.
 Tilk T.
 Alum¨ae, “Multi-domain recurrent neural net- work language model medical speech recognition,” Proc.
 Human Language Technologies, vol.
 pp.

 [5] L.
 Gwnol M.
 Petr, “Conversion recurrent neural net- work language models weighted ﬁnite state transducers automatic speech recognition,” Proc.
 Interspeech, pp.

 [6] E.
 Arisoy, S.
 Chen, B.
 Ramabhadran, A.
 Sethy, “Con- verting neural network language models back-off language models efﬁcient decoding automatic speech recognition,” IEEE Transactions Acoustics, Speech, Signal Process- ing, vol.
 no.
 pp.

 [7] Y.
 Si, Q.
 Zhang, T.
 Li, J.
 Pan, Y.
 Yan, “Preﬁx tree based n-best list re-scoring recurrent neural network language Proc.
 Inter- model used speech recognition system,” speech, pp.

 [8] T.
 Hori, Y.
 Kubo, A.
 Nakamura, “Real-time one-pass decoding recurrent neural network language model speech recognition,” Proc.
 ICASSP, pp.

 [9] K.
 Lee, C.
 Park, I.
 Kim, N.
 Kim, J.
 Lee, “Applying gpgpu recurrent neural network language model based network search real-time lvcsr,” Proc.
 Interspeech, pp.

 [10] Z.
 Huang, G.
 Zweig, B.
 Dumoulin, “Cache based recurrent neural network language model inference ﬁrst pass speech recognition,” Proc.
 ICASSP, pp.

 [11] X.
 Liu, X.
 Chen, Y.
 Wang, M.
 Gales, P.
 Woodland, “Two efﬁcient lattice rescoring methods using recurrent neural net- work language IEEE/ACM Trans.
 Audio, Speech Language Processing, vol.
 no.
 pp.

 [12] X.
 Chen, Y.
 Wang, X.
 Liu, M.
 Gales, P.
 Woodland, “Ef- ﬁcient gpu-based training recurrent neural network lan- guage models using spliced sentence bunch,” Proc.
 INTER- SPEECH, pp.

 [13] J.
 Kim, J.
 Chong, I.
 Lane, “Efﬁcient on-the-ﬂy hypothesis rescoring hybrid gpu/cpu-based large vocabulary contin- uous speech recognition engine,” Proc.
 INTERSPEECH, pp.

 [14] J.
 Chung, C¸
 G¨ulc¸ehre, K.
 Cho, Y.
 Bengio, “Empiri- cal evaluation gated recurrent neural networks sequence modeling,” CoRR, vol.

 [15] M.
 Gutmann A.
 Hyv¨arinen, “Noise-contrastive estimation unnormalized statistical models, applications natural image statistics,” Journal Machine Learning Research, vol.
 pp.

 Cerebrovascular diseases are common causes morbidity mortality adult population worldwide1-3.
 Most cerebrovascular diseases are found routine brain imaging CT MRI, 2D-DSA remains gold standard accurate angiographic evaluation characterization, particular arteriovenous malformations cerebral aneurysms dural arteriovenous fistulas
 Additional rotational angiography (3D-DSA) is used improve visualization spatial understanding vascular structures diagnostic work-up conditions.
 Currently, many angiographic systems, obtaining 3D-DSA requires rotational acquisitions; injection contrast (mask run) injection contrast (fill run).
 datasets are used compute log-subtracted projections are used reconstruct subtracted 3D-DSA volume
 Machine learning is discipline computer science, related statistics mathematical optimization, aims learn patterns large set examples demonstrate desired outcome behavior need explicit instructions
 context medical imaging, machine learning methods have been investigated computed aided detection diagnosis mammography pulmonary embolism 11-14, recent advances deep learning (i.e. specific machine learning technique) have demonstrated unprecedented performance many applications, including detections diabetic retinopathy breast cancer quantitative analysis brain tumors MRI computed-aided detection cerebral aneurysms MR angiography21 computer aided detection classification thoracic diseases
 recent advances deep learning universal approximation properties feedforward neural networks is hypothesized deep neural network is capable computing cerebral angiograms vascular information contained fill scan 3D-DSA exam acquired C-arm cone beam CT system.
 possible, potential benefits eliminating mask scan include reduction inter- sweep patient motion artifacts caused mis-registration mask fill scans radiation dose reduction least factor
 purpose work was develop test capability deep learning angiography (DLA) method, based convolutional neural networks (CNN), generate subtracted cerebral angiograms single contrast-enhanced exam need mask acquisition.
 Methods following sections, patient inclusion criteria image acquisition protocols are first presented, followed description datasets methods used train DLA model.
 image analysis statistical analysis are described.
 overall study schema is shown Figure
 Patient Cohort All studies were HIPAA compliant done Institutionally Review Board approved protocol.
 indicated rotational angiography exams assessment cerebrovascular abnormalities patients, scanned August April were collected.
 Cases were selected random fashion reduce potential bias patient selection.
 was thought randomized selection period result data set was representative varieties conditions are referred angiographic studies.
 Imaging Acquisition Reconstruction All subjects were imaged standard 3D-DSA data acquisition protocol using C- arm cone beam CT system (Axiom Artis zee; Siemens Healthineers).
 protocol consists acquisition cone beam CT acquistions(i.e. mask fill acquisitions) projection images second rotation time respectively.
 Angular coverage data acquisition is tube potential kVp, detector dose projection image equal µGy frame angular increments º frame.
 Iodinated contrast medium was injected proximal internal carotid artery vertebral artery initiation fill acquisition.
 subject, “native fill” subtracted volumes were reconstructed using vendor’s proprietary software (InSpace Reconstruction, syngo Workplace; Siemens Healthineers).
 reconstructions were performed using standard filtered backprojection edge enhancement, normal image characteristic, full field view (238x238 mm2) image matrix, mm image thickness/increment mm isotropic voxel size.
 effective dose acquisition protocols used study is mSv 6-second rotation acquisition mSv 13-second rotation acquisition is similar dose level reported others26,
 Training Dataset A training dataset consisting axial images patients labeled voxels was generated using information cone beam CT image fill scan subtracted images subtracted cone beam projection data.
 patient training dataset, vasculature extraction was performed manual thresholding subtracted images.
 selection threshold was based subjective assessment complete vasculature segmentation excluding image artifacts background noise threshold values range 500-700 HU.
 Large vessels, internal carotid artery (ICA), middle cerebral artery (MCA), cerebral artery (ACA), distal branches MCA ACA, vertebral artery (VA) posterior cerebral artery (PCA) were isolated three-dimensional connected component analysis
 Small regions connected large vessel were assumed be image artifacts were excluded final vasculature volume.
 previous steps, event remaining inter-sweep patient motion streak artifacts, vasculature volume was subject manual artifact removal.
 extraction bone tissue was performed subtracting vasculature volume contrast-enhanced images (i.e. fill scan) performing manual thresholding connectivity analysis (similar vasculature extraction) resulting images.
 connected regions including skull mandible were considered be bone.
 Remaining streaking artifacts metal implants bone volume were removed.
 soft tissue class was extracted thresholding fill images thresholds [-400,500] HU applying morphologic erosion.
 procedure described generates million, million, voxels vasculature, bone, soft tissue, patient.
 order mitigate class imbalance different number labeled voxels tissue class) reduce redundant training data similarity adjacent voxels, labeled voxels patient were included training, consisting vasculature voxels equal number extracted bone soft tissue voxels (i.e. random undersampling)
 Validation Testing Datasets validation dataset testing dataset were created using remaining image volumes subjects divided exams validation dataset exams testing dataset.
 datasets were created same procedure used generate training dataset, tissue labels were constrained region containing following anatomy: ICA, MCA, ACA, distal branches MCA ACA, VA, PCA, base anterior aspect skull, temporal bone, otic capsule surrounding soft tissue opposed entire head training dataset.
 exam validation testing dataset had same number labeled voxels tissue class.
 Neural Network Architecture Implementation A 30-layer convolutional neural network (CNN) ResNet architecture shown Figure was used.
 convolutional layers input layer, use filters rectified linear units activation function.
 input network is 41x41x5 volumetric image patch extracted contrast-enhanced image volume; network output consists 3-way connected layer softmax activation.
 Training inference is performed voxel-wise basis input volumetric image patch is labeled tissue class central voxel.
 DLA model was implemented using TensorFlow (Google Inc, Mountain View, CA).
 Network parameters were initialized using variance scaling trained scratch using synchronous stochastic gradient descent method batch size volumetric image patches using GTX Ti (NVIDIA, Santa Clara, CA) GPUs (256 image patches GPU).
 time required process case study varies minutes, depending size image volume.
 account class imbalance, tissue class had equal probability being included single batch (i.e. data re-sampling)
 learning rate was set be 1x10-3 momentum learning rate was reduced 1x10-4 1x10-5 epochs respectively.
 validation dataset was used monitor convergence generalization error model training.
 stopping was used validation error reached plateau iterations.
 Statistical Analysis trained DLA model was applied task tissue classification the, validation testing cohorts consisting image volumes subjects respectively.
 final vasculature tissue class was used generate 3D-DLA images.
 quantify generalization error trained model, vasculature classification was evaluated labeled voxel reference standard validation testing datasets.
 Two-by-two tables were generated patient accuracy, sensitivity (also known recall), positive predictive value (also known precision) dice similarity coefficients (also known F1-score) were calculated.
 CIs performance metric were reported.
 clinical 3D-DSA 3D-DLA images were subject qualitative assessment presence inter-sweep motion artifacts results were expressed frequencies percentages.
 Results Contrast-enhanced image volumes subjects (age ± years) underwent indicated rotational angiography exams assessment cerebrovascular abnormalities were used study.
 Contrast medium was injected proximal ICA patients (85%) injected VA patients (15%).
 Average CIs vasculature classification sensitivity, positive predictive value Dice similarity coefficient testing dataset were ([98.3, %), ([96.5, %), ([97.6, %), ([97.4, %) respectively.
 Table summarizes performance metrics vascular classification training, validation testing datasets.
 residual signal osseous structures was observed testing cases generated using 3D-DLA small regions otic capsule nasal cavity compared (23/62) 3D-DSA cases presented residual bone artifacts.
 Figure shows comparison MIP images derived 3D-DLA datasets patient evaluated posterior circulation.
 see residual bone artifacts induced inter-sweep patient motion are reduced 3D-DLA, improving conspicuity small vessels.
 Similarly, Figure shows lateral oblique MIP images derived 3D-DLA datasets patient evaluated anterior circulation.
 Results show reduced residual bone artifacts 3D-DLA images, particular anterior aspect skull temporal bone.
 Figure shows comparison volume rendering images clinical 3D-DSA patient small aneurysm anterior communicating artery large aneurysm MCA bifurcation.
 Discussion work, deep CNN was used learn generic opacified vasculature contrast enhanced C-arm cone beam CT datasets order generate cerebral angiogram, explicit definition cerebrovascular diseases specific vascular anatomy.
 datasets used model training, validation testing, were created applying simple image processing techniques minimum manual editing total subtracted contrast-enhanced cone beam CT images subjects.
 proposed DLA method is used improve image quality reducing image artifacts caused mis-registration mask fill scans 3D-DSA addition enabling potential radiation dose reduction.
 Many angiographic systems require rotational acquisitions (mask fill) reconstruction subtracted 3D-DSA.
 Others, use vascular segmentation thresholding allow vessel reconstruction be done availability mask.
 require rotations are susceptible artifacts caused potential miss-registrations mask fill projections.
 require use segmentation thresholding algorithms be subject errors related little contrast intensity and/or improper segmentation.
 Together, techniques remain standard care diagnosis treatment planning cerebrovascular diseases.
 Mis-registration artifacts arise conventional 3D-DSA imaging due small variations angular range differences occurring rotational acquisition potential patient motion mask fill runs.
 mask-free DLA method, eliminating need rotational acquisitions, theory, reduce chance motion mechanical instability patient motion reduces radiation dose required obtain 3D-DSA half systems require rotations.
 context medical imaging, machine learning methods have been investigated early 1990s11-14, recent unprecedented performance deep learning, has made major advances solving difficult problems science, were thought be intractable approached other means
 addition clever mathematical techniques availability large annotated many authors recognize parallel computing capabilities GPUs have played key role success deep learning applications, providing accelerations compared multicore single core CPUs
 training procedure network used study took hours.
 training procedure have taken 4-5 weeks multicore CPU computation architecture was used, making application unpractical.
 training procedure is performed needs be done GPU computing is available medical imaging community accessible cloud computing services such Google Cloud Platform (GCP) Amazon Web Service (AWS).
 Also, many standard open-source libraries used deep learning applications, are optimized be used conjunction GPUs. Once parameters model have been process analyzing new data was used training model inference) be further optimized production.
 method proposed study, uses voxel-wise training inference input network is small image region voxels voxel interest.
 approach has multiple inference be parallelized, other words, classification multiple voxels be performed same time.
 time required analyze new case is proportional number voxels be classified entire head targeted ROI) number generation available GPUs. throughput particular research implementation CNN model used study is voxels/s/GPU.
 approach results large training dataset consisting labeled voxels derived axial images exams.
 addition large training dataset, is important have large testing cohort order assure good model generalization better reflects technique be used practice.
 Having large testing cohort, helps determine training dataset is large achive desire level performance.
 DLA images were created validation testing cases were subject quantitative qualitative image analysis, study has limitations: First, use specific image acquisition protocol reconstruction selective intra-arterial contrast media injection proximal internal carotid artery vertebral artery limit clincal applications.
 Fine tuning model clinical validations prospective reader studies are required generalize results vasculature other organ systems, complex uncommon vascular abnormalities, 3D-DSA studies acquired using different image acquisition protocols modalities (e.g. injection IV contrast media, 4D-DSA, MDCT, etc.).
 kind prospective reader studies overcome limitation current qualitative evaluation study single reader (Dr. Strother).
 Second, study specific type deep CNN layers, implemented ResNet architecture was used demonstrate DLA application.
 selection well-known 30-layer ResNet is due fact architecture won image contest computer scientists (i.e. ILSVRC classification task), outperformed other types networks such AlexNet, VGGNet GoogLeNet, task natural images classification using ImageNet data set.
 type network architecture has outperformed other type networks medical imaging classification tasks deeper models (i.e. increased number layers) having improved classification accuracy
 However, remains unknown other architectures be used DLA be advanteages disadvantages networks.
 Furthernmore, additional optimization fine tuning DLA model hyper-parameters (e.g. number layers, number hidden units layer, learning rate, regularization schemes, etc.) is required optimal online implementation compatibility clinical workflow.
 Third, metallic objects are subtracted 3D-DSA images were used create training dataset, small movements metallic implants (e.g. aneurysm clip coil mass) occur cardiac cycle are, case subtracted images, sufficient create enough mis-registration artifacts allow detection implant presence.
 situation, addition high x-ray attenuation proximity vasculature metallic implants result imitation final DLA images.
 presence high attenuating object (e.g. metal Onyx) is known be intrinsic limitation mask-free angiography (vendors provide method obtain 3D-DSAs mask offer ability perform mask fill acquisition situations metal objects are known be present) clinical implications need be addressed expert reader study.
 Conclusions DLA method based CNNs generates cerebral angiograms contrast enhanced C-arm cone beam CT mask data acquisition was developed.
 Results indicate proposed method reduce mis-registration artifacts induced inter-sweep patient motion and, eliminating need mask acquisition, reduces radiation dose future clinical angiography.
 Acknowledgments authors like thank Dr. John W.
 Garrett grateful technical editorial assistance.
 References
 Go AS, Mozaffarian D, Roger VL, al.
 Executive Summary: Heart Disease Stroke Statistics—2013 Update.
 Report American Heart Association
 Wiebers DO.
 Unruptured intracranial aneurysms: natural history, clinical outcome, risks surgical endovascular treatment.
 Lancet 2003;362:103-
 Mohr JP, Parides MK, Stapf C, al.
 Medical management interventional therapy unruptured brain arteriovenous malformations (ARUBA): multicentre, non-blinded, randomised trial.
 Lancet 2014;383:614-621
 Ogilvy CS, Stieg PE, Awad I, al.
 AHA Scientific Statement: Recommendations management intracranial arteriovenous malformations: statement healthcare professionals special writing group Stroke Council, American Stroke Association.
 Stroke
 Hacein-Bey L, Provenzale JM.
 Current imaging assessment treatment intracranial aneurysms.
 AJR Am J Roentgenol
 Anxionnat R, Bracard S, Ducrocq X, al.
 Intracranial Aneurysms: Clinical Value Digital Subtraction Angiography Therapeutic Decision Endovascular Treatment.
 Radiology
 Gandhi D, Chen J, Pearl M, al.
 Intracranial dural arteriovenous fistulas: classification, imaging findings, treatment.
 AJNR Am J Neuroradiol 2012;33:1007-
 Fahrig R, Fox AJ, Lownie S, al.
 Use C-arm system generate true three- dimensional computed rotational angiograms: preliminary vitro vivo results.
 American Journal Neuroradiology 1997;18:1507-1514
 Strobel N, Meissner O, Boese J, al.
 Imaging Flat-Detector C-Arm Systems.
 In: Reiser MF, Becker CR, Nikolaou K, al., eds.
 Multislice CT.
 Berlin, Heidelberg: Springer Berlin Heidelberg;
 Erickson BJ, Korfiatis P, Akkus Z, al.
 Machine Learning Medical Imaging.
 RadioGraphics 2017;37:505-515
 Chan H-P, B.
 LS-C, Berkman S, al.
 Computer-aided detection mammographic microcalcifications: Pattern recognition artificial neural network.
 Medical Physics
 Wu Y, Kunio D, L.
 GM, al.
 Computerized detection clustered microcalcifications digital mammograms: Applications artificial neural networks.
 Medical Physics
 Zhang W, Kunio D, L.
 GM, al.
 Computerized detection clustered microcalcifications digital mammograms using shift-invariant artificial neural network.
 Medical Physics
 Wang S, Summers RM.
 Machine learning radiology.
 Medical Image Analysis
 LeCun Y, Bengio Y, Hinton G.
 Deep learning.
 Nature
 Gulshan V, Peng L, Coram M, al.
 Development validation deep learning algorithm detection diabetic retinopathy retinal fundus photographs.
 JAMA
 Huynh BQ, Li H, Giger ML.
 Digital mammographic tumor classification using transfer learning deep convolutional neural networks.
 Journal Medical Imaging
 Antropova N, Q.
 HB, L.
 GM.
 deep feature fusion methodology breast cancer diagnosis demonstrated imaging modality datasets.
 Medical Physics print]
 Akkus Z, Galimzianova A, Hoogi A, al.
 Deep Learning Brain MRI Segmentation: State Art Future Directions.
 Journal Digital Imaging 2017;30:449-459
 Korfiatis P, Kline TL, Lachance DH, al.
 Residual Deep Convolutional Neural Network Predicts MGMT Methylation Status.
 Journal Digital Imaging
 Nakao T, Hanaoka S, Nomura Y, al.
 Deep neural network-based computer- assisted detection cerebral aneurysms MR angiography.
 Journal Magnetic Resonance Imaging [Epub print]
 Wang X, Peng Y, Lu L, al.
 ChestX-ray8: Hospital-Scale Chest X-Ray Database Benchmarks Weakly-Supervised Classification Localization Common Thorax Diseases.
 IEEE Conference Computer Vision Pattern Recognition (CVPR);
 Lakhani P, Sundaram B.
 Deep Learning Chest Radiography: Automated Classification Pulmonary Tuberculosis Using Convolutional Neural Networks.
 Radiology
 Cybenko G.
 Approximation superpositions sigmoidal function.
 Mathematics Control, Signals Systems
 Hornik K.
 Approximation capabilities multilayer feedforward networks.
 Neural Networks
 Struffert T, Hauer M, Banckwitz R, al.
 Effective dose patient measurements flat-detector multislice computed comparison applications neuroradiology.
 European Radiology
 Lang S, Gölitz P, Struffert T, al.
 DSA Dynamic Visualization Cerebral Vasculature: A Single-Center Experience Cases.
 American Journal Neuroradiology
 Shapiro LG.
 Connected Component Labeling Adjacency Graph Construction.
 Machine Intelligence Pattern Recognition
 H, Garcia EA.
 Learning Imbalanced Data.
 IEEE Transactions Knowledge Data Engineering 2009;21:1263-1284
 LeCun Y, Boser BE, Denker JS, al.
 Handwritten Digit Recognition Back- Propagation Network.
 Advances Neural Information Processing Systems; 1990:396--
 K, Zhang X, Ren S, al.
 Identity Mappings Deep Residual Networks.
 In: Leibe B, Matas J, Sebe N, al., eds.
 Computer Vision – ECCV European Conference, Amsterdam, Netherlands, October Proceedings, Part IV.
 Cham: Springer International Publishing;
 K, Zhang X, Ren S, al.
 Deep Residual Learning Image Recognition.
 IEEE Conference Computer Vision Pattern Recognition (CVPR);
 K, Zhang X, Ren S, al.
 Delving Deep Rectifiers: Surpassing Human- Level Performance ImageNet Classification.
 IEEE International Conference Computer Vision (ICCV);
 Silver D, Huang A, Maddison CJ, al.
 Mastering game Go deep neural networks tree search.
 Nature
 Silver D, Schrittwieser J, Simonyan K, al.
 Mastering game Go human knowledge.
 Nature
 Shin HC, Roth HR, Gao M, al.
 Deep Convolutional Neural Networks Computer-Aided Detection: CNN Architectures, Dataset Characteristics Transfer Learning.
 IEEE Transactions Medical Imaging 2016;35:1285-1298 Table
 Summary performance metrics vascular classification training, validation testing datasets.
 Sensitivity (recall) 𝑻𝑷𝑻𝑷+𝑭𝑵 PPV (precision) 𝑻𝑷𝑻𝑷+𝑭𝑷 DSC (F1-score) Dataset Accuracy 𝑻𝑷+𝑻𝑵 𝑻𝑷+𝑻𝑵+𝑭𝑷+𝑭𝑵 Mean CI Mean CI Mean CI Mean CI Validation (n=8) [96.9, [96.4, [97.0, [98.0, Testing [97.6, [96.5, [97.4, [98.3, Figure
 Overall study schema.
 Figure
 Neural Network Architecture.
 Figure
 Comparison anterior lateral views MIP images derived A) 3D- DSA B) 3D-DLA datasets patient evaluated posterior circulation.
 Residual bone artifacts induced inter-sweep patient motion are reduced 3D-DLA, improving conspicuity small vessels pointed white arrows.
 Figure
 Comparison lateral oblique MIP images derived A) B) 3D-DLA datasets patient evaluated anterior circulation.
 Results show reduced residual bone artifacts 3D-DLA images, particular anterior aspect skull temporal bone.
 Figure
 Comparison volume rendering images A) clinical B) 3D-DLA patient small aneurysm anterior communicating artery large aneurysm MCA bifurcation pointed arrows.
 Consider following problem: Given n-element set A ⊆ Bm := {0, class sets A hidden element ∈ A.
 Given oracle answers queries type: is value ai?”.
 polynomial time algorithm input A, asks minimum number queries oracle ﬁnds hidden element a.
 is equivalent constructing minimum height decision tree A.
 decision tree is binary tree internal node is labeled index [m] leaf is labeled assignment ∈ Bm. internal node has outgoing edges is labeled other is labeled
 node is labeled i corresponds query “Is ai
 edge is labeled ξ corresponds answer ξ.
 decision tree is algorithm obvious way height is worst case complexity number queries.
 decision tree T is said be decision tree algorithm corresponds T predicts hidden assignment ∈ A.
 goal is construct small height decision tree A ⊆ Bm time polynomial m n := |A|.
 denote OPT(A) minimum height decision tree A.
 problem is related following problem exact learning [1]: Given class C boolean functions f X → {0,
 Construct poly(|C|,|X|) time optimal adaptive algorithm learns C membership queries.
 learning problem is equivalent constructing minimum height decision tree j = fi(xj)} fi is ith function C xj is set A = {a(i)|a(i) jth instance X.
 computer vision problem is related minimizing number “probes” (queries) needed determine ﬁnite set geometric ﬁgures is present image [4].
 game theory problem is related minimum number turns required order win guessing game.
 Previous New Results [4], Arkin al.
 showed (AMMRS-algorithm) node deci- sion tree chooses i partitions current set (the set assignments are consistent answers queries possible, height tree is factor log |A| optimal.
 I.e., log |A|- approximation algorithm.
 Moshkov [14] analysis shows algorithm is (ln|A|)-approximation algorithm.
 algorithm runs polynomial time m |A|.
 Hyaﬁl Rivest, [11], show problem constructing minimum depth decision tree is NP-Hard.
 reduction Laber Nogueira, [12] set cover inapproximability result Dinur Steurer [6] set cover implies cannot be approximated factor (1 − · ln|A| P=NP.
 Therefore, better approximation ratio be obtained constraint is added set A.
 Moshkov, [13], studied extended teaching dimension combinatorial mea- sure, ETD(A), set A ⊆ Bm. is maximum possible assign- ments b ∈ Bm minimum number indices ⊂ [m] b agrees most ∈ A.
 Moshkov showed results.
 ﬁrst is ETD(A) is lower bound OPT(A).
 second is exponential time algorithm asks (2ETD(A)/ log ETD(A)) log n queries.
 gives (ln (ln|A|)/ log ETD(A) approximation (exponential time) algorithm (since OPT(A) ≥ ETD(A)) same time (A)/ log ETD(A)-approximation algorithm (since OPT(A) ≥ log |A|).
 many interesting classes have small ETD dimension, latter re- sult gives small approximation ratio Moshkov algorithm runs exponential time.
 paper further study ETD measure.
 show poly- nomial time (1 − o(1))ETD(C)-approximation algorithm implies P=NP.
 There- fore, Moshkov algorithm cannot run polynomial time P=NP.
 show above AMMRS-algorithm, [4], is polynomial time (ln 2)ETD(C)- approximation algorithm.
 gives small approximation ratio classes small extended teaching dimension.
 reason studying ETD classes is following: ﬁnd ETD set A either get lower bound is better information theoretic lower bound log |A| get approximation algorithm better ratio ln|A|.
 is ETD(A) < log |A| AMMRS-algorithm has ratio (ln is better ln|A| ratio ETD(A) > log |A| Moshkov lower bound, ETD(A), OPT(A) is information theoretic lower bound log |A|.
 get above results, deﬁne new combinatorial measure called density DEN(A) set A.
 Q = DEN(A) is subset B ⊆ A such adversary give answers queries eliminate most fraction number elements B.
 forces learner ask least Q queries.
 show ETD(A) ≥ DEN(A) −
 other hand, show Q = DEN(A) query AMMRS-algorithm eliminates least (1 − fraction assignments A.
 gives polynomial time (ln 2)DEN(A)-approximation algorithm is (ln + 1)- approximation algorithm.
 order compare show (ETD(A) − ln|A| ≤ DEN(A) ≤ ETD(A) + random uniform (and therefore A), high probability DEN(A) = Θ(ETD(A)/ ln|A|).
 |A| > ETD(A), shows AMMRS-algorithm get better approximation ratio Moshkov algorithm.
 inapproximability results follows reduction Laber Nogueira, [12] set cover inapproximability result Dinur Steurer [6] fact DEN(A) ≤ ETD(A) + ≤ OPT(A) +
 apply above results learning class disjunctions pred- icates set predicates F membership queries [5].
 show ETD class is bounded degree d Hasse diagram.
 show Moshkov algorithm, class, runs polynomial time is (d/ log d)-approximation algorithm.
 |F| ≥ d (and many applications, |F| (cid:29) d), improves |F|-approximation algorithm SPEX [5] size Hasse diagram is polynomial.
 gives optimal algorithms degree d is constant.
 example, learning axis parallel rays constant dimension space.
 Deﬁnitions Preliminary Results section give deﬁnitions preliminary results Notation Let Bm = {0,
 Let A = {a(1),


 a(n)} ⊆ Bm be n-element set.
 write |A| number elements A.
 h ∈ Bm deﬁne + h = {a + h|a ∈ A} + (in square brackets) is bitwise exclusive elements Bm. integer q let [q] = {1,


 q}.
 paper, log x = log2 x.
 Optimal Algorithm denote OPT(A) minimum depth decision tree A.
 goal is build decision tree A small depth.
 n := |A|.
 log ≤ OPT(A) ≤ n following result is easy prove (see Appendix A) Lemma
 have OPT(A) = OPT(A + h).
 Extended Teaching Dimension section deﬁne extended teaching dimension.
 Let ∈ Bm be element.
 say set S ⊆ [m] is specifying set h respect |{a ∈ | (∀i ∈ S)hi = ai}|
 is, is most element A is consistent h entries S.
 Denote ETD(A, h) minimum size specifying set h respect A.
 extended teaching dimension A is ETD(A) = max h∈Bm ETD(A, h).
 (2) write ETDz(A) ETD(A,
 is easy see ETD(A, h) = ETDz(A + h) ETD(A) = ETD(A + h).
 say set S ⊆ [m] is strong specifying set h respect h ∈ |{a ∈ A | (∀i ∈ S)hi = ai}| = |{a ∈ | (∀i ∈ S)hi = ai}|
 is, h ∈ is element A is consistent h entries S.
 Otherwise, element A is consistent h S.
 Denote SETD(A, h) minimum size strong specifying set h respect A.
 strong extended teaching dimension A is SETD(A) = max h∈Bm SETD(A, h).
 (4) write SETDz(A) SETD(A,
 is easy see SETD(A, h) = SETDz(A + h) SETD(A) = SETD(A + h).
 (5) Obviously, ETD(A, h) ≤ min(m, n−1) ETD(A, h) ≤ SETD(A, h) ≤ min(m, show Lemma
 have ETD(A, h) ≤ SETD(A, h) ≤ ETD(A, h) + ETD(A) ≤ SETD(A) ≤ ETD(A) +
 Proof.
 fact ETD(A, h) ≤ SETD(A, h) follows deﬁnitions.
 Let S ⊆ [m] be specifying set h respect A.
 T := {a ∈ A | (∀i ∈ S)hi = have t := |T| ≤
 t = h ∈ S is strong specifying set h respect A.
 t = h element ∈ T is j ∈ [m] such aj (cid:54)= hj S ∪ {j} is strong specifying set h respect A.
 proves SETD(A, h) ≤ ETD(A, h)
 other claims follows immediately.
 Obviously, B ⊆ A ETD(B) ≤ ETD(A), SETD(B) ≤ SETD(A).
 Hitting Set section deﬁne hitting set A.
 hitting set A is set S ⊆ [m] such non-zero element ∈ A is j ∈ S such aj
 is, S hits element A element (if exists).
 size minimum size hitting set is denoted HS(A).
 show Lemma
 have HS(A) = SETDz(A).
 particular, SETD(A, h) = HS(A+ h) SETD(A) = maxh∈Bm HS(A + h).
 Proof.
 ∈ SETDz(A) is minimum size set S such {a ∈ A | (∀i ∈ S)ai = = {0} (cid:54)∈ is minimum size set S such {a ∈ A | (∀i ∈ S)ai = = ∅.
 set S hits nonzero elements A.
 other results follow (5) deﬁnition SETD.
 Density Set section deﬁne new measure DEN set.
 Let A = {a(1),


 a(n)} ⊆ Bm. deﬁne MAJ(A) ∈ Bm such MAJ(A)i = number ones (a(1) is greater equal number zeros MAJ(A)i = otherwise.
 denote MAX(A) maximum number ones (a(1) i =


 m.
 Let ,··· a(n) ,··· a(n) MAMI(A) = min h∈Bm MAX(A + h) = MAX(A + MAJ(A)).
 (7) j ∈ [m] ξ ∈ {0, let Aj,ξ = {a ∈ A | aj = ξ}.
 MAMI(A) = max min(|Aj,0|,|Aj,1|).
 deﬁne density set A ⊆ Bm DEN(A) = max B⊆A |B| − MAMI(B) (8) (9) Notice j ∈ [m] hit most MAX(A) elements have HS(A) ≥ |A| − MAX(A) (10) Bounds OPT section give upper lower bounds OPT.
 Lower Bound Moshkov results [10, information theoretic bound (1) give following lower bound.
 give proof Appendix A completeness.

 Let A ⊆ Bm be set.
 OPT(A) ≥ max(ETD(A), log |A|).
 Many lower bounds literature OPT(A) are based ﬁnding subset B ⊆ A such query is answer eliminates most small fraction E B.
 is lower bound OPT(A).
 best possible bound get using technique is DEN(A) (Lemma density deﬁned Section Lemma shows lower bound ETD(A) OPT(A) exceeds such bound.
 Appendix A prove Lemma
 have OPT(A) ≥ DEN(A).

 have ETD(A) ≥ DEN(A) −
 Proof.
 (7) (9) is B ⊆ A such DEN(A) = |B| − MAMI(B) |B| − MAX(B + h) h MAJ(B).
 (11) ETD(A) (2)≥ ETD(B, h) (6)≥ ETD(B) L2≥ SETD(B, h) L3= HS(B + h) (10)≥ |B| − MAX(B + h) (11) = DEN(A) − Appendix A prove Lemma
 have ETD(A) ≤ ln|A| · DEN(A) +
 is easy see (by standard analysis using Chernoﬀ Bound) random uniform A, positive probability, DEN(A) = O(1) ETD(A) = Θ(log |A|).
 See proof sketch Appendix A.
 bound Lemma is possible.
 Upper Bounds Moshkov [10, proved following upper bound.
 gave proof Appendix B completeness.

 Let A ⊆ {0, size n.
 OPT(A) ≤ ETD(A) + ETD(A) log ETD(A) log ≤ · ETD(A) log ETD(A) log n.
 [10,13], Moshkov gave example n-set AE ⊆ {0, ETD(AE) = E OPT(AE) = Ω((E/ log E) log n).
 upper bound above lemma is best possible.
 Polynomial Time Approximation Algorithm Given set A ⊆ Bm. Can construct algorithm ﬁnds hidden ∈ A OPT(A) queries?
 Obviously, unlimited computational power be done question is: close OPT(A) get polynomial time poly(m, n) is allowed construction?
 exponential time algorithm follows following OPT(A) = min i∈[m] max(OPT(Ai,0), OPT(Ai,1)) Ai,ξ = {a ∈ A | ai = ξ}.
 algorithm runs time least m! ≥ (m/e)m.
 See [3,
 give better exponential time algorithm?
 follows (Theo- rem use Moshkov result (Lemma give better exponential time approximation algorithm.
 Appendix B give simple proof Moshkov [10, result practice uses number specifying sets.
 extended teaching dimension is constant, algorithm is O(1)- approximation algorithm runs polynomial time.

 Let A be class sets A ⊆ Bm size n.
 is algorithm h ∈ Bm A ∈ A gives specifying set h respect size most E time T is algorithm A ∈ A constructs decision tree A depth most E + log E log ≤ E + log E OPT(A) queries runs time O(T log + nm).
 Proof.
 Follows Moshkov algorithm [10,
 See Appendix B.(cid:117)(cid:116) following result follows Theorem

 Let A ⊆ Bm be n-set.
 is algorithm ﬁnds hidden column time (cid:18) m (cid:19) · ETD(A) · log n asks most ETD(A) · ETD(A) · log log ETD(A) ≤ · min(ETD(A), log n) log ETD(A) OPT(A) queries.
 particular, ETD(A) is constant algorithm is O(1)-approximation algorithm runs polynomial time.
 Proof.
 ﬁnd specifying set h respect check ETD(A) row A.
 check takes time n.
 algorithm asks most ETD(A) · log queries, time complexity is stated Theorem.
 do poly(m, n) time?
 Hyaﬁl Rivest, [11], show problem ﬁnding OPT is NP-Complete.
 reduction Laber Nogueira, [12], set cover problem inapproximability result Dinur Steurer [6] set cover implies cannot be approximated (1− o(1))· ln n P=NP.
 [4], Arkin al.
 showed (the AMMRS-algorithm) ith query algorithm chooses index j partitions current node set (the ele- ments are consistent answers node) A possible, is, maximizes ∈ A|aj = ∈ A|aj = query complexity is factor (cid:100)log n(cid:101) optimal.
 AMMRS- algorithm, [4], runs time poly(m, n).
 Moshkov [4, analysis shows algorithm is ln n-approximation algorithm therefore is optimal.
 sec- tion give simple proof.
 Moshkov gave simple ETD(A)-approximation algorithm (Al- gorithm MEMB-HALVING-1 [10]).
 gave algorithm achieves query complexity Lemma (Algorithm MEMB-HALVING-2 [10]).
 is factor · min(ETD(A), log n) log ETD(A) optimal.
 is better ratio ln n, but, unfortunately, algo- rithms require ﬁnding minimum size specifying set problem ﬁnding minimum size specifying set h is NP-Hard, [2,
 achieve O(ETD(A))-approximation.
 following give sur- prising result.
 show AMMRS-algorithm is (ln 2)ETD(A)-approximation algorithm.
 show better ratio be achieved P=NP.

 AMMRS-algorithm runs time O(mn) ﬁnds hidden element ∈ A most DEN(A) · ln(n) ≤ min((ln ln n) · OPT(A) ≤ min((ln + ln n) · OPT(A) queries.
 Proof.
 Let B be subset A.
 Then, DEN(B) (9)≥ |B| − MAMI(B) MAMI(B) ≥ |B| − DEN(B) ≥ |B| − DEN(A) AMMRS-algorithm chooses node decision index j maximizes min(|Bj,0|,|Bj,1|) Bj,ξ = {a ∈ B|aj = ξ} B is set elements are consistent answers node, have max(|Bj,0|,|Bj,1|) = |B| − − min(|Bj,0|,|Bj,1|) (8) = |B| − − MAMI(B) ≤ (|B| − (cid:18) − DEN(A) (cid:19) Therefore, node v depth h decision tree, set B(v) elements are consistent answers node contains most (cid:19)h + (cid:18) (|A| − − DEN(A) elements.
 depth tree is most DEN(A) ln|A|.(cid:117)(cid:116) show approximation algorithm is optimal Theorem
 Let be constant.
 is polynomial time algorithm ﬁnds hidden element less (1 − )DEN(A) · ln|A| P=NP.
 Proof.
 Suppose algorithm exists.
 (1 − )DEN(A) ln|A| L5≤ (1 − ) ln|A|OPT(A).
 is, algorithm is (1 − ) ln|A|-approximation algorithm.
 Laber Nogueira, [12] gave polynomial time algorithm reduction minimum depth decision tree set cover Dinur Steurer [6] show is poly- nomial time (1 − · ln|A| set cover P=NP.
 Therefore, (cid:117)(cid:116) algorithm implies P=NP.
 Applications Disjunction Predicates section apply above results learning class disjunctions predicates set predicates F membership queries [5].
 Let C = {f1,


 be set boolean functions fi X → {0, X = {x1,


 xm}.
 Let AC = {(fi(x1),


 fi(xm)) | i =


 n}.
 write OPT(AC), ETD(AC), etc.
 OPT(C), ETD(C), etc.
 Let F be set boolean functions (predicates) domain X.
 consider class functions F∨ := {∨f∈Sf | S ⊆ F}.
 Equivalence Relation F∨ section, present equivalence relation F∨ deﬁne repre- sentatives equivalence classes.
 enables later sections focus representative elements F∨.
 Let F be set boolean functions domain X.
 equivalence relation = F∨ is deﬁned follows: disjunctions F1, F2 ∈ F∨ are equivalent (F1 = F2) F1 is equal F2.
 other words, represent same function X {0,
 write F1 ≡ F2 denote F1 F2 are identical; is, have same representation.
 example, consider f1, f2 {0, → {0, f1(x) = f2(x) = x.
 Then, f1 ∨ f2 = f1 f1 ∨ f2 (cid:54)≡ f1.
 denote F∗ ∨ set equivalence classes = write equiv- alence class [F ], F ∈ F∨.
 Notice [F1] = [F2], [F1 ∨ F2] = [F1] = [F2].
 Therefore, [F choose representative element be GF := ∨F (cid:48)∈SF (cid:48) S ⊆ F is maximum size set satisﬁes ∨S := ∨f∈Sf = F
 denote G(F∨) set representative ele- ments.
 Accordingly, G(F∨) = {GF | F ∈ F∨}.
 example, consider set F consisting functions f11, f12, f21, f22 {1, → {0, fij(x1, x2) = [xi ≥ j] [xi ≥ j] = xi ≥ j otherwise.
 := F∨ ﬁve representative functions G(F∨): are = elements Ray2 G(F∨) = {f11∨f12∨f21∨f22, f12∨f22, f12, f22, is zero function).
 Partial Order F∨ Hasse Diagram section, deﬁne partial order F∨ present related deﬁnitions.
 partial order, denoted ⇒, is deﬁned follows: F1⇒F2 F1 implies F2.
 Consider Hasse diagram H(F∨) G(F∨) partial order.
 maximum (top) element diagram is Gmax := ∨f∈F f
 minimum (bottom) element is Gmin := ∨f∈∅f i.e., zero function.
 Figures shows illustration Hasse diagram.
 Hasse diagram, G1 is descendant (resp., ascendent) G2 is (nonempty) downward path G2 G1 (resp., G1 G2), i.e., G1⇒G2 (resp., G2⇒G1) G1 (cid:54)= G2.
 G1 is immediate descendant G2 H(F∨) G1⇒G2, G1 (cid:54)= G2 is G ∈ G(F∨) such G (cid:54)= G1, G (cid:54)= G2 G1⇒G⇒G2.
 G1 is immediate ascendant G2 G2 is immediate descendant G1.
 denote De(G) As(G) sets immediate descendants immediate ascendants G, respectively.
 neighbours set G is Ne(G) = De(G) ∪ As(G).
 further denote DE(G) AS(G) sets G’s descendants ascendants, respectively.
 Deﬁnition
 degree G is deg(G) = |Ne(G)| degree deg(F∨) F∨ is maxG∈G(F∨) deg(G).
 G1 G2, deﬁne lowest common ascendent (resp., greatest com- mon descendant) G = lca(G1, G2) (resp., G = gcd(G1, G2)) be minimum (resp., maximum) element AS(G1) ∩ AS(G2) (resp., DE(G1) ∩ DE(G2)).

 Let G1, G2 ∈ G(F∨).
 Then, lca(G1, G2) = G1 ∨ G2.
 G1 ∨ G2 = G.
 particular, G1, G2 are distinct immediate descendants G, following result is [5] Witnesses subsection deﬁne term witness.
 Let G1 G2 be elements G(F∨).
 element ∈ X is witness G1 G2 G1(a) (cid:54)= G2(a).
 class boolean functions C domain X function G ∈ C say set elements W ⊆ X is witness set G C G(cid:48) ∈ C G(cid:48) (cid:54)= G is witness W G G(cid:48).
 Extended Teaching Dimension F∨ section prove Lemma
 h X → {0, h (cid:59) Gmax ETD(F∨, h)
 Otherwise, is G ∈ G(F∨) such ETD(F∨, h) ≤ |De(G)| + HS(As(G) ∧ ¯G) ≤ |Ne(G)| = deg(G) As(G) ∧ ¯G = {s ∧ ¯G | s ∈ As(G)}.
 particular, (cid:0)|De(G)| + HS(As(G) ∧ ¯G)(cid:1) ≤ deg(F∨).
 ETD(F∨) ≤ max G∈G(F∨) Proof.
 Let h X → {0, be function.
 h (cid:59) Gmax is assignment satisﬁes h(a) = Gmax(a) =
 G ∈ G(F∨), G ⇒ Gmax have G(a)
 set {a} is specifying set h respect F∨ ETD(F∨, h)
 Let ⇒ Gmax.
 Consider G ∈ G(F∨) such h⇒G imme- diate descendant G(cid:48) G have h (cid:59) G(cid:48).
 immediate descendent G(cid:48) G ﬁnd assignment such G(cid:48)(a) = h(a)
 is witness h G(cid:48).
 Therefore, is witness h descendant G(cid:48).
 Let A be set such assignments, i.e., descendant G witness.
 |A| ≤ |De(G)| A is witness set h descen- dants G.
 note h = G = Gmin has immediate descendants A = ∅.
 Consider hitting set B As(G)∧ ¯G size HS(As(G)∧ ¯G).
 immediate ascendant G(cid:48)(cid:48) G ﬁnd assignment b ∈ B such G(cid:48)(cid:48)(b)∧ ¯G(b) =
 G(cid:48)(cid:48)(b) = G(b) =
 G(b) = have h(b) = b is witness h G(cid:48)(cid:48).
 Therefore, b is witness h ascendant G(cid:48)(cid:48).
 B is witness set h ascendants G.
 Let G0 be element G(F∨) (that is descendant ascendant).
 Consider G1 = lca(G, G0).
 Lemma have G1 = G ∨ G0.
 G1 is ascendent G is witness ∈ B such G1(a) = G(a) =
 G0(a) = h(a) = is witness h G0.
 ∪ B is specifying set h respect G(F∨).
 F ∈ F∨ have F = GF ∈ G(F∨), A ∪ B is specifying set h respect F∨.
 ETD(F∨, h) ≤ |A| + |B| ≤ |De(G)| + HS(As(G) ∧ ¯G) result follows.
 Appendix C show ETD(F∨) = max G∈G(F∨) (cid:0)|De(G)| + HS(As(G) ∧ ¯G)(cid:1)
 (cid:117)(cid:116) have replaced |De(G)| HS(De(G)∧ G), Lemma Appendix C shows are equal.
 following result follows proof Lemma Lemma
 h X → {0, specifying set h respect F∨ size deg(F∨) be found time O(nm).
 Theorem have Theorem
 is algorithm learns F∨ time O(nm) asks most (cid:18) deg(F∨) log deg(F∨) (cid:19) + OPT(F∨) deg(F∨) + deg(F∨) log deg(F∨) log n ≤ membership queries.
 Learning Other Classes specifying set small size cannot be found polynomial time Theorem Lemma have Theorem
 class C have
 is algorithm learns C time (cid:18) m (cid:19) · ETD(C) · log n deg(C) asks most · ETD(C) · log log ETD(C)) ≤ · min(ETD(C)), log n) log ETD(C)) OPT(C) membership queries.
 particular, ETD(C) is constant algorithm runs polynomial time query complexity is (asymptotically) optimal.

 is algorithm learns C time O(nm) asks most DEN(C) · ln(n) ≤ min((ln ln n) · OPT(C) ≤ min((ln + ln n) · OPT(C) membership queries.
 References
 Dana Angluin.
 Queries concept learning.
 Machine Learning,

 Martin Anthony, Graham R.
 Brightwell, David A.
 Cohen, John Shawe-Taylor.
 exact speciﬁcation examples.
 Proceedings Fifth Annual ACM Conference Computational Learning Theory, COLT Pittsburgh, PA, USA, July 27-29, pages

 Esther M.
 Arkin, Michael T.
 Goodrich, Joseph S.
 B.
 Mitchell, David M.
 Mount, Christine D.
 Piatko, Steven Skiena.
 Point probe decision trees geometric concept classes.
 Algorithms Data Structures, Third Workshop, WADS ’93, Montr´eal, Canada, August Proceedings, pages

 Esther M.
 Arkin, Henk Meijer, Joseph S.
 B.
 Mitchell, David Rappaport, Steven Int.
 J.
 Comput.
 Geometry Appl., Skiena.
 Decision trees geometric models.


 Nader H.
 Bshouty, Dana Drachsler-Cohen, Martin T.
 Vechev, Eran Yahav.
 Learning disjunctions predicates.
 Proceedings 30th Conference Learning Theory, COLT Amsterdam, Netherlands, 7-10 July pages

 Irit Dinur David Steurer.
 Analytical approach parallel repetition.
 Sym- posium Theory Computing, STOC New York, NY, USA, May June pages

 M.
 R.
 Garey.
 Optimal binary identiﬁcation procedures.
 SIAM Journal Applied Mathematics,

 A.
 Goldman Michael J.
 Kearns.
 complexity teaching.
 J.
 Comput.
 Syst.
 Sci.,

 A.
 Goldman, Ronald L.
 Rivest, Robert E.
 Schapire.
 Learning binary Annual Symposium relations total orders (extended abstract).
 Foundations Computer Science, FOCS pages

 Tibor Heged¨us.
 Generalized teaching dimensions query complexity learning.
 Proceedings Eigth Annual Conference Computational Learn- ing Theory, COLT Santa Cruz, California, USA, July 5-8, pages

 Laurent Hyaﬁl Ronald L.
 Rivest.
 Constructing optimal binary decision trees is np-complete.
 Inf.
 Process.
 Lett.,

 Eduardo Sany Laber Loana Tito Nogueira.
 hardness minimum height decision tree problem.
 Discrete Applied Mathematics,

 M.
 Y.
 Moshkov.
 conditional tests.
 Problemy Kibernetiki.
 Sov.
 Phys.
 Dokl.,

 Mikhail Ju. Moshkov.
 Greedy algorithm decision tree construction real data tables.
 pages

 Ayumi Shinohara.
 Teachability computational learning.
 New Generation Com- put.,

 Ayumi Shinohara Satoru Miyano.
 Teachability computational learning.
 Algorithmic Learning Theory, First International Workshop, ALT ’90.
 Appendix A Appendix give proof lemmas Lemma
 have OPT(A) = OPT(A + h).
 Proof.
 (A + h) + h = A, is enough prove OPT(A + h) ≤ OPT(A).
 given decision tree T A depth OPT(A).
 node, v, T labeled j, such hj = exchange labels outgoing edges.
 change label leaf labeled + h.
 is easy show new tree is decision tree A + h.

 Let A ⊆ {0, be set.
 OPT(A) ≥ max(ETD(A), log |A|).
 Proof.
 lower bound log |A| is information theoretic lower bound.
 prove other bound.
 Let T be decision tree A = {a(1),


 a(n)} minimum depth.
 Consider path P T level chooses edge is labeled
 Let S be set labels internal nodes P be label leaf P
 a(j) is only element A satisﬁes a(j) i = i ∈ S.
 S is specifying set respect A.
 OPT(A) ≥ |S| ≥ ETDz(A).
 Now, Lemma h ∈ {0, have OPT(A) = OPT(A + h) ≥ ETDz(A + h) = ETD(A, h) OPT(A) ≥ (cid:117)(cid:116) maxh ETD(A, h) = ETD(A).

 have OPT(A) ≥ DEN(A).
 Proof.
 Let B ⊆ A be set such |B| MAMI(B) MAX(B + MAJ(B)) |B| − DEN(A) (9) (7) query i ∈ [m] is “ai”?), adversary answers MAJ(B)i.
 eliminates most MAX(B+MAJ(B)) elements B.
 algorithm is forced ask least (|B| − + MAJ(B)) queries.
 (cid:117)(cid:116) Lemma have ETD(A) ≤ ln|A| · DEN(A) +
 Proof.
 is h0 ∈ such ETD(A) L2≤ SETD(A) (4) = SETD(A, h0) L3= HS(A + h0).
 C ⊆ have DEN(C) (9) = max B⊆C (7)≥ max B⊆C |B| − MAMI(B) |B| − MAX(B + h0) |C| − MAX(C + h0) therefore, C ⊆ have MAX(C + h0) ≥ |C| − DEN(C) (9)≥ |C + h0| DEN(A) consider following sequence subsets A + h0, C0, C1,


 Ct C0 = A + h0 subset Ci+1 is deﬁned Ci follows: (13) is true Ci is ji ∈ [m] such ji hits least (|Ci| − elements Ci. Ci+1 contains elements Ci are hit ji.
 |Ci+1| − ≤ |Ci| − |Ci| − DEN(A) Therefore |Ci| ≤ (|A| − (cid:18) − = (|Ci| − − DEN(A) − DEN(A) +
 Let Ct be ﬁrst set sequence satisﬁes Ct = ∅ Ct = {0}.
 Deﬁne X = {ji|i =


 t−1}.
 X is hitting set A+h0 size t.
 Therefore, (12) have (cid:19) (cid:18) (cid:19)i ETD(A) ≤ HS(A + h0) ≤ t ≤ ln give proof sketch (cid:16) ln(|A| − − DEN(A) (cid:17)−1 + ≤ DEN(A) · ln|A|
 (cid:117)(cid:116) Lemma
 is set A ⊆ Bm size n m = poly(n) such ETD(A) = Ω(log n) DEN(A) = O(1).
 Proof.
 Consider random uniform set A ⊆ Bm size n.
 probability are k = (log n)/2 entries i1,


 ∈ [m] such A satisﬁes (cid:19)(cid:18) ai1 = ai2 = ··· = aik = is (cid:18)m (cid:19)n ≤ − Therefore, probability least ETDz(A) ≥ k ETD(A) = Ω(log n).
 probability subset B ⊆ A size |B| > has MAMI(B) ≤ |B|/100 is most (cid:18) Therefore probability least MAMI(B) ≥ |B|/100 DEN(A) = (cid:117)(cid:116) O(1).
 Appendix B appendix give proof Lemma is same proof Lemma [10].
 Lemma [10, Let A ⊆ {0, size n.
 OPT(A) ≤ ETD(A) + ETD(A) log ETD(A) log ≤ · ETD(A) log ETD(A) log n.
 Proof.
 Consider algorithm Figure
 Step algorithm deﬁnes hypothesis is bitwise majority vectors A(i,1).
 Step index y is found maximizes size (cid:110) (cid:111) A(i,k) (y,fy) := g ∈ A(i,k) | gy = fy Suppose variable i (in algorithm) gets values


 ≤ i ≤ variable k gets values


 ki.
 number membership queries asked algorithm is k1 + ··· + kt.
 ﬁrst prove following Claim i =


 − have |A(i+1,1)| ≤ |A(i,1)| max(2, ki) Proof.
 S is specifying set h, y ∈ S satisﬁes hy (cid:54)= ay is only column A is consistent h S.
 Therefore, h = Majority(A(i,1)), have |A(i+1,1)| ≤ |A(i,1)| (14) Let D = A(i,1) D(cid:48) = A(i+1,1).
 Suppose y1,


 are queries were asked ith stage let δj = ayj j


 ki.
 D(cid:48) = D(y1,δ1),(y2,δ2),...,(yki ,δki (disjoint union) D = D(y1,¯δ1) ∪ D(y1,δ1),(y2,¯δ2) ∪ ··· ∪ D(y1,δ1),(y2,δ2)...,(yki−1,δki−1),(yki ,¯δki ∪ D(cid:48).
 Let D(j) = D(y1,δ1),(y2,δ2)...,(yj ,δj ), set columns D are consistent target column ﬁrst j assignments y1,


 yj.
 D = D(0) (y1,¯δ1) ∪ D(1) (y2,¯δ2) ∪ ··· ∪ D(ki−1) (yki ,¯δki ∪ D(cid:48).
 ≤ j ≤ ki − fact took yj+1 (j + query yki implies |D(j) )|.
 Therefore, ≤ j ≤ ki − (yj+1,hyj+1 )| ≤ |D(j) | = |D(j) (ykj ,hykj | ≥ |D(j) (yj+1,hyj+1 | = |D(j) (ykj ,δkj )| ≥ |D(cid:48)|.
 (ykj ,hykj |D(j) (yj+1,δj+1) Therefore |D| = |D(0) (y1,¯δ1)| + |D(1) (y2,¯δ2)| + ··· + |D(ki−1) (yki ,¯δki )| + |D(cid:48)| ≥ ki · |D(cid:48)|.
 result claim follows.
 (cid:117)(cid:116) Let zi = max(2, ki).
 therefore(cid:80)t−1 ≤ |A(t,1)| ≤ i=1 zi i=1 log zi ≤ log n.
 E ≥ E ≤ t(cid:88) t−1(cid:88) ki ki = kt + log zi i=1 i=1 log zi ≤ kt + max ≤ E + log E ki log log zi log n ≤ log E log n.
 is easy show above is true E =
 prove time complexity.
 Finding specifying set iteration loop takes time T number iterations most log n.
 takes T log n time.
 ﬁrst iteration deﬁne array length |S| ≤ E contains |A(i,1) (z,hz)| z ∈ S.
 takes most |A(i,1)| · E time.
 have such array A(i,k) (z,hz), ﬁnd y (in Step time E update array A(i,k+1) = A(i,k) (y,hy) time (y,hy)|·E.
 time Repeat loop is most · E.
 |A(i+1,1)| ≤ |A(i,1)|/2, time loop is most · E.
 gives result.
 Algorithm: Find hidden column ∈ A.

 i ← k ← A(1,1) ← A.

 |A(i,1)| ≥ do
 h ← Majority(A(i,1))
 specifying set S h respect A(i,1)
 Repeat
 (cid:12)(cid:12)(cid:12)A(i,k)
 (z,hz k ← k +
 Find ← arg minz∈S Ask query is ay”?
 A(i,k+1) ← A(i,k) S ← S\{y}.




 (hy (cid:54)= ay |A(i,k+1)| =
 A(i+1,1) ← A(i,k+1), i ← i + k ←
 End
 column A(i,k).
 (y,ay Fig.

 algorithm ﬁnd hidden column ∈ give proof Proof.
 Theorem Consider following algorithm.
 ith query, algorithm deﬁnes set Ai ⊆ A columns are consistent answers queries were asked far.
 Consider < 
 algorithm searches j ∈ [m] such |Ai| ≤ |{a ∈ Ai | aj ≤ (1 − )|Ai|.
 such j ∈ [m] exists algorithm asks is aj?”.
 Let answer be ξ.
 Deﬁne Ai+1 = {a ∈ Ai | aj = ξ}.
 Obviously, case, |Ai+1| ≤ (1 − )|Ai|.
 such j ∈ [m] exists algorithm ﬁnds specifying set Th h := Majority(Ai), “Majority” is bitwise majority function.
 asks queries is aj” j ∈ Th. answers are consistent h Th is unique column c ∈ Ai consistent answers algorithm outputs index column.
 Otherwise, is j0 ∈ Th such aj0 (cid:54)= hj0.
 is easy see case |Ai+1| ≤ |Ai|.
 (cid:24) (cid:25)  = ln E/E get OPT(A) ≤ max (cid:24) log (cid:25)(cid:19) (cid:18) log(1/) log n log(1/(1 − )) ≤ log E log n.
 time complexity algorithm is O(T log + mn).
 (cid:117)(cid:116) (cid:19)(cid:19) log (cid:18) E log log E log2 E fact prove bound (cid:18) E OPT(A) ≤ E log log E log2 E + log E substituting  = (ln E)/(E(1 + ln ln E/ ln E)).
 Appendix C Appendix ﬁnd ETD(F∨) exactly.
 prove ETD(F∨) = max G∈G(F∨) |De(G)| + HS(As(G) ∧ ¯G).
 following result is [5].

 Let De(G) = {G1, G2,


 Gt} be set immediate descendants G.
 is witness G1 G, is witness Gi G i >
 is, G1(a) = G(a) = G2(a) = ··· = Gt(a) =
 Teaching Dimension minimum size witness set G C is called witness size is denoted TD(C, G).
 value TD(C) := max G∈C TD(C, G) is called teaching dimension C, [8,
 Obviously, ETD(C, G) ≥ TD(C, G), ETD(C) ≥ TD(C).
 Proof Lemma
 G ∈ F∨ have TD(F∨, G) ≥ |De(G)| + HS(As(G) ∧ ¯G).
 particular, ETD(F∨) = TD(F∨) = max G∈G(F∨) (cid:0)|De(G)| + HS(As(G) ∧ ¯G)(cid:1)
 Proof.
 Let B be witness set G Ne(G).
 Take G(cid:48) ∈ De(G).
 is ∈ B such G(cid:48)(a) = G(a) =
 ascendent G(cid:48)(cid:48) G have G(cid:48)(cid:48)(a) = is witness G ascendants.
 Lemma cannot be witness other descendent.
 similar way, witness ascendent G G cannot be witness descendent G G.
 TD(F∨, G) ≥ TD(Ne(G), G) = TD(De(G), G) + TD(As(G), G) (15) let S be witness set G As(G).
 G(cid:48)(cid:48) ∈ As(G) is ∈ S such G(cid:48)(cid:48)(a) = G(a) = is equivalent G(cid:48)(cid:48)(a)∧ ¯G(a) =
 Therefore, = |De(G)| + TD(As(G), G).
 TD(As(G), G) ≥ HS(As(G) ∧ ¯G).
 (15) gives result.
 Appendix D (cid:117)(cid:116) Example Classes fi1,i2,...,im(x1,


 = (cid:86)m Deﬁne class Raym n
 functions are fi1,i2,...,im (x1,


 xm) [n]m → {0, j=1[xj ≥ ij].
 is easy see class contains O(nm) functions Hasse degree is
 See Ray2 See ﬁgure example F Hasse degree
 Figure
 Fig.

 Hasse diagram Ray2 [x2 ≥ i].

 functions are fi(x1, x2) = [x1 ≥ i] gi(x1, x2) = 𝑓1,𝑓2,𝑓3,𝑓4,𝑔1,𝑔2,𝑔3,𝑔4𝑓2,𝑓3,𝑓4,𝑔2,𝑔3,𝑔4𝑓3,𝑓4,𝑔2,𝑔3,𝑔4𝑓4,𝑔2,𝑔3,𝑔4𝑔2,𝑔3,𝑔4𝑔3,𝑔4𝑔4𝑓2,𝑓3,𝑓4,𝑔3,𝑔4𝑓2,𝑓3,𝑓4,𝑔4𝑓2,𝑓3,𝑓4𝑓3,𝑓4𝑓4𝑓3,𝑓4,𝑔3,𝑔4𝑓4,𝑔3,𝑔4𝑓3,𝑓4,𝑔4𝑓4,𝑔4 Fig.

 Hasse diagram
 Fig.

 Hasse diagram F = {f1, f2, f3, g1, g2, g3, h1,


 h5} functions {1, × {1, → {0, fi(x1, y1) = [x1 ≥ i], gi(x1, x2) = [x2 ≥ i] hi(x1, x2) = [x1 + x2 ≥ i +
 𝑥1∨𝑥2∨𝑥30𝑥1∨𝑥2𝑥1∨𝑥3𝑥2∨𝑥3𝑥1𝑥2𝑥3𝑥1∨𝑥2∨𝑥1∨𝑥20𝑥1∨𝑥2𝑥1∨𝑥2𝑥2∨𝑥1𝑥1𝑥2𝑥1𝑥1∨𝑥2𝑥2𝑓1,𝑓2,𝑓3,𝑔1,𝑔2,𝑔3,ℎ1,ℎ2,ℎ3,ℎ4,ℎ5𝑓2,𝑓3,𝑔2,𝑔3,ℎ2,ℎ3,ℎ4,ℎ5𝑓2,𝑓3,𝑔3,ℎ3,ℎ4,ℎ5𝑓2,𝑓3,ℎ4,ℎ5𝑓3,ℎ4,ℎ5𝑓3,ℎ5ℎ5𝑓3,𝑔2,𝑔3,ℎ3,ℎ4,ℎ5𝑔2,𝑔3,ℎ4,ℎ5𝑔3,ℎ4,ℎ5𝑔3,ℎ5𝑓3,𝑔3,ℎ3,ℎ4,ℎ5𝑓3,𝑔3,ℎ4,ℎ5ℎ4,ℎ5
 Gaussian processes (GPs) give rise function space view modelling, whereby place prior distribution functions, reason properties likely functions prior (Rasmussen Williams,
 Given data, infer posterior distribution functions make predictions.
 generalisation behavior Gaussian process is determined prior support (which functions are priori possible) inductive biases (which functions are priori likely), are turn encoded kernel function.
 However, popular kernels, multiple kernel learning procedures, cannot extract expressive hidden representations, was envisaged neural networks (MacKay, Wilson,
 discover such representations, recent approaches have advocated building expressive ker- nel functions.
 instance, spectral mixture kernels (Wilson Adams, were introduced ﬂexible kernel learning extrapolation, modelling spectral density scale-location mix- ture Gaussians, promising results.
 However, Wilson Adams (2013) specify number mixture components hand, do characterize uncertainty mixture hyperparameters.
 kernel functions become expressive parametrized, becomes natural adopt function space view kernel learning—to represent uncertainty values kernel function, reﬂect belief kernel does have simple form.
 use Gaussian processes functions model apply function space view step hierarchical model—with prior distribution kernels.
 paper, introduce scalable distribution kernels modelling spectral density, Fourier transform kernel, L´evy process.
 consider scale-location mixtures Gaussians Laplacians basis functions L´evy process, induce prior kernels Conference Neural Information Processing Systems (NIPS Long Beach, CA, USA.
 gives rise peaked spectral densities occur practice—providing powerful inductive bias kernel learning.
 Moreover, choice basis functions allows kernel func- tion, conditioned L´evy process, be expressed closed form.
 prior distribution kernels has support stationary covariances—containing, instance, composition popular RBF, Mat´ern, rational quadratic, gamma-exponential, spectral mixture kernels.
 spectral mixture representation Wilson Adams (2013), proposed process allows natural automatic inference number mixture components spectral density model.
 Moreover, priors implied popular L´evy processes such gamma process symmetric α-stable process result stronger complexity penalties (cid:96)1 regularization, yielding sparse representations removing mixture components ﬁt noise.
 Conditioned distribution kernels, model data Gaussian process.
 form predictive distribution, take Bayesian model average GP predictive distributions large set possible kernel represented support prior kernels, weighted posterior probabilities kernels.
 procedure leads non-Gaussian heavy- tailed predictive distribution modelling data.
 develop reversible jump MCMC (RJ-MCMC) scheme infer posterior distribution kernels, including inference number components L´evy process expansion.
 scalability, pursue structured kernel interpolation (Wilson Nickisch, approach, case exploiting algebraic structure L´evy process expansion, O(n) inference O(1) predictions, compared standard O(n3) O(n2) computations inference predictions Gaussian processes.
 Flexible distri- butions kernels be valuable large datasets, contain additional structure learn rich statistical representations.
 key contributions paper are summarized follows:
 ﬁrst probabilistic approach inference spectral mixture kernels — incor- porate kernel uncertainty predictive distributions, realistic coverage extrapolations.
 feature is demonstrated Section
 Spectral regularization spectral kernel learning.
 L´evy process acts sparsity- inducing mixture components, pruning extraneous components.
 feature allows automatic inference model key hyperparameter be hand tuned original spectral mixture kernel paper.

 Reduced dependence good initialization, key practical improvement original spectral mixture kernel paper.

 natural interpretable function space view kernel learning.
 provide review Gaussian L´evy processes models prior distributions functions.
 Gaussian Processes A stochastic process f (x) is Gaussian process (GP) ﬁnite collection inputs X = {x1,··· xn} ⊂ RD, vector function values [f (x1),··· f (xn)]T is Gaussian.
 distribution GP is determined mean function m(x), covariance kernel k(x, x(cid:48)).
 GP used specify distribution functions is denoted f (x) ∼ GP(m(x), k(x, x(cid:48))), E[f (xi)] = m(xi) cov(f (x), f (x(cid:48))) = k(x, x(cid:48)).
 general- ization properties GP are encoded covariance kernel hyperparameters.
 exploiting properties joint Gaussian variables, obtain closed form expressions conditional mean covariance functions unobserved function values given observed function values.
 Given f (x) is observed n training inputs X values f = [f (x1),··· f (xn)]T predictive distribution unobserved function values f∗ n∗ testing inputs X∗ is given (1) (2) (3) f∗|X∗, X, θ ∼ N (¯f∗, cov(f∗)), ¯f∗ = mX∗ + KX∗,X K−1 cov(f∗) = KX∗,X∗ − KX∗,X K−1 X,X (f − mX ), X,X KX,X∗
 KX∗,X example denotes n∗ × matrix covariances evaluated X∗ X.
 popular radial basis function (RBF) kernel has following form: kRBF(x, x(cid:48)) = exp(−0.5(cid:107)x − x(cid:48)(cid:107)2 /(cid:96)2).
 (4) GPs RBF kernels are limited expressiveness act smoothing interpo- lators, only covariance structure learn data is length scale (cid:96), determines covariance decays distance.
 Wilson Adams (2013) introduce expressive spectral mixture (SM) kernel capable ex- tracting complex covariance structures RBF kernel, formed placing scale-location mixture Gaussians spectrum covariance kernel.
 RBF kernel comparison model single Gaussian centered origin frequency (spectral) space.
 L´evy Processes A stochastic process {L(ω)}ω∈R+ is L´evy process has stationary, independent increments is continuous probability.
 other words, L satisfy
 L(0) =
 L(ω0), L(ω1) − L(ω0),··· L(ωn) − L(ωn−1) are independent ∀ω0 ≤ ω1 ≤ ··· ≤ ωn,
 L(ω2) − L(ω1) d= L(ω2 − ω1) ∀ω2 ≥ ω1,
 P(|L(ω + h) − L(ω)| ≥ ε) = ∀ε > ∀ω ≥
 lim h→0 L´evy-Khintchine representation, dis- tribution (pure jump) L´evy process is com- determined L´evy measure.
 is, characteristic function L(ω) is given by: log E[eiuL(ω)] = (cid:0)eiu·β − − iu · β1|β|≤1 (cid:1) ν(dβ).
 (cid:90) Rd\{0} L´evy measure ν(dβ) is σ-ﬁnite measure satisﬁes following integra- bility condition (cid:90) (1 ∧ β2)ν(dβ) < ∞.
 Rd\{0} Figure Annotated realization compound Poisson process, special case L´evy process.
 ωj represent jump locations, βj represent jump magnitudes.
 L´evy process be viewed combination Brownian motion drift superposition independent Poisson processes differing jump sizes β.
 L´evy measure ν(dβ) determines expected number Poisson events unit time particular jump size β.
 Brow- nian component L´evy process be considered model.
 higher dimension input spaces ω ∈ Ω, deﬁnes general notion L´evy random measure, is characterized L´evy measure ν(dβdω) (Wolpert et
 show sample realizations L´evy processes be used draw sample parameters adaptive basis expansions.
 L´evy Process Priors Adaptive Expansions Suppose wish adaptive class expansions: simple manipulation, rewrite f X → R(cid:12)(cid:12)(cid:12) f (x) =(cid:80)J (cid:110) j=1 βjφ(x, ωj) f (x) form stochastic integral: specify (cid:111) (cid:90) J(cid:88) J(cid:88) (cid:90) J(cid:88) (cid:124) j=1 (cid:123)(cid:122) =dL(ω) (cid:125) f (x) = βjφ(x, ωj) = βj φ(x, ω)δωj (ω)dω = φ(x, ω) βjδωj (ω)dω j=1 j=1 Hence, specifying prior measure L(ω), specify prior parameters {J, (β1, ω1), ..., (βJ ωJ )} expansion.
 L´evy random measures provide 0246810x-5051015f(x)β1β2β3ω1ω2ω3 family priors suited purpose, is one-to-one correspondence jump behavior L´evy components expansion.
 illustrate point, suppose basis function parameters are one-dimensional consider integral dL(ω) ω.
 (cid:90) ω see Figure that(cid:80)J L(ω) = (cid:90) ω J(cid:88) j=1 J(cid:88) j=1 dL(ξ) = βjδωj (ξ)dξ = βj1[0,ω](ωj).
 j=1 βj1[0,ω](ωj) resembles sample path compound Poisson pro- cess, number jumps J, jump sizes βj, jump locations ωj corresponding number basis functions, basis function weights, basis function parameters respectively.
 use compound Poisson process deﬁne prior such piecewise constant paths.
 generally, use L´evy process deﬁne prior L(ω).
 L´evy-Khintchine representation, jump behavior prior is characterized L´evy measure ν(dβdω) controls mean number Poisson events region parameter space, encoding inductive biases model.
 number parameters framework is random, use form trans-dimensional reversible jump Markov chain Monte Carlo (RJ-MCMC) sample parameter space (Green,
 Popular L´evy processes such gamma process, symmetric gamma process, symmetric α-stable process possess desirable properties different situations.
 gamma process is able produce positive gamma distributed transforming output space.
 symmetric gamma process produce positive negative βj, according Wolpert al.
 (2011) achieve used isotropic geostatistical covariance functions.
 symmetric α-stable process produce heavy-tailed distributions βj is appropriate one expect basis expansion be dominated few weighted functions.
 dispense L´evy processes place Gaussian Laplace priors βj obtain (cid:96)2 (cid:96)1 regularization expansions, respectively, key beneﬁt particular L´evy process priors are implied priors coefﬁcients yield stronger complexity penalties regularization.
 property encourages sparsity expansions permits scalability MCMC algorithm.
 Refer supplementary material illustration joint priors coefﬁcients, exhibit concave contours contrast convex elliptical diamond contours (cid:96)2 (cid:96)1 regularization.
 Furthermore, log posterior L´evy process is log(J!) complexity penalty term encourages sparsity expansions.
 Refer Clyde Wolpert (2007) further details.
 L´evy Distributions Kernels section, motivate choice prior kernel functions describe generate samples prior distribution practice.
 L´evy Kernel Processes Bochner’s Theorem (1959), continuous stationary kernel be represented Fourier dual spectral density: S(s)e2πis(cid:62)τ ds, S(s) = k(τ )e−2πis(cid:62)τ dτ.
 (5) (cid:90) k(τ = RD (cid:90) RD Hence, spectral density characterizes stationary kernel.
 be desirable model spectrum kernel, view kernel estimation lens density estimation.
 order emulate sharp peaks characterize frequency spectra natural phenomena, model spectral density location-scale mixture Laplacian components: φL(s, ωj) = e−λj|s−χj|, ωj ≡ (χj, λj) ∈ [0, fmax] × R+.
 full speciﬁcation symmetric spectral mixture is λj (cid:104) ˜S(s) + ˜S(−s) (cid:105) S(s) = (6) (7) J(cid:88) j=1 ˜S(s) = βjφL(s, ωj).
 Laplacian spikes have closed form inverse Fourier transform, spectral density S(s) repre- sents following kernel function: J(cid:88) j=1 k(τ = βj λ2 j + cos(2πχjτ ).
 λ2 (8) parameters J, βj, χj, λj be interpreted Eq. (8).
 total number terms mixture is J, βj is scale jth frequency contribution, χj is central frequency, λj governs term decays high λ results conﬁdent, long-term periodic extrapolation).
 Other basis functions be used place φL model spectrum well.
 example, Gaussian mixture is chosen, maximum likelihood estimation learning procedure, obtain spectral mixture kernel (Wilson Adams,
 spectral density S(s) takes form adaptive expansion, deﬁne L´evy such densities corresponding kernels above form.
 chosen basis function φ(s, ω) L´evy measure ν(dβdω) say k(τ is drawn L´evy kernel process denoted k(τ ∼ LKP(φ, ν).
 Wolpert al.
 (2011) discuss necessary regularity conditions φ ν.
 summary, propose following hierarchical model functions f (x)|k(τ ∼ GP(0, k(τ )), τ = x − x(cid:48), k(τ ∼ LKP(φ, ν).
 (9) Figure shows samples L´evy process speciﬁed Eq. (7) cor- responding covariance kernels.
 show GP realization kernel func- tions.
 placing L´evy process spectral densities, induce L´evy kernel pro- cess stationary covariance functions.
 Sampling L´evy Priors discuss generate samples L´evy kernel process practice.
 kernel parameters are drawn accord- ing {J,{(βj, ωj)}J j=1} ∼ L´evy(ν(dβdω)), Eq. (8) is used evaluate k ∼ LKP(φL, ν) values τ.
 Recall Section choice L´evy measure ν is determined choice corresponding L´evy process vice versa.
 processes men- tioned produce sample paths in- many jumps (and cannot be sampled directly), jumps are small, processes be ap- proximated L2 compound Poisson pro- cess jump size distribution truncated ε.
 desired L´evy process is chosen truncation bound is basis expansion parameters are generated drawing J ∼ Poisson(ν+ samples β1,··· βJ ∼ πβ(dβ), J i.i.d. samples ω1,··· ωJ ∼ πω(dω).
 Refer supplementary ε = νε(R × Ω) gamma, symmetric gamma, material L2 error bounds formulas ν+ symmetric α-stable processes.
 form πβ(βj) depends choice L´evy process be found supplemen- tary material, further details Wolpert al.
 (2011).
 choose draw χ uninformed uniform prior reasonable range frequency domain, λ gamma distribution, λ ∼ Gamma(aλ, bλ).
 choices aλ, bλ, frequency limits are left hyperparame- ters, have own hyperprior distributions.
 drawing values specify Figure Samples L´evy kernel mix- ture distribution.
 (top) spectra Laplace components drawn L´evy process prior.
 (middle) corresponding stationary co- variance kernel functions prior mean standard deviations model, deter- mined samples.
 (bottom) GP samples respective covariance kernel functions.
 ε ), drawing J i.i.d. L´evy process realization, corresponding covariance function be evaluated an- alytical expression inverse Fourier transform (e.g. Eq. (8) Laplacian frequency mixture components).
 Scalable Inference Given observed data D = {xi, yi}N x∗ interpolation extrapolation.
 model observations y(x) hierarchical model: i=1, wish infer p(y(x∗)|D, x∗) test set inputs y(x)|f (x) = f (x) + ε(x), f (x)|k(τ ∼ GP(0, k(τ )), k(τ ∼ LKP(φ, ν).
 (10) (11) (12) Computing posterior distributions marginalizing LKP yield heavy-tailed non- Gaussian process y(x∗) = y∗ given inﬁnite Gaussian mixture model: ε(x) τ = x − x(cid:48), iid∼ N (0, σ2), p(y∗|D) = p(y∗|k,D)p(k|D)dk ≈ p(y∗|kh), kh ∼ p(k|D).
 (13) (cid:90) H(cid:88) h=1 Initialization Considerations compute approximating sum using H RJ-MCMC samples (Green,
 sample draws kernel posterior kh ∼ p(k|D) distribution.
 sample kh enables draw sample posterior predictive distribution p(y∗|D), estimate predictive mean variance.
 have chosen Gaussian observation model Eq. (10) (conditioned f (x)), inference procedures have introduced apply non-Gaussian likelihoods, such Poisson processes Gaussian process intensity functions, classiﬁcation.
 sum Eq. (13) requires drawing kernels distribution p(k|D).
 is difﬁcult dis- tribution approximate, is ﬁxed number parameters J varies.
 employ RJ-MCMC, extends capability conventional MCMC allow sequential samples different dimensions be drawn (Green,
 Thus, posterior distribution is limited coefﬁcients other parameters ﬁxed basis expansion, represent chang- ing number basis functions, required description L´evy processes described previous section.
 RJ-MCMC be used learn appropriate number basis functions expansion.
 case spectral kernel inferring number basis functions corresponds learning important frequency contributions GP kernel, lead new interpretable insights data.
 choice initialization procedure is important practical consideration machine learning tasks due severe multimodality likelihood surface (Neal,
 many cases, however, ﬁnd spectral kernel learning RJ-MCMC learn salient fre- quency contributions simple initialization, such uniform covering broad range frequencies many sharp peaks.
 frequencies are important describing data are attenuated removed RJ-MCMC learning.
 few RJ-MCMC iterations are needed discover salient frequencies way.
 Wilson (2014) proposes alternative structured approach initialization previous spectral ker- nel modelling work.
 First, pass (squared) data Fourier transform obtain empirical spectral density, be treated observed.
 Next, ﬁt empirical spectral density using standard Gaussian mixture density estimation procedure, assuming ﬁxed number mixture com- ponents.
 Then, use learned parameters Gaussian mixture initialization spectral mixture kernel hyperparameters, Gaussian process marginal likelihood optimization.
 observe successful adaptation procedure L´evy process method, replacing approximation Laplacian mixture terms using result initialize RJ-MCMC.
 Scalability other GP based kernel methods, computational bottleneck lies evaluation log marginal likelihood MCMC, requires computing (KX,X + σ2I)−1y log |KX,X + σ2I| n × n kernel matrix KX,X evaluated n training points X.
 di- rect approach computing Cholesky decomposition kernel matrix requires O(n3) computations O(n2) storage, restricting size training sets O(104).
 Furthermore, computation be performed iteration RJ-MCMC, compounding standard computa- tional constraints.
 However, bottleneck be overcome Structured Kernel Interpolation approach introduced Wilson Nickisch (2015), approximates kernel matrix ˜KX,X(cid:48) = MX KZ,ZM(cid:62) X(cid:48) exact kernel matrix KZ,Z evaluated smaller set m (cid:28) inducing points, sparse interpolation matrix MX facilitates computations.
 calculation reduces O(n + g(m)) computations O(n + g(m)) storage.
 described Wilson Nickisch impose Toeplitz structure KZ,Z g(m) = m log m, allowing RJ-MCMC procedure train massive datasets.
 Experiments conduct experiments total.
 order motivate model kernel learning later experiments, ﬁrst demonstrate ability L´evy process recover—through direct regression—an observed noise-contaminated spectrum is characteristic peaked nat- occurring spectra.
 second experiment demonstrate robustness RJ- MCMC sampler recovering generative frequencies known kernel, presence signiﬁcant noise contamination poor initializations.
 third experiment demonstrate ability method infer spectrum airline passenger data, per- form long-range extrapolations real data, demonstrate utility accounting un- certainty kernel.
 ﬁnal experiment demonstrate scalability method training model data point sound waveform.
 Code is available https: //github.com/pjang23/levy-spectral-kernel-learning.
 Explicit Spectrum Modelling begin applying L´evy process di- function modelling LARK regression), inference described Wolpert al.
 (2011), Laplacian basis func- tions.
 choose out class test function proposed Donoho Johnstone (1993) is standard wavelet literature.
 inhomogeneous function is deﬁned represent spectral densities arise scientiﬁc en- gineering applications.
 Gaussian i.i.d. noise is added give signal-to-noise ratio be consistent previous studies test func- tion Wolpert al.
 (2011).
 noisy test function LARK regression ﬁt are shown Figure
 synthetic spectrum is characterized L´evy process, “false positive” basis function terms ﬁtting noise owing strong regularization properties L´evy prior.
 contrast, GP regression RBF kernel learns length scale maximum marginal likelihood Gaussian process posterior ﬁt sharp peaks test function overﬁts additive noise.
 point experiment is show L´evy process Laplacian basis functions forms natural prior spectral densities.
 other words, samples prior look types spectra occur practice.
 Thus, process have powerful inductive bias used kernel learning, explore next experiments.
 Figure L´evy process regression noisy test function (black).
 ﬁt (red) captures lo- cations scales spike ignoring noise, falls short modes black spikes are parameterized (1 + |x|)−4 Laplacian.
 Figure Ground truth recovery known fre- quency components.
 spectrum Gaussian process was used generate noisy training data is shown black.
 noisy data erroneous spectral initialization shown dashed maximum posteriori estimate spectral density RJ- MCMC steps) is shown red.
 SM kernel identiﬁes salient frequencies, broader support, shown magenta.
 (right) Noisy training data are shown scatterplot, withheld testing data shown green.
 learned posterior predictive distribution (mean black, credible set grey) captures test data.
 Ground Truth Recovery next demonstrate ability method recover generative frequencies known kernel robustness noise poor initializations.
 Data are generated GP kernel having spectral Laplacian peaks, partitioned training testing sets containing points each.
 Moreover, training data are contaminated i.i.d. Gaus- sian noise (signal-to-noise ratio
 Based observed training data (depicted black dots Figure right), estimate kernel Gaussian process inferring spectral density (Figure left) using RJ-MCMC iterations.
 empirical spectrum initialization described section results discovery generative frequencies.
 recover salient fre- quencies poor initialization, shown Figure (left).
 comparison, train Gaussian SM kernel, initializing based empirical spec- trum.
 resulting kernel spectrum (Figure magenta curve) does recover salient frequencies, less conﬁdence higher over- head poor initialization spectral kernel learning RJ-MCMC.
 Spectral Kernel Learning Long-Range Extrapolation next demonstrate ability method perform long-range extrapolation real data.
 Figure shows time series monthly airline passenger data (Hyn- dman,
 data show long-term ris- ing trend short term seasonal wave- form, absence white noise artifacts.
 Wilson Adams (2013), ﬁrst data points are used train model last months (4 years) are withheld testing data, indicated green.
 initial- ization empirical spectrum RJ-MCMC steps, model is able automat- learn necessary frequencies shape spectral density capture rising trend seasonal waveform, allow- ing accurate long-range extrapolations with- pre-specifying number model compo- nents advance.
 experiment demonstrates impact accounting uncertainty kernel, withheld data appears crosses upper bound predictive bands SM ﬁt, model yields wider conservative predictive bands capture test data.
 SM extrapolations are sensitive choice parameter values, ﬁxing parameters kernel yield overconﬁdent predictions.
 L´evy process allows account range possible kernel parameters achieve broad coverage possible extrapolations.
 Note L´evy process spectral densities induces prior kernel functions.
 Figure shows side-by-side comparison covariance function draws prior posterior distribu- tions kernels.
 see sample covariance functions prior vary quite signiﬁcantly, are concentrated posterior, movement empirical covariance function.
 Figure Learning Airline passenger data.
 Training data is scatter plotted, withheld test- ing data shown green.
 learned posterior distribution proposed approach (mean black, credible set grey) captures periodicity rising trend test data.
 analogous interval using GP SM kernel is illustrated magenta.
 00.20.4Frequency100200300400Power01020304050X-10-505f(X) Figure Covariance function draws kernel (left) posterior (right) distributions, empirical covariance function shown black.
 RJ-MCMC, covariance distribution centers correct frequencies order magnitude.
 Figure Learning natural sound tex- ture.
 close-up training interval is displayed true waveform data scat- ter plotted.
 learned posterior distribu- tion (mean black, credible set grey) retains periodicity signal corrupted interval.
 samples are drawn posterior distribution.
 Scalability Demonstration A ﬂexible Bayesian approach kernel learning come additional computa- tional overhead.
 demonstrate scalability is achieved integration SKI (Wil- son Nickisch, L´evy process model.
 consider data point waveform, taken ﬁeld natural sound modelling
 L´evy kernel process is trained sound texture sample howling wind middle removed.
 Training involved initialization signal empirical covariance RJ-MCMC samples, took less hour using In- tel i7 GHz CPU GB memory.
 distinct mixture components model were au- identiﬁed RJ-MCMC proce- dure.
 learned kernel is used GP inﬁlling training points, taken down-sampling training data, is applied origi- nal Hz natural sound ﬁle inﬁlling.
 GP posterior distribution region interest is shown Figure sample realizations, appear capture qualitative behavior waveform.
 experiment demonstrates applicability proposed kernel learning method large datasets, shows promise extensions higher dimensional data.
 Discussion introduced distribution covariance kernel functions is suited modelling quasi- periodic data.
 have shown place L´evy process spectral density sta- tionary kernel.
 resulting hierarchical model allows incorporation kernel uncertainty predictive distribution.
 spectral regularization properties L´evy process priors, found trans-dimensional sampling procedure is suitable performing infer- ence model order, is robust initialization strategies.
 Finally, incorporated structured kernel interpolation training inference procedures linear time scalability, enabling experiments large datasets.
 key advances conventional spectral mixture kernels are being able discover number mixture components, representing uncertainty kernel.
 Here, considered dimensional inputs station- ary processes most elucidate key properties L´evy kernel processes.
 However, generalize process multidimensional non-stationary kernel learning infer- ring properties transformations inputs kernel hyperparameters.
 Alternatively, one consider neural networks basis functions L´evy process, inferring distributions parameters network numbers basis functions step automating neural network architecture construction.
 (Seconds)-0.4-0.200.20.4f(X) Acknowledgements.
 work is supported part Natural Sciences Engineering Re- search Council Canada (PGS-D National Science Foundation DGE IIS-1563887 awards.
 References Bochner, S.
 Lectures Fourier Integrals.(AM-42), volume
 Princeton University Press,
 Clyde, Merlise A Wolpert, Robert L.
 Nonparametric function estimation using overcomplete dictionaries.
 Bayesian Statistics,
 Donoho, D.
 Johnstone, J.M. Ideal spatial adaptation wavelet shrinkage.
 Biometrika,
 Green, P.J. Reversible jump monte carlo computation bayesian model determination.
 Biometrika,
 Green, P.J. Trans-dimensional Markov chain Monte Carlo, chapter
 Oxford University Press,
 Hyndman, R.J. Time series data library.

 http://www-personal.buseco.monash.
 edu.au/˜hyndman/TSDL/.
 MacKay, David J.C. Introduction Gaussian processes.
 Bishop, Christopher M.
 (ed.), Neural Networks Machine Learning, chapter pp.

 Springer-Verlag,
 Neal, R.M. Bayesian Learning Neural Networks.
 Springer Verlag,
 ISBN
 Rasmussen, C.
 E.
 Williams, C.
 K.
 I.
 Gaussian processes Machine Learning.
 MIT Press,
 Turner, R.
 Statistical models natural sounds.
 PhD thesis, University College London,
 Wilson, Andrew Gordon.
 Covariance kernels fast automatic pattern discovery extrapolation Gaussian processes.
 PhD thesis, University Cambridge,
 Wilson, Andrew Gordon Adams, Ryan Prescott.
 Gaussian process kernels pattern discovery extrapolation.
 International Conference Machine Learning (ICML),
 Wilson, Andrew Gordon Nickisch, Hannes.
 Kernel interpolation scalable structured Gaus- sian processes (KISS-GP).
 International Conference Machine Learning (ICML),
 Wolpert, R.L., Clyde, M.A., Tu, C.
 Stochastic expansions using continuous dictionaries: L´evy adaptive regression kernels.
 Annals Statistics,
 Supplementary Materials Sampling Levy Process Priors following formulas section are taken Wolpert al.
 (2011) reference.
 Suppose hyperparameters θ prior distributions J, β, ω, are drawn hyperprior distribution, πθ(dθ).
 order sample L´evy prior, follow steps are taken: θ ∼ πθ(dθ) J|θ ∼ Po(ν+ ε ), ε ≡ νε(R × Ω) ν+ {(βj, ωj)}J j=1|J, θ i.i.d.∼ πβ(βj)dβjπω(dωj) ε πβ are determined speciﬁc choice L´evy process are given formulas ν+ below.
 computational purposes, βj’s are truncated |βjη| > ε Poisson approximation true L´evy process, E|L[φ] − Lε[φ]|2 represents L2 error approximation given basis function φ.
 Below, E1(z) =(cid:82) ∞ t−1e−tdt.
 Gamma Process J ∼ Po(ν+ ε ), ε = γ|Ω|E1(ε) ν+ i.i.d.∼ πβ(βj)dβj, βj πβ(βj) = β−1 j e−βj η E1(ε) E|L[φ] − Lε[φ]|2 = γη−2(cid:107)φ(cid:107)2 − (1 + ε)e−ε] Symmetric Gamma Process J ∼ Po(ν+ ε ), ε = ν+ i.i.d.∼ πβ(βj)dβj, βj πβ(βj) = |βj|−1e−|βj|η E|L[φ] − Lε[φ]|2 = E|L[φ] − Lε[φ]|2 = Symmetric α-Stable Process J ∼ Po(ν+ ε ), ε = γ|Ω| ν+ i.i.d.∼ πβ(βj)dβj, βj πβ(βj) = − (1 + ε)e−ε] (cid:17) (cid:16) πα Γ(α)sin ε−α αεα |βj|−α−11{|βjη|>ε} (cid:21) (cid:20) Γ(α + (cid:16) πα (cid:17) ε2−α π(2 − α) sin Sparsity Inducing Properties L´evy Priors Figure illustrates contours joint distribution independent draws β different priors πβ(dβ).
 contours gamma process be taken upper-right quadrant symmetric gamma process.
 Gaussian Laplace priors β result (cid:96)2 (cid:96)1 regularization respectively.
 L´evy processes contrast yield curving contours, leading sparsity inducing effect similar (cid:96)p regular- ization p <
 Intuitively, discourages simultaneous large values β (cid:96)1 regularization added basis functions improve ﬁt.
 Figure Contour plot joint probability density function β draws different priors.
 Initialization Hyperparameter Tuning Initialization hyperparameter tuning be automated ﬁtting empirical spectrum data.
 is done following steps:
 needed, de-mean training data subtracting deterministic mean function such sample mean best line.
 Doing eliminate large peaks origin dominate rest spectrum.
 de-meaned training data {yj}n j=1 be input RJ-MCMC.

 Compute empirical spectral density Semp(s) = MATLAB, is calculated ﬁrst (cid:98) entries
 Sample empirical spectral density ﬁt Gaussian mixture J0 components sampled data.
 good initial guess J0 be done examining number peaks empirical spectrum.
 ∈ [0,
 j=1 yje−2πis(j−1)(cid:12)(cid:12)(cid:12)2 (cid:12)(cid:12)(cid:12)(cid:80)n J0(cid:88) j=1 − (s−χj )2 SGaussian(s) = αj
 Keep frequencies χj Gaussian ﬁt, using least squares, Laplacian basis function individual Gaussian component.
 j, minimize following objective sample grid points sk [−3σj, βj (cid:88) sk min λj ,βj e−λj|sk| − αj λj 2 − s2
 initial spectrum ﬁtted parameters J0(cid:88) j=1 Sinitial(s) = βj λj e−λj|s−χj| initial spectrum ﬁt airline data is shown Figure

 Tune hyperparameters: likelihood λ parameters initial spectrum.
 • λ is modelled prior Gamma(aλ, bλ), aλ bλ be estimated maximum • η−1 ∼ Gamma(aη, bη) controls expected value coefﬁcients βj.
 basis func- tions integrate sum βj’s is equal total area spectrum, Parseval’s identity represents total variance data.
 sample variance training data be used upper bound coefﬁcient values, aη bη be set
 • γ ∼ Gamma(aγ, bγ) is proportional expected number basis functions shown Section controls sparsity expansions.
 aγ bγ be set cover range values encourage sparsity.
 • symmetric α-stable process, < α < controls heaviness tails distribution βj smaller values α yielding heavier tails.
 α be set maximum likelihood initial βj’s.
 • ε be set based L2 truncation errors described Section Figure Initial Spectrum Fit
 work, consider seller oﬀering N products, N is large, pricing certain products inﬂuence demand others unknown ways.
 let pt P RN denote vector selected prices product is sold time period P t1,


 Tu, results total demands products period represented vector qt P RN
 Note qt represents (noisy) evaluation aggregate demand curve chosen prices pt, observe counterfactual demand have resulted had selected diﬀerent price-point.
 is referred bandit feedback online optimization literature [1].
 goal is setting prices time period maximize total revenue seller rounds).
 is equivalent minimizing negative revenue time: Rtpptq Rtpptq “ ´xqt, pty (1) Rpp1,


 pTq “ Tÿ t“1 ∗Work done part author was Microsoft Research maximize total proﬁts revenue redeﬁning pt diﬀerence product-prices cost product-unit.
 seller consider prices constraint set S Ă RN assume is convex throughout.
 ﬁnd optimal prices, introduce following linear model aggregate demands, is allowed change time nonstationary fashion: qt “ ct ´ Btpt (2) Here, ct P RN denotes baseline demand product round t.
 Bt P RNˆN is asymmetric matrix demand elasticities represents changing price product aﬀect demand product, demand other products well.
 conventional economic wisdom, Bt have largest entries diagonal demand product is driven price price other unrelated products.
 price increase leads falling demand, is reasonable assume Bt ľ are positive semi-deﬁnite (but Hermitian), implies round: Rt is convex function pt.
 observed aggregate demands time period are subject random ﬂuctuations driven noise term t P RN use  represent full set random eﬀects t1,


 Throughout, suppose noise round t is sampled i.i.d. mean-zero distribution ﬁnite variance.
 wealth past work dynamic pricing has posited similar demand models, most existing research has considered settings underlying model is changing time [2,
 standard statistical (or predictive) approaches pricing problem stationary formulations, allow ct, Bt change round be chosen.
 consideration is important dynamic markets seller faces new competitors consumers ever-changing preferences are seeking cheapest prices products [7].
 goal is select prices p1,


 pT minimize expected regret ErRpp1,


 pTq ´ Rpp˚,


 p˚qs com- pared selecting single best conﬁguration prices p˚ “ argmin Rtppq pPS chosen hindsight functions Rt have been revealed.
 Tÿ t“1 Low regret algorithms ensure case stationary underlying model, chosen prices converge optimal choice, nonstationary pricing procedure adapt intrinsic diﬃculty dynamic revenue- optimization problem being conservative [8].
 low (ie.
 opTq) regret regret existing methods is bounded Ωp?
 is achievable using algorithms online convex optimization bandit feedback, Nq, is undesirable large one is dealing vast number products [1,
 attain better bounds, adopt low-rank structural assumption variation demands changes time due d N underlying factors.
 setting, develop algorithms regret depends d N combining existing bandit methods low- dimensional projections selected online singular value decomposition.
 primary contributions work include: • nonstationary formulation dynamic pricing online convex optimization bandit feedback side-information observed demands • low-rank model high-dimensional demands based (latent) product features • Eﬃcient algorithms enable low-regret dynamic pricing high-dimensional, nonstationary (adversarial) settings, revealing product features As are aware, main result is ﬁrst online bandit optimization algorithm regret does scale ambient dimensionality action space.

 Related Work numerous bandit optimization techniques have been applied dynamic pricing problems, research area has been restricted stationary setting [2,
 Most similar work, Javanmard [6] developed bandit pricing-method assume demand depends linearly prices product-speciﬁc features.
 most literature work does consider realistic setting demands product depend (in time-varying fashion) price features other products.
 use standard bandit algorithms such settings scale sellers wish price vast number related products [1].
 High-dimensional dynamic pricing was considered [5], employ sparse maximum likelihood framework presumes stationarity underlying model is less robust adversarial environments comparison online optimization come equipped strong performance guarantees.
 generally, existing algorithms combine bandits low-dimensional sub- space estimation are designed stationary settings full online optimization problem (where underlying reward function is allowed vary time).
 ﬁeld online bandit optimization has seen many advances pioneering work Flaxman al.
 [9], none recent improvements achieves regret is independent dimensionality action-space [16,
 knowledge, Hazan et al.
 [18] is only existing work present online convex optimization algo- rithms regret depends intrinsic low rank structure ambient dimension.
 However, approach is suited dynamic pricing is restricted settings full-information (rather bandit feedback), linear noise-free (or stationary) reward functions, actions are constrained probability-simplex.

 Low-rank Model introduce model ct Bt (2) display low-rank changes time.
 practice, product i be described vector features ui P Rd (where d N ), be used determine similarity products baseline demands.
 Traditionally, natural method gauge underlying similarity products i j is inner product xui, ujyV “ i Vuj linear transformation feature-space given V ľ
 uT example, ui be binary vector indicating product i falls certain product- categories (where number categories d is less number products N ), V be diagonal matrix specifying cross-elasticity demand i Vuj ¨ pj be marginal eﬀect product category.
 example, uT demand product i results selecting pj price product j.
 introducing time-varying metric transformations Vt, model allows product-similarities evolve time.
 Given features ui represent prod- uct, assume following demand model, variation time exhibits low-rank structure: qt “ Uzt ´ UVtUT pt t (3) Here, rows U P RNˆd contain featurization N products, t represent random noise observed demands, zt P Rd explain variation baseline- demand time, (asymmetric) matrices Vt P Rdˆd specify latent changes demand-price relationship time.
 model, aggregate demand product i time t is governed prices products, weighted current feature-similarity product i.
 ensure revenue-optimization remains convex, restrict adversary choices satisfy Vt t.
 Note structural variation model is assumed be low-rank, noise observed demands be N -dimensional.
 round, pt qt are only quantities observed, t, zt, Vt remain unknown (and consider cases product features U are known unknown).

 Methods basic dynamic pricing strategy is employ gradient-descent gradient (GDG) online bandit optimization technique [9].
 naive application algorithm produces regret dependent number products N ensure updates method are applied d-dimensional subspace spanned U, leads regret bounds depend d N
 U is unknown, subspace is estimated online, similar fashion approach [18] online learning low-rank experts.
 deﬁne x “ UT p P Rd, low-rank model (3) Erts expected value revenue-objective round t be expressed as: ErRtppqs “ pT UVtUT p ´ pT Uzt “ xT Vtx ´ xT zt :“ ftpxq (4) Thus, intrinsic dimensionality problem is d, maximize expected revenues considering restricted set d-dimensional actions x functions ft projected constraint set: (5) (cid:32) UTpSq “ x P Rd x “ UT p p P S
 Dynamic Pricing Known Product Features certain markets, is clear featurize products [4].
 low-rank model (3) U is given, apply OPO-K method (Algorithm select prices.
 algorithm employs subroutines FindPrice, ProjectToFeasible solve convex optimization problems order compute certain projections.
 Here, use Unifptx P Rd ||x||2 “ denote uniform distribution unit Euclidean Rd. Intuitively, algorithm adapts GDG approach [9] select low-dimensional actions xt P Rd time point, seeks feasible price vector correspond- ing chosen xt.
 Note d N are many price-vectors p P RN map same low-dimensional vector x P Rd UT these, select one is previously-chosen prices FindPrice), ensuring additional stability dynamic pricing procedure.
 initial prices p0 be selected based external knowledge historical demand data.
 Online Pricing Optimization Known Product Features (OPO-K) Inputs: η, δ, α ą product feature matrix U P RNˆd, initial prices p0 P S Output: Sequence prices p1,


 pT seek maximize total revenue Set prices p0 P S observe (negative) revenue R0pp0q demands q0pp0q Deﬁne x1 “ UT p0 t


 T Draw ξt „ Unifptx P Rd ||x||2 “ setrxt :“ xt Set round t prices: pt “ FindPrice(rxt, U,S, pt´1), observe Rtpptq, qtpptq xt`1 “ ProjectToFeasible(xt ´ ηRtpptqξt, α, U, S) Algorithm FindPrice(x; U,S, pt´1) Inputs: x P Rd (vector), U P RNˆd (matrix), S Ă RN (convex set) Output: Closest price pt´1 is S maps x UT Return argmin pPRN ||p ´ pt´1||2 subject UT p “ x, p P S Algorithm ProjectToFeasible(x, α, U, S) Inputs: x P Rd (vector), α ą (scalar), U P RNˆd (matrix), S Ă RN (convex set) Output: Projection x set tp1 ´ αqUT p p P Su Ă Rd ˇˇˇˇp1 ´ αqUT p ´ x pp “ argmin Return p1 ´ αqUTpp pPS ˇˇˇˇ2 mild conditions, Theorem states OPO-K algorithm incurs OpT dq regret product features are priori known.
 result is derived Lemma shows Step algorithm corresponds online projected gradient descent (in expectation) smoothed version objective deﬁned as: pftpxq “ Eζ ftpx ζq Rtppqξ BpftBx “ d ¨ E,ξ ft is alternative objective function (which is equivalent Rt) given (4), ζ is sampled unit Rd. Lemma
 ξ „ Unifptx P Rk ||x||2 “ p P RN UT p “ x δξ P Rd: (6) (7) Proof.
 ErRtppqs “ ftpx` δξq, result follows Lemma [9].
 bound regret pricing adopt following assumptions: (A1) UTpSq contains ball radius rmin is contained ball radius (the constraint set is bounded well-scaled) rmax ě rmin (the number pricing rounds is large) (A2) T ą (A3) ´B ď ErRtppqs ď p P S t


 T (revenues are bounded) (A4) ftpxq is L-Lipschitz x P UTpSq t


 T (smooth revenue-functions) Theorem
 conditions (A1)-(A4) are met choose η “ rmax α “ δ p P S, exists universal constant C such that: δ “ T ´1{4 rmin Bdrmaxrmin Tÿ t“1 Rtpptq ´ Tÿ t“1 Rtppq E,ξ ď C ¨ T Brmax L B rmin (8) prices p1,


 pT selected OPO-K algorithm.
 Proof.
 ´B ď Rtppq ď p P S implies range ft is constrained r´B, x P UTpSq. Recall ft is convex function x (as required Vt p P S, deﬁne x “ UT p P UTpSq such that: ErRtppqs “ ftpxq.
 convexity S implies UTpSq is convex, proof result follows Theorem below.
 note S UTpSq are convex, choice η, δ, α ensuresrxt P UTpSq pt P S t.
 (Flaxman al.
 [9]).
 Suppose ft P r´B, Bs is convex, L-Lipschitz function x P Rd, set feasible actions U is convex, Euclidean balls radius rmax rmin containing contained-within U, respectively.
 low- dimensional actions x1,


 P Rd correspond (in expectation) iterates online projected gradient descent algorithm applied pft (as deﬁned (6)) satisfy: ftpxtq Tÿ t“1 ´ min xPU ftpxq ď L B rmin (9) choose η, δ, α Theorem
 relying implicit characteristics Rt, ft, show same OpT dq regret bound holds OPO-K algorithm impose following structural assumptions original linear low-rank model (3): t (A5) ||zt||2 ď b (A6) ||Vt||op ď b t (spectral norm ||¨||op is magnitude largest singular value) (A7) T ą (A8) U is orthogonal matrix such UT U “ Idˆd (A9) S “ tp P RN ||p||2 ď ru (with r ě Requiring columns U form orthonormal basis Rd, condition (A8) be enforced ﬁrst orthonormalizing given product features.
 Note orthogonality condition does restrict overall class models speciﬁed (3), describes case features used encode product are uncorrelated products (ie.
 minimally-redundant encoding) have been normalized products.
 simplify analysis, adopt (A9) presuming constraint set feasible product-prices is centered Euclidean ball (implying p vectors represent shifted/scaled prices).
 Corollary
 assumptions (A5)-(A9), choose η “ α “ δ r p P S, exists universal constant C such that: δ “ T ´1{4 bp1`dq?
 dr2p1`rq ď Cbrpr (10) Tÿ t“1 E,ξ Rtppq Rtpptq ´ Tÿ t“1 prices p1,


 pT selected OPO-K algorithm.
 Proof.
 show (A5)-(A9) imply necessary conditions Theorem hold rmax “ rmin “ r, B “ rbp1`rq, L “ p2r`1qb.
 Bounding simplifying Theorem inequality arises deﬁnitions produces desired result.
 Lemma implies (A1) holds rmax “ rmin “ r.
 Note (A3) holds since: ftpxq “ xT Vtx ´ xT zt ď ||x||2 ||zt||2||x||2 ď r2b rb Finally, show structural assumptions imply Lipshitz continuity ft, required (A4): ||∇xftpxq||2 “ ||pVT Lemma
 assumption (A9), orthogonal N ˆ d matrix U: UTpSq “ tx P Rd ||x||2 ď ru UUTppq P S p P S.
 t Vtqx ´ zt||2 ď ||zt||2 ď b @x P UTpSq Proof.
 Consider orthogonal extension U, matrix W “ rU,rUs P RNˆN formed appending N ´ d additional orthonormal columns U are orthogonal columns U.
 p P RN have: orthogonality implies U is isometry ||WT p||2 “ ||UT p||2 ||rUT p||2 ||UUT p||2 “ ||UT p||2 ď ||WT p||2 “ ||WWT p||2 “ ||p||2 W is isometry due fact WT “ W´1 W is square orthogonal Combined (A9), implies UUTppq P S ||x||2 ď r x P UTpSq. ﬁx arbitrary x P Rd satisﬁes ||x||2 ď r.
 orthogonality U: ||Ux||2 “ ||x||2 ď r ùñ Ux P S, UT Ux “ x ùñ x P UTpSq.
 Dynamic Pricing Latent Product Features many settings, is clear best represent products feature-vectors.
 adopting low-rank demand model (3), consider case U is unknown be estimated.
 improve identiﬁability resulting model, adopt (A8) section.
 Orthogonality implies U is isometry right-inverse UT
 Thus, given low-dimensional action x P set corresponding prices p “ Ux such UT p “ x.
 Lemma shows price selection-method is feasible corresponds changing Step OPO-K algorithm previous price pt´1.
 prices pt are multiplied noise term t revenue-function Rt, choosing minimum-norm prices help reduce variance total revenue generated approach.
 U is unknown, employ pt “ FindPrice(rxt, U,S, next price is regularized origin estimate pU P RNˆd, is restricted be orthogonal matrix.

 orthogonal matrix pU x P pUTpSq, deﬁne pp “ pUx P RN
 (A9), pp P S pp “ FindPrice(x,pU,S,
 Proof.
 Given x P pUTpSq, exists p P S pUT p “ x.
 Lemma implies ||pp||2 ď ||p||2 pp “ pUx “ pUpUT p P S set is centered Euclidean ball.
 note pUTpp “ x pUTpU “ Idˆd, rp is minimum-norm vector S is mapped x pUT
 E,ξ (11) Product Features Known Span Theorem consider minor modiﬁcation OPO-K algorithm price- previous price pt´1.
 knowing true latent features, result implies regret modiﬁed OPO-K algorithm be bounded selection Step is done using pt “ pUrxt being regularized number products N pU estimates column span U.

 Suppose spanppUq “ spanpUq, ie.
 orthogonal estimate has same pU used place underlying U parameters η, δ, α chosen Corollary column-span underlying (rank d) latent product-feature matrix.
 Let P S denote optimal pricing p1,


 pT be prices selected modiﬁed OPO-K algorithm
 conditions (A5)-(A9), exists universal constant C such that: Tÿ t“1 Rtpp˚q Rtpptq ´ Tÿ Tÿ Rtppq, letrp “ UUT p˚.
 Note E ď Cbrpr t“1 t“1 Proof.
 Deﬁning p˚ “ argmin pPS ”ř t“1 Rtprpq ”ř rp P S Lemma rp is optimal setting product-prices (in expectation).
 U pU share same column-span, t“1 Rtpp˚q exist low-dimensional action rx P Rk such rp “ pUrx.
 orthogonality pU: pUTrp “ pUTpUrx “rx, sorx P pUTpSq is feasible solution modiﬁed OPO-K algorithm.
 x P Rd p “ pUx P RN re-express expected revenue price vector introducing ft,pU function x parameterized pU, done (4): Convexity Rt p implies ft,pU is convex x pU.
 Note modiﬁed OPO-K version ft,pU, deﬁned (6).
 Thus, employing same argument show x P pUTpSq: Tÿ ft,pUpxq “ xTpUT UVtUTpUx ´ xTpUT Uzt “ ErRtppqs based Theorem deﬁning B, L, rmax, rmin proof Corollary algorithm is (in running online projected gradient descent smoothed (12) ´ ft,pUpxq ď Cbrpr ft,pUpxtq Eξ t“1 xt are low-dimensional actions chosen Step modiﬁed OPO-K algo- (13) rithm, such pt “ pUxt prices output method.
 conclude proof, Tÿ t“1 “ Tÿ t“1 ft,pUprxq E Rtpp˚q recall chosen pt: Rtpptq Tÿ t“1 E,ξ ft,pUpxtq Tÿ t“1 “ Eξ Product Features Unknown Span Noise-free Demands practice, span(U) be unknown.
 assume adversary is restricted positive deﬁnite Vt ą t is statistical noise observed demands (ie.
 qt “ Uzt ´ UVtUT pt round), Lemma shows ensure span(U) is revealed ﬁrst d observed demand vectors adding minuscule random perturbation initial prices selected ﬁrst d rounds.
 Thus, knowing latent produce feature subspace, absence noise observed demands enables realize low regret pricing strategy same OPO-K algorithm (applied ﬁrst d rounds).

 Suppose t round assume Vt ą
 t


 d: pt is distributed (uncentered) Euclidean ball positive radius, spanpq1,


 “ spanpUq surely.
 Proof.
 Lemma suppose pt “rpt ζt, ζt is uniformly drawn centered Euclidean ball nonzero radius RN zt, Vt,rpt are ﬁxed indepen- show linear independence holds surely, proceed proving randomness ζt.
 Note qj “ Usj sj “ zj ´ VjUT pj P Rd. Thus, spanpq1,


 Ď spanpUq spans be equal s1,


 are lin- early independent.
 Prpsj P spanps1,


 sj´1qq “ ă j ď d.
 ﬁrst note sj “ zj ´ VjUTrpj ´ s1 VjUTrpj ´ zj,


 sj´1 ´ zj, subspace has measure VjUT ζj.
 Vj ą is invertible U is orthogonal, VjUT ζj is distributed nondegenerate ellipsoid E Ă Rd nonzero variance projection Rd. includes directions orthogonal j ´ dimensional subspace spanned uniform distribution E (for j ď d).
 Product Features Unknown Span Noisy Demands observed demands are noisy spanpUq is unknown, select prices using OPO-L algorithm presented below.
 approach is similar previous OPO-K algorithm, maintain estimate latent product features’ span.
 estimator is updated online fashion averaged singular value decomposition (SVD) observed demands.
 Online Pricing Optimization Latent Product Features (OPO-L) Inputs: η, δ, α ą initial prices p0 P S, rank demand-variation d P r1, Ns Output: Sequence prices p1,


 pT seek maximize total revenue Set prices p0 P S observe (negative) revenue R0pp0q demands q0pp0q t


 T Initialize pQ N ˆ d matrix Initialize pU random N ˆ d matrix orthonormalize columns Deﬁne x1 “ pUT p0 Draw ξt „ Unifptx P Rd ||x||2 “ setrxt :“ xt Set round t prices: pt “ pUrxt observe Rtpptq, qtpptq xt`1 “ ProjectToFeasible(xt ´ ηRtpptqξt, α, pU, S) j rpt ´ mod ds, k “ ﬂoorpt{dq, update: pQ˚,j Ð Update columns pU top d left singular vectors pQ k qt k´1 pQ˚,j Step OPO-L algorithm corresponds online averaging ob- served demand vector qt historical observations stored jth column matrix pQ.
 computing singular value decomposition pQ “ rUrSrVT Step is performed setting pU equal ﬁrst d columns rU (the indices corresponding largest singular values inrS).
 pQ is changed round, singular columns pU remain orthonormal execution update operation Step be carried much greater computational eﬃciency leveraging existing SVD-update procedures [19,
 Note deﬁnition algorithm.
 quantify regret incurred algorithm, assume entries noise vector t are i.i.d. samples sub-Gaussian distribution t “


 T
 Re- call random variable X follows sub-Gaussian(σ2) distribution ErXs “ Prp|X| ą xq ď expp´ x2 x ą
 assumption sub-Gaussian noise is general, covering common settings noise is Gaussian, bounded, log-concave density, ﬁnite mixture sub-Gaussian variables [21].
 Furthermore, regret bound remains valid demands diﬀerent products are indepen- dent heteroscedastic, case assume sub-Gaussian parameter σ2 speciﬁes maximal variation observed demand products.
 Intuitively, averaging step OPO-L algorithm ensures statistical con- centration noise observed demands, such true column span underlying U be better revealed.
 concretely, let st “ zt ´ VtUT pt t t, q˚ t “ Ust, observed demands be written as: qt “ q˚ q˚ t are (unobserved) expected demands chosen prices.
 round T (assuming T is divisible T{dÿ d), jth column pQ is given by: T{dÿ j`dpi´1q sQ˚ ˚,j “ d sj`dpi´1q i“1 (14) pQ˚,j “ sQ˚ ˚,j d i“1 T{d i“1 j`dpi´1q exhibits rapid concentration measure, results average d random matrix theory imply span-estimator obtained ﬁrst d singular vectors pQ Step OPO-L algorithm converge column span sQ˚ P RNˆd (the average-matrix underlying expected demands jth column is deﬁned above).
 is useful sQ˚ shares same span underlying U.
 shows OPO-L algorithm achieves low-regret setting unknown product features, regret depends intrinsic rank d (rather number products N ).
 required sub-Gaussianpσ2{Nq noise-level be ensured rescaling observed demands, extending time- duration round t allow suﬃcient number (potential) customers, ensure random eﬀects individual’s concentrate aggregate observed demand.
 Note regret Theorem depends constant Q value is determined noise-level σ extreme singular values sQ˚ deﬁned (14).
 general, quantities measure adversarial environment seller is faced with.
 example, underlying low-rank variation is much smaller magnitude noise observations, be diﬃcult estimate span latent product features.
 control theory, signal- to-noise expression similar Q has been proposed quantify intrinsic diﬃculty system identiﬁcation linear quadratic regulator [22].

 Suppose unknown U is orthogonal rank d.
 Let “ argmin pPS denote optimal product pricing p1,


 pT be prices selected OPO-L al- iid„ sub-Gaussianpσ2{Nq gorithm parameters η, δ, α set Corollary
 t and(A5)-(A9) hold, exists universal constant C such that: Tÿ t“1 Rtppq Tÿ t“1 Rtpptq ´ Tÿ ¯) t“1 E,ξ Rtpp˚q ď CQrbp4r Q “ max nonzero singular values underlying rank d matrix sQ˚ deﬁned (14).
 σ2 σ1 (and σd) deﬁned largest (and smallest) σ2 E Tÿ t“1 Rtppq product features obtained Step OPO-L algorithm round t.
 Note Proof.
 notational convenience, suppose T is divisible d, T ě d ě noise-variation parameter σ ě proof.
 Recall proof Theorem low-rank demand redeﬁne p˚ Ð UUT p˚ P S ensure p˚ “ argmin
 suppose loss generality pPS optimal prices be expressed p˚ “ Ux˚ corresponding low-dimensional action x˚ P UTpSq. additional clarity, use pUt denote current Nˆd estimate underlying pUt are random variables are determined noise observed demands randomness employed pricing algorithm.
 Letting pt “ pUtxt denote prices chosen OPO-L algorithm round (and xt P pUT Tÿ Rtpptq ´ Rtpp˚q T ft,pUtpxtq ´ ft,pUtprxq ft,pUtpxtq ´ ft,Upx˚q ft,U is deﬁned (12) letrx “ proof Corollary ensures |ft,U| |ft,pUt| (for orthogonal pUt) are bounded ft,pUtprxq ´ ft,Upx˚q ft,pUtpxq rbp1 rq x P UTpSq, bound ﬁrst summand (15): t pSq corresponding low-dimensional actions), have: E Tÿ Tÿ Tÿ t“1 t“1 xPUTpSq E t“T t“T (15) t“T T ft,pUtpxtq ´ ft,Upx˚q ď rbp1 rq ¨ T t“1 bound second summand (15), ﬁrst point UTpSq “ pUT (since pUt are restricted be orthogonal).
 Thus, Algorithm is running classic gradient-free bandit method [9] optimize functions ft,pUt low- dimensional action-space UTpSq, second term is regret method stated Corollary t pSq Lemma Tÿ ft,pUtpxtq ´ ft,pUtprxq t“T ‰3{4 ď Cbrpr T ´ T d1{2 Finally, complete proof bounding third summand (15).
 Deﬁning ı inf OPO t“T t“T ď inf OPO ďpT ´ T ¨ E ft,pUtprxq ´ ft,Upx˚q O Ă Rdˆd set orthogonal d ˆ d matrices, have: ft,pUtpOx˚q ´ ft,Upx˚q choose pO P O orthogonal matrix such E||pUt bound Lemma t ě T ﬁxed above.
 Deﬁning ∆ “ Ux˚ ´ pUt plug deﬁnition ft,pU (12) simplify obtain following bound: Tÿ ft,pUtpOx˚q ´ ft,Upx˚q we’ve ﬁxed t “ argmax ft,pUtppOx˚q ´ ft,Upx˚q t1PrT Tÿ x˚ P UTpSq ùñ Ox˚ P UTpSq Lemma andrx is argmin UTpSq OPO pT ´ T ¨ E ď inf ft1,pUt1pOx˚q ´ ft1,Upx˚q pO ´ U||F satisﬁes pOx˚ P Rd, pO||opq||x˚||2 ď orthogonality pO,pUt, U pO ´ U||F ft,pUtppOx˚q ´ ft,Upx˚q ďE ďE ďp4||x˚||2Vt||op ¨ Er||∆||2s ||∆||2||zt||2 ||∆||2 ď p||U||op ||pUt ||pUt ďCrbp4r dσ2 Er||∆||2s ď ||x˚||2 ¨ E Lemma (recall ﬁxed t ě T
 ||∆||2 ||∆||2 ‰´1{2 ‰´1{2 (A5)-(A6) ď C T σ2 T dσ2 ||x˚||2 σ2 Crb p1 rqT p1 rqd1{2 Combining bounds summands (15) yields following upper bound, inequality presented Theorem be derived: ˘ Lemma
 pU produced Step OPO-L algorithm T rounds feasible low-dimensional action x P pUTpSq, exists orthogonal d ˆ d matrix pO ˘3{4 p4r T ´ T T ´ T ˙` σ2 universal constant C such that: ” ||pUpO ´ U||F ď CT ´1{2dσ2 σ2 sQ˚ deﬁned (14).
 σ1 σd denote largest smallest singular values underlying matrix Proof.
 proof relies standard random matrix concentration results general- ization Davis-Kahan-Wedin theory [23] presented below.
 (Yu al., [23]).
 Let σ1 ą ¨¨¨ ą σd ą denote nonzero singular values rank d matrix Q P RNˆd.
 left singular vectors Q are represented columns matrix U P RNˆd (such Q has SVD: UΣVT pU P RNˆd denotes left singular vectors other N ˆ d matrix pQ.
 Q,pQ P RNˆd, exists orthogonal matrix pO P Rdˆd such ||pUpO ´ U||F ď ||pQ ´ Q||op Lemma (variant Lemma [24]).
 Let E be N ˆ d matrix (with N ě d) i.i.d. entries drawn sub-Gaussianpσ2{Nq distribution.
 Then, probability ´ δ: ||pQ ´ Q||op σ2 ||E||op ď logp12q logp1{δq sE “ pQ´sQ˚ is mean T{d sub-Gaussianpσ2{Nq samples, be distributed Due averaging performed Step OPO-L algorithm, value sub-Gaussian E||sE||op “ σ2d N T E||sE||2 exp x“0
 Lemma implies: ż Prp||sE||op ą xq dx ¨˝´1 ˜c ż ´a ż x ¨ Prp||sE||op ą xq dx ¨˝´1 ˜c ż x ¨ exp πd x“0 log ´ x“0 “ op ď x“0 ¯ı ˛‚ dx ˛‚ dx log log ¨ erfp log log „a “ ď
 Combing Theorem concentration bounds implies exists d ˆ d op are upper-bounded log T ě d, σ ě E||sE||op E||sE||2 orthogonal matrix pO such that: ||pQ ´sQ˚||op ||pUpO ´ U||F ď σ2 ď dσ2 σ2 ||pQ ´sQ˚||2 ı¯ Pricing Imprecise Adversary Theorem below, illustrate basic scenario explicit bound constant Q Theorem be obtained.
 assume adversary control underlying baseline demand parameters zt (3).
 speciﬁcally, suppose round: zt “ z1 t (and Vt) be selected γt are stochastic terms adversary’s control.
 scenario, presume random dˆ d matrix Γ is drawn initial round such that: t γt, z1 (A10) entry Γi,j is sampled mean zero magnitude bounded b{2 (ErΓi,js “ |Γi,j| ď b{2 i, j).
 Recall constant b ą upper bounds magnitude zt speciﬁed (A5).
 values Γ have been sampled, suppose round t: γt “ Γ˚,j is taken be jth column matrix j pt ´ mod d (traversing columns Γ order).
 discussion, largest smallest nonzero singular values rank-d matrix A be denoted σ1pAq σdpAq, respectively.
 boundedness values Γ implies entries follow sub-Gaussianpb2{4q distribution, following result applies: Lemma (variant Theorem [25]).
 probability least ´ Cb ´ cb d: σdpΓq ě {?
 Cb ą cb P p0, are constants depend (polynomially) b.
 selecting z1 t, Vt, assume imprecise adversary is restricted ensure: d Cbdb ă such t: (A11) exists s ´ cb ||Γ˚,j||2 constants cb, Cb are given Lemma (see [25] details), r ě is used denote radius set feasible prices S.
 Note additional assumptions do conﬂict condition (A5) required Theorem (A10), (A11) t γt.
 assumptions place, provide ensure ||zt||2 ď b zt “ z1 explicit bound constant Q Theorem
 t||2`r¨||Vt||op ď s¨ ||z1 Theorem
 setting imprecise adversary conditions (A10) (A11) are met, τ P p d, Theorem holds cb Q ď dq ´ Cbsbd probability ě ´ τ random sampling Γ).
 ´ cb c2 Proof.
 Recall σ1 (and σd) denote largest (and smallest) singular values σ1 ď c1 σd ě c2 high probability, implies upper bound: Q ď maxt1, σ underlying rank d matrix sQ˚ deﬁned (14).
 suitable constants c1, c2: show p2c1
 ﬁrst note orthogonality U implies sQ˚ “ UsS has same nonzero singular values square matrixsS, jth column is given by: sS˚,j “ d σ1psQ˚q “ σ1psSq ď
 T{dÿ AssS has d columns, have: j`dpi´1q γj`pi´1qd ´ Vj`pi´1qdUT pj`pi´1qd z1 ||sQ˚,j||2 ď bp1 sq?
 d ¨ max (16) i“1 latter inequality derives fact (A9) orthogonality U imply: ||sS˚,j||2 ď d T{dÿ ||γj`pi´1qd||2 i“1 j`pi´1qd||2 r||Vj`pi´1qd||op ď p1 sq ¨ ||Γ˚,j||2 ď b p1 sq conditions (A10), (A11) Via similar reasoning, obtain bound: σ1psS ´ Γq ď sb (17) Subsequently, invoke Lemma implies probability ´ τ σdpΓq ě τ ´ cb Cb (18) Combining (17) (18), obtain high probability bound σd additive Weyl inequality (cf.
 [26]): σdpsQ˚q “ σdpsSq ě σdpΓq ´ σ1psS ´ Γq ě τ ´ cb ´ sb proof is completed deﬁning c1 “ bp1 sq?
 c2 “ τ ´ cb Cb simplifying resulting bound fact d s
 Cb probability ě ´ τ ´ sb
 Results evaluate performance method settings noisy demands are generated according equation (3), underlying structural parameters demand curves are sampled Gaussian distributions.
 section, assume pt qt represent rescaled prices aggregate demands, such feasible set S be ﬁxed centered sphere radius r “
 noise (rescaled) demands product is drawn ﬁxed Gaussian distribution: t „ Np0,
 proposed algorithms are compared GDG method online bandit optimization [9], simple explore-then-exploit technique.
 latter method, randomly sample pt ﬁrst T rounds S) remaining rounds, pt is ﬁxed best prices found explo- ration period.
 reﬂects typical approach used price products: experiment diﬀerent price-settings settle prices exhibited best results.
 experiments, ﬁrst set prices p0 used initialize method is taken be center feasible set S.
 Stationary Demand Model First, apply methods stationary setting underlying structural pa- rameters zt,“ Vt “ V are ﬁxed time.
 start simulation, sample entries z, V zij „ Np100, Vij „ Np0, U is ﬁxed random sparse binary matrix reﬂects d possible categories prod- uct belongs to.
 Subsequently, orthogonalize columns U project V V “ tV VT V ľ λIu λ ensure cross-product price elasticities are positive deﬁnite (and resulting online optimization problem is convex).
 Figure shows OPOK OPOL algorithms outperform basic GDG algorithm dimensionality N exceeds intrinsic rank d.
 N “ d is low-rank structure exploit, OPOK/OPOL algorithms match performance standard GDG bandit optimization.
 stationary setting is reliant, standard approach does perform sophisticated bandit optimization techniques.
 Surprisingly, OPOL method (which infer latent product features pricing strategy) outperforms OPOK approach, has access ground-truth product features.
 is SVD-estimated features represent subspace projected pricing variation impact overall observed demands.
 dimensionality-reduction OPOK algorithm ignores eﬀects noise selected prices observed demands.
 (A) N “ d “ (B) N “ d “ Figure Average cumulative regret (over repetitions standard-deviations shaded) diﬀerent pricing strategies underlying demand model is stationary.
 Model Demand Shocks Next, examine performance methods non-stationary setting underlying demand model changes times T{3
 start period r0, T{3s, rT{3, r2T{3, Ts: redraw underlying structural parameters zt, Vt same Gaussian distributions described previous section.
 Figure shows bandit techniques adapt changes underlying demand curves.
 regret bandit algorithms starts decreasing time, indicating begin outperforming optimal ﬁxed price chosen hindsight.
 again, low-rank methods are able achieve low regret large number products existing approaches, exhibiting same performance GDG algorithm is low-rank structure exploit.
 (A) N “ d “ (B) N “ d “ Figure Average regret repetitions standard-deviations shaded) pricing strategies underlying demand model contains structural shocks times T{3,
 Drifting Demand Model Finally, study methods non-stationary setting demand curves change time.
 Here, underlying structural parameters zt, Vt are drawn same described Gaussian distributions time t begin drift time following process: zt`1 “ zt w, Vt`1 “ ΠVpVt Wq (19) entries w W are i.i.d. samples Np0, Np0, distribu- tions, respectively.
 ΠV denotes projection matrix positive-deﬁnite set V deﬁned previously.
 Figure shows bandit pricing approach adapt changing demand curves, identifying better pricing strategy optimal ﬁxed price chosen hindsight.
 setting, low-rank methods much stronger performance GDG Explore is large number products handle.
 (A) N “ d “ (B) N “ d “ Figure Average regret repetitions standard-deviations shaded) various pricing strategies underlying demand model drifts time.

 Discussion exploiting low-rank structural condition emerges dynamic pricing problems, work introduces ﬁrst online bandit optimization algorithm regret depends intrinsic rank problem ambient dimensionality action space.
 low-rank bandit approach dynamic pricing scales large number products intercorrelated demand curves, underlying de- mand model is allowed vary time be chosen.
 applied various high-dimensional dynamic pricing systems involving stationary changing demand curves, approach outperforms standard bandit methods.
 Future extensions work include adaptations predictable sequences future demands be forecasted [27], generalizing convex formulation linear demand model [28].
 Note GDG algorithm [9] approach is based relies convexity revenue function, is SVD procedure learning latent product features necessitates linear demand curve.
 work focused dynamic pricing, remains interesting explore other bandit applications similar low-rank structural conditions prove useful.
 References [1] Dani V, Hayes TP, Kakade SM (2007) price bandit information online optimization.
 Neural Information Processing Systems [2] Keskin NB, Zeevi A (2014) Dynamic pricing unknown demand model: optimal semi-myopic policies.
 Operations Research [3] Besbes O, Zeevi A (2015) surprising suﬃciency linear models dynamic pricing demand learning.
 Management Science [4] Cohen M, Lobel I, Leme RP (2016) Feature-based dynamic pricing.
 ACM Conference Economics Computation [5] Javanmard A, Nazerzadeh H (2016) Dynamic pricing high-dimensions.
 arXiv:arXiv:160907574 [6] Javanmard A (2017) Perishability data: Dynamic pricing varying-coeﬃcient models.
 Journal Machine Learning Research [7] Witt U (1986) How complex economical behavior be investigated?
 example ignorant monopolist revisited.
 Behavioral Science [8] Shalev-Shwartz S (2011) Online learning online convex optimization.
 Founda- tions Trends Machine Learning [9] Flaxman AD, Kalai AT, McMahan HB (2005) Online convex optimization bandit setting: Gradient descent gradient.
 Proceedings Annual ACM-SIAM Symposium Discrete Algorithms [10] Kleinberg R, Leighton T (2003) value knowing demand curve: Bounds regret online posted-price auctions.
 Proceedings Annual IEEE Symposium Foundations Computer Science [11] Besbes O, Zeevi A (2009) Dynamic pricing knowing demand function: Risk bounds near-optimal algorithms.
 Operations Research [12] den Boer AV, Bert Z (2013) Simultaneously learning optimizing using controlled variance pricing.
 Management Science [13] Misra K, Schwartz EM, Abernethy J (2017) Dynamic online pricing incomplete information using multi-armed bandit experiments.
 Available SSRN: https: // ssrncom/ abstract= [14] Gopalan A, Maillard O, Zaki M (2016) Low-rank bandits latent mixtures.
 arXiv:160901508 [15] Djolonga J, Krause A, Cevher V (2013) High-dimensional gaussian process bandits.
 Neural Information Processing Systems [16] Hazan E, Levy KY (2014) Bandit convex optimization: Towards tight bounds.
 Neural Information Processing Systems [17] Bubeck S, Lee YT, Eldan R (2017) Kernel-based methods bandit convex opti- mization.
 Proceedings Annual ACM SIGACT Symposium Theory Computing [18] Hazan E, Koren T, Livni R, Mansour Y (2016) Online learning low rank experts.
 Conference Learning Theory [19] Brand M (2006) Fast low-rank modiﬁcations thin singular value decomposition.
 Linear Algebra Applications [20] Stange P (2008) eﬃcient update singular value decomposition.
 Pro- ceedings Applied Mathematics Mechanics [21] Honorio J, Jaakkola T (2014) Tight bounds expected risk linear classiﬁers PAC-Bayes ﬁnite-sample guarantees.
 Artiﬁcial Intelligence Statistics [22] Dean S, Mania H, Matni N, Recht B, Tu S (2017) sample complexity linear quadratic regulator.
 arXiv:171001688 [23] Yu Y, Wang T, Samworth R (2015) useful variant Davis-Kahan theorem statisticians.
 Biometrika [24] Rigollet P (2015).
 High dimensional statistics.
 MIT Opencourseware: http://ocw.mit.edu/courses/mathematics/18-s997-high-dimensional- statistics-spring-2015/lecture-notes/ [25] Rudelson M, Vershynin R (2008) Littlewood-Oﬀord problem invertibility random matrices.
 Advances Mathematics [26] Horn R, Johnson CR (1991) Topics Matrix Analysis.
 Cambridge Univ.
 Press [27] Rakhlin A, Sridharan K (2013) Online learning predictable sequences.
 Confer- ence Learning Theory [28] Hazan E, Levy KY, Shalev-Shwartz S (2016) graduated optimization stochas- tic non-convex problems.
 International Conference Machine Learning
 deep learning has emerged useful technique data classiﬁcation ﬁnding feature representations.
 consider scenario multi-class classiﬁca- tion.
 deep neural network maps feature vector class labels connection nodes multi-layer structure.
 Between adjacent layers weight matrix maps inputs (values previous layer) outputs (values current layer).
 Assume training set includes (yi, xi), i =


 l, xi ∈ (cid:60)n0 is feature vector yi ∈ (cid:60)K is label vector.
 xi is associated label k, yi = [0,





 ∈ (cid:60)K, (cid:124) (cid:123)(cid:122) (cid:125) k−1 K is number classes {1,


 K} are possible labels.
 collecting weights biases model vector θ having loss function ξ(θ; x, y), neural-network problem be written following optimization problem.
 min f (θ), f (θ) = θT θ + l(cid:88) i=1 ξ(θ; xi, yi).
 (1) (2) regularization term θT θ/2 avoids overﬁtting training data, parameter C balances regularization term loss term.
 function f (θ) is non-convex connection weights different layers.
 non-convexity large number weights have caused tremendous difﬁculties training large-scale deep neural networks.
 apply optimization algorithm solving calcula- tion function, gradient, Hessian be expensive.
 Currently, stochastic gradient (SG) methods are used way train deep neural networks (e.g., Bot- tou, LeCun Bottou, Zinkevich et Dean Moritz
 expensive operations be con- ducted GPU environments (e.g., Ciresan Krizhevsky et Hinton
 stochastic gradient methods, works such Martens (2010); Kiros (2013); et al.
 (2016) have considered Newton method using Hessian infor- mation.
 Other optimization methods such ADMM have been considered (Taylor
 model data set is large, distributed training is needed.
 Following design objective function (2), note is easy achieve data paral- lelism: data instances are stored different computing nodes, machine calculate local sum training losses independently.1 However, achieving model parallelism is difﬁcult complicated structure deep neural net- works.
 work, considering model is stored propose novel distributed Newton method deep learning.
 variable feature-wise data partitions, careful designs, are able use Jacobian matrix matrix-vector products Newton method.
 techniques are incorporated reduce running time memory consumption.
 First, reduce communication cost, propose diagonalization method such approximate Newton direction be obtained communication machines.
 Second, consider subsampled Gauss-Newton matrices reducing running time communication cost.
 Third, reduce synchronization cost, terminate process ﬁnding approximate Newton direction nodes have ﬁnished tasks.
 be focused, various types neural networks, consider stan- dard feedforward networks work.
 do consider other types such convolution networks are popular computer vision.
 work is organized follows.
 Section introduces existing Hessian-free New- ton methods deep learning.
 Section propose distributed Newton method training neural networks.
 develop novel techniques Section reduce running time memory consumption.
 Section analyze cost pro- deep neural networks data parallelism has been considered SG, Newton other optimization methods.
 example, et al.
 (2015) implement parallel Newton method letting node store subset instances.
 posed algorithm.
 Additional implementation techniques are given Section
 Section reviews existing optimization methods, experiments Section demonstrate effectiveness proposed method.
 Programs used experiments paper are available http://www.csie.ntu.edu.tw/˜cjlin/papers/dnn.
 Supplementary materials including list symbols additional experiments be found same web address.
 Hessian-free Newton Method Deep Learning section, begin introducing feedforward neural networks review existing Hessian-free Newton methods solve optimization problem.
 Feedforward Networks A multi-layer neural network maps feature vector class vector con- nection nodes.
 is weight vector adjacent layers map input vector (the previous layer) output vector (the current layer).
 network Figure is example.
 Let nm denote number nodes mth layer.
 use n0(input)-n1-


 -nL(output) represent structure network.3 weight ﬁgure is modiﬁed example http://www.texample.net/tikz/ examples/neural-network.
 n0 is number features nL = K is number classes.
 A0 B0 C0 A1 B1 Figure example feedforward neural networks.2 matrix W m bias vector bm mth layer are  wm wm
 ··· ···
 ··· wm  bm bm
 bm nm bm = wm nm−11 wm nm−12 nm−1nm nm−1×nm  wm wm
 wm wm
 W m = Let A2 B2 C2  s0,i = z0,i = xi be feature vector ith instance, sm,i zm,i denote vectors ith instance mth layer, respectively.
 use sm,i = (W m)T zm−1,i + bm, m =


 L, i =


 l zm,i j = σ(sm,i ), j =


 nm, m =


 L, i =


 l (3) derive value next layer, σ(·) is activation function.
 W columns are concatenated following vector (cid:20) wm = wm


 wm nm−11 wm


 wm nm−12


 wm


 wm (cid:21)T deﬁne θ =  w1 b1
 wL bL  weight vector whole deep neural network.
 total number parameters is L(cid:88) n = (nm−1 × + nm)
 m=1 zL,i is output vector ith data, loss function compare label vector yi, neural network solves following regularized optimization problem min f (θ), f (θ) = θT θ + l(cid:88) i=1 ξ(zL,i; yi), (4) C > is regularization parameter, ξ(zL,i; yi) is convex function zL,i.
 Note rewrite loss function ξ(θ; xi, yi) (2) ξ(zL,i; yi) zL,i is decided θ xi.
 work, consider following loss function ξ(zL,i; yi) = ||zL,i − yi||2.
 gradient f (θ) is ∇f (θ) = θ + l(cid:88) i=1 (J i)T∇zL,iξ(zL,i; yi), (5) (6)  J i = ∂zL,i ∂θ1
 ∂zL,i nL ∂θ1 ···
 ··· ∂zL,i ∂θn
 ∂zL,i nL ∂θn  nL×n i =


 l, (7) is Jacobian zL,i, is function θ.
 Hessian matrix f (θ) is ∇2f (θ) = + (J i)T BiJ i l(cid:88) ∂ξ(zL,i; yi) i=1 j=1 ∂zL,i is identity matrix Bi ts = ∂2ξ(zL,i; yi) t ∂zL,i ∂zL,i t =


 nL, s =


 nL.
 simplicity let ξi ≡ ξi(zL,i; yi).
 Hessian-free Newton Method l(cid:88) nL(cid:88) i=1  ∂2zL,i ∂θ1∂θ1
 ∂2zL,i ∂θn∂θ1 ···
 ··· ∂2zL,i ∂θ1∂θn
 ∂2zL,i ∂θn∂θn  (8) (9) standard Newton methods, kth iteration, ﬁnd direction dk minimizing following second-order approximation function value: min dT H kd + ∇f (θk)T d, (10) H k = ∇2f (θk) is Hessian matrix f (θk).
 solve (10), ﬁrst calculate gradient vector backward process based (3) following equations: ), i =


 l, m


 L, j =


 nm wm tj i =


 l, m


 L, t =


 nm−1 (11) (12) ∂ξi ∂zm,i j=1 ∂ξi ∂sm,i ∂ξi ∂zm−1,i ∂f ∂wm tj σ(cid:48)(sm,i ∂ξi ∂sm,i l(cid:88) i=1 wm tj + zm−1,i ∂ξi ∂sm,i m =


 L, j =


 nm, t =


 nm−1 l(cid:88) i=1 ∂ξi ∂sm,i ∂f ∂bm bm j + m =


 L, j =


 nm.
 (13) (14) Note summation (13) be l(cid:88) l(cid:88) i=1 i(cid:48)=1 zm−1,i(cid:48) ∂ξi ∂sm,i(cid:48) is simpliﬁed ξi is associated sm,i H k is positive deﬁnite, (10) is equivalent solving following linear system: H kd = −∇f (θk).
 (15) Unfortunately, optimization problem (10), is known objective function be non-convex H k is guaranteed be positive deﬁnite.
 Following Schraudolph use Gauss-Newton matrix approxima- tion Hessian.
 is, remove last term (8) obtain following positive-deﬁnite matrix.
 G = + l(cid:88) (J i)T BiJ i.
 i=1 (16) Note (9), Bi, i =


 l is positive semi-deﬁnite require ξ(zL,i; yi) is convex function zL,i.
 Therefore, using (15), solve following linear system ﬁnd dk deep neural networks.
 (Gk + λkI)d = −∇f (θk), (17) Gk is Gauss-Newton matrix kth iteration add term λkI be- cause considering Levenberg-Marquardt method (see details Section
 deep neural networks, total number weights be large, is hard store Gauss-Newton matrix.
 Therefore, Hessian-free algorithms have been applied solve (17).
 Examples include Martens (2010); Ngiam al.
 (2011).
 Specif- ically, conjugate gradient (CG) methods are used sequence Gauss- Newton matrix vector products are conducted.
 Martens (2010); Wang al.
 (2015) use R-operator (Pearlmutter, implement product storing Gauss- Newton matrix.
 use R operators Newton method is focus work, leave detailed discussion Sections II–III supplementary materials.
 Distributed Training Variable Partition main computational bottleneck Hessian-free Newton method is sequence matrix-vector products CG procedure.
 reduce running time, parallel matrix-vector multiplications be conducted.
 However, R operator discussed Section Section II supplementary materials is sequential.
 forward process results current layer be ﬁnished next.
 section, propose effective distributed algorithm training deep neural networks.
 Variable Partition using R operator calculate matrix-vector product, consider whole Jacobian matrix use Gauss-Newton matrix (16) matrix- vector products CG procedure.
 setting is possible following reasons.

 distributed environment is used.

 techniques do need store element Jaco- bian matrix.
 Details be described rest paper.
 begin split J i P partitions (cid:20) J i = (cid:21) ··· J i J i number columns J i is same number variables optimization problem, partition variables P subsets.
 Speciﬁcally, split neurons layer several groups.
 weights connecting group current layer group next layer form subset variable partition.
 example, assume have 150-200-30 neural network Figure
 splitting layers groups, have total number partitions P =
 partition (A0, A1) Figure is responsible × sub-matrix W
 addition, distribute variable bm partitions corresponding ﬁrst sub-group A0 B0 C0 A0,A1 A0,B1 B0,A1 B0,B1 C0,A1 C0,B1 A1 B1 A1,A2 A1,B2 A1,C2 B1,A2 B1,B2 B1,C2 A2 B2 C2 Figure example splitting variables Figure partitions split struc- ture 3-2-3.
 circle corresponds neuron sub-group layer, square is partition corresponding weights connecting neuron sub-group layer neuron sub-group next layer.
 mth layer.
 variables b1 is split partition (A0, A1) partition (A0, B1).
 variable partition, achieve model parallelism.
 Further, z0,i = xi data points are split feature-wise way nodes corresponding partitions layers
 Therefore, have data parallelism.
 variable partition, second term Gauss-Newton matrix (16) ith instance be represented  (J i BiJ i (J i P )T BiJ i 
 ···
 ··· (J i BiJ i (J i P )T BiJ i (J i)T BiJ i = CG procedure solve product Gauss-Newton matrix vector v is Gv =  (cid:80)l (cid:80)l Bi((cid:80)P P )T Bi((cid:80)P
 i=1(J i p=1 J i pvp) C v1 i=1(J i p=1 J i pvp) C vP  (18)  v1
 vP  v = 2 (cid:17) t = s, otherwise.
 is partitioned according variable split.
 (9) loss function deﬁned (5), Bi ts = ∂2(cid:16)(cid:80)nL j − yi j=1(zL,i ∂zL,i t ∂zL,i j)2(cid:17) (cid:16) t − yi t) ∂zL,i However, variable partition, J i be huge matrix.
 total space storing J i p, ∀i is nL × n × l.
 l, number data instances, is large such l × > storing J i p, requires more space n× n Gauss-Newton matrix.
 reduce memory consumption, propose effective techniques Sections variable partition, function, gradient, Jacobian calculations become complicated.
 discuss details Sections Distributed Function Evaluation From know evaluate function value single machine, im- plementation distributed environment is trivial.
 check details perspective individual partition.
 Consider partition involves neurons sets Tm−1 Tm layers m − m, respectively.
 Tm−1 ⊂ {1,


 nm−1} Tm ⊂ {1,


 nm}.
 (3) is process, assume sm−1,i i =


 l, ∀t ∈ Tm−1 are available current partition.
 goal is generate i =


 l, ∀j ∈ Tm sm,i pass partitions layers m m
 begin, calculate zm−1,i = σ(sm−1,i ), i =


 l t ∈ Tm−1.
 (19) Then, (3), following local values be calculated i =


 l, j ∈ Tm tj zm−1,i wm + bm Tm−1 is ﬁrst neuron sub-group layer m − tj zm−1,i wm otherwise.
  (cid:80) (cid:80) t∈Tm−1 t∈Tm−1 local sum (20) is sum values partitions layers m − m.
 (cid:88) (cid:16) sm,i j = Tm−1∈Pm−1 (cid:17) local sum (20) (21) i =


 l, j ∈ Tm, Pm−1 = {Tm−1 | Tm−1 is sub-group neurons layer m −
 resulting sm,i values be broadcasted partitions layers m m correspond neuron subset Tm. explain details (21) broadcast operation Section Allreduce Broadcast Operations goal (21) is generate broadcast sm,i values partitions layers m m + reduce operation seems be sufﬁcient.
 However, explain Section Jacobian evaluation product Gauss-Newton matrix vector, partitions layers m − m corre- sponding Tm need sm,i calculating zm,i j = σ(sm,i ), i =


 l, j ∈ Tm. (22) end, consider allreduce operation are values reduced partitions layers m − m, result is broadcasted them.
 is done, make same result sm,i available partitions layers m m choosing partition corresponding ﬁrst sub-group layer m conduct broadcast operation.
 Note partitions layers L − L (i.e., last layer), broadcast operation is needed.
 Consider example Figure
 partitions (A1, A2), (A1, B2), (A1, C2), get s1,i j j ∈ A1 calculated (21): (cid:88) (cid:123)(cid:122) (cid:124) (cid:88) (cid:124) t + b1 (cid:123)(cid:122) tjz0,i t∈B0 t∈A0 (cid:125) w1 (A0,A1) (cid:125) s1,i j = (cid:88) (cid:124) t∈C0 w1 (cid:123)(cid:122) (cid:125) (B0,A1) (C0,A1) w1 tjz0,i tjz0,i (23) local sums are available partitions (A0, A1), (B0, A1) (C0, A1) re- j j ∈ A1 are available spectively.
 ﬁrst conduct allreduce operation s1,i partitions (A0, A1), (B0, A1), (C0, A1).
 choose (A0, A1) broadcast values (A1, A2), (A1, B2), (A1, C2).
 Depending system conﬁgurations, suitable ways be considered im- plementing allreduce broadcast operations (Thakur
 Section IV supplementary materials give details implementation.
 derive loss value, need ﬁnal reduce operation.
 example j ∈ A2, B2, C2 available partitions Figure end have z2,i (A1, A2), (A1, B2), (A1, C2).
 need following reduce operation ||z2,i − yi||2 = j − yi (z2,i j)2 + (cid:88) j∈A2 (cid:88) j∈B2 j − yi (z2,i j)2 + (cid:88) j∈C2 j − yi (z2,i j)2 (24) let (A1, A2) have loss term objective value.
 have discussed calculation loss term objective value, need obtain regularization term θT θ/2.
 possible setting is loss-term calculation run reduce operation sum local regularization terms.
 example, partition corresponding neuron subgroups Tm−1 layer m − Tm layer m, local value is(cid:88) (cid:88) t∈Tm−1 j∈Tm (wm tj )2.
 (25) other hand, embed calculation forward process obtaining loss term.
 idea is append local regularization term (25) vector (20) allreduce operation (21).
 cost is negligible increase length vector one.
 allreduce broadcast resulting vector partitions layers m m corresponding neuron subgroup Tm. cannot let partition collect broadcasted value subsequent allreduce operations regularization terms previous layers be calculated several times.
 end, allow partition corresponding Tm layer m ﬁrst neuron subgroup layer m collect value include local regularization term subsequent allreduce operation.
 continuing forward process, end get whole regularization term.
 use Figure give illustration.
 allreduce operation (23) calculates (cid:88) (cid:124) t∈A0 (cid:88) j∈A1 (w1 tj)2 + (cid:123)(cid:122) (A0,A1) (cid:88) j∈A1 (cid:88) (cid:124) t∈B0 (b1 j )2 (cid:125) (cid:88) (cid:123)(cid:122) j∈A1 (cid:88) (cid:124) t∈C0 (cid:88) (cid:123)(cid:122) j∈A1 (cid:125) (cid:125) (B0,A1) (C0,A1) (w1 tj)2 (w1 tj)2 (26) resulting value is broadcasted (A1, A2), (A1, B2), (A1, C2).
 (A1, A2) collects value generate following local sum: (26) + end have (cid:88) (cid:88) t∈A1 j∈A2 (cid:88) j∈A2 (b2 j )2.
 (w2 tj)2 +
 (A1, A2) contains regularization terms (A0, A1), (B0, A1), (C0, A1), (A1, A2), (A0, B1), (B0, B1), (C0, B1), (B1, A2).

 (A1, B2) contains regularization terms (A1, B2), (B1, B2).

 (A1, C2) contains regularization terms (A1, C2), (B1, C2).
 extend reduce operation (24) generate ﬁnal value regu- larization term.
 Distributed Jacobian Calculation From (7) similar way calculating gradient (11)-(14), Jacobian matrix satisﬁes following properties.
 ∂zL,i ∂wm tj ∂zL,i ∂bm ∂zL,i ∂sm,i ∂zL,i ∂sm,i ∂sm,i ∂wm tj ∂sm,i ∂bm (28) (29) i =


 l, u


 nL, m =


 L, j =


 nm, t =


 nm−1.
 However, formulations do reveal are calculated distributed set- ting.
 Similar Section check details perspective variable par- tition.
 Assume current partition involves neurons sets Tm−1 Tm layers m − m, respectively.
 aim obtain following Jacobian components.
 ∂zL,i ∂wm tj ∂zL,i ∂bm ∀t ∈ Tm−1, ∀j ∈ Tm, u =


 nL, i =


 l.
 showing calculate them, ﬁrst get (3) ∂zL,i ∂sm,i ∂sm,i ∂wm tj ∂zL,i ∂zm,i = zm−1,i ∂zm,i ∂sm,i ∂zL,i ∂zm,i σ(cid:48)(sm,i ), ∂sm,i ∂bm = ∂zL,i ∂zL,i j = u, otherwise.
 (30) (31) (32) 1   (28)-(32), elements local Jacobian matrix be derived ∂zL,i ∂wm tj ∂zL,i ∂bm ∂zL,i ∂zm,i ∂zL,i ∂zm,i ∂zm,i ∂sm,i ∂zm,i ∂sm,i ∂sm,i ∂wm tj ∂sm,i ∂bm σ(cid:48)(sm,i )zm−1,i m < L, ∂zL,i ∂zm,i σ(cid:48)(sL,i u )zL−1,i m = L, j = u, (33) m = L, j (cid:54)= u, σ(cid:48)(sm,i m < L, ∂zL,i ∂zm,i σ(cid:48)(sL,i u m = L, j = u, m = L, j (cid:54)= u, (34) u =


 nL, i =


 l, t ∈ Tm−1, j ∈ Tm. discuss have values right-hand side (33) (34) available current computing node.
 (19), have zm−1,i ∀i =


 l, ∀t ∈ Tm−1 available forward process calculating function value.
 Further, (21)-(22) obtain zm,i layers m m+1, use allreduce operation reduce operation ∀i =


 l, ∀j ∈ Tm sm,i are available current partition layers m − m.
 Therefore, σ(cid:48)(sm,i (33)-(34) be obtained.
 remaining issue is generate ∂zL,i u /∂zm,i
 show be obtained backward process.
 discussion assumes are partition layers m − m, show details dispatching partitions
 generating ∂zL,i u /∂zm−1,i From (3) (30), ∂zL,i be calculated u /zm−1,i nm(cid:88) j=1 ∂zL,i ∂zm−1,i ∂zL,i ∂sm,i ∂sm,i ∂zm−1,i j=1 Therefore, consider backward process using ∂zL,i distributed system, (32) (35), ∂zL,i ∂zm,i σ(cid:48)(sm,i )wm tj
 (35) u /∂zm,i generate ∂zL,i u /∂zm−1,i (cid:80)  (cid:80) (cid:80) ∂zL,i ∂zm−1,i Tm∈Pm j∈Tm ∂zL,i ∂zm,i σ(cid:48)(sm,i )wm tj m < L, (36) Tm∈Pm σ(cid:48)(sL,i u )wL tu m = L, i =


 l, u


 nL, t ∈ Tm−1, Pm = {Tm | Tm is sub-group neurons layer m}.
 (37) Clearly, partition calculates local sum j ∈ Tm. reduce operation is needed sum values corresponding partitions layers m − m.
 Subsequently, discuss details transfer data partitions layers m − m
 Consider example Figure
 partition (A0, A1) get t ∈ A1, u =


 nL, i =


 l.
 ∂zL,i ∂z1,i (36), ∂zL,i ∂z1,i (cid:88) (cid:124) j∈A2 (cid:88) (cid:124) j∈B2 (cid:125) σ(cid:48)(s2,i j )w2 tj ∂zL,i ∂z2,i (cid:123)(cid:122) (A1,A2) (cid:88) (cid:124) j∈C2 (cid:125) σ(cid:48)(s2,i j )w2 tj ∂zL,i ∂z2,i (cid:123)(cid:122) (A1,B2) σ(cid:48)(s2,i j )w2 tj (38) ∂zL,i ∂z2,i (cid:123)(cid:122) (A1,C2) (cid:125) Note sums are available partitions (A1, A2), (A1, B2), (A1, C2), respectively.
 Therefore, (38) is reduce operation.
 Further, values obtained (38) are needed partitions (A0, A1) (B0, A1) (C0, A1).
 need broadcast operation values be available corresponding partitions.
 details implementing reduce broadcast operations, see Section IV supplementary materials.
 summarizes backward process calculate ∂zL,i u /∂zm,i Memory Requirement have mentioned Section storing elements Jacobian matrix be viable.
 distributing setting, store Jacobian elements corresponding current partition, |Tm−1| × |Tm| × × l (39) space is needed.
 propose technique save space noting (28) be written product terms.
 ﬁrst term is related Tm, second is related Tm−1: ∂zL,i ∂wm tj = [ ∂zL,i ∂sm,i ][ ∂sm,i ∂wm tj ] = [ ∂zL,i ∂zm,i σ(cid:48)(sm,i )][zm−1,i ].
 are available earlier calculation.
 Speciﬁcally, allocate space receive ∂zL,i u /∂zm,i previous layers.
 obtaining values, replace ∂zL,i ∂zm,i σ(cid:48)(sm,i (41) future use.
 Jacobian matrix is stored.
 Instead, use terms (40) product Gauss-Newton matrix vector CG procedure.
 See details Section Note need calculate ∀t ∈ store local sum reduce operation (36) getting ∂zL,i Tm−1, ∀u, ∀i.
 memory consumption is proportional u /∂zm−1,i l × × (|Tm−1| + |Tm|).
 setting reduces memory consumption storing Jaco- bian matrix (39).
 Sigmoid Activation Function discussion far, consider general differentiable activation function σ(sm,i ).
 implementation paper, consider sigmoid function output  j = σ(sm,i zm,i = m,i m < L, sm,i m = L.
 (42) layer: Then,  (cid:18) σ(cid:48)(sm,i = −s m,i −s m,i (cid:19)2 = zm,i (1 − zm,i m < L, m = L.
 (33)-(34) become  ∂zL,i ∂wm tj zm,i (1 − zm,i )zm−1,i ∂zL,i ∂bm ∂zL,i ∂zm,i zL−1,i  zm,i (1 − zm,i m < L, ∂zL,i ∂zm,i m = L, j = u, m = L, j (cid:54)= u, u =


 nL, i =


 l, t ∈ Tm−1, j ∈ Tm. Distributed Gradient Calculation gradient calculation, (4), ∂f ∂wm tj wm tj + ∂ξi ∂wm tj wm tj + l(cid:88) i=1 l(cid:88) nL(cid:88) i=1 u=1 ∂ξi ∂zL,i ∂zL,i ∂wm tj (43) ∂zL,i u /∂wm tj ∀t, ∀j are components Jacobian matrix; see matrix form (6).
 (33), have known calculate ∂zL,i u /∂wm tj
 Therefore, ∂ξi/∂zL,i is passed current partition, obtain gradient vector (43).
 be ﬁnished same backward process calculating Jacobian matrix.
 other hand, technique be introduced Section consider subset instances construct Jacobian matrix Gauss- Newton matrix.
 is, selecting subset S ⊂ {1,


 l}, J i,∀i ∈ S are considered.
 do have needed ∂zL,i u /∂wm tj (43).
 situation, consider backward process calculate gradient vector.
 derivation similar (33), ∂ξi ∂wm tj ∂ξi ∂zm,i σ(cid:48)(sm,i )zm−1,i m =


 L.
 (44) be ∂zL,i considering ∂ξi/∂zm,i (36), apply same back- ward process partition layers m − m − wait ∂ξi/∂zm−1,i partitions layers m − m: u /∂zm,i (cid:88) (cid:88) Tm∈Pm j∈Tm ∂ξi ∂zm−1,i σ(cid:48)(sm,i )wm tj ∂ξi ∂zm,i (45) i =


 l, t ∈ Tm−1, Pm is deﬁned (37).
 initial ∂ξi/∂zL,i backward process, loss function deﬁned (5), = ×(cid:16) ∂ξi ∂zL,i (cid:17) j − yi zL,i From (43), difference Jacobian calculation is obtain sum instances i.
 maintain terms related Tm−1 Tm avoid storing Jacobian elements.
 summation afford store ∂f /∂wm tj ∂f /∂bm j ∀t ∈ Tm−1, ∀j ∈ Tm. Techniques Reduce Computational, Communica- tion, Synchronization Cost section propose novel techniques make distributed Newton method practical approach deep neural networks.
 Diagonal Gauss-Newton Matrix Approximation (18) Gauss-Newton matrix-vector products CG procedure, notice communication occurs reducing P vectors


 J i J i P vP size O(nL), broadcasting sum nodes.
 avoid high communication cost distributed systems, consider diagonal blocks Gauss-Newton matrix approximation: i=1(J i BiJ i ˆG = +
 (cid:80)l i=1(J i P )T BiJ i (17) becomes P independent linear systems (cid:80)l  l(cid:88) i=1 (J i BiJ i + l(cid:88) (J i P )T BiJ i P + i=1 + λkI)dk = −gk
 + λkI)dk P = −gk P matrix-vector product becomes gk


 gk P are local components gradient: 
  gk
 gk ∇f (θk) =  (cid:80)l (cid:80)l i=1(J i BiJ i + C v1 Gv ≈ ˆGv =
 i=1(J i P )T BiJ i P vP + C vP  
 (49) (50) (51) (Gv)p be calculated using local information have independent linear systems.
 CG procedure partition, is terminated following relative stopping condition holds ||1 (J i p)T BiJ i pvp + + λk)vp + gk p|| ≤ σ||gk p|| (52) number CG iterations reaches pre-speciﬁed limit.
 σ is pre-speciﬁed tolerance.
 Unfortunately, partitions ﬁnish CG procedures different time, l(cid:88) i=1 situation results signiﬁcant waiting time.
 address synchronization cost, propose novel techniques Section past works have considered using diagonal blocks approximation Hessian.
 logistic regression, Bian et al.
 (2013) consider diagonal elements Hessian solve several one-variable sub-problems parallel.
 Mahajan al.
 (2017) study general setting using diagonal blocks is special case.
 Product Between Gauss-Newton Matrix Vector CG procedure main computational task is matrix-vector product.
 present techniques efﬁcient calculation.
 (51), pth partition, product local diagonal block Gauss-Newton matrix vector vp takes following form.
 (J i p)T BiJ i pvp.
 Assume pth partition involves sub-groups Tm−1 Tm layers j ∀j ∈ Tm. m− m, partition is responsible handle bias term bm p ∈ RnL×(|Tm−1|×|Tm|) vp ∈ R(|Tm−1|×|Tm|)×1.
 J i Let mat(vp) ∈ R|Tm−1|×|Tm| be matrix representation vp.
 uth component (J i pvp)u is (cid:88) (cid:88) ∂zL,i ∂wm tj (mat(vp))tj = t∈Tm−1 j∈Tm zm−1,i (mat(vp))tj.
 (53) ∂zL,i ∂sm,i (cid:88) (cid:88) t∈Tm−1 j∈Tm j∈Tm calculating 
 zm−1,i (mat(vp))tj (cid:88)  (cid:88) t∈Tm−1 ∂zL,i ∂sm,i (cid:88) t∈Tm−1 zm−1,i (vp)tj, ∀j ∈ Tm A direct calculation value requires O(|Tm−1| × |Tm|) operations.
 get u


 nL components, total computational cost is proportional nL × |Tm−1| × |Tm|.
 discuss technique reduce cost rewriting (53) needs O(|Tm−1| × |Tm|) cost, notice values are independent u.
 pvp)u, ∀u.
 total is, be stored reused calculating (J i computational cost is reduced |Tm−1| × |Tm| + × |Tm|.
 procedure deriving (J i p)T (BiJ i pvp) is similar.
 Assume ¯v = BiJ i pvp RnL×1.
 (40), p)T ¯v(cid:1) mat(cid:0)(J i tj nL(cid:88) nL(cid:88) u=1 u=1 ∂zL,i ∂wm tj ¯vu ∂zL,i ∂sm,i (cid:32) zm−1,i ¯vu = zm−1,i ∂zL,i ∂sm,i u=1 nL(cid:88) u=1 ∂zL,i ∂sm,i ¯vu, ∀j ∈ Tm (cid:33) ¯vu (55) (56) are independent t, calculate store computation (55).
 total computational cost is proportional |Tm−1| × |Tm| + × |Tm|, (57) is same (J i pvp).
 above discussion, assume diagonal blocks Gauss-Newton ma- trix are used.
 whole Gauss-Newton matrix is considered, calculate (J i p1)T (Bi(J i p2vp2)), partitions p1 p2.
 same techniques introduced section be applied (53) (55) are independent operations.
 Subsampled Hessian Newton Method From see computational cost Gauss-Newton matrix vector is proportional number data.
 reduce cost, subsampled Hessian Newton method (Byrd Martens, Wang et have been pro- posed selecting subset data iteration form approximate Hessian.
 ∇2f (θ) (15) use subset S have (cid:88) i∈S |S| ∇2 θθξ(zL,i; yi).
 Note zL,i is function θ.
 idea subsampled Hessian is large set points are same distribution, (cid:88) i∈S |S| ξ(zL,i; yi).
 is good approximation average training losses.
 neural networks consider Gauss-Newton matrix, (16) becomes following subsampled Gauss-Newton matrix.
 GS = |S| (cid:88) i∈S (J i)T BiJ i.
 denote subset kth iteration Sk. linear system (17) is changed (GSk + λkI)dk = −∇f (θk).
 variable partitions, independent linear systems are (cid:32) (cid:33) (cid:88) i∈Sk λkI + + |Sk| (J i BiJ i = −gk dk (cid:32) λkI + (cid:88) i∈Sk + |Sk| (J i P )T BiJ i (cid:33)
 P = −gk dk P
 (59) (60) using diagonal blocks Gauss-Newton matrix avoids communication partitions, resulting direction be good using whole Gauss-Newton matrix.
 extend approach Wang al.
 (2015) pay extra cost improving direction.
 idea is CG procedure using sub-sampled Hessian, consider full Hessian adjust direction.
 CG procedure use block diagonal approximation sub-sampled matrix GSk, consider whole GSk adjusting direction.
 Speciﬁcally, dk is obtained CG procedure, solve following two-variable optimization problem involves GSk. min β1,β2 (β1dk + β2 ¯dk)T GSk(β1dk + β2 ¯dk) + ∇f (θk)T (β1dk + β2 ¯dk), (61) ¯dk is chosen vector.
 new direction is dk ← β1dk + β2 ¯dk.
 follow Wang al.
 (2015) choose ¯dk = dk−1.
 Notice choose ¯d0 be vector.
 possible advantage considering dk−1 is is previous iteration using different data subset Sk−1 subsampled Gauss-Newton matrix.
 provides information instances current Sk. solve (61), GSk is positive deﬁnite, is equivalent solving follow- ing two-variable linear system.
  (dk)T GSkdk (¯dk)T GSkdk   β1 β2  =  −∇f (θk)T dk −∇f (θk)T ¯dk 
 (62) (¯dk)T GSkdk (¯dk)T GSk ¯dk Note construction (62) involves communication partitions; see detailed discussion Section V supplementary materials.
 effectiveness using (61) is investigated Section VII.
 situations, linear system (62) be ill-conditioned.
 set β1 β2 = ε is small number.
 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(dk)T GSkdk (¯dk)T GSkdk (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ ε, (¯dk)T GSkdk (¯dk)T GSk ¯dk (63) Synchronization Between Partitions setting (51) has made node conduct own CG procedure communication, wait nodes complete tasks getting next Newton iteration.
 synchronization cost be signiﬁcant.
 note running time partition vary following reasons.

 select subset weights layers partition, number variables partition be different.
 example, assume network structure is 50-100-2.
 last layer has neurons small number classes.
 weight matrix W m, partition last layers have most variables.
 contrast, partition ﬁrst layers have more variables.
 Therefore, split variables make partitions balanced possible.
 example be given introduce experiment settings Section
 node start ﬁrst CG iteration needed information is available.
 calculation information needed matrix-vector products involves backward process, partitions corresponding neurons last layers start CG procedure ﬁrst layers.
 reduce synchronization cost, possible solution is terminate CG pro- cedure partitions reaches CG stopping condition: (cid:88) i∈Sk ||(λk + )vp + |Sk| (J i p)T BiJ i pvp + gp|| ≤ σ||gp||.
 However, setting CG procedure terminate early partitions have conducted CG steps
 strike balance, implementation terminate CG procedure partitions following conditions are satisﬁed:
 partition has reached pre-speciﬁed minimum number CG iterations, CGmin.

 certain percentage partitions have reached stopping conditions, (64).
 Section conduct experiments different percentage values check effectiveness setting.
 Summary Procedure summarize Algorithm proposed distributed subsampled Hessian Newton algorithm.
 materials described section, explain other steps algorithm.
 First, most optimization algorithms, direction dk is suitable step size αk be decided ensure sufﬁcient decrease f (θk + αkdk).


 .} such consider backtracking line search selecting largest αk ∈ {1, following sufﬁcient decrease condition function value holds.
 f (θk + αkdk) ≤ f (θk) + ηαk∇f (θk)T dk, (65) η ∈ (0, is pre-deﬁned constant.
 Secondly, follow Martens (2010); Martens Sutskever (2012); Wang al.
 (2015) apply Levenberg-Marquardt method introducing term λkI linear system (17).
 Deﬁne ρk = f (θk + αkdk) − f (θk) αk∇f (θk)T dk + GSkdk ratio actual function reduction predicted reduction.
 Based following rule derives next λk+1.
  λk+1 = λk × drop ρk > λk ≤ ρk ≤ (66) λk × boost otherwise, (drop,boost) are given constants.
 Therefore, predicted reduction is true function reduction, reduce λk such direction closer Newton direction is considered.
 contrast, ρk is small, enlarge λk conservative direction negative gradient is considered.
 Note line search serves way adjust direction according function-value reduction, optimization literature line search Levenberg- Marquardt method are applied concurrently.
 Interestingly, recent studies Newton methods neural networks, techniques are considered.
 preliminary investigation Section VI supplementary materials shows using Levenberg- Marquardt method line search is helpful, more detailed studies be future research issue.
 Algorithm show master-master implementation, same program is used partition.
 careful designs are needed ensure partitions get consistent information.
 example, use same random seed ensure iteration partitions select same set Sk constructing subsampled Gauss-Newton matrix.
 Analysis Proposed Algorithm section, analyze memory requirement, computational cost, communication cost.
 assume full training set is used.
 subsampled Hessian method Section is applied, Jacobian calculation Gauss-Newton matrix vector product “l” term analysis be replaced subset size |S|.
 Memory Requirement Partition Assume partition corresponds neuron sub-groups Tm−1 layer m− Tm layer m.
 consider following situations.

 Local weight matrix: partition store local weight matrix.
 tj ∀t ∈ Tm−1, ∀j ∈ Tm. wm Tm−1 is ﬁrst neuron sub-group layer m − needs store j ∀j ∈ Tm. bm Therefore, memory usage partition local weight matrix is propor- tional |Tm−1| × |Tm| + |Tm|.

 Function evaluation: From Section store part zm−1,i zm,i vec- tors.4 memory usage partition is l × (|Tm−1| + |Tm|).

 Gradient evaluation: First, store ∂f ∂wm tj ∂f ∂bm t ∈ Tm−1, j ∈ Tm gradient evaluation.
 Second, backward process, (45), store ∂ξi ∂zm−1,i ∀t ∈ Tm−1, ∀i ∀j ∈ Tm, ∀i.
 ∂ξi ∂zm,i Therefore, memory usage partition is proportional (|Tm−1| × |Tm| + |Tm|) + l × (|Tm−1| + |Tm|).

 Jacobian evaluation: discussion Section memory consumption is proportional l × nL × (|Tm−1| + |Tm|).
 (69) summary, memory bottleneck is terms are related number in- stances.
 reduce memory use, have considered technique Section replace term l (69) smaller subset size |Sk|.
 discuss technique reduce memory consumption Section same vector is used store s vector is transformed z activation function.
 Computational Cost analyze computational cost partition.
 sake simplicity, make following assumptions.
 • mth layer neurons are split several sub-groups, has |Tm| elements.
 • Calculating activation function σ(s) needs operation.
 following analysis is partition layers m − m.

 Function evaluation: From Algorithm sm−1,i i =


 l, t ∈ Tm−1 are calculate (19) (20).
 dominant one is (20), compu- tational cost function evaluation is O(l × |Tm| × |Tm−1|).

 Gradient evaluation: Assume current partition has received ∂ξi/∂zm,i i =


 l, j ∈ Tm. From calculate l(cid:88) l(cid:88) i=1 i=1 ∂f ∂wm tj wm tj + wm tj + costs ∂ξi ∂wm tj ∂ξi ∂zm,i σ(cid:48)(sm,i )zm−1,i ∀t ∈ Tm−1, ∀j ∈ Tm, O(l × |Tm| × |Tm−1|).
 reduce operation (45), calculating local sum (cid:88) j∈Tm σ(cid:48)(sm,i tj i =


 l, t ∈ Tm−1.
 )wm ∂ξi ∂zm,i has similar cost.
 computational cost gradient evaluation is O(l × |Tm| × |Tm−1|).

 Jacobian evaluation: From (46) (47) Algorithm computational cost is O(nL × l × |Tm| × |Tm−1|).

 Gauss-Newton matrix-vector products: Following (57) Section compu- tational cost Gauss-Newton matrix vector products is CG iterations × (l × (|Tm−1| × |Tm| + × |Tm|))
 (73) (70)-(73), derive following conclusions.

 computational cost is proportional number training data, number classes, number variables partition.

 general, (72) (73) dominate computational cost.
 Especially, number CG iterations is large, (73) becomes bottleneck.

 subsampling techniques Section is used, l (72)-(73) is replaced size subset.
 computational cost partition Newton iteration be reduced.
 However, number iterations be increased.

 computational cost be reduced splitting neurons layer many sub-groups possible.
 However, partition corresponds computing partitions imply higher synchronization cost.
 total number neurons layer is different, size partition signiﬁcant vary, situation further worsens synchronization issue.
 Communication Cost have shown Section using diagonal blocks Gauss-Newton matrix, partition conducts CG procedure communicating others.
 However, communication cost occurs function, gradient, Jacobian evalua- tion.
 discuss details Jacobian evaluation situation others is similar.
 simplify discussion make following assumptions.

 mth layer neurons are split several sub-groups, has |Tm| elements.
 number neuron sub-groups layer m is nm/|Tm|.

 partition sends receives message time.

 Following Barnett al.
 time send receive vector v is α + β × |v|, |v| is length v, α is start-up cost transfer β is transfer rate network.

 time add vector v vector same size is γ × |v|.

 Operations (including communications) independent groups nodes be con- ducted parallel.
 trees Figure IV.3 supplementary ma- terials involve independent sets partitions.
 assume reduce operations be conducted parallel.
 From (36), partitions layers m − m correspond same neuron sub-group Tm−1 layer m − reduce operation


 nL, t ∈ Tm−1, i =


 l sums u /∂zm−1,i u = nm|Tm| vectors l × × |Tm−1| size.
 layer Figure is split groups A2, B2 C2, sub-group A1 layer vectors (A1, A2), (A1, B2) (A1, C2) are reduced.
 Following analysis Pjeˇsivac-Grbovi´c et al.
 communication cost reduce operation is O((cid:100)(log2( nm|Tm|)(cid:101) × (α + (β + γ) × (l × × |Tm−1|))
 (74) Note layers m − m nm−1 |Tm−1| reduce operations are conducted takes communication cost shown (74).
 However, assumption be parallelized.
 reduced vector size l × × |Tm−1| is broadcasted nm−2/|Tm−2| partitions.
 Similar (74), communication cost is O((cid:100)(log2( nm−2 |Tm−2|)(cid:101) × (α + β × (l × × |Tm−1|))).
 γ factor (74) does appear do need sum vectors.
 total communication cost Jacobian evaluation is sum (74) (75).
 make following conclusions.

 communication cost is proportional number training instances number classes.

 (74) (75), smaller |Tm−1| reduces communication cost.
 split neurons layer many groups following reasons.
 assumed independent sets partitions, operations including communication set be parallelized.
 independent sets higher synchronization cost.
 Second, are many partitions block diagonal matrix (49) be good approximation Gauss-Newton matrix.
 Other Implementation Techniques section, discuss additional techniques implemented proposed algo- rithm.
 Pipeline Techniques Function Gradient Evaluation discussion Section indicates proposed method memory require- ment, computational cost communication cost linearly increase number data.
 product Gauss-Newton matrix vector, have considered using subsampled Gauss-Newton matrices Section reduce cost.
 avoid function gradient evaluations become bottleneck, discuss pipeline technique.
 idea follows fact (4) ξi,∀i are independent other.
 situation is same (J i)T∇zL,iξ(zL,i; yi),∀i (6).
 Therefore, forward (or backward) process, results related instance xi are be passed partitions next (or pre- vious) layers.
 consider mini-batch implementation.
 Take function evalu- ation example.
 Assume {1,


 l} is split R equal-sized subsets S1,


 SR.
 variable partition layers m − m, showed local values (20) are obtained instances i =


 l.
 calculate (cid:88) t∈Tm−1 tj zm−1,i wm + bm j j ∈ Tm, i ∈ Sr. values are used calculate ∀i ∈ Sr. sm,i setting achieve better parallelism.
 Further, split


 l} subsets same size, memory space allocated subset be reused another.
 memory usage is reduced R folds.
 Sparse Initialization A well-known problem training neural networks is easy overﬁtting enormous number weights.
 Following approach Section Martens (2010), implement sparse initialization weights train deep neural networks.
 neuron mth layer, nm−1 weights connected it, assign several weights have values N (0, distribution.
 Other weights are kept zero.
 examine effectiveness initialization Sections Existing Optimization Methods Training Neural Networks Besides Newton methods considered work, many other optimization methods have been applied train neural networks.
 brieﬂy discuss used section.
 Stochastic Gradient Methods deep neural networks, is time-consuming calculate gradient vector (6), go whole training data set.
 using data instances, stochastic gradient (SG) methods choose example (yik, xik) derive following sub-gradient vector update weight matrix.
 ∇f ik(θk) = θk + (J ik)T∇zL,ik ξ(zL,ik; yik).
 gives standard setting SG methods.
 Assume epoch means SG procedure goes whole training data set
 Based frequent updates weight matrix, SG methods get reasonable solution few epochs.
 advantage SG methods is Algorithm is easy implement.
 However, variance gradient vector instance is large, SG methods have slow convergence.
 address issue, mini-batch SG method have been proposed accelerate convergence speed (e.g., Bottou, Dean Ngiam Baldi
 Assume Sk ⊂ {1,


 is subset training data.
 sub-gradient vector be follows: ∇f Sk(θk) = θk |Sk| (cid:88) i∈Sk (J i)T∇zL,iξ(zL,i; yi).
 However, SG methods meet ravines cause particular dimension appar- ent other dimensions, are easier drop local optima.
 Polyak (1964) proposes using previous direction momentum part current direction.
 set- ting decrease impact particular dimension.
 gives details mini-batch SG method momentum implemented Theano/Pylearn2 (Goodfellow et
 Many other variants SG methods have been proposed, has been shown (e.g., Sutskever mini-batch SG momentum is strong baseline.
 work do include other types SG algorithms comparison.
 Unfortunately, SG mini-batch SG methods have well known issue choosing suitable learning rate momentum coefﬁcient different problems.
 conduct experiments Section
 Experiments consider following data sets experiments.
 Sensorless come training test sets.
 split described below.
 HIGGS: binary classiﬁcation data set is high energy physics applica- tions.
 is selected experiments feedforward networks have been applied (Baldi
 Note scalar output y is enough represent classes binary classiﬁcation problem.
 Based idea, set nL have yi ∈ {−1,
 predicted outcome is ﬁrst class y ≥ is second class y <
 data set is used Section comparison results Baldi al.
 (2014).
 Letter: set is Statlog collection (Michie al., scale values feature be [−1,
 MNIST: data set hand-written digit recognition (LeCun al., is used benchmark classiﬁcation algorithms.
 consider scaled ver- sion, feature value is divided
 Pendigits: data set is Alimoglu Alpaydin (1996).
 Poker: data set is UCI machine learning repository
 has been studied by, example, Li (2010).
 Satimage: set is Statlog collection (Michie al., scale values feature be [−1,
 • SensIT Vehicle: data set, Duarte Hu (2004), includes signals acoustic seismic sensors order classify different vehicles.
 use original version scaling.
 Sensorless: data set is Paschke al.
 (2013).
 scale values feature be [0, conduct stratiﬁed random sampling select instances be test set rest data be training set.
 • SVHN: data, Google Street View images, consists colored images house numbers (Netzer et al.,
 scale data set [0, considering largest smallest feature values entire data set.
 M ≡ max max (xi)p m ≡ min min (xi)p.
 pth element xi is changed (xi)p ← (xi)p − m M − m • USPS: data set, Hull (1994), is used recognizing handwritten ZIP codes scale values feature be [−1,
 data sets, statistics Table are Detailed settings data such network structure are given Table
 decide suitable network structure is scope work, possible, follow setting earlier works.
 example, consider structure Wan al.
 (2013) MNIST Neyshabur et al.
 (2015) SVHN.
 Table model used SVHN is largest.
 number neurons layer is further increased, model be stored different machines.
 give parameters used algorithm.
 sparse initialization discussed nm−1(cid:101) are Section nm−1 weights connected neuron layer m, (cid:100)√ selected have non-zero values.
 CG stopping condition set σ CGmax =
 minimal number CG steps run partition, CGmin, is set be
 implementation Levenberg-Marquardt method, data sets used be found https://www.csie.ntu.edu.tw/˜cjlin/ libsvmtools/datasets/.
 Table Summary data sets: n0 is number features, l is number training instances, lt is number testing instances, K is number classes.
 lt K Data set Letter MNIST Pendigits Poker Satimage SensIT Vehicle Sensorless SVHN USPS HIGGS set initial λ1 =
 (drop, boost) constants (66) are (2/3,
 solving (61) get update direction CG procedure, set ε (63).
 Analysis Distributed Newton Methods have proposed several techniques improve basic implementation Newton method distributed environment.
 investigate effectiveness considering following methods.
 Note high memory consumption larger sets, implement subsampled Hessian Newton method Table Details distributed network data.
 Sampling rate is percentage training data used calculate subsampled Gauss-Newton matrix.
 Data set Letter MNIST Pendigits Poker SensIT Vehicle Sensorless Satimage SVHN USPS Sampling rate Network structure Split structure partitions 16-300-300-300-300-26 1-2-1-1-1-1 784-800-800-10 16-300-300-10 1-1-3-1 1-2-2-1 10-200-200-200-10 1-1-1-1-1 100-300-300-3 1-2-2-1 48-300-300-300-11 1-2-1-2-1 36-1000-500-6 1-2-2-1 3072-4000-4000-10 3-2-2-1 256-300-300-10 1-2-2-1 discussed Section
 subsampled-GN: use whole subsampled Gauss-Newton matrix deﬁned (58) conduct matrix-vector product CG procedure solve (61) get update direction CG procedure (Wang

 diag: is same subsampled-GN only diagonal blocks sub- sampled Gauss-Newton matrix are used; see (60).

 diag + sync is same diag consider technique Section reduce synchronization time.
 terminate CG procedure partitions have reached local stopping conditions (64).

 diag + sync is same diag + sync terminate CG procedure partitions have reached local stopping conditions (64).
 above methods, consider following implementation details.

 set C = l regularization parameter.

 run experiments G1 type instances Microsoft Azure let instance use core.
 instances are virtual machines same computer, setting ensures variable partition corresponds machine.

 make computational cost partition balanced possible, ex- periments choose partitions such maximum ratio num- bers variables (|Tm|×|Tm−1|) partitions is low possible.
 example, Pendigits, largest partition has × = weight vari- ables, smallest partition has × = weight variables, ratio being
 most data sets, ratio is lower numbers classes is small, making number variables partitions involving output smaller other partitions.
 Figure show comparison results have following observations.

 test accuracy number iterations, subsampled-GN general has fastest convergence rate.
 reason be direction subsampled-GN solving linear system (59) is full Newton direction other methods, consider further approximations Gauss-Newton matrix early termination CG procedure.
 However, cost iteration is high, training time see subsampled-GN become worse other approaches.

 early termination CG procedure reduce cost iter- ation.
 However, stop CG procedure early, total training time increase.
 example, diag + sync is fastest beginning least cost iteration.
 is fastest end MNIST, Letter, USPS, Satimage, Pendigits.
 However, has slowest ﬁnal convergence SensIT Vechicle, Poker, Sensorless.
 Take data set Poker example.
 listed Table variables are split partitions, CG procedure stops partition (i.e., partitions) reaches local stopping condition.
 partition have lightest computational load is earliest start solving local linear system.6 Thus other partitions have run CG iterations.
 approach diag + sync does terminate CG procedure early.
 Overall ﬁnd is efﬁcient backward process Section partitions corresponding last layers begin CG procedures others.
 stable.
 Therefore, subsequent comparisons stochastic gradient methods, use setting Newton method.
 space consideration, have evaluated techniques pro- posed Section
 following techniques leave details Sections VI VII supplementary materials.

 Section propose combining dk dk−1 update direction.
 show technique is effective.

 mentioned Section line search Levenberg-Marquardt (LM) method be needed.
 preliminary results show training speed is improved techniques are applied.
 Comparison Stochastic Gradient Methods Support Vector Machines (SVM) section, compare methods SG methods SVMs, are pop- used multi-class classiﬁcation.
 Settings methods are described follows.

 Newton: method use setting diag + sync considered Section let C = l.

 SVM (Boser et consider RBF kernel.
 K(xi, xj) = e−γ||xi−xj||2, xi xj are data instances, γ is kernel parameter chosen users.
 Note SVM solves optimization problem similar (4), regu- larization parameter, C, be decided well.
 conduct ﬁve-fold cross vali- dation training set select best C ∈ {2−5l,


 best γ ∈ {2−15,


 use library LIBSVM (Chang Lin, training prediction.

 SG: use code Baldi al.
 (2014), implements
 objective function is same network structure data set is identical corresponding used Newton, set regularization parameter C = l.
 major modiﬁcation make is replace activation functions ours.
 Baldi al.
 authors use tanh activation functions layers


 L − sigmoid function layer L, experiments Newton methods Section use sigmoid function layers


 L − linear function layer L.
 initial learning rate is selected {0.05, ﬁve-fold cross validation.
 initial learning rate has been selected, conduct training process generate model prediction test set.
 regards stopping condition training process, terminate Newton consider SVM formulation represented (2).
 form considered LIBSVM, terms C are combined together, C/l is actual parameter be selected.
 SVHN lengthy time parameter selection, selected instances stratiﬁed sampling conduct ﬁve-fold cross validation.
 8Following Baldi et al.
 (2014), regularized weights biases.
 several experiments, found performance is similar regularization biases.
 method iteration.
 SG, terminates minimal number epochs have been conducted objective function value validation set does improve last N epochs (see Algorithm
 implement stopping condition, SG split input training set training validation.9 SVM, use default stopping condition LIBSVM.10 investigate effect initialization considering following settings.

 sparse initialization discussed Section
 dense initialization discussed Baldi al.
 (2014).
 initial weights are drawn normal distribution N (0, ﬁrst layer, N (0, output layer, N (0, other hidden layers.
 biases are initialized zeros.
 make fair comparison, setting, Newton SG are trained same initial weights biases.
 present comparison test accuracy Table make following ob- servations.

 neural sparse initialization results better accuracy dense initialization does.
 difference be huge cases, such CV procedure need stopping condition training sub-problem.
 do 80-20 split folds data data are used implement stopping condition.
 terminates violation optimality condition calculated based gradient is smaller tolerance.
 training using SG data set Letter.
 low accuracy initialized SG Letter be poor differentiation neurons dense initialization (Martens,
 Other possible causes include vanishing gradient problem (Bengio al., activations are trapped saturation regime sigmoid function (Glorot Bengio,
 Note impact initialization scheme Newton method is much
 Between SG Newton, sparse initialization is see Newton gives higher accuracy.

 sparse initialization is used, Newton method training neural networks gives similar higher accuracy SVM.
 results are much better Poker SVHN.
 compare results MNIST reported earlier works.
 Wan al.
 (2013) use connected neural network 800-neuron hidden layers de- rive error rate setting dense initialization,12 sigmoid activations, dropout technique.
 same network structure same activation func- error rate is iteration.
 SVHN, compare results Neyshabur al.
 (2015), same network structure ours is adopted, use ReLU activations hidden layers.
 choose cross-entropy objective function, utilize observe similar phenomena experiments HIGGS Section See Table
 Wan al.
 initial weights are drawn N (0, different dense initialization use.
 dropout regularization.
 dense initialization,13 train network Path-SGD method, uses proximal gradient method solve optimization problem.
 report accuracy (see Figure accuracy obtained Newton method sparse initialization is
 Poker, note Li (2010) uses abc-logitboost obtain higher accuracy, setting is different ours.
 expands training set including half test set, remaining half test set used evaluation.
 issue found experiments is SG is sensitive initial learning rate.
 Table present test accuracy SG different initial rates Poker problem.
 inappropriate initial learning rate lead much worse accuracy.
 Detailed Investigation HIGGS Data compare AUC values obtained Newton SG implementations reported Baldi al.
 (2014) HIGGS.
 method, sampling rate cal- culating subsampled Gauss-Newton matrix is set be
 Following setting Section consider initializations (dense sparse).
 type initialization, SG Newton start same initial weights biases.
 Note SG results are different Baldi al.
 use different activation functions initial values weights biases.14 Neyshabur al.
 initial weights wm tj are drawn N (0, different dense initialization use.
 initialization setting is same dense initialization, values used are available.
 Table Test accuracy SVM, Newton SG.
 SVM, show parameters (C, used.
 SG, show (the initial learning rate, number epochs reach stopping criterion).
 bold-faced entries indicate best accuracy obtained using neural networks.
 SVM Neural Networks Dense Initialization Sparse Initialization Newton SG Newton SG Letter MNIST (27l, (0.025, (0.002, (23l, (0.002, (0.002, Pendigits (27l, (0.001, (0.002, Poker (2−1l, (0.005, (0.002, Satimage (2l, (0.01, (0.001, SensIT Vehicle (2l, (0.01, (0.01, Sensorless (25l, (0.01, (0.005, SVHN USPS (25l, (25l, (0.001, (0.001, (0.025, (0.001, resource constraints, did conduct validation procedure select SG’s initial learning rate.
 Instead, used learning rate following Baldi al.
 (2014).
 results are shown Table see Newton method gives best AUC values.
 Section have mentioned SG’s performance be sensitive initial learning rate.
 poor results SG Table be did conduct selection procedure.
 decide investigate effect initial learning rate AUC value network structure used Table Test accuracy Poker using SG different initial learning rates η.
 Dense initialization is used.
 Note η = does yield highest test accuracy, was selected experiments Table giving highest CV accuracy.
 Initial learning rate η Test accuracy earlier experiment Table
 compare running time, SG Newton run same G3 type machine cores Microsoft Azure.
 results AUC values versus number iterations training time are shown Figure
 see performance SG depends initial learning rate.
 experiments indicate SG yield good performances suitable parameters, parameter selection procedure is essential.
 contrast, Newton methods are more robust do need ﬁne tune parameters.
 Discussion Conclusions future works, list following directions.

 is important extend proposed method other types neural networks.
 example, convolutional neural networks (CNNs) are popular computer vision ap- plications (e.g., Krizhevsky Simonyan Zisserman,
 Table A comparison AUC obtained SG distributed Newton HIGGS data set.
 list results Baldi al.
 (2014) reference, “NA” means result is reported.
 See explanation Section different results SG Baldi et al.’s.
 Network Split Dense Initialization Sparse Initialization Newton SG Newton SG Baldi al.
 (2014) 28-300-1 28-600-1 28-1000-1 28-2000-1 2-2-1 2-3-1 2-4-1 2-8-1 28-300-300-1 2-2-1-1 28-300-300-300-1 2-2-2-1-1 28-300-300-300-300-1 2-2-2-2-1-1 NA NA CNNs have fewer weights method has potential train deep networks large-scale image classiﬁcation.

 Gauss-Newton matrix, consider other ways use approx- imate Hessian such recent works et al.
 (2016).

 results Tables consider model running Newton iterations.
 advantage Newton stochastic gradient is apply gradient-based stopping condition.
 plan investigate practical use.

 is known using suitable preconditioners reduce number CG steps solving linear system.
 Studies applying preconditioned CG methods training neural networks include, example, Chapelle Erhan (2011).
 plan investigate apply preconditioning distributed framework.
 summary, paper proposed techniques implement distributed Newton methods training large-scale neural networks, achieved data model parallelisms.
 Acknowledgements work was supported part MOST Taiwan grant 105-2218-E-002-033 Microsoft Azure Research programs.
 References Alimoglu, F.
 Alpaydin, E.
 (1996).
 Methods combining multiple classiﬁers based different representations pen-based handwritten digit recognition.
 Pro- ceedings Fifth Turkish Artiﬁcial Intelligence Artiﬁcial Neural Networks Symposium.
 Baldi, P., Sadowski, P., Whiteson, D.
 (2014).
 Searching exotic particles high-energy physics deep learning.
 Nature Communications,
 Barnett, M., Gupta, S., Payne, D.
 G., Shuler, L., De Geijn, R., Watts, J.
 (1994).
 Interprocessor collective communication library (InterCom).
 Proceedings Scalable High-Performance Computing Conference, pages
 Bengio, Y., Simard, P., Frasconi, P.
 (1994).
 Learning long-term dependencies gradient descent is difﬁcult.
 IEEE Transactions Neural Networks,
 Bian, Y., Li, X., Cao, M., Liu, Y.
 (2013).
 Bundle CDN: parallelized ap- proach large-scale l1-regularized logistic regression.
 Proceedings European Conference Machine Learning Principles Practice Knowledge Discov- ery Databases (ECML/ PKDD).
 Boser, B.
 E., Guyon, I., Vapnik, V.
 (1992).
 training algorithm optimal margin classiﬁers.
 Proceedings Fifth Annual Workshop Computational Learning Theory, pages
 ACM Press.
 Bottou, L.
 (1991).
 Stochastic gradient learning neural networks.
 Proceedings Neuro-Nımes,
 Bottou, L.
 (2010).
 Large-scale machine learning stochastic gradient descent.
 Proceedings COMPSTAT pages
 Byrd, R.
 H., Chin, G.
 M., Neveitt, W., Nocedal, J.
 (2011).
 use stochastic Hessian information optimization methods machine learning.
 SIAM Journal Optimization,
 Chang, C.-C.
 Lin, C.-J.
 (2011).
 LIBSVM: library support vector machines.
 ACM Transactions Intelligent Systems Technology,
 Software available http://www.csie.ntu.edu.tw/˜cjlin/libsvm.
 Chapelle, O.
 Erhan, D.
 (2011).
 Improved preconditioner Hessian free optimiza- tion.
 NIPS Workshop Deep Learning Unsupervised Feature Learning.
 Ciresan, D.
 C., Meier, U., Gambardella, L.
 M., Schmidhuber, J.
 (2010).
 Deep, big, simple neural nets handwritten digit recognition.
 Neural Computation,
 Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Le, Q.
 V., Mao, M.
 Z., Ranzato, M., Senior, A.
 W., Tucker, P.
 A., al.
 (2012).
 Large scale distributed deep networks.
 Advances Neural Information Processing Systems (NIPS)
 Duarte, M.
 Hu, Y.
 H.
 (2004).
 Vehicle classiﬁcation distributed sensor networks.
 Journal Parallel Distributed Computing,
 Glorot, X.
 Bengio, Y.
 (2010).
 Understanding difﬁculty training deep feed- forward neural networks.
 Proceedings International Conference Artiﬁcial Intelligence Statistics (AISTATS), pages
 Goodfellow, I.
 J., Warde-Farley, D., Lamblin, P., Dumoulin, V., Mirza, M., Pascanu, R., Bergstra, J., Bastien, F., Bengio, Y.
 (2013).
 Pylearn2: machine learning research library.
 He, K., Zhang, X., Ren, S., Sun, J.
 (2015).
 Delving deep rectiﬁers: Surpass- ing human-level performance ImageNet classiﬁcation.
 Proceedings IEEE International Conference Computer Vision (ICCV).
 He, X., Mudigere, D., Smelyanskiy, M., Tak´aˇc, M.
 (2016).
 Large scale distributed Hessian-free optimization deep neural network.
 arXiv preprint arXiv:1606.00511.
 Hinton, G.
 E., Deng, L., Yu, D., Dahl, G., rahman Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T., Kingsbury, B.
 (2012).
 Deep neural networks acoustic modeling speech recognition: shared views research groups.
 IEEE Signal Processing Magazine,
 Hull, J.
 J.
 (1994).
 database handwritten text recognition research.
 IEEE Transac- tions Pattern Analysis Machine Intelligence,
 Kiros, R.
 (2013).
 Training neural networks stochastic Hessian-free optimization.
 arXiv preprint arXiv:1301.3641.
 Krizhevsky, A., Sutskever, I., Hinton, G.
 E.
 (2012).
 ImageNet classiﬁcation deep convolutional neural networks.
 Pereira, F., Burges, C.
 J.
 C., Bottou, L., Weinberger, K.
 Q., editors, Advances Neural Information Processing Systems pages
 LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.
 (1998a).
 Gradient-based learning ap- plied document recognition.
 Proceedings IEEE,
 MNIST database available http://yann.lecun.com/exdb/mnist/.
 LeCun, Y., Bottou, L., Orr, G.
 B., M¨uller, K.-R.
 (1998b).
 Efﬁcient back- prop.
 Neural Networks, Tricks Trade, Lecture Notes Computer Science LNCS
 Springer Verlag.
 Li, P.
 (2010).
 empirical evaluation algorithms multi-class classi- ﬁcation: Mart, abc-mart, robust logitboost, abc-logitboost.
 arXiv preprint arXiv:1001.1020.
 Lichman, M.
 (2013).
 UCI machine learning repository.
 Mahajan, D., Keerthi, S.
 S., Sundararajan, S.
 (2017).
 distributed block coordinate descent method training l1 regularized linear classiﬁers.
 Journal Machine Learning Research,
 Martens, J.
 (2010).
 Deep learning Hessian-free optimization.
 Proceedings 27th International Conference Machine Learning (ICML).
 Martens, J.
 Sutskever, I.
 (2012).
 Training deep recurrent networks Hessian-free optimization.
 Neural Networks: Tricks Trade, pages
 Springer.
 Michie, D., Spiegelhalter, D.
 J., Taylor, C.
 C., Campbell, J., editors (1994).
 Ma- chine learning, neural statistical classiﬁcation.
 Ellis Horwood, Upper Sad- dle River, NJ, USA.
 Data available http://archive.ics.uci.edu/ml/ machine-learning-databases/statlog/.
 Moritz, P., Nishihara, R., Stoica, I., Jordan, M.
 I.
 (2015).
 SparkNet: Training deep networks Spark.
 arXiv preprint arXiv:1511.06051.
 Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., Ng, A.
 Y.
 (2011).
 Reading digits natural images unsupervised feature learning.
 NIPS Workshop Deep Learning Unsupervised Feature Learning.
 Neyshabur, B., Salakhutdinov, R.
 R., Srebro, N.
 (2015).
 Path-SGD: Path- normalized optimization deep neural networks.
 Cortes, C., Lawrence, N.
 D., Lee, D.
 D., Sugiyama, M., Garnett, R., editors, Advances Neural Information Processing Systems pages
 Ngiam, J., Coates, A., Lahiri, A., Prochnow, B., Le, Q.
 V., Ng, A.
 Y.
 (2011).
 optimization methods deep learning.
 Proceedings International Conference Machine Learning, pages
 Paschke, F., Bayer, C., Bator, M., M¨onks, U., Dicks, A., Enge-Rosenblatt, O., Lohweg, V.
 (2013).
 Sensorlose zustands¨uberwachung synchronmotoren.
 Pro- ceedings Computational Intelligence Workshop.
 Pearlmutter, B.
 A.
 (1994).
 Fast exact multiplication Hessian.
 Neural Computa- tion,
 Pjeˇsivac-Grbovi´c, J., Angskun, T., Bosilca, G., Fagg, G.
 E., Gabriel, E., Dongarra, J.
 J.
 (2007).
 Performance analysis MPI collective operations.
 Cluster Computing,
 Polyak, B.
 T.
 (1964).
 methods speeding convergence iteration meth- ods.
 USSR Computational Mathematics Mathematical Physics,
 Schraudolph, N.
 N.
 (2002).
 Fast curvature matrix-vector products second-order gradient descent.
 Neural Computation,
 Simonyan, K.
 Zisserman, A.
 (2014).
 deep convolutional networks large- scale image recognition.
 arXiv preprint arXiv:1409.1556.
 Sutskever, I., Martens, J., Dahl, G., Hinton, G.
 (2013).
 importance ini- tialization momentum deep learning.
 Proceedings 30th International Conference Machine Learning (ICML), pages
 Taylor, G., Burmeister, R., Xu, Z., Singh, B., Patel, A., Goldstein, T.
 (2016).
 Train- ing neural networks gradients: A scalable ADMM approach.
 Proceedings Thirty Third International Conference Machine Learning, pages
 Thakur, R., Rabenseifner, R., Gropp, W.
 (2005).
 Optimization collective com- munication operations MPICH.
 International Journal High Performance Com- puting Applications,
 Wan, L., Zeiler, M., Zhang, S., LeCun, Y., Fergus, R.
 (2013).
 Regularization neural networks using DropConnect.
 Proceedings 30th International Conference Machine Learning (ICML), pages
 Wang, C.-C., Huang, C.-H., Lin, C.-J.
 (2015).
 Subsampled Hessian Newton meth- ods supervised learning.
 Neural Computation,
 Zinkevich, M., Weimer, M., Smola, A., Li, L.
 (2010).
 Parallelized stochastic gra- dient descent.
 Lafferty, J., Williams, C.
 K.
 I., Shawe-Taylor, J., Zemel, R., Culotta, A., editors, Advances Neural Information Processing Systems pages
 Algorithm Function evaluation distributed system Let Tm−1 Tm be subsets neurons (m − mth layers corre- sponding current partition.
 input, i =


 l, t ∈ Tm−1.
 m = Read sm−1,i Wait sm−1,i Calculate zm−1,i i =


 l, t ∈ Tm−1.
 (19).
 end calculating (20), run allreduce operation have i =


 l j ∈ Tm, sm,i (27) available partitions layers m − m corresponding Tm. Tm−1 is ﬁrst neuron sub-group layer m − m < L broadcast values (27) partitions layers m m + corresponding neuron subgroup Tm; see description (23) Calculate l(cid:88) (cid:88) i=1 j∈TL ξ(zL,i j) + accumulated regularization terms TL is ﬁrst neuron sub-group layer L, run reduce operation get ﬁnal f; see (24).
 end end Algorithm Calculation ∂zL,i tributed system.
 Let Tm−1 Tm be subsets neurons (m − mth layers corre- u =


 nL, j =


 ,|Tm| dis- u /∂sm,i sponding current partition.
 m = L Calculate ∂zL,i ∂zm,i 2(zL,i u − yi u) j = u, j (cid:54)= u, u =


 nL, i =


 l, j ∈ Tm. Wait ∂zL,i u /∂zm,i end Calculate u =


 nL, i =


 l, j ∈ Tm. ∂zL,i ∂sm,i ∂zL,i ∂zm,i σ(cid:48)(sm,i ), u =


 nL, i =


 l, j ∈ Tm. (46) (47) m > Calculate local sum (cid:88) j∈Tm tj t ∈ Tm−1 wm ∂zL,i ∂sm,i do reduce operation obtain ∂zL,i ∂zm−1,i u =


 nL, i =


 l, t ∈ Tm−1.
 Tm is ﬁrst neuron sub-group layer m Broadcast values (48) partitions layers m − m − corresponding neuron sub-group Tm−1 layer m − see description (38).
 end end Algorithm A distributed subsampled Hessian Newton method variable partition.
 Given  ∈ (0, λ1, σ ∈ (0, η ∈ (0, CGmax, CGmin, r ∈ (0,
 Let p be index current partition generate initial local model vector p.
 Compute f (θ1).
 k =


 do Choose set Sk ⊂ {1,


 l}.
 p,∀i ∈ Sk. Compute gk solve linear system (60) CG obtain direction dk p J i end Update λk+1 based (66).
 end |Sk|(cid:88) + |Sk| p ≥ CGmax i=1 is satisﬁed #CGk ||(λkI + (J i p)T BiJ i p)dk p + gk p|| ≤ σ||gk p|| {# partitions ﬁnished ≥ r% × P #CGk p ≥ CGmin}, p is number CG iterations have been run far.
 p = β1dk p + β2dk−1 solving (61).
 #CGk Derive dk αk =
 true do p = θk Update θk+1 Tm Tm−1 are ﬁrst neuron subgroups layers L L− respec- p compute f (θk+1).
 p + αkdk tively, (65) is satisﬁed Notify partitions stop.
 end else Wait notiﬁcation stop.
 end stop notiﬁcation has been received break; end αk = αk/2.
 Standard stochastic gradient methods Given learning rate η.
 k =


 do Choose ik ∈ {1,


 l}.
 θk+1 = θk − η∇f ik(θk).
 end Algorithm Mini-batch stochastic gradient methods Theano/Pylearn2 (Goodfellow et
 Given epoch min epochs learning rate η, minimum learning rate ηmin = α = r = X = N = batch size b = |Sk| = initial momentum m0 ﬁnal momentum mf = decay factor γ = updating vector v ←
 counter ← N.
 lowest value ← ∞.
 epoch < min epochs counter > do Split whole training data K disjoint subsets, Sk, k =


 K.
 α ← min(epoch/min epochs,
 m ← (1 − α)m0 + αmf.
 k =


 K do v ← mv − max(η/γr, ηmin)∇f Sk(θ).
 θ ← θ + v.
 r r +
 end epoch ← epoch +
 Calculate function value h validation set.
 (h < (1 − X)× lowest value) ← N.
 counter ← counter −
 end lowest value ← min(lowest value, h).
 end (a) SensIT Vehicle (b) poker (c) MNIST (d) Letter Accuracy (%)Iterationssubsampled-GNdiagdiag + sync + sync 2000Testing Accuracy (%)Training time secondssubsampled-GNdiagdiag + sync + sync Accuracy (%)Iterationssubsampled-GNdiagdiag + sync + sync 2000Testing Accuracy (%)Training time secondssubsampled-GNdiagdiag + sync + sync Accuracy (%)Iterationssubsampled-GNdiagdiag + sync + sync Accuracy (%)Training time secondssubsampled-GNdiagdiag + sync + sync Accuracy (%)Iterationssubsampled-GNdiagdiag + sync + sync 6000Testing Accuracy (%)Training time secondssubsampled-GNdiagdiag + sync + sync (e) USPS (f) Pendigits (g) Sensorless (h) Satimage Figure A comparison different techniques implement distributed Newton meth- ods.
 Left: testing accuracy number iterations.
 Right: testing accuracy training time.
 Accuracy (%)Iterationssubsampled-GNdiagdiag + sync + sync Accuracy (%)Training time secondssubsampled-GNdiagdiag + sync + sync Accuracy (%)Iterationssubsampled-GNdiagdiag + sync + sync Accuracy (%)Training time secondssubsampled-GNdiagdiag + sync + sync Accuracy (%)Iterationssubsampled-GNdiagdiag + sync + sync 8000Testing Accuracy (%)Training time secondssubsampled-GNdiagdiag + sync + sync Accuracy (%)Iterationssubsampled-GNdiagdiag + sync + sync Accuracy (%)Training time secondssubsampled-GNdiagdiag + sync + sync (a) Dense initialization.
 (b) Sparse initialization.
 Figure A comparison SG Newton.
 28-300-300-1 network is applied train HIGGS.
 SG-x means initial learning rate x is used.
 Newton, iteration means go line line Algorithm SG, iteration means go whole training data once.
 curve SG-0.03 dense initialization is presented AUC value exceeds Left: AUC number iterations.
 Right: AUC training time seconds (log-scaled).
 + sync 50%SG-0.02SG-0.01SG-0.005SG-0.001 time seconds (log-scaled)diag + sync 50%SG-0.02SG-0.01SG-0.005SG-0.001 + sync 50%SG-0.03SG-0.02SG-0.01SG-0.005SG-0.001 time seconds (log-scaled)diag + sync 50%SG-0.03SG-0.02SG-0.01SG-0.005SG-0.001
 early work Legendre Gauss late XVIII century, linear nonlinear regression has employed space deﬁned input data project target desired re- sponse ﬁnd, training set, optimal set model parameters mean square error minimization.
 ap- proach has been embraced adaptive signal pro- cessing [1], control theory, pattern recognition machine learning communities [2], has become stan- dard perform function approximation.
 pursuit alternative is based theoretic rea- sons, i.e. expand horizon function approximation theory, impact current machine learning applica- tions is higher.
 conventional modeling approach, system created input-desired data pairs is linear model be substituted nonlinear model (e.g. artiﬁcial neural networks), means optimization becomes nonlinear parameters.
 Thanks XYZ agency funding.
 implies local minima exist performance sur- face, gradient search techniques become slow, cumber- is guarantee ﬁnding optimal solution.
 is current bottlenecks nonlinear modeling machine learning.
 methods, meanwhile, error training parameter, available error information be utilized provide novel approach function demonstrate here.
 vision is create universal learning systems are easy train guarantee converge optimum, called convex universal learning machines [3].
 CULMs are universal mappers architectures do have hidden layer do need train hidden layer weights.
 distinctive class CULMs are Kernel Adaptive Filters (KAFs) project input data Reproducing Kernel Hilbert Space (RKHS) using positive deﬁnite kernel function use linear methods train parameters [4].
 difﬁculty is employing representer theorem RKHS, ﬁlter output is computed past data, ﬁlter computation grows super-linearly respect sample number n, is unrealistic real word applications sparseness pro- cedures [5].
 other class, including reservoir computing, uses stochastic approaches based random hidden param- eters exempliﬁed Extreme Learning Machine (ELM), suffers incomplete theoretic understanding re- quires many tricks achieve useful reproducible results [6].
 spite shortcomings, ELMs are popular, means need achieve uni- versal processing generalization capacity function ap- proximation is unmet.
 propose new solution design CULMs based conventional Finite Impulse Response (FIR) linear model extended table lookup.
 using input span projection space, use full joint space projection space, hence approach is named Aug- mented Space Linear Model (ASLM).
 Augmented desired signal, framework ASLM expends data input space, assumed dimension L, L + dimensional space.
 L + independent bases span L space, means training set error be small adaptation method achieve using linear approach.
 are difﬁculties need be addressed Fig.

 application geometric structure ASLM ASLM.
 ﬁrst is weights go af- ter training one is connected desired, approaches one.
 means need regularization adaptation process.
 second difﬁculty is don’t have desired signal test phase! address issues here, use difference outputs input space desired joint space training phase (the training error) augment input space expanding input desired.
 store training errors table indexed input data.
 novel solution takes advantage extra infor- mation contained training errors, are wasted con- ventional least squares, approach nonlinear relationships linear model table.
 ASLM is adaptive linear architecture convex optimization, train- ing error is orthogonal bases, adaptation process needs be regularized.
 Meanwhile, computa- tional complexity training testing is lower nonlinear methods, is suited online learning algorithms.
 matter fact, ASLM is intermediate so- lution complexity-accuracy design space linear model (easy accurate) nonlin- ear models (complex be more accurate).
 Dif- ferent traditional linear nonlinear models, augmented space model framework makes full use training errors, improve others models (linear nonlin- ear) well.

 AUGMENTED SPACE LINEAR MODEL simplest implementation ASLM is presented be- low.
 left part Fig.1 shows conceptual least square solution ﬁnding best approximation (y) desired response (d) projecting d space spanned mul- tidimensional input x.
 minimum error is achieved y is orthogonal projection d input subspace.
 is sufﬁcient add error e output y obtain desired response d training set, is def- inition perpendicular input space.
 computing output ASLM, are using joint space evaluate desired.
 Consider set N pairs training input vectors desired output {xi, di}N i=1, i denotes discrete time in- stant.
 ﬁrst compute weights linear model input space training data, be evaluated Least Squares (LS) solution (1) w =(cid:0)δI + XT X(cid:1)−1 XT d (1) X = [x1,··· xN ]T d = [d1,··· dN ] δ is small value prevent rank deﬁciency.
 create table ad- dressed input relates input training error, store table.
 size table be training set size quantization is introduced.
 test phase, use current input ﬁnd closest entry table, read corresponding training set error approximate desired response, i.e. equation (2) ˆyi = yi + e∗ = wT xi + e∗ (2) yi is current output LS solution, e∗ is er- ror obtained training set, is corresponding closest xi index show right part Fig.1. l2 norm is used measure distance transformed sam- ples produced Hadamard product w◦x.
 Considering inputs whole training set, Hadamard metric get reasonable results ﬁnding closest sample.
 error e∗ equation (2) be good approximation desired conditions: (1): e∗ is good approximation current test sample xi training error; (2): error test set given input remains stationary training testing.
 second assumption be im- posed conventional functional approximation, ASLM, requirement applies instantaneous errors is demanding practical noisy conditions.
 realistic application cases, use quantization approach cut noise training data decreasing table size.
 ASLM is simplest model augmented space, use augment KAF nonlinear model.
 KAFs are universal models, is Input SpaceAugmented SpaceTrainTeste*x difﬁcult achieve good approximation desired linear combination Gaussian kernel, rattling insufﬁcient training data.
 order compute training error augmented space, ﬁrst train nonlinear model usual, ﬁxed weights, compute training errors once.
 create table addressed input store table mentioned before.
 test phase, use current input ﬁnd closest entry table, read corresponding training error approximate desired response, i.e. equation (2).
 don’t have w nonlinear models, measure distance input samples l2 norm directly.
 order improve efﬁciency ﬁnding nearest neighbor, use kd tree store data table searching complexity O(log(N )) [7].
 Hence, testing computational complexity augmented space model con- sists parts.
 is complexity algorithm (lin- ear nonlinear) compute system output, other is complexity searching best error aug- mented space model.
 testing computational complexity ASLM, example, is O(L+log(N )).
 training, ASLM is fast, needs create table least squares is faster train- ing nonlinear model.
 compare performance computational complexity proposed ASLM sev- eral linear nonlinear models next section.

 SIMULATION RESULTS Prediction Without Noise order evaluate role ASLM current methodologies function approximation system identi- ﬁcation, select competing Least Squares (LS) example optimal linear Near- est Neighbor (KNN) [8] memory based ap- proach Kernel Least Mean Square (KLMS), using Gaussian kernel, CULM rivals performance best nonlinear networks prediction.
 include KLMS Augmented space model (KLMS-AM), extra comparison, show general capabilities Augmented space model.
 hyper parameters were validated get best possible results including kernel size step size η regularization factor δ.
 simplicity, choose K = KNN algorithm parameters are showed last column table
 inclusion memory based method is judged important ASLM uses table lookup is similar spirit memory based approaches modeling.
 problem selected is prediction x component Lorenz system [4], has been studied liter- ature (order L=7 according Takens embedding theorem) [9].
 Lorenz data set is generated differential equation parameters β δ = ρ =
 ﬁrst order approximation is used step size parameter
 Segments samples are used train set following samples are testing set.
 normalize time series zero-mean unit variance.
 Performance is measured power error.
 Results are averaged independent training-test runs obtained sliding window generated data samples time.
 Table
 performance comparison linear nonlinear algorithm Algorithm Testing MSE Parameter Training MSE KLSM-AM ± ± ± ± ± KLMS ASLM KNN LS σ = η = K = K = K = δ show testing MSE training MSE ta- ble
 KLMS is online algorithm other algorithms are batch testing MSE KLMS is cal- culated last points converged learning curve.
 terms performance, see LS is worst performer.
 KNN is better, notice ASLM improves KNN performance same stor- age.
 KLMS is little better ASLM, be improved augmented space model well.
 However, take consideration accuracy computation time, ASLM appears good compromise performance nonlinear linear model.
 Fig.
 Fig.

 computation time storage comparison lin- ear nonlinear algorithm shows computation time storage test phase compared algorithms.
 terms simplicity resources, LS solution is unbeatable terms storage computation time.
 Compared KLMS KLMS-AM, ASLM is much comparable performance, is better performance LS algorithm.
 bottleneck ASLM is search best candidate table look up, is similar KNN.
 general locations linear nonlinear model respect stor- age computational time simulation are plotted 100102104Storage10-610-410-2100Computation timeLSKNNASLMKLMSKLMS-AM ellipsoid cloud points.
 is obvious ASLM is linear model nonlinear regression capacity, loca- tion Fig.
 deviates diagonal linking linear nonlinear models, shows efﬁciency.
 simulation, augmented space model shows sur- prising potential improve different models, make full use training error desired augmented space increase performance.
 Meanwhile, computational complexity searching table is smaller KLMS, performance improvement won’t bring big computational burden explains Fig.

 Prediction Noise section, data are same last experiment desired signal training data are corrupted dB mean Gaussian noise.
 Performance is measured power error.
 Results are averaged in- dependent training-test runs obtained sliding window generated data samples time.
 pur- pose experiment is compare performance training set is clean.
 is obvious performance ASLM suffer noise is added table.
 Therefore, use simple (sequential) Vector Quantization (VQ) method cut noise, build small size codebook small threshold  original in- put values table.
 Depending Euclidean distance, VQ is simple (linear complexity terms codebook size) [10,
 error center codebook is computed averaging training errors indexes are quantized same center.
 method is used KLMS constrain network size.
 ASLM, VQ decreases table size computational com- plexity, improves performance averaging local training errors.
 extra comparisons are added show improvements algorithms brought VQ, is QKLMS, KLMS-QAM QASLM.
 be fair, ﬁnal size codebooks are set hyper parame- ters are validated get best possible results (kernel size σ, step size η, quantization radium  regularization factor δ).
 Table
 performance comparison linear nonlinear algorithm Training MSE KNN LS Testing MSE Algorithm KLMS Parameter ± σ = η = ± σ = η =  = σ = η = QKLMS KLSM-AM ± KLSM-QAM ± σ = η = K=1,= ± ± K=1,= ± ± ASLM QASLM K = δ = K = K = show testing MSE, training MSE corre- sponding parameters table
 is easy notice linear model shows powerful robustness comparison, performance algorithms LS are affected compared last result.
 ASLM beat KNN algorithms get better results LS algorithm.
 quantization method, KLMS is best predictor experiment, is robust KLMS-AM ASLM, sum weighted Gaussian kernel remove noise extent.
 VQ reduces storage KLMS, KLMS-AM ASLM algorithms, im- proves performance KLMS-AM ASLM cutting noise table.
 However, noise removal VQ is obvious KLMS algorithm.
 Hence, KLMS-QAM shows best performance help VQ.
 QKLMS shows similar result KLMS, is better QASLM.

 CONCLUSION DISCUSSION presented new solution functional approximation problem, taking advantage linear solution, cor- recting estimate training error input sample is neighborhood current test phase input.
 essence, combine computational efﬁciency linear solution memory block encoding training errors originating nonlinearity data generation process, produce nonlinear response.
 conventional nonlinear function needs ﬁnd appro- priate parameters nonlinear mappers, is full dif- ﬁculties expensive.
 is rea- son ASLM displays interesting compromise space accuracy computation complexity con- ventional linear nonlinear solutions.
 However, noticing ASLM models linear error case, have shown same approach improve modeling nonlinear error KLMS-AM.
 ASLM is member CULM family has been investigated past.
 Conceptually, are proposing augment input projection space desired response, opens door study many different implementa- tions simple ASLM discussed paper.
 thrust research focus ways improve table look performance is rudimentary.
 noisy situations, think PCA obtain better deﬁnition input space, ﬁlter training error local modeling.
 fact, is interesting interpret training errors sensi- tivity unknown desired response be exploited Bayesian modeling.
 have implicit model input, speed search ﬁnd closest neighbor current input.
 simple modiﬁcations improve ASLM performance lead new applications be- yond functional approximation.
 therefore believe be vibrant line research years come.

 REFERENCES [1] Simon S Haykin, Adaptive ﬁlter theory, Pearson Edu- cation India,
 [2] Richard O Duda, Peter E Hart, David G Stork, Pat- tern classiﬁcation, John Wiley Sons,
 [3] Jose C Principe Badong Chen, “Universal approx- imation convex optimization: Gimmick real- ity?[discussion forum],” IEEE Computational Intelli- gence Magazine, vol.
 no.
 pp.

 [4] Weifeng Liu, Jose C Principe, Simon Haykin, Ker- nel adaptive ﬁltering: comprehensive introduction, vol.
 John Wiley Sons,
 [5] Weifeng Liu, Puskal P Pokharel, Jose C Principe, “The kernel least-mean-square algorithm,” IEEE Trans- actions Signal Processing, vol.
 no.
 pp.

 [6] Guang-Bin Huang, Qin-Yu Zhu, Chee-Kheong Siew, “Extreme learning machine: theory appli- cations,” Neurocomputing, vol.
 no.
 pp.

 [7] Hanan Samet, design analysis spatial data structures, vol.
 Addison-Wesley Reading, MA,
 [8] T.
 Cover P.
 Hart, “Nearest neighbor pattern classiﬁ- cation,” IEEE Transactions Information Theory, vol.
 no.
 pp.

 Edward N Lorenz, “Deterministic nonperiodic ﬂow,” Journal atmospheric sciences, vol.
 no.
 pp.

 [10] Badong Chen, Songlin Zhao, Pingping Zhu, Jos´e C “Quantized kernel least mean square algo- IEEE Transactions Neural Networks Pr´ıncipe, rithm,” Learning Systems, vol.
 no.
 pp.

 survival analysis, goal is estimate occurrence time risk unfavorable event future (e.g, death patient) inform decisions present time (e.g., help select treatment).
 classical models task are Aalen’s additive model [1] Cox’s proportional hazard model [2], regress attributes patient hazard function.
 suitable comparing populations patients, models were designed patient-speciﬁc prediction.
 reformulating survival analysis multi-task classiﬁcation problem, Yu et al.
 [3] show set ordered linear classiﬁers provides accurate predictions.
 Here, follow same classiﬁcation approach show using deep learning methods improves predictive performance survival data.
 straightforward use neural networks leads black-box predictors lack transparency offered linear models.
 overcome issue, employ contextual explanation networks [CEN, class models learn predict generating leveraging intermediate explanations.
 Explanations are deﬁned instance-speciﬁc simple (linear) models help interpret predictions are selected network make predictions patient time interval.
 CENs be based arbitrary deep architectures process variety input data modalities interpreting predictions terms selected attributes.
 demonstrate experiments, approach attains best performance interpretability.
 Background First, present setup used Yu al.
 [3].
 data is represented patient-speciﬁc attributes, X, times occurance event, T.
 times are converted m-dimensional binary vectors, Y := (y1,


 ym), indicate corresponding follow time.
 death occurred time t ∈ [ti, ti+1), yj ∀j ≤ i yk = ∀k > i.
 data point was censored (i.e., Machine Learning Healthcare (ML4H) Workshop, NIPS Long Beach, CA, USA.
 t ∈ [t2, t3) θ3 θ2 y2 x2 h2 y3 x3 h3 θ1 y1 x1 h1 t ∈ [t2, t3) θ3 θ2 y2 x2 h2 y3 x3 h3 θ1 y1 x1 h1 c1 c2 c3 h1 h2 h3 (a) Architecture used SUPPORT2 (b) Architecture used PhysioNet Figure CEN architectures used survival analysis experiments.
 Context encoders were time-distributed single hidden layer MLP (a) LSTM (b) produced inputs LSTM output time intervals (denoted h1, h2, h3 hidden states respectively).
 hidden state output LSTM was used generate corresponding θt were used construct log-likelihood CRF.
 lack information times t ∈ [ti, ti+1)), targets (yi+1,


 are regarded latent variables.
 Note m sequences are valid, i.e., assigned non-zero probability model, allows write following linear model: p(Y = (y1,


 | x, Θ) = exp(cid:0)(cid:80)m t=1 ytx(cid:62)θt(cid:1) t=k+1 x(cid:62)θt(cid:1) model is trained optimizing regularized log likelihood w.r.t. Θ := {θt}m t=1.
 training, get set linear time interval, used predicting survival probability.
 Contextual Explanation Networks Survival Analysis Here, take same structured prediction approach consider different setup.
 particular, assume data instance (patient record) is represented context, C, attributes, X, targets, Y.
 goal is learn model, pw(Y | X, C), parametrized w predict Y X C.
 Note inputs have representations, X C, X is set attributes be used interpret predictions1.
 Contextual explanation networks are deﬁned models assume following form: Y ∼ p(Y | X, θ), θ ∼ pw(θ | C), pw(Y | X, C) = (2) p(Y | X, θ) is predictor parametrized θ.
 Such predictors are called explanations, relate interpretable variables, X, targets, Y.
 conditional distribution pw(θ | C) is called context encoder processes context representation, C, generates parameters explanation, θ.
 survival analysis, want explanations be form linear CRFs given (1).
 Hence, contextual networks CRF-based explanations are deﬁned follows: p(Y | X, θ)pw(θ | C)dθ (cid:90) θt ∼ pw(θt | C), t ∈ {1,


 Y ∼ p(Y | X, θ1:m), p(Y = (y1, y2,


 | x, θ1:m) ∝ exp pw(θt | C) := δ(θt, φt w,D(c)), φt t=1 (cid:40) m(cid:88) yi(x(cid:62)θt) + ω(yt, yt+1) (3) w,D(c) := α(ht)(cid:62)D, ht := RNN(ht−1, c) (cid:41) A few things note here.
 model generates explanations patient time interval.
 Second, depending nature context representation, C, CENs process is common have data be multiple representations are low-level unstructured (e.g., image pixels, sensory inputs), other are high-level human-interpretable (e.g., categorical variables).
 ensure interpretability, like use deep networks process low-level representation (the context) construct explanations context-speciﬁc probabilistic models high-level features.
 generate θt time step using recurrent encoder (Figure
 use deterministic RNN-based encoder, φt, selects parameters explanations global dictionary, D, using soft attention (for details dictionary-based context encoding, see [4]).
 potentials attributes, x, targets, y1:m, are linear functions parameterized θ1:m; pairwise potentials targets, ω(yi, yi+1), ensure conﬁgurations (yi yi+1 = are improbable (i.e., ω(1, = −∞ = ω00, ω(0, = ω01, ω(1, = ω10 are learnable parameters).
 Given constraints, likelihood uncensored event time t ∈ [tj, tj+1) is (cid:41) x(cid:62)θi (cid:40) m(cid:88) (cid:40) m(cid:88) i=k+1 exp (cid:41) x(cid:62)θi  m(cid:88) (cid:40) m(cid:88) i=j  k=0 exp (cid:44) m(cid:88) (cid:41)(cid:44) m(cid:88) (cid:88) k=0 j∈C m(cid:88) (cid:88) i∈NC p(T = t | x, Θ) = exp x(cid:62)θi likelihood event censored time t ∈ [tj, tj+1) is p(T ≥ t | x, Θ) = exp x(cid:62)θi k=j+1 i=k+1 i=k+1 joint log-likelihood data consists parts: p(T = ti | xi, Θ) + L(Y, X; Θ) = p(T > tj | xj, Θ) (4) (5) (6) NC is set non-censored instances know outcome times, ti) C is set censored instances (for know censorship times, tj).
 objective is optimized using stochastic gradient descent.
 See [4] more details.
 Experiments experiments, consider datasets, models, metrics described below.
 compare CENs number baselines visualize learned explanations.
 Datasets.
 use available datasets survival analysis intense care unit (ICU) patients: (a) SUPPORT22, (b) data PhysioNet challenge3.
 data was preprocessed used follows: • SUPPORT2: data had patient records variables.
 selected variables C X features.
 Categorical features (such race sex) were one-hot encoded.
 values features were non-negative, ﬁlled missing values -1.
 CRF-based predictors, survival timeline was capped years converted discrete intervals days each.
 used patient records training, validation, testing.
 • PhysioNet: data had patient records, represented 48-hour irregularly sampled 37-dimensional time-series different measurements taken patient’s stay ICU.
 resampled mean-aggregated time-series min frequency.
 resulted large number missing values ﬁlled
 resampled time-series were used context, C, attributes, X, took values last available measurement variable series.
 CRF-based predictors, survival timeline was capped days converted discrete intervals.
 Models.
 baselines, use classical Aalen Cox models CRF [3], used X inputs.
 Next, combine CRFs neural encoders ways: apply CRFs outputs neural encoders (denoted MLP-CRF LSTM-CRF, trainable end-to-end).
 Similar models have been show successful natural language applications [5].
 Note parameters CRF layer assign weights latent features are interpretable terms attributes interest.
 (ii) use CENs CRF-based explanations, process context variables, C, using same neural networks (i) output parameters CRFs act attributes, X.


 Table Performance classical Cox Aalen models, CRF-based models, CENs use LSTM MLP context embedding CRF explanations.
 numbers are averages 5-fold cross-validation; std.
 are order least signiﬁcant digit.
 @K denotes temporal quantile, time point such K% patients data have died were censored point.
 SUPPORT2 PhysioNet Challenge Model Acc@25 Acc@50 Acc@75 RAE Model Acc@25 Acc@50 Acc@75 RAE Cox Aalen CRF MLP-CRF MLP-CEN Cox Aalen CRF LSTM-CRF LSTM-CEN Figure Weights CEN-generated CRF explanations patients SUPPORT2 dataset set inﬂuential features: dementia (comorbidity), avtisst (avg.
 TISS, days 3-25), slos (days study entry discharge), hday (day hospital study admit), yes (the patient had cancer), sfdm2 Coma Intub (intubated coma month sfdm2 SIP (sickness impact proﬁle score month
 Higher weight values correspond higher feature contributions risk death given time point.
 Metrics.
 Following Yu al.
 [3], use metrics speciﬁc survival analysis: (a) accuracy predicting survival patient times correspond population-level temporal quantiles (i.e., time points such corresponding percentage patients data had time last follow due censorship death) (b) relative absolute error (RAE) predicted actual time death non-censored patients.
 Quantitative results.
 results models are given Table
 implementation CRF baseline reproduces (and improves) performance reported Yu al.
 [3].
 CRFs built representations learned deep networks (MLP-CRF LSTM-CRF models) improve plain CRFs but, noted, be interpreted terms original variables.
 other hand, CENs outperform neural CRF models certain metrics (and match others) providing explanations survival Figure CEN-predicted survival curves random patients SUP- PORT2 test set.
 Color indicates death year leaving hospital.
 probability predictions patient point time.
 Qualitative results.
 inspect predictions CENs qualitatively, given visualize weights assigned corresponding explanation respective attributes time interval.
 Figure shows explanation weights subset inﬂuential features patients SUPPORT2 dataset were predicted survivor non-survivor.
 explanations allow better understand patient-speciﬁc temporal dynamics contributing factors survival rates predicted model (Figure
 information be used model diagnostics (i.e., help understand trust particular prediction) more ﬁne-grained information useful decision support.
 leaving hospital (weeks)sfdm2_SIP>=30sfdm2_Coma Intubca_yeshdayslosavtisstdementiaPatient ID: (Died)01020304050Time leaving hospital (weeks)Patient ID: (Survived)4202402550Time leaving hospital (weeks)0.00.20.40.60.81.0Survival probabilitySurvivedDied References [1] O.O. Aalen.
 “A linear regression model analysis life time”.
 In: Statistics Medicine, (1989).
 [2] DR Cox.
 “Regression Models Life-Tables”.
 In: Journal Royal Statistical Society.
 Series B (Methodological) (1972), pp.

 [3] Chun-Nam Yu, Russell Greiner, Hsiu-Chin Lin Vickie Baracos.
 “Learning patient-speciﬁc cancer survival distributions sequence dependent regressors”.
 In: Advances Neural Information Processing Systems.
 pp.

 [4] Maruan Al-Shedivat, Avinava Dubey Eric P Xing.
 “Contextual Explanation Networks”.
 In: preprint arXiv:1705.10301 (2017).
 [5] Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu Pavel Kuksa.
 “Natural language processing (almost) scratch”.
 In: Journal Machine Learning Research (2011).
 Graphs are universal representations pairwise relationship.
 Many real world data come form graphs; e.g., social networks, gene expression networks, knowledge graphs.
 improve performance graph-based learning tasks, such node classiﬁcation link prediction, much effort is made extend well-established network architectures, including recurrent neural networks (RNN) convolutional neural networks (CNN), graph data; see, e.g., Bruna et al.
 (2013); Duvenaud et al.
 (2015); Li et al.
 (2015); Jain al.
 (2015); Henaff al.
 (2015); Niepert al.
 (2016); Kipf Welling (2016a;b).
 learning feature representations graphs is important subject effort, here, focus feature representations graph vertices.
 vein, closest work applies convolution architecture is graph convolutional network (GCN) (Kipf Welling,
 Borrowing concept convolution ﬁlter image pixels linear array signals, GCN uses connectivity structure graph ﬁlter perform neighborhood mixing.
 architecture be summarized following expression: H (l+1) = σ( ˆAH (l)W (l)), ˆA is normalization graph adjacency matrix, H (l) contains embedding (row- wise) graph vertices lth layer, W (l) is parameter matrix, σ is nonlinearity.
 many graph algorithms, adjacency matrix encodes pairwise relationship training test data.
 learning model embedding is performed data simultaneously, least authors proposed.
 many applications, however, test data be graph be expanding new vertices (e.g. new members social network, new products recommender system, new drugs functionality tests).
 Such scenarios require inductive scheme learns model training set vertices generalizes augmentation graph.
 ∗These authors contribute equally.
 Published conference paper ICLR A severe challenge GCN is recursive expansion neighborhoods layers incurs expensive computations batched training.
 dense graphs powerlaw graphs, expansion neighborhood single vertex ﬁlls large portion graph.
 Then, usual mini-batch training involve large amount data batch, small batch size.
 Hence, scalability is pressing issue resolve GCN be applicable large, dense graphs.
 address challenges, propose view graph convolutions different angle interpret integral transforms embedding functions probability measures.
 Such view provides principled mechanism inductive learning, starting formulation loss stochastic version gradient.
 Speciﬁcally, interpret graph vertices are iid samples probability distribution write loss convolution layer integrals respect vertex embedding functions.
 integrals are evaluated Monte Carlo approximation deﬁnes sample loss sample gradient.
 alter sampling distribution (as importance sampling) reduce approximation variance.
 proposed approach, coined FastGCN, rids reliance test data yields controllable cost per-batch computation.
 time writing, notice published work GraphSAGE (Hamilton et proposes use sampling reduce computational footprint GCN.
 sampling scheme is economic, resulting substantial saving gradient computation, be analyzed more detail Section Experimental results Section indicate per-batch computation FastGCN is more order magnitude GraphSAGE, classiﬁcation accuracies are comparable.
 RELATED WORK past few years, several graph-based convolution network models emerged address- ing applications graph-structured data, such representation molecules (Duvenaud et al.,
 important stream work is built spectral graph theory (Bruna Henaff Defferrard et
 deﬁne parameterized ﬁlters spectral domain, in- spired graph Fourier transform.
 approaches learn feature representation whole graph be used graph classiﬁcation.
 line work learns embeddings graph vertices, Goyal Ferrara (2017) is recent survey covers several categories methods.
 major category consists factorization based yield embedding matrix factorizations; see, e.g., Roweis Saul (2000); Belkin Niyogi (2001); Ahmed al.
 (2013); Cao et al.
 (2015); Ou et al.
 (2016).
 methods learn representations training test data jointly.
 category is random walk based methods (Perozzi et Grover Leskovec, compute node representations exploration neighborhoods.
 LINE (Tang et al., is such tech- nique is motivated preservation ﬁrst second-order proximities.
 Meanwhile, appear few deep neural network architectures, capture nonlinearity graphs, such SDNE (Wang et
 motivated earlier, GCN (Kipf Welling, is model work is based.
 relevant work approach is GraphSAGE (Hamilton et learns node representations aggregation neighborhood information.
 proposed aggregators employs GCN architecture.
 authors acknowledge memory bottleneck GCN propose ad hoc sampling scheme restrict neighborhood size.
 sampling approach is based different principled formulation.
 major distinction is sample vertices neighbors.
 resulting computational savings are analyzed Section TRAINING INFERENCE THROUGH SAMPLING One striking difference GCN many standard neural network architectures is lack independence sample loss.
 Training algorithms such SGD batch generalization are designed based additive nature loss function respect independent data samples.
 graphs, other hand, vertex is convolved neighbors deﬁning sample gradient is efﬁcient compute is straightforward.
 Published conference paper ICLR Concretely, consider standard SGD scenario loss is expectation function g respect data distribution D: L = Ex∼D[g(W x)].
 Here, W denotes model parameter be optimized.
 course, data distribution is unknown minimizes empirical loss accessing n iid samples x1,


 n(cid:88) i=1 Lemp = g(W xi ∼ D, ∀ i.
 step SGD, gradient is approximated ∇g(W (assumed) unbiased sample ∇L.
 interpret gradient step makes sample loss g(W
 sample loss sample gradient involve single sample xi.
 graphs, longer leverage independence compute sample gradient ∇g(W xi) discarding information i’s neighboring vertices neighbors, recur- sively.
 therefore seek alternative formulation.
 order cast learning problem same sampling framework, let assume is inﬁnite) graph G(cid:48) vertex set V (cid:48) associated probability space (V (cid:48), F, P ), such given graph G, is induced subgraph G(cid:48) vertices are iid samples V (cid:48) according probability measure P
 probability space, V (cid:48) serves sample space F be event space (e.g., power set F (cid:48) resolve problem lack independence caused convolution, interpret layer network deﬁnes embedding function vertices (random are tied same probability measure are independent.
 See Figure
 Speciﬁcally, recall architecture GCN ).
 probability measure P deﬁnes sampling distribution.
 ˜H (l+1) = ˆAH (l)W (l), H (l+1) = σ( ˜H (l+1)), l =


 M − L = (cid:90) functional generalization, write ˜h(l+1)(v) = ˆA(v, u)h(l)(u)W (l) dP (u), h(l+1)(v) = σ(˜h(l+1)(v)), g(H (M )(i, :)).
 i=1 (1) l =


 M − (2) (3) L = Ev∼P [g(h(M )(v))] = g(h(M )(v)) dP (v).
 Here, u v are independent random have same probability measure P
 function h(l) is interpreted embedding function lth layer.
 embedding functions consecutive layers are related convolution, expressed integral transform, kernel ˆA(v, u) corresponds (v, u) element matrix ˆA.
 loss is expectation g(h(M )) ﬁnal embedding h(M ).
 Note integrals are usual Riemann–Stieltjes integrals, variables u v are graph vertices real however, distinction is matter formalism.
 Writing GCN functional form allows evaluating integrals Monte Carlo manner, leads batched training algorithm natural separation training test data, ∼ P inductive learning.
 layer l, use tl iid samples u(l) evaluate integral transform (2); is,


 u(l) tl ˆA(v, u(l) j )h(l) tl (u(l) j )W (l), h(l+1) tl+1 (v) := σ(˜h(l+1) tl+1 (v)), l =


 M − tl(cid:88) j=1 ˜h(l+1) tl+1 (v) := tl convention h(0) t0 ≡ h(0).
 Then, loss L (3) admits estimator (cid:90) tM(cid:88) i=1 Lt0,t1,...,tM := tM g(h(M tM (u(M )).
 follow result establishes estimator is consistent.
 proof is recursive application law large numbers continuous mapping theorem; is given appendix.
 Published conference paper ICLR Figure views GCN.
 left (graph convolution view), circle represents graph vertex.
 consecutive rows, circle i is connected (in gray line) circle j cor- responding vertices graph are connected.
 convolution layer uses graph connectivity structure mix vertex features/embeddings.
 right (integral transform embed- ding function next layer is integral transform (illustrated orange fanout shape) one previous layer.
 proposed method, integrals (including loss function) are evaluated using Monte Carlo sampling.
 graph view, vertices are subsam- pled bootstrapping manner layer approximate convolution.
 sampled portions are denoted solid blue circles orange lines.

 g σ are continuous, Lt0,t1,...,tM = L probability one.
 lim practical use, are given graph vertices are assumed be samples.
 Hence, need bootstrapping obtain consistent estimate.
 particular, network architecture (1), output H (M is split batches usual.
 use u(M tM denote batch vertices, come given graph.
 batch, sample (with replacement) i =


 l =


 M −
 Such procedure is equivalent layer obtain samples u(l) uniformly sampling rows H (l) l.
 obtain batch loss


 u(M tM(cid:88) i=1 Lbatch = tM g(H (M )(u(M :)), where, recursively, H (l+1)(v, :) = σ  tl tl(cid:88) j=1 ˆA(v, u(l) j )H (l)(u(l) j :)W (l) (4) (5) l =


 M −
  Here, n activation function σ is number vertices given graph is used account normalization difference matrix form (1) integral form (2).
 corresponding batch gradient be obtained applying chain rule H (l).
 See Algorithm
 VARIANCE REDUCTION tion(cid:82) ˆA(v, u)h(l) estimator, is interested improving variance.
 computing full variance is challenging nonlinearity layers, is possible consider single layer aim improving variance embedding function nonlinearity.
 Speciﬁcally, consider lth layer, function ˜h(l+1) (v) approximation convolu- tl+1 (u)W (l) dP (u).
 taking tl+1 samples v = u(l+1) tl+1 sample tl average ˜h(l+1) (v) admits variance captures deviation eventual loss contributed tl+1 layer.
 Hence, seek improvement variance.
 consider layer separately, do following change notation keep expressions


 u(l+1) Graph convolution viewIntegral transform viewbatchH(2)H(1)H(0)h(2)(v)h(1)(v)h(0)(v) Published conference paper ICLR Algorithm FastGCN batched training (one epoch) batch do layer l, sample tl vertices u(l) layer l do v is sampled next layer, ∇ ˜H (l+1)(v, :) ← tl end W ← W − η∇Lbatch end


 u(l) tl (cid:46) Compute batch gradient ∇Lbatch j )∇(cid:110) j :)W (l)(cid:111) ˆA(v, u(l) H (l)(u(l) tl(cid:88) j=1 (cid:46) SGD step Num.
 samples tl+1 → s tl → t Layer l + random variable v Samples i → vi u(l+1) j → uj u(l) joint distribution v u, aforementioned sample average is Function (v) → y(v) (u)W (l) → x(u) Layer l; random variable u ˜h(l+1) tl+1 h(l) tl s(cid:88) i=1 s(cid:88) t(cid:88) i=1 j=1 G := y(vi) = ˆA(vi, uj)x(uj) 
 have following result.
 Proposition
 variance G admits (cid:18) R = − (cid:19)(cid:90) Var{G} = R + e(v)2 dP (v) − st (cid:90)(cid:90) (cid:18)(cid:90) ˆA(v, u)2x(u)2 dP (u) dP (v), (6) (cid:19)2 (cid:90) e(v) = ˆA(v, u)x(u) dP (u).
 e(v) dP (v) variance (6) consists parts.
 ﬁrst part R leaves little room improvement, sampling v space is done layer.
 second part (the double integral), other hand, depends uj’s layer are sampled.
 current result (6) is consequence sampling uj’s using probability measure P
 perform importance sampling, altering sampling distribution reduce variance.
 Speciﬁcally, let Q(u) be new probability measure, uj’s are drawn from.
 hence deﬁne new sample average approximation yQ(v) := ˆA(v, uj)x(uj) dP (u) dQ(u) u1,


 ut ∼ Q, quantity interest j=1 yQ(vi) = s(cid:88) i=1 s(cid:88) i=1 GQ := (cid:32) (cid:33)
 (cid:12)(cid:12)(cid:12)(cid:12)uj dP (u) dQ(u) ˆA(vi, uj)x(uj) (cid:33) (cid:12)(cid:12)(cid:12)(cid:12)uj (cid:32)  t(cid:88) j=1 t(cid:88) Clearly, expectation GQ is same G, regardless new measure Q.
 following result gives optimal Q.

 (cid:20)(cid:90) (cid:21) ˆA(v, u)2 dP (v) (7) b(u)|x(u)| dP (u) (cid:82) b(u)|x(u)| dP (u) dQ(u) = b(u) Published conference paper ICLR variance GQ admits Var{GQ} = R + st (cid:20)(cid:90) (cid:21)2 b(u)|x(u)| dP (u) (8) R is deﬁned Proposition
 variance is minimum choices Q.
 drawback deﬁning sampling distribution Q manner is involves |x(u)|, changes training.
 corresponds product embedding matrix H (l) parameter matrix W (l).
 parameter matrix is updated iteration; matrix product is expensive compute.
 Hence, cost computing optimal measure Q is high.
 compromise, consider different choice Q, involves b(u).
 following proposition gives precise deﬁnition.
 resulting variance be smaller (6).
 practice, however, ﬁnd is helpful.
 Proposition
 dQ(u) = b(u)2 dP (u) (cid:82) b(u)2 dP (u) (cid:90) b(u)2 dP (u) (cid:90) b(u) is deﬁned (7), variance GQ admits Var{GQ} = R + st R is deﬁned Proposition
 x(u)2 dP (u), (9) choice probability measure Q, ratio dQ(u)/dP (u) is proportional b(u)2, is integral ˆA(v, u)2 respect v.
 practical use, network architec- ture deﬁne probability mass function vertices given graph: q(u) = (cid:107) ˆA(:, u)(cid:107)2/ (cid:107) ˆA(:, u(cid:48))(cid:107)2, u ∈ V (cid:88) u(cid:48)∈V  tl tl(cid:88) sample t vertices u1,


 ut according distribution.
 expression q, see has dependency l; is, sampling distribution is same layers.
 summarize, batch loss Lbatch (4) is expanded H (l+1)(v, :) = σ ˆA(v, u(l) j )H (l)(u(l) j :)W (l) j=1 q(u(l) j j ∼ q, u(l) l =


 M −
 major difference (5) (10) is former obtains samples latter according q.
 scaling summation changes.
 corresponding batch gradient be obtained applying chain rule H (l).
 See Algorithm
  Algorithm FastGCN batched training (one epoch), improved version vertex u, compute sampling probability q(u) ∝ (cid:107) ˆA(:, u)(cid:107)2 batch do layer l, sample tl vertices u(l) layer l do v is sampled next layer,


 u(l) ∇ ˜H (l+1)(v, :) ← tl end W ← W − η∇Lbatch end tl according distribution q (cid:46) Compute batch gradient ∇Lbatch ∇(cid:110) j :)W (l)(cid:111) H (l)(u(l) (cid:46) SGD step j=1 ˆA(v, u(l) j j Published conference paper ICLR INFERENCE sampling approach described preceding subsection separates test data training.
 Such approach is inductive, opposed transductive is common many graph algorithms.
 essence is cast set graph vertices iid samples probability distribution, learning algorithm use gradient consistent estimator loss perform parameter update.
 Then, inference, embedding new vertex be computed using full GCN architecture (1), approximated sampling is done parameter learning.
 Generally, using full architecture is straightforward easier implement.
 COMPARISON WITH GRAPHSAGE GraphSAGE (Hamilton et al., is proposed architecture generating vertex embed- dings aggregating neighborhood information.
 shares same memory bottleneck GCN, caused recursive neighborhood expansion.
 reduce computational footprint, au- thors propose restricting immediate neighborhood size layer.
 Using notation sample size, samples tl neighbors vertex lth layer, size expanded neighborhood is, worst case, product tl’s.
 other hand, FastGCN samples vertices neighbors layer.
 total number involved vertices is most sum tl’s, product.
 See experimental results Section order-of-magnitude saving actual computation time.
 EXPERIMENTS follow experiment setup Kipf Welling (2016a) Hamilton et al.
 (2017) demon- strate effective use FastGCN, comparing original GCN model Graph- SAGE, following benchmark tasks: (1) classifying research topics using Cora citation data set (McCallum (2) categorizing academic papers Pubmed database; (3) predicting community structure social network modeled Reddit posts.
 data sets are downloaded accompany websites aforementioned references.
 graphs have more nodes higher node representative large dense set- ting method is motivated.
 Statistics are summarized Table
 adjusted training/validation/test split Cora Pubmed align supervised learning scenario.
 Speciﬁcally, labels training examples are used training, opposed small portion semi-supervised setting (Kipf Welling,
 Such split is coherent other data set, Reddit, used work GraphSAGE.
 Additional experiments using original split Cora Pubmed are reported appendix.
 Table Dataset Statistics Dataset Cora Pubmed Reddit Nodes Edges Classes Features Training/Validation/Test Implementation details are following.
 networks (including comparison) contain layers usual.
 codes GraphSAGE GCN are downloaded accompany websites latter is adapted FastGCN.
 Inference FastGCN is done full GCN network, mentioned Section Further details are contained appendix.
 ﬁrst consider use sampling FastGCN.
 left part Table (columns “Sam- pling”) lists time classiﬁcation accuracy number samples increases.
 illustration purpose, equalize sample size layers.
 Clearly, samples, per-epoch training time increases, accuracy (as measured using micro F1 scores) improves generally.
 interesting observation is given input features H (0), product ˆAH (0) bottom layer does change, means chained expansion gradient respect W (0) Published conference paper ICLR Table Beneﬁt precomputing ˆAH (0) input layer.
 Data set: Pubmed.
 Train- ing time is seconds, per-epoch (batch size
 Accuracy is measured using micro F1 score.
 Sampling F1 t1 Time Precompute Time F1 Figure Prediction accuracy: uniform versus impor- tance sampling.
 data sets top bottom are ordered same Table
 last step is constant training.
 Hence, precompute product sampling layer gain efﬁciency.
 compared results are listed right part Table (columns “Precompute”).
 sees training time decreases accuracy is comparable.
 Hence, experiments follow use precomputation.
 Next, compare sampling approaches FastGCN: uniform importance sampling.
 Fig- ure summarizes prediction accuracy approaches.
 shows importance sampling yields higher accuracy does uniform sampling.
 altered sampling distri- bution (see Proposition Algorithm is compromise alternative optimal distribution is impractical use, result suggests variance used sampling is smaller uniform sampling; i.e., term (9) stays closer (8) does (6).
 possible reason is b(u) correlates |x(u)|.
 Hence, later experiments apply importance sampling.
 demonstrate proposed method is original GCN GraphSAGE, maintaining comparable prediction performance.
 See Figure
 bar heights indicate per-batch training time, log scale.
 sees GraphSAGE is sub- stantial improvement GCN large dense graphs (e.g., Reddit), smaller ones Pubmed), GCN trains faster.
 FastGCN is fastest, least order magnitude improvement compared runner (except Cora), orders mag- nitude speed compared slowest.
 Here, training time FastGCN is respect sample size achieves best prediction accuracy.
 seen table right, accuracy is comparable best other methods.
 Micro F1 Score FastGCN Cora GraphSAGE-GCN GraphSAGE-mean GCN (batched) GCN (original) Pubmed Reddit NA Figure Per-batch training time seconds (left) prediction accuracy (right).
 timing, GraphSAGE refers GraphSAGE-GCN Hamilton al.
 (2017).
 timings using other ag- gregators, such GraphSAGE-mean, are similar.
 GCN refers using batched learning, opposed original version is nonbatched; more details implementation, see appendix.
 nonbatched version GCN runs memory large graph Reddit.
 sample sizes FastGCN are data sets.
 ScoreUniformImportance1025500.80.850.9F1 ScoreUniformImportance size0.90.920.94F1 ScoreUniformImportanceCoraPubmedReddit10-310-210-1100Time (seconds)FastGCNGraphSAGEGCN Published conference paper ICLR discussion period, authors GraphSAGE offered improved implementation codes alerted GraphSAGE was suited massive graphs.
 reason is small graphs, sample size (recalling is product layers) is comparable graph size hence improvement is marginal; moreover, sampling overhead affect timing.
 fair comparison, authors GraphSAGE kept sampling strategy improved implementation original codes eliminating redundant calculations sampled nodes.
 per-batch training time GraphSAGE compares more smallest graph Cora; see Table
 Note implementation does affect large graphs (e.g., Reddit) observation orders magnitude training remains valid.
 Table Further comparison per-batch training time (in seconds) new implementation GraphSAGE small graphs.
 new implementation is PyTorch rest are Ten- sorFlow.
 FastGCN GraphSAGE-GCN (old GraphSAGE-GCN (new impl) GCN (batched) Cora Pubmed Reddit NA CONCLUSIONS have presented FastGCN, fast improvement GCN model proposed Kipf Welling (2016a) learning graph embeddings.
 generalizes transductive training inductive manner addresses memory bottleneck issue GCN caused recursive expansion neighborhoods.
 crucial ingredient is sampling scheme reformulation loss gradient, alternative view graph convoluntions form integral transforms embedding functions.
 have compared proposed method GraphSAGE (Hamilton et published work proposes using sampling restrict neighborhood size, sampling schemes differ algorithm cost.
 Experimental results indicate approach is orders magnitude GCN GraphSAGE, maintaining comparable prediction performance two.
 simplicity GCN architecture allows natural interpretation graph convolutions terms integral transforms.
 Such view, yet, generalizes many graph models formulations are based ﬁrst-order neighborhoods, examples include MoNet applies (meshed) manifolds (Monti et many message-passing neural networks (see e.g., Scarselli et al.
 (2009); Gilmer et al.
 (2017)).
 proposed work elucidates basic Monte Carlo ingredients estimating integrals.
 generalizing other networks aforementioned, additional effort is investigate variance reduction improve estimator, rewarding avenue future research.
 REFERENCES Amr Ahmed, Nino Shervashidze, Shravan Narayanamurthy, Vanja Josifovski, Alexander J.
 Smola.
 Distributed large-scale natural graph factorization.
 Proceedings Interna- tional Conference World Wide Web, WWW ’13, pp.

 ISBN
 Mikhail Belkin Partha Niyogi.
 Laplacian eigenmaps spectral techniques embedding clustering.
 Proceedings International Conference Neural Information Processing Systems: Natural Synthetic, NIPS’01, pp.

 Joan Bruna, Wojciech Zaremba, Arthur Szlam, Yann LeCun.
 Spectral networks connected networks graphs.
 CoRR, abs/1312.6203,
 Shaosheng Cao, Wei Lu, Qiongkai Xu. Grarep: Learning graph representations global structural information.
 Proceedings ACM International Conference Informa- tion Knowledge Management, CIKM ’15, pp.

 ISBN
 Published conference paper ICLR Micha¨el Defferrard, Xavier Bresson, Pierre Vandergheynst.
 Convolutional neural networks graphs fast localized spectral ﬁltering.
 CoRR, abs/1606.09375,
 David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan Aspuru-Guzik, Ryan P Adams.
 Convolutional networks graphs learning molecular ﬁngerprints.
 C.
 Cortes, N.
 D.
 Lawrence, D.
 D.
 Lee, M.
 Sugiyama, R.
 Garnett (eds.), Advances Neural Information Processing Systems pp.

 Curran Associates, Inc.,
 J.
 Gilmer, S.S. Schoenholz, P.F. Riley, O.
 Vinyals, G.E. Dahl.
 Neural message passing quantum chemistry.
 ICML,
 Palash Goyal Emilio Ferrara.
 Graph embedding techniques, applications, performance: A survey.
 CoRR, abs/1705.02801,
 Aditya Grover Jure Leskovec.
 Node2vec: Scalable feature learning networks.
 Proceedings ACM SIGKDD International Conference Knowledge Discovery Data Mining, KDD ’16, pp.

 ISBN
 William L.
 Hamilton, Rex Ying, Jure Leskovec.
 graphs.
 CoRR, abs/1706.02216,
 Inductive representation learning large Mikael Henaff, Joan Bruna, Yann LeCun.
 Deep convolutional networks graph-structured data.
 CoRR, abs/1506.05163,
 Ashesh Jain, Amir Roshan Zamir, Silvio Savarese, Ashutosh Saxena.
 Structural-rnn: Deep learning spatio-temporal graphs.
 CoRR, abs/1511.05298,
 Thomas N.
 Kipf Max Welling.
 Semi-supervised classiﬁcation graph convolutional net- works.
 CoRR, abs/1609.02907,
 TN.
 Kipf M.
 Welling.
 Variational graph auto-encoders.
 NIPS Workshop Bayesian Deep Learning.

 Yujia Li, Daniel Tarlow, Marc Brockschmidt, Richard S.
 Zemel.
 Gated graph sequence neural networks.
 CoRR, abs/1511.05493,
 Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, Kristie Seymore.
 Automating construction internet portals machine learning.
 Inf.
 Retr., July
 ISSN 1386-4564.
 F.
 Monti, D.
 Boscaini, J.
 Masci, E.
 Rodala, J.
 Svoboda, M.M. Bronstein.
 Geometric deep learning graphs manifolds using mixture model CNNs. CVPR,
 Mathias Niepert, Mohamed Ahmed, Konstantin Kutzkov.
 Learning convolutional neural net- works graphs.
 CoRR, abs/1605.05273,
 Mingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, Wenwu Zhu.
 Asymmetric transitivity pre- serving graph embedding.
 Proceedings ACM SIGKDD International Conference Knowledge Discovery Data Mining, KDD ’16, pp.

 ISBN 4232-2.
 Bryan Perozzi, Rami Al-Rfou, Steven Skiena.
 Deepwalk: Online learning social repre- sentations.
 Proceedings 20th ACM SIGKDD International Conference Knowledge Discovery Data Mining, KDD ’14, pp.

 ISBN
 Sam T.
 Roweis Lawrence K.
 Saul.
 Nonlinear dimensionality reduction linear embed- ding.
 Science,
 ISSN 0036-8075.
 doi:

 F.
 Scarselli, M.
 Gori, A.C. Tsoi, M.
 Hagenbuchner, G.
 Monfardini.
 graph neural network model.
 IEEE Transactions Neural Networks,
 Published conference paper ICLR Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, Qiaozhu Mei.
 Line: Large-scale information network embedding.
 Proceedings International Conference World Wide Web, WWW ’15, pp.

 ISBN
 Daixin Wang, Peng Cui, Wenwu Zhu.
 Structural deep network embedding.
 Proceedings ACM SIGKDD International Conference Knowledge Discovery Data Mining, KDD ’16, pp.

 ISBN
 PROOFS Proof Theorem
 samples are iid, strong law large numbers, t0(cid:88) j=1 ˜h(1) t1 (v) = t0 ˆA(v, u(0) j )h(0)(u(0) j )W (0) j=1 t1 ˜h(2) t2 (v) = t1 (v) = σ(˜h(1) hence has nothing do variable u v statement.
 Similarly, converges ˜h(1)(v).
 Then, activation function σ is continuous, continuous mapping theorem implies h(1) t1 (v)) converges t1 (u)W (1) dP (u) converges ˜h(2)(v) = h(1)(v) = σ(˜h(1)(v)).
 Thus,(cid:82) ˆA(v, u)h(1) (cid:82) ˆA(v, u)h(1)(u)W (1) dP (u), note probability space is respect 0th layer t1(cid:88) converges to(cid:82) ˆA(v, u)h(1) (cid:90) (cid:18)(cid:90) t1 (u)W (1) dP (u) ˜h(2)(v).
 simple induction Proof Proposition
 Conditioned expectation y(v) is variance is times ˆA(v, u)x(u), i.e., completes rest proof.
 ˆA(v, u(1) j )h(1) t1 (u(1) j )W (1) ˆA(v, u)x(u) dP (u) = e(v), E[y(v)|v] = Var{y(v)|v} = (12) Instantiating (11) (12) iid samples v1,


 P taking variance expectation front, respectively, obtain ˆA(v, u)2x(u)2 dP (u) − e(v)2 (cid:19) (11) y(vi) = Var e(vi) e(v) dP (v) (cid:41) (cid:90) e(v)2 dP (cid:18)(cid:90) (cid:19)2 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) v1,


 v1,


 Var (cid:34) (cid:40) Var s(cid:88) i=1 s(cid:88) i=1 (cid:40) s(cid:88) i=1 Then, applying law total variance s(cid:88) i=1 (cid:40) s(cid:88) i=1 (cid:90)(cid:90) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) v1,


 Var = Var y(vi) conclude proof.
 st ˆA(v, u)2x(u)2 dP (u) dP (v) − st e(v)2 dP (v).
 (cid:90) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) v1,


 Var (cid:40) s(cid:88) i=1 Published conference paper ICLR Proof Theorem
 Conditioned variance yQ(v) is times ˆA(v, u)x(u) dP (u) dQ(u) (where u ∼ Q), (cid:32)(cid:90) ˆA(v, u)2x(u)2dP (u)2 i.e., Var{yQ(v)|v} = Then, following proof Proposition overall variance is dQ(u) (cid:90)(cid:90) ˆA(v, u)2x(u)2 dP (u)2 dP (v) dQ(u) = R + st Var{GQ} = R + st − (cid:33) (cid:90) b(u)2x(u)2dP (u)2 dQ(u) Hence, optimal dQ(u) be proportional b(u)|x(u)| dP (u).
 integrate unity, have case dQ(u) = Var{GQ} = R + b(u)|x(u)| dP (u) (cid:82) b(u)|x(u)| dP (u) (cid:20)(cid:90) b(u)|x(u)| dP (u) (cid:21)2 st Proof Proposition
 Conditioned variance yQ(v) is times i.e., ˆA(v, u)x(u) Var{yQ(v)|v} = dP (u) dQ(u) (cid:32)(cid:20)(cid:90) ˆA(v, u) sgn(x(u)) b(u) b(u)|x(u)| dP (u), (cid:90) (cid:21)2(cid:90) ˆA(v, u)2 b(u)|x(u)| dP (u) b(u)2 dQ(u) − e(v)2 (cid:33) rest proof follows Proposition
 B ADDITIONAL EXPERIMENT DETAILS B.1 BASELINES GCN: original GCN cannot work large graphs (e.g., Reddit).
 modiﬁed batched version removing sampling FastGCN (i.e., using nodes sampling few batch).
 small graphs (Cora Pubmed), compared results original GCN.
 GraphSAGE: training time comparison, use GraphSAGE-GCN employs GCN aggregator.
 is fastest version choices aggregators.
 accu- racy comparison, compared GraphSAGE-mean.
 used codes https: //github.com/williamleif/GraphSAGE.
 Following setting Hamilton al.
 (2017), use layers neighborhood sample sizes S1 = S2 =
 fair comparison method, batch size is set be same FastGCN, hidden dimension is
 B.2 EXPERIMENT SETUP Datasets: Cora Pubmed data sets are https://github.com/tkipf/gcn.
 explained paper, kept validation index test index unchanged changed training index use remaining nodes graph.
 Reddit data is http: //snap.stanford.edu/graphsage/.
 Experiment Setting: preformed hyperparameter selection learning rate model di- mension.
 swept learning rate set {0.01,
 hidden dimension Fast- GCN Reddit is set other data sets, is
 batch size is Published conference paper ICLR Cora Reddit, Pubmed.
 Dropout rate is set
 use Adam opti- mization method training.
 test phase, use trained parameters graph nodes sampling.
 more details please check codes temporary git repository https://github.com/matenure/FastGCN.
 Hardware: Running time is compared single machine 4-core GHz Intel Core i7, RAM.
 C ADDITIONAL EXPERIMENTS C.1 TRAINING TIME COMPARISON Figure main text compares per-batch training time different methods.
 Here, list total training time reference.
 is impacted convergence SGD, contributing factors include learning rate, batch size, sample size.
 See Table
 orders-of- magnitude speedup per-batch time is weakened convergence speed, sees substantial advantage proposed method overall training time.
 Note original GCN trains batched version, does scale memory limitation.
 Hence, fair comparison be gauged batched version.
 show Figure evolution prediction accuracy training progresses.
 Table Total training time seconds).
 FastGCN Cora GraphSAGE-GCN GCN (batched) GCN (original) Pubmed Reddit NA Figure Training/test accuracy versus training time.
 left right, data sets are Cora, Pubmed, Reddit, respectively.
 C.2 ORIGINAL DATA SPLIT FOR CORA AND PUBMED As explained Section increased number labels used training Cora Pubmed, align supervised learning setting Reddit.
 reference, present results using original data split fewer training labels.
 fork separate version FastGCN, called FastGCN-transductive, uses training test data learning.
 See Table
 10-2100102Training time (seconds)00.20.40.60.81Training accuracyFastGCNGraphSAGEGCN (batched)10-2100102104Training time (seconds)0.80.820.840.860.880.9Training (batched)100105Training time (seconds)0.70.750.80.850.90.95Training accuracyFastGCNGraphSAGEGCN (batched)10-2100102Training time (seconds)0.20.40.60.81Test (batched)10-2100102104Training time (seconds)0.820.840.860.880.9Test (batched)100105Training time (seconds)0.750.80.850.90.95Test accuracyFastGCNGraphSAGEGCN (batched) Published conference paper ICLR results GCN are consistent reported Kipf Welling (2016a).
 labeled data are training GCN is fast.
 FastGCN beats Pubmed.
 accuracy results FastGCN are inferior GCN, limited number training labels.
 transductive version FastGCN-transductive matches accuracy GCN.
 results GraphSAGE are curious.
 suspect model overﬁts data, perfect training accuracy is attained.
 note subtlety training GCN (original) is slower is reported Table fewer labels are used here.
 reason is adopt same hyperparameters Kipf Welling (2016a) reproduce F1 scores work, whereas Table better learning rate is found boosts performance new split data, case GCN (original) converges faster.
 Table Total training time test accuracy Cora Pubmed, original data split.
 Time is seconds.
 FastGCN FastGCN-transductive GraphSAGE-GCN GCN (original) Cora Pubmed Time F1 Time F1 D CONVERGENCE Strictly speaking, training algorithms proposed Section do follow existing theory SGD, gradient estimator, consistent, is biased.
 section, ﬁll gap deriving convergence result.
 Similar case standard SGD convergence rate depends properties objective function, analyze simple case; comprehensive treatment is scope present work.
 convenience, need separate system notations same notations appearing main text bear different meaning here.
 abbreviate “with probability one” “w.p.1” short.
 use f (x) denote objective function assume is differentiable.
 Differentiability is restriction nondifferentiable case, analysis follows needs change gradient subgradient.
 key assumption made f is is convex; is, exists positive real number l such f (x) − f (y) ≥ (cid:104)∇f (y), x − y(cid:105) + (cid:107)x − y(cid:107)2, (13) x y.
 use g denote gradient estimator.
 Speciﬁcally, denote g(x; ξN ), ξN being random variable, consistent estimator ∇f (x); is, N→∞ g(x; ξN = ∇f (x) w.p.1. lim Moreover, consider SGD update rule xk+1 = xk − γk g(xk; ξ(k) N ), (14) ξ(k) N is indepedent sample ξN kth update.
 following result states update converges order O(1/k).

 Let be (global) minimum f assume (cid:107)∇f (x)(cid:107) is bounded constant G >
 γk = (lk)−1, exists sequence Bk Bk ≤ max{(cid:107)x1 − x∗(cid:107)2, G2/l2} such (cid:107)xk − x∗(cid:107)2 → Bk w.p.1. Published conference paper ICLR Proof.
 Expanding (cid:107)xk+1 − x∗(cid:107)2 using update rule obtain (cid:107)xk+1 − x∗(cid:107)2 = (cid:107)xk − x∗(cid:107)2 − xk − x∗(cid:105) + γ2 k(cid:107)gk(cid:107)2, gk ≡ g(xk; ξ(k) conditioned xk, N ).
 given xk, gk converges ∇f (xk) w.p.1, have (cid:107)xk+1 − x∗(cid:107)2 → (cid:107)xk − x∗(cid:107)2 − (xk), xk − x∗(cid:105) + γ2 (15) other hand, applying strict convexity (13), ﬁrst taking x = xk, y = x∗ taking x = x∗, y = xk, obtain k(cid:107)∇f (xk)(cid:107)2 w.p.1. (cid:104)∇f (xk), xk − x∗(cid:105) ≥ l(cid:107)xk x∗(cid:107)2.
 (16) Substituting (16) (15), have conditioned xk, (cid:107)xk+1 − x∗(cid:107)2 → Ck w.p.1 Ck ≤ (1 − − x∗(cid:107)2 + γ2 kG2 = (1 − − x∗(cid:107)2 + G2/(l2k2).
 (17) consider randomness xk apply induction.
 base case k = theorem holds B2 = C1.
 theorem holds k = T let L = max{(cid:107)x1 − x∗(cid:107)2, G2/l2}.
 Then, taking probabilistic limit xT sides (17), have CT converges w.p.1 limit is less equal (1 − )(L/T + G2/(l2T ≤ L/(T +
 Letting limit be BT +1, complete induction proof.

 Learning long-term dependencies is key challenge current machine learning approaches artiﬁcial intelligence.
 ability human beings reconcile long-term dependencies immediate context, i.e., adapt use knowledge has been gained be relevant current frame-of-reference, is indispensable.
 important example ability, smaller scale, is ability predict characters words sentence document based one’s past experience (for example, form encountered constructions phrases), general subject dealt document, precise wording speciﬁc sentence question.
 Recurrent neural network based architectures have made signiﬁcant progress towards having machine mimic ability.
 Recurrent neural networks (RNNs) condition present representation state world entire history inputs (or “observations” reinforcement learning parlance), are natural ﬁt learning abstracted features.
 simple RNN represent arbitrary functions have capacity solve tasks involving dependencies arbitrary time-scales.
 practice, complex architectures have proven essential solving many tasks.
 reason is vanishing gradient problem (Hochreiter, Bengio makes difﬁcult simple RNNs learn long-term dependencies.
 Successful RNN architectures, such LSTMs (Hochreiter Schmidhuber, incorporate memory mechanisms ameliorate problem vanishing gradient.
 fundamental issue is learning detect long-term dependencies involves funda- difﬁcult credit assignment problem: absence prior information, past event ∗ Work begun author was MILA École Polytechnique Montréal c(cid:13) J.R.A. Moniz D.
 Krueger.
 MONIZ KRUEGER be responsible current events.
 Architectural features such memory mechanisms encode implicit priors help credit assignment problem.
 Memory mechanisms allow model remember past information time-scales, credit be assigned events distant past.
 seek encode additional implicit prior temporal hierarchy creation novel memory structure.
 particular, suggest selective memory access nesting approach constructing temporal hierarchies memory.
 prior work hierarchical memory exists, LSTM (and variants) are popular deep learning model sequential tasks, such character-level language modeling.
 default Stacked LSTM architecture uses sequence LSTMs stacked top other process data, input layer being output previous layer.
 work, propose explore novel Nested LSTM architecture (NLSTM), envision potential drop-in replacement stacked LSTM.
 NLSTMs, LSTM memory cells have access inner memory, read write using standard LSTM gates.
 key feature allows model implement effective temporal hierarchy conventional Stacked LSTM.
 NLSTM, (outer) memory cell are free read write relevant long-term information inner cell.
 contrast, stacked LSTMs, upper-level activations (analogous inner memories) are accessed produce output, contain short-term information is relevant current prediction.
 other words, primary difference stacked LSTMs Nested LSTMs is idea selective access inner memories NLSTM implements.
 frees inner memories remember process events longer time scales, events are relevant immediate present.
 visualizations demonstrate inner memories NLSTMs do fact operate longer time-scales higher-level memories stacked LSTM.
 experiments show NLSTMs outperform Stacked LSTMs wide range tasks.

 Related Work problem learning effective temporal hierarchies dealing long-term dependencies is studied context RNNs reinforcement learning.
 comprehensive review topic is scope, review recent works focus distinctive aspects approach.
 Doing credit assignment long time-scales is central problem reinforcement learning.
 options framework RL (Sutton et enables long-term planning sequences abstracted actions called
 Selecting option amounts enacting subpolicy selects primitive actions time-step (or own options).
 learning options has received attention (Stolle Precup, Brunskill Li, including recent gradient-based approaches (Arulkumaran al., Bacon successful applications have used hand-crafted options.
 Deep learning approaches temporal abstraction Currently, RNNs are stacked creating multi-layer feedforward network time-step.
 Hermans Schrauwen (2013) argue stacking results more abstract, long-term features; Zhang et al.
 (2016) argue be case.
 stacking, nesting increases recurrent depth, improve performance Zhang et al.
 (2016).
 Pascanu al.
 (2013) add NESTED LSTMS multi-layer input, output, recurrent connections alternative stacking; deep recurrent connections increase recurrent depth, are used.
 Multi-layer input connections have been used, however, state-of-the-art speech-recognition (Hannun et Amodei systems; systems incorporate stacked RNNs. model is based popular Long Short-term Memory (LSTM) (Hochreiter Schmid- huber, architecture.
 hidden states LSTMs include internal memory cells, use identity connections store long-term memories.
 LSTM forget/remember1 gate (Gers et allows memories be forgotten (adaptive) multiplicative decay identity connections.
 wide variety network architectures based inspired LSTMs have been proposed (Graves et Cho Chung Kalchbrenner et Danihelka Cheng
 popular know is Gated Recurrent Unit (GRU) (Cho et Chung
 GRUs function LSTMs, do feature internal memory; entire hidden state is exposed external computational units.
 moves opposite direction work, is focused creating more internal memory.
 recent works apportion more total hidden-state internal memories (Cheng et al., Rocki, way involves nesting.
 Greff al.
 (2015); Jozefowicz et al.
 (2015) evaluate architectural variants LSTMs GRUs; Greff al.
 (2015) remove components standard LSTMs, whereas Jozefowicz et al.
 (2015) use evolutionary search procedure search wider space possible models.
 LSTM remember gates model decay memories different units different rates, do encourage different units model different levels temporal dependency.
 other works attempt encode temporal hierarchy prior recurrent model.
 Temporal hierarchies units coded hand, Clockwork RNNs (Koutnik et al., hierarchical RNNs (Hihi Bengio,
 approach seems be preferable model learn operate appropriate time-scales.
 Chung al.
 (2015) present differentiable approach problem, based adding additional gating mechanisms.
 downside work is model size grows number layers hierarchy.
 recently, Chung al.
 (2016) straight-through estimator (Hinton, Yoshua Bengio, train model makes “crisp” binary decisions update different recurrent units.
 Recent work Deep Learning considers augmenting RNN architectures novel memory mechanisms inspired computer memory architectures (Graves et Joulin Mikolov, Grefenstette al., neural short-term memory mechanisms (Ba et al.,
 Storing accessing memories provide paths gradient ﬂow, but, RNNs, using backpropagation time becomes expensive sequences become long.
 standard solution problem is truncate gradient ﬂow number time-steps.
 Zaremba Sutskever (2015) attempt use reinforcement learning REINFORCE algorithm (Williams, solve problem context training Neural Turing Machines (NTMs).

 Nested output gate LSTMs encodes intuition memories are relevant present time-step be worth remembering.
 Nested LSTMs use intuition create temporal
 support efforts Ba al.
 (2016) reverse counter-intuitive naming convention.
 MONIZ KRUEGER hierarchy memories.
 Access inner memories is gated same way, longer-term information is relevant be accessed selectively.
 Figure Nested LSTM architecture architecture LSTM, equations updating cell state gates are given = σi(xtW xi + ht−1W hi + bi) f t = σf (xtW xf + ht−1W hf + bf ct = f t (cid:12) ct−1 + (cid:12) σc(xtW xc + ht−1W hc + bc) ot = σo(xtW xo + ht−1W ho + bo) ht = ot (cid:12) σh(ct) (1) (2) (3) (4) (5) (6) Note equations are similar deﬁned Graves (2013), do include peephole connections.
 Nested LSTMs replace addition operation used compute ct LSTMs learned, stateful function, ct = mt(ft (cid:12) ct−1, (cid:12) gt).
 refer state function, m time t inner memory, calling function compute ct computes mt+1.
 chose implement memory function LSTM memory cell, producing nested LSTM (see Figure illustration).
 memory function be Nested LSTM cell, permitting deep nesting.
 Given architecture choices, input hidden states memory function NLSTM become: (cid:101)ht−1 = f t (cid:12) ct−1 (cid:101)xt = (cid:12) σc(xtW xc + ht−1W hc + bc) ct =(cid:101)ht−1 +(cid:101)xt particular, note memory function is entire system reduces classical LSTM, cell update becomes (7) (8) (9) x x σi σc σo σf xt ht-1 xt ht-1 xt ht-1 ht xt ht-1 x x σi ct x NESTED LSTMS (a) LSTM (b) Stacked LSTM (c) Nested LSTM Figure Computational graphs LSTM, Stacked LSTM Nested LSTM.
 hidden state, outer memory cell, inner memory cell are represented h, c, d, respectively.
 current hidden state inﬂuence contents next inner memory cell directly, inner memory inﬂuences hidden state outer memory.
 architectural variant Nested LSTM proposed LSTM is used memory function, working inner LSTM is governed by: (cid:101)it =(cid:101)σi((cid:101)xt(cid:102)W xi +(cid:101)ht−1(cid:102)W hi +(cid:101)bi) (cid:101)f t =(cid:101)σf ((cid:101)xt(cid:102)W xf +(cid:101)ht−1(cid:102)W hf +(cid:101)bf (cid:101)ct = (cid:101)f t (cid:12)(cid:101)ct−1 +(cid:101)it (cid:12)(cid:101)σc((cid:101)xt(cid:102)W xc +(cid:101)ht−1(cid:102)W hc +(cid:101)bc) (cid:101)ot =(cid:101)σo((cid:101)xt(cid:102)W xo +(cid:101)ht−1(cid:102)W ho +(cid:101)bo) (cid:101)ht =(cid:101)ot (cid:12)(cid:101)σh((cid:101)ct) ct =(cid:101)ht cell state update outer LSTM becomes: (10) (11) (12) (13) (14) (15)
 Experiments evaluate Nested LSTMs wide variety datasets tasks: Penn Treebank Corpus (Marcus et al., larger Text8 dataset (both representing standard character-level language modeling, Text8 being much larger Penn Treebank Corpus), Chinese Poem Generation dataset (Zhang Lapata, (which requires character-level language modeling much smaller sequences less temporal dependency is common, larger number characters be found English), MNIST Glimpses task (Ba (which is classiﬁcation task, contains temporal dt MONIZ KRUEGER dependencies).
 show that, spite tasks representing diverse scenarios nested LSTMs improve performance corresponding stacked LSTM baselines comparable number parameters.
 set σi, σf σo,(cid:101)σi,(cid:101)σf and(cid:101)σo sigmoid activation function,(cid:101)σc,(cid:101)σh σh tanh, σc identity function Nested LSTM experiments.
 following experiments, initialize hyperparameters Nested LSTM stacked LSTM baselines identically.
 specify hyperparameters, mentioned, hyperparameters use are identical used Krueger al.
 (2016); Cooijmans et al.
 (2016).
 initialize nested stacked LSTMs’ ﬁrst input gates (which convert input vector having vocabulary number elements cell_size number elements) using Glorot initialization scheme Glorot Bengio (2010), other gates LSTM (the other input, remember output gates, ﬁnal output gate) are initialized using orthogonal initialization Saxe et al.
 (2013).
 try match number parameters different stacked LSTM baselines possible 2-layer Nested LSTM adjusting number hidden units.
 is possible 2-layer stacked baseline, is achieve single- layered 3-layered stacked LSTM.
 show results single-layered LSTMs: one same number parameters reference paper, number parameters larger used model.
 choose number hidden units 3-layered stacked LSTM surpass number parameters used model.
 Thus, model has equal number parameters 2-layered stacked LSTM, is disadvantage larger single layered LSTM 3-layered stacked LSTM, outperforms baselines.
 Visualization analyze cell activations look depend other, visualize changes cell activation states inner outer cell Nested LSTM sequence is fed it.
 do model trained Penn Treebank dataset described Section visualize cell states sequence test set is fed Nested LSTM Karpathy al.
 (2015).
 show resulting visualization Figure
 cells visualizations have been shown are ﬁrst cells model.
 visualization, see inner LSTM’s cell activations tend be consistent many time-steps, outer LSTM’s cell activations ﬂuctuate more rapidly.
 visualization demonstrates NLSTM hierarchy works expected: outer memory operates shorter time-scale uses inner memory store longer-term information.
 contrast similar visualization 2-layer stacked LSTM baseline, Figure
 higher layer memory (which is away" input) operates time- scale lower layer memory, ﬂuctuates inner cells NLSTM.
 indicates NLSTM’s ability process remember information multiple levels nested memories frees model remember information longer-periods, supports intuition nested memory structures form effective temporal hierarchies.
 NESTED LSTMS Figure A visualization cell activations corresponding input character inner cell (left) outer cell (right).
 Red implies negative cell state value, blue positive one.
 darker shade implies larger magnitude.
 case states inner LSTM, visualize tanh((cid:101)ct) (since(cid:101)ct is constrained), case outer LSTM, visualize ct.
 Figure visualization tanh(cn t ), representing cell corresponding input character ﬁrst (right) second (left) stacked layers.
 Red implies negative cell state value, blue positive one.
 darker shade implies larger magnitude.
 Penn Treebank Character-level Language Modeling Penn Treebank dataset (Marcus contains words, standard train:validation:test split.
 train models dataset perform character-level prediction, given input sequence, measure negative log likelihood (NLL) loss bits character (BPC, deﬁned NLL divided ln2).
 MONIZ KRUEGER Figure BPC function epoch character-level language modeling PTB’s test validation sets Model LSTM LSTM LSTM LSTM NLSTM cell size Params Valid Test Table BPC Losses Nested LSTM versus various baselines.
 test BPC losses correspond respective model’s loss epoch had minimum valid BPC (also shown).
 Penn Treebank, ﬁrst baseline is single layer LSTM hidden units, following prior works (Graves, Krueger Memisevic, Krueger et Cooijmans et
 compare architecture 2-layer 3-layer stacked LSTMs 2-layer nested LSTMs. number hidden units model is chosen (approximately) balance capacity parameters.
 choose single layered LSTM larger number parameters 2-layered LSTM NLSTM models.
 train using Adam (Kingma Ba, learning rate sequences batches clip gradients threshold aforementioned papers.
 However, train non-overlapping sequences, normalization believe improve results).
 train model epochs.
 ﬁnd nested LSTMs yield improvement .035 BPC stacked LSTMs using same number hidden units layers, which, turn, outperforms other baseline models.
 Notably, models 3-layered stack LSTM single-layer network, suggesting common use single-layer nets task is sub-optimal.
 Learning curves are presented Figure
 Chinese Poetry Generation Here, use subset Chinese Poem Generation dataset Zhang Lapata (2014) comprised quatrains 5-characters each, standard speciﬁed train:validation:test split.
 task is different PTB task: sequence length (and result length NESTED LSTMS Model LSTM LSTM LSTM LSTM NLSTM cell size Params Valid Test Table Perplexity Nested LSTM versus various baselines Chinese Poetry Generation dataset Figure Perplexity function epoch character-level prediction ChinesePG’s train validation sets temporal dependency) is much shorter, number characters is orders magnitude larger.
 follow hyperparameter set used LSTM character-level prediction task used baseline Yu al.
 (2017) (using learning rate
 Following suit, keep baselines single-layered LSTM cell size
 other baselines are single layered LSTM cell size 2-layered stacked LSTM cell size 3-layered stacked LSTM cell size
 compare baselines Nested LSTM cell size
 Note small cell sizes, number parameters models is same (around spite different numbers layers (except case single layered LSTM cell size has parameters).
 models compare against, however, have more equal parameters compared Nested LSTM (except 32-cell single layered LSTM, is introduce additional 40-cell single layered baseline).
 measure performance using perplexity (which is eN LL), Che al.
 (2017).
 ﬁnd Nested LSTM outperforms baselines perplexity ˜45 test set.
 Surprisingly, ﬁnd single-layered LSTM baselines outperform corresponding stacked LSTM baselines, one-layered LSTM fewer parameters.
 is small cell size used experiment.
 However, observe Nested LSTM outperforms single-layered LSTM case well, pointing robustness respect model size (i.e., number cells, extension, parameters, used model).
 MONIZ KRUEGER Model LSTM LSTM LSTM LSTM NLSTM cell size Params Valid NLL Test NLL Valid Accuracy (%) Test Accuracy (%) Table NLL percentage error Nested LSTM versus various baselines MNIST Glimpses task.
 epoch percentage error has been show corresponds model had lowest percentage error validation set.
 NLL, model’s validation NLL has been used determine epoch test NLL is examined.
 Figure Plots NLL (top) percentage error (bottom) MNIST glimpses’ train validation sets (versus epoch) MNIST Glimpses MNIST Glimpses task, introduced Ba al.
 image (with pixel values normalized range [0, is split quadrants.
 Glimpses quadrant (in form alternate rows columns), followed entire quadrant are fed model (with elements sequence, element comprising pixels).
 model predicts integer input represented.
 hyperparameter set use is similar chosen pMNIST task Krueger al.
 (2016): train models RMS Prop Tieleman Hinton (2012) learning rate epochs (note here, use used decay rate used there), clip gradients maximum norm
 aforementioned pMNIST use cell single layer LSTM baseline, cell single layer, cell two-layered stacked cell three-layered stacked LSTM baseline, compare baselines cell Nested LSTM.
 Nested LSTM outperforms (stacked) LSTM baselines terms NLL error percentage, validation test datasets.
 particular, reduces validation error compared next best performing model 2-layered stacked LSTM), validation NLL (down case single-layered LSTM).
 NESTED LSTMS text8 Model LSTM LSTM LSTM LSTM NLSTM cell size Params Valid Test Table BPC Nested LSTM versus various baselines text8 task Figure BPC vs epoch curves character-level prediction text8’s train validation sets text8 dataset comprises ﬁrst cleaned-up version enwik8 (which is comprised text Wikipedia)
 earlier experiments, keep hyperparameters identical Krueger al.
 (2016); Cooijmans et al.
 (2016), do use normalization, train non- overlapping sequences: use learning rate batch size sequence length gradient clipping threshold adam optimizer (Kingma Ba,
 model is trained epochs.
 baselines include celled single layered LSTMs, celled two-layered stacked LSTM celled three-layered stacked LSTM baseline, pitted celled Nested LSTM.
 too, observe model outperforms closest baseline (a 2-layered stacked LSTM) valid (1.363 vs respectively) test (1.445 vs respectively) sets.
 indicates proposed Nested LSTM is robust different model sizes (as shown improvement affords Chinese Poem generation task, small model was used), larger models trained large datasets beneﬁt nested architecture.

 Conclusions Nested LSTMs (NLSTM) are simple extension LSTM model add depth nesting, opposed stacking.
 inner memory cells NLSTM form internal memory, is accessible other computational elements outer memory cells, implementing form temporal hierarchy.
 NLSTMs outperform stacked LSTMs similar numbers parameters experiments, result deﬁned temporal hierarchies activations memory MONIZ KRUEGER cells compares stacked LSTMs. Thus, NLSTMs represent promising alternative stacked models.
 thank Christopher Beckham help early experiments, Tegan Maharaj Nicolas Ballas helpful discussions.
 thank developers Theano (Theano Development Team, Fuel, Blocks (van Merriënboer et
 acknowledge computing resources provided ComputeCanada CalculQuebec.
 References Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, al.
 Deep speech End-to-end speech recognition english mandarin.
 International Conference Machine Learning, pages
 Kai Arulkumaran, Nat Dilokthanakul, Murray Shanahan, Anil Anthony Bharath.
 Classifying options deep reinforcement learning.
 CoRR, abs/1604.08153,
 URL http://arxiv.
 org/abs/1604.08153.
 J.
 Ba, G.
 Hinton, V.
 Mnih, J.
 Z.
 Leibo, C.
 Ionescu.
 Using Fast Weights Attend Recent Past.
 ArXiv e-prints, October
 Jimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, Catalin Ionescu.
 Using fast weights attend recent past.
 Advances Neural Information Processing Systems, pages
 Pierre-Luc Bacon, Jean Harb, Doina Precup.
 option-critic architecture.
 CoRR, abs/1609.05140,
 URL http://arxiv.org/abs/1609.05140.
 Yoshua Bengio, Patrice Simard, Paolo Frasconi.
 Learning long-term dependencies gradient descent is difﬁcult.
 Neural Networks, IEEE Transactions on,
 Emma Brunskill Lihong Li. Pac-inspired option discovery lifelong reinforcement learning.
 ICML, pages
 Tong Che, Yanran Li, Ruixiang Zhang, R Devon Hjelm, Wenjie Li, Yangqiu Song, Yoshua Bengio.
 Maximum-likelihood augmented discrete generative adversarial networks.
 arXiv preprint arXiv:1702.07983,
 Jianpeng Cheng, Li Dong, Mirella Lapata.
 Long short-term memory-networks machine reading.
 arXiv preprint arXiv:1601.06733,
 Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio.
 Learning phrase representations using rnn encoder-decoder statistical machine translation.

 Junyoung Chung, Çaglar Gülçehre, KyungHyun Cho, Yoshua Bengio.
 Empirical evaluation gated recurrent neural networks sequence modeling.
 CoRR, abs/1412.3555,
 URL http://arxiv.org/abs/1412.3555.
 NESTED LSTMS Junyoung Chung, Caglar Gülçehre, Kyunghyun Cho, Yoshua Bengio.
 Gated feedback recurrent neural networks.
 CoRR, abs/1502.02367,
 Junyoung Chung, Sungjin Ahn, Yoshua Bengio.
 Hierarchical multiscale recurrent neural networks.
 CoRR, abs/1609.01704,
 URL http://arxiv.org/abs/1609.01704.
 Tim Cooijmans, Nicolas Ballas, César Laurent, Caglar Gulcehre, Aaron Courville.
 Recurrent batch normalization.
 arXiv preprint arXiv:1603.09025,
 Ivo Danihelka, Greg Wayne, Benigno Uria, Nal Kalchbrenner, Alex Graves.
 Associative short-term memory.
 arXiv preprint arXiv:1602.03032,
 Felix A.
 Gers, Jürgen Schmidhuber, Fred Cummins.
 Learning forget: Continual prediction lstm.
 Neural Computation,
 Xavier Glorot Yoshua Bengio.
 Understanding difﬁculty training deep feedforward neural networks.
 Proceedings Thirteenth International Conference Artiﬁcial Intelligence Statistics, pages
 A.
 Graves.
 Generating Sequences Recurrent Neural Networks.
 ArXiv e-prints, August
 Alex Graves, Santiago Fernández, Jürgen Schmidhuber.
 Multi-dimensional Recurrent Neural Networks, pages
 Springer Berlin Heidelberg, Berlin, Heidelberg,
 ISBN 540-74690-4.
 doi: 10.1007/978-3-540-74690-4\_56.
 URL http://dx.doi.org/10.1007/
 Alex Graves, Greg Wayne, Ivo Danihelka.
 Neural turing machines.
 CoRR, abs/1410.5401,
 URL http://arxiv.org/abs/1410.5401.
 Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, Phil Blunsom.
 Learning transduce unbounded memory.
 CoRR, abs/1506.02516,
 URL http://arxiv.org/ abs/1506.02516.
 Klaus Greff, Rupesh Kumar Srivastava, Jan Koutník, Bas R.
 Steunebrink, Jürgen Schmidhuber.
 LSTM: A search space odyssey.
 CoRR, abs/1503.04069,
 URL http://arxiv.org/ abs/1503.04069.
 Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, al.
 Deep speech: Scaling end-to-end speech recognition.
 arXiv preprint arXiv:1412.5567,
 Michiel Hermans Benjamin Schrauwen.
 Training analysing deep recurrent neural networks.
 Advances Neural Information Processing Systems, pages
 Salah El Hihi Yoshua Bengio.
 Hierarchical recurrent neural networks long-term dependencies.
 Advances Neural Information Processing Systems.

 Geoffrey Hinton.
 Neural networks machine learning coursera video lectures geoffrey hinton.

 URL https://www.coursera.org/course/neuralnets.
 MONIZ KRUEGER Sepp Hochreiter.
 Untersuchungen zu dynamischen neuronalen netzen.
 Master’s thesis, Institut fur Informatik, Technische Universitat, Munchen,
 Sepp Hochreiter Jürgen Schmidhuber.
 short-term memory.
 Neural computation,
 Armand Joulin Tomas Mikolov.
 Inferring algorithmic patterns stack-augmented recurrent nets.
 CoRR, abs/1503.01007,
 URL http://arxiv.org/abs/1503.01007.
 Rafal Jozefowicz, Wojciech Zaremba, Ilya Sutskever.
 empirical exploration recurrent network architectures.
 Journal Machine Learning Research,
 Nal Kalchbrenner, Ivo Danihelka, Alex Graves.
 Grid short-term memory.
 CoRR, abs/1507.01526,
 URL http://arxiv.org/abs/1507.01526.
 Andrej Karpathy, Justin Johnson, Li Fei-Fei.
 Visualizing understanding recurrent networks.
 arXiv preprint arXiv:1506.02078,
 Diederik Kingma Jimmy Ba. Adam: A method stochastic optimization.
 arXiv preprint arXiv:1412.6980,
 Jan Koutnik, Klaus Greff, Faustino Gomez, Juergen Schmidhuber.
 clockwork rnn.
 arXiv preprint arXiv:1402.3511,
 David Krueger Roland Memisevic.
 Regularizing rnns stabilizing activations.
 arXiv preprint arXiv:1511.08400,
 David Krueger, Tegan Maharaj, János Kramár, Mohammad Pezeshki, Nicolas Ballas, Nan Rosemary Ke, Anirudh Goyal, Yoshua Bengio, Hugo Larochelle, Aaron C.
 Courville, Chris Pal.
 Zoneout: Regularizing rnns preserving hidden activations.
 CoRR, abs/1606.01305,
 URL http://arxiv.org/abs/1606.01305.
 Matt Mahoney.
 test data,
 URL http://mattmahoney.net/dc/textdata.
 Mitchell P Marcus, Mary Ann Marcinkiewicz, Beatrice Santorini.
 Building large annotated corpus english: penn treebank.
 Computational linguistics,
 Razvan Pascanu, Çaglar Gülçehre, Kyunghyun Cho, Yoshua Bengio.
 construct deep recurrent neural networks.
 CoRR, abs/1312.6026,
 URL http://arxiv.org/abs/
 Kamil Rocki.
 Recurrent memory array structures.
 CoRR, abs/1607.03085,
 URL http: //arxiv.org/abs/1607.03085.
 Andrew M Saxe, James L McClelland, Surya Ganguli.
 Exact solutions nonlinear dynamics learning deep linear neural networks.
 arXiv preprint arXiv:1312.6120,
 Martin Stolle Doina Precup.
 Learning options reinforcement learning.
 International Symposium Abstraction, Reformulation, Approximation, pages

 NESTED LSTMS Richard S Sutton, Doina Precup, Satinder Singh.
 Between mdps semi-mdps: A framework temporal abstraction reinforcement learning.
 Artiﬁcial intelligence,
 Theano Development Team.
 Theano: Python framework fast computation mathematical expressions.
 arXiv e-prints, abs/1605.02688, May
 Tijmen Tieleman Geoffrey Hinton.
 Lecture 6.5-rmsprop: Divide gradient running average recent magnitude.
 COURSERA: Neural Networks Machine Learning,
 Bart van Merriënboer, Dzmitry Bahdanau, Vincent Dumoulin, Dmitriy Serdyuk, David Warde-Farley, Jan Chorowski, Yoshua Bengio.
 Blocks fuel: Frameworks deep learning.
 CoRR, abs/1506.00619,
 Ronald J.
 Williams.
 Simple statistical gradient-following algorithms connectionist reinforcement learning.
 Mach.
 Learn., May
 ISSN 0885-6125.
 doi:
 URL http://dx.doi.org/10.1007/BF00992696.
 Aaron Courville Yoshua Bengio, Nicholas Léonard.
 Estimating propagating gradients stochastic neurons.
 CoRR, abs/1305.2982,
 URL http://arxiv.org/abs/1305.

 Lantao Yu, Weinan Zhang, Jun Wang, Yong Yu. Seqgan: sequence generative adversarial nets policy gradient.
 Thirty-First AAAI Conference Artiﬁcial Intelligence,
 Wojciech Zaremba Ilya Sutskever.
 Reinforcement learning neural turing machines.
 CoRR, abs/1505.00521,
 URL http://arxiv.org/abs/1505.00521.
 Saizheng Zhang, Yuhuai Wu, Tong Che, Zhouhan Lin, Roland Memisevic, Ruslan Salakhutdinov, Yoshua Bengio.
 Architectural complexity measures recurrent neural networks.
 arXiv preprint arXiv:1602.08210,
 Xingxing Zhang Mirella Lapata.
 Chinese poetry generation recurrent neural networks.
 EMNLP, pages

 development accurate force fields1–3 efficient simulation large small systems has been cornerstone modern computational chemistry.
 popularity force fields is driven low computational cost relative more accurate transferable quantum mechanical (QM) methods, such density function theory4 post-Hartree-Fock5–7 methods.
 However, parametrizing universal force fields--applicable chemical system chemical environment--has remained elusive goal due restrictive functional form classical force fields.
 reason, “zoo” force fields has been developed last years applications various regions chemistry physics, such materials, proteins, carbohydrates, small drug-like molecules.8–11 Drawing line system- specific force fields work fail is difficult task.
 recent years, machine learning (ML) methods have been applied many areas chemistry physics research.12–19 Specifically, ML approaches prediction interatomic potential energy surfaces (referred ML potentials) have exhibited chemical accuracy compared QM models computational cost classical force fields.20–31 ML potentials promise bridge speed
 accuracy gap force fields QM methods.
 Many recent studies rely philosophy parametrization chemical system time22,26, single component bulk systems28,29 many equilibrium structures, i.e. QM7 QM9 datasets32,33.
 parametrization system time achieve high accuracy small amounts QM, has downside generate additional QM data train new ML model new chemical system.
 Using approach study requires extra parametrization time due non-universality potentials.
 Additionally, parametrization equilibrium geometries does attempt describe range conformations visited atomistic simulation.
 reasons, single system equilibrium dataset ML potentials do aim build extensible transferable (universal) ML potential.
 work ANAKIN-ME (ANI) method developing ANI-1 potential34 is example universal ML atomistic potential organic molecules.
 methodology is built concept atomic environment descriptor developed Behler Parrinello35 refined perform large diverse datasets organic molecules.
 key aspect ANI methodology was focus dataset diversity, promotes learning low level interactions (by utilizing localized descriptors) better transferability.
 training ANI-1 model, calculated structural conformations distinct small organic molecules using DFT.36 ANI-1 dataset was built exhaustive sampling molecules containing C, N, O atoms GDB-11 database, H atoms added saturate configurations.
 is based philosophy dataset construction samples small molecule conformational configurational space same time.
 ANI-1 potential was shown be accurate systems atoms more, demonstrating extensibility transferability much larger molecules training set.
 phenomenon, ML model is trained small systems (which be thought fragments large systems), demonstrated be extensible large systems has been confirmed other recent studies.37–39 Other recent work had success developing universal ML property predictors organic based chemical systems local minima.30 comes developing optimizing ML model training datasets, human intuition drives experiment design.
 resulting datasets tend be clustered, sparse, incomplete; recent work finds people tend favor inclusion “successful” experiments tend forget “failed” experiments.40 comprehensive incorporation data is strength ML approaches artificial intelligence (AI).
 sufficient data, AI-driven machine choose next step experiments simulations humans, speeding optimization given dataset, reducing overall amount data required.
 robotics transforms chemical synthesis41, manufacturing, transportation, constituting modern industrial revolution,42,43 achieving analogous revolution computational methods require AI particular emulation scientific intuition, reasoning decision making.
 Such ambitious program be accomplished require incremental progress AI algorithms are developed.
 work present automated approach dataset generation training universal ML potentials.
 is based concept active learning (AL), has been applied develop single system ML potentials.37,44–47 develop two-component technique training universal ML potentials.
 first component is dataset reduction algorithm eliminating redundancy existing training set.
 second is active learning algorithm based query committee48 (QBC) approach selecting new training data.
 complete rigorous validation universal potentials, develop COmprehensive Machine-learning Potential (COMP6) benchmark suite organic bio-molecules.
 COMP6 benchmark samples chemical space (for molecules containing C, H, N, O) molecules larger included training set, non-covalent interactions S66x8 benchmark49.
 COMP6 benchmark is available GitHub [https://github.com/isayev/ASE_ANI].
 Using active learning scheme, potential be trained accuracy ANI-1 using less data, sampling smaller molecules.
 further exploration chemical potential (dubbed ANI-1x) out-performs ANI-1, being trained dataset only size.
 II.
 METHODS context work, goal active learning is infer accurate predictor labeled training data.
 labeled data are input-output pairs y), output y represents correct answer question associated input X.
 problem ML potential training, label y be “yes” / “no” answer potential describes molecule X.
 part active learning process, question be answered given substance.
 Query Committee (QBC) approach uses disagreement models trained similar data infer correctness ensemble’s prediction.
 is following reasoning: ensemble predictors has high standard deviation, models ensemble have high error ground truth.
 Therefore, studies provided selection compounds have high standard deviation ensemble predictions search chemical space be employed sample high error regions chemical space automatically, minimizing need redundant QM calculations.
 Several empirical evidence method sampling improves overall fitness ML potentials single systems.37,50 work, apply concept massive search chemical space develop superior training set universal ML ANI34 potentials.
 ANI potentials are applicable organic molecules containing C, H, N, O.
 minimal modification, same approach be used other areas chemical sciences, e.g. materials.
 A.
 Sample selection Query Committee show how, rigorous statistical way, obtain priori information new samples be included subsequent generations ML potential training set.
 priori information is obtained QBC48 algorithm.
 QBC measures disagreement students (models) committee (ensemble), algorithm selects new examples students disagree preset inclusion criterion.
 Finally, new reference data selected examples are obtained included next committee training iteration.
 test agreement, choose include new data point 𝑖 test cases generate value 𝜌(cid:3036) greater inclusion criterion 𝜌(cid:3548), 𝜌(cid:3036) is defined as, 𝜌(cid:3036) = 𝜎(cid:3036) (cid:3493)𝑁(cid:3036)
 equation 𝜎(cid:3036) is standard deviation predictions ensemble (see Section IIE details) ANI potentials 𝑁(cid:3036) is number atoms given test system.
 square root is applied 𝑁(cid:3036) potentials are atomistic, total energy error is assumed be random distribution, centered zero, atom.
 is, cancellation error per atom basis lead low atom errors (and standard deviations case) larger molecules square root is applied.
 is necessary using single value 𝜌(cid:3548) test molecules varying numbers atoms is done work.
 Figure Example choosing value 𝜌(cid:3548) captures errors (𝜀) kcal/mol GDB07to09 benchmark set using initial (before using active learning) ANI model ensemble.
 value accomplished is found be 𝜌(cid:3548)
 value 𝜌(cid:3548) used query committee results selection test data.
 𝜀 are greater 𝜀 corresponding ρ > 𝜌(cid:3548) are greater Splitting dataset ρ = 𝜌(cid:3548) results total energy RMSE ANI ensemble prediction
 reference DFT kcal/mol values ρ > 𝜌(cid:3548) kcal/mol values ρ 𝜌(cid:3548).
 (cid:3002)(cid:3015)(cid:3010)}(cid:3036) (cid:3032)(cid:3041)(cid:3046) − 𝐸(cid:3021),(cid:3036) Figure provides example inclusion criterion 𝜌(cid:3548) is determined.
 2-dimensional (cid:3019)(cid:3006)(cid:3007))(cid:3627)/(cid:3493)𝑁(cid:3036) 𝑁(cid:3036) is number atoms ith density plot, 𝜀(cid:3036) = (cid:3627)𝑀𝐴𝑋({𝐸(cid:3021) molecule.
 Therefore, 𝜀(cid:3036) is largest atom prediction error model ensemble ANI models test molecule i.
 test data used example is GDB07to09 test set, is described Section IIC.
 ANI model used determine 𝜌(cid:3548) example is ANI model initialized AL process (Section IIB).
 value 𝜌(cid:3548) is determined choice value 𝜀 is considered large, percentage epsilon be considered fail cases.
 Therefore, 𝜌(cid:3548) = was selected is value allows selection 𝜀(cid:3036) > kcal/mol.
 example Figure determines 𝜌(cid:3548) = kcal/mol leads selection test data molecules fail agreement test.
 evidence choice 𝜀(cid:3036) allows statistical determination poorly fit data, is shown selecting data (i.e. 𝜌(cid:3036)), complete test set 𝜀(cid:3036) are greater However, is considering 𝜀(cid:3036) > kcal/mol correspond 𝜌(cid:3036) > 𝜌(cid:3548)
 shows determined 𝜌(cid:3548) leads selection data greater number 𝜀(cid:3036) > kcal/mol population.
 further validation approach, application concept is shown choose “bad” data calculating RMSE potential energy (𝐸) mean prediction ensemble ANI models
 reference DFT calculations.
 𝑖 molecular structures corresponding 𝜌(cid:3036) > 𝜌(cid:3548), 𝐸 RMSE is kcal/mol.
 other hand, 𝑖 molecular structures corresponding 𝜌(cid:3036) ≤ 𝜌(cid:3548) 𝐸 RMSE is kcal/mol.
 Therefore, statistical way, method chooses new data is higher error compared GDB07to09, is generated.
 enough processing time HPC resources, rate-limiting step QBC data selection cycle using ANI potentials is training new ensemble ANI models.
 Complete training single network takes minutes data points single NVIDIA Tesla V100 GPU.
 reduce number models QBC is used batches, searching configurational conformational (chemical) space tens thousands new reference data points fail agreement test.
 Finally, labels (reference potential energies, 𝐸(cid:3019)(cid:3006)(cid:3007)) are computed molecules selected batch.
 process lead redundant data; however, alternative, retraining new model ensemble addition new data point, be slow.
 B.
 Automatic chemical space sampling active learning Figure
 automated AL workflow data generation.
 algorithm contains steps: a) existing dataset reduction, b) configurational search, c) conformational search.
 Figure shows overall workflow iterative AL algorithm.
 algorithm is initialized existing random sampling generated dataset, contain large amounts redundant data.
 initial dataset (ANI-1 work) is reduced iterative approach goal minimizing overall dataset size impacting predictive performance.
 reduction algorithm is provided detail Figure
 Figure is initialized random subsampled original ANI-1 dataset.
 Then, iteratively, remaining data are tested, subsets fail cases are added training set.
 Here, fail case is defined |𝐸(cid:3002)(cid:3015)(cid:3010) − 𝐸(cid:3005)(cid:3007)(cid:3021)|/√𝑁 > 𝑘𝑐𝑎𝑙/𝑚𝑜𝑙, N is number atoms molecule.
 algorithm is terminated less data added training set are considered fail cases.
 remaining < high error data are added final dataset.
 Hyper- parameters reduction algorithm be tuned reduce redundancies data, cost more cycles, therefore, longer run time.
 final reduced dataset is used bootstrap remaining cycles active learning algorithm.
 dataset such ANI-1 is available, step be replaced generation small amount sampled data many small, C, N, O atoms, molecules.
 However, lead active learning cycles required achieving desired result.
 reduced configurational search (Figure is initialized.
 configurational search is carried sampling external database small molecules (e.g. GDB- ChEMBL53–55, generated dipeptides using RDKit [www.rdkit.org], generated dimers), embedding molecule space RDKit, optimizing initial structure UFF56 force field.
 See supplemental information Section S1.2.3 details dimer generation.
 Next, ANI energies are computed using ensemble ANI models trained current AL dataset (see Section IID details ensemble prediction training).
 Finally, 𝜌(cid:3036) = 𝜎(cid:3036)/√𝑁, 𝜎(cid:3036) is standard deviation ensemble’s energy predictions molecule i 𝑁 is number atoms molecule, is computed.
 test include molecule corresponding given 𝜌(cid:3036) is 𝜌(cid:3036) > 𝜌(cid:3548).
 selection 𝜌(cid:3548) is explained Section IIA.
 molecules fail test are included new conformer sampling set.
 molecules added conformer sampling set are geometry optimized correct reference QM level theory using tight SCF optimization convergence criteria.
 configurational search complete, conformational search cycle (Figure is initialized, conformer sampling set (a set equilibrium molecules generated configuration sampling step) is used generate set new non-equilibrium molecules (𝑋(cid:3552)).
 conformers 𝑋(cid:3552) are generated techniques, are designed sample various regions chemical space.
 sampling techniques are listed below.
  Diverse normal mode sampling (DNMS).
 version normal mode sampling presented previous work36, diversity selection used reduce redundant data bias equilibrium structures.
 detailed description DNMS is provided supplemental information (SI) section S1.2.1.  K random trajectory sampling (RTS).
 run short (4 ps) molecular dynamics simulations, ensemble ANI starting random velocities equal heated simulation time.
 dynamics, step QBC is used check current structure fails agreement test.
 structure is reached fails test, simulation is terminated, new QM data is generated inclusion training set.
 is repeated generate multiple new samples.
 detailed description RTS is provided SI section S1.2.2.  Molecular dynamics generated dimer sampling.
 Dimers are generated placing orienting molecules conformer sampling set box periodic boundary conditions.
 molecular dynamics simulation is carried box.
 steps box is fragmented dimer pairs desired cutoff radius.
 new dimer pair is tested using QBC approach, failed tests are kept new data, QM properties are generated inclusion training set.
 detailed description dimer sampling approach used is provided SI section S1.2.3. new data is selected, labels are computed included training set, new ensemble ANI potentials is trained.
 conformational search cycles are repeated model stops improving COMP6 benchmarks (see details Section IIC).
 entire cycle is restarted configurational sampling step.
 process is carried produce total cycles including many configurational conformational searching cycles.
 work refer various intermediate active learned ANI models AL1 AL6.
 AL6 potential is final potential reached work is referred ANI-1x potential, is provided free python package integrated atomic simulation environment (ASE) package57 [https://github.com/isayev/ASE_ANI].
 first row Table provides information final dataset work, labeled ANI-1x.
 Notably, size ANI-1x dataset, structures, is size dataset used training original ANI-1 potential (22M).
 C.
 Development COMP6 benchmark suite Table Description final active learning generated training dataset (ANI-1x) COMP6 benchmark datasets.
 Mean relative energy range is average range relative energies set conformers.
 Energy prediction range is real prediction range benchmark; is range ANI model predicts energies in.
 [energy units: kcal/mol] Purpose Dataset Training Testing ANI-1x S66x8 ANI-MD GDB7to9 GDB10to13 Tripeptides DrugBank Molecule Source ANI-1 + AL S66x8 PDB GDB-11 GDB-13 RDKit DrugBank Configurations (Conformations) (5,496,771) (528) (1,791) (36,000) (47,670) (1,984) (13,379) Atoms/Molecule mean (std.
 dev.) Mean Relative Energy Range (5) (7) (72) (3) (4) (7) (20) Energy Prediction Range validate active learning process generates ANI potential outperforms original ANI-1 potential, cycle’s resulting AL ANI potentials outperforms previous versions AL ANI develop comprehensive machine learned potential (COMP6) benchmark.
 COMP6 is benchmark suite composed rigorous benchmarks cover broad regions organic bio-chemical space (for molecules containing C, N, O, H atoms) sixth built existing S66x849 noncovalent interaction benchmark.
 new benchmark sets are referred GDB7to9, GDB10to13, Tripeptides, DrugBank, ANI-MD.
 See Table detailed description.
 benchmarks range mean molecule size atoms atoms, largest being atoms.
 Below is description methods used develop benchmark.
 Energies forces non- equilibrium molecular conformations presented have been calculated using ωB97x58 density functional 6-31G(d) basis set59 implemented Gaussian electronic structure software.
 Hirshfeld charges molecular dipoles are included benchmark.
 analysis properties be done future work.
  S66x8 Benchmark.
 dataset is built original S66x849 benchmark comparing accuracy different methods describing noncovalent interactions biological molecules.
 S66x8 is developed dimeric systems involving hydrogen bonding, pi-pi stacking, London interactions, mixed influence interactions.
 keen reader question use benchmark dispersion corrections, dispersion corrections such D361 correction Grimme al.
 are posteriori additions produced energy, comparison correction is equivalent comparison same dispersion corrections applied models.
  ANI Molecular Dynamics (ANI-MD) Benchmark.
 Forces ANI-1x potential are applied run vacuum molecular dynamics time step using Langevin thermostat well-known drug molecules small proteins.
 System sizes range atoms.
 random subsample frames trajectory is selected, reference DFT single point calculations are performed obtain QM energies forces.
  GDB7to9 Benchmark.
 GDB-11 subsets containing heavy atoms (C, N, O) are subsampled embedded space using RDKit [www.rdkit.org].
 total molecule SMILES [opensmiles.org] strings are selected: heavy- atom set.
 resulting structures are optimized tight convergence criteria, normal modes/force constants are computed using reference DFT model.
 Finally, diverse normal mode sampling (DNMS) is carried generate non-equilibrium conformations.
  GDB10to13 Benchmark.
 Subsamples SMILES strings heavy-atom subsets GDB-1151,52 SMILES strings heavy- atom subsets GDB-1362 database are selected.
 DNMS is utilized generate random non-equilibrium conformations.
  Tripeptide Benchmark.
 random tripeptides containing H, C, N, O are generated using FASTA strings embedded space using RDKit.
 GDB7to9, molecules are optimized, normal modes are computed.
 DNMS is utilized generate random non-equilibrium conformations.
  DrugBank Benchmark.
 benchmark is developed subsampling DrugBank63 database real drug molecules.
 SMILES strings containing C, N, O are selected.
 GDB7to9 benchmark, molecules are embedded space, normal modes are computed.
 DNMS is utilized generate random non-equilibrium conformations.
 D.
 Error metrics validation COMP6 benchmark suite work uses error metrics comparing different versions ANI potentials: potential energy (𝐸), conformer energy difference (∆𝐸), atomic force component errors (𝐹).
  Potential energy (𝐸) error is comparison 𝐸(cid:3036) (cid:3014)(cid:2869), potential energies produced model M1 molecule i, 𝐸(cid:3036) potential energies produced model M2 molecule i.
 conformer energy difference (∆𝐸) error is calculated set conformers.
 benchmark dataset K sets conformers are supplied, molecular configuration.
 given set conformers k, conformer energy difference conformers 𝑖 𝑗 (cid:3014),(cid:3038).
 Finally, error is given model M is obtained computing ∆𝐸(cid:3036)(cid:3037) (cid:3014)(cid:2870),(cid:3038) k, 𝑖, 𝑗 > 𝑖 + models M1 M2.
 calculated ∆𝐸(cid:3036)(cid:3037)  atomic force (𝐹) error metric is comparison individual components (x, (cid:3014)(cid:2869),(cid:3038) ∆𝐸(cid:3036)(cid:3037) (cid:3014),(cid:3038) = 𝐸(cid:3036) (cid:3014),(cid:3038) − 𝐸(cid:3037) y, z) atom’s force vector conformations included given benchmark.
 Comparisons are given mean absolute error (MAE), root mean squared error (RMSE) article.
 comparison MAE RMSE give information outliers model’s predictions.
 example, models have same MAE prediction given benchmark RMSE be much higher another.
 reason, is good practice provide MAE RMSE comparing methods benchmark.
 E.
 Property prediction ensemble ANI models energy force prediction use mean prediction ensemble ANI potentials.
 concept using ensemble mean ML model prediction is common practice ML community.
 Recently, has been adopted area ML molecular property prediction.23,37,64 All potentials used generate results work utilize mean prediction ensemble 𝐿 = ANI potentials trained 5-fold cross validation split training dataset.
 potential energy (𝐸) is represented by, 𝐸 = (cid:3013) (cid:3533) 𝐸(cid:3036) (cid:3036)(cid:2880)(cid:2869) 𝐸(cid:3036) is potential energy prediction ensemble’s 𝐿 ANI models.
 models are independent, atomic forces ensemble be derived component wise mean forces 𝐿 individual ANI models.
 use ensemble described decreases ANI vs.
 DFT 𝐸 RMSE kcal/mol, ∆𝐸 RMSE kcal/mol, 𝐹 RMSE kcal/mol × Å(cid:2879)(cid:2869) entire COMP6 benchmark.
 III.
 RESULTS DISCUSSIONS supplemental information (SI) provided work contains various tables detailing results obtained COMP6 benchmark ANI potentials discussed work.
 SI tables S1 S7 provide analysis ∆𝐸, 𝐸, 𝐹 errors obtained subsequent active learned ANI potentials, AL1 AL6, original ANI-1 potential.
 Note released ANI-1x potential is identical AL6 ANI potential.
 SI Tables S8 S10 supply analysis individual ANI-MD trajectory results ANI-1x potential.
 Table S9 provides atom energy errors ANI-1x potential vs.
 DFT shows mean energy prediction RMSE atom trajectories is kcal/mol atom.
 level accuracy is par single molecule bulk metal ML potentials described recent work J.
 Behler.26 SI Table S11 provides details ANI models introduced work.
 Finally, SI Tables S12 Figure
 Force correlation plots comparing ANI-1x, DFTB (3ob-3-1 parameter set bio-molecules), PM6 DFT reference calculations are provided left right, respectively, complete ANI-MD benchmark.
 Molecules ANI-MD benchmark are composed mean atoms largest being Trp-cage (1L2Y), 20-residue (312-atom) protein.
 DFTB PM6 are provided baseline comparison.
 Mean absolute errors (MAE) root mean squared errors (RMSE) are provided bottom right figure.
 color bar scale is same figures allowing proper density comparison.
 S17 give errors conformers select energy ranges ANI-1x potential.
 tables show much lower errors conformations are accessible room temperature molecular dynamics simulations.
 shown Table S17, accessible conformations (within have 𝐸 MAE/RMSE kcal/mol atom ∆𝐸 MAE/RMSE kcal/mol atom complete COMP6 benchmark.
 Figure provides evidence ANI-1x force prediction capabilities.
 Also, most tables supplemental information establish accuracy ANI potential force prediction COMP6 benchmark suite.
 construction, ANI potentials provide analytic energy- conservative forces, requirement molecular dynamics simulations.
 is noteworthy force training, be expensive, is required achieve force prediction results.
 forces compared DFT correlation density plots Figure are trajectories combined COMP6 ANI-MD benchmark.
 compare same figures ANI-1x (left), DFTB (center), PM6 (right).
 DFTB PM6 are included baseline comparison.
 is rigorous test case ML potential’s force prediction molecules supplied dataset range atoms, average size atoms.
 breakdown errors trajectory ANI-MD benchmark is supplied supplemental information, Tables S8 S10.
 closest comparison literature be found recent work system specific ML potential alanine tripeptide force RMSE kcal/mol × Å(cid:2879)(cid:2869) was achieved test data MD trajectory.37 force error work was obtained training energies analytic forces fragments molecule being tested.
 case ANI-1x potential, was used predict forces creation ANI-MD benchmark, MAE/RMSE kcal/mol × Å(cid:2879)(cid:2869) is obtained
 posteriori DFT calculations random frames molecule’s dynamics trajectories.
 𝐹 MAE/RMSE neutralized 20-residue Trp-cage (1L2Y) 10-residue Chignolin (1UAO) proteins are kcal/mol × Å(cid:2879)(cid:2869) kcal/mol × Å(cid:2879)(cid:2869), respectively.
 ANI-1x exhibits force MAE/RMSE kcal/mol × Å(cid:2879)(cid:2869) energy range kcal/mol tripeptide benchmark (non-equilibrium conformations generated tripeptides) COMP6 (see supplemental information Table S14), Figure
 Comparison potential energy (E) RMSE obtained entire COMP6 benchmark vs.
 training set size (total molecular conformation included training set).
 x-axis represents progression active learning process.
 Plot points are obtained ANI potentials (blue) trained various versions active learned dataset ANI potential (red) trained original ANI-1 dataset.
 molecular impressive, is accessible energy range molecular dynamics simulations.
 Finally, considering ANI-1x potential was utilized generate ns molecular dynamics simulations, ANI-MD benchmark geometries were sampled, speaks applicability forces molecular dynamics general molecular systems.
 mentioned results ANI-1x potential were obtained need direct force training.
 Figure provides plot 𝐸 RMSE achieved COMP6 vs.
 dataset size various active learned datasets original ANI-1 dataset.
 data points, active learned ANI potentials outperform original ANI-1 potential entire COMP6 benchmark.
 active learned ANI potential reaches data points times outperforms ANI-1 is approaching chemical accuracy reference DFT calculations.
 new COMP6 benchmark, diversity selection normal mode sampling helps ensure uniform sampling energy states energy range being fit tested within.
 Therefore, general errors COMP6 vs.
 ANI-1 potential’s original results are expected be much higher complex benchmark results published rigorous test sets original ANI- work.
 Table provides average energy ranges benchmark COMP6 final training set (ANI-1x), energy prediction (atomization energy) range.
 Most benchmarks COMP6 (all ANI-MD benchmark) were used active learning process validate improvement accuracy universality new active learned ANI models.
 Figure provides learning curves intermediate active learned ANI potentials benchmark COMP6.
 Supplemental information Table S11 provides information chemical space sampled datasets.
 horizontal dashed lines Figure
 Individual COMP6 benchmark learning curves successive versions active learned potentials.
 RMSE is provided properties: potential energy (𝐸), conformer energy differences (∆𝐸), force components (𝐹).
 error bars solid lines represent standard deviation ANI models ensemble used make mean prediction.
 horizontal lines represent mean prediction ANI-1.
 Figure represent original ANI-1 ensemble predictions benchmarks property corresponding color.
 AL1 is ANI potential used initialize active learning process.
 was trained reduced (Figure version heavy atom subsets ANI-1 dataset.
 AL2 AL6 are successive versions active learned ANI potentials.
 More details active learning cycle shown Figure is provided SI Table S11.
 active learning process, small molecules (one C, N, O atoms) were sampled, size molecules sampled increased active learning process continued.
 AL3 is AL models begin statistically match outperform original ANI-1 model most metrics.
 is notable AL3 accomplished feat having sampled conformations molecules 7-heavy atoms GDB-
 shows active learning techniques employed work chemical space better random sampling techniques.
 considering ANI-1 dataset includes conformations larger, 8-heavy atom, molecules.
 Eventually, AL4 AL5 steps, amino acids, generated dipeptides, generated small molecule dimers small ChEMBL molecules were added sampling set.
 is apparent large drop error AL4 AL5 DrugBank, Tripeptides, S66x8 benchmarks.
 Active learning sampling was driven GDB-11’s 9-heavy atom subset sampling production AL6.
 Supplemental information Tables S2 S7 provide full tables used make Figure Table S1 describes benchmarks once.
 latest ANI potential, ANI-1x (shown AL6), achieves remarkable property prediction complete benchmark errors (MAE/RMSE) kcal/mol (𝐸), kcal/mol (∆𝐸), kcal/mol × Å(cid:2879)(cid:2869) (F) full energy range benchmark.
 general, ANI potential’s fitness improves Figure standard deviation (shown vertical error bars) property prediction given ensemble decreases well.
 is sign model ensemble is obtaining enough chemical interaction information active learning models begin agreeing predictions larger systems.
 final iteration active learning cycles, active learned dataset data points is used training ANI-1x potential.
 ANI-1x potential outperforms ANI-1 potential properties benchmarks.
 ANI-1x dataset is size original ANI- dataset, contains total data points.
 process submitting paper learned recent work Herr al.65 was cautioned using specific sampling technique (i.e. MD, normal mode, meta-dynamics sampling) measure accuracy ML potential says nothing model performs specific sampling technique: “No measure [ML potential] single sampling technique contains quantitative information general performance sampling technique.”65 agree statement is true system specific potentials test sets where, example, NMS sample specific energy minima, does hold universal ML potentials test sets span relevant regions conformational configurational (chemical) space.
 following points argue statement is false regards NMS sampling COMP6 benchmark.
 configurational sampling conducted COMP6 is random sampling out-of-sample molecules random embedding RDKit.
 Second, NMS used generate states energy minima overlaps states produced MD sampling techniques.
 Third, restraints specific energy ranges allow controlled tests accessible regions chemical space, same space sampled MD.
 Therefore, large set out-of-sample molecules is used entire accessible region interest is considered, quantitative information universal ML potential performs MD simulations is obtained sampling technique used.
 empirical evidence this, ANI-1x potential tested ANI- MD benchmark (mean energy range kcal/mol) has F MAE/RMSE kcal/mol × Å(cid:2879)(cid:2869) (SI Table S10).
 Compare complete COMP6 benchmark (95% NMS sampled test data) restricted energy range kcal/mol; exhibits F MAE/RMSE kcal/mol × Å(cid:2879)(cid:2869) (SI Table S12).
 standard deviation model’s prediction MD vs.
 NMS sampled data places reasonable statistical fluctuation.
 conclusion be drawn NMS test data provides quantitative information model perform samples random MD simulations.
 IV.
 CONCLUSIONS pursuit automated dataset generation development universal machine learned introduce automatic active learning techniques sampling explored regions chemical space.
 algorithm begins reduction existing dataset remove redundant data loss accuracy.
 New conformations molecules are generated normal mode sampling, molecular dynamics sampling, random dimer sampling.
 algorithm samples new molecular configurations variety sources diversify exploration chemical space.
 result is new potential (ANI-1x) developed successive generations active learning process.
 ANI-1x potential is packaged user-friendly Python library, is available GitHub [https://github.com/isayev/ASE_ANI].
 introduce COMP6 benchmark monitoring progress active learning cycles comparison future universal potentials.
 ANI-1x potential achieves errors (MAE/RMSE) kcal/mol (𝐸), kcal/mol (∆𝐸), kcal/mol × Å(cid:2879)(cid:2869) (F) testing points kcal/mol energy minima complete COMP6 benchmark.
 available is made comparing COMP6 benchmark suite consists diverse benchmark test sets.
 COMP6 benchmark suite potentials [https://github.com/isayev/ANI1_dataset].
 provided, properties are calculated using ωB97x density functional 6-31G(d) basis set, however, be recomputed using desired quantum level theory.
 complete transparency, provide exact error metrics used measure accuracy COMP6 benchmark suite.
 is hope COMP6 benchmark provide universal ML potential development community rigorous benchmark comparison ML potential methods organic molecules extrapolative regime.
 COMP6 benchmark suite constitutes first benchmark kind comparison universal ML potentials changing ever-growing field.
 future ML ANI-1x potential was trained less conformations molecular configuration training set, compared ANI-1 dataset.
 accuracy ANI-1x potential is par best single molecule material ML potentials, most single molecule parametrized ML potentials require many hundreds thousands conformations parametrize single system.
 further validates configurational conformational big data sampling philosophy introduced original ANI-1 work.
 mean molecule size ANI-1x active learning training set is total atoms (8 heavy generation more accurate post-Hartree-Fock datasets plausible.
 high-level universal accuracy achieved ANI-1x potential be attributed capacity neural networks learn low level interactions developed descriptors.
 hypothesize use localized descriptors (i.e., atomic environment vector34 modified angular symmetry function) cutoff contribute ability.
 contrasts descriptor sets represent entire chemical environment once, interactions be inferred entire set non-local descriptors ML model.
 Given prospects high-throughput experiments, robotic synthesis, intelligent software, are witnessing transformation science data-driven automated discovery.
 envisioned chemical AI imitates human decision making transferring responsibility objective machine learning system.
 successful overall, approach revolutionize way computational methods are developed.
 possible building block construct such AI, introduced automated workflow select calculate QM training data accurate, transferable, extensible ML potentials.
 techniques aid generation universal potentials wide variety current future ML models.
 SUPPLEMENTARY INFORMATION ACKNOWLEDGEMENTS J.S.S thanks University Florida graduate student fellowship Los Alamos National Laboratory Center Non-linear Studies resources hospitality.
 work was performed, part, Center Integrated Nanotechnologies, Office Science User Facility operated U.S. Department Energy (DOE) Office Science.
 Los Alamos National Laboratory, affirmative action equal opportunity employer, is operated Los Alamos National Security, LLC, National Nuclear Security Administration U.S. Department Energy contract DE-AC52-06NA25396.
 O.I. acknowledges support DOD-ONR (N00014-16-1-2311) Eshelman Institute Innovation award.
 authors acknowledge Extreme Science Engineering Discovery Environment (XSEDE) award DMR110088, is supported National Science Foundation grant number ACI-1053575.
 research part was done using resources provided Open Science Grid66,67 is supported National Science Foundation U.S. Department Energy's Office Science.
 acknowledge support U.S. Department Energy LANL/LDRD Program work.
 authors thank Roman Zubatyuk Kipton Barros invaluable discussions topics presented work.
 J.A. Maier, C.
 Martinez, K.
 Kasavajhala, L.
 Wickstrom, K.E. Hauser, C.
 Simmerling, J.
 Chem.
 Theory Comput.
 (2015).
 K.
 Vanommeslaeghe, E.
 Hatcher, C.
 Acharya, S.
 Kundu, S.
 Zhong, J.
 Shim, E.
 Darian, O.
 Guvench, P.
 Lopes, I.
 Vorobyov, A.D. Mackerell, J.
 Comput.
 Chem.
 (2010).
 T.A. Halgren, J.
 Comput.
 Chem.
 (1996).
 K.S. Thanthiriwatte, E.G. Hohenstein, L.A. Burns, C.D. Sherrill, J.
 Chem.
 Theory Comput.
 (2011).
 H.J. Monkhorst, Int.
 J.
 Quantum Chem.
 (1977).
 G.D. Purvis R.J. Bartlett, J.
 Chem.
 Phys.
 (1982).
 D.
 Cremer, Wiley Interdiscip.
 Rev.
 Comput.
 Mol.
 Sci.
 (2011).
 J.
 Huang A.D. Mackerell, J.
 Comput.
 Chem.
 (2013).
 H.
 Sun, J.
 Phys.
 Chem.
 (1998).
 K.N. Kirschner, A.B. Yongye, S.M. Tschampel, J.
 González-Outeiriño, C.R. Daniels, B.L. Foley, R.J. Woods, J.
 Comput.
 Chem.
 (2008).
 J.A. Maier, C.
 Martinez, K.
 Kasavajhala, L.
 Wickstrom, K.E. Hauser, C.
 Simmerling, J.
 Chem.
 Theory Comput.
 (2015).
 T.
 Moot, O.
 Isayev, R.W. Call, S.M. McCullough, M.
 Zemaitis, R.
 Lopez, J.F. Cahoon, A.
 Tropsha, Mater.
 Discov.
 (2016).
 M.
 Ragoza, J.
 Hochuli, E.
 Idrobo, J.
 Sunseri, D.R. Koes, J.
 Chem.
 Inf.
 Model.
 (2017).
 B.
 Liu, B.
 Ramsundar, P.
 Kawthekar, J.
 Shi, J.
 Gomes, Q.
 Luu Nguyen, S.
 Ho, J.
 Sloane, P.
 Wender, V.
 Pande, ACS Cent.
 Sci.
 (2017).
 J.N. Wei, D.
 Duvenaud, A.
 Aspuru-Guzik, ACS Cent.
 Sci.
 (2016).
 R.
 Ramakrishnan, P.O. Dral, M.
 Rupp, O.A. Von Lilienfeld, J.
 Chem.
 Theory Comput.
 (2015).
 A.
 Lavecchia, Drug Discov.
 Today (2015).
 O.
 Isayev, C.
 Oses, C.
 Toher, E.
 Gossett, S.
 Curtarolo, A.
 Tropsha, Nat.
 Commun.
 (2017).
 E.
 Kim, K.
 Huang, A.
 Saunders, A.
 McCallum, G.
 Ceder, E.
 Olivetti, Chem.
 Mater.
 (2017).
 B.
 Kolb, B.
 Zhao, J.
 Li, B.
 Jiang, H.
 Guo, J.
 Chem.
 Phys.
 (2016).
 M.
 Hellström J.
 Behler, Phys.
 Chem.
 Chem.
 Phys.
 (2017).
 T.H. Ho, N.-N.
 Pham-Tran, Y.
 Kawazoe, H.M. Le, J.
 Phys.
 Chem.
 (2016).
 N.
 Lubbers, J.S. Smith, K.
 Barros, Preprint https://arxiv.org/abs/1710.00017 (2017).
 K.T. Schütt, F.
 Arbabzadah, S.
 Chmiela, K.R. Müller, A.
 Tkatchenko, Nat.
 Commun.
 (2017).
 K.
 Yao, J.E. Herr, S.N. Brown, J.
 Parkhill, J.
 Phys.
 Chem.
 Lett.
 (2017).
 J.
 Behler, Angew.
 Chemie Int.
 Ed. (2017).
 V.
 Botu, R.
 Batra, J.
 Chapman, R.
 Ramprasad, J.
 Phys.
 Chem.
 C (2017).
 S.
 Kondati Natarajan, T.
 Morawietz, J.
 Behler, Phys.
 Chem.
 Chem.
 Phys.
 (2015).
 J.
 Behler, R.
 Martoňák, D.
 Donadio, M.
 Parrinello, Phys.
 Status Solidi Basic Res.
 (WILEY‐VCH Verlag, pp.

 K.
 Yao, J.E. Herr, D.W. Toth, R.
 Mcintyre, J.
 Parkhill, Chem.
 Sci.
 (2018).
 K.
 Schütt, P.-J.
 Kindermans, H.E. Sauceda Felix, S.
 Chmiela, A.
 Tkatchenko, K.-R.
 Müller, Adv.
 Neural Inf.
 Process.
 Syst.
 edited I.
 Guyon, U.
 V Luxburg, S.
 Bengio, H.
 Wallach, R.
 Fergus, S.
 Vishwanathan, R.
 Garnett (Curran Associates, Inc., pp.

 R.
 Ramakrishnan, P.O. Dral, M.
 Rupp, O.A. von Lilienfeld, Sci.
 Data (2014).
 M.
 Rupp, A.
 K.-R.
 Muller, O.A. von Lilienfeld, Phys.
 Rev.
 Lett.
 (2012).
 J.S. Smith, O.
 Isayev, A.E. Roitberg, Chem.
 Sci.
 (2017).
 J.
 Behler M.
 Parrinello, Phys.
 Rev.
 Lett.
 (2007).
 J.S. Smith, O.
 Isayev, A.E. Roitberg, Sci.
 Data (2017).
 M.
 Gastegger, J.
 Behler, P.
 Marquetand, Chem.
 Sci.
 (2017).
 B.
 Huang O.
 Anatole Von Lilienfeld, Preprint https://arxiv.org/abs/1707.04146 (2017).
 K.
 Yao, J.E. Herr, D.W. Toth, R.
 Mcintyre, J.
 Parkhill, Preprint http://arxiv.org/abs/1711.06385 (2017).
 P.
 Raccuglia, K.C. Elbert, P.D.F. Adler, C.
 Falk, M.B. Wenny, A.
 Mollo, M.
 Zeller, S.A. Friedler, J.
 Schrier, A.J. Norquist, Nature (2016).
 P.J. Kitson, G.
 Marie, J.-P.
 Francoia, S.S. Zalesskiy, R.C. Sigerson, J.S. Mathieson, L.
 Cronin, Science (80-.
 ).
 (2018).
 T.
 Chapman, Nature (2003).
 R.D. King, J.
 Rowland, S.G. Oliver, M.
 Young, W.
 Aubrey, E.
 Byrne, M.
 Liakata, M.
 Markham, P.
 Pir, L.N. Soldatova, A.
 Sparkes, K.E. Whelan, A.
 Clare, Science (80-.
 ).
 (2009).
 E.
 V.
 Podryabinkin A.
 V.
 Shapeev, Comput.
 Mater.
 Sci.
 (2017).
 N.J. Browning, R.
 Ramakrishnan, O.A. von Lilienfeld, U.
 Roethlisberger, J.
 Phys.
 Chem.
 Lett.
 (2017).
 P.O. Dral, A.
 Owens, S.N. Yurchenko, W.
 Thiel, J.
 Chem.
 Phys.
 (2017).
 A.A. Peterson, R.
 Christensen, A.
 Khorshidi, Phys.
 Chem.
 Chem.
 Phys.
 (2017).
 H.S. Seung, M.
 Opper, H.
 Sompolinsky, Proc.
 Fifth Annu.
 Work.
 Comput.
 Learn.
 Theory COLT ’92 (ACM Press, New York, New York, USA, pp.

 B.
 Brauer, M.K. Kesharwani, S.
 Kozuch, J.M.L. Martin, Phys.
 Chem.
 Chem.
 Phys.
 (2016).
 I.
 Kruglov, O.
 Sergeev, A.
 Yanilkin, A.R. Oganov, Sci.
 Rep.
 (2017).
 T.
 Fink, H.
 Bruggesser, J.L. Reymond, Angew.
 Chemie Int.
 (2005).
 T.
 Fink J.L. Raymond, J.
 Chem.
 Inf.
 Model.
 (2007).
 S.
 Jupp, J.
 Malone, J.
 Bolleman, M.
 Brandizi, M.
 Davies, L.
 Garcia, A.
 Gaulton, S.
 Gehant, C.
 Laibe, N.
 Redaschi, S.M. Wimalaratne, M.
 Martin, N.
 Le Nov??re, H.
 Parkinson, E.
 Birney, A.M. Jenkinson, Bioinformatics (2014).
 A.P. Bento, A.
 Gaulton, A.
 Hersey, L.J. Bellis, J.
 Chambers, M.
 Davies, F.A. Krüger, Y.
 Light, L.
 Mak, S.
 McGlinchey, M.
 Nowotka, G.
 Papadatos, R.
 Santos, J.P. Overington, Nucleic Acids Res.
 D1083 (2014).
 M.
 Davies, M.
 Nowotka, G.
 Papadatos, F.
 Atkinson, G.
 van Westen, N.
 Dedman, R.
 Ochoa, J.
 Overington, Challenges (2014).
 A.K. Rappe, C.J. Casewit, W.A. Goddard III, K.S. Colwell, W.M. Skiff, J.
 Am. (1992).
 A.
 Hjorth Larsen, J.
 JØrgen Mortensen, J.
 Blomqvist, I.E. Castelli, R.
 Christensen, M.
 Dułak, J.
 Friis, M.N. Groves, B.
 Hammer, C.
 Hargus, E.D. Hermes, P.C. Jennings, P.
 Bjerre Jensen, J.
 Kermode, J.R. Kitchin, E.
 Leonhard Kolsbjerg, J.
 Kubal, K.
 Kaasbjerg, S.
 Lysgaard, J.
 Bergmann Maronsson, T.
 Maxson, T.
 Olsen, L.
 Pastewka, A.
 Peterson, C.
 Rostgaard, J.
 SchiØtz, O.
 Schütt, M.
 Strange, K.S. Thygesen, T.
 Vegge, L.
 Vilhelmsen, M.
 Walter, Z.
 Zeng, K.W. Jacobsen, J.
 Phys.
 Condens.
 Matter (2017).
 J.
 Da Chai M.
 Head-Gordon, J.
 Chem.
 Phys.
 (2008).
 R.
 Ditchfield, W.J. Hehre, J.A. Pople, J.
 Chem.
 Phys.
 (1971).
 G.
 M.
 J.
 Frisch, W.
 Trucks, H.B. Schlegel, G.E. Scuseria, M.A. Robb, J.R. Cheeseman, G.
 Scalmani, V.
 Barone, B.
 Mennucci, G.A. Petersson, H.
 Nakatsuji, M.
 Caricato, X.
 Li, H.P. Hratchian, A.F. Izmaylov, J.
 Bloino, G.
 Zheng, J.L. Sonnenberg, Gaussian, Inc.
 Wallingford, CT (2009).
 S.
 Grimme, J.
 Antony, S.
 Ehrlich, H.
 Krieg, J.
 Chem.
 Phys.
 (2010).
 L.C. Blum J.-L.
 Reymond, J.
 Am. Chem.
 Soc.
 (2009).
 V.
 Law, C.
 Knox, Y.
 Djoumbou, T.
 Jewison, A.C. Guo, Y.
 Liu, A.
 Maciejewski, D.
 Arndt, M.
 Wilson, V.
 Neveu, A.
 Tang, G.
 Gabriel, C.
 Ly, S.
 Adamjee, Z.T. Dame, B.
 Han, Y.
 Zhou, D.S. Wishart, Nucleic Acids Res.
 D1091 (2014).
 J.
 Gilmer, S.S. Schoenholz, P.F. Riley, O.
 Vinyals, G.E. Dahl, Preprint https://arxiv.org/abs/1704.01212 (2017).
 J.E. Herr, K.
 Yao, R.
 McIntyre, D.
 Toth, J.
 Parkhill, Preprint http://arxiv.org/abs/1712.07240 (2017).
 R.
 Pordes, D.
 Petravick, B.
 Kramer, D.
 Olson, M.
 Livny, A.
 Roy, P.
 Avery, K.
 Blackburn, T.
 Wenaus, F.
 Würthwein, I.
 Foster, R.
 Gardner, M.
 Wilde, A.
 Blatecky, J.
 McGee, R.
 Quick, J.
 Phys.
 Conf.
 Ser.
 (IOP Publishing, p.

 I.
 Sfiligoi, D.C. Bradley, B.
 Holzman, P.
 Mhashilkar, S.
 Padhi, F.
 Würthwein, WRI World Congr.
 Comput.
 Sci.
 Inf.
 Eng.
 CSIE (IEEE, pp.

 Supplementary information for: “Less is more: sampling chemical space active learning” Justin S.
 Smith1, Ben Nebgen3, Nicholas Lubbers3, Olexandr Isayev2,*, Adrian E.
 Roitberg1,* Chemistry, University Florida, Gainesville, FL USA Eshelman School Pharmacy, University North Carolina Chapel Hill, Chapel Hill, NC USA Alamos National Laboratory, Los Alamos, NM USA * Corresponding authors; email: OI (olexandr@olexandrisayev.com) AER (roitberg@ufl.edu) S1 Methods S1.1 ANI Ensemble Preparation Single network architectures vary size data set used train models active learning process.
 Table S11 describes models presented work.
 Network sizes (depth number parameters) were determined hyper parameter searches conducted configurational sampling step.
 Parameters atomic environment vector1 (a numerical vector used describe atoms local chemical environment) used active learning process were constant are provided released ANI-1x model.
 initial learning rate is used.
 Early stopping is utilized training network, model fails improve validation set predictions epochs training is stopped.
 Learning rate annealing is utilized such model stops early, training is restarted learning rate times previous learning rate.
 Termination training is achieved learning rate is less ×
 adam2 update method is used update weights training.
 Ensembles ANI potentials are prepared using 5-fold cross validation split data set mentioned hyper parameters.
 Training ensemble 5-fold cross validation split ensures ensemble was trained entire data set maximum performance.
 testing models hold training data set, use benchmarks COMP6 benchmark suite determine fitness ensembles prediction.
 do are interested getting potentials are accurate transferable extensible.
 COMP6 benchmarks rigorous test case be achieved testing hold training data set molecules benchmarks are average much larger included training set.
 allows testing extrapolation larger structures, is great importance universal ML potential, testing interpolation seen molecules.
 However, mean hold test set performance is supplied Table S11 AL models published work.
 S1.2 Sampling methods S1.2.1 Diverse Normal Mode Sampling (DNMS).
 modify normal mode sampling (NMS) technique introduced Smith el al.1 avoid data set clustering equilibrium conformations.
 technique follows NMS, molecule is optimized desired QM level theory, ωb97x 6-31g(d) basis set work, frequency calculations are performed obtain normal mode coordinates corresponding harmonic force constants.
 NMS, N random non-equilibrium conformations are generated perturbing molecule normal mode coordinates.
 diverse normal mode sampling atomic environment vector1 (referred AEV; numerical vector used describe chemical environment atom molecule) C, N, O atoms N conformations generated NMS is stored.
 squared Euclidean distance matrix N AEVs is computed.
 Finally, K diverse conformers is selected N original conformers using max-min diversity selector algorithm implemented RDKit [http://www.rdkit.org/] cheminformatics software package.
 sampling, using query committee approach introduce main article test i conformers K selected diverse conformers.
 generate QM energies forces 𝜌(cid:3036) > 𝜌(cid:3548) (Section IIA main article) add new data training set next iteration active learning algorithm.
 S1.2.2 K Random Trajectory Sampling (KRTS).
 Random trajectory sampling is carried set seed molecules conformational sampling data set.
 Given molecular configuration, random set Boltzmann distributed velocities equal are generated.
 Molecular dynamics using Langevin thermostat time step is initialized.
 system is heated
 steps dynamics, 𝜌(cid:3036) (Section IIA main article) is computed.
 𝜌(cid:3036) > 𝜌(cid:3548) dynamics is terminated.
 DFT reference data is computed final conformation added training set next iteration active learning cycle.
 trajectory reaches encountering 𝜌(cid:3036) > 𝜌(cid:3548) new data is added training set.
 Many trajectories be run seed molecules back generate multiple new reference data.
 S1.2.3 MD Generated Dimer Sampling Dimers are generated active learning scheme large box hundreds selected small molecules conformational sampling set) random positions orientation is generated.
 Molecular dynamics periodic boundary conditions using current version ANI active learned potential is ran box molecules X ps.
 molecular dynamics run, box is decomposed dimers intermolecular distances less
 Query committee (Section S1.1) is performed generated dimers, selecting dimer 𝜌(cid:3036) > 𝜌(cid:3548).
 DFT reference calculations are performed selected dimers obtain refence training data.
 new reference training data is added training data set next iteration active learning cycle.
 X was chosen be small (10fs) time algorithm stops generating new dimers, X is increased iteratively.
 ANI-1x data set X is set
 𝝈 𝑬𝑹 𝝈 𝑬𝑴 Table S1.
 Complete COMP6 benchmark suite results various ANI potentials.
 Errors conformer energy differences (∆E), potential energies (E), force components (F) active learned ANI potentials ANI-AL1 ANI-1X compared original ANI-1 potential.
 results are combination benchmarks COMP6 benchmark suite.
 µ σ are arithmetic mean standard deviation, respectively.
 M R are MAE RMSE, respectively.
 Units energy are kcal × mol(cid:2879)(cid:2869) units force are kcal × mol(cid:2879)(cid:2869) × Å(cid:2879)(cid:2869).
 𝝁 𝑬𝑹 ANI Model AL1 AL2 AL3 AL4 AL5 AL6 ANI-1 Table S2.
 DrugBank COMP6 benchmark results various ANI potentials.
 Errors conformer energy differences (∆E), potential energies (E), force components (F) active learned ANI potentials ANI-AL1 ANI-1X compared original ANI-1 potential.
 µ σ are arithmetic mean standard deviation, respectively.
 M R are MAE RMSE, respectively.
 Units energy are kcal × mol(cid:2879)(cid:2869) units force are kcal × mol(cid:2879)(cid:2869) × Å(cid:2879)(cid:2869).
 𝝈 𝑬𝑹 𝝁 ∆𝑬𝑹 𝝈 𝝁 ∆𝑬𝑴 𝝈 ∆𝑬𝑹 𝝁 𝑭𝑹 𝝁 𝑬𝑴 ∆𝑬𝑴 𝝁 𝑭𝑴 𝝈 𝑬𝑴 𝝈 𝑭𝑴 𝝈 𝑭𝑴 𝝈 𝑭𝑹 𝝈 𝑭𝑹 ANI Model AL1 AL2 AL3 AL4 AL5 AL6 ANI-1 ∆𝑬𝑴 𝝁 ∆𝑬𝑴 𝝈 ∆𝑬𝑹 𝝁 ∆𝑬𝑹 𝝈 𝝁 𝑬𝑴 𝝁 𝑬𝑹 𝝁 𝑭𝑴 𝝁 𝑭𝑹 𝝁 Table S3.
 Tripeptide COMP6 benchmark.
 Errors conformer energy differences (∆E), potential energies (E), force components (F) active learned ANI potentials ANI-AL1 ANI-1X compared original ANI-1 potential.
 µ σ are arithmetic mean standard deviation, respectively.
 M R are MAE RMSE, respectively.
 Units energy are kcal × mol(cid:2879)(cid:2869) units force are kcal × mol(cid:2879)(cid:2869) × Å(cid:2879)(cid:2869).
 ANI Model ∆𝑬𝑴 AL1 AL2 AL3 AL4 AL5 AL6 ANI-1 𝝁 ∆𝑬𝑹 𝝈 𝝈 ∆𝑬𝑹 𝝁 𝑬𝑴 𝝁 𝑭𝑴 𝝁 𝑬𝑹 𝝁 𝑭𝑹 𝝈 𝑬𝑴 𝝈 𝑭𝑹 𝝈 𝑬𝑹 𝝈 𝑭𝑴 Table S4.
 GDB07to09 COMP6 benchmark.
 Errors conformer energy differences (∆E), potential energies (E), force components (F) active learned ANI potentials ANI-AL1 ANI-1X compared original ANI-1 potential.
 µ σ are arithmetic mean standard deviation, respectively.
 M R are MAE RMSE, respectively.
 Units energy are kcal × mol(cid:2879)(cid:2869) units force are kcal × mol(cid:2879)(cid:2869) × Å(cid:2879)(cid:2869).
 ANI Model ∆𝑬𝑴 AL1 AL2 AL3 AL4 AL5 AL6 ANI-1 𝝁 ∆𝑬𝑹 𝝈 𝝁 ∆𝑬𝑴 𝝈 ∆𝑬𝑹 𝝁 𝑬𝑴 𝝁 𝑭𝑴 𝝁 𝑬𝑹 𝝁 𝑭𝑹 𝝈 𝑬𝑴 𝝈 𝑭𝑴 𝝈 𝑬𝑹 𝝈 𝑭𝑹 Table S5.
 GDB10to13 COMP6 benchmark.
 Errors conformer energy differences (∆E), potential energies (E), force components (F) active learned ANI potentials ANI-AL1 ANI-1X compared original ANI-1 potential.
 µ σ are arithmetic mean standard deviation, respectively.
 M R are MAE RMSE, respectively.
 Units energy are kcal × mol(cid:2879)(cid:2869) units force are kcal × mol(cid:2879)(cid:2869) × Å(cid:2879)(cid:2869).
 ANI Model ∆𝑬𝑴 AL1 AL2 AL3 AL4 AL5 AL6 ANI-1 𝝁 𝝈 ∆𝑬𝑹 𝝁 ∆𝑬𝑴 𝝁 𝑭𝑹 𝝁 𝑬𝑹 𝝁 𝑬𝑴 𝝁 𝑭𝑴 𝝈 ∆𝑬𝑹 𝝈 𝑬𝑴 𝝈 𝑭𝑴 𝝈 𝑬𝑹 𝝈 𝑭𝑹 Table S6.
 S66x8 COMP6 benchmark.
 Errors conformer energy differences (∆E), potential energies (E), force components (F) active learned ANI potentials ANI-AL1 ANI-1X compared original ANI-1 potential.
 µ σ are arithmetic mean standard deviation, respectively.
 M R are MAE RMSE, respectively.
 Units energy are kcal × mol(cid:2879)(cid:2869) units force are kcal × mol(cid:2879)(cid:2869) × Å(cid:2879)(cid:2869).
 ANI Model ∆𝑬𝑴 AL1 AL2 AL3 AL4 AL5 AL6 ANI-1 𝝁 𝝈 ∆𝑬𝑹 𝝁 ∆𝑬𝑴 𝝁 𝑬𝑹 𝝁 𝑬𝑴 𝝁 𝑭𝑴 𝝁 𝑭𝑹 𝝈 ∆𝑬𝑹 𝝈 𝑬𝑴 𝝈 𝑭𝑴 𝝈 𝑬𝑹 𝝈 𝑭𝑹 Table S7.
 ANI-MD COMP6 benchmark.
 Errors conformer energy differences (∆E), potential energies (E), force components (F) active learned ANI potentials AL1 AL6 compared original ANI-1 potential.
 µ σ are arithmetic mean standard deviation, respectively.
 M R are MAE RMSE, respectively.
 Units energy are kcal × mol(cid:2879)(cid:2869) units force are kcal × mol(cid:2879)(cid:2869) × Å(cid:2879)(cid:2869).
 ANI Model ∆𝑬𝑴 AL1 AL2 AL3 AL4 AL5 AL6 ANI-1 𝝁 𝝈 ∆𝑬𝑹 𝝁 ∆𝑬𝑴 𝝁 𝑬𝑹 𝝁 𝑭𝑹 𝝁 𝑬𝑴 𝝈 𝑬𝑹 𝝈 𝑬𝑴 𝝈 ∆𝑬𝑹 𝝁 𝑭𝑴 𝝈 𝑭𝑴 𝝈 𝑭𝑹 Table S8.
 Individual ANI-MD COMP6 benchmark trajectories.
 Errors conformer energy differences (∆E), potential energies (E), force components (F) ANI-1x potential vs DFT reference calculations conformations molecule ANI-MD COMP6 benchmark.
 Units energy are kcal × mol(cid:2879)(cid:2869) units force are kcal × mol(cid:2879)(cid:2869) × Å(cid:2879)(cid:2869).
 Per conformation (conf.) energy force prediction timings are included ANI-1x potential.
 System Acetaminophen Caffeine Salbutamol Atomoxetine Lisdexamfetamine Capsaicin Oseltamivir Retinol Fentanyl Tolterodine Ranolazine Atazanavir Chignolin (1UAO) TrpCage (1L2Y) E MAE E RMSE E range F MAE F RMSE F range ∆E MAE ∆E RMSE ∆E range Time(ms) conf.
 Table S9.
 Individual ANI-MD COMP6 benchmark trajectories atom.
 Per atom errors conformer energy differences (∆𝐸) potential energies (𝐸) ANI-1x potential vs DFT reference calculations conformations molecule ANI-MD COMP6 benchmark.
 Units energy are kcal mol(cid:2879)(cid:2869).
 System atoms Acetaminophen Caffeine Salbutamol Atomoxetine Lisdexamfetamine Capsaicin Oseltamivir Retinol Fentanyl Tolterodine Ranolazine Atazanavir Chignolin (1UAO) TrpCage (1L2Y) Mean 𝐸(cid:3019)(cid:3014)(cid:3020) √𝑁 𝐸(cid:3019)(cid:3014)(cid:3020) ∆𝐸(cid:3019)(cid:3014)(cid:3020) √𝑁 ∆𝐸(cid:3019)(cid:3014)(cid:3020) Table S10.
 Individual ANI-MD trajectories COMP6 benchmark ANI, DFTB, PM6.
 ANI-1x, DFTB PM6 vs DFT reference calculation errors conformer energy differences (∆𝐸) force components (F) conformations molecule ANI-MD COMP6 benchmark.
 Units energy are kcal × mol(cid:2879)(cid:2869) units force are kcal × mol(cid:2879)(cid:2869) × Å(cid:2879)(cid:2869).
 𝐃𝐅𝐓𝐁 ∆𝐄𝐌𝐀𝐄 𝐃𝐅𝐓𝐁 ∆𝐄𝐑𝐌𝐒 𝐀𝐍𝐈 ∆𝐄𝐑𝐌𝐒 𝐏𝐌𝟔 𝐀𝐍𝐈 ∆𝐄𝐌𝐀𝐄 𝐏𝐌𝟔 ∆𝐄𝐌𝐀𝐄 System Acetaminophen Atazanavir Atomoxetine Caffeine Capsaicin Chignolin (1UAO) Fentanyl Lisdexamfetamine Oseltamivir Ranolazine Retinol Salbutamol Tolterodine TrpCage (1L2Y) Mean 𝐀𝐍𝐈 𝐅𝐌𝐀𝐄 𝐏𝐌𝟔 𝐅𝐌𝐀𝐄 𝐀𝐍𝐈 𝐅𝐑𝐌𝐒 ∆𝐄𝐑𝐌𝐒 𝐃𝐅𝐓𝐁 𝐅𝐌𝐀𝐄 𝐃𝐅𝐓𝐁 𝐅𝐑𝐌𝐒 𝐏𝐌𝟔 𝐅𝐑𝐌𝐒 Table S11.
 ANI model details.
 Model details individual active learned ANI models (ANI-AL) network ensemble trained original ANI-1 data set.
 AL version is internal versioning scheme allows tracking data included given model version.
 AL cycles are number active learning conformational search cycles completed produce given ANI-AL model.
 Parameters is total number parameters models, consisted input size total hidden layers varying numbers nodes layer.
 Test set potential energy (E) RMSE are provided kcal × mol(cid:2879)(cid:2869).
 Model AL version AL Cycles Parameters AL1 AL2 AL3 AL4 AL5 ANI-1X (AL6) ANI-1 Test set RMSE (E) Configurational Sampling GDB-1 GDB-1 GDB-1 GDB-1 GDB-1 GDB-1 dimers; amino acids; dipeptides; CheMBLE GDB-1 GDB-1 dimers; amino acids; dipeptides; CheMBLE GDB-1 Table S12.
 Complete COMP6 benchmark ANI-1x select energy ranges.
 Errors conformer energy differences (∆E), potential energies (E), force components (F) active learned ANI potential ANI-1x select energy ranges test set.
 test set given energy range is built considering conformations given molecule energy range (shown column minimum energy conformer set conformations.
 results are combination benchmarks COMP6 benchmark suite.
 µ σ are arithmetic mean standard deviation, respectively.
 M R are MAE RMSE, respectively.
 Units energy are kcal × mol(cid:2879)(cid:2869) units force are kcal × mol(cid:2879)(cid:2869) × Å(cid:2879)(cid:2869).
 ANI Model 𝝁 𝑭𝑴 𝝁 𝑬𝑹 𝝈 𝑬𝑹 𝝈 𝑭𝑴 𝝁 𝑭𝑹 𝝈 𝑭𝑹 Energy Range ∆𝑬𝑴 𝝁 ∆𝑬𝑴 𝝈 ∆𝑬𝑹 𝝁 ∆𝑬𝑹 𝝁 𝑬𝑴 𝝈 𝑬𝑴 𝝈 ANI-1x Table S13.
 DrugBank COMP6 benchmark ANI-1x select energy ranges.
 Errors conformer energy differences (∆E), potential energies (E), force components (F) active learned ANI potential ANI-1x select energy ranges test set.
 test set given energy range is built considering conformations given molecule energy range (shown column minimum energy conformer set conformations.
 µ σ are arithmetic mean standard deviation, respectively.
 M R are MAE RMSE, respectively.
 Units energy are kcal × mol(cid:2879)(cid:2869) units force are kcal × mol(cid:2879)(cid:2869) × Å(cid:2879)(cid:2869).
 ANI 𝝁 𝑭𝑴 Model 𝝁 𝑬𝑹 𝝁 𝑭𝑹 𝝈 𝑭𝑴 𝝈 𝑬𝑹 𝝈 𝑭𝑹 Energy Range ∆𝑬𝑴 𝝁 ∆𝑬𝑴 𝝈 ∆𝑬𝑹 𝝁 ∆𝑬𝑹 𝝁 𝑬𝑴 𝝈 𝑬𝑴 𝝈 ANI-1x Table S14.
 Tripeptide COMP6 benchmark ANI-1x select energy ranges.
 Errors conformer energy differences (∆E), potential energies (E), force components (F) active learned ANI potential ANI-1x select energy ranges test set.
 test set given energy range is built considering conformations given molecule energy range (shown column minimum energy conformer set conformations.
 µ σ are arithmetic mean standard deviation, respectively.
 M R are MAE RMSE, respectively.
 Units energy are kcal × mol(cid:2879)(cid:2869) units force are kcal × mol(cid:2879)(cid:2869) × Å(cid:2879)(cid:2869).
 ANI 𝝁 𝑭𝑴 Model 𝝁 𝑬𝑹 𝝁 𝑭𝑹 𝝈 𝑭𝑴 𝝈 𝑬𝑹 𝝈 𝑭𝑹 Energy Range ∆𝑬𝑴 𝝁 ∆𝑬𝑴 𝝈 ∆𝑬𝑹 𝝁 ∆𝑬𝑹 𝝁 𝑬𝑴 𝝈 𝑬𝑴 𝝈 ANI-1x Table S15.
 GDB07to09 COMP6 benchmark ANI-1x select energy ranges.
 Errors conformer energy differences (∆E), potential energies (E), force components (F) active learned ANI potential ANI-1x select energy ranges test set.
 test set given energy range is built considering conformations given molecule energy range (shown column minimum energy conformer set conformations.
 µ σ are arithmetic mean standard deviation, respectively.
 M R are MAE RMSE, respectively.
 Units energy are kcal × mol(cid:2879)(cid:2869) units force are kcal × mol(cid:2879)(cid:2869) × Å(cid:2879)(cid:2869).
 ANI 𝝁 𝑭𝑴 Model 𝝁 𝑬𝑹 𝝁 𝑭𝑹 𝝈 𝑭𝑴 𝝈 𝑬𝑹 𝝈 𝑭𝑹 Energy Range ∆𝑬𝑴 𝝁 ∆𝑬𝑴 𝝈 ∆𝑬𝑹 𝝁 ∆𝑬𝑹 𝝁 𝑬𝑴 𝝈 𝑬𝑴 𝝈 ANI-1x Table S16.
 GDB10to13 COMP6 benchmark ANI-1x select energy ranges.
 Errors conformer energy differences (∆E), potential energies (E), force components (F) active learned ANI potential ANI-1x select energy ranges test set.
 test set given energy range is built considering conformations given molecule energy range (shown column minimum energy conformer set conformations.
 µ σ are arithmetic mean standard deviation, respectively.
 M R are MAE RMSE, respectively.
 Units energy are kcal × mol(cid:2879)(cid:2869) units force are kcal × mol(cid:2879)(cid:2869) × Å(cid:2879)(cid:2869).
 ANI 𝝁 𝑭𝑴 Model 𝝁 𝑬𝑹 𝝁 𝑭𝑹 𝝈 𝑭𝑴 𝝈 𝑬𝑹 𝝈 𝑭𝑹 Energy Range ∆𝑬𝑴 𝝁 ∆𝑬𝑴 𝝈 ∆𝑬𝑹 𝝁 ∆𝑬𝑹 𝝁 𝑬𝑴 𝝈 𝑬𝑴 𝝈 ANI-1x Table S17.
 Complete COMP6 benchmark atom errors ANI-1x select energy ranges.
 Per atom errors conformer energy differences (∆E) potential energies (E) achieved active learned ANI potential ANI-1x select energy ranges entire COMP6 benchmark.
 test set given energy range is built considering conformations given molecule energy range (shown column minimum energy conformer set conformations.
 µ is arithmetic mean.
 M R are MAE RMSE, respectively.
 Units energy are kcal × mol(cid:2879)(cid:2869) atom.
 ANI Model Energy Range 𝝁 𝑬𝑴 𝝁 𝑬𝑹 ANI-1x 𝝁 ∆𝑬𝑴 𝝁 ∆𝑬𝑹 Smith, J.
 S.; Isayev, O.; Roitberg, A.
 E.
 ANI-1: Extensible Neural Network Potential DFT Accuracy Force Field Computational Cost.
 Chem.
 Sci.

 Kingma, D.; Ba, J.
 A Method Stochastic Optimization.
 arXiv:1412.6980 [cs.LG]
 Brauer, B.; Kesharwani, M.
 K.; Kozuch, S.; Martin, J.
 M.
 L.
 S66x8 Benchmark Noncovalent Interactions Revisited: Correlated Ab Initio Methods Density Functional Theory.
 Phys.
 Chem.
 Chem.
 Phys.
 (31),
 (1) (2) (3)
 visual world is ﬁlled great variety textures, present images ranging multispectral satellite views mi- croscopic pictures tissue samples (Fig.

 powerful visual cue color, texture provides useful information identifying objects regions interest images.
 Texture is different color refers spatial organization set basic elements primitives (i.e., fundamental microstruc- tures natural images atoms preattentive human visual   Li Liu (li.liu@oulu.ﬁ) Jie Chen (jie.chen@oulu.ﬁ) Paul Fieguth (pﬁeguth@uwaterloo.ca) Guoying Zhao (guoying.zhao@oulu.ﬁ) Rama Chellappa (rama@umiacs.umd.edu) Matti Pietik¨ainen (matti.pietikainen@oulu.ﬁ) University Oulu, Finland National University Defense Technology, China University Waterloo, Canada University Maryland, USA perception [81].
 textured region obey statistical prop- erties, exhibiting repeated textons degree variability appearance relative position [51].
 Tex- tures range stochastic regular everything (see Fig.

 longstanding, fundamental challenging problem ﬁelds computer vision pattern recognition, texture anal- ysis has been topic intensive research [80] due signiﬁcance understanding texture per- ception process works human vision impor- tant role plays wide variety applications.
 analysis texture embraces several problems including clas- siﬁcation, segmentation, synthesis shape texture [202].
 Signiﬁcant progress has been made ﬁrst areas, shape texture receiving attention.
 Typical applications texture analysis include medi- cal image analysis [41, quality inspection con- tent based image retrieval [128, analysis satellite aerial imagery [84, face analysis [3, biometrics [118, object recognition [188, texture synthesis computer graphics image compression [56, robot vi- sion autonomous navigation unmanned aerial vehicles.
 ever-increasing amount image video data due surveil- lance, handheld devices, medical imaging, robotics etc.
 offers endless potential further applications texture analysis.
 Texture representation, i.e., extraction features de- scribe texture information, is core texture analysis.
 ﬁve decades continuous research, many kinds theories algorithms have emerged, major surveys rep- resentative work follows.
 majority texture features be found comparative studies [32,
 Tuceryan Jain [202] identiﬁed ﬁve major cat- egories features texture discrimination: statistical, geometri- cal, structural, model based, ﬁltering based features.
 Ojala al.
 carried comparative study evaluate tex- ture feature performance.
 Randen Husøy [170] re- viewed most major ﬁltering based texture features performed comparative performance evaluation.
 Zhang Tan [238] reviewed invariant texture feature extraction methods.
 Zhang al.
 evaluated performance several major invariant lo- Li Liu al.
 Fig.
 Texture is important characteristic many types images.
 cal texture descriptors.
 book “Handbook Texture Analysis” edited Mirmehdi al.
 [137] contains representative work texture analysis — feature extrac- tion synthesis, texture image acquisition classiﬁ- cation.
 book “Computer Vision Using Local Binary Patterns” Pietik¨ainen al.
 [165] provides excellent overview theory Local Binary Pattern (LBP) use solv- ing various kinds problems computer vision, biomedical applications biometric recognition systems.
 Huang al.
 presented review LBP variants applica- tion area facial image analysis.
 book “Local Binary Pat- terns: New Variants Applications” Brahnam al.
 [16] is collection several new LBP variants applica- tions face recognition.
 recently, Liu et al.
 conducted taxonomy recent LBP variants performed large scale performance evaluation forty texture features.
 Raad al.
 presented review exemplar based texture synthesis approaches.
 published surveys [32, reviewed compared methods
 Simi- larly, articles [170, covered approaches
 are more recent surveys [16, focused texture features based LBP.
 emer- gence many powerful texture analysis techniques has given rise further increase research activity texture research none published surveys provides exten- sive survey time.
 time change, believe is need updated survey, motivating present work.
 thorough review survey existing focus paper, contribute more progress texture analysis.
 goal is overview core tasks key challenges texture, deﬁne taxonomies representative approaches, provide re- view texture datasets, summarizing performance state art available databases.
 According different visual representations, survey categorizes texture classiﬁcation literature broad types: BoW-based, CNN- based attribute-based.
 BoW-based methods are organized according key components.
 CNN-based methods are cat- egorized using pretrained CNN models, using ﬁnetuned CNN models, using handcrafted deep convolutional networks.
 remainder paper is organized follows.
 Related background, including problem applications, progress made past decades, challenges problem, are summarized Section
 Section Section give detailed review texture representation techniques tex- ture classiﬁcation providing taxonomy more group prominent alternatives.
 summarization benchmark texture databases state art performance is given Section
 conclude paper discussion several promising di- rections texture representation Section
 Background Problem Texture analysis be divided areas: classiﬁcation, seg- mentation, synthesis, shape texture [202].
 Texture classi- ﬁcation [96, deals designing algorithms declaring given texture region image belonging set known texture categories training samples have been provided.
 Texture classiﬁcation be binary hy- pothesis testing problem, such differentiating texture being given class, such distinguishing healthy pathological tissues medial image analysis.
 goal texture segmentation is partition given image disjoint regions homogeneous texture [77,
 Texture syn- thesis is process generating new texture images are equivalent given texture sample [48,
 textures provide powerful shape cues, approaches shape texture attempt recover dimensional shape textured object image.
 be noted con- cept “texture” have different connotations deﬁnitions de- pending given objective.
 Classiﬁcation, segmentation, synthesis are related studied, shape texture receiving attention.
 Nevertheless, tex- ture representation is core problems.
 Texture representation, texture classiﬁcation, form primary focus survey.
 classical pattern recognition problem, texture classiﬁca- tion consists critical subproblems: texture rep- resentation classiﬁcation [78].
 is agreed extraction powerful texture features plays im- portant role, poor features are used best classiﬁer fail achieve good results.
 survey is explic- concerned texture synthesis, studying synthesis be instructive, example, classiﬁcation textures analysis synthesis [56] model is constructed synthesiz- ing textures inverted purposes classiﬁcation.
 TexturesinnaturalimagesMicroscopicAerialLightFieldSyntheticApertureRadarProstatecancerNodularChestXRayNormal A Survey Recent Advances Texture Representation Fig.
 evolution texture representation past decades (see discussion Section
 result, include representative texture modeling methods discussion.
 Markov Random Field (MRF) [34, fractal models [85,
 Summary Progress Past Decades Milestones texture representation past decades are listed Fig.

 study texture analysis be traced earliest work Julesz [80] studied theory hu- man visual perception texture suggested texture be modelled using kth order statistics — cooccurrence statistics intensities k-tuples pixels.
 Indeed, early work texture features such well known Gray Level Cooc- currence Matrix (GLCM) method [68, were driven perspective.
 Aiming seeking essential ingredients terms features statistics human texture perception, early Julesz [81, proposed texton theory explain texture preattentive discrimination, states textons (composed local conspicuous features such corners, blobs, terminators crossings) are elementary units preattentive human texture perception ﬁrst order statistics textons have percep- tual signiﬁcance: textures having same texton densities be discriminated.
 Julesz’s texton theory has been stud- ied has inﬂuenced development texture analysis methods.
 Research texture features late 1980s early focused well-established areas:
 Filtering approaches, convolve image bank ﬁlters followed nonlinearity.
 pioneering ap- proach was Laws [94], bank separable ﬁlters was applied, subsequent ﬁltering methods including Ga- bor ﬁlters [15, Gabor wavelets [128], wavelet pyra- mids simple linear ﬁlters Differences Gaussians [122].

 Statistical modelling, characterizes texture images arising probability distributions random ﬁelds, such end last century was renaissance texton- based including Zhu’s work et al.
 mathematical modelling textures textons.
 notable stride was Bag Textons (BoT) [100] Bag Words (BoW) [35, approaches, dictionary textons is generated, images are represented orderless histograms texton dictionary.
 need invariant feature representations was recognized, reduce eliminate sensitivity variations such illumination, scale, rotation, view point etc.
 gave rise development local invariant descriptors, mile- stone texture features such Scale Invariant Feature Transform (SIFT) [116] LBP [143].
 Such local hand-crafted texture de- scriptors dominated many domains computer vision turning point deep Convolutional Neural Networks (CNN) [90] achieved record-breaking image classiﬁcation accu- racy.
 time research focus has been deep learning methods many problems computer vision, including texture analysis [28,
 texture representations LBP [143], BoT [100], Fisher Vector (FV) [180], wavelet Scattering Con- volution Networks (ScatNet) [19]) have been used most areas image understanding computer vision.
 re- sult, division texture descriptors more generic im- age video descriptors has been
 study texture representation continues play important role computer vi- sion pattern recognition.
 Key Challenges spite several decades development, texture features have been capable performing level sufﬁcient real- world textures are complex meet real-time requirements many computer vision applications.
 TexturePerceptionModel(BelaJulesz)1970GLCM(Haralicketal.)19902014DCNNforImageNet(Krizhevskyetal.)FV-CNN(Cimpoietal.)1980198320152004201019621973Wavelet(StephaneMallat)SIFT(DavidLowe)20062002MultiScaleLBP(Ojalaetal.)VideoGoogle(SivicandZisserman)FractalModel(Kelleretal.)ImprovedFV(Perronninetal.)ScatteringConvolutionalNetwork(Bruna(cid:3)etal.)CNNforTextureSynthesis(Gatys(cid:3)etal.)UnifiedTheoryforTextureModeling(Zhuetal.)texturerepresentationsintheearlyyearstexturerepresentationsinthenewcentury(thefocusofthissurvey)20082012MRFTextureModel(CrossandJain)BagofTextons(LeungandMalik)2000GaborWavelets(ManjunathandMa)HarrisCornersandLaplacianBlobs(Lazebniketal.)LBP-TOPforDynamicTexture(ZhaoandPietikäinen)LBPforFacialTexture(Ahonenetal.)Textonboost(Shottonetal.)LawsFilterMasks(KennethLaws)GaussianMRF(ChellappaandChatterjee)DeCAFandIFV(Cimpoietal.)TheTextonTheory(BelaJulesz,Nature)GaborFilters(MarkTurner) Li Liu al.
 Fig.
 Illustrations challenges texture recognition.
 Dramatic intraclass variations: (a) Illumination (b) View point local nonrigid deformation, (c) Scale variations, (d) Different instances same category.
 Small interclass variations make problem harder still: (e) Images FMD database, (f) Images LFMD database (photographed light-ﬁeld camera).
 reader is invited identify material category foreground surfaces image (e) (f).
 correct answers are (from left right): (e) glass, leather, plastic, wood, plastic, metal, wood, metal plastic; (f) leather, fabric, metal, metal, paper, leather, water, sky plastic.
 Section gives details regarding texture databases.
 inherent difﬁculty obtaining powerful texture representations lies balancing competing goals: high quality representation high efﬁciency.
 High Quality Related Challenges consist large intraclass appearance variations caused changes illumination, rotation, scale, blur, noise, occlusion, etc.
 small in- terclass appearance differences, requiring texture representations be high robustness distinctiveness.
 Illustrative examples are shown Fig.

 further difﬁculty is obtaining sufﬁcient training data form labeled examples, are available limited amounts due collection time cost.
 High Efﬁciency Related Challenges include large number different texture categories high dimen- sional representations.
 have polar opposite motivations: big data, associated grand challenges scala- bility/complexity huge problems, tiny devices, growing need deploying compact efﬁcient texture representations resource limited platforms such embedded handheld devices.
 Bag Words based Texture Representation goal texture representation texture feature extraction is transform input texture image feature vector de- scribes properties texture, facilitating subsequent tasks such texture classiﬁcation, illustrated Fig.

 texture is spatial phenomenon, texture representation cannot be based single pixel, requires analysis patterns local pixel neighborhoods.
 Therefore, texture image is transformed pool local features, are aggregated global representation entire image region.
 properties texture are invariant, tex- Fig.
 goal texture representation is transform input texture im- age feature vector describes properties texture, facilitating subsequent tasks such texture recognition.
 texture image is ﬁrst transformed pool local features, are aggregated global representation entire image region.
 ture representations are based orderless aggregation local texture features, such sum max operation.
 Julesz [81] introduced “textons”, refer basic image features such elongated blobs, bars, crosses, terminators, elementary units preattentive human texture perception.
 Julesz’s texton studies were limited exclusive focus artiﬁcial texture patterns natural tex- tures.
 addition, Julesz did provide rigorous deﬁnition textons.
 Subsequently, texton theory fell disfavor model texture discrimination inﬂuential work Leung Malik [100] revisited textons gave operational deﬁni- tion texton cluster center ﬁlter response space.
 enabled textons be generated image, opened possibility learning universal texton dictionary images.
 Texture images be statisti- represented histograms texton dictionary, referred A Survey Recent Advances Texture Representation Bag Textons (BoT) approach.
 BoT was ini- developed context texture recognition [100, was introduced generalized image retrieval [194] classiﬁ- cation [35], was referred Bag Features (BoF) or, more commonly, Bag Words (BoW).
 research community has witnessed prominence BoW model decade many improvements were proposed.
 BoW Pipeline remainder section introduce methods component, summarized Table
 Local Texture Descriptors All local texture descriptors aim provide local representations invariant contrast, rotation, scale, other criteria.
 primary categorization is descriptor is applied densely, pixel, opposed sparsely, certain lo- cations interest.
 BoW pipeline is sketched Fig.
 consisting following basic steps.

 Local Patch Extraction.
 given image, pool N image patches is extracted sparse set points interest [96, ﬁxed grid [88, pixel position [143,
 i=1,
 Local Patch Representation.
 Given extracted N patches, local texture descriptors are applied obtain set pool tex- ture features D dimension.
 denote local features N patches image {xi}N i=1, xi ∈ RD.
 Ideally, local descrip- tors be distinctive same time robust variety possible image transformations, such scale, rotation, il- lumination, viewpoint changes.
 High quality local texture de- scriptors play critical role BoW pipeline.

 Codebook Generation.
 objective step is gen- erate codebook (i.e., texton dictionary) K codewords {wi}K wi ∈ RD based training data.
 codewords be learned (e.g., kmeans [95, predeﬁned way (such LBP [143]).
 size nature codebook affects represen- tation followed discrimination power.
 key is generate compact discriminative codebook enable accurate efﬁcient classiﬁcation.

 Feature Encoding.
 Given generated codebook extracted local texture features {xi} image, role fea- ture encoding is represent local feature xi code- book, mapping xi number code- words, resulting feature coding vector vi (e.g. vi ∈ RK).
 steps BoW pipeline, feature encoding is core compo- nent links local representation feature pooling, inﬂuencing texture classiﬁcation terms accuracy speed.
 Thus, many studies have focused developing powerful feature encoding, such vector quantization / kmeans, sparse cod- ing [120, Locality constrained Linear Coding (LLC) [216], Vector Locally Aggregated Descriptors (VLAD) [79], Fisher Vector (FV) [30,

 Feature Pooling.
 global feature representation y is pro- duced using feature pooling strategy aggregate coded feature vectors {vi}.
 Classical pooling methods include average pooling, max pooling, Spatial Pyramid Pooling (SPM) [97,

 Feature Classiﬁcation.
 global feature is used basis classiﬁcation, many approaches are possible [78, Nearest Neighbor Classiﬁer (NNC), Support Vector Ma- chines (SVM), neural networks, random forests.
 SVM is used classiﬁers BoW based representa- tion.
 Sparse Texture Descriptors develop sparse texture descriptor, region interest detec- tor be designed is able reliably detect sparse set regions, reliably stably, various imaging conditions.
 detected regions undergo geometric normalization, local descriptors are applied encode image con- tent.
 series region detectors local descriptors has been proposed, excellent surveys [134,
 sparse ap- proach was introduced texture recognition Lazebnik al.
 followed Zhang al.
 [239].
 types complementary region detectors, i.e. Harris afﬁne detector Mikolajczyk Schmid [133] Laplacian blob detector G˚arding Lindeberg [55], were used detect afﬁne covariant regions, meaning region content is afﬁne invariant.
 detected region be thought tex- ture element having characteristic elliptic shape distinc- tive appearance pattern.
 order achieve afﬁne invariance, elliptical region was normalized rotation invariant descriptors, spin image (SPIN) Rotation Invariant Fea- ture Transform (RIFT) descriptor, were applied.
 result, texture image feature channels were extracted (two de- tectors × descriptors), feature channel kmeans clustering is performed form signature.
 Earth Mover’s Distance (EMD) [177] was used measuring similarity be- tween image signatures NNC was used classiﬁcation.
 Al- SPIN RIFT achieve rotation invariance, lack dis- tinctiveness spatial information is lost due fea- ture pooling schemes.
 Following Lazebnik al.
 [96], Zhang et al.
 presented evaluation multiple region detector types, levels geometric invariance, multiple local texture descriptors, SVM classiﬁer kernels based effective measures comparing distri- butions (signatures EMD distance vs.
 standard BoW Chi Square distance) texture object recognition.
 Regarding local description, Zhang et al.
 [239] used SIFT descriptor1 addition SPIN RIFT.
 recommended practical texture recognition seek incorporate multiple types complementary features, local invariance properties exceeding required given application.
 Other local region detectors have been used texture description, such Scale Descriptors measure scales salient textons [83].
 Originally, SIFT is comprised detector descriptor, are used isolation now; survey, speciﬁed, SIFT refers de- scriptor, common practice community.
 Li Liu al.
 Fig.
 General pipeline BoW model.
 See Table refer Section detail discussion.
 Features are computed handcrafted detectors descriptors SIFT RIFT, applied local texture descriptors handcrafted ﬁlters CNNs. CNN features be computed end end manner using ﬁnetuned CNN models.
 local features are quantized visual words codebook.
 Dense Texture Descriptors number features derived sparse set interesting points is smaller total number image pixels, re- sulting compact feature space.
 However, sparse approach be inappropriate many texture classiﬁcation tasks: ◦ Interest point detectors produce sparse output ◦ A sparse output small image produce sufﬁcient ◦ are issues regarding repeatability detectors, stability selected regions instability orienta- tion estimation [135].
 regions robust statistical characterization.
 miss important texture elements.
 result, extracting local texture features pixel is popular representation, subject following discussion.
 (1) Gabor Filters are popular texture descrip- tors, motivated relation models early visual systems mammals joint optimum resolution time frequency [77,
 illustrated Fig.
 Gabor ﬁlters be considered orientation scale tunable edge bar de- tectors.
 Gabor wavelets are generated appropriate rotations dilations following product elliptical Gaussian complex plane wave: exp(j2πω), (cid:19)(cid:21) φ(x, y) = (cid:18) x2 (cid:20) (cid:18) (u − ω)2 exp (cid:20) Fourier transform is ˆφ(x, = exp v2 y2 (cid:19)(cid:21) ω is radial center frequency ﬁlter frequency domain, σx σy are standard deviations elliptical Gaussian x y.
 Thus, Gabor ﬁlter bank is deﬁned parameters including frequencies, orientations parameters Gaussian enve- lope.
 literature, different parameter settings have been sug- gested, ﬁlter banks created parameter settings work general.
 Details derivation Gabor wavelets parameter selection be found [99,
 Schmid Filters (SFilters) [181] consist rotation- ally invariant Gabor-like ﬁlters form (cid:18) x2 + y2 (cid:19)(cid:21) (cid:20) cos (cid:32) πβ(cid:112)x2 + y2 (cid:33) φ(x, y) = exp β is number cycles harmonic function Gaussian envelope ﬁlter.
 ﬁlters are shown Fig.
 be seen, ﬁlters have rotational symmetry.
 (3) Filters Leung Malik (LM Filters) [100, pi- oneered problem classifying textures varying view- point illumination.
 LM ﬁlters used local texture fea- ture extraction are illustrated Fig.

 particular, marked milestone giving operational deﬁnition textons: cluster centers ﬁlter response vectors.
 work has been followed other researchers [35,
 han- dle effects caused imaging, proposed textons were cluster centers ﬁlter responses stack images representative viewpoints lighting, illustrated Fig.

 texture classiﬁcation algorithm, images texture were registered transformed local fea- tures LM Filters.
 ﬁlter response vectors selected images same pixel were concatenated obtain feature vector local texture representation.
 Subse- quently, feature vectors were input BoW pipeline texture classiﬁcation.
 downside method is is suitable classifying single texture image unknown imaging conditions, arises practical applications.
 (4) Maximum Response (MR8) Filters Varma Zisser- man [213] consist root ﬁlters ﬁlter responses.
 ﬁlter bank contains ﬁlters multiple orientations outputs are pooled recording maximum ﬁlter response orientations, order achieve rotation invariance.
 root ﬁlters are subset LM Filters [100] Fig.
 retaining rotational symmetry ﬁlters, edge ﬁlter, bar ﬁlter scales orientations.
 Recording maximum response (cid:311)(cid:312)(cid:313)(cid:314)Feature Detectors DescriptorsFeatureVectorsFeature Encoding (with Codebook)OrderlessPoolingPredefined FiltersOne Pass CNN/MultiPass CNNd-dimp-dimq-dimmaps ofmaps CornersLaplacian BlobsRIFT, SIFTTexture Image (Size )key pointsClassifierAveragePoolingFullyConnectedsoftmaxOrReorderingintoaFeatureVector A Survey Recent Advances Texture Representation Table A summary components BoW representation pipeline, was sketched Fig.

 Step Approach Highlights Sparse Descriptors • (Harris+Laplacian)(RIFT+SPIN) [96] • (Harris+Laplacian)(RIFT+SPIN+SIFT) [239] Keypoint detectors novel descriptors SPIN RIFT A comprehensive evaluation multiple keypoint detectors, feature descriptors, classiﬁer kernels.
 Dense Descriptors • Gabor Wavelets • LMﬁlters [100] • Schmid Filters • MR8 [213] • Patch Intensity [214] • LBP [143] • Random Projection [109] • Sorted Random Projection [110] • Basic Image Features (BIFs) [33] • Weber Local Descriptor (WLD) [33] Fractal Based Descriptors • MultiFractal Spectrum [229] Predeﬁned kmeans clustering [35, GMM modeling Sparse Coding based learning [161, Voting Based Methods • Hard Voting [100, • Soft Voting [2, Fisher Vector (FV) Based Methods • FV [158] • Improved FV (IFV) [28, • VLAD [79, Reconstruction Based Methods Joint optimum resolution time frequency; Multiscale multiorientation analysis.
 propose Bag Texton (BoT) model (i.e. BoW Gabor ﬁlters; Rotation invariant.
 invariant ﬁlters low-dimensional ﬁlter response space.
 Challenge dominant role ﬁlter descriptors propose image raw intensity feature.
 Fast binary features gray scale invariance; Predeﬁned codebook.
 introduce compressive sensing random projection texture classiﬁcation.
 Efﬁcient effective approach random projection achieve rotation invariance.
 Introduce BIFs Grifﬁn Lillholm texture classiﬁcation; Predeﬁned codebook.
 descriptor based Weber’s Law.
 Invariant bi-Lipschitz mapping.
 codebook learning step; Computationally efﬁcient.
 Most used method; Cannot capture overlapping distributions feature space.
 Considers cluster centers covariances describe spreads clusters.
 Sparse representation based; Minimize reconstruction error data; Computationally expensive.
 Require large codebook learned kmeans); combine nonlinear SVM.
 Quantize feature nearest codeword; Fast compute; Codes are sparse high dimensional.
 Assigns feature multiple codewords; Does minimize reconstruction error.
 Require small codebook; high dimension; Combines efﬁcient linear SVM.
 GMM-based; Encodes higher order Efﬁcient compute.
 Uses signed square rooting L2 normalization; State art performance texture classiﬁcation.
 simpliﬁed version FV.
 Enforce sparse representation; Explores manifold structure data; Minimize reconstruction error.
 Leverage fact natural images are sparse; Optimization is expensive.
 • Sparse Coding [161, • Local constraint Linear Coding (LLC) [28, Local sparsity; Fast computation approximated LLC.
 Average Pooling Max Pooling Spatial Pyramid Pooling (SPM) Nearest Neighbor Classiﬁer (NNC) [109, Kernel SVM [239] Linear SVM [30] used pooling scheme texture representation.
 used combination sparse coding LLC.
 Preserving spatial information; Higher feature dimensionality.
 Simple elegant nonparametric classiﬁer; Popular texture classiﬁcation.
 combination Chi Square BoW based representation.
 Suitable high-dimensional feature representation FV VLAD.
 Fig.
 Illustration Gabor wavelets used [128].
 Fig.
 Illustration rotationally invariant Gabor-like ﬁlters used [181].
 parameter (σ, β) pair takes values (2,1), (4,1), (4,2), (6,1), (6,2), (6,3), (8,1), (8,2), (8,3), (10,1), (10,2), (10,3) (10,4).
 orientations reduces number responses Fig.
 LMﬁlter bank has mix edge, bar spot ﬁlters multiple scales orientations.
 has total ﬁlters: Gaussian derivative ﬁlters orientations scales, Laplacian Gaussian ﬁlters Gaussian ﬁlters.
 (3 scales anisotropic ﬁlters, isotropic), resulting so-called MR8 ﬁlter bank.
 Realizing shortcomings Leung Malik’s method [100], Varma Zisserman [213] attempted improve classiﬁcation single texture sample image unknown imaging condi- tions, bypassing registration step, learning textons aggregating ﬁlter responses different images.
 other steps classiﬁcation process were same Leung Malik.
 Experimental results showed MR8 out- performed LM Filters Schmid Filters.
 Later, Hayman et al.
 (a) Real Part(b) Imaginary Part Li Liu al.
 Fig.
 Illustration process texton dictionary learning proposed Leung Malik [100].
 image different lighting viewing direc- tions is ﬁltered using ﬁlter bank illustrated Fig.

 response vectors are concatenated form data vectors length Nf ilNim.
 data vectors are clustered using kmeans algorithm obtain textons.
 Fig.
 illustration SRP descriptor: extracting SRP features exam- ple local image patch size ×
 (a) Sorting pixel intensities; (b,c) Sorting pixel differences.
 Fig.
 Illustration Patch Descriptor proposed [214]: raw inten- sity vector is used local representation.
 [70] showed SVM enhance texture classiﬁ- cation performance MR8 features.
 (5) Patch Descriptors Varma Zisserman [214] chal- lenged dominant role ﬁlter banks [132, texture analysis, developed simple Patch Descriptor keeping raw pixel intensities square neighborhood form feature vector, illustrated Fig.

 replacing MR8 ﬁlter responses Patch Descriptor texture classiﬁcation [213], Varma Zisserman [214] observed good classiﬁca- tion performance using compact neighborhoods (3× ﬁxed size neighborhood Patch Descriptor leads superior classiﬁcation compared ﬁlter banks same support.
 clear limitation Patch Descriptor is sensitivity change (brightness, rotation etc.).
 (6) Random Projection (RP) Sorted Random Projec- tion (SRP) features Liu Fieguth [109] were inspired theories sparse representation compressed sensing [21,
 Taking advantage sparse nature textured images, small set random features is extracted local image patches projecting local patch feature vectors lower dimensional feature subspace.
 random projection is ﬁxed, distance-preserving embedding capable alleviating curse dimensionality [8,
 random features are embedded BoW perform tex- ture classiﬁcation.
 Patch Descriptors, RP features remain sensitive image rotation.
 improve robustness, Liu et al.
 proposed sorting RP features, illustrated Fig.
 whereby Fig.
 circular neighborhood used derive LBP code: central pixel xc p spaced neighbors circle radius r.
 rings pixel values are sorted, reference orientation, ensuring rotation invariance.
 kinds local features are used, based raw intensities other gradients (radial dif- ferences angular differences).
 Random functions sorted local features are taken obtain SRP features.
 was shown SRP outperformed RP robust texture classiﬁcation [111,
 (7) Local Binary Patterns Ojala al.
 [141] marked be- ginning LBP methodology, followed simpler rotation invariant version Pietik¨ainen al.
 [164], “uniform” patterns reduce feature dimensionality [143].
 Texture representation requires analysis pat- terns local pixel neighborhoods, are de- scribed joint distribution.
 However, stable estimation joint distributions is infeasible, small neighborhoods, combinatorics joint distributions.
 Considering joint distribution: g(xc, x0,


 (1) TextureImage1imN12filN12filN12filNkmeans3DTextonsLMfiltersFilteredimagesConcatenationfilimNNTextureImage2TextureImageLMfiltersLMfiltersTexturesamples(differentilluminationandviewpoint,registered)FeaturevectorsofdimensionalityfilNConcatenatedfeaturevectorofdimensionalityfilNfilNfilNALocalPatchPatchFeatureVectorCircularsort sort Circx sort 0,0x2,{}ciix3,{}ciix1,{}ciixAngular-DiffRadial-DiffRad1,{}ii'Rad2,{}ii'Rad'sort sort sort Rad3,{}ii'Ang1,{}ii'Ang2,{}ii'Ang'sort sort sort Ang3,{}ii'(a)(b)(c)RPMatrix) CircCircyx )RadRady )'AngAngy )' SRPFeaturesSortedPixelValuesSortedPixelDifferencesLocalSRPFeatureExtraction61617180798478827700011111Binary:11110001Decimal:2412Sp1,0()0,0xsxxt­ ® ¯()iicbsxx 102piiib ¦0b1pb1bExample: A Survey Recent Advances Texture Representation Fig.
 LBP representative variants (see text discussion).
 Fig.
 Illustration calculation BIF features.
 gray value xc corresponds center pixel {xn}p−1 n=0 gray values p spaced pixels circle radius r, shown Fig.

 Ojala al.
 [143] argued much information original joint distribution (1) textural characteristics is conveyed joint difference distribution g(x0 − xc, x1 − xc,


 xc).
 size joint histogram was minimized keeping sign difference, illustrated Fig.

 certain degree rotation invariance is achieved cyclic shifts LBPs, i.e., grouping LBPs are ac- rotated versions same underlying pattern.
 dimensionality representation (which grows p) is high, Ojala et al.
 [143] introduced uniformity measure identify p(p uniform LBPs classiﬁed remaining nonuniform LBPs single group.
 changing parameters p r, derive LBP quantization angular space spatial resolution, such multiscale analysis be accomplished combining multiple operators varying r.
 prominent advantages LBP are invariance monotonic gray scale change, low computational complex- ity, ease implementation.
 [143], LBP started receive increasing attention com- puter vision pattern recognition, texture facial analysis, LBP milestones presented Fig.

 Gabor ﬁlters LBP provide complementary information (LBP captures small ﬁne details, Gabor ﬁlters encode appearance information broader range scales), Zhang et al.
 [240] proposed Local Gabor Binary Pattern (LGBP) extracting LBP features im- ages ﬁltered Gabor ﬁlters different scales orientations, enhance representation power, followed subsequent Gabor- LBP approaches [74,
 Additional important LBP vari- ants include LBP-TOP, proposed Zhao Pietik¨ainen [241], milestone using LBP dynamic texture analysis; Lo- cal Ternary Patterns (LTP) Tan Triggs [200], introducing pair thresholds split coding scheme allows encoding pixel similarity; Local Phase Quantization (LPQ) Ojansivu al.
 quantizing Fourier transform phase local neighborhoods is, design, tolerant com- mon types image blurs; Completed LBP (CLBP) Guo al.
 [65], encoding signs magnitudes lo- cal differences; Median Robust Extended LBP (MRELBP) Liu al.
 [113] enjoys high low compu- Fig.
 First-order square symmetric neighborhood WLD computation.
 tational complexity, strong robustness image rotation noise.
 years developments, LBP is simple texture operator, has laid foundation direc- tion research dealing local image video descriptors.
 large number LBP variants have been proposed improve robustness increase discriminative power appli- cability different types problems, interested readers are referred excellent surveys [74,
 recent experimen- tal survey [114] compared recent promising LBP variants deep convolutional networks, evaluating robustness mul- tiple classiﬁcation challenges, best overall performance was obtained MRELBP.
 CNN-based methods are be- ginning dominate, LBP research remains evidenced signiﬁcant recent work [66,
 (8) Basic Image Features (BIF) approach [33] is similar LBP [143], is based predeﬁned codebook learned training.
 shares advantages LBP methods based codebook learning clustering.
 contrast LBP, BIF probes image using Gaussian derivative ﬁlters [62, whereas LBP computes differences pixel neighbors.
 Derivative Gaussians (DtG), consisting ﬁrst second order derivatives Gaussian ﬁl- ter, detect local basic symmetry structure image, allows achieving exact rotation invariance [52].
 BIF feature extraction is summarized Fig.
 pixel image is ﬁltered DtG ﬁlters, labeled class maximiz- ing values.
 simple six-dimensional BIF histogram be used global texture representation, histogram categories produces coarse representation, therefore others (e.g., Crosier Grifﬁn [33]) have performed multiscale analysis calculated joint histograms multiple scales.
 (9) Weber Law Descriptor (WLD) [27] is based fact human perception pattern depends change stimulus original intensity stimulus.
 WLD consists components: differential excitation ori- entation.
 small patch size × shown Fig.
 19962014LBPforFacialTexture(Ahonenetal.)20162010LBP(Ojalaetal.)20062002PairwiseRotationInvarianceCooccurrenceLBP(PRICoLBP)(Qietal.)CompletedLBP(CLBP)(Guoetal.)200820122000LocalTernaryPattern(LTP)(TanandTriggs)1998MedianRobustExtendedLBP(MRELBP)(Liuetal.)RotationInvariantLBP(Pietikainenetal.)2004MultiScaleRotationInvariantUniformLBP(Ojalaetal.)LocalGaborBinaryPattern(LGBP)(Zhangetal.)CenterSymmetricLBP(CSLBP)(Heikkilaetal.)LBP-TOPforDynamicTexture(ZhaoandPietikainen)HOG-LBP(Wangetal.),10s01s20s02s11sijijijsrV x(,)ijijrxc x:ALocalPatch=2002ssO 22200211()4sssJ 2210012,,max()/2,ssyOJOJ­½r°° ®¾r°°¯¿,yO label2xmIfthenyJ (cid:15)label4xmlabel5xmlabel6xmIfthenIfthenIfthen,yO label3xmIfthen2210012,ssy label1xmIfthenClassifyaccordingtothelargestvalueFilterResponsesCompute,OJ Li Liu al.
 differential excitation is relative intensity ratio (cid:32)(cid:80)7 (cid:33) ξ(xc) = arctan i=0 (xi − xc) xc orientation component is derived local gradient orientation θ(xc) = arctan x7 − x3 x5 − x1 Both ξ θ are quantiﬁed histogram, offering global representation.
 use multiple neighborhood sizes sup- ports multiscale generalization.
 Fractal Based Descriptors Fractal Based Descriptors present well-founded alternative dealing scale [126], have be- come popular texture features due lack discrimina- tive power [212].
 Recently, inspired BoW approach, re- searchers revisited fractal method proposed MultiFrac- tal Spectrum (MFS) method [229, invariant view- point changes, nonrigid deformations local afﬁne illumination changes.
 basic MFS method was proposed [229], MFS was deﬁned simple image features, such intensity, gradient Laplacian Gaussian (LoG).
 texture image is transformed n feature maps such intensity, gradient LoG ﬁlter features.
 n feature map, is clustered k clusters (i.e. k codewords) kmeans.
 Then, codeword la- bel map is obtained is decomposed k binary feature maps: pixels assigned codeword i are labeled rest pixels
 binary feature map, box counting algo- rithm [230] is used estimate fractal dimension feature.
 Thus, total k fractal dimension features are computed feature map, forming kD feature vector (referred fractal spec- trum) global representation image.
 Finally, n different feature maps, n fractal spectrum feature vectors are con- catenated MFS feature.
 MFS was improved replacing simple image intensity gradient features advanced ones, such SIFT wavelets, combining ideas MFS LBP.
 downside MFS approach is requires high resolution images obtain stable features.
 Codebook Generation Textures characterization requires analysis repeat- ing patterns, sufﬁce characterize textures pursuit has had important implications series practical problems, such dimensionality reduction, variable decoupling, biological modelling [148,
 extracted set local tex- ture features is versatile, redundant [100].
 be expected set prototype features (i.e. code- words textons) exist be used create global representations textures natural images [100, similar way speech language words, phrases sentences).
 Fig.
 Contrasting ideas BoW, VLAD FV.
 (a) BoW: Counting number local features assigned codeword.
 encodes zero order statistics distribution local descriptors.
 (b) VLAD: Accumulating differences local features assigned codeword.
 (c) FV: Fisher vector extends BOW encoding higher order statistics (ﬁrst second order), retaining information ﬁtting error best ﬁt.
 exist variety methods codebook generation.
 Cer- tain approaches, such LBP [143] BIF [33], have discussed, use predeﬁned codebooks, by- passing codebook learning step.
 approaches requiring learned codebook, kmeans cluster- ing [96, Gaussian Mixture Models (GMM) [28, are popular successful methods.
 GMM modeling considers cluster centers co- variances, describe location spread/shape clus- ters, whereas kmeans clustering cannot capture overlapping distri- butions feature space considers distances cluster centers, generalizations kmeans multiple proto- types cluster allow limitation be relaxed.
 GMM kmeans methods learn codebook unsupervised manner, recent approaches focus building discriminative ones [233,
 addition, signiﬁcant research thread is reconstruc- tion based codebook learning assump- tion natural images admit sparse decomposition re- dundant basis (i.e., dictionary codebook).
 methods focus learning nonparametric redundant dictionaries facilitate sparse representation data minimize reconstruction error data.
 discrimination is primary goal tex- ture classiﬁcation, researchers have proposed construct discrim- inative dictionaries incorporate category speciﬁc in- formation [120,
 codebook is used basis encoding feature vectors, codebook generation is interleaved feature en- coding, described next.
 Feature Encoding illustrated Fig.
 given image is transformed pool local texture features, global image representation is derived feature encoding generated codebook.
 ﬁeld texture classiﬁcation, group used encoding strategies major categories: ◦ Voting based ◦ Fisher Vector based ◦ Reconstruction based
 gradient sample’s likelihoodwrttheparametersofGMM,scaledbytheinversesquarerootoftheFisherinformationmatrix.
 Survey Recent Advances Texture Representation Comprehensive comparisons encoding methods image clas- siﬁcation be found [24,
 Voting based methods.
 intuitive way quantize local feature is assign nearest codeword codebook, referred hard voting [100,
 histogram quan- tized local descriptors be computed counting number local features assigned codeword; histogram con- stitutes baseline BoW representation (as illustrated Fig.
 (a)) other methods improve.
 starts learning codebook {wi}K i=1, kmeans clustering.
 Given set local texture descriptors {xi}N extracted image, encoding representation descriptor x hard voting is (cid:26) i = argminj((cid:107)x − wj(cid:107)) (3) v(i) = otherwise.
 histogram set local descriptors is aggregate encoding vectors {vi}N i=1 sum pooling.
 Hard voting overlooks codeword uncertainty, label image features nonrepre- sentative codewords.
 improvement hard voting scheme, soft voting [2, employs several nearest codewords en- code local feature soft manner, such weight assigned codeword is inverse function distance feature, kernel deﬁnition distance.
 Voting based methods yield histogram representation dimensionality K, number bins histogram.
 Fisher Vector based methods.
 counting number oc- currences codewords, standard BoW histogram representa- tion encodes zeroth order statistics distribution de- scriptors, is rough approximation probabil- ity density distribution local features.
 Fisher vector ex- tends histogram approach encoding additional information distribution local descriptors.
 Based origi- nal FV encoding [158], improved versions were proposed such Improved FV (IFV) [159] VLAD [79].
 brieﬂy describe IFV [159] here, best knowledge achieves best performance texture classiﬁcation [28,
 Theory practical issues regarding FV encoding be found [180].
 IFV encoding learns soft codebook GMM, shown Fig.
 (c).
 IFV encoding local feature is computed as- signing codeword, turn, computing gradient soft assignment respect GMM parameters2.
 IFV encoding dimensionality is D is dimensionality feature space K is number Gaussian mixtures.
 BoW be considered special case FV case gradient computation is restricted mixture weight parame- ters GMM.
 BoW, requires large codebook size, FV be computed smaller codebook (typi- lower computational cost codebook learning step.
 other hand, resulting dimension FV encoding vector (e.g. tens thousands) is sig- higher BoW (thousands), makes unsuitable nonlinear classiﬁers, offers good performance simple linear classiﬁers.
 VLAD encoding scheme proposed J´egou al.
 [79] be thought simpliﬁed version FV, uses Fig.
 Contrasting ideas hard voting, sparse coding, LLC.
 kmeans, GMM, records ﬁrst-order statistics second order.
 particular, records residuals (the difference local features codewords), shown Fig.
 (b).
 Reconstruction based methods.
 Reconstruction based meth- ods aim obtain information-preserving encoding vector allows reconstruction local feature small number codewords.
 Typical methods include sparse coding Local constraint Linear Coding (LLC), are contrasted Fig.

 Sparse coding was proposed [147] model natural image statistics, texture classiﬁcation other problems such image classiﬁcation [232] face recognition [223].
 sparse coding, local feature x be approximated sparse decomposition x ≈ Wv learned codebook W = [w1, w2,


 wK], leveraging sparse nature underlying image [147].
 sparse encoding be solved argminv(cid:107)x − Wv(cid:107)2 (cid:107)v(cid:107)0 ≤ s.
 (4) s.t. s is small integer denoting sparsity level, limiting number non-zero entries v, measured (cid:107)v(cid:107)0.
 Learning re- dundant codebook facilitate sparse representation lo- cal features is key importance sparse coding [1].
 Methods [120, are based learning C class-speciﬁc code- books, texture class approximating local feature using constant sparsity s.
 C different codebooks pro- vides C different reconstruction errors, be used classiﬁcation features.
 class speciﬁc codebooks were optimized reconstruction, signiﬁcant improvements have been shown optimizing discriminative power [38, approach is, however, associated high computational cost, number texture classes C is large.
 Locality constrained linear coding (LLC) [216] projects local descriptor x local linear subspace spanned q codewords codebook size K closest (in Euclidean distance), resulting K dimensional encoding vector en- tries are indices q codewords closest x.
 projection x span q closest code- words is solved following: argminv(cid:107)x − Wv(cid:107)2 + λ (cid:107)x − wi(cid:107)2 v(i)exp (cid:18) K(cid:88) k=1 s.t. (cid:19)2 v(i) = K(cid:88) k=1 derivative weights, is considered make little contribution performance, is removed IFK [159].
 λ is small regularization constant σ adjusts weight decay speed.
 Li Liu al.
 Fig.
 Contrasting classical ﬁltering based texture features, CNN, BoW LBP.
 Feature Pooling Classiﬁcation goal feature pooling [14] is integrate combine coded feature vectors {vi}i, vi ∈ Rd given image ﬁ- nal compact global representation yi is robust im- age transformations noise.
 used pooling methods include sum pooling, average pooling max pooling
 Speciﬁcally, let V = [v1, ..., vN ] ∈ Rd×N denote coded features N locations.
 u denoting row V, u is reduced single scalar operation average, max), reducing V d-dimensional feature vector.
 Realizing pooling entire image disregards information regarding spatial de- pendencies, Lazebnik et al.
 [97] proposed simple spatial pyramid pooling (SPM) scheme partitioning image ﬁne subregions computing histograms local features found subregion average max pooling.
 ﬁnal global representation is concatenation histograms extracted subregions, resulting higher dimensional representation preserves more spatial information [201].
 Given pooled feature, given texture sample be clas- siﬁed.
 Many classiﬁcation approaches are possible [78, al- Nearest Neighbor Classiﬁer (NNC) Support Vector Machines (SVM) are widely-used classiﬁers BoW representation.
 Different distance measures be used, such EMD distance [96], KL divergence widely-used Chi Square distance [109,
 high-dimensional features, IFV VLAD, linear SVM represent better choice [79,
 CNN based Texture Representation A large number CNN-based texture representation methods have been proposed recent years record-breaking image classiﬁcation result [90] achieved
 key success CNNs is ability leverage large labeled datasets learn high quality features.
 Learning CNNs, however, amounts esti- mating millions parameters requires large number annotated images, issue constrains appli- cability CNNs problems limited training data.
 key discovery, regard, was CNN features pretrained large datasets were found transfer many other problems, including texture analysis, modest adaption effort [25,
 current literature tex- ture classiﬁcation includes examples employing pretrained generic CNN models performing ﬁnetuning speciﬁc texture classiﬁcation tasks.
 survey classify CNN based texture representa- tion methods categories, form basis following sections: using pretrained generic CNN models, ◦ using ﬁnetuned CNN models, ◦ using handcrafted deep convolutional networks.
 representations have had widespread inﬂuence image understanding; representative examples are given Table
 Using Pretrained Generic CNN Models Given behavior CNN transfer, success pretrained CNN models lies feature extraction encoding steps.
 Similar Section describe ﬁrst used networks pretraining feature extraction process.
 (1) Popular Generic CNN Models serve good choices extracting features, including AlexNet [90], VGGNet [192], GoogleNet [198], ResNet [72] DenseNet [75].
 networks, AlexNet was proposed earliest, general others are deeper more complex.
 full review net- works is scope paper, refer readers (d) Reformulation LBP using convolutional filters(a) Traditional Multiscale Multiorientation Filtering(c) Random Projections BoW based Texture Representation(b) Basic module Standard DCNN Feature Maps Previous LayerMaximunAverageGaussianReLUSigmoidConvolutionNonlinearityn filtersPooling RSVMInput imageLabelFilterResponsesFeature VectorsAverageGaussianModulusSquaringSigmoidConvolutionNonlinearityClassifierFeatureMapsn filtersLocal Energy EstimatePooling Local Energy Function11111111Histogram poolingInput imageFilterResponsesFeature vectorHeavisideStepFunctionn binary fIlters (multiorientation)NonlinearityFeatureMapsn filtersWeighted Feature MapWeightingConvolutionClassifierSVMOrderlessPoolingSVMInput imageFilter ResponsesCodebookEncodingn random projections size MapRandom ProjectionsOrderlessPooling-1-1-1-111111-1-1-1-1111-1-1-11111-1-1-111111111-1-1111111-1-1-1-11111Histogram PoolingFeature vectorFeature Maps Next Layer-1-1-1-1-1-1-1 A Survey Recent Advances Texture Representation Table CNN based texture representation Approach Using Pretrained Generic CNN Models [30] (Section Traditional feature encoding pooling; New pooling such bilinear pooling [104, LFV Highlights • AlexNet [90] • VGGM [25, • VGGVD [192] • GoogleNet [198] • ResNet [72] Using Finetuned CNN Models (Section • TCNN [6] • BCNN [106, • Compact BCNN [54] • FASON [39] • NetVLAD [7] • DeepTEN [237] [196] Achieved image classiﬁcation result ImageNet; historical turning point feature representation handcrafted CNN.
 Similar complexity AlexNet, better texture classiﬁcation performance.
 Much deeper Much Larger model size AlexNet VGGM; Much better texture recognition performance AlexNet VGGM.
 Much deeper AlexNet; Small pretrained model size; used texture classiﬁcation.
 deeper VGGVD; Smaller model size (ResNet AlexNet.
 End-to-end learning Using global average pooling; Combining outputs multiple CONV layers.
 Introducing novel orderless bilinear feature pooling method; Generalizing Fisher Vector VLAD; Good representation high feature dimensionality.
 Adopting Random Maclaurin Projection Tensor Sketch Projection reduce dimensionality bilinear features (e.g. (5122) Maintain similar performance BCNN; Combining ideas TCNN [6] Compact BCNN [54].
 Plugging VLAD layer CNN network last CONV layer.
 Similar NetVLAD [7], integrating encoding layer top CONV Generalizing orderless pooling methods such VLAD FV CNN trained end end.
 Texture Speciﬁc Deep Convolutional Models (Section • ScatNet [19] • PCANet [23] Use Gabor wavelets comvolution; Mathematical interpretation CNNs; Features being stable deformations preserving high frequency information; Inspired ScatNet [19], using PCA ﬁlters replace Gabor wavelets;Using LBP histogramming feature pooling; local invariance.
 original papers [72, excellent surveys [11, additional details.
 Brieﬂy, shown Fig.
 (b), typical CNN applies following opera- tions:
 Convolution number linear
 Nonlinearities, such sigmoid rectiﬁcation,
 Local pooling subsampling.
 operations are related traditional ﬁlter bank methods used texture analysis [170], shown Fig.
 (a), key differences CNN ﬁlters are learned di- data handcrafted, CNNs have hi- erarchical architecture learning abstract levels rep- resentation.
 operations are related RP approach (Fig.
 (c)) LBP (Fig.
 (d)).
 Several large-scale image datasets are used CNN pretraining.
 ImageNet dataset, classes images [178] is most used, fur- ther source scene-centric dataset MITPlaces [243].
 Comprehensive evaluations feature transfer effect CNNs purpose texture classiﬁcation have been conducted [28, following critical insights.
 model transfer, features extracted different layers exhibit different classiﬁcation performance.
 Experiments conﬁrm fully-connected layers CNN, role is classiﬁcation, tend exhibit worse generalization ability trans- ferability, need retraining ﬁnetuning transfer target.
 contrast convolutional layers, feature extractors, coarser convolutional layers acting abstract features, transfer well.
 is, convolutional descriptors are committed speciﬁc dataset connected descriptors.
 re- sult, source training set is relevant classiﬁcation accuracy different datasets, similarity source target plays critical role using pretrained CNN model [10].
 Finally, was found deeper models transfer better, deepest convolutional descriptors give best perfor- mance, superior fully-connected descriptors, proper encoding techniques are employed (such FVCNN←CNN fea- tures Fisher Vector encoder).
 (2) Feature Extraction A CNN be viewed composi- tion fL ◦ ··· ◦ f2 ◦ f1 L layers, output layer Xl = (fl ◦ ··· ◦ f2 ◦ f1)(I) consists Dl feature maps size W l × H l.
 Dl responses spatial location form Dl dimensional feature vector.
 network is called convolutional layers are implemented (nonlinear) ﬁlters, sense act input.
 bottom top layers, image undergoes convolution (nonlinear) ﬁlters, receptive ﬁeld ﬁlters number feature channels increases, size feature maps decreases.
 last several layers typical CNN are called Connected (FC) because, seen ﬁlters, sup- port is same size input Xl−1, lack locality.
 contrast, earlier layers act be referred convolutional.
 straightforward approach CNN-based texture clas- siﬁcation is extract descriptor connected lay- ers network [29, FC6 FC7 descriptors AlexNet [90].
 connected layers are pretrained discrim- inatively, be advantage disadvantage, de- pending information captured be trans- ferred domain interest [25,
 fully-connected descriptors have global receptive ﬁeld are viewed global features suitable classiﬁcation SVM classiﬁer.
 contrast, convolutional layers CNN be used ﬁlter banks extract local features [29,
 Compared global fully-connected descriptors, lower level convolutional descriptors are robust image transformations such translation Li Liu al.
 occlusion.
 features are extracted output convolutional layer, linear ﬁlters (excluding ReLU max pooling, any), are combined traditional encoders global representation.
 last convolu- tional layer VGGVD (very deep layers) [192] yields set descriptor vectors; [28, types CNNs AlexNet [90], DeCAF [43], VGGM [25] VGGVD [192] were considered feature extraction.
 (3) Feature Encoding Pooling A set features extracted convolutional connected layers resembles set texture features described Section traditional fea- ture encoding methods discussed Section be employed.
 [30], Cimpoi al.
 evaluated several encoders, i.e. stan- dard BoW [100], LLC [216], VLAD [79] IFV [159] (reviewed Section CNN features, showed best per- formance is achieved IFV.
 FVCNN descriptors be com- pressed same dimensionality FC descriptors PCA preserving classiﬁcation performance [30].
 Later, Song al.
 [196] proposed neural network transform FVCNN descrip- tors lower dimensional representation.
 shown Fig.
 transferred FVCNN (LFVCNN) descriptors are obtained passing dimensional FVCNN descriptors images multilayer neural network consisting connected, l2 normalization layers, ReLU layers.
 Recently, Gatys al.
 [56] showed Gram rep- resentations extracted various layers VGGNet [192] be inverted texture synthesis.
 work Gatys al.
 ignited renewed interest texture synthesis [207].
 Notably, Gram matrix representation used approach is identical bilinear pooling CNN features Lin al.
 [106], were proved be good texture recognition [104].
 traditional encoders introduced Section bilinear feature pooling is orderless representation input image hence is suitable modeling textures.
 discussed previously, ﬁeld descriptors Xl ∈ RW l×H l×Dl be obtained given layer l CNN.
 set features {xl indexed location i be extracted.
 Bilinear CNN (BCNN) de- scriptors are obtained computing outer product xl i)T feature xl i itself, are reordered feature vectors, are pooled sum obtain ﬁnal global repre- sentation.
 dimension bilinear descriptor is (Dl)2, is high (e.g.
 BCNN descriptors VGGNet [192] were evaluated SVM classiﬁer texture recognition.
 was shown [104, texture classiﬁ- cation performance BCNN FVCNN were identi- cal, indicating bilinear pooling is good Fisher vector pooling texture recognition.
 was found [104, BCNN descriptor last convolutional layer performed best, agreement [30].
 i ∈ RDl}W lH l i=1 i(xl Using Finetuned CNN Models Pretrained CNN models, discussed Section have achieved impressive performance texture recognition, training methods is multistage pipeline involves feature extrac- tion, codebook generation, feature encoding classiﬁer training.
 Consequently, methods cannot take advantage utilizing full capability neural networks representation learning.
 Gen- ﬁnetuning CNN models task-speciﬁc training datasets (or learning scratch large-scale task-speciﬁc datasets are available) is expected improve strong performance achieved pretrained CNN models [25,
 using ﬁne- tuned CNN model, global image representation is gen- erated end end manner; is, network produce ﬁnal visual representation additional explicit encoding pooling steps, illustrated Fig.

 ﬁnetuning CNN, last connected layer is modiﬁed have B nodes corre- sponding number classes target dataset.
 na- ture datasets used ﬁnetuning is important learning dis- criminative CNN features.
 pretrained CNN model is capable discriminating images different objects scene classes, be effective discerning difference different textures (material types) image ImageNet contain different types textures (materials).
 size dataset used ﬁnetuning matters well, small datasets be inad- equate complete learning.
 best knowledge, properties texture rep- resentation ﬁnetuning large-scale CNN VGGNet [192] training scratch using texture dataset has been explored texture analysis, due fact large-scale texture dataset same size ImageNet [178] MITPlaces [243] does exist.
 Most existing texture datasets are small, discussed Section according [6, ﬁnetun- ing VGGNet [192] AlexNet [90] existing texture datasets leads negligible performance difference.
 shown Fig.
 (a), typical CNN VGGNet [192], output last convolutional layer is reshaped single feature vector (spa- sensitive) fed connected layers (i.e., order sen- sitive pooling).
 global spatial information is necessary an- alyzing global shapes objects, has been realized is great importance analyzing textures due need orderless representation.
 FVCNN descriptor shows higher recognition performance FCCNN, pretrained VGGVD model is ﬁnetuned texture dataset (i.e. ﬁnetuned FCCNN descriptor) [30,
 Therefore, orderless feature pooling output convolution layer is desirable end end learning.
 addition, orderless pooling doesn’t require input image be ﬁxed size, motivating series innovations designing novel CNN architectures texture recognition [6,
 Texture CNN (TCNN) based AlexNet, illustrated Fig.
 (b), was developed [6].
 utilizes global average pooling transform ﬁeld descriptor Xl ∈ RW l×H l×Dl given convolutional layer l CNN Dl dimension vector is connected connected layer.
 TCNN has fewer pa- rameters lower complexity AlexNet.
 addition, Andrea- rczyk Whelan [6] proposed fuse global average pooled vector intermediate convolutional layer last convolutional layer concatenation introduced connected layers, combination resembles hypercolumn feature developed [69].
 Andrearczyk Whelan [6] observed ﬁnetuning network was pretrained texture-centric dataset achieves results other texture datasets compared network pretrained object-centric dataset same A Survey Recent Advances Texture Representation Fig.
 Comparison Fine Tuned CNNs: (a) standard CNN, (b) TCNN [6], (c) BCNN [107], (d) Compact Bilinear Pooling [54], (e) FASON [39].
 size, size dataset network is pre- trained ﬁnetuned inﬂuences performance ﬁnetuning.
 observations suggest large texture dataset bring signiﬁcant contribution CNNs ap- plied texture analysis.
 classiﬁcation performance was inferior FVCNN.
 Similar NetVLAD [7], Deep Texture Encoding Network (DeepTEN) was introduced [237] integrating encoding layer top convolutional layers, generalizing pooling methods such VLAD FV CNN trained end end.
 BCNN [107], shown Fig.
 (c), Lin et al.
 proposed replace connected layers orderless bilinear pool- ing layer, was discussed Section method was applied texture classiﬁcation [104] obtained better results FVCNN being trained end end, representational power bilinear features comes cost high dimensional feature representations, in- duce substantial computational burdens require large amounts training data, motivating several improvements BCNN.
 Gao al.
 [54] proposed compact bilinear pooling, shown Fig.
 (d), utilizes Random Maclaurin Projection Tensor Sketch Projection reduce dimensionality bilinear representations maintaining similar performance full BCNN fea- ture [107] reduction number learned parame- ters.
 combine ideas [6] [54], Dai al.
 [39] proposed effective fusion network called FASON (First Second Or- der information fusion Network) combines ﬁrst second order information ﬂow, illustrated Fig.
 (e).
 FASON, global average pooled vector compact bilinear pooled fea- ture convolutional layer was referred ﬁrst second or- der statistics, respectively.
 types features were gener- ated different convolutional layers concatenated form single feature vector was connected connected softmax layer end end training.
 [87], Kong Fowlkes proposed represent bilinear features matrix applied low rank bilinear classiﬁer.
 resulting classiﬁer be eval- uated computing bilinear feature map allows large reduction computational time decreasing effective number parameters be learned.
 BCNN model [107] variants [39, are successful texture classiﬁcation ﬁne-grained image recognition.
 are works attempting integrate CNN VLAD FV pooling end end manner.
 NetVLAD network was proposed plugging VLAD-like layer CNN network last convolutional layer allows training end end.
 model was designed place recognition, applied texture classiﬁcation Song al.
 [196] was found Using Handcrafted Deep Convolutional Networks addition CNN based methods reviewed Sections handcrafted deep convolutional networks [19, deserve attention.
 Recall standard CNN architecture (as shown Fig.
 (b)) consists multiple trainable building blocks stacked top followed supervised classiﬁer.
 block consists convolutional ﬁlter bank layer, nonlinear layer, feature pooling layer.
 Similar CNN architecture, Bruna Mallat [19] proposed inﬂu- ential Scattering convolution Network (ScatNet), illustrated Fig.

 key difference CNN, convolutional ﬁlters are learned data, is convolutional ﬁlters ScatNet are predetermined are wavelet ﬁlters, such Gabor Haar wavelets, learning is required.
 Moreover, ScatNet cannot go CNN; Bruna Mallat [19] sug- gested convolutional layers, energy third layer scattering coefﬁcients is negligible.
 Speciﬁcally, be seen Fig.
 ScatNet cascades wavelet transform convolutions modulus nonlinearity averaging poolers.
 is shown [19] ScatNet computes translation-invariant image representations are stable deformations preserve high frequency in- formation recognition.
 shown Fig.
 average pooled feature vector stage is concatenated form global feature representation image, is input simple PCA classiﬁer recognition.
 surprisingly, such pre- ﬁxed network has demonstrated high performance texture recognition [19,
 downside ScatNet is feature extraction stage is time consuming, di- mensionality global representation feature is low (several hundreds).
 ScatNet has been extended achieve rotation scale invariance [190, applied other problems texture such object recognition [153].
 Importantly, mathematical analysis ScatNet explains important properties FC(4096)CONV(Dfilters)AveragePoolingFC(4096)ConcatenatingCONV(Cfilters)AveragePooling(b)TCNN(e)FASONSoftMaxCONV(Dfilters)CompactBilinearPoolingCONV(Cfilters)AveragePoolingCompactBilinearPoolingAveragePoolingSoftMaxCONV(Dfilters)CNN1CONV(Dfilters)CNN2BilinearPooling(c)BCNN(d)CompactBCNNCNN1andCNN2canbesameordifferentSoftMaxDim:C+DDim:D2SoftMaxRandomMatrix1(D(cid:104)D1)RandomMatrix2(D×D1)OuterProductElementwiseMultiplicationCONV(Dfilters)CNN1CONV(Dfilters)CNN2Dim:D1CompactBilinearPoolingDim:C+D+D1+D1(a)StandardCNNFC(4096)CONV(Dfilters)VectorizingFC(4096)CONV(Cfilters)SoftMaxDim:HWDOutput:H(cid:104)W(cid:104)D Li Liu al.
 Attribute-Based Texture Representation recent years, recognition texture categories has been ex- studied has shown substantial progress, thanks texture representations reviewed Sections
 De- spite rapid progress, development deep learning techniques, remain reaching goal com- prehensive scene understanding [89].
 traditional goal was recognize texture categories based perceptual dif- ferences material types, textures have other properties, shown Fig.
 speak banded shirt, striped zebra, striped tiger.
 Here, banded striped are referred visual texture attributes describe texture patterns using human-interpretable semantic words.
 texture attributes, textures shown Fig.
 (d) be described braided, falling single category Describable Textures Dataset (DTD) database [28].
 study visual texture attributes [13, was vated signiﬁcant interest raised visual attributes [49,
 Visual attributes allow describing objects signif- greater detail category label are impor- tant towards reaching goal comprehensive scene understand- ing [89], support important applications such de- tailed image search, question answering, robotic interactions.
 Texture attributes are important component visual attributes, objects are characterized pattern.
 support advanced image search applications, such spe- ciﬁc queries image search engines (e.g. striped skirt, skirt).
 investigation texture attributes de- tailed semantic texture description offers signiﬁcant opportunity close semantic gap texture modeling support appli- cations require ﬁne grained texture description.
 Nevertheless, are several papers [13, investigating texture attributes far, is systematic study attempted.
 are essential issues studying texture attribute based representation:
 identiﬁcation universal texture attribute vocabulary describe wide range textures;
 establishment benchmark texture dataset, annotated semantic attributes;
 reliable estimation texture attributes images, based low level texture representations, such methods re- viewed Sections
 Tamura al.
 [199] proposed set attributes describing textures: coarseness, contrast, directionality, line-likeness, regular- ity roughness.
 Amadasun King [5] reﬁned idea ﬁve attributes coarseness, contrast, business, complexity, strength, were tuned general applicability low compu- tational costs.
 Later, Bhushan et al.
 [12] studied texture attributes perspective psychology, asking subjects cluster collection texture adjectives according similarity iden- tiﬁed major clusters.
 Recently, inspired work [12, Matthews al.
 attempted enrich texture analysis semantic at- tributes.
 identiﬁed used texture attributes Fig.
 transferred Fisher Vector (LFV): use neural networks dimensionality reduction FVCNN descriptor.
 Fig.
 Illustration similar handcrafted deep convolutional networks: ScatNet [19] PCANet [23].
 CNN architectures, is few works provides detailed theoretical understanding CNNs. Inspired ScatNet, Chan et al.
 [23] proposed sim- ple convolutional network called PCANet.
 contrasted Fig.
 key difference is PCANet uses PCA ﬁlters are learned training data, predeﬁned Gabor wavelets.
 addition, PCANet uses LBP encoding [143] histogramming feature pooling.
 simple variations PCANet, RandNet LDANet, were introduced [23], sharing same topol- ogy PCANet, convolutional ﬁlters are random ﬁlters [109] learned Linear Discriminant Analysis (LDA).
 Compared ScatNet, feature extraction PCANet is much faster, according [114], PCANet representation does have local invariance texture classiﬁcation perfor- mance is good.
 Input:FVCNNDescriptorFCLayerl2NormLayerReLULayerFCLayerl2NormLayerReLULayerConcatenateOutput:LFVCNNDescriptorLocallytransferredFisherVector(LFV):Use2KNeuralNetworksforDimensionalityReductionInputImageAGaborWavelet(FrequencyDomain)ConvolutionConvolutionConvolutionGaussianSmoothingFeaturePoolingGlobalAveragePoolingFeaturePoolingFeaturePoolingFeaturePoolingConcatenatedVectorConvConvConvConvGaborWaveletsModulus(Nonlinearity)ConvolutionPCAfiltersConvolutionLBPPoolingFeaturePoolingScatteringConvolutionalNetwork(ScatNet)PCANetDeepNetworkArchitecture Survey Recent Advances Texture Representation Texture Datasets Performance Texture Datasets history computer vision research, datasets have played essential role.
 serve source training data means evaluating algorithms, stimulate ﬂood research interest drive research new challenging directions.
 is evidenced fact re- cent large scale ImageNet dataset [178] has enabled breakthroughs visual recognition research using new class deep learning algorithms [90].
 big data era, becomes urgent enrich texture datasets promote future research.
 are many texture datasets ﬁelds medical analysis, robotics, recogni- tion, analysis.
 section, discuss existing natural tex- ture image datasets have been released used research community texture classiﬁcation, summarized Table
 Brodatz texture database [17], derived Brodatz Album [18], is earliest, used fa- mous texture database.
 has large number classes (111), class having image.
 Many texture repre- sentation approaches exploit Brodatz database evaluations [86, most cases entire database is utilized, recent studies [58,
 database has been criticized lack intraclass variations such scale, rotation, perspective illumi- nation.
 Vision Texture Database (VisTex) [115, is early well-known database.
 Built MIT Multimedia Lab, has classes textures, image.
 VisTex textures are imaged natural lighting conditions, have ex- tra visual cues such shadows, lighting, depth, perspective, appearance real-world images.
 VisTex is used texture synthesis segmentation, image-level tex- ture classiﬁcation.
 texture recognition has evolved classifying real world textures large intraclass variations due changes camera pose illumination, leading development number benchmark texture datasets based various real-world material instances.
 famous used is Columbia-Utrecht Reﬂectance Texture (CUReT) dataset [40], different material textures taken varying image conditions controlled lab environment.
 materials database has been imaged different viewing illumination conditions.
 effects specularities, interreﬂec- tions, shadowing, other surface normal variations are shown Fig.
 (a), impact is highlighted.
 CUReT is considerable improvement Brodatz, such ef- fects are absent.
 Based original CUReT, Varma Zisser- man [213] built subset texture classiﬁcation, became used benchmark assess classiﬁcation performance.
 CUReT has limitations signiﬁcant scale change most textures limited in-plane rotation.
 Thus, discrimina- tive texture feature rotational invariance achieve high recognition rates [19].
 Noticing limited scale invariance CUReT, researchers Royal Institute Technology (KTH) introduced dataset Fig.
 Objects rich textures daily life.
 Visual texture attributes mesh, spotted, striated, spotted striped provide detailed vivid de- scriptions objects.
 selecting single adjective eleven clus- ters identiﬁed Bhushan al.
 [12].
 Then, eleven tex- ture released available human-provided labeling classes texture Outex database [142].
 texture image, asking subject sim- ply identifying presence absence texture attribute, Matthews al.
 [131] proposed framework pairwise compar- ison, subject was shown texture images simulta- prompted choose image exhibiting more attribute, motivated use relative attributes [155].
 approach allows Outex textures be placed con- tinuum according exhibit attribute.
 order bridge semantic gap Matthews et al.
 [131] proposed method similar soft margin SVM estimate level expression attribute (a quality referred at- tribute’s strength) texture image based texture features LBP [143], beneﬁts texture attribute based repre- sentation were demonstrated texture retrieval.
 performing screening process adjectives iden- tiﬁed Bhushan al.
 [12], Cimpoi et al.
 [28] obtained texture attribute vocabulary English adjectives, including words such banded, cobwebbed, freckled, knitted zigzagged.
 col- lected DTD internet, providing example images attribute.
 DTD dataset serves good start seek representation recognizing texture attributes images.
 furthermore provide comparison BoW- CNN-based tex- ture representation methods attribute estimation, demonstrat- ing texture attributes are excellent texture descriptors, trans- ferring datasets.
 Bormann al.
 [13] introduced set seventeen human comprehensible attributes color texture char- acterization.
 attributes include color properties ten structural properties.
 collected new database named Robotics Domain Attributes Database (RDAD) indoor ser- vice robotics context.
 RDAD dataset contains images ev- eryday object surface textures is annotated seventeen attributes respective object surface class.
 compared ﬁve low level texture representation approaches at- tribute prediction, found objects be described seventeen attributes.
 Clearly, attributes are suited precise description different object tex- ture classes deserves further attention.
 Blemished, bumpy, lined, marbled, random, repetitive, speckled, spiralled, webbed, woven, wrinkled Table Summary commonly-used texture databases.
 References Total Images Texture Classes Image Size Texture Dataset Brodatz VisTex CUReT Outex KTHTIPS UIUC No. [18] [40] [142] [70, [96] KTHTIPS2a [22, KTHTIPS2b [22, UMD ALOT RawFooT [229] [20] [37] FMD DreTex UBO2014 OpenSurfaces DTD MINC MINC2500 GTOS LFMD RDAD [183, [152] [220] [9] [28] [10] [10] [231] [217] [13] Wild Imaging Environment Controlled Gray Color Gray Color Color Controlled Color Controlled Color Controlled Gray Color Controlled Color Controlled × × × × × × × × × Gray Wild Wild Wild × × × × × Color Controlled Color Controlled Color Color Controlled Color Synthesis Color Color Color Color unﬁxed unﬁxed unﬁxed Wild Wild Wild Wild × Color × Controlled × Color Uncontrolled × Color Uncontrolled Illumination Changes Rotation Changes Viewpoint Changes Scale Changes Small Small Small Small Small Small Small Li Liu al.
 Download Link [17] [215] [36] [151] [91] [206] [91] [91] [208] [4] [172] [50] [45] [205] [149] [46] [136] [136] Image Instances Categories Year Content Instances Objects Instances Objects Instances Materials Instances Materials Instances Materials Materials Instances Materials Categories Materials Categories Instances Objects Instances Materials Materials Instances Materials Categories Materials Instances Materials Categories Materials Attributes Categories Materials Materials Clutter Clutter Clutter Instances Materials Materials Categories Objects Instances [63] [102] [176] Fig.
 Image examples category KTHTIPS2.
 called Textures varying Illumination, Pose, Scale” (KTHTIPS) [70, imaging ten CUReT materials different illuminations, different poses, different dis- tances, fewer settings lighting view- ing angle CUReT.
 KTHTIPS was created extend CUReT directions: (i) providing variations scale, (ii) imaging different samples CUReT materials different set- tings.
 supports study recognizing different samples CUReT materials; instance, does training CUReT enable good recognition performance KTHTIPS?
 pose varia- tions, rotation variations KTHTIPS is small.
 details KTHTIPS images acquisition is described [154].
 Experiments Brodatz VisTex used different nonover- lapping subregions same image training testing; experiments CUReT KTHTIPS used different subsets images imaged identical sample training test- ing.
 KTHTIPS2 was ﬁrst datasets offer considerable variation class.
 groups textures in- stance, type materials (e.g., wool).
 is built KTHTIPS provides considerable extension imaging physical, planar samples eleven materials [154].
 are versions KTHTIPS2 database, i.e. KTHTIPS2a KTHTIPS2b, latter having more images.
 Oulu Texture (Outex) database was collected Ma- chine Vision Group University Oulu [142].
 has largest number different texture classes (320), class having images photographed illuminations rotation angles, limited scale variation.
 Based Ou- tex, series benchmark test suites were derived evalua- tions texture classiﬁcation segmentation algorithms [142].
 them, benchmark datasets Outex TC00010 Ou- tex TC00012 [143] designated testing rotation illumination invariance, appear papers.
 UIUC (University Illinois Urbana-Champaign) dataset collected Lazebnik al.
 [96] contains texture classes, class having uncalibrated, unregistered images.
 has sig- niﬁcant variations scale viewpoint nonrigid defor- mations (see Fig.
 (b)), has less severe illumination variations CUReT.
 challenges database are are few sample images class, signiﬁcant variations classes.
 UIUC improves CUReT terms large intraclass variations, is smaller CUReT number classes number images class.
 UMD (University Maryland) dataset [229] contains texture classes, class having uncalibrated, unregistered high- resolution images.
 Similar UIUC, has signiﬁcant viewpoint scale variations uncontrolled illumination conditions.
 textures are imaged variable truncation, viewpoint, illu- mination, UIUC UMD have stimulated creation texture representations are invariant signiﬁcant viewpoint changes.
 Amsterdam Library Textures (ALOT) database [20] con- sists texture classes, class having images.
 was collected controlled lab environment different A Survey Recent Advances Texture Representation lighting conditions.
 has larger number tex- ture classes UIUC UMD, has little scale, rotation viewpoint variations, is difﬁcult recognize.
 provides versions different resolutions.
 Drexel Texture (DreTex) dataset [152] contains different textures, was imaged times different (known) illumination directions, multiple distances, different in-plane plane rotations.
 contains stochas- tic regular textures.
 Raw Food Texture database (RawFooT), has been spe- designed investigate robustness texture represen- tation methods respect variations lighting conditions [37].
 consists texture classes raw food, class having images acquired lighting conditions differ light direction, illuminant color, intensity, combination factors.
 has variations rotation, viewpoint scale.
 Due rapid progress texture representation performance many methods datasets described are close saturation, KTHTIPS2b being exception due increased complexity.
 However, most datasets introduced (excluding KTHTIPS2) make simplifying assumption tex- tures ﬁll images, often, is limited intraclass variability, due single limited number instances, captured controlled scale, viewpoint illumination.
 recent years, re- searchers have set sights more complex recognition prob- lems textures appear poor viewing conditions, low resolution, realistic cluttered backgrounds.
 Flickr Mate- rial Database (FMD) [183, was built address limitations, collecting many different object instances Internet grouped different material Categories, exam- ples shown Fig.
 (e).
 FMD [183] focuses identify- ing materials such plastic, ﬁber glass.
 MIT re- searchers pointed humans identify materials, automatic identiﬁcation materials computers is challenging [185].
 is worth mentioning mate- rial recognition employs texture features, is fact differ- ent texture identiﬁcation, object recognition scene recog- nition [185].
 limitations FMD dataset is size is small, containing material classes images class.
 UBO2014 dataset [220] contains material categories, having different physical instances.
 material instance was measured full bidirectional texture function images (a sampling viewing lighting di- rections), resulting total more synthesized images.
 synthesized material dataset allows classifying mate- rials complex real-world scenarios.
 Motivated recent interests visual attributes [49, Cimpoi al.
 [28] identiﬁed vocabulary texture at- tributes based seminal work Bhusan al.
 [12] stud- ied relationship used English words perceptual properties textures, identifying set words sufﬁcient describing wide variety texture patterns.
 human-interpretable texture attributes characterize tex- tures, shown Fig.

 Based texture attributes, introduced corresponding DTD dataset consisting texture images attribute, downloading images Internet Fig.
 Describing textures goal DTD is understand generate human interpretable descriptions such ex- amples above.
 Fig.
 Examples material segments OpenSurfaces dataset.
 effort support real world applications.
 large intr- aclass variations DTD are different traditional texture datasets CUReT, UIUC UMD, sense im- ages shown Fig.
 belong braided class, whereas traditional sense textures belong different texture categories.
 FMD, Bell et al.
 [9] released OpenSurfaces (OS) has images consumer photographs, containing number high-quality texture material seg- ments.
 Images OS have real-world context, contrast prior databases CUReT, KTHTIPS FMD, image texture category texture ﬁlls whole image.
 OS has segments (as shown shown Fig.
 support variety applications.
 Many segments are annotated material viewpoint, re- ﬂectance, object names scene class, lacks ex- haustive annotations.
 number segments material category be unbalanced OS; example, wood class has segments water class has
 Nevertheless, OS has large number categories samples prior databases.
 Using OS dataset seed, Bell et al.
 [10] introduced large material dataset named Materials Context Database (MINC) material recognition segmentation wild, image samples shown Fig.

 MINC has total material samples different material categories, wood category having largest number samples (564,891 images) wallpaper category having smallest number samples (14,954 images).
 MINC is diverse, has more sam- ples category, is larger previous datasets.
 Bell al.
 concluded large well-sampled dataset such MINC is key real-world material recognition segmentation.
 Concurrent work Bell al.
 [10], Cimpoi et al.
 derived new dataset OS conduct study material describable texture attribute recognition clutter.
 segments OS have complete set annotations, Cimpoi et al.
 selected subset segments annotated material names, removing material classes containing fewer seg- cracked, pittedbraided, knitted, woven, interlacedhoneycombed, flecked, meshedFabricBrickFood Li Liu al.
 have rotation gray scale invariances, give perfect accuracies, revealing limitations CNN based descriptors be- ing sensitive image degradations.
 usual advantages CNN based methods, is cost high computational complexity memory requirements.
 believe traditional texture descriptors, efﬁcient LBP [143] robust variants such MRELBP [114, have merits cases real time computation is priority robustness image degra- dations is needed [114].
 commonly-used tex- ture benchmarks Table are small compared large scale image datasets, such ImageNet [178] MITPlaces [243] com- used CNN training, texture datasets are able provide sufﬁcient training data take advantage CNN learning.
 be seen Table highest classiﬁ- cation scores Outex TC10, Outex TC12, CUReT, KTHTIPS, UIUC, UMD ALOT are excess few texture representation approaches achieve more accuracy datasets.
 inﬂuential work Cimpoi al.
 reported perfect classiﬁca- tion accuracies pretrained CNN features texture classiﬁ- cation, subsequent representative CNN based approaches have reported results datasets performance is saturated, datasets are large allow ﬁnetuning obtain improved results.
 FMD, DTD KTHTIPS2b are challenging other texture datasets listed Table is shown Fig.
 comparing texture category sep- aration UIUC FMD, challenging datasets appear recent works evaluating texture rep- resentation methods.
 However, IFV encoding VG- GVD descriptors [30], progress datasets has been slow, incremental improvements accuracy efﬁciency obtained building more complex deeper CNN architectures.
 be observed Table LBP type methods (LBP [143], MRELBP [113] BIF [33]) adopt predeﬁned codebook have efﬁcient feature extraction step remaining methods listed Table
 BoW based methods require codebook learning, codebook learn- ing, feature encoding pooling process are similar, distin- guishing factors are computation feature dimensionality local texture descriptor.
 commonly-used local tex- ture descriptors, approaches ﬁrst detecting local regions interest followed local descriptors, such SIFT, RIFT SPIN [96, are slowest have high dimensionality.
 CNN based methods developed [28, CNN feature extraction is performed multiple scaled ver- sions original texture image, requires more computa- tional time.
 general, CNN pretraining ﬁnetuning is efﬁcient, whereas CNN model training is time consuming.
 [114], Scat- Net is expensive feature extraction stage, has medium feature dimensionality.
 Finally, fea- ture classiﬁcation stage linear SVM is ker- nel
 Liu et al.
 [114] proposed extensive benchmark measuring robustness number texture features different classiﬁcation challenges, including changes rotation, scale, illumination, number classes, different types image degradation, computational complexity.
 Fourteen datasets Fig.
 Image samples MINC database.
 ﬁrst row are images food category, second row are images foliage.
 ments.
 Cimpoi al.
 annotated dataset eleven tex- ture attributes, selected OS segments do trigger sufﬁcient frequency attributes (as discussed Section
 Similarly, Robotics Domain Attributes Database (RDAD) [13] contains categories everyday indoor object surface tex- tures labeled set seventeen texture attributes, collected addresses target domain everyday objects surfaces service robot encounter.
 Wang al.
 [217] introduced new light-ﬁeld dataset ma- terials, called Light-Field Material Database (LFMD).
 con- tains twelve material categories, images taken Lytro Illum camera.
 light-ﬁelds capture multiple view- points single shot, contain reﬂectance infor- mation, be helpful material recognition.
 goal LFMD is investigate light-ﬁeld information im- proves performance material recognition.
 Compared FMD [183], LFMD has more material classes (fur sky) are common natural scenes.
 LFMD is important step towards light-ﬁeld based material recognition.
 Finally, Xue al.
 [231] built material database named Ground Terrain Outdoor Scenes (GTOS) study use spa- tial angular reﬂectance information outdoor ground terrain material recognition.
 consists images cover- ing classes outdoor ground terrain varying weather lighting conditions.
 material classes have instances instance is imaged viewing directions.
 viewpoints dataset are sampled angle space.
 Performance Table presents performance summary representative meth- ods applied popular benchmark texture datasets.
 is clear major improvements have come powerful local texture descriptors such MRELBP [114, ScatNet [19] CNN- based descriptors [30] advanced feature encoding meth- ods IFV [159].
 advance CNN architectures, CNN- based texture representations have demonstrated strengths texture classiﬁcation, recognizing textures large appearance variations, such textures KTHTIPS2b, FMD DTD.
 shelf CNN based descriptors, combination IFV feature encoding, have advantages benchmark datasets, Outex TC10 Outex TC12 texture descriptors, such MRELBP [114, ScatNet [19] (cid:7072)(cid:6558)(cid:5315)(cid:3374)(cid:1791)(cid:7072)(cid:11550)(cid:12971)(cid:2139)(cid:7072)(cid:11550)(cid:3374)(cid:1791)(cid:4714)(cid:4648)(cid:6208)(cid:1791)(cid:7569)(cid:1318)(cid:7163)(cid:17820)(cid:2568)(cid:2374)(cid:16374)(cid:9961)(cid:2568)(cid:2374)(cid:4714)(cid:5334)(cid:2568)(cid:2374)(cid:9888)(cid:5334)(cid:20172)(cid:14498)(cid:4558)(cid:1467)(cid:12971)(cid:2139)(cid:3374)(cid:1791)(cid:1973)(cid:4585)(cid:5418)(cid:12539)(cid:5284)(cid:5334)(cid:104)(cid:9888)(cid:5334)(cid:4558)(cid:20668)(cid:2591)(cid:6615)(cid:10393)(cid:1411)(cid:16024)(cid:19858)(cid:4558)(cid:1467)(cid:104)(cid:20172)(cid:14498)(cid:6247)(cid:3910)(cid:10393)(cid:1411)(cid:16024)(cid:19858)(cid:4558)(cid:1467)(cid:21)(cid:19)(cid:19)(cid:104)(cid:20172)(cid:14498)(cid:4558)(cid:20668)(cid:2591)(cid:6615)(cid:7552)(cid:7113)(cid:16024)(cid:19858)(cid:4558)(cid:1467)(cid:104)(cid:20172)(cid:14498)(cid:4558)(cid:20668)(cid:2591)(cid:6615)(cid:7552)(cid:17240)(cid:10393)(cid:1411)(cid:4558)(cid:1467)(cid:1085)(cid:3370)(cid:4554)(cid:20172)(cid:14498)(cid:1085)(cid:2591)(cid:6615)(cid:7552)(cid:7113)(cid:16024)(cid:19858)(cid:7538)(cid:8978)(cid:284)(cid:284)(cid:1913)(cid:10135)(cid:2568)(cid:2374)(cid:284)(cid:4671)(cid:284)(cid:284)(cid:284)(cid:284)(cid:284)(cid:284)(cid:284)(cid:1085)(cid:3370)(cid:4554)(cid:20172)(cid:14498)(cid:1085)(cid:2591)(cid:6615)(cid:7552)(cid:7113)(cid:16024)(cid:19858)(cid:7538)(cid:8978)(cid:284)(cid:284)(cid:284)(cid:284)(cid:1085)(cid:3370)(cid:4554)(cid:20172)(cid:14498)(cid:1085)(cid:2591)(cid:6615)(cid:13545)(cid:10806)(cid:4750)(cid:5719)(cid:12971)(cid:2139)(cid:284)(cid:284)(cid:284)(cid:26)(cid:25)(cid:27)(cid:104)(cid:20172)(cid:14498)(cid:4558)(cid:20668)(cid:2591)(cid:6615)(cid:7552)(cid:7113)(cid:16024)(cid:19858)(cid:4558)(cid:1467)(cid:24)(cid:20)(cid:21)(cid:104)(cid:20172)(cid:14498)(cid:1085)(cid:2591)(cid:6615)(cid:7552)(cid:7113)(cid:16024)(cid:19858)(cid:12971)(cid:2139)(cid:284)(cid:284)(cid:284)(cid:284)(cid:284)(cid:284)(cid:284)(cid:21)(cid:19)(cid:19)(cid:104)(cid:21)(cid:19)(cid:19)(cid:20172)(cid:14498)(cid:4558)(cid:20668)(cid:2591)(cid:6615)(cid:7552)(cid:7113)(cid:16024)(cid:19858)(cid:4558)(cid:1467)(cid:284)(cid:284)(cid:284)(cid:20172)(cid:14498)(cid:4558)(cid:20668)(cid:2591)(cid:6615)(cid:7552)(cid:7113)(cid:16024)(cid:19858)(cid:12971)(cid:2139)(cid:20172)(cid:14498)(cid:4558)(cid:20668)(cid:2591)(cid:6615)(cid:7552)(cid:7113)(cid:16024)(cid:19858)(cid:12971)(cid:2139)(cid:104)(cid:104)(cid:284)(cid:4671)(cid:4671)(cid:284)(cid:284)(cid:4671)(cid:4671)(cid:284)(cid:104)(cid:20172)(cid:14498)(cid:4558)(cid:20668)(cid:2591)(cid:6615)(cid:7552)(cid:7113)(cid:16024)(cid:19858)(cid:4558)(cid:1467)(cid:284)(cid:4671)(cid:4671)(cid:284)(cid:104)(cid:9888)(cid:5334)(cid:6247)(cid:3910)(cid:2591)(cid:6615)(cid:10393)(cid:1411)(cid:16024)(cid:19858)(cid:4558)(cid:1467)(cid:284)(cid:284)(cid:284)(cid:104)(cid:9888)(cid:5334)(cid:6247)(cid:3910)(cid:2591)(cid:6615)(cid:7552)(cid:7113)(cid:16024)(cid:19858)(cid:4558)(cid:1467)(cid:284)(cid:284)(cid:284)(cid:284)(cid:104)(cid:20172)(cid:14498)(cid:1085)(cid:2591)(cid:6615)(cid:7552)(cid:7113)(cid:16024)(cid:19858)(cid:7538)(cid:8978)(cid:284)(cid:284)(cid:284)(cid:284) Survey Recent Advances Texture Representation Fig.
 t-distributed Stochastic Neighbor Embedding (tSNE) [119] textures IFV encoding VGGVD features [30] UIUC dataset (b) FMD dataset.
 classes UIUC are separable FMD.
 most used texture sources were used benchmark.
 extensive evaluation recent promising LBP variants descriptors based deep convolutional networks was carried best overall performance ob- tained MRELBP feature.
 Discussion Conclusion importance texture representations is fact have extended many different problems textures.
 ﬁrst comprehensive survey texture representations new century, paper has highlighted recent achieve- ments, provided structural categories methods accord- ing roles feature representation, analyzed merits demerits, summarized existing popular texture datasets, discussed performance representative approaches.
 Al- most practical application is compromise classiﬁca- tion accuracy, robustness image degradations, compactness efﬁciency, number training data available, cost power consumption implementations.
 signiﬁcant progress has been made last be beginning new era, is ample room future research, leads following discussions.
 Large Scale Texture Dataset Collection.
 increas- ing volume image video data creates new opportunities challenges.
 complex variability big image data reveals inadequacies conventional handcrafted texture descriptors brings opportunities representation learning techniques, such deep learning, aim learning good representations au- data.
 recent unprecedented success deep learning image classiﬁcation object recognition is insepara- ble availability large-scale annotated image datasets such ImageNet [178] MS COCO [105].
 However, deep learning based texture analysis has kept pace rapid progress witnessed other ﬁelds, such image classiﬁcation, due unavailability large-scale texture database.
 result is signiﬁcant motivation good, large-scale texture dataset, advance texture analysis, using deep learning.
 Efforts need be devoted study kinds texture datasets are needed future research.
 Effective Robust Texture Representations.
 signiﬁcant progress recent years most texture descriptors, ir- respective handcrafted learned, have been ca- pable performing level sufﬁcient real world textures.
 ultimate goal community is develop texture repre- sentations discriminate massive image texture categories possible scenes, level compa- rable human visual system.
 practical applications, factors such signiﬁcant changes illumination, rotation, viewpoint scale, image degradations such occlusions, image blur random noise call discriminative robust texture rep- resentations.
 input psychological research visual perception biology human visual system be welcome.
 Compact Efﬁcient Texture Representations.
 is ten- sion demands big data desire com- pact efﬁcient feature representations.
 Thus, hand, era big data many existing texture representations are failing keep pace emerging big dimensionality [236], leading pressing need new strategies dealing scal- ability, high computational complexity, storage.
 other hand, is growing need deploying compact resource-efﬁcient feature representations platforms low en- ergy embedded vision sensors handheld devices.
 Many existing descriptors fail contexts, current general trend deep CNN architectures has been de- velop deeper complicated networks, advances requiring massive data power hungry GPUs training, suitable be deployed mobile platforms have limited resources.
 result, is growing interest building compact efﬁ- cient CNN-based features [73,
 CNNs out- perform classical texture descriptors LBP, remains be seen approaches be effective resource-limited contexts, degree LBP / CNN hybridization be considered, such recent lightweight CNN architectures
 Li Liu al.
 Reduced Dependence Large Amounts Data.
 Deep learn- ing features have promoted performance visual recognition, including texture recognition, deep learn- ing feature representations need be learned large amount training data.
 Certainly, are many applications tex- ture representations are useful limited amounts annotated training data be available collecting labeled train- ing data is expensive (such visual inspection, facial micro- expression recognition, age estimation medical texture anal- ysis).
 Possible research directions be development learnable local feature descriptors small medium amounts training data, [47, explore effective transfer learning methods.
 Semantic Texture Attributes.
 Progress image texture repre- sentation understanding, substantial, has been focused low-level feature representation.
 However, order address advanced human-centric applications such detailed im- age search human-robotic interaction, low-level understanding be sufﬁcient.
 Texture patterns are type visual abstraction describe less richness detail objects themselves, has been limited research describing textures human-like texture attributes.
 Future efforts be devoted direction go texture identiﬁcation categorization, develop semantic describable texture attributes be predicted low-level texture representations, explore ﬁne- grained compositional structure analysis texture patterns.
 Effect Smaller Image Size.
 Performance evaluation texture descriptors is done texture datasets consisting rel- large images.
 large number applications ability analyze small image sizes high speed is vital, including facial image analysis, interest region description, segmentation, defect detection, tracking.
 Many existing texture descriptors fail respect, be important evaluate per- formance new descriptors, was done [182].
 References
 Aharon M., Elad M., Bruckstein A.
 (2006) ksvd: algorithm de- signing overcomplete dictionaries sparse representation.
 IEEE Trans- actions signal processing
 Ahonen T., Pietik¨ainen M.
 (2007) Soft histograms local binary pat- terns.
 In: Proceedings Finnish signal processing symposium, vol p.

 Ahonen T., Hadid A., Pietikainen M.
 (2006) Face description lo- cal binary patterns: Application face recognition.
 IEEE trans pattern analysis machine intelligence (2009) http://aloi.science.uva.nl/public_
 ALOT alot/
 Amadasun M., King R.
 (1989) Textural features corresponding tex- tural properties.
 IEEE Transactions systems, man, Cybernetics
 Andrearczyk V., Whelan P.
 (2016) Using ﬁlter banks convolutional neural networks texture classiﬁcation.
 Pattern Recognition Letters
 Arandjelovic R., Gronat P., Torii A., Pajdla T., Sivic J.
 (2016) NetVLAD: CNN architecture weakly supervised place recognition.
 In: Inter- national Conference Computer Vision Pattern Recognition, pp.

 Baraniuk R., Davenport M., DeVore R., Wakin M.
 (2008) A simple proof restricted isometry property random matrices.
 Constructive Approximation
 Bell S., Upchurch P., Snavely N., Bala K.
 (2013) Opensurfaces: A annotated catalog surface appearance.
 ACM Transactions Graphics
 Bell S., Upchurch P., Snavely N., Bala K.
 (2015) Material recognition wild materials context database.
 In: International Confer- ence Computer Vision Pattern Recognition, pp.

 Bengio Y., Courville A., Vincent P.
 (2013) Representation learning: review new perspectives.
 IEEE trans pattern analysis machine intelligence
 Bhushan N., Rao A.
 R., Lohse G.
 L.
 texture lexicon: Under- standing categorization visual texture terms relationship texture images.
 Cognitive Science
 Bormann R., Esslinger D., Hundsdoerfer D., Haegele M., Vincze M.
 (2016) Texture characterization semantic attributes: Database algorithm.
 In: 47th International Symposium Robotics, pp.

 Boureau Y., Ponce J., LeCun Y.
 (2010) theoretical analysis feature pooling visual recognition.
 In: International Conference machine learning, pp.

 Bovik A., Clark M., Geisler W.
 (1990) Multichannel texture analysis using localized spatial ﬁlters.
 IEEE Trans Pattern Analysis Machine Intelligence
 Brahnam S., Jain L., Nanni L., Lumini A.
 (2014) Local binary patterns: new variants applications.
 Springer
 Brodatz (1966) brodatz.html
 Brodatz P.
 (1966) Textures: A Photographic Album Artists De- signers.
 Dover Publications, New York, USA
 Bruna J., Mallat S.
 (2013) Invariant scattering convolution networks.
 IEEE trans Pattern Analysis Machine Intelligence
 Burghouts G., Geusebroek J.
 (2009) Material speciﬁc adaptation color invariant features.
 Pattern Recognition Letters –
 Candes E.
 J., Tao T.
 (2006) Near-optimal signal recovery random projections: Universal encoding strategies?
 IEEE transactions infor- mation theory
 Caputo B., Hayman E., Mallikarjuna P.
 (2005) Class speciﬁc material categorisation.
 In: International Conference Computer Vision, vol pp.

 Chan T., Jia K., Gao S., Lu J., Zeng Z., Ma Y.
 (2015) PCANet: A sim- ple deep learning baseline image classiﬁcation?
 IEEE Trans Image Processing
 Chatﬁeld K., Lempitsky V., Vedaldi A., Zisserman A.
 devil is details: evaluation recent feature encoding methods.
 In: BMVC, vol p.

 Chatﬁeld K., Simonyan K., Vedaldi A., Zisserman A.
 (2014) Return devil details: Delving deep convolutional nets.
 British Machine Vision Conference
 Chellappa R., Chatterjee S.
 (1985) Classiﬁcation textures using Gaus- sian Markov Random ﬁelds.
 IEEE Trans Acoustics, Speech, Signal Processing
 Chen J., Shan S., C., Zhao G., Pietikainen M., Chen X., Gao W.
 (2010) robust local image descriptor.
 IEEE Trans Pattern Anal Mach Intell
 Cimpoi M., Maji S., Kokkinos I., Mohamed S., Vedaldi A.
 (2014) De- scribing textures wild.
 In: International Conference Computer Vision Pattern Recognition, pp.

 Cimpoi M., Maji S., Vedaldi A.
 (2015) Deep ﬁlter banks texture recognition segmentation.
 In: International Conference Computer Vision Pattern Recognition, pp.

 Cimpoi M., Maji S., Kokkinos I., Vedaldi A.
 (2016) Deep ﬁlter banks texture recognition, description, segmentation.
 International Journal Computer Vision
 Cinbis R.
 G., Verbeek J., Schmid C.
 (2016) Approximate ﬁsher kernels non-iid image models image categorization.
 IEEE transactions pattern analysis machine intelligence
 Conners R.
 W., Harlow C.
 A.
 (1980) theoretical comparison tex- ture algorithms.
 IEEE Trans Pattern Analysis Machine Intelligence (3):204–222
 Crosier M., Grifﬁn L.
 D.
 (2010) Using basic image features texture classiﬁcation.
 Int J Comput Vision
 Cross G., Jain A.
 (1983) Markov random ﬁeld texture models.
 IEEE Trans Pattern Analysis Machine Intelligence (1):25–39 A Survey Recent Advances Texture Representation
 Csurka G., Dance C., Fan L., Willamowski J., Bray C.
 (2004) Visual categorization bags keypoints.
 In: ECCV Workshop statistical learning computer vision
 CUReT (1999) http://www.cs.columbia.edu/CAVE/ software/curet/html/about.php
 Cusano C., Napoletano P., Schettini R.
 (2016) Evaluating color texture descriptors large variations controlled lighting conditions.
 Jour- nal Optical Socienty America A
 Dahl A., Larsen R.
 (2011) Learning dictionaries discriminative image patches.
 In: British Machine Vision Conference
 Dai X., Ng J.
 Y.-H., Davis L.
 S.
 (2017) FASON: First Second Order information fusion Network texture recognition.
 In: IEEE Confer- ence Computer Vision Pattern Recognition, pp.

 Dana K., Van Ginneken B., Nayar S., Koenderink J.
 (1999) Reﬂectance texture real world surfaces.
 ACM Transactions Graphics
 Depeursinge A., Al-Kadi O., Mitchell J.
 (2017) Biomedical Texture Analysis.
 Academic Press
 Ding C., Choi J., Tao D., Davis L.
 S.
 (2016) Multi-directional multi- level dual-cross patterns robust face recognition.
 IEEE trans pattern analysis machine intelligence
 Donahue J., Jia Y., Vinyals O., Hoffman J., Zhang N., Tzeng E., Darrell T.
 (2014) DeCAF: A deep convolutional activation feature generic visual recognition.
 In: International conference machine learning, pp.

 Donoho D.
 L.
 (2006) Compressed sensing.
 IEEE Transactions infor- mation theory
 Drexel (2012) texture/
 DTD (2014) http://www.robots.ox.ac.uk/˜vgg/ data/ dtd/
 Duan Y., Lu J., Feng J., Zhou J.
 (2017) Context aware local binary fea- ture learning face recognition.
 IEEE Trans Pattern Analysis Ma- chine Intelligence
 Efros A.
 A., Leung T.
 K.
 (1999) Texture synthesis nonparametric sampling.
 In: International Conference Computer Vision, vol pp.

 Farhadi A., Endres I., Hoiem D., Forsyth D.
 (2009) Describing objects attributes.
 In: International Conference Computer Vision Pattern Recognition, pp.

 FMD (2009) http://people.csail.mit.edu/ celiu/ CVPR2010/FMD/
 Forsyth D., Ponce J.
 (2012) Computer Vision: A Modern Approach, Edition
 Freeman W., Adelson E.
 design use steerable ﬁlters.
 IEEE Trans Pattern analysis machine intelligence
 Fritz M., Hayman E., Caputo B., Eklundh J.
 (2004) KTH- TIPS database.
 http://www.nada.kth.se/cvap/databases
 Gao Y., Beijbom O., Zhang N., Darrell T.
 (2016) Compact bilinear pooling.
 In: International Conference Computer Vision Pattern Recognition, pp.

 G˚arding J., Lindeberg T.
 (1996) Direct computation shape cues using scale-adapted spatial derivative operators.
 International Journal Com- puter Vision
 Gatys L., Ecker A., Bethge M.
 (2015) Texture synthesis using convolu- tional neural networks.
 In: Advances Neural Information Processing Systems, pp.

 Gatys L., Ecker A., Bethge M.
 (2016) Image style transfer using convo- lutional neural networks.
 In: CVPR, pp.

 Georgescu B., Shimshoni I., Meer P.
 (2003) Mean shift based clustering high dimensions: texture classiﬁcation example.
 In: International Conference Computer Vision, vol p.

 Girshick R., Donahue J., Darrell T., Malik J.
 (2014) Rich feature hi- erarchies accurate object detection semantic segmentation.
 In: International Conference Computer Vision Pattern Recognition, pp.

 Giryes R., Sapiro G., Bronstein A.
 M.
 (2016) Deep neural networks random gaussian weights: universal classiﬁcation strategy?
 IEEE Trans Signal Processing
 Grifﬁn L., Lillholm M., Crosier M., van Sande J.
 (2009) Basic image features (bifs) arising approximate symmetry type.
 Scale Space Variational Methods Computer Vision pp.

 Grifﬁn L.
 D., Lillholm M.
 (2010) Symmetry sensitivities derivative- of-gaussian ﬁlters.
 IEEE trans pattern analysis machine intelligence
 Ground Terrain Outdoor Scenes (GTOS) (2016) http:// computervision.engr.rutgers.edu/
 Gu J., Wang Z., Kuen J., Ma L., al.
 (2017) Recent advances convo- lutional neural networks.
 Pattern Recognition
 Guo Z., Zhang L., Zhang D.
 (2010) A completed modeling local bi- nary pattern operator texture classiﬁcation.
 IEEE Trans Image Pro- cess
 Guo Z., Wang X., Zhou J., J.
 (2016) Robust texture image represen- tation scale selective local binary patterns.
 IEEE Trans Image Pro- cessing
 Haralick R.
 (1979) Statistical structural approaches texture.
 Pro- ceedings IEEE
 Haralick R., Shanmugam K., Dinstein I.
 (1973) Textural features image classiﬁcation.
 IEEE Trans Systems, Man, Cybernetics (6):610–621
 Hariharan B., Arbel´aez P., Girshick R., Malik J.
 (2015) Hypercolumns object segmentation ﬁne-grained localization.
 In: International Conference Computer Vision Pattern Recognition, pp.

 Hayman E., Caputo B., Fritz M., Eklundh J.
 (2004) signiﬁcance real world conditions material classiﬁcation.
 European Conference Computer Vision pp.

 C., Li S., Liao Z., Liao M.
 (2013) Texture classiﬁcation PolSAR data based sparse coding wavelet polarization textons.
 IEEE Trans Geoscience Remote Sensing
 K., Zhang X., Ren S., Sun J.
 (2016) Deep residual learning im- age recognition.
 In: International Conference Computer Vision Pattern Recognition, pp.

 Howard A., Zhu M., Chen B., Kalenichenko D., Wang W., Weyand T., Andreetto M., Adam H.
 (2017) Mobilenets: Efﬁcient convolutional neu- ral networks mobile vision applications.
 In: CVPR
 Huang D., Shan C., Ardabilian M., Wang Y., Chen L.
 (2011) Local bi- nary patterns application facial image analysis: survey.
 IEEE Transactions Systems, Man, Cybernetics Part C
 Huang G., Liu Z., Weinberger K.
 Q., van der Maaten L.
 (2017) connected convolutional networks.
 In: International Conference Computer Vision Pattern Recognition
 Huang Y., Wu Z., Wang L., Tan T.
 (2014) Feature coding image clas- siﬁcation: A comprehensive study.
 IEEE trans pattern analysis ma- chine intelligence
 Jain A., Farrokhnia F.
 (1991) Unsupervised texture segmentation using Gabor ﬁlters.
 Pattern Recognition
 Jain A., Duin R., Mao J.
 (2000) Statistical pattern recognition: review.
 IEEE Trans pattern analysis machine intelligence
 Jegou H., Perronnin F., Douze M., S´anchez J., Perez P., Schmid C.
 (2012) Aggregating local image descriptors compact codes.
 IEEE Trans Pattern Analysis Machine Intelligence
 Julesz B.
 (1962) Visual pattern discrimination.
 IRE transactions In- formation Theory
 Julesz B.
 (1981) Textons, elements texture perception, interactions.
 Nature
 Julesz B., Bergen J.
 (1983) Human factors behavioral science: Tex- tons, fundamental elements preattentive vision perception textures.
 Bell System Technical Journal
 Kadir T., Brady J.
 (2002) Scale, saliency scene description.
 PhD thesis, Oxford University
 Kandaswamy U., Adjeroh D., Lee M.
 (2005) Efﬁcient texture analysis SAR imagery.
 IEEE Trans Geoscience Remote Sensing
 Keller J., Chen S., Crownover R.
 (1989) Texture description segmen- tation fractal geometry.
 Computer Vision, Graphics, Image Processing
 Kim K., Jung K., Park S., Kim H.
 (2002) Support vector machines texture classiﬁcation.
 IEEE trans pattern analysis machine intelli- gence
 Kong S., Fowlkes C.
 Low rank bilinear pooling ﬁne-grained classiﬁcation.
 In: International Conference Computer Vision Pat- tern Recognition, pp.
 Li Liu al.

 Kong S., Wang D.
 (2012) Multilevel feature descriptor robust texture classiﬁcation locality constrained collaborative strategy.

 Krishna R., Zhu Y., Groth O., Johnson J., Hata K., Kravitz J., Chen S., Kalantidis Y., Li L., Shamma D., Bernstein M., FeiFei L.
 (2017) Visual genome: Connecting language vision using crowdsourced dense im- age annotations.
 International Journal Computer Vision
 Krizhevsky A., Sutskever I., Hinton G.
 (2012) ImageNet classiﬁcation deep convolutional neural networks.
 In: Advances neural infor- mation processing systems, pp.

 KTHTIPS (2004) http://www.nada.kth.se/cvap/ databases/kth-tips/download.html
 Kumar N., Berg A., Belhumeur P.
 N., Nayar S.
 (2011) Describable vi- sual attributes face veriﬁcation image search.
 IEEE Trans Pattern Analysis Machine Intelligence
 Lategahn H., Gross S., Stehle T., Aach T.
 (2010) Texture classiﬁcation modeling joint distributions local patterns gaussian mixtures.
 IEEE Trans Image Processing
 Laws K.
 (1980) Rapid texture identiﬁcation.
 In: Proc.
 SPIE Conf.
 Image Processing Missile Guidance, vol pp.

 Lazebnik S., Schmid C., Ponce J.
 (2003) sparse texture representation using afﬁne-invariant regions.
 In: International Conference Computer Vision Pattern Recognition, vol pp.
 II–II
 Lazebnik S., Schmid C., Ponce J.
 (2005) sparse texture representa- tion using local afﬁne regions.
 IEEE Trans Pattern Anal Mach Intell
 Lazebnik S., Schmid C., Ponce J.
 (2006) Beyond bags features: Spa- tial pyramid matching recognizing natural scene categories.
 In: Inter- national Conference Computer vision pattern recognition, vol pp.

 LeCun Y., Bengio Y., Hinton G.
 (2015) Deep learning.
 Nature
 Lee T.
 S.
 (1996) Image representation using Gabor wavelets.
 IEEE Transactions pattern analysis machine intelligence
 Leung T., Malik J.
 (2001) Representing recognizing visual appearance materials using three-dimensional textons.
 International journal computer vision
 Levi G., Hassner T.
 (2015) Emotion recognition wild convolu- tional neural networks mapped binary patterns.
 In: ACM ICMI, pp.
 http://eceweb1.rutgers.edu/˜kdana/
 LFMD (2016) code.html
 Li S.
 (2009) Markov Random Field Modeling Image Analysis.
 Springer Science Business Media
 Lin T., Maji S.
 (2016) Visualizing understanding deep texture repre- sentations.
 In: IEEE Conference Computer Vision Pattern Recog- nition, pp.

 Lin T., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Doll´ar P., Zitnick L.
 (2014) Microsoft COCO: Common objects context.
 In: ECCV, pp.

 Lin T., RoyChowdhury A., Maji S.
 (2015) Bilinear cnn models ﬁne- grained visual recognition.
 In: Proceedings IEEE International Conference Computer Vision, pp.

 Lin T., RoyChowdhury A., Maji S.
 (2017) Bilinear convolutional neural networks ﬁne-grained visual recognition.
 IEEE Trans Pattern Analy- sis Machine Intelligence
 Lin X., Zhao C., Pan W.
 (2017) Towards accurate binary convolutional neural network.
 In: NIPS, pp.

 Liu L., Fieguth P.
 (2012) Texture classiﬁcation random features.
 IEEE Trans Pattern Analysis Machine Intelligence –586
 Liu L., Fieguth P., Kuang G., Zha H.
 Sorted random projections robust texture classiﬁcation.
 In: International Conference Com- puter Vision, IEEE, pp.

 Liu L., Fieguth P., Kuang G., Clausi D.
 Sorted random projections robust rotation invariant texture classiﬁcation.
 Pattern Recognition
 Liu L., Fieguth P., Wang X., Pietik¨ainen M., Hu D.
 (2016) Evaluation LBP deep texture descriptors new robustness benchmark.
 In: European Conference Computer Vision
 Liu L., Lao S., Fieguth P., Guo Y., Wang X., Pietikainen M.
 (2016) Me- dian robust extended local binary pattern texture classiﬁcation.
 IEEE Trans Image Processing
 Liu L., Fieguth P., Guo Y., Wang X., Pietik¨ainen M.
 (2017) Local binary features texture classiﬁcation: Taxonomy experimental study.
 Pattern Recognition
 Liu Y., Tsin Y., Lin W.
 promise perils regular texture.
 International Journal Computer Vision
 Lowe D.
 Distinctive image features scale-invariant key- points.
 International journal computer vision
 Lu J., Liong V.
 E., Zhou J.
 Simultaneous local binary feature learning encoding homogeneous heterogeneous face recog- nition.
 IEEE trans pattern analysis machine intelligence
 Ma L., Tan T., Wang Y., Zhang D.
 (2003) Personal identiﬁcation based iris texture analysis.
 IEEE Trans Pattern Analysis Machine Intel- ligence
 Maaten L., Hinton G.
 (2008) Visualizing data using t-SNE.
 Journal Machine Learning Research
 Mairal J., Bach F., Ponce J., Sapiro G., Zisserman A.
 (2008) Discrimina- tive learned dictionaries local image analysis.
 In: IEEE Conference Computer Vision Pattern Recognition, IEEE, pp.

 Mairal J., Ponce J., Sapiro G., Zisserman A., Bach F.
 (2009) Supervised dictionary learning.
 In: Advances neural information processing sys- tems, pp.

 Malik J., Perona P.
 Preattentive texture discrimination early vision mechanisms.
 JOSA A
 Malik J., Belongie S., Shi J., Leung T.
 (1999) Textons, contours regions: Cue integration image segmentation.
 In: International Con- ference Computer Vision, vol pp.

 Mallat S.
 (1989) A theory multiresolution signal decomposition: wavelet representation.
 IEEE Trans Pattern Analysis Machine Intel- ligence
 Mallikarjuna P., Tavakoli A., Fritz M., Hayman E., Caputo B., Eklundh J.
 (2006) KTH-TIPS2 database.
 http://www.nada.kth.se/ cvap/databases /kth-tips/kth-tips2.pdf
 Mandelbrot B., Pignoni R.
 fractal geometry nature.
 Free- man, New York
 Manjunath B., Chellappa R.
 (1991) Unsupervised texture segmentation using markov random ﬁeld models.
 IEEE Trans Pattern Analysis Machine Intelligence
 Manjunath B.
 S., Ma W.-Y.
 (1996) Texture features browsing retrieval image data.
 IEEE Trans Pattern Analysis Machine Intel- ligence
 Mao J., Jain A.
 (1992) Texture classiﬁcation segmentation using multiresolution simultaneous autoregressive models.
 Pattern Recogni- tion
 Marszałek M., Schmid C., Harzallah H., J.
 van W.
 (2007) Learn- ing object representations visual object class recognition.
 In: Visual Recognition Challange workshop conjunction ICCV
 Matthews T., Nixon M.
 S., Niranjan M.
 (2013) Enriching texture analy- sis semantic data.
 In: International Conference Computer Vision Pattern Recognition, pp.

 Mellor M., Hong B.-W., Brady M.
 (2008) Locally rotation, contrast, scale invariant descriptors texture analysis.
 IEEE Transactions Pattern Analysis Machine Intelligence
 Mikolajczyk K., Schmid C.
 (2002) afﬁne invariant interest point de- tector.
 European Conference Computer Vision pp.

 Mikolajczyk K., Schmid C.
 (2005) performance evaluation lo- cal descriptors.
 IEEE trans pattern analysis machine intelligence
 Mikolajczyk K., Tuytelaars T., Schmid C., Zisserman A., Matas J., Schaffalitzky F., Kadir T., Van Gool L.
 (2005) comparison afﬁne region detectors.
 International journal computer vision 65(1-2):43–
 MINC (2015) http://opensurfaces.cs.cornell.edu/ publications/minc/
 Mirmehdi M., Xie X., Suri J.
 (2008) Handbook Texture Analysis.
 Imperial College Press, London, UK
 Nanni L., Lumini A., Brahnam S.
 (2010) Local binary patterns vari- ants texture descriptors medical image analysis.
 Artif Intell Med A Survey Recent Advances Texture Representation
 Napoletano P.
 (2017) Hand crafted vs learned descriptors color texture classiﬁcation.
 In: International Workshop Computational Color Imaging, pp.

 Ohanian P., Dubes R.
 (1992) Performance evaluation classes textural features.
 Pattern recognition
 Ojala T., Pietik¨ainen M., Harwood D.
 (1996) A comparative study tex- ture measures classiﬁcation based featured distributions.
 Pattern recognition
 Ojala T., M¨aenp¨a¨a T., Pietik¨ainen M., Viertola J., Kyll¨onen J., Huovi- nen S.
 (2002) Outex-new framework empirical evaluation texture analysis algorithms.
 In: International Conference Pattern Recogni- tion, vol pp.

 Ojala T., Pietik¨ainen M., Maenp¨a¨a T.
 (2002) Multiresolution gray-scale rotation invariant texture classiﬁcation local binary patterns.
 IEEE Trans Pattern Anal Mach Intell
 Ojansivu V., Heikkil¨a J.
 (2008) Blur insensitive texture classiﬁcation us- ing local phase quantization.
 In: International Conference Image Signal Processing, pp.

 Ojansivu V., Rahtu E., Heikkila J.
 (2008) Rotation invariant local phase quantization blur insensitive texture analysis.
 In: International Con- ference Pattern Recognition, pp.

 Okazawa G., Tajima S., Komatsu H.
 (2015) Image statistics underlying natural texture selectivity neurons macaque v4.
 Proceedings National Academy Sciences
 Olshausen B., Field D.
 (1996) Emergence simple cell receptive ﬁeld properties learning sparse code natural images.
 Nature
 Olshausen B.
 A., Field D.
 J.
 (1997) Sparse coding overcomplete basis set: A strategy employed v1?
 Vision research
 Open Surfaces (2013) http://opensurfaces.cs.cornell.

 Oquab M., Bottou L., Laptev I., Sivic J.
 (2014) Learning transferring mid-level image representations using convolutional neural networks.
 In: International Conference Computer Vision Pattern Recognition, pp.

 Outex (2002) http://www.outex.oulu.fi/ index.php?
 page=outex_home
 Oxholm G., Bariya P., Nishino K.
 scale geometric texture.
 European Conference Computer Vision pp.

 Oyallon E., Mallat S.
 (2015) Deep roto-translation scattering ob- ject classiﬁcation.
 In: IEEE Conference Computer Vision Pattern Recognition, pp.

 P.
 Mallikarjuna, M.
 Fritz, A.
 Tavakoli Targhi, E.
 Hayman, B.
 Caputo, J.
 KTH-TIPS KTH-TIPS2 databases (2004) http://www.nada.kth.se/cvap/ databases/kth-tips/ documentation.html
 Parikh D., Grauman K.
 (2011) Relative attributes.
 In: International Con- ference Computer Vision, pp.

 Patterson G., Xu C., Su H., Hays J.
 (2014) sun attribute database: Beyond categories deeper scene understanding.
 International Journal Computer Vision
 Peikari M., Gangeh M.
 J., Zubovits J., Clarke G., Martel A.
 L.
 (2016) Triaging relevant regions pathology whole slides breast cancer: A texture based approach.
 IEEE trans medical imaging
 Perronnin F., Dance C.
 (2007) Fisher kernels visual vocabularies image categorization.
 In: International Conference Computer Vision Pattern Recognition, pp.

 Perronnin F., Sanchez J., Mensink T.
 (2010) Improving ﬁsher kernel large scale image classiﬁcation.
 In: European Conference Com- puter Vision, vol pp.

 Petrou M., Sevilla P.
 (2006) Image Processing: Dealing Texture,
 Picard R.
 W., Kabir T., Liu F.
 (1993) Real-time recognition en- tire brodatz texture database.
 In: International Conference Computer Vision Pattern Recognition, pp.

 Pichler O., Teuner A., Hosticka B.
 (1996) comparison texture fea- ture extraction using adaptive Gabor ﬁltering, pyramidal tree struc- tured wavelet transforms.
 Pattern Recognition
 Peyr´e G.
 (2009) Sparse modeling textures.
 Journal Mathematical vol
 Wiley Online Library Imaging Vision
 Pietik¨ainen M., Ojala T., Xu Z.
 (2000) Rotation invariant texture classi- ﬁcation using feature distributions.
 Pattern Recognition
 Pietik¨ainen M., Hadid A., Zhao G., Ahonen T.
 (2011) Computer vision using local binary patterns.
 Springer, London, UK
 Portilla J., Simoncelli E.
 P.
 (2000) A parametric texture model based joint statistics complex wavelet coefﬁcients.
 International journal computer vision
 Pun C., Lee M.
 (2003) Log-polar wavelet energy signatures rotation scale invariant texture classiﬁcation.
 IEEE trans pattern analysis machine intelligence
 Quan Y., Xu Y., Sun Y., Luo Y.
 (2014) Lacunarity analysis image pat- terns texture classiﬁcation.
 In: International Conference Computer Vision Pattern Recognition, pp.

 Raad L., Davy A., Desolneux A., Morel J.
 (2017) A survey exemplar based texture synthesis.
 arXiv preprint arXiv:170707184
 Randen T., Husoy J.
 (1999) Filtering texture classiﬁcation: com- parative study.
 IEEE Trans Pattern Analysis Machine Intelligence
 Rastegari M., Ordonez V., Redmon J., Farhadi A.
 (2016) XNORNet: ImageNet classiﬁcation using binary convolutional neural networks.
 In: ECCV, pp.

 Raw Food Texture (RFT) (2016) http://www.ivl.disco.
 unimib.it/minisites/ rawfoot/download.php
 Reed T., Wechsler H.
 (1990) Segmentation textured images gestalt organization using spatial/spatial-frequency representations.
 IEEE Trans Pattern Analysis Machine Intelligence
 Reed T.
 R., Dubuf J.
 H.
 (1993) review recent texture segmen- tation feature extraction techniques.
 CVGIP: Image understanding
 Ren J., Jiang X., Yuan J.
 (2013) Noise resistant local binary pattern embedded error-correction mechanism.
 IEEE Transactions Image Processing
 Robotics Domain Attributes Database (RDAD) (2016) http:// wiki.ros.org/ ipa_texture_classification
 Rubner Y., Tomasi C., Guibas L.
 Earth Mover’s Distance metric image retrieval.
 International Journal Computer Vision
 Russakovsky O., Deng J., Su H., Krause J., Satheesh S., Ma S., Huang Z., Karpathy A., Khosla A., Bernstein M., al.
 (2015) Imagenet large scale visual recognition challenge.
 International Journal Computer Vision
 Ryu J., Hong S., Yang H.
 Sorted consecutive local binary pat- tern texture classiﬁcation.
 IEEE Trans Image Processing
 Sanchez J., Perronnin F., Mensink T., Verbeek J.
 (2013) Image classiﬁ- cation ﬁsher vector: Theory practice.
 International Journal Computer Vision
 Schmid C.
 (2001) Constructing models content based image retrieval.
 In: International Conference Computer Vision Pattern Recogni- tion, vol pp.

 Schwartz G., Nishino K.
 (2015) discovering local visual material attributes.
 In: International Conference Computer Vision Pattern Recognition, pp.

 Sharan L., Rosenholtz R., Adelson E.
 (2009) Material perception: see brief glance?
 Journal Vision
 Sharan L., Liu C., Rosenholtz R., Adelson E.
 (2013) Recognizing mate- rials using inspired features.
 International journal com- puter vision
 Sharan L., Rosenholtz R., Adelson E.
 H.
 (2014) Accuracy speed material categorization real-world images.
 Journal vision
 Sharif Razavian A., Azizpour H., Sullivan J., Carlsson S.
 (2014) CNN features shelf: astounding baseline recognition.
 In: Inter- national Conference Computer Vision Pattern Recognition work- shops, pp.

 Sharma G., Jurie F.
 Local higher order statistics (LHS) describing images statistics local non-binarized pixel patterns.
 Computer Vision Image Understanding
 Shotton J., Winn J., Rother C., Criminisi A.
 (2009) Textonboost im- age understanding: Multiclass object recognition segmentation modeling texture, layout, context.
 International Journal Computer Vision Li Liu al.

 Sifre L.
 (2014) Rigid motion scattering image classiﬁcation,
 PhD thesis, ´Ecole Polytechnique
 Sifre L., Mallat S.
 (2012) Combined scattering rotation invariant tex- ture analysis.
 In: Proc.
 European Symp.
 Artiﬁcial Neural Networks
 Sifre L., Mallat S.
 (2013) Rotation, scaling deformation invariant scattering texture discrimination.
 In: International Conference Computer Vision Pattern Recognition, pp.

 Simonyan K., Zisserman A.
 (2015) deep convolutional networks large-scale image recognition
 Simonyan K., Parkhi O., Vedaldi A., Zisserman A.
 (2013) Fisher vector faces wild.
 In: BMVC, vol p.

 Sivic J., Zisserman A.
 (2003) Video google: A text retrieval approach object matching videos.
 In: International Conference Computer Vision, vol pp.

 Skretting K., Husøy J.
 (2006) Texture classiﬁcation using sparse frame- based representations.
 EURASIP Journal Advances Signal Pro- cessing
 Song Y., Zhang F., Li Q., Huang H., O’Donnell L., Cai W.
 (2017) transferred ﬁsher vectors texture classiﬁcation.
 In: International Con- ference Computer Vision Pattern Recognition, pp.

 Sulc M., Matas J.
 (2014) Fast features invariant rotation scale texture.
 In: European Conference Computer Vision, pp.

 Szegedy C., Liu W., Jia Y., Sermanet P., Reed S., Anguelov D., Erhan D., Vanhoucke V., Rabinovich A.
 (2015) Going convolutions.
 In: International Conference Computer Vision Pattern Recogni- tion, pp.

 Tamura H., Mori S., Yamawaki T.
 (1978) Textural features correspond- ing visual perception.
 IEEE Transactions Systems, Man, Cy- bernetics
 Tan X., Triggs B.
 (2007) Enhanced local texture feature sets face recognition difﬁcult lighting conditions.
 Analysis modeling faces gestures pp.

 Timofte R., Van Gool L.
 (2012) training-free classiﬁcation framework textures, writers, materials.
 In: BMVC, vol p.

 Tuceryan M., Jain A.
 (1993) Handbook pattern recognition com- puter vision.
 chap Texture Analysis, pp.

 Turner M.
 (1986) Texture discrimination gabor functions.
 Biological Cybernetics
 Tuytelaars T., Mikolajczyk K., al.
 (2008) Local invariant feature detec- tors: survey.
 Foundations trends computer graphics vision
 UBO2014 (2016) http://cg.cs.uni-bonn.de/en/ projects/ btfdbb/download/ubo2014/
 UIUC (2005) http://www-cvr.ai.uiuc.edu/ ponce_grp/ data/
 Ulyanov D., Vedaldi A., Lempitsky V.
 (2017) Improved texture net- works: Maximizing quality diversity feed forward stylization texture synthesis.
 In: International Conference Computer Vision Pattern Recognition (2009) http://www.cfar.umd.edu/˜fer/
 UMD website-texture/texture.html
 Valkealahti K., Oja E.
 (1998) Reduced multidimensional cooccurrence histograms texture classiﬁcation.
 IEEE Trans Pattern Analysis Machine Intelligence
 Van Gemert J., Geusebroek J., Veenman C., Smeulders A.
 (2008) Kernel codebooks scene categorization.
 In: European conference com- puter vision, pp.

 Van Gool L., Dewaele P., Oosterlinck A.
 (1985) Texture analysis anno
 Computer Vision, Graphics, Image Processing
 Varma M., Garg R.
 (2007) Locally invariant fractal features statistical texture classiﬁcation.
 In: International Conference Computer Vision, pp.

 Varma M., Zisserman A.
 (2005) statistical approach texture classi- ﬁcation single images.
 Int J Comput Vision
 Varma M., Zisserman A.
 (2009) statistical approach material clas- siﬁcation using image patches.
 IEEE Trans Pattern Anal Mach Intell
 VisTex (1995) http://vismod.media.mit.edu/vismod/ imagery/VisionTexture/
 Wang J., Yang J., Yu K., Lv F., Huang T., Gong Y.
 (2010) Locality- constrained linear coding image classiﬁcation.
 In: International Con- ference Computer Vision Pattern Recognition, IEEE, pp.

 Wang T., Zhu J., Hiroaki E., Chandraker M., Efros A.
 A., Ramamoor- thi R.
 (2016) light ﬁeld dataset cnn architectures material recognition.
 In: European Conference Computer Vision, pp.

 Webb A., Copsey K.
 (2011) Statistical Pattern Recognition (Third Edi- tion).
 Wiley
 Wei L., Levoy M.
 (2000) Fast texture synthesis using tree-structured vec- tor quantization.
 In: Internation Conference Computer graphics interactive techniques, pp.

 Weinmann M., Gall J., Klein R.
 (2014) Material classiﬁcation based training data synthesized using btf database.
 In: European Conference Computer Vision, pp.

 Weszka J.
 S., Dyer C.
 R., Rosenfeld A.
 (1976) A comparative study texture measures terrain classiﬁcation.
 IEEE Trans Systems, Man, Cybernetics (4):269–285
 Winn J., Criminisi A., Minka T.
 (2005) Object categorization learned universal visual dictionary.
 In: International Conference Computer Vision, vol pp.

 Wright J., Yang A., Ganesh A., Sastry S., Ma Y.
 (2009) Robust face recognition sparse representation.
 IEEE Trans Pattern Analysis Ma- chine Intelligence
 Wu Y., Zhu S., Liu X.
 (2000) Equivalence julesz ensembles FRAME models.
 International Journal Computer Vision
 Xie J., Hu W., Zhu S., Wu Y.
 (2015) Learning sparse FRAME models natural image patterns.
 International Journal Computer Vision 114(2-
 Xie X., Mirmehdi M.
 (2007) TEXEMS: texture exemplars defect detection random textured surfaces.
 IEEE Trans Pattern Analysis Machine Intelligence
 Xu J., Boddeti V.
 N., Savvides M.
 (2017) Local binary convolutional neural networks.
 In: International Conference Computer Vision Pattern Recognition
 Xu Y., Huang S., Ji H., Fermuller C.
 (2009) Combining powerful local global statistics texture description.
 In: Computer Vision Pattern Recognition,
 CVPR
 IEEE Conference on, pp.

 Xu Y., Ji H., Ferm¨uller C.
 (2009) Viewpoint invariant texture descrip- tion using fractal analysis.
 International Journal Computer Vision
 Xu Y., Yang X., Ling H., Ji H.
 (2010) new texture descriptor us- ing multifractal analysis multiorientation wavelet pyramid.
 In: Inter- national Conference Computer Vision Pattern Recognition, pp.

 Xue J., Zhang H., Dana K., Nishino K.
 (2017) Differential angular imag- ing material recognition.
 In: International Conference Computer Vision Pattern Recognition
 Yang J., Yu K., Gong Y., Huang T.
 (2009) Linear spatial pyramid match- ing using sparse coding image classiﬁcation.
 In: International Con- ference Computer Vision Pattern Recognition, pp.

 Yang L., Jin R., Sukthankar R., Jurie F.
 (2008) Unifying discriminative visual codebook generation classiﬁer training object category recognition.
 In: International Conference Computer Vision Pat- tern Recognition, pp.

 Ylioinas J., Hong X., Pietik¨ainen M.
 (2013) Constructing local binary pattern statistics soft voting.
 In: Scandinavian Conference Image Analysis, pp.

 Zhai H., Liu C., Dong H., Ji Y., Guo Y., Gong S.
 (2015) Face veriﬁcation aging based deep convolutional networks local binary pat- terns.
 In: International Conference Intelligent Science Big Data Engineering, pp.

 Zhai Y., Ong Y.-S., Tsang I.
 emerging “big dimensionality”.
 IEEE Computational Intelligence Magazine
 Zhang H., Jia X., Dana K.
 (2017) Deep TEN: Texture encoding network.
 In: International Conference Computer Vision Pattern Recogni- tion
 Zhang J., Tan T.
 (2002) Brief review invariant texture analysis meth- ods.
 Pattern recognition
 Zhang J., Marszalek M., Lazebnik S., Schmid C.
 Local features kernels classiﬁcation texture object categories: compre- hensive study.
 International Journal Computer Vision A Survey Recent Advances Texture Representation
 Zhang W., Shan S., Gao W., Chen X., Zhang H.
 (2005) Local gabor binary pattern histogram sequence (LGBPHS): A novel nonstatistical model face representation recognition.
 In: International Confer- ence Computer Vision, vol pp.

 Zhao G., Pietik¨ainen M.
 (2007) Dynamic texture recognition using local binary patterns application facial expressions.
 IEEE Trans Pattern Anal Mach Intell
 Zheng L., Yang Y., Tian Q.
 (2017) SIFT meets CNN: A decade survey instance retrieval.
 IEEE Trans Pattern Analysis Machine Intelli- gence
 Zhou B., Lapedriza A., Xiao J., Torralba A., Oliva A.
 (2014) Learning deep features scene recognition using places database.
 In: Advances neural information processing systems, pp.

 Zhu S.
 (2003) Statistical modeling conceptualization visual pat- terns.
 IEEE Trans Pattern Analysis Machine Intelligence
 Zhu S., Wu Y., Mumford D.
 (1998) Filters, random ﬁelds maximum entropy (FRAME): uniﬁed theory texture modeling.
 Inter- national Journal Computer Vision
 Zhu S., Liu X., Wu Y.
 (2000) Exploring texture ensembles efﬁcient markov chain monte “trichromacy” theory texture.
 IEEE Trans Pattern Analysis Machine Intelligence
 Zhu S., Guo C., Wang Y., Xu Z.
 (2005) are textons?
 International Journal Computer Vision Li Liu al.
 (cid:5) (cid:63) (cid:5) (cid:5) (cid:63) (cid:63) (cid:63) (cid:63) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) TraditionalBoWbasedTextureRepresentation CNNbasedTextureRepresentation
 recent years, Gaussian process regression has become prime regression technique (Rasmussen Williams,
 Roughly, Gaussian process be viewed suitable2 probability distribution set functions, condition observations using Bayes’ rule.
 resulting mean function is used regression.
 sample random functions, so-called realizations, distribution.
 strength Gaussian process regression lies avoiding overﬁtting ﬁnding functions complex describe behavior present given noisy unstructured data.
 Gaussian processes are applied observations are rare expensive produce.
 Applications range, many others, robotics et al., biology (Honkela global optimization (Osborne astrophysics (Garnett et engineering (Thewes et al.,
 Incorporating justiﬁed assumptions prior helps full information content scarce ob- servations be utilized create precise regression model.
 Examples such assumptions are smooth rough behavior, trends, homogeneous heterogeneous noise, lo- cal global behavior, periodicity (cf.
 §4 (Rasmussen Lange-Hegermann <markus.lange.hegermann@rwth-aachen.de>.
 to: Markus processes are maximum entropy ﬁnite mean variance unknown behavior Jaynes Bretthorst,
 Williams,
 Such assumptions are incorporated covariance structure Gaussian process.
 certain physical laws, given certain linear differ- ential equations, be incorporated covariance structures Gaussian process priors.
 Thereby, random realizations mean function posterior physical laws3.
 example, (Macˆedo Castro, constructed covariance structures divergence-free curl-free vector ﬁelds, (Wahlstr¨om et Solin et used model electromagnetic phenomena.
 ﬁrst step towards systematizing construction was achieved (Jidling al.,
 certain cases, parametrization solutions physical laws be found computation does terminate.
 Having found parametrization, assume Gaus- sian process parametrizing functions push forward.
 results Gaussian process solutions physical laws.
 Section paper combines ideas (Jidling al., algorithm computes parametrization exists, reports failure does exist.
 algorithm is homological result algebraic system theory (cf.
 §7.(25)
 paper adds information Gaussian processes ways: (i) restricting solutions linear operator matrices constructing suitable prior (ii) conditioning observations using Bayes’ rule.
 constructions are compatible, com- bine strict, global information equations noisy, local information observations.
 author views combination techniques homological algebra machine learning main result paper, notational simplicity, refrain using phrases “almost surely” “up equivalence” paper.
 say realizations Gaussian process such laws is element equivalence class w.r.t. sure equality has property, continuity smoothness.
 Algorithmic Linearly Constrained Gaussian Processes construction covariance functions satisfying physical laws proof concept.
 shows typical application.
 constructs Gaussian process such realizations satisfy inhomogeneous Maxwell equations electromagnetism.
 Conditioning Gaussian process single observa- tion electric current yields, expected, magnetic ﬁeld circling electric current.
 Gaussian processes are precise interpo- lation lack regards: missing extrapolation capabilities high computation time, amount observations.
 problems have, cer- tain degree, been addressed: several fast approximations Gaussian process regression (Titsias, Hensman et Wilson et Izmailov Dong al., more powerfull covariance structures (Lee et al., Wilson Adams, Wilson et Calandra have been proposed.
 paper ad- dresses problems complementary angle.
 linear differential equations allow extrapolate reduce needed amount observations, improves computation time.

 Differential Equations Gaussian Processes section is expository summarizes Gaus- sian processes differential operators act them.
 Subsection summarizes Gaussian process regression.
 introduce differential (Subsection other operators (Subsection sketch connection constructing priors (Subsection
 Gaussian processes Gaussian process g = GP(µ, k) is distribution set functions Rd → R(cid:96) such function values g(x1),


 g(xn) x1,


 ∈ Rd have joint Gaussian distribution.
 is speciﬁed mean function µ Rd → R(cid:96) x (cid:55)→ E(g(x)) positive semideﬁnite covariance function k Rd ⊕ Rd → R(cid:96)×(cid:96)(cid:23)0 (x, x(cid:48)) (cid:55)→ E((g(x) − µ(x))(g(x(cid:48)) − µ(x(cid:48)))T
 Assume regression model yi = g(xi) condition n observations {(xi, yi) ∈ R1×d ⊕ R1×(cid:96) | i =


 n}
 Denote k(x, X) ∈ R(cid:96)×(cid:96)n resp.
 k(X, X) ∈ R(cid:96)n×(cid:96)n(cid:23)0 (covariance) matrices obtained concatenating matri- ces k(x, resp.
 positive semideﬁnite block partitioned matrix blocks k(xi, xj).
 Write µ(X) ∈ R(cid:96)×n matrix obtained concatenating vectors µ(xi) y ∈ R1×(cid:96)n row vector obtained concatenating rows yi.
 posterior GP(cid:0) x (cid:55)→ µ(X) + (y − µ(X))k(X, X)−1k(x, X)T (x, x(cid:48)) (cid:55)→ k(x, x(cid:48)) − k(x, X)k(X, X)−1k(x(cid:48), X)T(cid:1) is Gaussian process mean function x (cid:55)→ µ(X) + (y − µ(X))k(X, X)−1k(x, X)T is used regression model.
 Differential equations on, let R = R[∂x1,


 be polynomial ring partial differential operators.
 ring models lin- ear (partial) differential equations constant coefﬁcients, acts vector space F = C∞(Rd, R) smooth functions, ∂xi acts partial derivative w.r.t. xi.
 set realizations Gaussian process squared ex- ponential covariance function is dense F (cf.
 Thm.
 Prop.
 (Simon-Gabriel Sch¨olkopf,
 speaking, Gaussian processes are linear objects stochastic processes.
 Hence, is surprising ﬁnd rich interplay Gaussian processes linear differential equations.
 class Gaussian processes is closed matrices B ∈ R(cid:96)×(cid:96)(cid:48)(cid:48) linear differential operators constant coefﬁcients.
 Let g = GP(µ, k) be Gaussian process realizations space F (cid:96)(cid:48)(cid:48) vectors functions F entries.
 Deﬁne Gaussian process B∗g Gaussian process induces pushforward measure B Gaussian measure induced g.
 holds B∗g = GP(Bµ(x), Bk(x, x(cid:48))(B(cid:48))T B(cid:48) denotes operation operators B functions argument x(cid:48) ∈ Rd (cf.
 Thm.
 (Bertinet Agnan,
 covariance matrices such Gaussian processes (1) are singular.
 is be expected, B∗g is dense F (cid:96).
 numerical stability, assume model yi = g(xi) + ε small Gaussian white noise term ε adopt k adding var(ε) k(xi, xi) observations xi.
 Let g = GP(0, k(x, be scalar univari- ate Gaussian process differentiable realizations.
 Gaussian process derivatives functions is given (cid:2) ∂ (cid:19) ∂x∂x(cid:48) k(x, x(cid:48)) interpret Gaussian process(cid:2) ∂ (cid:3) (cid:3) ∗ g = GP ∗ g taking derivatives measurement data producing regression model derivatives.
 ∂x ∂x (cid:18) ∂2 Parametrizations Further operator rings Algorithmic Linearly Constrained Gaussian Processes say Gaussian process is function space, realizations are contained said space.
 A ∈ R(cid:96)(cid:48)×(cid:96) deﬁne solution set solF (A) := {f ∈ F (cid:96) | Af =
 Such solutions sets Gaussian processes are connected.
 Let g = GP(µ, k) be Gaussian process F (cid:96)×1.
 g is Gaussian process solF (A) A ∈ R(cid:96)(cid:48)×(cid:96) µ ∈ solF (A) A∗(g − µ) is constant zero process.
 Proof.
 Assume g is Gaussian process solF (A).
 Then, mean function is realization, µ ∈ solF (A).
 Furthermore, ˜g := (g − µ) = GP(0, k) have realizations are annihilated A, A∗˜g is constant zero process.
 Conversely, assume µ ∈ solF (A) A∗(g − µ) is constant zero process.
 implies = A∗(g − µ) = A∗g − A∗µ = A∗g, realizations g become zero pushforward A.
 particular, realizations g are contained solF (A).
 goal is construct Gaussian processes realizations dense solution set solF (A) operator matrix A ∈ R(cid:96)(cid:48)×(cid:96).
 following remark, implicit (Jidling al., is ﬁrst step answer.
 Remark Let A ∈ R(cid:96)(cid:48)×(cid:96) B ∈ R(cid:96)×(cid:96)(cid:48)(cid:48) Let g = GP(0, k) be Gaussian process F (cid:96)(cid:48)(cid:48) set realizations B∗g is contained solF (A).
 AB

 Then, follows Lemma A∗(B∗g) = (AB)∗g = =
 call B ∈ R(cid:96)×(cid:96)(cid:48)(cid:48) parametrization solF (A) solF (A) = BF (cid:96)(cid:48)(cid:48)
 Parametrizations yield denseness realizations Gaussian process B∗g solF (A).
 Proposition Let B ∈ R(cid:96)×(cid:96)(cid:48)(cid:48) be parametrization solF (A) A ∈ R(cid:96)(cid:48)×(cid:96).
 Let g = GP(0, k) be Gaussian process dense F (cid:96)(cid:48)(cid:48)
 set realizations B∗g is dense solF (A).
 proposition is consequence partial derivatives be- ing bounded, hence continuous, F is equipped Fr´echet topology generated family semi- norms (cid:107)f(cid:107)a,b := sup i∈Zd≥0 |i|≤a sup z∈[−b,b]d | ∂ ∂zi f (z)| a, b ∈ Z≥0 (cf.
 §10
 con- tinuous surjective map induced B dense set dense set.
 theory presented differential equations constant coefﬁcients holds other rings R linear operators solution spaces F.
 following operator rings are prominent examples.
 polynomial ring R = R[x1,


 xd] models polynomial equations acts set F smooth functions deﬁned (Zariski-)open set Rd. model ordinary linear differential equations ratio- nal4 coefﬁcients consider Weyl algebra R = R(t)(cid:104)∂t(cid:105), non-commutative relation ∂tt = t∂t + represent- ing product rule differentiation.
 Here, consider solutions set F smooth functions deﬁned co-ﬁnite set.
 polynomial ring R = R[σx1,


 ] models linear shift equations constant coefﬁcients acts set F = RZd≥0 d-dimensional sequences translation arguments.

 Computing parametrizations last section, constructing parametrization B solF (A) yields Gaussian process dense solution set solF (A) operator matrix A ∈ R(cid:96)(cid:48)×(cid:96).
 Subsection gives necessary sufﬁcient conditions parametriza- tion exist Subsection describes computation.
 turns computations are algebraic R.
 Existence parametrizations turns decide parametrization exists algebraically, operations R do involve F.
 r-ker(A) denote right kernel A ∈ R(cid:96)(cid:48)×(cid:96), r-ker(A) = {m ∈ R(cid:96)×1 | Am =
 l-ker(A) denote left kernel A, l-ker(A) = {m ∈ R1×(cid:96)(cid:48) | mA
 Abusing notation, denote matrix left resp.
 kernel rows resp.
 columns generate kernel R-module.
 Let A ∈ R(cid:96)(cid:48)×(cid:96).
 Deﬁne matrices B = r-ker(A) A(cid:48) = l-ker(B).
 solF (A(cid:48)) is largest subset solF (A) is parametrizable B parametrizes solF (A(cid:48)).
 constrast vector left kernel right kernel A is A (up equivalence) case operator rings.
 solution set solF (A(cid:48)) is subset controllable behaviors solF (A).
 major changes polynomial, holonomic, meromor- phic coefﬁcients.
 Algorithmic Linearly Constrained Gaussian Processes Corollary Theorem solF (A) is parametrizable rows A A(cid:48) generate same row- module.
 AB = is case rows A(cid:48) are contained row module generated rows A.
 case, solF (A) is parametrized B.
 Furthermore, Gaussian process g realizations dense F (cid:96)(cid:48)(cid:48) leads Gaussian process B∗g realizations dense solF (A).
 proof theorem corollary see Thm.
 (Zerz et al., (cf.
 Thm.
 Alg.
 Lemma (Zerz, §7.(24) (Oberst, addi- tional characterizations, generalizations, proofs using more homological machinery see (Quadrat, Barakat, Seiler Zerz, Chyzak Robertz, references therein.
 approach assigns prior parametrising functions pushes prior forward prior solution set solF (A).
 paramerization is canonical, different parametrizations lead different priors.
 did lead practical problems, yet.
 Algorithms Summarizing Theorem Corollary algorithmi- cally, need compute right kernels (of A), compute left kernels (of B), decide rows (of A(cid:48)) are contained row module (generated rows A).
 computations are application Gr¨obner basis algorithms.
 recent decades, Gr¨obner bases algorithms have be- come core algorithms computeralgebra, manifold applications others geometry, system theory, natural sciences, automatic theorem proving.
 Generalizing Gaussian algorithm, reduced Gr¨obner bases generalize reduced echolon form systems linear operator equations.
 particular, using compute solutions R (not F) homogeneous system compute, exists, particular solution R (not F) inhomogeneous system.
 Solving homo- geneous systems is same computing right resp.
 left kernel ﬁnding relations (the generalization lin- ear dependencies) columns resp.
 rows matrix.
 Solving inhomogeneous equations decides ele- ment is contained module.
 uniqueness reduced Gr¨obner bases decides submodule equality.
 formal description Gr¨obner bases exceed scope note.
 Instead, refer excellent litera- ture (Sturmfels, Eisenbud, Adams Loustau- nau, Greuel Pﬁster, Gerdt, Buchberger, show next section use Gr¨obner bases computer algebra systems.
 are imple- mented various computer algebra systems, e.g., Singular (Decker et al., Macaulay2 (Grayson Stillman) are well-known open source implementations.
 Hyperparameters Many covariance functions5 incorporate hyperparameters advanced methods add more hyperparam- eters Gaussian processes, see e.g. (Snelson Calandra Wilson Adams, addi- tional ﬂexibility.
 approach paper is opposite.
 restrict Gaussian process prior, solutions oper- ator matrix, guarding overﬁtting.
 prior parametrizing functions can, does, contain hyperparameters.
 be determined maximizing likelihood, usual.
 Many important applications contain unknown parameters equations.
 Such parameters be estimated likelihood, conditioning data.
 ordinary differential equations, solution set operator matrix is direct sum parametrizable functions ﬁnite dimensional set functions, con- stant resp.
 variable coefﬁcients, due Smith form resp.
 Jacobson form.
 many cases, particular case con- stant coefﬁcients, solution set ﬁnite dimensional summand be computed.
 paper allows compute parametrizable summand solution set estimate parameters hyperparameters summands together.

 Examples reproduce well-known fact divergence-free ﬁelds be parametrized curl oper- ator.
 has been used connection Gaussian pro- cesses model electric magnetic phenomena (Macˆedo Castro, Wahlstr¨om Solin et
 same algebraic computation constructs prior tangent ﬁelds sphere.
 computer algebra system Macaulay2 (Grayson Still- man) performs Gr¨obner basis computations.
 Let R be polynomial ring indeterminates, interpret polynomial ring Q[∂1, ∂2, ∂3] differential operators resp.
 polynomial ring Q[x1, x2, x3] indeterminates.
 i1 o1 = R o1 PolynomialRing R=QQ[d1,d2,d3] mean function contains hyperparameters.
 additional hyperparameters are compared non-parametric Gaussian process model.
 Algorithmic Linearly Constrained Gaussian Processes consider matrix A = (cid:2)∂1 ∂2 ∂3 (cid:3) representing divergence resp.
 normals circles centered origin.
 i2 o2 = | d1 d2 d3 | A=matrix{{d1,d2,d3}} o2 Matrix R <--- R right kernel A is given operator B representing curl resp.
 tangent spaces circles centered origin.
 i3 o3 | -d2 B = generators kernel | d1 -d3 | -d3 | d2 d1 | Matrix R <--- R o3 right kernel A(cid:48) B is A, matrix B is parametrization matrix solutions A.
 i4 transpose B o4 = | d1 d2 d3 | A1 = transpose generators kernel Matrix R <--- R o4 (As kernel Macaulay2 yields kernels, compute left kernel transposition.) construct prior tangent ﬁelds sphere assuming equal covariance functions k uncorrelated parametrizing functions.
 mean ﬁeld is demonstrated Figure covariance function tangent ﬁeld is k(x1, z1, x2, y2, z2)· −y1y2 − z1z2 x1y2 x1z2 y1x2 −x1x2 − z1z2 y1z2 z1x2 z1y2 −x1x2 − y1y2 
 Maxwell’s equations electromagnetism uses curl divergence operators building blocks.
 is well-known result solutions inhomogeneous Maxwell equations are parametrized electric mag- netic potentials.
 verify use parametrization construct Gaussian process, such realizations Maxwell’s equations.
 condition prior single observation ﬂowing electric current, leads magnetic ﬁeld circling current.
 compu- tations have been performed Maple using Involutive package (Blinkov
 homogenous Mawell equations are given opera- Figure
 Taking squared exponential covariance function k Example yields above smooth mean tangent ﬁeld sphere conditioning distributed points equator opposite tangent vectors pointing north south each.
 visible vectors are displayed bigger.
 tor matrix Mh :=  ∂z −∂y −∂t ∂x −∂z ∂x −∂t ∂y ∂y ∂t −∂x ∂x ∂z −∂t −∂y ∂z ∂t ∂y −∂z ∂x  ∂t ∂z ∂y −∂x applied components electric ﬁeld components magnetic (pseudo) ﬁeld.
 have set constants
 right kernel Mh is zero, particular parametrization exists.
 inhomogeneous Maxwell equations addi- tional components electric current additional component electric ﬂux are given operator
 Using Gr¨obner matrix Mi := computes right kernel (cid:20) (cid:21) Mh   ∂x ∂y ∂z −∂t∂x −∂t∂y −∂t∂z ∂2 x + ∂2 y + ∂2 ∂t −∂z ∂y z − ∂2 ∂2 y + ∂2 −∂y ∂x −∂z ∂x ∂t∂x ∂t ∂z −∂x −∂y ∂x −∂z ∂y ∂t∂y ∂2 x + ∂2 z − ∂2 ∂t −∂y ∂x −∂z ∂x −∂z ∂y y − ∂2 ∂2 x + ∂2 ∂t∂z Mi veriﬁes is parametrization set Algorithmic Linearly Constrained Gaussian Processes > T := E[3]; (cid:20)0 (cid:2)1 (cid:21) t3 Dt solutions inhomogeneous Maxwell equations.
 assume squared exponential covariance functions k := exp(cid:0) (cid:0)(x1 − x2)2 + (y1 − y2)2 +(z1 − z2)2 + (t1 − t2)2(cid:1)(cid:1) zero mean function uncorrelated parametris- ing functions (electric magnetic potential).
 result- ing covariance matrix be found Appendix A demonstration see Figure
 Figure
 condition prior Example electric current z-direction electric ﬂux origin x = y = z = t
 diagram shows mean posterior magnetic ﬁeld (z, t) = (0, 0)-plane.
 expected, circles point electric current.
 mean ﬁeld has closed form (cid:0)x2 + y2 − exp(cid:0)− (cid:0)x2 + y2(cid:1)(cid:1) · (cid:20)−y (cid:21) Example consider one-dimensional Weyl algebra R = R(t)(cid:104)∂t(cid:105).
 allows stronger notion basis Gr¨obner Jacobson form (Jacobson,
 is similar Smith normal form PID’s, multiplying invertible matrices sides yields diagonal matrix.
 use Maple packages Janet OreModules (Blinkov et Chyzak
 > with( Janet ): with( OreModules ): > Alg := DefineOreAlgebra( diff=[ Dt, t ], polynom=[ t ] ): consider system dt x(t) = t3u(t) Example
 Jacobson form is just(cid:2)1
 time-varying control > E := ElementaryDivisors( [ diff(x(t),t)-tˆ3*u(t) ], [ t ], [ x, u ] ): > T1 := E[1]; (cid:2)− t3 (cid:3) > Mult( T1, [[ Dt, -tˆ3 ]], T, Alg ); particular, base change matrix T system is free generator parametrizable matrix (cid:20)0 (cid:21) original system is parametrizable (cid:20)0 (cid:21) (cid:20) (cid:21) t3 ∂t B = T · parametrizing functions squared exponential co- (t1 − t2)2) variance functions k(t1, t2) = exp(− (cid:19) (cid:18) mean function, covariance function (x, u) is (cid:34) t1−t2 (cid:35) t2−t1 t3 (t2−t1−1)(t1−t2−1) exp t3 t3 (t1 − t2)2 − are exceptional points domain trans- formed system, T removes domain original system.
 restrict set smooth functions C∞(R>0, R) deﬁned positive real numbers.
 demonstration priors see Figures
 x(t) u(t) Figure
 state function x(t) system Example be inﬂuenced assigning input function u(t).
 example


 setting x(1) u(t) = leads above posterior mean.
 model yields x(5) ≈ t4+1 t ∈ {1, close to(cid:82) t3 t4+1 dt ≈
 A.
 Covariance matrices covariance matrix 3-dimensional electric ﬁeld E, 3-dimensional magnetic (pseudo) ﬁeld B, 3- dimensional electric current C 1-dimensional electric ﬂux F Maxwell’s equations constructed Example is KE,E KE,B KE,C KE,F KB,E KB,B KB,C KB,F KC,E KC,B KC,C KC,F KF,E KF,B KF,C KF,F  K := · Algorithmic Linearly Constrained Gaussian Processes x(t) u(t) -1 -2 Figure
 prescribe desired behavior state x(t) Example let Gaussian process construct suitable input u(t).
 Starting x(1) give u(t) time step control x(t) zero, e.g., setting x(t) t ∈



 matrices KE,E = (2 − (t1 − t2)2)I3 − vT v KB,B = (2 − vvT )I3 − vT v KC,C = (4 + − t2)2 − (vvT ))vT v +((vvT − (t1 − t2)2)2 − t2)2 − + KF,F = (t1 − t2)2(vvT + (vvT )2 − + KB,E = (t1 − t2)(v ∧ v) KC,E = (t1 − t2)((2 + vvT − (t1 − t2)2)I3 v) KF,E = ((t1 − t2)2 + vvT − KC,B = (vvT − (t1 − t2)2 ∧ v) KF,B =(cid:2)0  using KF,C = −(t1 − t2)(vvT + (t1 − t2)2 identity matrix, z1 − z2 −(y1 − y2) := −(z1 − z2) x1 − x2 y2 − y2 y1 − y2 −(x1 − x2) .
 (cid:2)x1 − x2 (cid:3), z1 − z2 v ∧ v I3 := Acknowledgments authors thanks M.
 Barakat, S.
 Gutsche, C.
 Kaus, D.
 Moser, S.
 Posur, O.
 Wittich discussions concern- ing paper, W.
 Plesken, A.
 Quadrat, D.
 Robertz, E.
 Zerz introducing algebraic background paper, S.
 Thewes introducing stochastic background paper, authors (Jidling al., providing starting point work.
 References Adams, William W.
 Loustaunau, Philippe.
 introduc- tion Gr¨obner bases.
 Graduate Studies Mathematics.
 American Mathematical Society,
 Barakat, Mohamed.
 Purity ﬁltration ﬁne structure autonomy.
 Proceedings International Symposium Mathematical Theory Networks Sys- tems MTNS pp.
 Budapest, Hungary,
 Bertinet, A.
 Agnan, Thomas C.
 Reproducing Kernel Hilbert Spaces Probability Statistics.
 Kluwer Academic Publishers,
 Blinkov, Yuri A., Cid, Carlos F., Gerdt, Vladimir P., Plesken, Wilhelm, Robertz, Daniel.
 MAPLE Package JANET: I.
 Polynomial Systems.
 II.
 Linear Partial Dif- Proceedings International ferential Equations.
 Workshop Computer Algebra Scientiﬁc Computing, pp.

 (http://www.mathb.
 rwth-aachen.de/Janet).
 Buchberger, Bruno.
 algorithm ﬁnding basis elements residue class ring dimensional polynomial ideal.
 J.
 Symbolic Comput., 41(3-4):475–
 Translated German original Michael P.
 Abramson.
 Calandra, Roberto, Peters, Jan, Rasmussen, Carl E., Deisenroth, Marc P.
 Manifold Gaussian processes regression.
 International Joint Conference Neural Networks, pp.

 doi:

 Chyzak, Fr´ed´eric, Quadrat, Alban, Robertz, Daniel.
 Effective algorithms parametrizing linear control sys- tems Ore algebras.
 Appl.
 Algebra Engrg.
 Comm.
 Comput.,
 Chyzak, Fr´ed´eric, Quadrat, Alban, Robertz, Daniel.
 symbolic package study mul- Applications time tidimensional linear systems.
 delay systems, volume Lecture Notes Con- trol Inform.
 Sci., pp.

 Springer, Berlin,
 (http://www.mathb.rwth-aachen.de/ OreModules).
 Decker, Wolfram, Greuel, Gert-Martin, Pﬁster, Gerhard, Sch¨onemann, Hans.
 SINGULAR 4-1-0 — A computer algebra system polynomial computations.
 http: //www.singular.uni-kl.de,
 Deisenroth, Marc Peter, Fox, Dieter, Rasmussen, Carl Edward.
 Gaussian processes data-efﬁcient learn- ing robotics control.
 IEEE Trans.
 Pattern Anal.
 Mach.
 Intell.,
 Dong, Kun, Eriksson, David, Nickisch, Hannes, Bindel, David, Wilson, Andrew Gordon.
 Scalable log de- terminants gaussian process kernel learning.

 (arXiv:1711.03481).
 Duvenaud, David.
 Automatic Model Construction Gaussian Processes.
 PhD thesis, University Cam- bridge,
 Algorithmic Linearly Constrained Gaussian Processes Eisenbud, David.
 Commutative Algebra View Toward Algebraic Geometry, volume Graduate Texts Mathematics.
 Springer-Verlag,
 Garnett, Roman, Ho, Shirley, Schneider, Jeff G.
 Find- ing galaxies shadows quasars Gaussian processes.
 Bach, Francis R.
 Blei, David M.
 (eds.), ICML, volume JMLR Workshop Conference Proceedings, pp.

 JMLR.org,
 Gerdt, Vladimir P.
 Involutive algorithms computing Gr¨obner bases.
 Computational commutative non- commutative algebraic geometry, volume NATO Sci.
 Ser.
 III Comput.
 Syst.
 Sci., pp.


 Grayson, Daniel R.
 Stillman, Michael E.
 Macaulay2, software system research algebraic geometry.
 http://www.math.uiuc.edu/Macaulay2/.
 Greuel, G.
 Pﬁster, G.
 Singular introduction com- mutative algebra.
 Springer-Verlag,
 contri- butions Olaf Bachmann, Christoph Lossen Hans Sch¨onemann.
 Hensman, James, Fusi, Nicol´o, Lawrence, Neil D.
 Gaus- sian processes big data.
 Proceedings Twenty- Ninth Conference Uncertainty Artiﬁcial Intelligence,
 Honkela, Antti, Peltonen, Jaakko, Topa, Hande, Chara- pitsa, Iryna, Matarese, Filomena, Grote, Korbinian, Stun- nenberg, Hendrik G., Reid, George, Lawrence, Neil D., Rattray, Magnus.
 Genome-wide modeling transcription kinetics reveals patterns rna production delays.
 Proceedings National Academy Sci- ences,
 doi:

 Lee, Jaehoon, Bahri, Yasaman, Novak, Roman, Schoen- holz, Samuel S., Pennington, Jeffrey, Sohl-Dickstein, Jascha.
 Deep neural networks Gaussian processes,
 (arXiv:1711.00165).
 Macˆedo, Ives Castro, Rener.
 Learning divergence-free curl-free vector ﬁelds matrix-valued kernels.
 Instituto Nacional Matematica Pura e Aplicada, Brasil, Tech.

 Oberst, Ulrich.
 Multidimensional constant linear systems.
 Acta Appl.
 Math.,
 Osborne, Michael A., Garnett, Roman, Roberts, Stephen J.
 Gaussian processes global optimization.
 international conference learning intelligent optimization (LION3), pp.

 Quadrat, Alban.
 introduction constructive al- Journ´ees gebraic analysis applications.
 Nationales Calcul Formel, volume Les cours du CIRM, pp.

 CIRM, Luminy,
 (http://ccirm.cedram.org/ccirm-bin/ fitem?id=CCIRM_2010__1_2_281_0).
 Quadrat, Alban.
 Syst`emes Structures – Une approche math´ematique des syst`emes par l’analyse alg´ebrique constructive.
 April
 Habilitation thesis.
 Quadrat, Alban.
 Grade ﬁltration linear functional systems.
 Acta Appl.
 Math.,
 doi: 10.1007/s10440-012-9791-2.
 Rasmussen, Carl Edward Williams, Christopher K.
 I.
 Gaussian Processes Machine Learning (Adaptive Computation Machine Learning).
 MIT Press,
 Izmailov, Pavel, Novikov, Alexander, Kropotov, Dmitry.
 Scalable Gaussian processes billions inducing inputs tensor train decomposition,
 (arXiv:math/1710.07324).
 Robertz, Daniel.
 Recent progress algebraic anal- ysis approach linear systems.
 Multidimensional Syst.
 Signal Process., April
 doi: 10.1007/s11045-014-0280-9.
 Jacobson, N.
 Theory Rings.
 Mathematical surveys monographs.
 American Mathematical Society,
 Jaynes, Edwin T.
 Prior probabilities.
 IEEE Transactions systems science cybernetics,
 Jaynes, Edwin T.
 Bretthorst, G.
 Larry.
 Probability Theory: Logic Science.
 Cambridge University Press,
 Jidling, Carl, Wahlstr¨om, Niklas, Wills, Adrian, Sch¨on, Thomas B.
 constrained Gaussian processes.

 (arXiv:1703.00787).
 Seiler, Werner M.
 Zerz, Eva.
 inverse syzygy prob- lem algebraic systems theory.
 PAMM,
 Simon-Gabriel, C.-J.
 Sch¨olkopf, B.
 Kernel distribution embeddings: Universal kernels, characteristic kernels kernel metrics distributions.
 Technical report,
 (arXiv:1604.05251).
 Snelson, Edward, Rasmussen, Carl Edward, Ghahra- mani, Zoubin.
 Warped gaussian processes.
 Thrun, Sebastian, Saul, Lawrence K., Schlkopf, Bernhard (eds.), NIPS, pp.

 MIT Press,
 Algorithmic Linearly Constrained Gaussian Processes Solin, Arno, Kok, Manon, Wahlstr¨om, Niklas, Sch¨on, Thomas B., S¨arkk¨a, Simo.
 Modeling interpola- tion ambient magnetic ﬁeld Gaussian processes.

 (arXiv:1509.04634).
 Sturmfels, Bernd.
 is...
 Gr¨obner basis?
 Notices AMS,
 Thewes, Silja, Lange-Hegermann, Markus, Reuber, Christoph, Beck, Ralf.
 Advanced Gaussian Process Modeling Techniques.
 Design Experiments (DoE) Powertrain Development.

 Titsias, Michalis K.
 Variational learning inducing vari- ables sparse Gaussian processes.
 Artiﬁcial Intelli- gence Statistics pp.

 Treves, F.
 Topological Vector Spaces, Distributions Kernels.
 Dover books mathematics.
 Academic Press,
 Wahlstr¨om, Niklas, Kok, Manon, Sch¨on, Thomas B., Gustafsson, Fredrik.
 Modeling magnetic ﬁelds using Gaussian processes.
 Proceedings Inter- national Conference Acoustics, Speech, Signal Processing (ICASSP),
 Wilson, Andrew G.
 Adams, Ryan Prescott.
 Gaussian process kernels pattern discovery extrapolation.
 ICML (3), volume JMLR Workshop Conference Proceedings, pp.

 JMLR.org,
 Wilson, Andrew G., Dann, Christoph, Nickisch, Hannes.
 Thoughts scalable Gaussian processes.

 (arXiv:1511.01870).
 Wilson, Andrew G., Hu, Zhiting, Salakhutdinov, Rus- lan, Xing, Eric P.
 Deep kernel learning.

 arXiv:1511.02222).
 Zerz, Eva.
 Topics multidimensional linear systems theory, volume Lecture Notes Control Information Sciences.

 Zerz, Eva, Seiler, Werner M, Hausdorf, Marcus.
 inverse syzygy problem.
 Communications Algebra, (6):2037–2047,
 Learning rank is important research topic infor- mation retrieval data mining, aims learn ranking model produce query-specﬁc ranking list.
 ranking model establishes relationship pair data samples combining corresponding features optimal way [1].
 score is assigned pair evaluate relevance forming global ranking list pairs.
 success learning rank solutions has brought wide spectrum applications, including online advertising [2], natural language processing [3] multimedia retrieval [4].
 Learning appropriate data representation suitable scoring function are vital steps ranking problem.
 Traditionally, feature mapping models data distribu- tion latent space match relevance relationship, scoring function is used quantify relevance measure [1]; however, ranking problem real world emerges multiple facets data patterns are mined diverse domains.
 example, universities are posi- tioned based numerous factors weights used quality evaluation different ranking agencies.
 Therefore, global agreement sources domains be achieved maintaining high ranking performance.
 Multi-view learning has received wide attention special focus subspace learning [5], [6] co-training [7], few attempts have been made ranking problems [8].
 introduces new paradigm model combine information encoded multiple views enhance learning performance.
 Speciﬁcally, subspace learning ﬁnds common space different input modalities using optimization criterion.
 Canonical Correlation Analysis (CCA) [9], [10] is prevailing unsupervised method used measure cross-view correlation.
 contrast, Multi- view Discriminant Analysis (MvDA) [6] is supervised learning technique seeking discriminant features views maximizing between-class scatter minimizing within-class scatter underlying feature space.
 Furthermore, generalized multi-view embedding method [5] was proposed using graph embedding frame- work numerous unsupervised supervised learning techniques extension nonlinear transforms includ- ing (approximate) kernel mappings [11], [12] neural networks [5], [13].
 nonparametric version [5] was proposed [14].
 other hand, co-training [7] was introduced maximize mutual agreement distinct views, be extended multiple inputs training pairs views.
 solution learning rank problem was provided minimizing pairwise ranking difference using same co-training mechanism [8].
 are several applications ben- eﬁt multi-view learning rank topic has been studied date [15].
 Ranking multi-facet objects is performed using com- posite indicators.
 usefulness composite indicator depends selected functional form weights associated component facets.
 Existing solutions university ranking are example using subjective weights method composite indicators.
 However, functional form assigned weights are difﬁcult deﬁne.
 Consequently, is high disparity eval- uation metric agencies, produced ranking lists cause dissension academic institutes.
 How- ever, observation is that, indicators different agencies overlap have high correlation other.
 present example Fig.
 show that, several attributes THE dataset [16], including teaching, research, student staff ratio student number are correlated attributes ARWU dataset [17].
 motivation paper is ﬁnd composite ranking exploiting correlation individual rankings.
 Earlier success multi-view subspace learning pro- vides promising way composite ranking.
 Concatenat- ing multiple views single input overlooks possible view discrepancy does exploit mutual agreement single optimization process.
 rest paper is organized follows.
 Section describe related work proposed methods.
 proposed methods are introduced Section
 Section present quantitative results show effectiveness proposed methods.
 Finally, Section concludes paper.
 RELATED WORK Learning rank Learning rank aims optimize combination data representation ranking problems [18].
 has been used number applications, including image retrieval ranking [4], [19], image quality ratings [20], online advertising [2], text summarization [8].
 Solutions problem be decomposed several key compo- nents, including input feature, output vector scoring function.
 framework is developed training scoring function input feature output ranking list, then, scoring ranking new data.
 Traditional methods include engineering feature using PageRank model [21], example, combine obtaining output.
 Later, research was focused training scoring function improve ranking outputs.
 ranking methods be organized categories scoring function: pointwise approach, pairwise apporach, listwise approach.
 consider pairwise approach paper review related methods follows.
 preference net- work is developed [22] evaluate pairwise order documents.
 network learns preference function binary ranking output using additional scoring function.
 RankNet [23] deﬁnes cross-entropy loss learns neural network model ranking.
 Assuming scoring function be linear ranking problem be transformed binary classiﬁca- tion problem, therefore, many classiﬁers are available be applied ranking document pairs.
 RankBoost [25] adopts Adaboost algorithm [26], focuses classﬁcation errors pair documents, subsequently, improves overall output.
 Ranking SVM [27] applies SVM perform pairwise classiﬁcation.
 GBRank is ranking method based Gradient Boost Tree [28].
 Semi-supervised multi-view ranking (SmVR) [8] follows co-training scheme rank pairs samples.
 Moreover, recent efforts focus using evaluation metric guide gradient respect ranking pair training.
 studies include AdaRank [29], optimizes ranking errors classiﬁcation error adaptive way, LambdaRank [30].
 However, methods consider case single view inputs, multi-view learning rank is overlooked.
 Bipartite ranking pairwise approach ranking methods serves basis ranking method, therefore, reviewed section.
 Suppose training data Fig.
 correlation matrix measurements Times Higher Education (THE) Academic Ranking World Universities (ARWU) rankings.
 data is extracted aligned based performance common universities ranking agencies.
 reddish color indicates high correlation, matrix elements low correlation are represented bluish colors.
 agreement ranking.
 goal is study direct multi-view subspace learning ranking.
 paper offers multi-objective solution ranking capturing relevant information feature mapping view views.
 Moreover, propose end-to-end method optimize trade-off view- speciﬁc ranking discriminant combination multi- view ranking.
 end, improve cross-view ranking performance maintaining individual ranking objectives.
 Intermediate feature representation neural net- work are exploited ranking solutions.
 Speciﬁcally, ﬁrst contribution is provide related methods adopting autoencoder-like network.
 ﬁrst train network learn view-speciﬁc feature mappings, maximize correlation intermediate represen- tations using unsupervised discriminant pro- jection common latent space.
 stochastic optimization method is introduced ﬁt correlation criterion.
 autoencoding sub-network view reconstruc- tion objective feedforward sub-networks joint correlation-based objective are optimized entire network.
 projected feature representations common subspace are combined used learn ranking function.
 second contribution described Fig.
 is end-to-end multi-view learning rank solution.
 sub-network view is trained own ranking objective.
 Then, features intermediate layers sub- networks are combined discriminant mapping common space, training towards global ranking objective.
 result, network assembly is developed enhance joint ranking mimimum view-speciﬁc ranking loss, achieve maximum view i yq i )}, q ∈ is organized query-sample pairs {(xq i ∈ Rd is d-dimensional feature vector {1,


 Q}, xq i ∈ is pair query i-th sample, yq relevance score, number query-speciﬁc samples is Nq. perform pairwise transformation relevance prediction query-sample pair, samples belong same query are evaluated [24].
 modeled probability pair paper is deﬁned pq i (φ) + exp(φ(xi) − φ(xq)) k, y(cid:48) k) = (xq − xi, yq φ x → R is linear scoring function φ(x) = a(cid:62)x, maps input feature vectors scores.
 Due linearity, transform feature vectors relevance score (x(cid:48) i ).
 case ordered list (r) raw input, data sample xi paired query xq is investigated, raw orders (ri, rq) are transformed yq i = ri > rq.
 pairwise ranking, relevance yq i = query sample are relevant, yq i = otherwise.
 k) becomes new feature vector input data nonlinear transforms sub- space learning.
 probability be rewritten feature difference (x(cid:48) i = ri < rq; yq k, y(cid:48) pk(φ) = + exp(−φ(x(cid:48) k)) + exp(−a(cid:62)x(cid:48) k) (1) objective make right order ranking be formulated cross entropy loss such that, (cid:0)yq Q(cid:88) K(cid:88) q=1 Nq(cid:88) (cid:0)y(cid:48) i=1 (cid:96)Rank = arg = min i log pq i + (1 − yq k log pk) + (1 − y(cid:48) i log pq i )(cid:1) k) log pk)(cid:1), (2) k=1 is proved [23] is upper bound pairwise 0-1 loss function optimized using gradient de- scent.
 logistic regression softmax function neural networks be used learn scoring function.
 Multi-view deep learning Multi-view learning considers enhancing feature dis- criminability taking inputs diverse sources.
 important approach follow is subspace learning, is traced CCA [31], [32] input domains, multi-view extension, has been studied [33], [34], [35].
 approach be generalized using higher-order correlation [35].
 main idea techniques is project data representations domains common subspace optimizing mutual correlation.
 Subspace learning supervision has been studied.
 Multi-view Discriminant Analysis [6] performs dimensionality reducation features multiple views exploiting class information.
 Recently, methods were generalized same framework [5], [36], accommodates multiple views, supervision nonlinearity.
 Co-training [7] ﬁrst trains seperate regressors then, maximizes agreements.
 Deep learning, exploits nonlinear transform raw feature space, has been studied multi- view scenario.
 multi-modal deep autoencoder [37] was proposed taking nonlinear representations pair views learn common characteristics.
 Deep CCA [13] is two-view method maximizes pair- wise correlation using neural networks.
 Thereafter, two- view correlated autoencoder was developed [38], [39] objectives correlate view pairs reconstruct individual view same network.
 Multi-view Deep Network [40] was proposed extension MvDA [6].
 optimizes ratio trace graph embedding [41] avoid complexity solutions closed form [42].
 paper, however, show trace ratio optimization be solved updates multi-view networks.
 Deep Multi-view Canonical Correla- tion Analysis (DMvCCA) Deep Multi-view Modular Discriminant Analysis (DMvMDA) [5] are related work, hence, are described following sections.
 Deep Multi-view Canonical Correlation Analysis (DMvCCA) idea DMvCCA [5] is ﬁnd common subspace using set linear transforms W project mapped input samples Zv vth view correlation is maximized.
 Speciﬁcally, aims maximize Tr (cid:18) V(cid:80) V(cid:80) Tr(cid:0) V(cid:80) i=1 i=1 j=1 j(cid:54)=i W(cid:62) (cid:19) (cid:1) W(cid:62) i Zi L Z(cid:62) j Wj i Zi L Z(cid:62) i Wi (3) JDMvCCA = arg max Wv,v=1,...,V matrix L = − N ee(cid:62) centralizes input data matrix view v, e is vector ones,
 deﬁning cross-view covariance matrix views i j ˜Zi ˜Zj, ˜Zv, v


 V is centered Σij data projection matrix W, has column vector Wv vth view, be obtained solving generalized eigenvalue problem  Σ12 ··· Σ1V Σ21 ··· Σ2V

 ΣV ···



  W = λ  Σ11 ··· Σ22 ···


 ··· ΣV V


  W.
 (4) shows solution problem is derived maximal inter-view covariances minimal intra-view covariances.
 Deep Multi-view Modular Discriminant Analysis (DMvMDA) DMvMDA [5] is neural network-based multi-view solu- tion LDA maximizes ratio determinant between-class scatter matrix view pairs within-class scatter matrix.
 Mathematically, is written projection matrix DMvMDA is derived optimizing objective function Tr(cid:0) V(cid:80) V(cid:80) Tr(cid:0) V(cid:80) i=1 j=1 i=1 (cid:1) (cid:1) (5) W(cid:62) i ZiLBZ(cid:62) j Wj W(cid:62) i ZiLW Z(cid:62) i Wi JDMvMDA = arg max Wv,v=1,...,V between-class Laplacian matrix is LB = ep e(cid:62) p − NpNq ep e(cid:62) q ).
 within-class Laplacian matrix is p=1 N q=1 C(cid:88) C(cid:88) LW = − C(cid:88) c=1 MODEL FORMULATION ﬁrst introduce formulation MvCCAE MvM- DAE, extension multi-view subspace learn- ing rank.
 end-to-end ranking method is de- scribed.
 Here, output sub-network Fv is denoted Zv = Fv(Xv).
 Then, have V(cid:88) V(cid:88) V(cid:88) Wi W(cid:62) ∂f ∂Zi j Zj L, j(cid:54)=i j=1 (7) i=1 ∂g ∂Zi i=1 Wi W(cid:62) i Zi L.
 (8) using (7) (8) following quotient rule, derive stochastic optimization MvCCAE be ∂JMvCCAE (cid:19) − f ∂g ∂Zv (cid:18) g2 − ∂ ∂Zv ∂f ∂Zv V(cid:88) (cid:96)AE(Xv;Gv(Fv(·))).
 gradient compute autoencoding loss (cid:96)AE is derived view-speciﬁc sub-networks Fv Gv. sub-network Fv is optimized ∂Zv obtain ∂Fv output Zv, gradient Gv network respect parameters be obtained using chain rule ∂Gv(Xv) ∂Zv (cid:62).
 ecec Nc ∂Zv Multi-view Canonically Correlated Auto-Encoder (MvCCAE) Multi-view Modularly Discriminant Auto-Encoder (MvMDAE) contrast DMvCCA DMvMDA, non- linear correlation multiple views is propose multi-objective solution maximizing between-view correlation minimizing reconstruc- tion error view source.
 Given data matrix X = {X1, X2,


 XV V views, encoding network F decoding network G, projection matrix W, objective MvCCAE is formulated follows, (cid:16) JMvCCAE = arg J (cid:48) (cid:96)AE introduce new objective J (cid:48) DMvCCA, loss function vth autoencoder is (cid:96)AE(Xv;Gv(Fv(·))) = l (cid:107)∇XvF l v(Xv)(cid:107)2, L2 regularization lth intermediate layer vth view denoted Zl v(Xv).
 Here, α ρ are controlling parameters trade-off terms.
 (cid:107)Xv − Gv(Fv(Xv))(cid:107)2 + ρ(cid:80) Xv;Gv(Fv(·)) DMvCCA − α v = F l V(cid:88) (6) (cid:17) Similar MvCCAE, objective MvMDAE is opti- mize view-speciﬁc reconstruction error cross- view correlation follows, JMvMDAE = arg (cid:48) Xv;Gv(Fv(·)) DMvMDA − α V(cid:88) (cid:16) (cid:17) (cid:96)AE (10) Optimization detailed optimization is derived replacing lapla- cian matrix MvCCAE LB LW
 let g = Tr i Wi i Zi LB Z(cid:62) j Wj (cid:33) (cid:33) i=1 j(cid:54)=i j=1 W(cid:62) W(cid:62) (cid:32) V(cid:88) V(cid:88) (cid:32) V(cid:88) i Zi LW Z(cid:62) V(cid:88) V(cid:88) V(cid:88) Wi W(cid:62) j(cid:54)=i j=1 i=1 i=1 Wi W(cid:62) ∂g ∂Zi i=1 i Zi LW
 j Zj LB, (11) (12) f = Tr Optimization Then, have Following objective DMvCCA [5], aim optimize trace ratio (3) let ∂f ∂Zi W(cid:62) i Zi L Z(cid:62) j Wj f = Tr (cid:32) V(cid:88) V(cid:88) (cid:32) V(cid:88) j(cid:54)=i j=1 i=1 i=1 (cid:33) (cid:33) g = Tr W(cid:62) i Zi L Z(cid:62) i Wi stochastic optimization MvMDAE be derived using (11), (12) applying quotient rule follows, ∂JMvMDAE ∂Zv (cid:18) g2 − ∂ ∂Zv (cid:19) − f ∂g ∂Zv (cid:96)AE(Xv,Gv(Fv(·))).
 (13) ∂f ∂Zv V(cid:88) gradient objective be calculated using chain rule, stochastic gradient descent (SGD) is used mini-batches optimization.
 Multi-view Subspace Learning Rank (MvSL2R) Multi-view subspace learning rank is formulated based fact projected feature common sub- space be used train scoring function ranking.
 generate training data intersection ranking samples views have same samples various representations different view origins.
 overall ranking agreement is made calculating average voting intersected ranking orders V(cid:88) r = rv.
 (14) performing pairwise transform section ranking data, have input X = {X1, X2,


 XV V views cross-view relevance scores y obtained average ranking orders r.
 proposed ranking method consists feature mapping common sub- space, training logistic regressor scoring function, predicting relevance new sample pairs using probability function pv(Xv) = + exp(−a(cid:62)W(cid:62) v Fv(Xv)) (15) Wv is data projection matrix vth view, is weight logistic regressor described (1).
 summarize steps algorithm below.
 Multi-view Subspace Learning Rank.
 Function MvSL2R (X, Y, k); Input feature vectors V views X = {X1, X2,


 XV }, relevance y, dimensionality subspace k.
 predicted relevance probabilities p = {p1, p2,


 pV new data.
 Perform nonlinear transformation obtain representation Zv vth autoencoder.
 Perform subspace learning optimizing (6) (10) obtain projection matrix W = [W1W2


 WV ](cid:62).
 Train logistic regressor (1) scoring function obtain weight matrix a.
 Predict new sample pairs relevance probabilities using (15) trained sub-networks F G, obtained weights W a.
 Deep Multi-view Discriminant Ranking (DMvDR) Multi-view Subspace Learning Rank provides promis- ing method MvCCAE MvMDAE.
 does have direct connection ranking.
 Continuing idea multi-objective optimization, propose optimize view-speciﬁc joint ranking single network shown Fig.

 Taking university ranking example, various ranking lists are generated different agencies, agency uses different set attributes represent universities.
 training, given inputs X = {X1, X2,


 XV }, cross entropy loss (2) is optimized view-speciﬁc relevance y joint view relevance y.
 Based evaluation attributes Xv, v =


 V are trained view-speciﬁc sub-network Fv. nonlinear representations Zv = Fv(Xv), v =


 V are inputs joint network H W(cid:62) Zv, v =


 V mappings generate joint university ranking list.
 is input view-speciﬁc network Gv, minimizes distance original ranking rv.
 exploit effectiveness intermediate layers Zv view-speciﬁc sub-networks Fv Gv, ranking loss DMvDR.
 detailed procedure method is described below.
 gradient view-speciﬁc sub-network Gv is calculated output y respect parameters.
 loss passes view-speciﬁc Fv Gv sub- gradient be calculated respect output view-speciﬁc Fv sub-network Then, gradient ∂yv ∂y ∂Gv respect network weights be determined backpropagation [43].
 sub-networks contain several layers Sigmoid functions.
 fused sub-network H is updated gradient ranking loss cross-view relevance scores y.
 Similar generation training data MvSL2R, ﬁnd intersection ranking data different representations measurements various sources, perform pairwise transform have sample pairs input X cross-view ranking orders r (14).
 result, input S fused sub-network H is concatenation nonlinear mapping V view-speciﬁc networks Fv Z1 W(cid:62) Z2


 W(cid:62) ∂Z = ∂y1 S = [W(cid:62) V ZV ](cid:62).
 (16) ∂yv ∂Zv


 ∂ZV ∂Z1 testing, distinguish possible scenarios: (a) samples are aligned presented view, results nonlinear mappings are combined same manner training phase generate fused ranking list p end H sub-network; (b) are missing samples unaligned test data, S = W(cid:62) v Zv vth view.
 resulting view- speciﬁc prediction pv maintains cross-view agree- ment is ranked trained joint network.
 gradient ∂y ∂H be calculated afterwards using SGD.
 ∂S ∂y Joint ranking is achieved using multi-view subspace embedding layer.
 Similar MvMDAE, take map- pings outputs sub-networks Fv. Fig.
 System diagram Deep Multi-view Discriminant Ranking (DMvDR).
 features X = {X1, X2,


 XV are extracted data representations different views fed individual sub-network Fv obtain nonlinear representation Zv vth view.
 results are passed pipelines networks.
 line goes projection W, maps Zv common subspace, concatenation is trained optimize fused ranking loss fused sub-network H.
 other line connects Zv sub-network Gv,∀v =


 V optimization vth ranking loss.
 gradient multi-view subspace embedding (MvSE) trace ratio form is calculated combining (11) (12): ∂JMvSE ∂Zv g2 ∂f ∂Zv − f ∂g ∂Zv (17) (cid:18) (cid:19) embedding layer is important gradient is passed fused sub-network H.
 Meanwhile, is back- ward propagated layers Fv reach input Xv. turn, parameters Gv are affected outputs Fv sub-networks.
 update view-speciﬁc Fv depends view- speciﬁc ranking output y cross-view relevance y is common sub-network pipelines networks.
 v-th sub-networks Fv Gv are optimized respect gradient ∂y
 Meanwhile, training error respect ∂Xv fused ranking y is passed multi-view subspace embedding (MvSE) S (16) input fused sub-network H.
 resulting gradient sub-network Fv is given ∂JDMvDR (cid:96)Rank(Xv, yv;Gv(Fv(·))) − α ∂Zv V(cid:88) (cid:96)Rank(S, y;H(·)), ∂Zv ∂JMvSE ∂Zv − β ∂Zv (18) α β are scaling factors controlling magnitude ranking loss.
 Similar other sub- gradients respect parameters be obtained following chain rule.
 update entire network DMvDR be summarized using SGD mini-batches.
 parameters sub-network are denoted θ = {θF1 θFv


 θFV θG1, θGv


 θGV θH}.
 gradient de- ∂θJDMvDR, η is learning scent step is ∆θ = −η ∂ rate.
 gradient update step time t be written chain rule collectively:


 ∆θtFV


 ∆θtGV ∆θtH} ∆θt ={∆θtF1 ∆θtG1 = − ∂(cid:96)rank ∆θtGv ∂y ∆θtH = − ∂(cid:96)rank ∂y ∂JMvSE ∂Zv − ∂(cid:96)rank ∂y ∆θtFv ∆θtF2 ∆θtG2 · ∂y ∂Gv · ∂y ∂H · ∂Zv ∂Fv · ∂y ∂S · ∂y ∂Zv · ∂Zv ∂Fv − ∂(cid:96)rank ∂y · ∂S ∂Zv · ∂Zv ∂Fv (19) generate training data using pairwise transform presented Section testing, test samples be transformed pairs evaluate relative relevance sample query.
 raw ranking data be fed trained model predict overall ranking positions.
 Fused Ranking lossXRankingloss1Ranking loss loss VFGHABCDEABCDEABCDEABCDEABCDEABCDEABCDEABCDE1XVX2WVWW2W1WZ1Z2ZV EXPERIMENTS section, evaluate performance proposed multi-view learning rank methods challeng- ing problems: university ranking, multi-linguistic ranking image data ranking.
 proposed methods are compared related subspace learning co-training methods.
 subspace learning methods follow steps proposed Section ranking.
 compare perfor- mance following methods experiments: • Best Single View: method shows best performance Ranking SVM [27] individ- ual views.
 Feature Concat: method concatenate fea- tures common samples training Ranking SVM [27].
 • LMvCCA [5]: linear multi-view CCA method.
 • LMvMDA [5]: linear supervised method multi- view subspace learning.
 • MvDA [6]: linear supervised method multi-view subspace learning.
 differs above view difference is encoded method.
 • SmVR [8]: semi-supervised method seeks global agreement ranking.
 belongs cat- egory co-training.
 develop complete data following experiments training comparison subspace learning methods is fair.
 Therefore, SmVR becomes supervised method paper.
 • DMvCCA [5]: nonlinear extension LMvCCA • DMvMDA [5]: nonlinear extension LMvMDA using neural networks.
 using neural networks.
 • MvCCAE: ﬁrst proposed multi-view subspace learning rank method proposed paper.
 • MvMDAE: supervised multi-view subspace learning rank method proposed paper.
 • DMvDR: end-to-end multi-view learning rank method proposed paper.
 present quantitative results using several evaluation metrics including Mean Average Precision (MAP), clas- siﬁcation accuracy Kendal’s tau.
 Average Precision (AP) measures relevance query sample pairs respect same query, MAP score calcu- lates mean AP queries [44].
 performing pairwise transform ranking data, relevance pre- diction be considered binary classiﬁcation problem, classiﬁcation is utilized evaluation.
 Kendal’s tau measures ordinal association lists samples.
 present experimental results graphically, following measures are used.
 Mean Average Preci- sion (MAP) score, is average precision ranks recall changes, is illustrated 11-point interpo- lated precision-recall curves (PR curve) show ranking performance.
 ROC curve provides graphical representation binary classiﬁcation performance.
 shows true positive rates false positive rate different thresholds.
 correlation plots show linear correlation coefﬁcients ranking lists.
 University Ranking university ranking dataset available Kaggle.com [45] collects world ranking data rating agencies, including Times Higher Education (THE) World Univer- sity Ranking, Academic Ranking World Universities (ARWU), Center World University Rankings (CWUR).
 political controversial inﬂuences, are considered authorities university ranking.
 measurements are used feature vectors feature preprocessings, includes feature standardiza- tion removal categorical variables groundtruth indicators including ranking orders, university name, location, year total scores.
 common universities are considered training.
 pair- wise transform year, samples are generated training data.
 entire data is considered testing.
 data distribution (after binary transform) common universities is shown Fig.

 (a) Raw data distribu- tion.
 Projected data distri- bution.
 Fig.
 left plot shows data distribution con- catenating measurements features common universities different agencies
 right plot shows concatenated projected features using MvMDAE same universities.
 make several observations data distri- bution Fig.

 pairwise transform is applied university ranking data, assigns original ranking data classes.
 dimension- ality data is reduced 2-dimensional using PCA order display plots Fig.

 data is labelled colors red green indicating relevance samples.
 notice high overlap classes case raw data (left plot Fig.
 data right is separated projection using proposed MvMDAE.
 shows discrimination power proposed supervised embedding method.
 Furthermore, rank correlation matrix plots is pre- sented Fig.
 correlations pairs ranking lists views predicted list denoted ’Fused’.
 Histograms ranking data are shown matrix diagonal, scatter plots data pairs appear diagonal.
 slopes least-squares reference lines scatter plots are equal displayed correlation coefﬁcients.
 fused ranking list is produced pro- posed DMvDR, results are generated common universities
 ﬁrst take closer look TABLE Average Prediction Results (%) University Ranking Datasets
 Methods Best Single View Feature Concat LMvCCA [5] LMvMDA [5] MvDA [6] SmVR [8] DMvCCA [5] DMvMDA [5] MvCCAE (ours) MvMDAE (ours) DMvDR (ours) Kendal’s tau Accuracy Fig.
 Rank correlation matrix views fused view.
 correlations views 1-3.
 correlation co- efﬁcients are low, highest (0.81) view others are
 contrast, fused rank has high correlation view.
 scatter plots reference lines are correlation coefﬁcients are demonstrating proposed DMvDR exploits global agreement view.
 average prediction results different university datasets proposed competing methods are reported Table
 Due misalignment ranking data datasets, make ranking prediction based view input, is elaborated Section observe Ranking SVM [27] single feature concatenation performs compared other methods.
 shows data is heterogeneous, combining features cannot enhance joint ranking.
 Kendal’s tau linear subspace learning methods are higher nonlinear counterparts.
 is due fact nonlinear methods aim maximize correlation embedding space, scoring function is optimized ranking.
 contrast, DMvDR optimizes entire ranking process, is conﬁrmed highest ranking classiﬁcation performance.
 Multi-lingual Ranking Multi-lingual Ranking is performed Reuters RCV1/RCV2 Multi-lingual, Multi-view Text Categorization Test collection [3].
 use Reuters indicate dataset paragraphs.
 is large collection documents news ariticles written ﬁve languages, grouped categories topic.
 bag words (BOW) based TF-IDF weighting method [44] is used represent documents.
 vocabulary has size average is sparse.
 views are numbered follows: • View original English documents; • View English documents translated French; • View English documents translated German; • View English documents translated Italian; • View English documents translated Spanish.
 Due high dimensionality, BOW representation document is projected using sparse SVD 50- dimensional compact feature vector.
 randomly select samples category view training data.
 training data composed samples is generated pairs English documents based pairwise transform Section translations other languages are used augmenting views.
 select samples categories create test dataset document pairs.
 considering ranking function linear proved [24], make document pairs comparable balance assigning data other class opposite sign feature vectors, number samples is distributed classes.
 ﬁrst PR ROC curves Fig.

 have translations English documents, sample is aligned views perform joint learning prediction multi-lingual ex- periments.
 experiments start views English translation French, views are aug- mented documents other languages.
 Subspace ranking methods are trained embedding increasing number views, while SmVR co-training takes views time, average performance pairs is reported.
 proposed methods competing ones are included plots Fig.

 proposed DMvDR performs best views be seen PR ROC plots Fig.

 SmVR is second lower precision less area curve compared DMvDR.
 remaining methods, DMvMDA performs PR curves ROC plots.
 results are consistent views.
 consider English documents transla- tions other languages experiment.
 observe quantitative MAP accuracy results Table
 shows linear methods (a) PR curve Reuters.
 Fig.
 PR ROC curves 2-5 views applied Reuters dataset.
 (b) ROC curve Reuters.
 TABLE Quantitative Results (%) Reuter Dataset.
 Methods Feature Concat LMvCCA [5] LMvMDA [5] MvDA [6] SmVR [8] DMvCCA [5] DMvMDA [5] MvCCAE (ours) MvMDAE (ours) DMvDR (ours) views views views views MAP@100 Accuracy MAP@100 Accuracy MAP@100 Accuracy MAP@ Accuracy viewsSmVRDMvMDAMvCCAEMvMDAEDMvDR00.51Recall0.550.60.650.70.750.80.850.90.95Precision3 views00.51Recall0.550.60.650.70.750.80.850.90.95Precision4 views00.51Recall0.50.550.60.650.70.750.80.850.90.95Precision5 views00.51FP Rate00.10.20.30.40.50.60.70.80.91TP Rate2 viewsSmVRDMvMDAMvCCAEMvMDAEDMvDR00.51FP Rate00.10.20.30.40.50.60.70.80.91TP Rate3 views00.51FP Rate00.20.40.60.81TP Rate4 views00.51FP Rate00.20.40.60.81TP Rate5 views feature concatenation have similar results are inferior nonlinear methods classiﬁca- tion.
 Note nonlinear subspace learning methods cannot provide superior MAP scores, be explained fact embedding is intended construct discrimative feature space classifying pairs data.
 observe MAP scores accuracies are stable views.
 be interpreted global ranking agreement be reached certain level languages correspond other.
 is conﬁrmed end-to-end solution provides highest scores, SvMR is few percent- ages behind.
 features different views follow similar data distribution, co-training method performs competes proposed DMvDR.
 Image Data Ranking Image data ranking is problem evaluate relevance images represented different types fea- tures.
 adopt Animal Attributes (AWA) dataset [46] problem due diversity animal appear- ance large number classes.
 dataset is composed animal classes total images, animal attributes.
 follow feature generation [5] adopt feature types forming views: Label Encoding: Image Feature VGG-16 pre-trained model: 1000- dimensional feature vector is produced image resizing × taken outputs f c8 layer 16-layer VGGNet [47].
 • Class 100-dimensional Word2Vector is extracted class label.
 map visual feature image text feature space using ridge regressor similar setting [5] genenate set textual feature, connection visual world.
 text embedding space is constructed training skip-gram [48] model entire English Wikipedia articles, including words.
 • Attibute Encoding: 85-dimensional feature ve- tor be produced similar idea above.
 class animals contains typical patterns attribute, × lookup table be constructed connect classes attributes [49], [50].
 map image feature attribute space produce mid-level feature.
 generate image ranking data follows.
 classes animal images, ﬁnd pairs images in-class pairs out-of-class image pairs class.
 end training data pairs.
 Similarly, have test data pairs.
 select images class used training data separate set samples test data.
 images are used queries: are associated in-class images positive sample pairs negative sample pairs.
 negative sample pairs, randomly select classes remaining animal classes time, image class is associated query image study.
 TABLE Quantitative Results (%) AWA Dataset.
 Fig.
 PR ROC curves AWA.
 Methods Feature Concat LMvCCA [5] LMvMDA [5] MvDA [6] SmVR [8] DMvCCA [5] DMvMDA [5] MvCCAE (ours) MvMDAE (ours) DMvDR (ours) MAP@100 Accuracy observe performance methods animal dataset Fig.
 Table
 DMvDR outperforms other competing methods large margin shown plots Fig.

 Due variety data distribution different feature types view inputs, co-training type SmVR compete end-to-end solution.
 Table observe performance feature concatenation suffers same problem.
 other hand, proposed subspace ranking methods produces satisfactory classiﬁcation rates precisions remain low.
 implies scoring function is critical be trained feature mappings.
 other linear nonlinear subspace ranking methods have similar performance lower position.
 CONCLUSION Learning rank has been popular research topic numerous applications, multi-view ranking remains new research topic.
 paper, aimed associate multi-view subspace learning methods ranking problem proposed methods direction.
 MvCCAE is unsupervised multi-view embed- ding method, MvMDAE is supervised counter- part.
 incorporate multiple objectives, Rate00.20.40.60.81TP RateSmVRDMvMDAMvCCAEMvMDAEDMvDR correlation maximization hand, reconstruction error minimization other hand, have been ex- tended multi-view subspace learning rank scheme.
 Finally, DMvDR is proposed exploit global agreement minimizing individual ranking losses single optimization process.
 experimental results validate superior performance DMvDR compared other subspace co-training methods multi-view datasets homogeneous heterogeneous data represen- tations.
 future, explore scenario exists missing data, is scope current proposed subspace ranking methods training.
 Mul- tiple networks be combined concatenating outputs, further optimized single sub-network.
 solution be applicable homogeneous represen- tations.
 REFERENCES T.-Y.
 Liu, “Learning rank information retrieval,” Foundations Trends Information Retrieval, vol.
 no.
 pp.

 [2] Y.
 Zhu, G.
 Wang, J.
 Yang, D.
 Wang, J.
 Yan, J.
 Hu, Z.
 Chen, “Optimizing search engine revenue sponsored search,” Pro- ceedings international ACM SIGIR conference Research development information retrieval.
 pp.

 [3] M.
 Amini, N.
 Usunier, C.
 Goutte, “Learning multiple observed views application multilingual text cat- egorization,” Advances Neural Information Processing Systems
 Curran Associates, Inc., pp.

 J.
 Yu, D.
 Tao, M.
 Wang, Y.
 Rui, “Learning rank using user clicks visual features image retrieval,” IEEE transactions cybernetics, vol.
 no.
 pp.

 [4] [5] G.
 Cao, A.
 Iosiﬁdis, K.
 Chen, M.
 Gabbouj, “General- ized multi-view embedding visual recognition cross- modal retrieval,” IEEE Transactions Cybernetics, doi:
 [6] M.
 Kan, S.
 Shan, H.
 Zhang, S.
 Lao, X.
 Chen, “Multi-view discriminant analysis,” IEEE Transactions Pattern Analysis Machine Intelligence (T-PAMI), vol.
 no.
 pp.
 Jan
 [7] A.
 Blum T.
 Mitchell, “Combining labeled unlabeled data co-training,” Proceedings eleventh annual conference Computational learning theory.
 pp.

 [8] N.
 M.-R.
 Amini, C.
 Goutte, “Multiview semi- supervised learning ranking multilingual documents,” Machine Learning Knowledge Discovery Databases, pp.

 [9] D.
 R.
 Hardoon, S.
 Szedmak, J.
 Shawe-Taylor, “Canonical correlation overview application learning methods,” Neural computation, vol.
 no.
 pp.

 [10] J.
 Rupnik J.
 Shawe-Taylor, “Multi-view canonical correlation analysis,” Slovenian KDD Conference Data Mining Data Warehouses (SiKDD pp.

 [11] A.
 Iosiﬁdis, A.
 Tefas, I.
 Pitas, “Kernel reference discriminant analysis,” Pattern Recognition Letters, vol.
 pp.

 [12] A.
 Iosiﬁdis M.
 Gabbouj, “Nystr¨om-based approximate kernel subspace learning,” Pattern Recognition, vol.
 pp.

 [13] G.
 Andrew, R.
 Arora, J.
 Bilmes, K.
 Livescu, “Deep canonical correlation analysis,” Proceedings 30th International Confer- ence Machine Learning, pp.

 [14] G.
 Cao, A.
 Iosiﬁdis, M.
 Gabbouj, “Multi-view nonparametric discriminant analysis image retrieval recognition,” IEEE Signal Processing Letters, vol.
 no.
 pp.
 Oct
 [15] F.
 Feng, L.
 Nie, X.
 Wang, R.
 Hong, T.-S.
 Chua, “Computational social indicators: case study chinese university ranking,” Proceedings 40th International ACM SIGIR Conference Research Development Information Retrieval.
 New York, NY, USA: ACM, pp.

 [16] “The times higher education world university ranking,” https://www.timeshighereducation.com/world-university- rankings,
 [17] N.
 C.
 Liu Y.
 Cheng, “The academic ranking world univer- sities,” Higher education Europe, vol.
 no.
 pp.

 T.
 Liu, J.
 Wang, J.
 Sun, N.
 Zheng, X.
 Tang, H.-Y.
 Shum, “Picture collage,” IEEE Transactions Multimedia (TMM), vol.
 no.
 pp.

 [19] X.
 Li, T.
 Pi, Z.
 Zhang, X.
 Zhao, M.
 Wang, X.
 Li, P.
 S.
 Yu, “Learn- ing bregman distance functions structural learning rank,” IEEE Transactions Knowledge Data EngineeringS, vol.
 no.
 pp.

 [20] O.
 Wu, Q.
 You, X.
 Mao, F.
 Xia, F.
 Yuan, W.
 Hu, “Listwise learn- ing rank exploring structure objects,” IEEE Transactions Knowledge Data Engineering, vol.
 no.
 pp.

 [21] L.
 Page, S.
 Brin, R.
 Motwani, T.
 Winograd, “The pagerank citation ranking: Bringing order web.” Stanford InfoLab, Tech.

 [22] W.
 W.
 Cohen, R.
 E.
 Schapire, Y.
 Singer, “Learning order things,” Advances Neural Information Processing Systems, pp.

 [23] C.
 Burges, T.
 Shaked, E.
 Renshaw, A.
 Lazier, M.
 Deeds, N.
 Hamil- ton, G.
 Hullender, “Learning rank using gradient descent,” Proceedings international conference Machine learning.
 pp.

 [24] R.
 Herbrich, T.
 Graepel, K.
 Obermayer, “Large margin rank boundaries ordinal regression,”
 [25] Y.
 Freund, R.
 Iyer, R.
 E.
 Schapire, Y.
 Singer, “An efﬁcient boosting algorithm combining Journal machine learning research, vol.
 pp.

 [26] Y.
 Freund R.
 E.
 Schapire, “A desicion-theoretic generalization on-line learning application boosting,” European conference computational learning theory.
 Springer, pp.

 [27] T.
 Joachims, “Optimizing search engines using clickthrough data,” Proceedings eighth ACM SIGKDD international conference Knowledge discovery data mining.
 pp.

 [28] J.
 H.
 Friedman, “Greedy function approximation: gradient boost- ing machine,” Annals statistics, pp.

 [29] J.
 Xu H.
 Li, “AdaRank: boosting algorithm information retrieval,” Proceedings 30th annual international ACM SI- GIR conference Research development information retrieval.
 pp.

 [30] C.
 J.
 Burges, R.
 Ragno, Q.
 V.
 Le, “Learning rank nons- mooth cost functions,” Advances neural information processing systems, pp.

 [31] H.
 Hotelling, “Relations sets variates,” Biometrika, [32] M.
 Borga, “Canonical correlation: tutorial,” http://people.imt.
 pp.

 liu.se/∼magnus/cca/tutorial/tutorial.pdf,
 [33] A.
 A.
 Nielsen, “Multiset canonical correlations analysis mul- tispectral, multitemporal remote sensing data,” IEEE Trans- actions Image Processing (TIP), vol.
 no.
 pp.

 [34] Y.
 Gong, Q.
 Ke, M.
 Isard, S.
 Lazebnik, “A multi-view embed- ding space modeling internet images, tags, seman- tics,” International Journal Computer Vision, vol.
 no.
 pp.

 [35] Y.
 Luo, D.
 Tao, K.
 Ramamohanarao, C.
 Xu, Y.
 Wen, “Tensor canonical correlation analysis multi-view dimension reduc- tion,” IEEE Transactions Knowledge Data Engineering, vol.
 no.
 pp.
 Nov
 [36] A.
 Sharma, A.
 Kumar, H.
 Daume III, D.
 W.
 Jacobs, “General- ized multiview A discriminative latent space,” Proceed- ings IEEE Conference Computer Vision Pattern Recognition (CVPR).
 IEEE, pp.

 [37] J.
 Ngiam, A.
 Khosla, M.
 Kim, J.
 Nam, H.
 Lee, A.
 Y.
 Ng, “Multimodal deep learning,” Proceedings international conference machine learning (ICML-11), pp.

 [38] W.
 Wang, R.
 Arora, K.
 Livescu, J.
 Bilmes, “On deep multi-view representation learning,” Proceedings International Conference Machine Learning (ICML), pp.

 [39] S.
 Chandar, M.
 M.
 Khapra, H.
 Larochelle, B.
 Ravindran, “Correlational neural networks,” Neural computation,
 [40] M.
 Kan, S.
 Shan, X.
 Chen, “Multi-view deep network cross-view classiﬁcation,” Proceedings IEEE Conference Computer Vision Pattern Recognition, pp.

 [41] S.
 Yan, D.
 Xu, B.
 Zhang, H.-J.
 Zhang, Q.
 Yang, S.
 Lin, “Graph embedding extensions: general framework dimension- ality reduction,” IEEE Transactions Pattern Analysis Machine Intelligence (T-PAMI), vol.
 no.
 pp.

 [42] Y.
 Jia, F.
 Nie, C.
 Zhang, “Trace ratio problem revisited,” IEEE Transactions Neural Networks, vol.
 no.
 pp.

 Y.
 LeCun, L.
 Bottou, G.
 B.
 Orr, K.-R.
 M ¨uller, “Efﬁcient back- Springer, pp.
 prop,” Neural networks: Tricks trade.

 [44] C.
 D.
 Manning, P.
 Raghavan, H.
 Sch ¨utze al., Introduction Cambridge university press Cambridge, information retrieval.
 vol.
 no.

 [45] “World university rankings: kaggle dataset.” https://www.kaggle.com/mylesoneill/world-university- rankings,
 [46] C.
 H.
 Lampert, H.
 Nickisch, S.
 Harmeling, “Attribute-based classiﬁcation zero-shot visual object categorization,” IEEE Transactions Pattern Analysis Machine Intelligence (T-PAMI), vol.
 no.
 pp.

 [47] K.
 Simonyan A.
 Zisserman, “Very deep convolutional net- works large-scale image recognition,” International Conference Learning Representations (ICLR),
 [48] T.
 Mikolov, I.
 Sutskever, K.
 Chen, G.
 S.
 Corrado, J.
 Dean, “Distributed representations words phrases com- positionality,” Advances neural information processing systems pp.

 [49] C.
 Kemp, J.
 B.
 Tenenbaum, T.
 L.
 Grifﬁths, T.
 Yamada, N.
 Ueda, “Learning systems concepts inﬁnite relational model,” AAAI, vol.
 p.

 [50] D.
 N.
 Osherson, J.
 Stern, O.
 Wilkie, M.
 Stob, E.
 E.
 Smith, “Default probability,” Cognitive Science, vol.
 no.
 pp.

 variety application areas, processing systems are scheduled maintain stability meet various other objectives.
 Indeed, basic prob- lem scheduling theory has been ﬁnd study poli- cies accomplish task diﬀerent modeling assumptions.
 practice however, human experts be able manage real-world processing systems, is non-trivial quantify costs objectives govern expert schedulers.
 example, operating room scheduling, ad hoc metrics have been ap- plied attempt model cost delays, e.g. [1], metrics are subjective.
 Delphi Method is used management science model expert opinions such methods have algorith- mic guarantees are reliable [2].
 paper, present online algorithm allows emulate expert scheduler based observations backlog queues system observations expert’s scheduling decisions.
 use term “emu- late” mean parametric form learned policy converge parametric form ex- pert policy cases, yield scheduling de- cisions average converge expert’s decisions.
 oﬀers data-driven way designing autonomous scheduling systems.
 consider projective cone scheduling (PCS) model has applications manufacturing, call/service centers, communication networks [3,
 Email address: neal.m.master@berkeley.edu (Neal Master) algorithm paper uses multiplicative weight update (MWU) method [5].
 MWU method has been used several areas including solving zero-sum games [6], solving linear programs [7], inverse opti- mization [8].
 PCS policy be written maximization, techniques are similar used [8].
 authors apply MWU algorithm ﬁxed horizon learn objective expert is solving sequence linear programs.
 results diﬀer [8] several ways.
 is queue- ing dynamics consider, expert’s objective vary time whereas [8] objective is constant.
 related issue is [8], expert has decision variable dimension dimension parameter being learned is n.
 case, expert has decision variable dimension (i.e. are n queues system), need estimate Θ(n2) parameters.
 note paper provide algorithm be applied horizon is known priori.
 goal inferring parts optimization model data is well-studied problem many other ap- plications.
 example, genetic algorithm heuristics have been applied estimate objective constraints linear program data envelopment analysis context [9].
 goal imputing objective function convex optimization problem has been considered opti- mization community, e.g.
 papers rely heav- convexity objective feasible set.
 approach does apply PCS context set feasible scheduling actions is discrete non-convex.
 Preprint January paper is related inverse reinforcement learn- ing.
 Inverse reinforcement learning is problem esti- mating rewards Markov decision process (MDP) given observations MDP evolves opti- mal policy [12].
 Inverse reinforcement learning be used emulate expert decision makers (referred “appren- ticeship learning” machine learning community) underlying dynamics are Markovian [13].
 PCS model, such assumption is made results do require Markovian dynamics.
 remainder paper is organized follows.
 Sec- tion speciﬁes PCS model consider.
 Section presents algorithms relevant guarantees.
 Be- cause take MWU approach problem, guar- antees are bounds average loss.
 However, provide concentration bound gives guarantees tail loss distribution.
 provide simple nu- merical demonstration algorithms Section
 Section discuss extensions results conclude Section

 Projective Cone Scheduling Dynamics section summarize PCS model presented [4] comment connection model pre- sented [3].
 PCS model has n queues inﬁnite waiting room following arbitrary queueing dis- cipline.
 Time is discretized slots t ∈ Z+
 backlog queue i beginning time slot t is xt(i).
 back- log queues be written vector xt ∈ Zn +.
 number customers arrive queue i end time slot t is at(i).
 arrivals queues be written vector ∈ Zn +.
 Scheduling conﬁg- urations are chosen ﬁnite set S (cid:40) Zn con- +.
 ﬁguration st ∈ S is chosen time slot t queue i, min{st(i), xt(i)} customers are served be- ginning time slot.
 take departure vector dt = min{st, xt} ∈ Zn + minimum is taken component-wise.
 gives following dynamics xt+1 = xt − dt + (1) x0 ∈ Zn + is arbitrary.
 Note arrival vector is allowed depend previous scheduling conﬁgurations, previous arrivals, previous backlogs arbitrary way.
 scheduling conﬁgurations are chosen solving maximization s∈S (cid:104)s, Bxt(cid:105) = max s∈S max (cid:88) i,j s(i)B(i, j)xt(j) (2) B ∈ Rn×n is symmetric positive-deﬁnite non-positive oﬀ-diagonal elements.
 assume S is endowed arbitrary ordering used breaking ties.
 PCS policy deﬁnes broad class scheduling policies particular note taking B identity matrix, return typical maximum weight matching scheduling algorithm.
 B is matrix, B is symmetric, are p = n(n + n2 free param- eters need be learned.
 Consequently, represent projective cone scheduler upper- triangular array matrix.
 particular, take b(i, i) ∝ B(i, i) i ∈ [n] b(i, j) ∝ −B(i, j) (i, j) ∈ {(i, j) ∈ [n] × [n] i < j}2.
 assume i,j b(i, j) =
 write projective cone scheduling decision fol- lows: loss generality (cid:80) B(i, j)s(i)xt(j) B(i, i)s(i)xt(i) B(i, j)(s(i)xt(j) + s(j)xt(i)) (cid:111) (cid:111) st = arg max = arg max s∈S = arg max s∈S i=1 j=1 s∈S (cid:104)s, Bxt(cid:105) n(cid:88) (cid:110)(cid:88) (cid:88) (cid:110)(cid:88) −(cid:88) i<j = arg max s∈S b(i, i)s(i)xt(i) i<j (cid:44) µ(xt; b) convenience, let deﬁne b(i, j)(s(i)xt(j) + s(j)xt(i)) (3) (4) i = j i (cid:54)= j −1 σ(i, j) = (cid:88) i≤j (cid:26) write follows: µ(xt; b) = arg max s∈S σ(i, j)b(i, j)(s(i)xt(j) + s(j)xt(i)) (5) Note deﬁne normalized version xt, follows, yt = xt xt/(cid:107)xt(cid:107)1 (cid:107)xt(cid:107)1 = (6) have st = µ(xt; b) = µ(yt; b).
 Modeling customer having uniform determin- istic service time is motivated applications computer systems particular, packet switch schedul- ing.
 However, PCS models non-deterministic service times have been considered literature [3].
 How- ever, results [3] apply case B is diagonal.
 have opted present algorithms use notation Z+ = {0,

 .}.
 use notation [k] = {1,


 k}.
 context non-diagonal case feel hav- ing Θ(n2) parameters is interesting having Θ(n) parameters.
 algorithms be applied case stochastic service times; is discussed other extensions Section
 note previous literature PCS [3, has required variety additional assumptions.
 ex- ample, [4] is assumed arrival process is mean ergodic.
 do require such assumption more- over, results [3] [4] are stability guarantees, make assumptions stability system.

 Learning Algorithm section present algorithm.
 ﬁrst present ﬁnite horizon algorithm leverage present inﬁnite horizon algorithm.
 show average error is O(ln(T )(cid:112)ln(p)/T ).
 provide bound fraction observations error exceeds average case bound.
 algorithms are applied online fash- ion.
 do focus computational issues, note computing µ(yt; b) is diﬃcult prob- lem.
 However, are local search heuristics allow eﬃcient computation µ(yt+1; b) based solution µ(yt; b) [14].
 algorithms require computation µ(yt; ˆbt) ˆbt is estimate b time t online algorithm is appropriate want use previous solution warm start.
 presenting algorithms, consider loss function interest.
 expert are trying emulate is speciﬁed array seem reasonable want estimates ˆbt converge b.
 However, goal is reasonable seem.
 S is discrete, is possible diﬀerent values b render same scheduling decisions.
 Consequently, goal recovering b be ill-posed.
 aim emulate expert scheduler want ˆst = µ(yt; ˆbt), scheduling decision induced estimate ˆbt, be same st, expert’s scheduling decision.
 Hence, loss penalize discrepancies st ˆst.
 leads consider ˆbt ˆst loss time t is (cid:88) i≤j δt = ˆst − st.
 b = ˆbt, have st = ˆst (cid:96)t =
 addition, st = ˆst have (cid:96)t b (cid:54)= ˆbt.
 deﬁnition µ allow show (cid:96)t ≥
 advantage loss function is allows give guarantees are independent statistics arrival process.
 example, suppose are arrivals subset queues.
 case, be unreasonable expect be able estimate rows columns b relevant queues.
 generally, arrival process suﬃciently modes system.
 considering ˆst ˆbt simul- taneously, provide bounds presence pathological arrival processes.
 A Finite Horizon Algorithm ﬁrst present Algorithm ﬁnite horizon algorithm requires knowledge horizon.
 is multiplicate weights update algorithm time hori- zon is used set learning rate.
 Online Parameter Learning Fixed Horizon Input ((y1, s1),


 (yT sT )) observations Output: (ˆb1,


 ˆbT parameter estimates Output: (ˆs1,


 ˆsT scheduling estimates η ←(cid:112)ln(p)/T ˆbt ← wt/(cid:80) w1 ← triangular array t


 T do i≤j wt(i, j) ˆst ← µ(yt; ˆbt) mt ← triangular array δt = ˆst − st ˆst (cid:54)= st zt = δt/(cid:107)δt(cid:107)∞ (i, j) ∈ [n]2 i ≤ j do mt(i, j) ← σ(i, j)(zt(i)yt(j) + zt(j)yt(i)) end end wt+1 ← wt(1 − ηmt) component-wise end Theorem
 Let D = max (cid:107)u − v(cid:107)∞ p = n(n +
 following inequality: T > ln(p) output (cid:114) T(cid:88) t=1 Proof.
 Note mt ∈ [−1, η < apply [5, Corollary (cid:96)t = σ(i, j)(ˆbt(i, j)−b(i, j))(δt(i)yt(j)+δt(j)yt(i)) (7) ≤ (cid:96)t ln(p) (8) T(cid:88) t=1 (cid:88) ≤ T(cid:88) i≤j (cid:88) mt(i, j)ˆbt(i, j) t=1 i,j (mt(i, j) + η |mt(i, j)|)b(i, j) + ln(p) (9) i≤j ˆbt(i, j) = have |mt(i, j)| ≤ (cid:80) following: T(cid:88) t=1 (cid:88) ≤ T(cid:88) i≤j (cid:88) mt(i, j)ˆbt(i, j) mt(i, j)b(i, j) + ηT + (10) ln(p) i,j t=1 is minimized η = (cid:112)ln(p)/T
 Rearranging in- A straightforward calculation shows upper bound equality applying fact give following: T(cid:88) (cid:88) t=1 (cid:114) i≤j ≤ ln(p) mt(i, j)ˆbt(i, j) − T(cid:88) (cid:88) t=1 i≤j mt(i, j)b(i, j) apply speciﬁcs mt.
 deﬁnition D, (cid:107)δt(cid:107)∞ ≤ D.
 gives following: σ(i, j)(δt(i)yt(j) + δt(j)yt(i))ˆbt(i, j) σ(i, j)(−δt(i)yt(j) − δt(j)yt(i))b(i, j) Algorithm Online Parameter Learning Unknown Horizon Input ((y1, s1), (y2, s2),

 observations Output: (ˆb1, ˆb2,

 parameter estimates Output: (ˆs1, ˆs2,

 scheduling estimates T−1 (cid:44) Tk (cid:44) ln(p)) k ∈ {0,

 .}.
 t =


 do Tk < t ≤ Tk+1 Apply Algorithm T ≡ Tk re-initializing wt end end Theorem
 Suppose T ≥ T0.
 Deﬁne lg(·) (cid:100)log2(·)(cid:101).
 output Algorithm satiﬁes following in- equality: T(cid:88) t=1 ≤ (cid:96)t ≤ lg (cid:19)(cid:114) (cid:18) T0 ln(p) (13) Note are same bounds Theorem additional factor lg(2T /T0).
 Proof.
 note proof [5, Corollary does require initial weights be uniform Theorem applies initialization line Algorithm
 convenience, let Uk = (12) take K = lg(T /T0).
 Applying Theorem stage Algorithm gives following: (cid:96)t t=1 ≤ T(cid:88) ≤ K(cid:88) Tk(cid:88) (cid:96)t ≤ K(cid:88) TkUk ≤ K(cid:88) (cid:112) ≤ + + ≤ TK k=0 k=0 k=0 t=1 (cid:112) Tk (14) ﬁrst inequality follows fact (cid:96)t ≥ second inequality follows extending sum T TK; third fourth inequalities follow The- orem
 penultimate inequality follows fact {Tk} is increasing sequence ﬁnal inequality follows TK be more
 Dividing T gives desired result.
 Concentration previous results provided bounds average loss algorithms.
 section, provide bounds tail distribution loss.
 gives guarantee fraction observations loss exceeds average case bound tends zero.
 T(cid:88) T(cid:88) t=1 t=1 i≤j (cid:88) (cid:88) (cid:114) i≤j ≤ ln(p) (cid:88) i≤j ≥(cid:88) i≤j Note ˆst = µ(yt; ˆbt) µ is deﬁned terms maximization.
 Therefore, σ(i, j)(ˆst(i)yt(j) + ˆst(j)yt(i))ˆbt(i, j) σ(i, j)(s(i)yt(j) + s(j)yt(i))ˆbt(i, j) s ∈ S.
 shows term ﬁrst Ces`aro sum (12) is non-negative.
 Similarly, term second Ces`aro sum (12) is non-negative.
 gives lower bound zero.
 Rearranging terms leaves desired results.
 Inﬁnite Horizon Algorithm present Algorithm inﬁnite horizon algo- rithm changes learning rate.
 Al- gorithm applies “doubling trick” Algorithm
 idea is deﬁne epochs [Tk, Tk+1] Tk = ln(p)) k ≥ T−1
 duration kth epoch is Tk epoch apply Algo- rithm
 poly-logarithmic factors T gives same convergence rate had Algorithm

 Let ≤ T (cid:96)t > lg (cid:12)(cid:12)(cid:12)(cid:12)(cid:26) (cid:16) T0 (cid:17)(cid:113) ln(p) T +  (cid:27)(cid:12)(cid:12)(cid:12)(cid:12) fT () = (15) be fraction observations time T ≥ T0 loss exceeds average-case bound least .
  have (cid:17)(cid:113) ln(p) fT () ≤ − (cid:16) lg (16) T +  T0 hence, lim T→∞ fT () =
 (17) Proof.
 observed loss sequence {(cid:96)t}T t=1 deﬁnes point measure R+ point has mass
 Applying Markov’s Inequality measure gives fT () ≤ lg lg T +  T +  (cid:16) (cid:16) T0 T0 (cid:17)(cid:113) ln(p) (cid:17)(cid:113) ln(p) (cid:18) lg T0 (cid:19)(cid:114) lim T→∞ Rearranging upper bound gives ﬁrst result.
 second result take limit note (a) Evolution ˆbt (cid:19)(cid:114) (cid:18) T0 ln(p) T(cid:88) t=1 (cid:96)t lg · · ln(p)

 Numerical Demonstration demonstrate Algorithm small example n queues.
 time slot, number arriving customers is distributed Z+.
 mean number arriving customers is queue mean number arriving customers is
 arrivals are independent time slots queues.
 take b = (cid:20) ,(cid:2) ,(cid:2) (cid:3)(cid:48) (cid:21) (cid:3)(cid:48) ,(cid:2) (cid:3)(cid:48)(cid:111) (cid:110)(cid:2) (cid:3)(cid:48) S = choice b shows expert scheduler prioritizes queue expert has preference serve queues simultaneously.
 simulate system run Algorithm T = time slots x0 = (cid:2) (cid:3)(cid:48)
 results are shown Figure
 note Figure shows ˆbt does converge b.
 see decimal places) (cid:20) ˆbT = (cid:21) (b) Error learned scheduling decisions (c) Average realized loss Figure Output Algorithm example Section 0.450.500.55ˆbt(0,0)b(0,0)0.270.300.33ˆbt(0,1)b(0,1)0.00.20.40.60.81.0Time1e60.180.200.22ˆbt(1,1)b(1,1)0.00.20.40.60.81.0Time1e60.00.51.01.52.0kˆst−stk∞0.00.20.40.60.81.0Time1e610-310-210-1100101Average LossRealizedUpper Bound majority simulation parameter estimates do change.
 reason is (as shown Figure ˆbT yields same scheduling decisions b.
 algorithm learns emulate expert scheduler loss becomes weights stop updating.
 possibility was discussed beginning Section
 Figure shows average loss does tend zero, upper bound proved Theorem is loose situation.
 is expected due generality theorem.
 means concentration bound Theorem is conservative.
 Indeed, simulation see ft() t  >
 other words, observed loss exceeds average case bound.

 Extensions discuss extensions algorithms.
 ﬁrst note replace line Algorithm same goals human manager.
 have provided several theoretical guarantees have demonstrated eﬃcacy algorithm simple example.
 paper opens door few area future work.
 idea is provide tighter bounds depend statistical properties arrival process.
 beneﬁt current approach is does require as- sumptions arrival process clear downside is resulting bounds are loose.
 algorithm uses information arrival process have faster convergence rates tighter bounds.
 idea is investigate impact approx- imate computation µ.
 mentioned Section large-scale problems, computing µ(y; b) is gener- diﬃcult problem heuristic approaches are typ- taken practice.
 area future work be consider such approximation “noise” aﬀects ability emulate expert scheduler.
 wt+1 ← wt · exp(−ηwt).
 References new algorithm be Hedge-style algorithm be able use apply other results (e.g. [5, The- orem obtain similar upper bounds average loss.
 note modify algorithms obtain tighter upper bounds impose additional as- sumptions expert.
 expert have simple objective leads prioritization queues others.
 case, have b(i, j) i (cid:54)= j.
 having triangular ar- ray p parameter estimates, keep track n estimates.
 Θ(ln(p)) = Θ(ln(n)), con- vergence rate change have smaller constant factors.
 Other sparsity patterns be handled similar fashion.
 diagonal case is simpler be need use σ(i, j) keep track appropriate signs.
 noted Section continuous-time PCS model heterogenous stochastic service times was con- sidered [3].
 algorithms be applied setting updating algorithm af- ter customer arrivals departures dis- crete time slots.
 [3], B is diagonal apply simpliﬁcations mentioned
 theorems hold require state up- date happen spaced intervals – algorithms require stream observed backlogs observed scheduling actions.

 Conclusions Future Work paper have proposed algorithm learns scheduling policy emulates behavior expert projective cone scheduler.
 oﬀers data-driven way designing automated scheduling policies achieve [1] N.
 Master, Z.
 Zhou, D.
 Miller, D.
 Scheinker, N.
 Bambos, P.
 Glynn, Improving predictions pediatric surgical durations supervised learning, International Journal Data Science Analytics (2017)
 [2] C.
 Okoli, S.
 D.
 Pawlowski, Delphi method research tool: example, design considerations applications, Infor- mation management (1)
 [3] M.
 Armony, N.
 Bambos, Queueing dynamics maximal throughput scheduling switched processing systems, Queue- ing systems (3)
 [4] K.
 Ross, N.
 Bambos, Projective cone scheduling (PCS) algo- rithms packet switches maximal throughput, IEEE/ACM Transactions Networking (3) (2009)
 [5] S.
 Arora, E.
 Hazan, S.
 Kale, Multiplicative Weights Up- date Method: Meta-Algorithm Applications., Theory Computing (1)
 [6] Y.
 Freund, R.
 E.
 Schapire, Adaptive game playing using mul- tiplicative weights, Games Economic Behavior (1-2) (1999)
 [7] S.
 A.
 Plotkin, D.
 B.
 Shmoys, ´E.
 Tardos, Fast approximation algorithms fractional packing covering problems, Math- ematics Operations Research (2) (1995)
 [8] A.
 B¨armann, S.
 Pokutta, O.
 Schneider, Emulating Expert: Interna- Inverse Optimization Online Learning, tional Conference Machine Learning, pp.

 in: [9] M.
 D.
 Troutt, A.
 A.
 Brandyberry, C.
 Sohn, S.
 K.
 Tadisina, Lin- ear programming system identiﬁcation: general nonnega- tive parameters case, European Journal Operational Research (1) (2008)
 [10] A.
 Keshavarz, Y.
 Wang, S.
 Boyd, Imputing convex objec- tive function, in: IEEE International Symposium Intelligent Control, IEEE, pp.

 [11] J.
 Thai, A.
 M.
 Bayen, Imputing variational inequality function convex objective function: A robust approach, Journal Mathematical Analysis Applications.
 [12] A.
 Y.
 Ng, S.
 J.
 Russell, Algorithms inverse reinforcement learning., in: International Conference Machine Learning, pp.

 [13] P.
 Abbeel, A.
 Y.
 Ng, Apprenticeship learning inverse re- inforcement learning, in: International Conference Machine learning, ACM, pp.

 [14] K.
 Ross, N.
 Bambos, Local search scheduling algorithms maximal throughput packet switches, in: Annual Joint Con- ference IEEE Computer Communications Societies (INFOCOM), Vol.
 IEEE, pp.

 Computational models generating audio signals are means exploring understanding perception sound.
 Natural sounds, deﬁned everyday non-music, non-speech sounds, are appealing medium study perception exclude cognitive factors such language musical interpretation.
 McDermott [1] used synthesis means demonstrate human auditory system utilises time-averaged statistics subband amplitudes classify sound textures.
 similar vein, Turner [2] constructed synthesis model based probabilistic latent variable analysis same subband amplitudes.
 main advantage latent variable approach is possibility uncovered latent behaviour represent i) primitive source generated signal, ii) latent information human auditory system encodes calculates time-averaged statistics.
 Latent Force Modelling Natural Sounds Latent variable analysis captures correlations multiple dimensions modelling data’s shared dependence unobserved (latent) variable function.
 is, nature, ill-posed; aim predict latent functions mapping latent space observation data.
 such, many potential solutions exist cannot guarantee prediction encode true sound source true perceptual representation.
 ill-posed nature problem necessitates use prior information.
 is suggested nonnegativity, smoothness sparsity form suitable set prior assumptions real life signals.
 argue imposing such constraints, simple scalar mapping latent space observation space is insuﬃcient capture complex behaviour observe subband amplitude envelopes audio signal.
 construct latent force model (LFM) [3] incorporate prior knowledge amplitude envelopes behave discrete diﬀerential equation models exponential decay [4].
 Utilising state space formulation augment standard LFM including current state information many discrete time steps.
 allows capture phenomena such feedback, damping extent reverberation.
 probabilistic approach latent functions are modelled Gaussian processes, provide uncertainty information predictions whilst guaranteeing latent functions are smooth.
 Nonnegativity is imposed nonlinear transformation.
 Evaluating latent representations is straightforward.
 Objective measures ability reconstruct observation data don’t inform interpretability predictions.
 hypothesise latent functions capture meaningful information, generative model based synthesising latent functions are similar generate realistic data projected observation space.
 paper introduce generative model, applicable wide range natural sounds, based extended LFM1 (Section
 Comparative models based variants nonnegative matrix factorisation (NMF) are implemented perform evaluation-by-synthesis, shows listeners perceive LFM approach generate realistic sounds cases NMF is eﬃcient reconstruction error perspective (Section
 perceptual similarity sounds is determined direct comparison waveforms, comparison statistics [1].
 Hence is argued prior information natural sounds take statistical form [2].
 argue Section statistical representations be improved inclusion assumptions physical behaviour sound, resulting hybrid statistical-physical prior.
 Matlab source code example stimuli be found c4dm.eecs.qmul.ac.uk/ audioengineering/natural_sound_generation Latent Force Modelling Natural Sounds order analyse sound McDermott [1] Turner [2] utilise subband ﬁltering approach time-frequency analysis, signal is split diﬀerent frequency channels bank band-pass ﬁlters.
 time- frequency representation is formed tracking amplitude envelopes subband.
 McDermott generates sound textures designing objective function allows statistics synthetic signal be matched target signal.
 Turner utilises probabilistic time-frequency analysis combined probabilistic latent variable analysis represent similar features.
 Turner’s approach has advantage parameters have been optimised, new amplitude envelopes be generated drawing samples latent distribution.
 be noted samples drawn model exhibit fast attack slow decay observe audio amplitude envelopes, model is symmetric.
 NMF is ubiquitous technique decomposing time-frequency audio data [6,7,8], common criticism is inability take account temporal information.
 common approach dealing issue is impose smoothness latent functions, idea being smoothness is proxy local correlation neighbouring time steps.
 Temporal NMF (tNMF) imposes smoothness penalising latent functions change [8] placing Gaussian process [9].
 alternative approach is use hidden Markov model capture changes audio signal’s spectral make time [10].
 High resolution NMF (HR-NMF) models temporal evolution sound utilising assumption natural signals are sum modulated sinusoids, frequency channel being assigned own decay parameter estimated using expectation-maximisation [11].
 Latent Force Models incorporate prior assumptions data-driven analysis use latent force models (LFMs) [3], probabilistic modelling approach assumes M observed output functions are produced R < M unobserved (latent) functions ur being passed set diﬀerential equations.
 chosen set diﬀerential equations represents physical behaviour present system are simplistic manner, technique improve ability learn data [12].
 is achieved placing Gaussian process (GP) [13] R latent functions, calculating cross-covariances (which involves solving ODEs), performing regression.
 was shown Hartikeinen S¨arkka [5] that, certain conditions, equivalent regression task be performed reformulating model (i.e. ODE representing physical knowledge system) state space (SS) form, reformulating GP stochastic diﬀerential equation (SDE), combining joint SS model: = f (x(t)) + Lw(t)
 (1) x(t) represents state vector containing SDE {ur(t), ˙ur(t), ...}R m=1 states r=1, w(t) is white noise process, f is transition dx(t) dt Latent Force Modelling Natural Sounds function is dependent set ODE parameters GP / SDE hyperparameters, L is vector determining states are driven white noise.
 model’s discrete form is x[tk] = ˆf (x[tk−1], ∆tk) + q[tk−1] (2) ∆t is time step size, ˆf is discretised transition function q[tk−1] ∼ N (0, Q[∆tk]) is noise term process noise matrix Q.
 corresponding output measurement model is y[tk] = Hx[tk] + [tk], [tk] ∼ N (0, σ2) (3) measurement matrix H selects outputs joint model.
 posterior process x[tk], i.e. solution (2), is GP linear case such ﬁltering distribution p(x[tk] | y[t1], ..., y[tk]) is Gaussian.
 Hence state estimation be performed Kalman ﬁltering smoothing [14].
 However, f is nonlinear function, is case wish impose nonnegativity latent functions, calculation predictive ﬁltering distributions involves integrating equations are combination Gaussian processes nonlinear functions.
 approximate solutions integrals using Gaussian cubature rules.
 approach is known cubature Kalman ﬁlter (CKF) [15].
 Kalman update steps provide means calculate marginal data likelihood p(y[t1:T ] | θ).
 Model parameters θ be estimated data maximising likelihood using gradient-based methods.
 Latent Force Models Audio Signals obtain amplitude data desired form pass audio signal equivalent rectangular bandwidth (ERB) ﬁlter bank.
 use Gaussian process probabilistic amplitude demodulation (GPPAD) [16] calculate subband envelopes corresponding carrier signals.
 GPPAD allows control demodulation time-scales GP lengthscale hyperparameters.
 are concerned varying behaviour correlated frequency spectrum, accordance observation human auditory system summarises sound statistics time [1].
 Fast-varying behaviour is relegated carrier signal be modelled independent ﬁltered noise.
 number channels ﬁlter bank demodulation lengthscales be set ﬁrst analysis stage.
 Keeping number total model parameters small is priority (see Section set number ﬁlters lengthscales such capture amplitude behaviour occurring durations slower.
 Augmented Latent Force Models Amplitude Envelopes use ﬁrst order diﬀerential equation model exponential decay occurs audio amplitude envelopes [4].
 simplistic model Latent Force Modelling Natural Sounds does take account varying decay behaviour due internal damping, feedback other nonstationary eﬀects occur sound is generated propagates towards listener.
 require nonnegativity latent functions, is imposed nonlinear transformation, use nonlinear LFM general is (2) nonlinear ˆf
 ﬁrst order ODE discrete form is ˙xm[tk] = −Dmxm[tk] + Smrg(ur[tk]) (4) r=1 m ..., M M is number frequency channels.
 Dm Smr are damping sensitivity parameters g(u) = log(1+eu) is positivity-enforcing nonlinear transformation.
 model progresses forwards time step size ∆t using Euler’s method: xm[tk+1] = xm[tk] + ∆t ˙xm[tk].
 account complex behaviour mentioned occurs real audio signals, extend discrete model such predictions current time step tk be inﬂuenced predictions multiple time steps past.
 [4] augment model adding parameter γm controls “linearity” decay.
 ﬁnal model becomes R(cid:88) P(cid:88) p=1 P(cid:88) R(cid:88) q=0 r=1 ˙xm[tk] = −Dmxγm m [tk] + Bmpxm[tk−p] + Smrqg(ur[tk−q])
 restrict γm ∈ [0.5, sounding objects strong internal damping expect γm be small, representing linear decay.
 Parameters Bmp are feedback coeﬃcients determine current output is aﬀected output behaviour p time steps past.
 Smrq are lag parameters determine sensitive current output is input r q time steps
 lag term is important modes vibration sounding object tend be activated diﬀerent times due deformations object vibrates, due interaction multiple modes vibration.
 capture eﬀects due reverberation.
 feedback terms allow varied decay behaviour can’t be described simple exponential decay.
 challenge is incorporate ﬁltering procedure.
 do augmenting state vector x[tk] transition model ˆf (x[tk−1], ∆tk) = x[tk] + ∆t ˙x[tk] new rows corresponding delayed terms.
 Fig.
 shows time step current states X[tk] = {xm[tk]}M m=1, U [tk] = {ur[tk]}R r=1 are “passed down” such next time step are locations corresponding feedback lag terms.
 performing Kalman ﬁlter prediction step, augmented states are included inﬂuence predictions current state, predictions augmented entries are exact copies previous time step.
 (6) Fig.
 shows latent prediction metal impact sound latent force, R =
 mean distribution is minimum least squares error Latent Force Modelling Natural Sounds Fig.

 augmented LFM stores terms previous time steps state vector.
 Blue represents output predictions X (amplitudes), green represents latent predictions U
 step, predictions pass feedback lag state locations.
 entire state is used predict next step’s outputs latents Kalman ﬁltering.
 pass discrete model (5) reconstruct amplitude envelopes.
 single latent force, observe complex behaviour has been learnt.
 latent force is smooth sparse, reconstructed envelopes have slow decay sparsity.
 Generating Novel Instances Natural Sounds A signiﬁcant beneﬁt probabilistic approaches such LFM tNMF is that, providing uncertainty information predictions, provide means sample new latent functions learnt distribution.
 passing new functions model generate amplitude envelopes.
 envelopes modulate carrier signals produced using sinusoids- plus-noise approach based analysis original carriers.
 subbands are summed create new synthetic audio signal distinct original similar characteristics.
 Sampling prior learnt distribution generates functions appropriate smoothness magnitude, desired energy sparsity is guaranteed.
 Latent functions are modelled independently, practice tend co-occur are activated similar regions signal.
 use GPPAD demodulate latent functions varying envelope, ﬁt GP squared exponential covariance function envelope [13].
 sample high-level envelope use modulate generated latent functions; results product is latent behaviour sparse energy, demonstrated Fig.

 Optimisation Settings set model parameters {Dm, Bmp, Smrq, γm, λr}, GP lengthscales λr, large R, P increase.
 alleviate issues occur parameter space becomes large sparsify feedback sensitivity parameters.
 tktk 1tk+1KalmanﬁlterpredictionKalmanﬁlterpredictionfeedbacklagX[tk]X[tk 1]U[tk 1]U[tk 2]X[tk 2]U[tk] Latent Force Modelling Natural Sounds Fig.

 LFM applied metal impact sound, mean conﬁdence latent distribution shown.
 mean is passed model (5) reconstruct envelopes.
 Complex behaviour is maintained using single force.
 example, P = ﬁx Bmp zero p ∈ [3, such parameters are included optimisation procedure.
 Reliability optimisation procedure suﬀers number parameters increases, practice M frequency channels are optimised together.
 select envelopes contributing energy train model observations channels.
 remaining channels are appended optimised whilst keeping already-trained parameters ﬁxed.
 improves reliability prioritises envelopes high energy.
 skip prediction steps periods signal are low amplitude, speeds ﬁltering step.
 adjustments, optimisation takes days second sound sample.
 Evaluation evaluate method collated set audio recordings, selected being representative everyday natural sounds2.
 Music speech sounds were included, were sounds signiﬁcant frequency modulation, model doesn’t capture behaviour.
 Reconstruction Error Original Sound analyse ability reconstruct original data projecting latent representation output space.
 LFM means passing mean learnt distribution model (5).
 Fig.
 shows reconstruction RMS error cosine distance LFM relative NMF recordings.
 smoothness constraint enforced placing GP latent functions impacts reconstruction.
 is demonstrated fact tNMF performs RMS error perspective.
 LFM has descriptive power, is capable achieving lower RMS error unconstrained NMF.
 however, tNMF outperforms other models based cosine distance.
 From freesound.org Natural Sound Stimulus set: mcdermottlab.mit.
 edu/svnh/Natural-Sound/Stimuli.html Latent Force Modelling Natural Sounds Fig.

 LFM generative model latent forces applied applause sound.
 high-level modulator (black line (b)) is calculated demodulating latent forces.
 Listening Test Novel Sounds Objective results suggest smoothness constraints harm reconstruction original signal.
 However, aim is learn realistic latent representations be foundation generative model.
 test suitability, designed experiment compare generative models based LFM, NMF tNMF.
 approach outlined Section was used model types.
 NMF is non-probabilistic, does provide immediate way sample new data, GPs were ﬁt latent functions analysis.
 experiment followed multi-stimulus subjective quality rating paradigm3: participants were shown pages (order randomised), sound example, asked listen reference recording rate generated sounds (2 model anchor) based credibility new sound same type reference.
 Ratings were scale score representing realistic sound.
 Fig.
 shows mean realism ratings.
 Whilst variation was large sound LFM was rated realistic other methods.
 test signiﬁcance applied generalised linear mixed eﬀects beta regression, sound example participant were treated random eﬀects.
 Table shows mean realism rating was highest LFM regardless number latent functions.
 diﬀerence was signiﬁcant level LFM vs.
 NMF latent functions.
 suggests sounds requiring many latent functions capture behaviour, such test was run implemented Web Audio Evaluation Tool: github.com/BrechtDeMan/WebAudioEvaluationTool Latent Force Modelling Natural Sounds Fig.

 Reconstruction error LFM plotted relative NMF.
 Crosses represent median, error bars range ﬁrst third quartile.
 textural sounds, LFM oﬀer signiﬁcant gain statistical approaches.
 wind recording Fig.
 textural sound envelopes do exhibit clear exponential decay, was captured best tNMF.
 sounds latent fn.
 latent fns.
 latent fns.
 LFM vs.
 LFM vs.
 tNMF <1e-04 <1e-05 <0.001 NMF vs.
 Estimate p value Estimate value Estimate value Estimate p value -0.0272 <1e-04 <1e-05 Table
 GLMM three-way comparison applied listening test results.
 LFM received higher mean ratings, conﬁdence decreases number latent forces, indicated increasing p values.
 Estimate be interpreted ratio increase realism rating choosing model model B.
 Conclusion results show order extend existing synthesis techniques larger class sounds, is important utilise knowledge natural sound behaves.
 achieved using latent force modelling capture exponential decay, augmented standard approach include feedback delay many discrete time steps.
 Doing allowed make smooth, sparse latent predictions argue are representative real source generated given sound.
 claim is supported fact generative model based LFM was rated realistic listeners alternatives based variants NMF, cases was superior reconstruction original signal.
 Resonance, decay modulations subband amplitudes were captured model, is ﬂexible be applicable sounds ranging glass breaking dogs barking.
 nonlinear ODE representing physical knowledge contains large number parameters, making approach impractical cases, more compact model be huge beneﬁt.
 Eﬃcient nonlinear ﬁltering methods numerical ODE solvers make computation time acceptable.
 Future work includes behaviour occurring multiple time scales once, models frequency modulation other nonstationary eﬀects expand class sounds such techniques be applied.
 123Numberoflatentfunctions024RMSerrorrelativetoNMF123Numberoflatentfunctions-101CosinedistancerelativetoNMFNMFLFMtNMF Latent Force Modelling Natural Sounds Fig.

 Mean realism ratings obtained listening test.
 References
 McDermott, J.H., Simoncelli, E.P.: Sound texture perception statistics auditory periphery: Evidence sound synthesis.
 Neuron (2011)
 Turner, R.E.: Statistical Models Natural Sounds.
 PhD thesis, UCL (2010)
 Alvarez, M.A., Luengo, D., Lawrence, N.D.: Latent force models.
 In: Int.
 Conf.
 Artiﬁcial Intelligence Statistics (AISTATS).
 Volume

 Wilkinson, W.J., Reiss, J.D., Stowell, D.: Latent force models sound, Int.
 Conf.
 Digital Audio Eﬀects (2017)
 Hartikainen, J., S¨arkk¨a, S.: Sequential inference latent force models.
 In: Int.
 Conf.
 Uncertainty Artiﬁcial Intelligence (UAI-11).

 F´evotte, C., Bertin, N., Durrieu, J.L.: Nonnegative matrix factorization itakura-saito divergence: application music analysis.
 Neural Computation (2009)
 Smaragdis, P., Brown, J.C.: Non-negative matrix factorization polyphonic mu- sic transcription.
 In: WASPAA.

 Virtanen, T.: Monaural sound source separation nonnegative matrix factoriza- tion temporal continuity sparseness criteria.
 IEEE transactions audio, speech, language processing (2007)
 Turner, R.E., Sahani, M.: Time-frequency analysis probabilistic inference.
 IEEE Transactions Signal Processing (2014)
 Mysore, G.J., Smaragdis, P., Raj, B.: Non-negative hidden markov modeling audio application source separation.
 In: LVA/ICA, Springer (2010)
 Badeau, R.: Gaussian modeling mixtures non-stationary signals time- frequency domain (HR-NMF).
 In: WASPAA.

 Alvarez, M.A., Luengo, D., Lawrence, N.D.: Linear latent force models using Gaus- sian processes.
 IEEE Transactions pattern analysis machine intelligence (2013)
 Rasmussen, C.E., Williams, C.K.I.: Gaussian Processes Machine Learning.
 MIT Press (2006)
 S¨arkk¨a, S.: Bayesian Filtering Smoothing.
 Cambridge University Press (2013)
 Hartikainen, J., Seppanen, M., Sarkka, S.: State-space inference non-linear latent force models application satellite orbit prediction.
 Interna- tional Conference Machine Learning (ICML).

 Turner, R.E., Sahani, M.: Demodulation probabilistic inference.
 IEEE Trans- actions Audio, Speech, Language Processing (2011) LFMNMFtNMF00.20.40.60.81Realismrating1latentfunctionCymbalMetalHammeringGunshotBasketballLFMNMFtNMF2latentfunctionsBreathingBellGlassDogFrogKeysStairsLFMNMFtNMF3latentfunctionsGravelWindChimesApplauseCameraStairsDishesBikeHorn
 relaxation).
 QRS complex Electrocardiogram (ECG) represents electrical activity human heart.
 is composite waves: P, Q, R, S T.
 P wave is Atria depolarization (Atrial Contraction), T wave is depolarization ventricles (Ventricular represents ventricles depolarization (Ventricular contraction).
 QRS complex, ST segment, PR interval, RR interval, PR segment, QT interval are important sections ECG signal diagnosis different cardiac diseases, arrhythmia.
 arrhythmia is alteration regular rate rhythm heartbeat.
 feature extraction pre-processing, Automatic ECG classification is emerging tool cardiologists medical diagnosis effective treatments.
 Traditional methods classify ECG signal include classification steps.
 Various kinds noise artefacts are first removed, signal is segmented calculate features vectors time, using feature reduction algorithm reduce dimensionality.
 Finally, classification features form input machine learning algorithm such Support Vector Machines (SVMs) [2], Neural Networks (NNs) [3] ensemble learning [4].
 Deep learning (DL) based neural network models have achieved great success multiple fields such natural language processing, computer vision, biomedical signal processing others.
 DLs overcome challenge tedious feature engineering task helps parameterizing traditional NN many layers [5].
 Convolution Neural Networks (CNNs) Recurrent Neural Networks (RNNs) Short-term Memory Network used architectures learn appropriate filters reduce input dimensionality, LSTMs are useful model system dynamics [5].
 models.
 CNNs (LSTM) are Many approaches have been performed classify various cardiac arrhythmias used DL models.
 Authors [6] developed new model predicted arrhythmias single-lead ECG.
 consisted 34- layer CNN maps sequence ECG samples sequence rhythm classes.
 new neural network architecture was proposed [7] anomaly detection ECG time signals.
 consisted stacking multiple recurrent LSTM layers.
 approach based DL classification ECG signals was proposed [8].
 consisted layers: feature representation layer softmax regression layer.
 feature representation layer was trained raw ECG data unsupervised way employing stacked denoising autoencoders sparsity constraint.
 paper, propose novel deep neural network combines CNN LSTM learn sequence data containing term patterns unknown length extracted ECG signals.
 model does require explicit pre-processing, discover hidden structures different ECG entities learn dependencies.

 output one-layer CNN is fed stack recurrent LSTM layers.
 CNN is constructed top signal vectors large corpus ECG data learn higher-level representations PQRST regions.
 approach, task is formulated temporal sequence predicting problem be solved sequence-to-sequence learning framework.
 new model classifies ECG signals normal sinus rhythm (N), atrial fibrillation alternative rhythm (O), noisy (~).
 used PhysioNet Challenge dataset [9] consists ECG signals sampled Hz lasting s were records s.
 refer proposed deep architecture CL3 (One CNN LSTMs).
 rest paper is organized follows.
 Detailed descriptions proposed deep model are presented sections
 Section reports experimental settings evaluation metrics, discusses experimental results.
 Finally, conclusions future directions are outlined section

 CL3 Model architecture CL3 model is illustrated Figure consists main components: is representation learning, CNN, other is sequence learning, stack LSTMs. applying one-layer CNN raw input sequence extract local discriminative features, layers LSTMs are put top previous CNN encode sequential patterns.
 Then, dense layer is added process output third LSTM.
 Finally, softmax function is adopted predict class.
 Model architecture Representation learning rectified component consists CNN max-pooling layer.
 convolution layer performs operations sequentially: 1D-convolution filters, batch normalization, (ReLU) activation.
 Pooling layer downsamples inputs using max operation.
 are reasons use max- pooling layer here.
 reduces computation upper layers.
 extract local dependencies keep salient information.
 obtained vectors are fed second component is sequence linear unit learning.
 used filters be comparative other filtering schemes (e.g., use PCA wavelet components is typical biomedical signals).
 system parameters including number filters pooling size be found section
 Dropout Batch Normalization blocks are explained section ECG recording is considered univariate time series is denoted X={x1, x2,…,xN}, N represents length ECG signal.
 CNN extracts i-th feature ai i-th ECG sample xi follows: 𝑎𝑖 = 𝐶𝑁𝑁𝜃(𝑥𝑖) CNNθ(xi) is function transforms ECG signal feature vector ai using CNN θ parameter represent number filters.
 features vectors {a1, a2,…, aN} are forwarded sequence learning component.
 (1) Figure
 CL3 architecture.
 Sequence learning component is created stacking multiple LSTM hidden layers top other, output sequence layer forming input sequence next.
 LSTM layers are used.
 Specifically, input upper LSTM layer (u) takes hm(t) middle LSTM layer (m), middle LSTM layer takes hl(t) lower LSTM layer (l).
 outputs second layer third layer are same operation LSTMl LSTMm. Formally, suppose are N features obtained CNN {a1, a2,…,aN} organized t=1…N denotes time index ECG samples, sequence learning component is defines follows: 𝑙 𝑐𝑡 ℎ𝑡 𝑚, 𝑐𝑡 ℎ𝑡 𝑢, 𝑐𝑡 ℎ𝑡 = 𝐿𝑆𝑇𝑀𝜃𝑙 𝑚 = 𝐿𝑆𝑇𝑀𝜃𝑚 𝑢 = 𝐿𝑆𝑇𝑀𝜃𝑢 𝑙 (ℎ𝑡−1 𝑚 (ℎ𝑡−1 𝑢 (ℎ𝑡−1 𝑐𝑡−1 𝑚 𝑐𝑡−1 𝑢 𝑐𝑡−1 𝑎𝑡) 𝑚 ℎ𝑡 𝑙 𝑢 ℎ𝑡 𝑚) (2) (3) (4) LSTM denotes function processes sequences features using stacked LSTMs parametrized θl, θm θu lower, middle upper layers; h c are vectors hidden cell states LSTMs; ℎ0 are set vectors.
 𝑢 𝑎𝑛𝑑 𝑐𝑁+1 𝑙 ℎ𝑁+1 𝑙 𝑐0 A dense layer forms final layer output is passed softmax function output is probability distribution labels [5].
 Learning CL3 ECG Classification entire model is trained minimizing cross- entropy error.
 Given training sample x corresponding label y∈{1,2,…,K} K is number possible labels (ECG classes), estimated probabilities 𝑦̃𝑗 ∈ [0,1] label j ∈{1, …, K}, error is defined equation be
 used Root Mean Square Propagation (RMSprop) optimization method calculates magnitude recent gradients normalize gradients optimize model parameters following objective function: 𝑗=1 = 𝑗}log (𝑦̃𝑗) 𝐿(𝑥, 𝑦) = ∑ (5) Dropout was employed prevent overfitting training: was applied input LSTM, Recurrent Dropout was employed drop neurons recurrent connections LSTM Batch Normalization was used keep values in-bounds avoid saturation various processing steps.
 was performed outputs CNN LSTM layers.
 defined maximum Efficient batch-oriented training requires fixed-length input; length (max_length) ECG signals dataset.
 ECG signal has length less max_length was padded zeros.
 Zero label is added sequence labels be referred PAD class.
 other hand, ECG signals are longer max_length were split multiple sequences such new sequence length was less equal max_length.
 large majority signals had samples (30 s), significant number were length (18000), chose max_length
 class vector contains different labels [PAD, N, AF, O, ~] are one-hot encoded.
 final target sequence input sequence is constructed repeating binary vector rep_length times, defined equation
 𝑟𝑒𝑝_𝑙𝑒𝑛𝑔𝑡ℎ = 𝑚𝑎𝑥_𝑙𝑒𝑛𝑔𝑡ℎ/𝑡𝑎𝑟𝑔𝑒𝑡_𝑓𝑎𝑐𝑡𝑜𝑟 target_factor is determined experimentally.
 final train input shape is (Number sequences be trained* max_length*1), refers single input signal dimension, final output shape is given (Number sequences be trained * rep_length * number classes).

 Experimental settings results deep model was implemented Python using Keras library TensorFlow backend, provides efficient functionality CPUs GPUs. hyper- parameters were chosen according experiment results are follows: a) Batch size is Max_length input sequence is weight matrix is initialized
 b) CNN number filters kernel size are equal
 c) Max pooling layer, pool size is
 d) LSTM layers, number cells is equal dropout recurrent dropout are e) BN layers, zeros, ones, zeros, ones are used momentum, epsilon, beta initializer, gamma initializer, moving mean initializer moving variance initializer, [5].
 f) dense layer, normal distribution centered zero is used kernel initializer output shape is (9000,
 is result target_factor=18 sample decimation effect max pooling layer, represents number classes.
 classifier performance was evaluated terms F1-measure is calculated F1=2*(P*R)/(P+R).
 represents harmonic mean Precision (P) Recall (R).
 P is ratio true positives predicted positives given P=TP/(TP+FP).
 more information, refer [9].
 Table
 Results different experiments entries.
 σ is cross-validation F1 standard deviation.
 10-folds CV Model Entry Network variant(s) Class σ (%) F1 (%) F1 (%) CL3 Batch size=500 CL3 II CL3 Overall Batch size=500 Class weight Overall Dilated CNN=2 N AF O ~ N AF O ~ N AF O ~ Overall III experiments were conducted evaluate effectiveness proposed deep architecture ECG signal classification.
 Table displays 10-fold cross validation scores experiments best entries submitted challenge server.
 be noted Experiment exhibited best F1 (83.30%), standard deviation (σ) (1.80) is higher Experiment (1.50).
 Experiment imbalanced class issue was dealt applying weights misclassification sequences.
 F1 was better Experiment σ is higher result Entry II performed poorer first one F1
 Dilated convolution layer was employed experiment results showed performed worse Experiment yielding lower F1s higher σ entry score reached
 conclude Entry yielded best overall score submitted entries training using hidden dataset.
 final score version relabelling was
 showed CL3 is suitable diagnosis different cardiac diseases good accuracy.
 Future work focus refining model applying ensemble deep learning framework decrease information loss overfitting problems, class imbalance problem.
 overcome Figure
 Class prediction original class.
 Figure depicts sample ECG examples A00001, A00002, A00003, A00004, A00005 A00006 were labelled cardiologists N, N, N, A, A N, CL3 Entry I.
 be seen A00001, A00002, A00003 A00006 recordings, CL3 predict ECG samples same label assigned expert, A00004 A0005, are ECG samples be learnt new deep architecture.
 addition that, Figure shows F1 results time class.
 be noticed figure model performance begun degrade classes ~ AF, middle training.
 is due fact minority classes, performances majority classes keep increasing training (and supporting decision use final prediction classification).
 are
 Conclusions Figure
 F1-measure curves class vs training prediction Entry I.
 References [1] Davey P.
 ECG glance.
 John Wiley Sons,
 [2] Zhao Q, Zhang L.
 ECG Feature extraction classification using wavelet transform support vector machines.
 ICNN&B'05.
 -1092.
 [3] Prasad GK, Sahambi JS.
 Classification ECG using multi-resolution analysis neural networks.
 TENCON2003.
 227-231.
 [4] Zeng XD, Chao S, Wong F.
 Ensemble learning heartbeat type classification.
 ICSSE2011.
 320-325.
 [5] Buduma N, Locascio N, Fundamentals Deep Learning: Designing Next-Generation Machine, O’Reilly,
 [6] Rajpurkar P, Hannun AY, Haghpanahi M, Bourn C, Ng AY.
 Cardiologist-Level Convolutional Neural Networks.
 arXiv:1707.01836.
 [7] Chauhan S, Vig L.
 Anomaly detection ECG time signals Arrhythmia Detection deep long short-term memory networks.
 DSAA, 1-7.
 [8] Al Rahhal MM, Bazi Y, AlHichri H, Alajlan N, Melgani F, Yager RR.
 Deep learning approach active classification electrocardiogram signals.
 Information Sciences.
 1:340-54.
 [9] Gari Clifford, Chengyu Liu, Benjamin Moody, Ikaro Silva, Qiao Li, Alistair Johnson, Roger Mark.
 AF Classification Short Single Lead ECG Recording: PhysioNet Computing Cardiology Challenge
 Computing Cardiology (Rennes: IEEE).
 (In Press) A new deep learning model, named CL3, automatic classification cardiac arrhythmias based raw single- lead ECGs is proposed.
 CL3 uses CNN extract features are introduced stack LSTMs learn hidden patterns ECG epochs little manual parameter tuning required.
 results Address correspondence.
 Philip Warrick, PeriGen.
 Inc.
 Montreal, Canada.
 philip.warrick@gmail.com Masun Nabhan Homsi, Universidad Simón Bolívar.
 mnabhan@usb.ve
 Creating model computer system be used tasks such predicting future resource usage detecting anomalies is challenging problem.
 Most current systems heuristics simplistic assumptions workloads system statistics.
 (cid:140)ese heuristics are one-size-(cid:128)ts-all solution be applicable wide range applications systems environments.
 (cid:140)is limitation is striking considering wide range problems be approached sophisticated model computing system: example, resource process schedulers OSs orchestrators clusters[5, use simple heuristics, o(cid:137)en struggle get performance right[11, monitoring systems objec- tive is detection anomalies have used machine-learning approaches network-based scenarios[2, less more systems-heavy domain.
 Tailoring prediction models speci(cid:128)c situations, however, be complex: have take account interplay systems components running heterogeneous applications, being able adapt changing state system.
 Considering developing generic heuristics is time-consuming task, creating tailor-made solutions hand is worth e(cid:130)ort.
 However, are several recent developments bring closer developing systems models perform be(cid:138)er existing generic methods based simple heuristics.
 Machine learning is becoming e(cid:130)ective e(cid:129)cient learning large amounts data.
 Moreover, have much be(cid:138)er under- standing ways embed heterogeneous feature types (categor- ical, numerical, structured, temporal) joint representation amenable downstream tasks.
 extract collect right input be able create tailor-made models outperform generic heuristics.
 paper, present ongoing work integrating systems telemetry ranging standard resource usage statistics kernel library calls applications machine learning model.
 Intuitively, ML model approximates, point time, state system allows solve tasks such resource usage prediction anomaly detection.
 achieve goal, leverage readily-available information does require changes applications run system.
 train recurrent neural networks such Long Short-Term Memory (LSTM) neural networks [8] learn model system consideration.
 proof concept, train models predict future resource usage running applications.
 DATA COLLECTION learn model system is good predicting future resource need collect data present.
 obvious approach is collect data resources want predict such CPU memory usage statistics.
 (cid:140)is follows idea that, many cases, previous values resource usage have least in(cid:131)uence current resource usage.
 example, memory consumption o(cid:137)en increase decrease time.
 CPU usage is spiky, here, many processes, phases low activity high activity be apparent.
 Such resource usage information is available Unix-like systems; however, is accounted per-process basis.
 (cid:140)is is useful process scheduling more coarse- grained scheduling jobs services, comprise several processes run sequence parallel, need aggregate measurements.
 this, monitor process group, is, processes spawned initial process do request leave group).
 Aggregation is cumbersome is easy way look processes belonging group given group ID; requires traversing processes asking process group belong to.
 (cid:140)is, resource information, be collected reading /proc/<pid>/stat.
 Alternatively, new cgroup be created initial process spawned it.
 (cid:140)e resource requirements process group are resources used cgroup.
 However, high-level usage statistics provide deeper insights state process.
 be useful have least rough understanding process “is doing” runtime.
 possibilities are limited want stay generic require ancillary internal information is speci(cid:128)c certain problem domain (such information input data), requiring speci(cid:128)c compiling linking steps.
 Using pro(cid:128)ler measure functions are being run long, instance, requires symbol table is available (stripped).
 are options, however, inspect program behavior requiring such additional information.
 analyzing system calls program get rough under- standing process is doing information is does rely code annotation additional symbols.
 system calls have obvious relationship certain kinds resources.
 write, read similar system calls work (cid:128)les sockets, translates disk network I/O.
 want predict I/O, relationship is obvious; CPU usage, is strong relationship: example, disk I/O o(cid:137)en correlates low CPU usage, process is waiting I/O accesses (cid:128)nished.
 System calls are traced: strace[10] is standard tool available Unix-like systems, days, overhead is low few percent high number system calls none are system calls happening.
 case workloads high numbers system calls, perf[1] be used sample system calls tracing single one, reducing overhead.
 Conversely, higher level detail is required, ltrace[3] be used catch SysML’18, February Stanford, CA, USA Florian Schmidt, Mathias Niepert, Felipe Huici Figure predict resource collect typical telemetry data le(cid:133)), application’s system calls (bottom le(cid:133)), time period t.
 (cid:135)e variable number calls is transformed (cid:128)xed-size vector word embedding.
 vectors are combined used input LSTM predicts resource usage future time period + i.
 library interactions.
 proof concept, developed ML model integrates usage statistics sequences system calls joint representations.
 DATA PREPROCESSING order prepare data such usage statistics input ML models, need discretize time intervals several reasons.
 First, data makes sense values time period: was CPU utilization last second?
 many bytes were wri(cid:138)en disk?
 Second, eventual goal resource allocation, have predict resource usage time period scheduler uses time slice.
 Finally, calculating resource usage certain time period provides (cid:128)xed-size value: information be interpreted single value be combined input vector (cid:128)xed size.
 system calls, however, (cid:128)xed-size representation is straightforward generate.
 System calls occur (seemingly) random times are discrete events opposed continuous numerical values.
 time period second thousands system calls, none, occur.
 Fortunately, transform se- quences system calls (cid:128)xed-sized vector representation, use representation learning approaches sequence data such word2vec skip-gram model [13].
 applying representation learning approaches sequences words learn meaningful vector representations, apply methods sequences system calls (and other types event sequences occurring system) learn representations events.
 collect corpus system call sequences, ran various types applications run system consideration.
 used data learn system call embeddings model similar skip-gram model [13].
 result, take system calls occurring time period, interpret “sentence,” use event embeddings create (cid:128)xed-size vector representation.
 have (cid:128)xed-sized representa- tion resource usage statistics (cid:128)xed-size representation system calls, use data train ML systems model, shown Figure depicts overall architecture.
 NEURAL NETWORKS FOR SYSTEMS MODELING (cid:140)e objective work is learn maintain model computing system particular level abstraction.
 end, systems are state-based and, given current state, want Figure Looking future tends increase prediction error, taking more history account mitigates e(cid:130)ect.
 use model system make predictions applications future behavior.
 Several recent neural network based machine learning architectures maintain sort in- ternal state.
 Examples are recurrent networks such LSTMs [8] variants [4], memory networks [17, neural Turing machines [6], name few.
 are taking advantage methods developing system model maintains vector (hidden) representation current state system is trained minimize expected error (here: root-mean- square error (RMSE)) predicting future resource usage.
 keep model simple use case resource usage prediction, train LSTM collected preprocessed data con- sisting past usage statistics system calls.
 (cid:140)e learning system calls embeddings be performed preprocessing step end-to-end architecture.
 conducted preliminary experiments collecting sys- tem calls various applications create system call corpus.
 collected resource usage system calls scienti(cid:128)c computing toolchain executed number bash python scripts, interleaved I/O- CPU-heavy phases.
 embedded system calls trained LSTM data.
 (cid:140)e model is trained minimize RMSE CPU usage (as value i seconds future.
 Figure shows results, varying predict future, much history take account prediction.
 Acknowledgments—(cid:140)is project has received funding European Union’s Horizon research innovation pro- gramme grant agreement
 𝐶𝑃𝑈𝑅𝑆𝑆⋮𝐷𝑖𝑠𝑘𝑤𝑟𝑖𝑡𝑒 stat() open() read() stat() stat() open() read() mmap() mmap() write() write() close() Mem CPU 𝐸𝑚𝑏1𝐸𝑚𝑏2⋮𝐸𝑚𝑏𝑛 𝐶𝑃𝑈𝑅𝑆𝑆⋮𝐷𝑖𝑠𝑘𝑤𝑟𝑖𝑡𝑒𝐸𝑚𝑏1𝐸𝑚𝑏2⋮𝐸𝑚𝑏𝑛 𝐶𝑃𝑈𝑡+𝑖𝑅𝑆𝑆𝑡+𝑖⋮𝐷𝑖𝑠𝑘𝑤𝑟𝑖𝑡𝑒,𝑡+𝑖 ⋮ ⋮ error [RMSE]Prediction future [seconds]History taken account1 second10 seconds Representation Learning Resource Usage Prediction SysML’18, February Stanford, CA, USA REFERENCES [1] [3] [n.
 d.].
 perf: Linux pro(cid:128)ling performance counters.
 h(cid:138)ps://perf.wiki.kernel.
 org.
 ([n.
 d.]).
 [2] A.
 L.
 Buczak E.
 Guven.

 Survey Data Mining Machine Learning Methods Cyber Security Intrusion Detection.
 IEEE Communications Surveys Tutorials (Secondquarter
 h(cid:138)ps://doi.org/10.1109/COMST.
 Juan Cespedes al.
 [n.
 d.].
 ltrace: library call tracer.
 h(cid:138)p://www.ltrace.org/.
 ([n.
 d.]).
 Ivo Danihelka, Greg Wayne, Benigno Uria, Nal Kalchbrenner, Alex Graves.

 Associative Long Short-Term Memory.
 Proceedings (cid:138)e Interna- tional Conference Machine Learning.

 [5] Ali Ghodsi, Matei Zaharia, Benjamin Hindman, Andy Konwinski, Sco(cid:138) Shenker, Ion Stoica.

 Dominant Resource Fairness: Fair Allocation Multiple Resource Types.
 Proceedings USENIX Conference Networked Systems Design Implementation (NSDI’11).
 USENIX Association, Berkeley, CA, USA,
 h(cid:138)p://dl.acm.org/citation.cfm?id=1972457.1972490 [6] Alex Graves, Greg Wayne, Ivo Danihelka.

 Neural Turing Machines.
 [4] CoRR abs/1410.5401 (2014).
 [7] Ryan Hnarakis.

 Perfect Xen: A Performance Study Emerging Xen Scheduler.
 Master’s thesis.
 California Polytechnic State University.
 [8] Sepp Hochreiter J¨urgen Schmidhuber.

 Short-Term Memory.
 Neural Comput.
 (Nov.

 h(cid:138)ps://doi.org/10.1162/neco.1997.9. [9] Michael Isard, Vijayan Prabhakaran, Jon Currey, Udi Wieder, Kunal Talwar, Andrew Goldberg.

 (cid:139)incy: Fair Scheduling Distributed Computing Clusters.
 Proceedings ACM SIGOPS Symposium Operating Systems [11] Principles (SOSP ’09).
 ACM, New York, NY, USA,
 h(cid:138)ps://doi.org/10.
 [10] Paul Kranenburg, Dmitry Levin, al.
 [n.
 d.].
 strace: Linux syscall tracer.
 h(cid:138)ps: //strace.io/.
 ([n.
 d.]).
 Jean-Pierre Lozi, Baptiste Lepers, Justin Funston, Fabien Gaud, Vivien (cid:139)´ema, Alexandra Fedorova.

 (cid:140)e Linux Scheduler: A Decade Wasted Cores.
 Proceedings Eleventh European Conference Computer Systems (EuroSys ’16).
 ACM, New York, NY, USA, Article pages.
 h(cid:138)ps://doi.org/10.1145/ [12] Anshul Makkar.

 Scope Performance Credit-2 Scheduler.
 Xen Project Developer Summit.
 [13] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, Je(cid:130) Dean.

 Distributed representations words phrases compositionality.
 Advances neural information processing systems.

 [14] Taeshik Shon Jongsub Moon.

 Hybrid Machine Learning Approach Network Anomaly Detection.
 Inf.
 Sci.
 (Sept.

 h(cid:138)ps: //doi.org/10.1016/j.ins.2007.03.025 [15] Chandandeep Singh Pabla.

 Fair Scheduler.
 Linux Journal (Aug.

 [16] Robin Sommer Vern Paxson.

 Closed World: Using Ma- chine Learning Network Intrusion Detection.
 Proceedings IEEE Symposium Security Privacy (SP ’10).
 IEEE Computer Society, Washington, DC, USA,
 h(cid:138)ps://doi.org/10.1109/SP.2010.25 [17] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus.

 End- To-End Memory Networks.
 Advances Neural Information Processing Systems.

 Jason Weston, Sumit Chopra, Antoine Bordes.

 Memory Networks.
 CoRR abs/1410.3916 (2014).
 [18]
 recent years, neural networks many hidden lay- ers, are called deep neural networks, have been used lot machine learning visual comput- ing tasks, delivering unprecedented good results [15,
 other machine learning approaches relying linear models kernel extensions), neural networks are nonlinear models nonlinear activation functions used neurons.
 Properties ap- plications neural networks have been studied [1].
 earlier, theoretical results, such Universal Approx- ∗Indicates equal contributions.
 imation Theorem, states continuous func- tion compact subset Rn be approximated feed-forward network single hidden layer containing ﬁnite number neurons [5, have alluded po- tential deep neural more understanding has be developed.
 Recent years have witnessed various efforts end.
 particular, are studies aiming investigating reason deeper networks perform better.
 was shown [6], exist families functions be approximated deep neu- ral network shallow one, number hid- den units networks is same.
 researchers investigated relationship depth neural networks complexity func- tions are computable networks.
 was shown that, using Rectiﬁed Linear Unit (ReLU) [7] activation function, number response regions neural network grows number hidden layers polynomial number neurons layer.
 appears be intuitive, more number hidden layers network number hierarchical nonlinear mappings inputs go through, hence enabling ﬂexible representation net- work.
 thread research towards increasing ﬂexi- bility networks is using better activa- tion function, is source nonlinearity model.
 widespread usage ReLU, research activation functions affect ﬂexibility network has been discussed.
 paper, propose novel activation function named Power Linear Unit (PoLU).
 prove networks using PoLU are able increase maximal number re- sponse regions.
 further compare PoLU other state- of-the-art activation functions different networks different datasets.
 Experimental results demonstrate PoLU outperforms leading activation functions have been used.
 Figure
 plot PoLU (n = ReLU ELU.
 plot PoLU n = {1,
 plot derivatives PoLU n = {1,
 Related Work Rectiﬁed Linear Unit (ReLU), is called ramp function sometimes, was applied restricted Boltz- mann machine (RBM) [20], neural networks [7] later.
 is popular activation function nowadays.
 ReLU activation function be deﬁned f R → R, f (x) = max(0, x), x is input neuron.
 Comparing traditional activation functions logistic sigmoid units hyperbolic tangent units, are anti-symmetric, ReLU is one-sided.
 prop- erty encourages network be sparse outputs hidden units are sparse), plausible.
 experiments stated [7], sparsity network be
 Using ReLUs activation functions decreases computational cost, be implemented IF statements.
 Most im- portantly, ReLU alleviates problem vanishing gradi- ent [12], derivative positive part is
 vanishing problem occurs sigmoid hyperbolic tangent units, gradients vanish epochs training (due horizontal asymptotes) stop learning corresponding parts network.
 ReLU has several are potential problems.
 instance, ReLUs ”die” sometimes: neuron outputs corresponding weights be updated again, gradients are
 issue is that, ReLU is non-negative, mean outputs ReLUs layer be positive, leads bias shift effect [4], decrease speed learning.
 overcome problems, researchers proposed variants ReLU.
 Leaky Rectiﬁed Linear Unit (LReLU) [18] sets output be proportional input small proportional constant α (i.e. α =
 is equivalent f (x) = max(x, αx).
 def- inition activation functions, neurons won’t gradient is small non-zero.
 Parametric Recti- ﬁed Linear Unit (PReLU) [9] makes α be learnable parameter ﬁxed constant.
 PReLU needs extra space time complexity learn αs, Randomized Leaky Rectiﬁed Linear Unit (RReLU) [25] was proposed, α is random number sampled uniform dis- tribution.
 Shifted Rectiﬁed Linear Unit (SReLU) Ex- ponential Linear Unit (ELU) [4] try make mean activation towards such gradients are closer natural gradient [16], hence speeding learning process.
 ELU makes gradient negative part be non-zero avoid dead neuron.
 However, considering function ELU negative part, α(ex − see represents slope function x → saturation value (i.e. value y x → −∞) be changed vary value α.
 So, is impossible change slope curve x = keeping asymptote y = −1.
 lat- ter section, demonstrate different saturation performance y = −1 is better choice ELU (see Fig.3).
 like develop activation function slope x → is indepen- dent asymptote saturation.
 proposed Power Linear Units (PoLUs), adopt advantages activation functions mentioned
 output PoLUs positive input is designed be identity avoid gradient vanishing problem.
 Second, PoLUs have non-zero output negative inputs, such output mean units is close zero, reduce bias shift effect.
 Thirdly, is saturation neg- ative part PoLU, makes noise-robust negative inputs.
 Last least, PoLUs are able map more portions layer’s input space same range using power function increase response re- gions neural network.
 More details are presented next section.

 Power Linear Unit section propose PoLU analyze rela- tion PoLU number response regions.
 Based [19], redeﬁne terms such idea be extended general case.
 ﬁrst giving deﬁnitions.
 Deﬁnition
 Let fn R → R be function, Power Linear Unit (PoLU) be deﬁned follows: (cid:40) (cid:40) fn(x) = (1 − x)−n − derivative be expressed as: f(cid:48) n(x) = − x)−n−1 x ≥ x < x ≥ x < (1) parameter n controls rate change PoLUs negative part.
 Fig.1(b) (c) show plots PoLU derivative PoLU different power values n.
 Similar many proposed models activation functions, keeping identity positive section helps PoLUs gradient vanishing problem.
 PoLUs have non-zero outputs saturation plateau negative inputs.
 increase ability learn stable representation, make mean output units closer zero, reduces bias shift effect [4].
 contrast proposed activation functions, PoLU has intersection y = x negative regime n > is proven be good increasing number response regions (see proof Theorem
 ELU has same property α > using such value α push mean activation (as α is scaling factor), leads worse performance.
 Deﬁnition
 [19] Let F RM → RN be map, S ⊆ RM T ⊆ RM
 F identiﬁes S T F (S) = F (T ).
 Deﬁnition
 response region function F is maxi- mal connected subset domain F is differen- tiable monotonic.
 proposed activation function is nonlinear neg- ative input, deﬁne response region general way requiring F be differentialbe monotonic being linear).
 deﬁnition, Lemma [22] leads Theorem proofs hold un- der deﬁnition

 maximal number response regions functions computed neural network, has n0 input units, hidden layer n1 PoLUs, is bounded by(cid:80)n0 j=0 (cid:0)n1 (cid:1).
 Figure
 plots different curves n
 (a) blue: y = fn(x); red: y = fn(−x); (b) y = ˆϕn(x); (c) y = ϕn(x,
 distance troughts origin is (d) blue: y = S2(x); red(upper): y = b; red(lower): y = a.
 intervals are subsets {x | < S2(x) < b} are mapped same set.

 maximal number response regions functions computed neural network, has n0 input units, L hidden layers ni ≥ n0 PoLUs i-th layer, is bounded (cid:32)L−1(cid:89) (cid:22) (cid:23)n0(cid:33) n0(cid:88) (cid:18)nL (cid:19) i=1 j=0 (3) Proof.
 ﬁrst PoLUs. Let n > ˆϕn R → R be function deﬁned as: ˆϕn(x) = fn(x) + fn(−x) (4) is easy prove is intersection y = x negative region PoLU n > causes local minima exist y = ˆϕn(x).
 function formed ReLUs, following construction [19], identify regions, ˆϕn, is formed PoLUs, identify regions.
 inputs go afﬁne maps reach activation functions, choose weights bias afﬁne transformations.
 Consider modiﬁed version ˆϕn, ϕn R × [0, → R, is deﬁned follows: ϕn(x, d) = fn(an(d)x + bn(d)) + fn(−an(d)x + bn(d)) choose suitable > b (i) rescale ϕ such ϕ(−1, d) = ϕ(0, d) = ϕ(1, d).
 (ii) separate troughs horizontal line length equals < Fig.
 is plot distance troughs origin is equal Consider layer n1 PoLUs n0 input, n1 n0.
 PoLUs are separated n0 disjoint subsets cardinality p, is largest number greater n1/n0 (i.e. p = ≤ (cid:98)n1/n0(cid:99) k ∈ N), remaining PoLUs are ignored.
 loss generality, consider j-th input layer, j ∈ {1, ..., n0}.
 choose input weights biases, functions be constructed following way: h1(x) = ϕn(x, d1) h2(x) = ϕn(x, d2)
 hk(x) = ϕn(x, dk) (6) let (−ci, hi(−ci)) (ci, hi(ci)) be coordinates local minima hi(x) i ∈ {1, ..., k}.
 use pair fn construct function.
 Fig.2(a) (b) show plots h1 h2 respectively.
 construct function deﬁned follows: SN (x) = aihi(x) ∀N ∈ {1, ..., k} N(cid:88) i=1 ˜h(x) = Sk(x) (7) (8) h1, set d1 such troughs are stick together.
 h2, set d2 > c1 ensure are local minima function y = S2(x) = a1h1(x) + a2h2(x) constant a1, a2.
 keep constructing Si i > similar way.
 general, have d1 < c1 < d2 < c2 < d3 < c3 <
 < dk < ck <
 setting, have troughs y Sk(x).
 choosing suitable coefﬁcients construct function p local minima, have same value y-coordinates.
 Fig.2(d) shows plot y = S2(x), intervals [−1,−c2], [−c2,−d2], [−d2,−c1], [−c1, [0, c1], [c1, d2], [d2, c2], [c2, exists least subset interval such subsets are mapped same interval S2.
 Now, following intervals: [−1,−ck], [−ck,−dk], [−dk,−ck−1], ..., [−c1, [0, c1], [c1, d2], ..., [ck−1, dk], [dk, ck], [ck, (9) is subset intervals is mapped same interval.
 Therefore, ˜h identiﬁes = regions input domain.
 ˜h is linear combination h1, ..., hk, treat output current layer neural net- work.
 considering n0 subset, conclude that, ˜h identiﬁes regions total.
 Using same strategy [22, conclude that, maximal number response regions computed neural network using PoLUs is bounded (cid:4) (cid:107)n0(cid:17)(cid:80)n0 (cid:106) ni (cid:0)nL (cid:1).
 i=1 j=0 proof holds neural networks PoLUs(n > ELUs(α have intersection y = x negative input.
 networks PoLUs(n ≤ ELUs(α ReLUs, lower bounds remain same [19].
 number response re- gions functions computed neural network, is measure ﬂexibility network, networks PoLUs(n > ELUs(α > are considered be more ﬂexible.
 Note that, prove PoLU has more response regions ELU does, cannot deter- mine new activation functions increase number response regions have better perfor- mance.
 are many factors affect performance network, bias shift value saturation negative input, etc.
 Hence, ”the larger number response regions, better network is” is ﬁnal conclusion only advantage PoLU has.

 Experiments section, ﬁrst evaluate impact α ELUs setting α ELU-Network CIFAR-100 dataset.
 evaluate PoLUs different convolutional neural networks using different power values n ∈ {1, ﬁve benchmark datasets: MNIST [17], CIFAR-10 [14], CIFAR-100 [14], Street View House Num- bers [21], ImageNet [23].
 Compared other state- of-the-art activation functions, including Exponential Lin- ear Units (ELUs), used activation func- tion, Rectiﬁed Linear Units (ReLUs), convolutional neural networks PoLUs present best performance datasets.
 Several deep neural networks different number layers are implemented demonstrate Po- LUs are compatible ELUs ReLUs convo- lutional neural networks.
 experiments are implemented using deep learning toolbox Keras [2] tensorﬂow back- end.
 Note that, run experiments activation func- tions SReLU Leaky ReLU well, obtained similar re- sults comparable reported ELU paper (i.e. accuracy difference is
 Therefore, make Figure
 plots show results using simple-ELU-Net CIFAR-100 dataset, using ELU α ∈ {0.5,
 (a-b) testing error (c) plots ELU different α.
 ELU α = has best performance them.
 Table
 table shows result MNIST dataset SVHN dataset.
 SVHN, Simple-ELU-Net network are used testing.
 MNIST, is used testing.
 use ReLU, ELU PoLU(n = datasets.
 best result network is bold.
 SVHN MNIST CNN + NN simple-ELU-Net ReLUs ELUs PoLUs (n=1) PoLUs (n=1.5) PoLUs (n=2) plots more clear easier demonstrate differences ELU, PoLU baseline activation function ReLU, do contain performance SReLu Leaky ReLU plots.
 MNIST dataset MNIST dataset [17] contains training testing samples × pixel size.
 im- age is drawn greyscale handwritten digits 0-9.
 MNIST dataset has been studied evaluating dif- ferent neural networks, utilize dataset assess performance PoLUs shallow neural net- work.
 train network convolutional layers connected layers followed softmax layer is arranged stack [1× [1× C], [1×10×Sof tmax]).
 max-pooling layer stride is applied end second stack.
 leverage ratio regularize network.
 results are provided Table ReLUs achieved best results testing error equaling
 CIFAR-10 CIFAR-100 dataset CIFAR-10 CIFAR-100 [14] datasets are similar.
 datasets contain color images size, are split training testing samples.
 only difference is CIFAR-10 is drawn classes CIFAR-100 con- tains classes.
 same neural network struc- tures followed different softmax layer are implemented datasets.
 neural networks implemented are ELU-Network convolutional layers [4](named simple-ELU-Net), ELU-Network convolutional layers [4](named ELU-Net), (iii) VGG16-structure-like neural network [24], (iv) Deep Residual Network (ResNet) 50-layer structure [10].
 mentioned implement ELU- Network convolutional layers ELUs assigned different α values evaluate effect α relationship slope saturation.
 assessment PoLUs assigned different power values n are based comparison ELUs ReLUs utilizing ELU-Network containing convolutional layers CIFAR-100 dataset.
 simple ELU-Network is arranged stacks ([1 × × [1× [1× [1× [1× [1× [1× (in stack, ﬁrst number is num- ber layers, second number represents number ﬁlters third number is size ﬁlter).
 stack last stacks), max-pooling layer stride is applied.
 Followed instruction [4], dropout ratio Figure
 plots are results using Simple-ELU-Net different activation functions (PoLU(n ∈ {1, ELU, ReLU) CIFAR-100 dataset.
 (a)-(c): training loss; (d)-(f): testing error.
 curve represents average runs.
 PoLU n achieves best performance.
 is applied last layer stack L2-weight decay regularization is set
 Learning rate is initial- ized drop
 learn- ing rate decays factor epochs.
 total number epochs is
 Stochastic Gradient Descent is used momentum set Global contrast normaliza- tion ZCA whitening are mentioned [8] are applied dataset.
 Moreover, training samples are cropped × size random horizon- tal ﬂipping images padded pixels borders.
 network different activation function is trained times, mean standard deviation testing errors CIFAR-10 CIFAR-100 are shown Table
 Fig.4 presents comparison different activa- tion functions based testing error training loss CIFAR-100.
 results demonstrate PoLUs power value n achieves best results oth- ers, satisfy prediction.
 Additionally, training loss PoLUs different power values drops faster ReLU comparative speed ELU.
 evaluate deeper complex neural networks contain more parameters, implemented layers.
 VGG16-structure-like neural network ELU- Network convolutional VGG16- structure-like neural work is derived [24], number units last connected layers changed size input feature maps are ×
 structure neural network is arranged stacks ([2 × × [2 × × [3 × [3× [3× [2× F C], [1× × F C]).
 Max-pooling size × stride is applied stack last con- nected layers.
 dropout ratio is set last connected layers L2-regularization term convolutional layer is set
 learning rate is initialized decays factor epochs.
 optimizer neural network is SGD momentum equals sophisticated ELU- Network is arranged stacks ([1 × × [1 × × [1× [1× [1×896×3, [1×1024× × × [1 × × [1 × ×
 Similar simpler ELU-Network mentioned is 2× max-pooling layer stride stack Table
 table shows result CIFAR-10 dataset.
 Different Networks (simple-ELU-Net, ELU-Net, VGG16, ResNet-50) activation function (ReLU, ELU, PoLU(n = are used.
 best result network is bold.
 ReLUs ELUs PoLUs (n=1) PoLUs (n=1.5) PoLUs (n=2) ResNet-50 VGG16 simple-ELU-Net ELU-Net last stacks.
 padding is applied convo- lutional layer keep dimension unchanged.
 initial dropout ratio, L2-Regularization, momentum value are same simpler ELU-Network.
 VGG16- structure-like network sophisticated ELU-Network, dataset is preprocessed described [8] global contrast normalization ZCA whitening.
 Thanks success much deeper neural networks image classiﬁcation, experiments different ac- tivation functions take deeper neural network such Deep Residual Network (ResNet) consider- ation.
 Comparing neural networks mentioned above, ResNet is much deeper structure is kind different is shortcut connection stack input next stack input.
 described [10], skip connection structure solve vanishing gradient problem is described previous section.
 difference is, is batch normalization layer activation function.
 Batch normalization provide input batch stack zero mean unit variance, compensate disad- vantages ReLUs. Therefore, difference perfor- mance ReLUs, ELUs, PoLUs ResNet is signiﬁcant, compared other networks.
 Such phe- nomenon is observed Table
 However, implemented ResNet-50 last residual stages ending utilizing convolutional layer stride downsample feature maps, result worse performance is described [10].
 difference test errors ResNet-50 ReLUs CIFAR-10 dataset is
 evaluation different α ELUs CIFAR-100 dataset using ELU-Network convolutional layers is presented Fig.3. results show ELUs α = achieve better performance compared α = α saturation value is small large, push mean activation
 is evidence disadvantages ELUs, mentioned previous section.
 Street View House Number (SVHN) dataset Street View House Number (SVHN) [21] dataset is collected Google Street View, focusing color images house numbers.
 are formats dataset, second one was used experi- ments.
 second format, image is ﬁxed size × pixels most center part image is digit.
 are training testing sam- ples.
 are extra samples be used additional training samples.
 SVHN be viewed similar MNIST datasets are focused digits.
 However, SVHN is harder MNIST images SVHN are cropped real world color images.
 Moreover, size image is × neural networks are designed CIFAR-10 CIFAR-100 be implemented SVHN.
 im- plemented ELU-Network convolutional layers shallower neural network contains convolutional layers connected layer followed softmax layer.
 shallow neural network is arranged stack ([1×32×3], [1×32×3], [1× [1× F C], [1× Sof tmax]).
 dataset is preprocessed followed [26] local con- trast normalization.
 training testing samples are used, set extra samples is considered.
 ﬁnal performance is shown Table
 ImageNet evaluate PoLU ImageNet dataset con- tains more training images belonging classes.
 are images val- idation testing respectively.
 network imple- mented has same structure proposed [4], layers are arranged ([1 [3 × [5× [3× F C], [1× × Sof tmax]).
 stack, × max-pooling layer stride is applied spatial pyramid pool- ing (SPP) [1,2,4] levels is employed ﬁrst FC layer [9].
 Fig.5, network ELU has sim- ilar performance network PoLU (a) (b) plots observe that, comparing net- work ELUs, one PoLUs has lower loss last several epochs, has better ﬁ- nal accuracy plots (c) (d).
 Table
 table shows result CIFAR-100 dataset.
 Different Networks (simple-ELU-Net, ELU-Net, VGG16, ResNet-50) activation function (ReLU, ELU, PoLU(n = are used.
 best result network is bold.
 ReLUs ELUs PoLUs (n=1) PoLUs (n=1.5) PoLUs (n=2) ResNet-50 VGG16 simple-ELU-Net ELU-Net ReLUs are signiﬁcant.
 experiments, implemented power value n >
 However, due too-large slope x → activation functions are sensitive input values.
 PoLUs map inputs little difference larger area Po- LUs reach saturation rate change to- wards saturation is increased.
 Moreover, experiments implemented ELUs setting differ- ent values α, draw conclusion slope saturation cannot be set large.
 time complexity experiments, demon- strated Fig.
 Fig.
 similar ELU, PoLU achieve same training loss training accuracy less epochs compared ReLU.
 Therefore, PoLU is complex ReLU takes more training time training time reaching best accuracy ReLU get is less.
 Note extra time spent calculation acti- vation function is small portion, comparing total computational time network.
 training same number total training time ELU/PoLU network ReLU network ImageNet are hours hours respectively.

 Conclusions proposed new activation function Power Linear Unit (PoLU), uses identity function power func- tion construct positive negative sections respec- tively.
 PoLU is direction efforts ELU, are variants ReLU, PoLU more attention is negative part activation func- tion.
 was motivated observation saturation value slope negative part activation function affect perfor- mance network.
 contrast, ELU focuses better saturation values using exponential computation.
 Note saturation value change change slope.
 have shown ELU saturation value performs best, implies slope cannot be changed want use optimal value.
 proposed PoLU avoid dilemma: achieve desired saturation being able adjust slope based changing power parameter n, means negative region Figure
 plots training loss testing error(%) ImageNet.
 training loss iterations; (b) testing error(%) iterations; (c) training loss -250kth iterations; (d) testing error(%) iterations.
 Discussion result performance different neural networks dif- ferent activation functions illustrates PoLUs ELUs are better ReLUs deep neural network are batch normalization layers, indicates PoLUs ELUs overcome bias shift problem push input’s mean other side.
 However, result MNIST dataset shows ReLU work ELU PoLU n = suggests bias shift effect have less inﬂuence shallow neu- ral networks.
 observation comparison ELU-Network VGG16-structure-like network CIFAR-100 dataset is that, network consists convolutional layers, ELU-Network, PoLUs ELUs, contain negative part, perform better ReLUs, are connected lay- ers, difference performance PoLUs, ELUs Advances neural information processing systems, pages
 Y.
 Le Cun, I.
 Kanter, S.
 A.
 Solla.
 Eigenvalues co- variance matrices: Application neural-network learning.
 Physical Review Letters,
 Y.
 LeCun, L.
 Bottou, Y.
 Bengio, P.
 Haffner.
 Gradient- based learning applied document recognition.
 Proceed- ings IEEE,
 [18] A.
 L.
 Maas, A.
 Y.
 Hannun, A.
 Y.
 Ng. Rectiﬁer nonlin- Proc.
 earities improve neural network acoustic models.
 ICML, volume
 G.
 F.
 Montufar, R.
 Pascanu, K.
 Cho, Y.
 Bengio.
 number linear regions deep neural networks.
 Advances neural information processing systems, pages
 [20] V.
 Nair G.
 E.
 Hinton.
 Rectiﬁed linear units improve restricted boltzmann machines.
 Proceedings 27th international conference machine learning (ICML-10), pages
 Y.
 Netzer, T.
 Wang, A.
 Coates, A.
 Bissacco, B.
 Wu, A.
 Y.
 Ng. Reading digits natural images unsupervised fea- ture learning.
 NIPS workshop deep learning un- supervised feature learning, volume page
 R.
 Pascanu, G.
 Mont´ufar, Y.
 Bengio.
 number inference regions deep feed networks piece- wise linear activations.
 CoRR, abs/1312.6098,
 [23] O.
 Russakovsky, J.
 Deng, H.
 Su, J.
 Krause, S.
 Satheesh, S.
 Ma, Z.
 Huang, A.
 Karpathy, A.
 Khosla, M.
 Bernstein, A.
 C.
 Berg, L.
 Fei-Fei.
 ImageNet Large Scale Visual Recognition Challenge.
 International Journal Computer Vision (IJCV),
 K.
 Simonyan A.
 Zisserman.
 deep convolu- tional networks large-scale image recognition.
 CoRR, abs/1409.1556,
 [25] B.
 Xu, N.
 Wang, T.
 Chen, M.
 Li. Empirical evaluation rectiﬁed activations convolutional network.
 arXiv preprint arXiv:1505.00853,
 [26] M.
 D.
 Zeiler R.
 Fergus.
 Stochastic pooling regular- ization deep convolutional neural networks.
 arXiv preprint arXiv:1301.3557,
 PoLU intersect y = x keeping same asymp- tote saturation means setting n >
 demonstrated advantage having proper slope negative part activation function, networks using PoLU have larger number response regions, helps improve nonlinearity capacity neural net- work.
 Experimental results showed PoLU outperforms other state-of-the-art most networks.
 References [1] M.
 Anthony P.
 L.
 Bartlett.
 Neural network learning: Theoretical foundations.
 cambridge university press,
 [2] F.
 Chollet al.
 Keras.
 fchollet/keras,
 https://github.com/ [3] D.
 Ciregan, U.
 Meier, J.
 Schmidhuber.
 Multi-column deep neural networks image classiﬁcation.
 Computer Vision Pattern Recognition (CVPR), IEEE Confer- ence on, pages
 IEEE,
 D.-A.
 Clevert, T.
 Unterthiner, S.
 Hochreiter.
 Fast accurate deep network learning exponential linear units (elus).
 arXiv preprint arXiv:1511.07289,
 [5] G.
 Cybenko.
 Approximation superpositions sig- moidal function.
 Mathematics Control, Signals, Sys- tems (MCSS),
 O.
 Delalleau Y.
 Bengio.
 Shallow vs.
 deep sum-product Advances Neural Information Processing networks.
 Systems, pages
 X.
 Glorot, A.
 Bordes, Y.
 Bengio.
 Deep sparse rectiﬁer neural networks.
 Aistats, volume page
 [8] I.
 J.
 Goodfellow, D.
 Warde-Farley, M.
 Mirza, A.
 Courville, arXiv preprint Y.
 Bengio.
 Maxout networks.

 [9] K.
 He, X.
 Zhang, S.
 Ren, J.
 Sun.
 Delving deep rectiﬁers: Surpassing human-level performance imagenet classiﬁcation.
 CoRR, abs/1502.01852,
 [10] K.
 He, X.
 Zhang, S.
 Ren, J.
 Sun.
 Deep residual learn- ing image recognition.
 Proceedings IEEE Con- ference Computer Vision Pattern Recognition, pages
 [11] G.
 Hinton, L.
 Deng, D.
 Yu, G.
 E.
 Dahl, A.-r.
 Mohamed, N.
 Jaitly, A.
 Senior, V.
 Vanhoucke, P.
 Nguyen, T.
 N.
 Sainath, al.
 Deep neural networks acoustic modeling speech recognition: shared views research groups.
 IEEE Signal Processing Magazine,
 S.
 Hochreiter.
 vanishing gradient problem learn- Interna- ing recurrent neural nets problem solutions.
 tional Journal Uncertainty, Fuzziness Knowledge- Based Systems,
 K.
 Hornik, M.
 Stinchcombe, H.
 White.
 Multilayer feed- forward networks are universal approximators.
 Neural net- works,
 A.
 Krizhevsky G.
 Hinton.
 Learning multiple layers features tiny images.

 [15] A.
 Krizhevsky, I.
 Sutskever, G.
 E.
 Hinton.
 classiﬁcation deep convolutional neural networks.
 Imagenet
 world is dynamic, constant state ﬂux.
 Traditional learning systems learn static models historical data are unable adjust concept drift — changes distributions data are drawn.
 growing body experimental machine learning research investigates incremental learn- ers seek adjust models appropriate confronted concept drift (Gaber Gama Rodrigues, Aggarwal, ˇZliobaite, Bifet Nguyen Brzezinski Stefanowski, Krempl Gama et Ditzler et
 paper seeks inform line research identifying relationships types Nayyar A.
 Zaidi, Geoﬀrey I.
 Webb, Francois Petitjean, Germain Forestier Faculty Information Technology, Monash University, Clayton, VIC Australia E-mail: {ﬁrstname.lastname}@monash.edu Nayyar A.
 Zaidi al.
 Fig.
 illustration hypothesized sweet way interaction drift rate, forgetting rate, learner’s bias variance expected error.
 concept drift properties learners best handle forms drift.
 Speciﬁcally, propose investigate hypotheses —
 drift-rate/forgetting-rate nexus.
 rate concept drift increases, model accuracy general be maximized increasing forgetting rates, conversely, drift rate decreases, model accuracy general be maximized decreasing forgetting rates.
 increasing forgetting rates means focusing recent evidence reducing window sizes increasing decay rates.
 Decreasing forgetting rates means focusing longer term evidence increasing window sizes decreasing decay rates.

 forgetting-rate/bias-variance-proﬁle nexus.
 forgetting rates increase, model accuracy general be maximized altering bias/variance proﬁle learner decrease variance, conversely, forgetting rates increase, model accuracy general be maximized decreasing bias.
 ﬁrst hypotheses is intuitive.
 faster world is changing, less relevance older information have.
 consequence, aggressive forgetting mechanisms, smaller windows higher decay rates, be required exclude older examples trade-oﬀ provid- ing additional information is relevant current state-of-the-world is misleading is weighted latter.
 second hypothesis derives hypothesis learning smaller quantities data, lower variance learners maximize accuracy due ability avoid whereas learning larger datasets lower bias learners maximize accuracy, due ability model details present large data (Brain Webb,
 past data forget, smaller eﬀective data quantity learn and, therefore, high-forgetting rate, low-variance models are desirable low-bias models low-forgetting rate.
 Drift RateSlowFastModel Forgetting RateSlowFastModel Bias- VarianceLow BiasHigh BiasLow VarianceHigh VarianceThe Sweet Path Title Suppressed Due Excessive Length Put hypothesized eﬀects imply sweet path concept drift illustrated Figure whereby lowest error low drift rate be achieved low bias learner low forgetting rate lowest error high drift rate be achieved low variance learner high forgetting rate.
 bulk paper (Section Section comprises detailed experi- ments investigate hypotheses hypothesized sweet path.
 discuss implications directions future research Section
 Background supervised machine learning seek learn model M predict value (or probability value) y target variable Y example x = (cid:104)x1,


 xa(cid:105) input variable X = (cid:104)X1,


 Xa(cid:105).
 learn M training set T = {(cid:104)x1, y1(cid:105),


 ,(cid:104)xs, ys(cid:105)}.
 incremental learning, training set is presented learner sequence period time learner updates M light new example set examples is encountered.
 Concept drift occurs distribution Pt(X, Y data are drawn time t diﬀers subsequent time u, Pu(X, Y measure magnitude drift time t u, D(t, u), mea- sure distance probability distributions Pt(X, Y Pu(X, Y rate drift time t by: Ratet = lim n→∞ nD (t−0.5/n, t+0.5/n) (1) (Webb et
 observations paper hold distance mea- sure is metric, such Total Variation Distance (Levin et
 Forgetting mechanisms are standard strategy dealing concept drift.
 main forgetting mechanisms are windowing, sliding window is maintained containing W recent examples; decay weighting, greater weight is placed recent examples lesser weight older ones (Gama et
 Experimental setup explore drift-rate/forgetting-rate/bias-variance-proﬁle nexus, re- incremental learner learn sliding windows decay.
 course, require means varying learner’s bias/variance proﬁle.
 experiments, use semi-naive Bayesian method AnDE (Webb et satisﬁes requirements.
 model has tuneable Note papers (Gama et al, distinguish real concept drift P (Y | X) changes, virtual concept drift P (X) changes.
 purposes paper do distinguish these, distinction does appear pertinent.
 Nayyar A.
 Zaidi al.
 parameter n controls representation bias variance.
 n = (in AnDE), gets naive Bayes classiﬁer is biased has low variance.
 Higher values n decrease bias cost increase variance lower values decrease variance cost increased bias.
 Second, AnDE model be represented using counts observed marginal frequencies, dimensionality is controlled n.
 described be updated reﬂect sliding window incremental decay need relearning entire model.
 goal Bayesian methods is factorize joint distribution: P(y, x).
 AnDE model factorizes joint distribution as: (cid:88) a(cid:89) i=1 δ(xs)ˆP(y, xs)  (cid:1) indicates set size-n subsets {1,


 a} δ(xα) is s∈(A n) ˆPA(n-1)DE(y, x) ˆP(xi | y, xs)/ (cid:88) s∈(A n) (cid:88) s∈(A n) (2) δ(xs) δ(xs) ˆPAnDE(y, x) = (cid:0)A function is training data contains object value xα,
 Window-based Adaptation A sliding window supports learning last W data points be achieved queue-based data structure.
 time step new data point (cid:104)x, y(cid:105) arrives: – Increment relevant count statistics based (cid:104)x, y(cid:105) – Push (cid:104)x, queue – queue length exceeds W – (cid:104)˜x, ˜y(cid:105) = De-queue.
 – Decrement relevant count statistics based (cid:104)˜x, be seen parameter W controls forgetting rate.
 Large W means large windows, slow forgetting small W means small windows, hence fast forgetting.
 Decay-based Adaptation support incremental exponential decay, adding count statistics data point x step t, is required is counts count table are decayed.
 example Nxi,y denotes stored count number times attribute i takes value xi class attribute takes value y, is decayed as: Nxi,y = Nxi,y ∗ exp(−D), Title Suppressed Due Excessive Length Fig.
 Simple illustration structure superparent k-DB classiﬁers.
 (Left) super- parent 1-DB, variable takes more parent other class, (Right) superparent 2-DB, variable takes more parents other class.
 D is decay parameter.
 W window-based be seen parameter D controls model adaptation rate.
 Large D means large decay, hence fast model adaptation-rate, and, small D means small decay, slow model adaptation-rate.
 Data Generation test hypotheses, require data streams varying drift rates.
 end create framework generate synthetic data manipulate rate drift.
 represent probability distribution using common formal- ism doing Bayesian network.
 changing probability dis- tribution node change probability distributions children descendant nodes, order allow systematic manipula- tion drift rate minimize number parent nodes.
 Speciﬁcally, sample superparent k-DB (Keogh Pazzani, distributions.
 show structure superparent 2-DB Figure
 superparent k- DB model, attribute Xi other ﬁrst k attributes takes X1,


 Xk Y parents.
 superparent 1-DB structure attribute Xi other X1 takes Y X1 parents superparent 2-DB structure attribute other X1 X2 takes Y X1 X2 parents.
 structures are shown Figure
 Specifying such networks is simple standard process is:
 Specify empty) set πXi parents node Xi. 0-DB1-DB2-DBP(xi|y)P(xi|y,xj)P(xi|y,xi,xk)yx1x2x3xayx1x2x3xayx1x2x3xa Nayyar A.
 Zaidi al.

 Conditional-Probability-Table (CPT) node.
 speciﬁes probability distribution values attribute combi- nation values parents.
 network is speciﬁed, use Ancestral Sampling generate data therefrom.
 use following simple (heuristic) procedure generate network: – All attributes including class are binary.
 – Set initial number attributes
 – aim half attributes have parents, remaining half have parents order resulting distributions do ﬁt biases single AnDE learner.
 Note, however, are creating superparent k-DB structures, X1 have Y parent X2 have X1 Y parents.
 Therefore, practice, have attribute parent, attributes parents attributes parents.
 – allow direct control rate drift, do allow parent at- tributes drift.
 maximize diversity, wish minimize number parent attributes, is use superparent 1-DB 2-DB structures more general k-DB structures.
 end, X3 X50, random select X1 X2, Y parents.
 X51 X100, Y X1 X2 are assigned parents.
 – structure is speciﬁed, randomly initialize CPTs. Note, have fulﬁll sum-to-one constraint P(Xi = | πXi) + P(Xi = | πXi) =
 end, combination values parent randomly select value P(Xi = | πXi), set P(Xi | πXi) = − P(Xi = | πXi).
 – Next, add more binary attributes have parents hence represent noise.
 CPTs nodes are initialized above.
 total, have attributes.
 sample distribution deﬁned network, we: – Choose class y uniformly random selecting
 – i = sample xi distribution deﬁned P (Xi y, x1,


 xi−1).
 Introducing Drift introduce drift want change CPTs controlled manner.
 need strictly control change need systematically increase decrease rate drift.
 end ensure — – Y X1 X2 (the nodes are parents other nodes) do drift, therefore, CPTs be changed data generation process.
 – Drift occurs T steps.
 – Drift inﬂuences X% attributes.
 Title Suppressed Due Excessive Length ﬁrst constraint is necessary changing parent probabilities change child probabilities complex manner is diﬃcult manage.
 second third constraints ensure is short term directionality drift.
 Simply drifting attribute drift rate step results steps canceling previous step attribute.
 However, attributes are drifted step drift occurs T steps ensures non-trivial drift lasts non-trivial period time.
 experiments, set T X is set
 is, half attributes are selected 10-th step are drifted.
 drifting process is controlled single parameter ∆.
 method is designed ensure higher value ∆ leads fast drift, smaller value ∆ lead slow drift.
 node is drifted, ∆ is added subtracted CPT values manner ensure values sum value exceeds falls hence maintaining valid probability distribution.
 Experimental Analysis have seen parameter n controls bias/variance proﬁle model, parameters W D control forgetting rate, parameter ∆ controls rate drift.
 section, present experiments study interaction factors.
 use ∆ represent drift, ∆ = medium drift ∆ = slow drift.
 select wider range values explored exemplars demonstrate clear diﬀerences outcomes.
 example rates presented, ∆ = is clear fast medium forgetting rates provides lower error, ﬁrst hypothesis predicts.
 generate data streams successive time steps, time step drawing example randomly probability distribution step drifting distribution steps.
 use prequential evaluation, whereby time step current model is applied classify next example data stream example is used update model.
 plot resulting error rates, point plot is average error successive time steps.
 run experiment times NB A1DE times A2DE (due there being insuﬃcient time complete more runs).
 present averages runs.
 ﬁrst present results fast drift.
 Figure presents results using win- dows forgetting Figure presents results using decay forgetting.
 average prequential error window size decay rate achieves lowest such error is listed classiﬁers (Figures
 see NB, predicted, lowest error is achieved fast forgetting (window size decay rate
 Nayyar A.
 Zaidi al.
 (a) (b) (c) (d) Fig.
 Windowing Fast Drift (∆ = – Variation prequential loss NB (Figure A1DE (Figure A2DE (Figure window sizes
 Figure Comparison NB (error = A1DE (error = A2DE (error = window size.
 However, contrary expectations, A1DE A2DE achieve lowest error slower forgetting.
 is models fail face such rapid drift.
 Recall A1DE estimate attribute Xi attribute Xj P(Y, Xi) P(Xj | Y, Xi).
 A2DE estimate attribute Xi, attribute Xj attribute Xk – P(Y, Xi, Xj) P(Xk | Y, Xi, Xj).
 distributions Y X1 X2 are drifting, others are drifting rapid rate.
 Larger window sizes classiﬁers produce more accurate estimates unvarying probabilities, P(Y, X1), P(Y, X2), P(X1 | Y, X2), P(X2 | Y, X1) P(Y, X1, X2), whereas window size provides accurate estimates remaining probabilities are small provide accurate estimates distributions change duration window estimate be accurate.
 relative error is dominated ability estimate invariant probabilities performance approximates learning stationary distribution majority attributes are noise attributes.
 intermediate drift rate (∆ < intermediate bias learner A1DE starts outperform NB.
 Figures show prequential 0-1 Loss Step (t)0.20.30.40.50.6ErrorNB (0.05)W = = = Step (t)0.20.30.40.50.6ErrorA1DE (0.05)W = = = Step (t)0.20.30.40.50.6ErrorA2DE (0.05)W = = = Step (t)0.20.30.40.50.6ErrorNB vs.
 A1DE vs.
 A2DENBA1DEA2DE Title Suppressed Due Excessive Length (a) (b) (c) (d) Fig.
 Decay Fast Drift (∆ = – Variation prequential loss NB (Figure A1DE (Figure A2DE (Figure decay rates
 Figure Comparison NB (error = A1DE (error = A2DE (error = decay rate.
 diﬀerent window sizes decay rates drift delta size
 is apparent intermediate drift rate intermediate window size (50) intermediate decay rate (0.05) lowest error learner intermediate bias (A1DE) minimizes overall error.
 Figures shows comparison NB, A1DE A2DE best respective window size decay rate, be seen A1DE results best performance.
 Figures show prequential error slow drift rate, ∆ = varying window sizes decay rates.
 Figure compares perfor- mance models respective best window size Figure best decay rate.
 cases is apparent A2DE achieves lowest error.
 Thus, scenarios fast, intermediate slow drift ﬁnd relationship predicted sweet path drift rate, forgetting rate bias/variance proﬁle.
 Step (t)0.10.20.30.40.50.6ErrorNB (0.05)D = = = Step (t)0.20.30.40.50.6ErrorA1DE (0.05)D = = = Step (t)0.20.30.40.50.6ErrorA2DE (0.05)D = = = Step (t)00.10.20.30.40.50.6ErrorNB vs.
 A1DE vs.
 Nayyar A.
 Zaidi al.
 (a) (b) (c) (d) Fig.
 Windowing Medium Drift (∆ = – Variation prequential loss NB (Figure A1DE (Figure A2DE (Figure window sizes
 Figure Comparison NB (error = A1DE (error = A2DE (error = window size.
 Exploiting insights sweet path designing practical learners have demonstrated generalizable falsiﬁable hypothesis is consis- tent experimental outcomes.
 raises question resulting insights be used create new eﬀective learners respond con- cept drift.
 Unfortunately, doing appears be trivial.
 allow drift rates vary sweet path suggests need learners adapt changes drift rates corresponding adapta- tion forgetting rates bias/variance proﬁles.
 are devise practical algorithmic solution complex problem.
 assess practical implications hypotheses designing prac- tical learning compare performance real-world drifting data AnDE classiﬁers diﬀering forgetting rates bias/variance proﬁles range state art concept drift classiﬁers.
 use standard benchmark drift datasets, details are given Table
 Nu- Step (t)00.10.20.30.40.50.6ErrorNB (0.01)W = = = Step (t)00.10.20.30.40.50.6ErrorA1DE (0.01)W = = = Step (t)00.10.20.30.40.50.6ErrorA2DE (0.01)W = = = Step (t)0.020.040.060.080.10.120.14ErrorNB vs.
 A1DE vs.
 A2DENBA1DEA2DE Title Suppressed Due Excessive Length (a) (b) (c) (d) Fig.
 Decay Medium Drift (∆ = – Variation prequential loss NB (Fig- ure A1DE (Figure A2DE (Figure varying decay rates
 Figure Comparison NB (error = A1DE (error = A2DE (error = decay rates.
 #Instances #Attributes #Classes PowerSupply Airlines ElectricNorm Sensor Table Details datasets.
 meric attributes are discretized using IDAW discretization (Webb, intervals.
 compare performance standard learning techniques:
 AccuracyUpdatedEnsemble (Brzezinski Stefanowski,
 AccuracyWeightedEnsemble (Wang al,
 DriftDetectionMethodClassiﬁer (Gama et
 DriftDetectionMethodClassiﬁerEDDM (Baena-Garcıa et al,
 HoeﬀdingAdaptiveTree (Bifet Gavald`a,
 HoeﬀdingOptionTree (Pfahringer et Step (t)00.10.20.30.40.50.6ErrorNB (0.01)D = = = Step (t)00.10.20.30.40.50.6ErrorA1DE (0.01)D = = = Step (t)00.10.20.30.40.50.6ErrorA2DE (0.01)D = = = Step (t)00.020.040.060.080.1ErrorNB vs.
 A1DE vs.
 Nayyar A.
 Zaidi al.
 (a) (b) (c) (d) Fig.
 Windowing Slow Drift (∆ = – Variation prequential loss NB (Figure A1DE (Figure A2DE (Figure window sizes
 Figure Comparison NB (error = A1DE (error = A2DE (error = window size.

 HoeﬀdingTree (Hulten et
 LeveragingBag (Bifet et
 OzaBag (Oza Russell,
 OzaBagAdwin (Bifet et
 OzaBoost (Oza Russell,
 OzaBoostAdwin (Oza Russell, Babcock et
 be seen Table AccWeightedEns (AccuracyWeightedEnsem- ble) achieved lowest error PowerSupply, AccUpdatedEn (AccuracyUp- dateEnsemble) lowest Airlines, whereas LeveregingBag (Levereg- ingBagging) achieved lowest error ElectricNorm Sensor.
 following, use (best) results benchmarks see adaptive AnDE decay window based adaptation perform relative results.
 compare performance adaptive AnDE window-based adap- tation Figure depicting results various window sizes.
 low- est error obtained dataset standard techniques, is 010002000300040005000Time Step (t)00.10.20.30.4ErrorNB (0.0005)W = = = Step (t)00.10.20.30.4ErrorA1DE (0.0005)W = = = Step (t)00.10.20.30.4ErrorA2DE (0.0005)W = = = Step (t)00.010.020.030.04ErrorNB vs.
 A1DE vs.
 A2DENBA1DEA2DE Title Suppressed Due Excessive Length (a) (b) (c) (d) Fig.
 Decay Slow Drift (∆ = – Variation 0-1 Loss NB (Figure A1DE (Figure A2DE (Figure varying decay rates
 Figure Comparison NB (error = A1DE (error = A2DE (error = decay size.
 plotted horizontal blue line comparison.
 be seen PowerSupply, adaptive AnDE achieve lower error lowest twelve state-of-the-art techniques.
 Airlines, A1DE achieves lowest error ElectricNorm Sensors, A2DE achieves errors of:
 comparison adaptive AnDE decay-based adaptation is given Figure
 Similar window-based results, adaptive AnDE achieved lower er- ror lowest achieved state-of-the-art techniques PowerSupply dataset.
 Airlines ElectricNorm, A1DE achieved error respectively, Sensor, A2DE achieved error
 Note are suggesting learners are suited practical use.
 is need ﬁnd eﬀective mechanisms dynami- select forgetting rates bias/variance proﬁles.
 Also, learners outperformed best state-of-the-art, many learners Step (t)00.10.20.30.4ErrorNB (0.0005)D = = = Step (t)00.10.20.30.4ErrorA1DE (0.0005)D = = = Step (t)00.10.20.30.4ErrorA2DE (0.0005)D = = = Step (t)00.010.020.030.04ErrorNB vs.
 A1DE vs.
 Nayyar A.
 Zaidi al.
 AccUpdatedEns OzaBagAdwin DriftDetClassiﬁer DriftDetClassiﬁerEDDM (1) (2) ASHoeﬀdingTree HoeﬀdingTree (5) (6) (3) OzaBag (7) (4) HoeﬀdingAdaptiveTree (8) OzaBoost AccWeightedEns LeveragingBag OzaBoostAdwin (9) (10) (11) (12) PowerSupply Airlines ElectricNorm Sensor PowerSupply Airlines ElectricNorm Sensor PowerSupply Airlines ElectricNorm Sensor Table Comparison 0-1 Loss performance standard concept drift techniques real-world datasets: PowerSupply, Airlines, ElectricNorm, Sensor.
 best results are highlighted bold font.
 have performed sweep had been performed meta- parameters.
 Conclusions have proposed novel, generalizable falsiﬁable hypotheses inter-relationships drift rates, forgetting mechanisms, bias/variance proﬁles error.
 experiments AnDE learners have been con- sistent predictions hypotheses, be sweet path whereby drift rate increases optimal forgetting rates in- crease; forgetting rates increase optimal model variance proﬁle decrease.
 AnDE classiﬁers are suited study due eﬃcient support incremental learning windowing decay range bias/variance proﬁles provide.
 studies real-world data show framework result prequential accuracies are competitive best state- of-the-art.
 Development practical techniques selecting adjusting forgetting rates bias/variance proﬁles drift rates vary remains promising avenue future research.
 intriguing insight invites further investigation arises observation fast drift lowest bias learner achieved lowest error slowest forgetting rate, apparent disagreement hypotheses.
 discuss Section is due failure learn fast drift- ing attributes hence performance being dominated attribute Title Suppressed Due Excessive Length Fig.
 Comparison adaptive NB, A1DE A2DE based window-based adaptation real world datasets: PowerSupply, Airlines, ElectricNorm, Sensor.
 Horizontal blue line depicts best performance standard techniques.
 subspace is stationary (Y X1 X2).
 gives support argu- ment (Webb al, press) is important analyze drift marginal distributions course global level.
 invites development learners bring diﬀerent forgetting rates bias/variance proﬁles bear diﬀerent attribute sub-spaces diﬀerent times rates drift vary.
 hope hypotheses provide insights optimize wide range mechanisms handling concept drift stimulate future research.
 Acknowledgments material is based work supported Air Force Oﬃce Scientiﬁc Research, Asian Oﬃce Aerospace Research Development (AOARD) award number FA2386-17-1-4033.
 Size (W)0.90.951ErrorPowerSupplyNBA1DEA2DEBest Method100101102103104105Window Size (W)0.340.360.380.40.42ErrorAirlinesNBA1DEA2DEBest Method100101102103104105Window Size (W)0.120.140.160.180.20.22ErrorElectricNormNBA1DEA2DEBest Method100101102103104105Window Size (W)0.30.40.50.60.70.80.9ErrorSensorNBA1DEA2DEBest Method Nayyar A.
 Zaidi al.
 Fig.
 Comparison adaptive NB, A1DE A2DE based decay-based adaptation real world datasets: PowerSupply, Airlines, ElectricNorm, Sensor.
 Horizontal blue line depicts best performance standard techniques.
 Code code used work be downloaded repository: https://github.com/ nayyarzaidi/SweetPathVoyager.
 References Aggarwal C (2009) Data Streams: Overview Scientiﬁc Applications, Springer Berlin Heidelberg, pp
 DOI 10.1007/978-3-642-02788-8 Babcock B, Datar M, Motwani R (2002) Sampling moving window streaming data.
 In: Proceedings Thirteenth Annual ACM-SIAM Symposium Discrete Algorithms, Society Industrial Applied Mathematics, pp Baena-Garcıa M, Campo- ´Avila J, Fidalgo R, Bifet A, Gavalda R, Morales-Bueno R (2006) Early drift detection method.
 In: Fourth International Workshop Knowledge Discovery Data Streams, vol pp Bifet A, Gavald`a R (2009) Adaptive learning evolving data streams.
 In: Advances Intelligent Data Analysis VIII, Springer, pp Bifet A, Holmes G, Pfahringer B, Kirkby R, Gavald`a R (2009) New ensemble methods evolving data streams.
 In: Proceedings ACM SIGKDD international confer- ence Knowledge discovery data mining, ACM, pp Bifet A, Holmes G, Pfahringer B (2010) Leveraging bagging evolving data streams.
 In: Machine Learning Knowledge Discovery Databases, Springer, pp 10-610-410-2100Decay Rate (D)0.90.951ErrorPowerSupplyNBA1DEA2DEBest Method10-610-410-2100Decay Rate (D)0.340.360.380.40.42ErrorAirlinesNBA1DEA2DEBest Method10-610-410-2100Decay Rate (D)0.120.140.160.180.20.22ErrorElectricNormNBA1DEA2DEBest Method10-610-410-2100Decay Rate (D)0.30.40.50.60.70.80.9ErrorSensorNBA1DEA2DEBest Method Title Suppressed Due Excessive Length Bifet A, Gama J, Pechenizkiy M, Zliobaite (2011) Handling concept drift: Importance, challenges solutions.
 PAKDD-2011 Tutorial, Shenzhen, China Bishop C (2006) Pattern Recognition Machine Learning (Information Science Statis- tics).
 Springer-Verlag New York, Inc., Secaucus, NJ, USA Brain D, Webb G (1999) eﬀect data set size bias variance classiﬁcation learning.
 In: Richards D, Beydoun G, Hoﬀmann A, Compton P (eds) Proceedings Fourth Australian Knowledge Acquisition Workshop (AKAW-99), University New South Wales, Sydney, pp Brzezinski D, Stefanowski J (2014) Reacting diﬀerent types concept drift: accuracy updated ensemble algorithm.
 Neural Networks Learning Systems, IEEE Transactions Ditzler G, Roveri M, Alippi C, Polikar R (2015) Learning nonstationary environments: survey.
 IEEE Computational Intelligence Magazine Gaber MM, Zaslavsky A, Krishnaswamy S (2005) Mining data streams: review.
 ACM Sigmod Record Gama J, Rodrigues P (2009) Overview Mining Data Streams, Studies Compu- tational Intelligence, vol Springer Berlin / Heidelberg, pp
 DOI Gama J, Medas P, Castillo G, Rodrigues P (2004) Learning drift detection.
 In: SBIA Lecture Bazzan A, Labidi S (eds) Advances Artiﬁcial Intelligence Notes Computer Science, vol Springer Berlin Heidelberg, pp DOI 10.1007/978-3-540-28645-5 Gama J, ˇZliobaite I, Bifet A, Pechenizkiy M, Bouchachia A (2014) A survey concept drift adaptation.
 ACM Computing Surveys DOI Hulten G, Spencer L, Domingos P (2001) Mining time-changing data streams.
 In: Proceed- ings Seventh ACM SIGKDD International Conference Knowledge Discovery Data Mining, KDD-01, ACM, pp Keogh E, Pazzani M (1999) Learning augmented Bayesian classiﬁers: comparison distribution-based classiﬁcation-based approaches.
 In: Proceedings Interna- tional Workshop Artiﬁcial Intelligence Statistics, pp Krempl G, Zliobaite I, Brzezinski D, Hullermeier E, Last M, Lemaire V, Noack T, Shaker A, Sievi S, Spiliopoulou M, Stefanowski J (2014) Open challenges data stream mining research.
 In: ACM SIGKDD Explorations Newsletter, Springer, vol pp Levin D, Peres Y, Wilmer E (2008) Markov Chains Mixing Times.
 American Mathe- matical Soc.
 Nguyen H, Woon Y, Ng W (2014) A survey data stream clustering classiﬁcation.
 Knowledge Information Systems pp Oza N, Russell S (2001) Online bagging boosting.
 In: Artiﬁcial Intelligence Statistics Morgan Kaufmann, pp Pfahringer B, Holmes G, Kirkby R (2007) New options Hoeﬀding trees.
 In: Orgun M, Thornton J (eds) AI Advances Artiﬁcial Intelligence, Lecture Notes Computer Science, vol Springer, pp DOI 10.1007/978-3-540-76928-6 Wang H, Fan W, Yu PS, Han J (2003) Mining concept-drifting data streams using ensem- ble classiﬁers.
 In: Proceedings Ninth ACM SIGKDD International conference Knowledge Discovery Data Mining, KDD-03, ACM, pp Webb G (2014) Contrary popular belief incremental discretization be sound, compu- eﬃcient useful streaming data.
 In: Proceedings IEEE International Conference Data Mining, pp DOI
 Webb G, Boughton J, Zheng F, Ting K, Salem H (2012) Learning extrapolation marginal full-multivariate probability distributions: Decreasingly naive Bayesian clas- siﬁcation.
 Machine Learning DOI 10.1007/s10994-011-5263-6 Webb G, Hyde R, Cao H, Nguyen H, Petitjean F (2016) Characterizing concept drift.
 Data Mining Knowledge Discovery DOI 10.1007/s10618-015-0448-4 Webb G, Lee L, Goethals B, Petitjean F (in press) Analyzing concept drift shift sample data.
 Data Mining Knowledge Discovery ˇZliobaite (2010) Learning concept drift: overview.
 CoRR abs/1010.4784, URL http://arxiv.org/abs/1010.4784,
 reinforcement learning agent interacts environment, receiving rewards way indicate quality decisions.
 agent’s task is learn behave way maximizes reward.
 Model- based reinforcement learning (MBRL) techniques approach problem learning predictive model envi- ronment applying planning algorithm model make decisions.
 (Szita Szepesv´ari, are many advantages learning model environment, MBRL is challenging practice, minor ﬂaws model planner result catastrophic failure.
 result model-based methods have been successful large-scale problems, few notable exceptions (e.g. Abbeel
 paper addresses important understudied problem MBRL: learning reward function.
 is common Computer Science, Franklin Marshall Col- lege, Lancaster, Pennsylvania, USA.
 Correspondence to: Erik Talvitie <erik.talvitie@fandm.edu>.
 Preliminary work, review.
 Figure
 Shooter domain.
 work model learning ignore reward function (e.g. Bellemare et Oh Chiappa or, model be used planning, assume reward function is given (e.g. Ross Bagnell, Talvitie, Ebert
 Indeed, is true accurate model environment’s dynamics be learned, reward learning is straightforward – problems be decoupled.
 However, paper see model class is mispeciﬁed (i.e. representation does admit accurate model), is inevitable problems genuine interest, learning reward function becomes more complicated.
 Example better understand limitations dynamics model impact reward learning, consider simpliﬁed video game Shooter (Talvitie, pictured Figure
 bottom screen is spaceship move right ﬁre bullets, ﬂy upward.
 ship ﬁres bullet agent receives reward.
 top screen are targets.
 bullet hits target middle (bullseye), target explodes agent receives reward; hit is worth reward.
 Figure shows explosions indicate much reward agent receives.
 is typical treat predicting next state predict- ing reward separate learning problems.
 former agent learn map input state action next state.
 latter agent learn map state action reward.
 example agent learn associate presence explosions reward.
 How- ever, decomposed approach fail dynamics model is imperfect.
 instance, say dynamics model case is fac- Learning Reward Function Misspeciﬁed Model tored MDP, predicts value pixel next image based neighborhood centered pixel.
 Figure shows short sample rollout such model, sampling state based previous sampled state.
 second image rollout illustrates model’s ﬂaw: predicting pixel marked question mark model cannot account presence bullet target.
 Hence, errors appear subsequent image (marked red outlines).
 reward be associated erroneous im- age?
 value learned model assigns have dra- matic impact extent model is useful planning is clear amount traditional data associating environment states rewards answer question.
 provided, “perfect” reward function answer question; reward function assign value state be accurate states are reachable environment.
 seems best case sake planning be reward model predict reward ﬂawed state, preserving semantics target has been hit bullseye.
 order achieve this, reward model be trained states environment generate.
 remainder paper formalizes intuition.
 Sec- tion presents novel error bound value estimates terms reward error, taking rewards ﬂawed states generated model.
 Section prac- tical implications theoretical insight are discussed, leading extension existing Hallucinated DAgger- MC algorithm, provides theoretical guarantees deterministic MDPs, model class is misspec- iﬁed.
 Section demonstrates approach suggested theoretical results produce good plan- ning performance ﬂawed model, reward models learned typical manner (or “perfect” reward func- tions) lead catastrophic planning failure.

 focus Markov decision processes (MDP).
 en- vironment’s initial state s1 is drawn distribution µ.
 step t environment is state st.
 agent selects action causes environment transi- tion new state sampled transition distribution: st+1 ∼ P st
 environment emits reward, Rat st
 assume rewards are bounded [0, M ].
 policy π speciﬁes way behave MDP.
 Let π(a | s) be probability π chooses action state s.
 sequence actions a1:t let P (s(cid:48) | s, a1:t) = P a1:t be probability reaching s(cid:48) starting s taking actions sequence.
 state s, action a, policy π, let Dt s,a,π be state-action distribution obtained Figure
 ﬂawed model generate states reward function is undeﬁned.
 T (s, a) = (cid:80)T s,a,π ξ,π = E(s,a)∼ξ Dt t=1 γt−1 E(s(cid:48),a(cid:48))∼Dt T (s) = Ea∼πs [Qπ t steps, starting state s action following policy π.
 state action distribution ξ, let s,a,π.
 let S be set states reach- Dt able ﬁnite time policy non-zero probability.
 observe behavior P R states contained S.
 T -step state-action value policy, Qπ T (s, a) repre- sents expected discounted sum rewards obtained taking action state s executing π additional T − steps: Qπ Ra(cid:48) s(cid:48)
 Let T -step state value V π T (s, a)].
 Let Qπ = Qπ∞, V π = V π∞.
 agent’s goal be learn policy π maximizes Es∼µ[V π(s)].
 MBRL seek learn dynamics model approx- imating P reward model ˆR, approximating R, use combined model ˆR) produce policy planning algorithm.
 let ˆD, ˆQ, ˆV represent corresponding quantities using learned model.
 assume ˆP ˆR are deﬁned ˆS ⊇ S; be states ˆS P R are undeﬁned, be known priori states are.
 Let P represent dynamics model class, set models learning algorithm produce corre- let R be reward model class.
 work are interested common case dynamics model is misspeciﬁed: is ˆP ∈ P matches P s ∈ S.
 case is impossible learn accurate model; agent make good decisions ﬂaws learned model.
 results paper permit reward model be misspeciﬁed.
 Bounding Planning Performance ease analysis focus attention simple one- ply Monte Carlo planning algorithm (one-ply MC), similar “rollout algorithm” (Tesauro Galperin,
 state-action pair (s, a), planner executes N T -step Learning Reward Function Misspeciﬁed Model sample rollouts using ˆP starting s, taking action a, following rollout policy ρ.
 step rollout, ˆR gives reward.
 Let ¯Q(s, a) be average discounted return rollouts starting state s action a.
 large N, ¯Q approximate ˆQρ T (Kakade,
 execution policy ˆπ be greedy respect ¯Q.
 Talvitie (2015) bounds quality ˆπ.
 policy π state-action distribution ξ, let ξ,π,T be error T -step state-action values model assigns policy: ξ,π,T following be adapted existing bound (Talvitie,

 Let be value function returned applying depth T one-ply Monte Carlo model ˆP rollout policy ρ.
 Let be greedy w.r.t. ¯Q.
 policy π state-distribution µ, T (s, a)|(cid:3).
 T (s, a) − ˆQπ val = E(s,a)∼ξ (cid:2)|Qπ val (cid:2)V π(s) − V ˆπ(s)(cid:3) ≤ s∼µ − γ ξ,ρ,T val + mc, (cid:16) ξ(s, a) (1 − γ)µ(s)ˆπs(a) + γ(cid:80) ¯Q − ˆQρ mc = B is Bellman operator).
 Dµ,ˆπ(s, a) + z,b Dµ,π(z, b)P b T(cid:107)∞ + ρ Dµ,π(s, a) + z (s)ˆπs(a) (cid:17) T (cid:107)∞ (here T − V ρ mc term captures error due properties one-ply MC algorithm: error sample average ¯Q sub- optimality T -step value function respect ρ.
 ξ,ρ,T term captures error due model.
 see model’s usefulness planning is tied accuracy value assigns rollout policy.
 Thus, order obtain good plan aim learn model ξ,ρ,T val is small.
 val Error Dynamics Model Assuming reward function is known, bound ξ,ρ,T be adapted Ross Bagnell (2012) terms one-step prediction error dy- namics model.

 policy π state-action distribution ξ, val T−1(cid:88) t=1 val ≤ M ξ,π,T − γ (γt − γT E (s,a)∼Dt ξ,π s − ˆP s (cid:107)1 (cid:2)(cid:107)P (cid:3).
 Combining Lemmas yields overall bound con- trol performance terms model error.
 However, recent work offers tighter bound special case.
 Let true dynamics P be deterministic, let rollout policy ρ be (Bowling al., action selected ρ is independent current state, given history actions.
 state-action distribu- tion ξ, let H t be joint distribution environment (cid:2)(cid:80) state, model state, action single action sequence is sampled ρ executed model ξ,ρ(s1, z1, a1) = ξ(s1, a1) environment.
 So, H z1 = s1 (0 otherwise) t H t ξ,ρ(st, zt, at) = | a1)P a1:t−1 (st) ˆP a1:t−1 E(s1,a1)∼ξ P is deterministic, let σa1:t be unique state re- sults starting state s taking action sequence a1:t.
 Talvitie (2017) offers following result: Theorem
 P is deterministic, blind policy ρ state-action distribution ξ, (zt)(cid:3).
 ρ(a2:t a2:t−1 s1 s1 T(cid:88) t=1 val ≤ M ξ,ρ,T T−1(cid:88) T−1(cid:88) ≤ t=1 ≤ − γ γt t=1 (cid:2)(cid:107)Dt (cid:2)1 − ˆP γt−1 E (s,a)∼ξ (s,z,a)∼H t ξ,ρ (γt − γT E (s,a)∼Dt ξ,ρ s,a,ρ − ˆDt s,a,ρ(cid:107)1 z (σa s )(cid:3) (cid:2)1 − ˆP s )(cid:3).
 s (σa (1) (2) (3) (cid:3) Inequality is Lemma specialized deterministic case, expressing bound terms one-step pre- diction error ˆP
 Inequality gives bound terms error discounted distribution states T -step rollouts.
 is tightest bound three, practice is difﬁcult optimize objective di- rectly.
 Inequality gives bound terms hallucinated one-step error, called considers accuracy model’s predictions based states generated own sample rollouts (z), states generated environment (s).
 optimize hallucinated model be rolled parallel environment, trained predict next environment state “hallucinated” state model rollout.
 Talvitie (2017) shows approach improve planning performance model class is mispeciﬁed.
 Similar approaches have had em- pirical success MBRL tasks (Talvitie, Venkatraman et al., sequence prediction tasks (Venkatraman et al., Oh Bengio
 Talvitie (2017) shows relative tightness hal- lucinated error bound does hold general stochastic dynamics arbitrary rollout policies.
 However, note assumptions are limiting ﬁrst appear.
 common rollout policy chooses actions uni- formly randomly, is blind.
 Furthermore, P is assumed be deterministic, is assumed be complex be captured ˆP
 agent’s perspective, un-modeled complexity manifest ap- parent stochasticity.
 example Oh et al.
 (2015) learned dynamics models Atari games, are de- terministic (Hausknecht et human players Learning Reward Function Misspeciﬁed Model perceive be stochastic due complexity.
 remainder paper focus special case deterministic dynamics blind rollout policies.

 Incorporating Reward Error suggested Talvitie (2017), is straightforward extension Theorem account reward error.

 P is deterministic, blind policy ρ state-action distribution ξ, val ≤ T(cid:88) ξ,ρ,T s,a,ρ(cid:107)1 s(cid:48) − ˆRa(cid:48) s(cid:48) − ˆRa(cid:48) s(cid:48)(cid:12)(cid:12)(cid:3) s,a,ρ − ˆDt (cid:2)(cid:12)(cid:12)Ra(cid:48) (cid:2)(cid:107)Dt (cid:2)(cid:12)(cid:12)Ra(cid:48) s(cid:48)(cid:12)(cid:12)(cid:3) (cid:2)1 − ˆP s(cid:48)(cid:12)(cid:12)(cid:3) (cid:2)(cid:12)(cid:12)Ra(cid:48) (cid:2)1 − ˆP s(cid:48) − ˆRa(cid:48) s )(cid:3) z (σa ξ,ρ t=1 + M ≤ T(cid:88) t=1 (s(cid:48),a(cid:48))∼Dt ξ,ρ γt−1 E (s,a)∼ξ (s(cid:48),a(cid:48))∼Dt ξ,ρ γt−1 T(cid:88) t=1 γt−1 T−1(cid:88) γt (s,z,a)∼H t ≤ T(cid:88) t=1 γt−1 t=1 − γ T−1(cid:88) t=1 (s(cid:48),a(cid:48))∼Dt ξ,ρ (γt − γT E (s,a)∼Dt ξ,ρ (cid:3) (4) (5) (6) s )(cid:3).
 s Proof.
 derivation inequality is below.
 rest follow Theorem
 T (s, a)|(cid:3) s,a,ρ(s(cid:48), a(cid:48))Ra(cid:48) Dt s(cid:48) (cid:16) − ˆDt s,a,ρ(s(cid:48), a(cid:48)) ˆRa(cid:48) s(cid:48) s,a,ρ(s(cid:48), a(cid:48))Ra(cid:48) Dt s(cid:48) s(cid:48) + Dt − ˆDt s,a,ρ(s(cid:48), a(cid:48)) ˆRa(cid:48) s(cid:48) s,a,ρ(s(cid:48), a(cid:48)) ˆRa(cid:48) s(cid:48) ξ,ρ,T val = E (s,a)∼ξ T (s, a) − ˆQρ (cid:16) (cid:2)|Qρ (cid:34)(cid:12)(cid:12)(cid:12)(cid:12) T(cid:88) γt−1 (cid:88) (cid:34)(cid:12)(cid:12)(cid:12)(cid:12) T(cid:88) γt−1 (cid:88) t=1 t=1 (s(cid:48),a(cid:48)) (s(cid:48),a(cid:48)) = E (s,a)∼ξ = E (s,a)∼ξ − Dt s,a,ρ(s(cid:48), a(cid:48)) ˆRa(cid:48) (cid:16) γt−1 (cid:88) (cid:34)(cid:12)(cid:12)(cid:12)(cid:12) T(cid:88) t=1 = E (s,a)∼ξ s,a,ρ(s(cid:48), a(cid:48))(Ra(cid:48) Dt s(cid:48) − ˆRa(cid:48) s(cid:48) (s(cid:48),a(cid:48)) s,a,ρ(s(cid:48), a(cid:48)) − ˆDt + (Dt s,a,ρ(s(cid:48), a(cid:48))) ˆRa(cid:48) s(cid:48) (cid:35) (cid:17)(cid:12)(cid:12)(cid:12)(cid:12) (cid:35) (cid:17)(cid:12)(cid:12)(cid:12)(cid:12) (cid:17)(cid:12)(cid:12)(cid:12)(cid:12) ≤ T(cid:88) t=1 γt−1 (cid:12)(cid:12)Ra(cid:48) s(cid:48) − ˆRa(cid:48) s(cid:48)(cid:12)(cid:12) (cid:2)(cid:13)(cid:13)Dt (s(cid:48),a(cid:48))∼Dt ξ,ρ T(cid:88) + M γt−1 E (s,a)∼ξ s,a,ρ − ˆDt s,a,ρ (cid:13)(cid:13)(cid:3), t=1 gives result.
 is bounds break value error parts: reward error dynamics error.
 reward error measures accuracy reward model environment states encountered policy ρ.
 dynamics error mea- sures probability model generate correct states rollouts, assigning maximum reward error (M) dynamics model generates incorrect states.
 view corresponds common MBRL practice: train dynamics model assign high proba- bility correct states reward model map environment states rewards.
 However, discussed Section bounds are conservative (and generating erroneous state be catas- trophic associated reward is reasonable.
 derive bound accounts this.

 P is deterministic, blind policy ρ state-action distribution ξ, ξ,ρ,T γt−1 E(s,z,a)∼H t ξ,ρ Proof.
 ξ,ρ,T val = E(s1,a1)∼ξ ρ (s1, a1) − ˆQT s − ˆRa (cid:2)(cid:12)(cid:12)Ra (cid:12)(cid:12)(cid:3).
 ρ (s1, a1)(cid:12)(cid:12)(cid:3) = E (s1,a1)∼ξ = E (s1,a1)∼ξ st,at t=1 t=1 val ≤ T(cid:88) (cid:2)(cid:12)(cid:12)QT (cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) T(cid:88) γt−1(cid:88) (cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:0)Ra1 (cid:18)(cid:88) −(cid:88) (cid:88) −(cid:88) zt (st)Rat st − ˆRa1 (st)Rat st P a1:t−1 P a1:t−1 s1 s1 s1 s1 s1 zt st zt (st) ˆP a1:t−1 s1 P a1:t−1 s1 (cid:88) st (cid:88) st,zt note t P a1:t−1 s1 ˆP a1:t−1 zt (zt) ˆRat zt Dt s1,a1,ρ(st, at)Rat st zt,at ˆDt −(cid:88) γt−1(cid:88) T(cid:88) (cid:1) + −(cid:88) a2:t t=2 s1,a1,ρ(zt, at) ˆRat zt ρ(a2:t | a1) (cid:35) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) (cid:35) (cid:19)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) (st)Rat st ˆP a1:t−1 s1 (zt) ˆRat zt ˆP a1:t−1 s1 (zt) s1 ˆP a1:t−1 (zt)(cid:0)Rat st (zt) ˆRat zt − ˆRat zt (cid:88) (cid:1) st P a1:t−1 s1 (st) Learning Reward Function Misspeciﬁed Model (cid:12)(cid:12) + T(cid:88) t=2 − ˆRa1 s1 (cid:34)(cid:12)(cid:12)Ra1 (cid:88) s1 st,zt Thus (s1,a1)∼ξ val ≤ E ξ,ρ,T (cid:88) T(cid:88) ρ(a2:t | a1) a2:t γt−1 t=1 P a1:t−1 s1 (st) ˆP a1:t−1 s1 (cid:2)(cid:12)(cid:12)Ra s − ˆRa (cid:12)(cid:12)(cid:3).
 (s,z,a)∼H t ξ,ρ γt−1 (zt)(cid:12)(cid:12)Rat st (cid:12)(cid:12)(cid:35) − ˆRat zt Similar hallucinated one-step error dynamics model (inequality Theorem imagines model environment are rolled parallel.
 measures error rewards generated model rollout rewards corresponding steps environ- ment rollout.
 call hallucinated reward error.
 However, bound Theorem is focused model placing high probability “correct” states, hallucinated reward error be small state sequence sampled dynamics model is “incorrect”, sequence rewards is similar.
 such, show bound is tighter inequality

 P is deterministic, blind policy ρ state-action distribution ξ, γt−1 E(s,z,a)∼H t ξ,ρ T(cid:88) t=1 ≤ T(cid:88) t=1 (cid:12)(cid:12)(cid:3) s − ˆRa (cid:2)(cid:12)(cid:12)Ra (cid:2)(cid:12)(cid:12)Ra(cid:48) s(cid:48) − ˆRa(cid:48) γt−1 (s(cid:48),a(cid:48))∼Dt ξ,ρ T−1(cid:88) t=1 γt (s,z,a)∼H t ξ,ρ s(cid:48)(cid:12)(cid:12)(cid:3) (cid:2)1 − ˆP s )(cid:3).
 z Proof.
 T(cid:88) γt−1 t=1 H t (cid:2)(cid:12)(cid:12)R(s, a) − ˆR(z, a)(cid:12)(cid:12)(cid:3) ξ,ρ(s, z, a)(cid:12)(cid:12)R(s, a) − ˆR(z, a)(cid:12)(cid:12) ξ,ρ(s, s, a)(cid:12)(cid:12)R(s, a) − ˆR(s, a)(cid:12)(cid:12) ξ,ρ(s, z, a)(cid:12)(cid:12)R(s, a) − ˆR(z, a)(cid:12)(cid:12).
 H t ξ,ρ t=1 s,z,a (s,z,a)∼H t γt−1(cid:88) T(cid:88) γt−1(cid:88) T(cid:88) T(cid:88) γt−1 (cid:88) t=1 s,a H t t=1 s,z(cid:54)=s,a breaks expression terms.
 consider ﬁrst term: s,a t=1 Dt H t γt−1(cid:88) T(cid:88) ξ,ρ(s, s, a)(cid:12)(cid:12)R(s, a) − ˆR(s, a)(cid:12)(cid:12) γt−1(cid:88) ≤ T(cid:88) γt−1 (cid:88) T(cid:88) T(cid:88) ξ,ρ(s, a)(cid:12)(cid:12)R(s, a) − ˆR(s, a)(cid:12)(cid:12).
 ξ,ρ(s, z, a)(cid:12)(cid:12)R(s, a) − ˆR(z, a)(cid:12)(cid:12) γt−1 (cid:88) ≤ M s,z(cid:54)=s,a ξ,ρ(s, z, a).
 H t H t t=1 s,a consider second term: t=1 s,z(cid:54)=s,a (7) Recall H ξ,ρ(s, z, a) s (cid:54)= z.
 Thus, t=1 t=1 s,z(cid:54)=s,a s,z(cid:54)=s,a T(cid:88) γt−1 (cid:88) γt (cid:88) T−1(cid:88) (cid:32) (cid:88) (cid:88) (cid:88) (cid:0)1 − ˆP a(cid:48) T−1(cid:88) s(cid:48),z(cid:48),a(cid:48) s(cid:48),z(cid:48),a(cid:48) s,z(cid:54)=s = M = M = M = M γt (s,z,a)∼H t ξ,ρ t=1 H t ξ,ρ(s, z, a) (cid:33) T−1(cid:88) H t+1 ξ,ρ (s, z, a) P a(cid:48) s(cid:48) (s) ˆP a(cid:48) (z) s(cid:48) )(cid:1) T−1(cid:88) z(cid:48) (σa(cid:48) t=1 γtH t ξ,ρ(s(cid:48), z(cid:48), a(cid:48)) γtH t ξ,ρ(s(cid:48), z(cid:48), a(cid:48)) t=1 [1 − ˆP z (σa s )].
 (8) Combining lines yields result.
 next section discusses practical implications result MBRL algorithms extends existing MBRL algorithm incorporate insight.

 Implications MBRL is ﬁrst observation difﬁculties inherent reward learning.
 Sorg al.
 argued have model planner are limited way, reward functions other true reward lead better planning performance.
 Accordingly, policy gradient approaches have been employed learn reward functions use online planning algorithms, providing beneﬁt reward function is known (Sorg et al., Bratman et Guo
 Tamar al.
 (2016) take idea logical extreme, treating en- tire model planning algorithm policy parameterization, adapting improve control Learning Reward Function Misspeciﬁed Model performance minimize measure predic- tion error.
 appealing directness, approach offers little theoretical insight makes model useful planning.
 Furthermore, are advantages optimizing quantities other planning performance; allows model exploit incoming data is unclear improve agent’s policy (for instance agent has seen little reward).
 provides more speciﬁc guidance choose set ﬂawed models.
 attempting opti- mize control performance, result suggests take advantage model error signals offering guarantees terms control performance.
 is notable that, Theorem Theorem does contain term measuring dynamics error.
 dynamics model is important; choices ˆP hallucinated reward error be made small others be high (for instance ˆP loops single state).
 low halluci- nated reward error does require dynamics place high probability “correct” states.
 fact, be dynamics unrelated environment yield best reward predictions.
 intriguingly suggests dynamics model reward model parameters be adapted optimize hallucinated reward error.
 introduced Predictrons (Silver et al., Value Prediction Networks (Oh et al., are attempts do – adapt model’s dynamics improve reward prediction.
 see theoretical support approaches encour- agement more study direction.
 Still, practice be much harder learn predict reward sequences state sequences, reward signal is sparse.
 be difﬁcult tie reward prediction er- ror dynamics model parameters way allows theoretical performance guarantees.
 possible interpretation Theorem is reward model be customized dynamics model.
 is, hold dynamics model ﬁxed, result gives clear objective reward model.
 suggests algorithmic structure dynamics model is trained own objective, reward model is trained minimize hallucinated error respect learned dynamics model.
 clear downside ap- proach is general ﬁnd best combination dynamics model reward model; be less accurate dynamics model results lower hallucinated reward error.
 advantage is allows exploit prediction error signal dynamics model removes circular dependence dynamics model reward model.
 paper explore third option extending ex- isting Hallucinated DAgger-MC algorithm
 resulting algorithm is similar orig- inal, leave detailed description analysis appendix focus key, high-level points.
 Section presents empirical results illustrating impact training reward model minimize hallucinated error.
 Hallucinated DAgger-MC Reward Learning “Data Aggregator” (DAgger) algorithm (Ross Bag- nell, was ﬁrst implementable MBRL al- gorithm performance guarantees agnostic model class.
 did, however, require planner be opti- mal.
 DAgger-MC (Talvitie, relaxed assumption, accounting limitations particular suboptimal planner (one-ply MC).
 Hallucinated DAgger-MC (or H- DAgger-MC) (Talvitie, altered DAgger-MC opti- mize hallucinated error, one-step error.
 algorithms were presented assump- tion reward function was known priori.
 see Section reward function cannot be ignored.
 reward function is given, algorithms fail due issues described Section high level, H-DAgger-MC proceeds iterations.
 iteration batch data is gathered sampling state- action pairs using mixture current plan “exploration distribution” (to ensure important states are plan visit them).
 rollout policy is used generate parallel rollouts environment model sampled state-action pairs, form training examples.
 collected data is used update dynamics model, is used produce new plan be used next iteration.
 augment H-DAgger-MC, adding reward learning step iteration (rather assuming reward is given).
 rollout, training examples mapping “hallucinated” model states real environment rewards are collected used update reward model.
 extended H- DAgger-MC algorithm offers theoretical guarantees similar original algorithm.
 Essentially, • exploration distribution is similar state visita- tion distribution good policy, • mc is small, learning algorithms dynamics model reward model are no-regret, • reward model class R contains low hallucinated reward error model respect lowest halluci- nated prediction error model P, limit H-DAgger-MC produce good policy.
 Learning Reward Function Misspeciﬁed Model discussed Section does guarantee H- DAgger-MC ﬁnd best performing combination dynamics model reward model, training dynamics model does take hallucinated reward error account.
 is, improvement original H-DAgger-MC result good performance be assured is low error dynamics model P, is low error reward model R.
 completeness’ sake, more detailed description anal- ysis algorithm be found appendix.
 turn empirical evaluation algorithm.

 Experiments section illustrate impact optimizing hallu- cinated reward error Shooter example described Section using DAgger-MC H-DAgger-MC1.
 one-ply MC planner used uniformly random rollouts depth action step.
 exploration distri- bution was generated following optimal policy (1− γ) probability termination step.
 discount factor was γ = iteration training rollouts were generated resulting policy was evaluated episode length
 discounted return obtained policy iteration is reported, averaged trials.
 dynamics model pixel was learned using Con- text Tree Switching (Veness et similar FAC-CTW algorithm (Veness
 position model takes input values pixels w × h neighborhood position previous timestep.
 Data was shared positions.
 reward was ap- proximated linear function action, learned stochastic weighted gradient descent.
 feature repre- sentation contained binary feature possible × conﬁguration pixels position.
 representation admits accurate reward model.
 qualitative observations presented section were robust wide range choices step size gradient descent.
 Here, experiment best performing step size approach is selected experiments practical alteration has been made H-DAgger-MC algorithm.
 H-DAgger-MC requires “unrolled” dynamics model (with separate model step rollout, making predictions based output previous model).
 is important H-DAgger-MC’s theoretical guarantees, Talvitie (2017) found single dynamics model steps be learned, provided training rollouts had limited depth.
 Following Talvitie (2017), ﬁrst iterations ﬁrst example training rollout code experiments be available publication.
 is added dynamics model dataset; ﬁrst examples are added.
 entire rollout was used train reward model.
 DAgger-MC does require unrolled dynamics model truncated training rollouts was implemented presented (Talvitie, single dynamics model full training rollouts.
 Results consider DAgger-MC H-DAgger-MC perfect reward model, reward model trained environment states rollouts, reward model trained “hallucinated” states Algorithm
 perfect reward model is someone familiar rules game specify; checks presence explosions target positions gives appropriate value explosion is present (subtracting action is “shoot”).
 Results are presented variations Shooter problem.
 NO MODEL LIMITATIONS ﬁrst experiment apply algorithms Shooter, described Section
 Here, dynamics model uses × neighborhood, is sufﬁcient make accurate predictions.
 Figure shows discounted return policies generated DAgger-MC H-DAgger- MC, averaged independent trials.
 shaded region surrounding curve represents conﬁdence inter- val.
 gray line marked “Random” shows average discounted return uniform random policy (with conﬁdence interval).
 gray line marked “Perfect Model” shows average discounted return one-ply MC planner using perfect model.
 performance DAgger-MC is compara- ble planning perfect model.
 ob- served Talvitie (2017), perfect reward model H-DAgger-MC performs worse DAgger-MC; dynamics model H-DAgger-MC receives noisier data is less accurate.
 see learned reward model yields performance perfect reward model, hallucinated training! perfect reward model relies speciﬁc screen conﬁguations are likely appear ﬂawed sample rollouts, learned reward model generalizes screens seen training.
 course, is coincidental generalization is beneﬁcial; standard training reward model is trained environment states, giving guidance erroneous model states.
 Hallucinated train- ing trains reward model make reasonable predictions model rollouts, yields per- formance, comparable DAgger-MC.
 see learning reward function way mitigates shortcoming H-DAgger-MC, making effective Learning Reward Function Misspeciﬁed Model (a) No model limitations (b) Moving (2nd-order Markov) (c) Pixel models use × neighborhood Figure
 Performance DAgger-MC H-DAgger-MC variations Shooter domain.
 practice accurate model be learned.
 FAILURE THE MARKOV ASSUMPTION Next consider version shooter presented Talvitie (2017) bullseye target moves side side, making environment second-order Markov.
 model is Markov, cannot predict movement bullseyes, representation is sufﬁcient predict other pixel.
 Figure shows results.
 Talvitie (2017) DAgger-MC fails case.
 model’s limitation prevents predict- ing bullseyes, resulting errors compound rollouts, rendering useless.
 ob- served, H-DAgger-MC performs trains model produce stable rollouts.
 cases see learned reward models perfect reward model, hallucinated reward training yields best performance, helping mitigate impact ﬂaws DAgger-MC’s model.
 FLAWED FACTORED STRUCTURE see importance hallucinated reward training consider original Shooter domain (with static bullseyes), limit size neigh- borhood used predict pixel, described Section Figure shows results.
 DAgger-MC fails.
 Again see learned reward models yield bet- ter performance perfect reward function, hallucinated training guides reward model be useful planning, ﬂaws dynamics model.
 case, see H-DAgger-MC fails combined perfect reward model, performs reward model trained environment states.
 Hallucinated training helps dynamics model produce stable sample rollouts, does correct fun- damental limitation: dynamics model cannot predict shape explosion target is hit.
 result, reward model bases predictions explosions occur environment fail predict reward agent hits target sample rollouts.
 Hallucinated training, contrast, specializes reward model ﬂawed dynamics model, allowing performance comparable planning perfect model.

 Conclusion paper has introduced hallucinated reward error, measures extent rewards sample rollout model match rewards parallel rollout environment.
 conditions, quantity is related control performance traditional measure model quality (reward error envi- ronment states error state transition).
 have seen dynamics model is ﬂawed, reward functions learned typical manner “perfect” reward functions given lead catastrophic planning failure.
 reward function is trained minimize hallucinated reward error, accounts model’s ﬂaws, improving performance.
 050100150200Iteration024681012Avg.DiscountedReturnPerfectModelRandomPerfectRewardStandardTrainingHallucinatedDAgger-MC050100150200Iteration024681012Avg.DiscountedReturnPerfectModelRandomPerfectRewardStandardTrainingHallucinatedH-DAgger-MC050100150200Iteration024681012Avg.DiscountedReturnPerfectModelRandomPerfectRewardStandardTrainingHallucinatedDAgger-MC050100150200Iteration024681012Avg.DiscountedReturnPerfectModelRandomPerfectRewardStandardTrainingHallucinatedH-DAgger-MC050100150200Iteration024681012Avg.DiscountedReturnPerfectModelRandomPerfectRewardStandardTrainingHallucinatedDAgger-MC050100150200Iteration024681012Avg.DiscountedReturnPerfectModelRandomPerfectRewardStandardTrainingHallucinatedH-DAgger-MC Learning Reward Function Misspeciﬁed Model References Abbeel, Pieter, Coates, Adam, Quigley, Morgan, Ng, Andrew Y.
 application reinforcement learning aerobatic helicopter ﬂight.
 Advances Neural Infor- mation Processing Systems (NIPS), pp.

 Bellemare, Marc G., Veness, Joel, Talvitie, Erik.
 Skip context tree switching.
 Proceedings Inter- national Conference Machine Learning (ICML), pp.

 Bengio, Samy, Vinyals, Oriol, Jaitly, Navdeep, Shazeer, Noam.
 Scheduled sampling sequence prediction recurrent neural networks.
 Advances Neural Infor- mation Processing Systems (NIPS), pp.

 Bowling, Michael, McCracken, Peter, James, Michael, Neufeld, James, Wilkinson, Dana.
 Learning pre- dictive state representations using non-blind policies.
 Proceedings International Conference Ma- chine Learning (ICML), pp.

 Bratman, Jeshua, Singh, Satinder, Sorg, Jonathan, Lewis, Richard.
 Strong mitigation: Nesting search good policies search good reward.
 Proceed- ings International Conference Autonomous Agents Multiagent Systems (AAMAS), pp.

 Chiappa, Silvia, Racani`ere, S´ebastien, Wierstra, Daan, Mohamed, Shakir.
 Recurrent environment simulators.
 Proceedigns International Conference Learning Representations (ICLR),
 Ebert, Frederik, Finn, Chelsea, Lee, Alex X., Levine, Sergey.
 Self-supervised visual planning temporal skip connections.
 Proceedings Annual Con- ference Robot Learning (CoRL), volume Pro- ceedings Machine Learning Research (PMLR), pp.

 Guo, Xiaoxiao, Singh, Satinder P., Lewis, Richard L., Lee, Honglak.
 Deep learning reward design im- prove monte carlo tree search ATARI games.
 Pro- ceedings Twenty-Fifth International Joint Confer- ence Artiﬁcial Intelligence (IJCAI), pp.

 Hausknecht, Matthew, Lehman, Joel, Miikkulainen, Risto, Stone, Peter.
 neuroevolution approach general atari game playing.
 IEEE Transactions Computational Intelligence AI Games,
 Kakade, Sham Machandranath.
 sample complexity reinforcement learning.
 PhD thesis, University London,
 Oh, Junhyuk, Guo, Xiaoxiao, Lee, Honglak, Lewis, Richard L, Singh, Satinder.
 Action-conditional video prediction using deep networks atari games.
 Ad- vances Neural Information Processing Systems (NIPS), pp.

 Oh, Junhyuk, Singh, Satinder, Lee, Honglak.
 Value prediction network.
 Advances Neural Information Processing Systems pp.

 Ross, Stephane Bagnell, Drew.
 Agnostic system iden- tiﬁcation model-based reinforcement learning.
 Proceedings International Conference Ma- chine Learning (ICML), pp.

 Silver, David, van Hasselt, Hado, Hessel, Matteo, Schaul, Tom, Guez, Arthur, Harley, Tim, Dulac-Arnold, Gabriel, Reichert, David P., Rabinowitz, Neil, Barreto, Andr´e, Degris, Thomas.
 predictron: End-to-end learning planning.
 Proceedings International Con- ference Machine Learning (ICML), pp.

 Sorg, Jonathan, Lewis, Richard L, Singh, Satinder.
 Re- ward design online gradient ascent.
 Advances Neural Information Processing Systems (NIPS), pp.

 Sorg, Jonathan, Singh, Satinder P, Lewis, Richard L.
 Internal rewards mitigate agent boundedness.
 Proceed- ings 27th International Conference Machine Learning (ICML), pp.

 Sorg, Jonathan, Singh, Satinder P, Lewis, Richard L.
 Optimal rewards leaf-evaluation heuristics plan- ning agents.
 Proceedings Twenty-Fifth AAAI Conference Artiﬁcial Intelligence (AAAI), pp.

 Szita, Istv´an Szepesv´ari, Csaba.
 Model-based reinforce- ment learning tight exploration complexity bounds.
 Proceedings 27th International Con- ference Machine Learning (ICML), pp.

 Talvitie, Erik.
 Model regularization stable sample roll- outs.
 Proceedings 30th Conference Uncer- tainty Artiﬁcial Intelligence (UAI), pp.

 Talvitie, Erik.
 Agnostic system identiﬁcation monte carlo planning.
 Proceedings AAAI Con- ference Artiﬁcial Intelligence (AAAI), pp.

 Talvitie, Erik.
 Self-correcting models model-based re- inforcement learning.
 Proceedings Thirty-First AAAI Conference Artiﬁcial Intelligence (AAAI), pp.

 Learning Reward Function Misspeciﬁed Model Tamar, Aviv, Wu, Yi, Thomas, Garrett, Levine, Sergey, Abbeel, Pieter.
 Value iteration networks.
 Advances Neural Information Processing Systems (NIPS), pp.

 Tesauro, Gerald Galperin, Gregory R.
 On-line policy Advances improvement using monte-carlo search.
 Neural Information Processing Systems (NIPS), pp.

 Veness, Joel, Ng, Kee Siong, Hutter, Marcus, Uther, William T.
 B., Silver, David.
 Monte-Carlo AIXI Approxi- mation.
 Journal Artiﬁcial Intelligence Research (JAIR),
 Veness, Joel, Ng, Kee Siong, Hutter, Marcus, Bowling, Michael.
 Context tree switching.
 Proceedings Data Compression Conference (DCC), pp.

 Venkatraman, Arun, Hebert, Martial, Bagnell, J.
 An- drew.
 Improving multi-step prediction learned time series models.
 Proceedings AAAI Confer- ence Artiﬁcial Intelligence (AAAI), pp.

 Venkatraman, Arun, Capobianco, Roberto, Pinto, Lerrel, Hebert, Martial, Nardi, Daniele, Bagnell, J Andrew.
 Improved learning dynamics models control.
 International Symposium Experimental Robotics, pp.


 A.
 Hallucinated DAgger-MC Details Hallucinated DAgger-MC, earlier variations DAgger, requires ability reset initial state distribution µ ability reset “exploration distribution” ν.
 exploration distribution ensures agent encounter states be visited good policy.
 performance bound H-DAgger-MC depends part quality selected ν.
 addition assuming particular form planner (one-ply MC blind rollout H-DAgger-MC requires dynamics model be “unrolled”.
 learning single ˆP H-DAgger-MC learns set ˆP


 T−1} ⊆ P, model ˆP i is responsible predicting outcome step i rollout, given state sampled ˆP i−1.
 impractical assumption is important theoretically, Talvitie (2017) showed prac- tice single P be used steps; experiments Section make use practical alteration.
 augments H-DAgger-MC learn reward model dynamics model.
 particular, H- DAgger-MC proceeds iterations, iteration producing ˆR1).
 k ←


 K do probability...
 (cid:46) First sample ξ Algorithm Hallucinated DAgger-MC (+ reward learning) Require: LEARN-DYNAMICS, LEARN-REWARD, explo- ration distr.
 ν, MC-PLANNER(blind rollout policy ρ, depth T ), iterations N, rollouts iteration K.
 E1 (maybe using ν) ← LEARN-DYNAMICS(D1:T−1 ).
 Get initial datasets D1:T−1 Initialize ˆP Initialize ˆR1 ← LEARN-REWARD(E1).
 Initialize ˆπ1 ← MC-PLANNER( ˆP n ←


 N do Sample (x, b) ∼ D ˆπn Reset (x, b) ∼ ν.
 (1−γ)/4: Sample x ∼ µ, b ∼ ˆπn(· | x).
 γ/4: Reset (y, c) ∼ ν Sample x ∼ P (· | y, c), b ∼ ˆπn(· | x) Let s ← x, z ← x, ← b.
 t ←


 T − do (cid:46) Parallel rollouts...
 Sample s(cid:48) ∼ P (· | s, a).
 Add (cid:104)z, a, s(cid:48)(cid:105) Dt n.
 (cid:46) (DAgger-MC adds (cid:104)s, a, s(cid:48)(cid:105)) Add (cid:104)z, a, Ra s γt−1(cid:105)) (cid:46) (Standard approach adds (cid:104)s, a, Ra Sample z(cid:48) ∼ ˆP t Let ← s(cid:48), z ← z(cid:48), sample ∼ ρ.
 s γt−1(cid:105) En. | z, a).
 end Add (cid:104)z, a, Ra s γT−1(cid:105) En. (cid:46) (Standard approach adds (cid:104)s, a, Ra end ← LEARN-DYNAMICS( ˆP ˆP n−1 ˆRn ← LEARN-REWARD( ˆRn−1, En) ˆπn ← MC-PLANNER( ˆP ˆRn).
 end return sequence ˆπ1:N s γT−1(cid:105)) D1:T−1 new plan, is turn used collect data train new model.
 iteration state-action pairs are sampled using current plan exploration distribution (lines world model are rolled parallel generate hallucinated training examples (lines 14-21).
 resulting data is used update model.
 add reward model learning process, collect training ex- amples state transition examples rollout.
 parts model have been new plan is generated subsequent iteration.
 Note dynamics model is “unrolled”, is single reward model is responsible predicting reward step rollout.
 assume re- ward learning algorithm is performing weighted regression training example is weighted γt−1 rollout step t occurred).
 Learning Reward Function Misspeciﬁed Model A.1. Analysis H-DAgger-MC derive theoretical guarantees new version H-DAgger-MC.
 analysis is similar existing DAgger variants (Ross Bagnell, Talvitie, proof is included completeness.
 Let H t be distribution H-DAgger-MC samples training example depth t (lines pick initial state-action pair, lines roll out).
 Deﬁne average error dynamics model depth t be ξπ,ˆπn (s, a) = (cid:16) Dµ,ˆπn(s, a) + Dµ,π(s, a) (1 − γ)µ(s)ˆπn(a | s) + γ Dµ,π(z, b)P b (cid:17) z (s)ˆπn(a | s) (cid:88) z,b Then, combining above Theorem N(cid:88) N(cid:88) − γ ≤ − γ T(cid:88) t=1 (s,a)∼ξπ,ˆπn [| ˆQρ T,n(s, a) − Qρ T (s, a)|] + ¯mc γt−1 (s, z, a) ∼ H t,n π,ˆπn [ ˆRn (s, z, a)] + ¯mc ,ρ note t n, (cid:2) ˆRn (s, z, a)(cid:3) (s,z,a)∼H t,n π,ˆπn ,ρ (cid:88) (cid:88) (cid:88) s(cid:48),a(cid:48) s(cid:48),a(cid:48) (cid:88) s(cid:48),a(cid:48) s(cid:48)(cid:48),a(cid:48)(cid:48) Dµ,ˆπn (s(cid:48), a(cid:48)) (s,z,a)∼H t,n s(cid:48) ,a(cid:48) ,ρ Dµ,π(s(cid:48), a(cid:48)) (s,z,a)∼H t,n Dµ,π(s(cid:48)(cid:48), a(cid:48)(cid:48))P a(cid:48)(cid:48) s(cid:48) ,a(cid:48),ρ s(cid:48)(cid:48) (s(cid:48))ˆπn(a(cid:48) | s(cid:48)) (cid:88) s(cid:48),a(cid:48) − γ (s,z,a)∼H t,n s(cid:48) ,a(cid:48),ρ µ(s(cid:48))ˆπn(a(cid:48) | s(cid:48)) Dµ,ˆπn (s(cid:48), a(cid:48)) (s,z,a)∼H t,n s(cid:48) ,a(cid:48) ,ρ (s,z,a)∼H t,n s(cid:48) ,a(cid:48) ,ρ (cid:88) s(cid:48),a(cid:48) cπ cπ ≤ (cid:88) (cid:88) s(cid:48),a(cid:48) s(cid:48),a(cid:48) s(cid:48)(cid:48),a(cid:48)(cid:48) (cid:88) s(cid:48),a(cid:48) − γ (s,z,a)∼H t,n s(cid:48) ,a(cid:48) ,ρ µ(s(cid:48))ˆπn(a(cid:48) | s(cid:48)) (s,z,a)∼H t,n s(cid:48) ,a(cid:48) ,ρ ν(s(cid:48), a(cid:48)) (cid:88) (s,z,a)∼H t,n ν(s(cid:48)(cid:48), a(cid:48)(cid:48))P a(cid:48)(cid:48) s(cid:48) ,a(cid:48) ,ρ s(cid:48)(cid:48) (s(cid:48))ˆπn(a(cid:48) | s(cid:48)) (s, z, a)(cid:3) (cid:2) ˆRn (s, z, a)(cid:3) (cid:2) ˆRn (s, z, a)(cid:3) (cid:2) ˆRn (s, z, a)(cid:3) (cid:2) ˆRn (cid:2) ˆRn (s, z, a)(cid:3) (cid:2) ˆRn (s, z, a)(cid:3) (cid:2) ˆRn (s, z, a)(cid:3) (cid:2) ˆRn (s, z, a)(cid:3) N(cid:88) n=1 N(cid:88) T(cid:88) t=1 ¯t prd = E(s,z,a)∼H t [1 − ˆP t s | z, a)].
 n(σa Let  ˆRn (s, z, a) = |R(s, a) − ˆRn(z, a)| let ¯hrwd = γt−1 E(s,z,a)∼H t [ ˆRn (s, z, be average reward model error.
 Finally, let Dt be distribution H-DAgger-MC samples s rollout lines 14-21.
 error reward model respect environment states is γt−1 E(s,a)∼Dt [|R(s, a) − ˆR(s, a)|].
 N(cid:88) T(cid:88) t=1 ¯erwd = Dµ,π(s,a) ν(s,a) ν = sups,a policy π, let cπ represent mis- match discounted state-action distribution π exploration distribution ν.
 Now, consider se- quence policies ˆπ1:N generated H-DAgger-MC.
 Let be uniform mixture policies sequence.
 Let T (cid:107)∞ ¯mc = be error induced choice planning algorithm, averaged iterations.

 H-DAgger-MC, policies are such policy π, (cid:80)N (cid:107) ¯Qn− ˆQρ T,n(cid:107)∞ + ρ T −V ρ (cid:2)V π(s) − V ¯π(s)(cid:3) ≤ s∼µ (cid:16) cπ ν ¯hrwd + ¯mc − γ T−1(cid:88) (cid:17) ≤ − γ cπ ¯erwd + γt−1¯t prd + ¯mc.
 t=1 s∼µ (cid:2)V π(s) − V ˆπn(s)(cid:3).
 N(cid:88) Proof.
 Recall (cid:2)V π(s) − V ¯π(s)(cid:3) = (cid:2)V π(s) − V ˆπn (s)(cid:3) ≤ s∼µ Lemma n ≥ s∼µ − γ (s,a)∼ξπ,ˆπn [| ˆQρ T,n(s, a) − Qρ T (s, a)|] + ¯mc, (cid:18) (cid:88) s(cid:48),a(cid:48) ≤ cπ Dµ,ˆπn (s(cid:48), Learning Reward Function Misspeciﬁed Model (s, z, a)(cid:3) (cid:2) ˆRn (s, z, a)(cid:3) (cid:2) ˆRn (s, z, a)(cid:3) (cid:2) ˆRn (s, z, a)(cid:3) (cid:2) ˆRn (s,z,a)∼H t,n s(cid:48),a(cid:48) ,ρ ν(s(cid:48), a(cid:48)) (cid:88) (s,z,a)∼H t,n ν(s(cid:48)(cid:48), a(cid:48)(cid:48))P a(cid:48)(cid:48) s(cid:48) ,a(cid:48) ,ρ s(cid:48)(cid:48) (s(cid:48))ˆπn(a(cid:48) | s(cid:48)) (cid:88) (cid:88) s(cid:48),a(cid:48) s(cid:48),a(cid:48) s(cid:48)(cid:48),a(cid:48)(cid:48) (cid:88) s(cid:48),a(cid:48) − γ (s,z,a)∼H t,n s(cid:48),a(cid:48) ,ρ µ(s(cid:48))ˆπn(a(cid:48) | s(cid:48)) (cid:2) ˆRn (s,z,a)∼H t,n (s, z, a)(cid:3).
 s(cid:48),a(cid:48) ,ρ (cid:2) ˆRn (s, z, a)(cid:3) = (cid:2) ˆRn (s, z, a)(cid:3).
 (s,a)∼ξn(s,a) = cπ (s,z,a)∼H t,n ξn ,ρ t = (s,z,a)∼H t,n ξn,ρ t > (s,z,a)∼H t,n ξn ,ρ (cid:88) (cid:2) ˆRn (s, z, a)(cid:3) (cid:20) (cid:88) Theorem ¯hrwd = N(cid:88) T(cid:88) (cid:18) T(cid:88) T−1(cid:88) t=1 t=1 t=1 N(cid:88) N(cid:88) n=1 T(cid:88) T−1(cid:88) t=1 t=1 + ≤ (s,z,a)∼H t γt−1 E (s,a)∼Dt γt−1 (s,z,a)∼H t (cid:2) ˆRn(s, z, a)(cid:3) (cid:2)|R(s, a) − ˆRn(s, a)|(cid:3) s | z, a)|(cid:3)(cid:19) (cid:2)1 − ˆP t n(σa (cid:2)|R(s, a) − ˆRn(s, a)|(cid:3) (cid:2)1 − ˆP t s | z, a)|(cid:3) n(σa (s,z,a)∼H t γt−1 E (s,a)∼Dt N(cid:88) n=1 T−1(cid:88) ≤ ¯erwd + γt−1¯t prd.
 t=1 gives second inequality.
 Note result holds comparison policy π.
 Thus, ¯mc is small learned models have low error, ν is similar state-action distribution good policy, ¯π compare it.
 said, Lemma shares limitations comparable results other DAgger algorithms.
 focuses L1 loss, is practical learning objective.
 assumes expected loss iteration be computed (i.e. are many samples iteration).
 applies average policy ¯π, ˆπN
 Ross Bagnell (2012) discuss extensions address more practical loss functions, ﬁnite sample bounds, results ˆπN
 Lemma says models have low train- ing error, resulting policy be good.
 does promise models have low training error.
 Fol- lowing Ross Bagnell (2012) note ¯t prd ¯hrwd be interpreted average loss online learner problem deﬁned aggregated datasets.
 horizon depth t let ¯t pmdl be error best dynamics model P training distribution depth, retrospect.
 Speciﬁcally, ¯tP = inf P (cid:48)∈P (s,z,a)∼H t [1 − P (cid:48)(σa s | z, a)].
 N(cid:88) (s1,a1)∼ξn st,zt,at a1:t−1 ρ(a2:t | a1) P a0:t−1 s1 (st | s1, a0:t−1) ˆP (cid:2) ˆRn (s, z, a)(cid:3).
 (s,z,a)∼H t (zt | s1, a0:t−1) (cid:21)  ˆRn (st, zt, at) Thus, putting together, have shown (cid:2)V π(s) − V ¯π(s)(cid:3) N(cid:88) T(cid:88) s∼µ ≤ − γ cπ n=1 t=1 − γ cπ ν ¯hrwd + ¯mc.
 (cid:2) ˆRn (s, z, a)(cid:3) γt−1 (s,z,a)∼H t + ¯mc Similarly, let ¯R = inf R(cid:48)∈R N(cid:88) T(cid:88) t=1 γt−1 (s,z,a)∼ ˜H t [R(cid:48)(s, z, a)] have proven ﬁrst inequality.
 Furthermore, be error best reward model R retrospect.
 Learning Reward Function Misspeciﬁed Model average regret dynamics model depth t is prd − ¯tP.
 reward model is ¯rrgt = prgt = ¯t ¯t ¯hrwd − ¯R.
 no-regret online learning algorithm, average regret approaches N → ∞.
 gives following bound H-DAgger-MC’s performance terms model regret.

 H-DAgger-MC, policies are such policy π, (cid:2)V π(s) − V ¯π(s)(cid:3) s∼µ ≤ − γ ≤ − γ (cid:16) cπ ν (¯R + ¯rrgt) + ¯mc T−1(cid:88) t=1 (cid:17) cπ ¯erwd + γt−1(¯tP + ¯t prgt) + ¯mc prgt → ≤ t ≤ T −
 learning algorithms are no-regret N → ∞, ¯rrgt → ¯t Theorem says R contains low-error reward model relative learned dynamics models then, discussed above, ¯mc is small ν visits important states, resulting policy yield good performance.
 P R contain perfect models, ¯π be comparable plan generated perfect model.
 noted Talvitie (2017), result does promise H-DAgger-MC achieve performance best available set dynamics models.
 model rollout depth is trained minimize prediction error given input distribution provided shallower models regard effect deeper models.
 is possible better overall error be achieved increasing prediction error depth exchange favorable state distribution deeper models.
 Similarly, discussed Section H-DAgger-MC achieve performance best available combination dynamics reward models.
 dynamics model is trained regard impact reward model.
 be dynamics model higher prediction error allow lower hallucinated reward error.
 H-DAgger-MC does take possibility account.
 long history deep learning (Hinton Salakhutdinov, Autoencoders have Salakhutdinov Hinton, Vincent Kingma Welling,
 most cases, autoencoders operate continuous representations, making bottleneck (Hinton Salakhutdinov, denoising (Vincent et al., adding variational compo- nent (Kingma Welling,
 many cases though, discrete latent representation is better ﬁt.
 Language is discrete, autoregressive models based sequences discrete symbols yield impressive results.
 discrete representation be fed reasoning planning system act bridge other part larger system.
 reinforcement learning action spaces are continuous, Metz et al.
 (2017) show discretizing using autoregressive models yield improvements.
 Unluckily, using discrete latent variables is challenging deep learning.
 continuous interactions autoregressive component cause difﬁculties.
 success (Bowman et Yang et task autoencoding text presence autoregressive decoder has remained challenge.
 work present architecture autoencodes sequence s N discrete symbols vocabulary (e.g., tokenized sentence), K-fold (we test K = K = compressed sequence c(s) ⌈ N K ⌉ latent symbols new vocabulary is learned.
 compressed sequence is generated minimize perplexity conditional) language model trained predict next token c(s) ◦ s: concatenation c(s) original sequence s.
 gradient signals vanish propagating discrete compression func- tion c(s) be hard train.
 solve problem, draw old technique seman- tic hashing (Salakhutdinov Hinton,
 There, discretize dense vector computes σ(v + n) σ is sigmoid function n represents annealed Gaussian noise pushes network use middle values v.
 enhance method using saturating sigmoid straight-through pass only bits passed forward.
 techniques, described detail below, allow forgo annealing noise provide stable discretization mechanism requires annealing additional loss factors.
 test discretization technique amending language models s autoencoded sequence c(s).
 compare perplexity achieved s c(s) component, contrast value number bits used c(s).
 argue number is proper measure performance discrete autoencoder.
 is easy compute captures performance autoencoding part model.
 quantitative measure allows compare technique introduce other methods, show performs better Gumbel- Softmax (Jang et Maddison et al., context.
 discuss use adding autoencoded part c(s) sequence model.
 present samples character-level language model show latent symbols correspond words phrases architecture c(s) is local.
 ehen, introduce decoding method c(s) is sampled s is decoded using beam search.
 method alleviates number problems observed beam search pure sampling.
 show decoding method be used obtain diverse translations sentence neural machine translation model.
 summarize, main contributions paper are: (1) discretization technique works extra losses parameters tune, (2) way measure performance autoencoders sequence models baselines, (3) improved way sample sequence models trained autoencoder part.
 TECHNIQUES Below, introduce discretization method, autoencoding function c(s) com- plete model use experiments.
 code hyperparameter settings needed repli- cate experiments are available open-source1.
 DISCRETIZATION BY IMPROVED SEMANTIC HASHING mentioned discretization method stems semantic hashing (Salakhutdinov Hinton,
 discretize b-dimensional vector ﬁrst add noise, vn = v + n.
 noise n is drawn b-dimensional Gaussian distribution mean standard deviation (deviations work ﬁne, see ablations below).
 sum is component-wise, are operations below.
 Note noise is used training, evalua- tion inference =
 vn compute vectors: v1 = σ′(vn) v2 = (vn < σ′ is saturating sigmoid function (Kaiser Sutskever, Kaiser Bengio, σ′(x) = max(0,
 vector v2 represents discretized value v is used evaluation inference.
 training, forward pass use v1 half time v2 other half.
 backward pass, let gradients ﬂow v1, used v2 forward computation2.
 denote vector v discretized above way vd.
 Note v is b-dimensional vd have b bits.
 other parts system predict vd softmax, want number bits be large.
 experiments stick b = vd is vector bits, be interpreted integer − =
 dense vectors representing activations sequence models have much larger dimen- sionality (often see details experimental section below).
 discretize such high-dimensional vector ﬁrst have simple fully-connected layer converting v = dense(w,
 notation, dense(x, n) denotes fully-connected layer applied x mapping n dimensions, i.e., dense(x, n) = xW + B W is learned matrix shape d × n, d is dimensionality x, B is learned bias vector size n.
 discretized vector vd is converted high-dimensional vector using 3-layer feed-forward network: h1a = dense(vd, filter_size) h1b = dense(1.0 vd, filter_size) transformer vae.py https://github.com/tensorflow/tensor2tensor be done TensorFlow using: v2 += tf.stop gradient(v1).
 h2 = dense(relu(h1a + h1b), filter_size) result = dense(relu(h2), hidden_size) Above, time apply dense create new weight matrix bias be learned.
 relu function is deﬁned standard way: relu(x) = max(x,
 network use large filter size; experiments set hidden size was
 suspect allows above network recover discretization bottleneck simulating distribution w encountered training.
 Given dense, high-dimensional vector w denote corresponding result returned network bottleneck(w) corresponding discrete vector v2 discrete(w).
 GUMBEL-SOFTMAX FOR DISCRETIZATION alternative discretization method, consider studied Gumbel-Softmax (Jang et Maddison et
 case, given vector w compute discreteg(w) applying linear layer mapping elements, resulting logits l.
 evaluation inference pick index l maximum value discreteg(w) vector bottleneckg(w) is computed embedding.
 training ﬁrst draw samples g Gumbel distribution: g ∼ − log(− log(u)), u ∼ U(0, are uniform samples.
 Then, (Jang compute log-softmax l, set: yi = exp((xi + gi)/τ Pi + gi)/τ low temperature τ vector is 1-hot vector representing maximum index l.
 higher temperature, is approximation (see Figure Jang al.
 (2016)).
 multiply vector y embedding matrix compute bottleneckg(w) training.
 AUTOENCODING FUNCTION Having functions bottleneck(w) discrete(w) Gumbel-Softmax describe architecture autoencoding function c(s).
 assume s is sequence dense vectors, e.g., coming embedding vectors tokenized sentence.
 halve size s, ﬁrst layers 1-dimensional convolutions kernel size padding sides (SAME-padding).
 use ReLU non-linearities layers layer-normalization (Ba et
 add input result, forming residual block.
 Finally, process result convolution kernel size stride halving size s.
 local version function do ﬁnal strided convolution, residual block.
 autoencode sequence s shorten K-fold, K = ﬁrst apply above step k times obtaining sequence s′ is K times shorter.
 put discretization bottleneck described above.
 ﬁnal compression function is given c(s) = bottleneck(s′) architecture described is depicted Figure
 Note that, perform convolutions kernel step, network has access large context: · receptive ﬁelds convolutions last step.
 That’s consider local version.
 strided convolutions, i-th symbol local c(s) has access ﬁxed symbols sequence s compress them.
 Training c(s) deﬁned scratch is hard, beginning training s′ is generated many layers untrained convolutions are getting gradients discretization bottleneck.
 help training, add side-path c(s) discretization: use c(s) = s′ ﬁrst training steps.
 pretraining stage network reaches loss everything needed reconstruct s is encoded s′.
 switching c(s) = loss is high improves further training.
 AUTOENCODING SEQUENCE MODEL test autoencoding function c(s) use preﬁx sequence s sequence model.
 Normally, sequence model generate i-th element s conditioning elements Single step Autoencoding function c(s) length×hidden size length×hidden size relu convk=3 s=1 layer-norm convk=2 s=2 ×3 single step ×k bottleneck length/2×hidden size length/K×hidden size Figure Architecture autoencoding function c(s).
 write convk=a volutional layer kernel size stride b.
 See text more details.
 s=b denote con- si+1 si si+1 si reverse c(s) Standard language model.
 Autoencoder-augmented language model.
 Figure Comparison standard language model autoencoder-augmented model.
 architecture c(s) is presented Figure arrows si s<i depict dependence.
 s that, s<i, other inputs.
 language model condition s<i neural machine translation model condition input sentence (in other language) s<i.
 do change sequence models way other adding sequence c(s) preﬁx s.
 Actually, reasons analogous (Sutskever et ﬁrst reverse sequence c(s), add separator symbol (#), concatenate s, depicted Figure
 use separate set parameters model predicting c(s) make sure models predicting s c(s) have same capacity.
 architecture sequence model use Transformer (Vaswani et
 Trans- former is based multiple attention layers was introduced context neural machine translation.
 focused autoencoding function c(s) did tune sequence model work: used defaults baseline provided Transformer authors (6 layers, hidden size ﬁlter size varied parameters relevant c(s).
 EXPERIMENTS experimented autoencoding different sequence tasks: (1) character-level language model, (2) word-level language model, (3) word-level translation model.
 goal (1) was check technique works all, character sequences are amenable compression shorter sequences objects larger vocabulary.
 (2), wanted check good results obtained (1) hold input is larger vocabulary compressed space.
 Finally, (3) want check method is applicable conditional models be used improve decoding.
 use LM1B corpus (Chelba language modelling tokenize using subword (wordpiece) tokenizer (Sennrich et vocabulary words word- pieces.
 translation, use WMT English-German corpus, tokenized vocab- ulary words word-pieces3.
 used https://github.com/tensorflow/tensor2tensor data preparation.
 Problem LM-en (characters) LM-en (word) NMT-en-de (word) LM-en (word, Gumbel-Softmax) NMT-en-de (word, Gumbel-Softmax) ln(p) ln(p’) K DSAE Table Log-perplexities word sequence models autoencoders, autoencoding efﬁciency.
 Results Gumbel-Softmax depend tuning; see text details.
 report qualitative quantitative results.
 First, focus measuring perfor- mance autoencoder quantitatively.
 do introduce measure discrete autoen- coder performance sequence tasks compare semantic hashing based method Gumbel- Softmax scale.
 DISCRETE SEQUENCE AUTOENCODING EFFICIENCY Sequence models trained next-symbol prediction are trained (and evaluated) based perplexity token reach.
 Perplexity is deﬁned H is entropy (in bits) distribution.
 Therefore, language model reaches per-word perplexity p, say p sentence s be said compress word s log(p) bits information.
 Let assume model is allowed access additional bits information s decoding.
 autoencoding case, let peek c(s) decoding s, c(s) has K = times symbols b = bits symbol.
 c(s) has information capacity bits word.
 autoencoder was aligned needs language model, allowing peek c(s) lower information needs bits word.
 perplexity p′ model access c(s) satisfy log2(p′) = perplexity be p′
 Getting autoencoder c(s) aligned language model is hard, practice perplexity p′ is higher.
 measure (and calculate many bits has c(s) part contributed lowering perplexity.
 calculate log2(p) − log2(p′) then, c(s) is K-times shorter s uses b bits, deﬁne discrete sequence autoencoding efﬁciency as: DSAE = K(log2(p) − log2(p′)) K(ln(p) − ln(p′)) b ln(2) second formulation is useful raw numbers are given natural logarithms, is case neural networks training.
 Deﬁned way, DSAE measures many available bits c(s) are used model peeks autoencoded part.
 Note models have autoencod- ing capacity higher number bits word log(p) indicates.
 case achieving DSAE=1 is impossible log(p′) = autoencoding is perfect.
 be careful reporting DSAE such over-capacitated models.
 does method perform DSAE does compare Gumbel-Softmax?
 Ta- ble list log-perplexties baseline autoencoder models.
 report numbers global version c(s) problems compare Gumbel-Softmax word-level problems.
 did manage run Gumbel-Softmax character-level data baseline conﬁguration requires much memory needs learn embeddings latent discrete symbol).
 found results Gumbel-Softmax depend tempera- ture parameter τ is annealed training.
 tuned runs smaller model chose best conﬁguration.
 was enough, many runs Gumbel-Softmax utilize small portion discrete symbols.
 added extra loss term increase variance Gumbel-Softmax ran tuning runs optimize loss term.
 used best Noise standard deviation ln(p) ln(p’) K DSAE Table Autoencoder-augmented language models different noise deviations.
 values noise (0.0) deviation yield DSAE
 conﬁguration experiments above.
 Still, did manage get information autoen- coded translation model, got efﬁciency language model (see Table
 method, other hand, was efﬁcient character-level language modeling, reach efﬁciency, retained high efﬁciency word-level language modeling task.
 translation efﬁciency goes c(s) function does take inputs account, be able compress right parts align conditional model outputs s depending inputs.
 efﬁciency is useful sampling model, shown below.
 SENSITIVITY TO NOISE make sure autoencoding method is stable, experiment different standard devi- ations noise n semantic hashing part.
 perform experiments word-level language modelling smaller model conﬁguration (3 layers, hidden size ﬁlter size
 results, presented Table show method is robust amount noise.
 Interestingly, see method works noise (standard deviation
 suspect is due fact half time forward computation use discrete values pass gradients dense part.
 Also, note standard deviation works, fact saturating sigmoid is saturated values · σ(2.4) − =
 Finally, deviation small model achieves DSAE much worse achieved large baseline model better larger baseline model Gumbel-Softmax.
 DECIPHERING LATENT CODE Having trained models, try ﬁnd discrete latent symbols have inter- pretable meaning.
 start asking simpler question: do latent symbols correspond ﬁxed phrases topics?
 ﬁrst investigate 32-fold compressed character-level language model.
 set c(s) random latent symbols [l1, l2, l3, l4] decode s beam search, goods are subject Member States’ environmental security aspects common agricultural policy.
 Now, ﬁnd second symbol c(s) stands anything ﬁxed, replace third symbol second one, hoping phrase be repeated.
 Indeed, decoding s new c(s) = [l1, l2, l2, l4] beam search obtain: goods are charged EUR night EUR night stay night.
 Note beginning sentence remained same, did change ﬁrst symbol, see repetition EUR night.
 Could be is second latent symbol stands for?
 were EUR ﬁrst sentence.
 Let try changing ﬁrst symbol different one.
 c(s) = [l5, l2, l2, decoded s is: All suited large suite large living room suites are available.
 see repetition again, different phrase.
 are forced conclude latent code is meaning latent symbols depend other symbols them.
 Failing decipher code model, try 8-fold compressed character-level language model uses local version function c(s).
 Recall (see Section local function c(s) 8-fold compression generates latent symbol exact symbols correspond s, context.
 simpler c(s) model has lower DSAE, expect latent symbols be more context-independent.
 indeed: pick ﬁrst latent symbols random ﬁx third, fourth ﬁfth be same, obtain following: It’s studio, gallery gallery
 prices health gallery gallery
 offer hotels least gallery gallery
 ﬁxed latent symbol corresponds word gallery various contexts.
 Let ignore context-dependence, ﬁrst symbols, choose one repeat them.
 are few sample decodes: Come earth culturalized climate climate
 Come contribution itself, itself,
 Come learn countless threat gas threat...
 ﬁrst samples see latent symbol corresponds climate itself, respectively.
 Note words phrases are 7-characters long character space), proba- due architecture c(s).
 last sample see different phenomenon: latent symbol seems correspond X threat, X depends context, showing latent code has interesting structure.
 MIXED SAMPLE-BEAM DECODING results know discretization method works see interesting patterns latent code.
 use autoencoder models practice?
 well-known problem autoregressive sequence models is decoding.
 settings possible outputs are restricted, such translation, obtain good results beam search.
 results obtained beam search lack diversity (Vijayakumar
 Sampling improve diversity, introduce artifacts change semantics translation.
 present example problem Figure
 pick English sentence validation set English-German dataset translate using beam search sampling (left middle columns).
 left column, show top results beam search using baseline model autoencoder).
 is necessary speak German see are similar; only difference ﬁrst last are spaces ”%”.
 Further beams are this, providing real diversity.
 middle column show results sampled baseline model.
 is more diversity them, share most ﬁrst half changed semantics sentence second half.
 part African-Americans, accounted voters State becomes american voters were voters state ﬁrst case, African-Americans, accounted people State second one, African-Americans, elected people State third case.
 illustrates dangers sampling different words decoding.
 Using model access autoencoded part c(s) presents option: sample c(s) run search sequence s appropriate c(s).
 way do introduce low-level artifacts sampling, preserve high-level diversity.
 sample c(s) train language model c(s) same architecture model s (and conditioned input), different set weights.
 use standard multinomial sampling model obtain c(s) run beam search model s sampled c(s).
 English sentence: example, general election Florida, early voters were African- Americans, accounted voters State.
 Base model, beam decoding.
 Base model, sampling.
 Mixed decoding.
 der W¨ahrend Parla- mentswahlen Florida beispielsweise waren % der fr¨uhen W¨ahler Afroamerikaner, die jedoch % der W¨ahler im Staat ausmachten.
 der W¨ahrend Parla- mentswahlen Florida beispielsweise waren % der fr¨uhen W¨ahler Afroamerikaner, die jedoch % der W¨ahler im Staat stellten.
 der W¨ahrend Parla- mentswahlen Florida beispielsweise waren der fr¨uhen W¨ahler Afroamerikaner, die jedoch der W¨ahler im Staat ausmachten.
 waren zum Beispiel bei den Parlamentswahlen Florida % der fr¨uhen W¨ahler Afroamerikaner.
 Die amerikanischen W¨ahler waren aber nur % der W¨ahler im Staat.
 waren w¨ahrend der Parla- mentswahlen Florida % der fr¨uhen W¨ahler Afroamerikaner, die aber nur % der Bev¨olkerung im Staat ausmachten.
 waren w¨ahrend der Parla- mentswahlen Florida der fr¨uhen W¨ahler Afroamerikaner, die jedoch der Bev¨olkerung im Staat w¨ahlten.
 sich stellte Es beispiel- sweise im Verlauf der Parla- mentswahlen Florida heraus, dass der fr¨uhen W¨ahler zu den Amerikan- ern z¨ahlten, die allerdings der W¨ahler des Staates betrafen.
 ist zum Beispiel Dabei im Laufe der Parlamentswahlen Florida den fr¨uhen Wahlen der Afro- Amerikaner die allerdings der W¨ahler des Staates betrafen.
 vertreten, Hauptwahlen fr¨uhen W¨ahler der beispielsweise waren w¨ahrend der afrikanische Amerikaner, die f¨ur einen An- teil von der W¨ahler im Staat verantwortlich waren.
 Florida Figure Decoding baseline autoencoder-enhanced sequence-to-sequence models.
 right column Figure show samples obtained way.
 see, samples are more diverse preserve semantics original sentence, sometimes strange syntax.
 back-translate ﬁrst example as: turned out, example, course parliamentary elections Florida, early voters are African-Americans, were, however, voters state.
 Note addition turned restructuring sentence.
 third sample whole order is reversed, starts voters
 election phrase.
 Obtaining such samples differ phrase order other aspects preserve semantics has been challenge neural translation.
 CONCLUSION work, study text autoencoders (Bowman et Yang et al., is combined research discrete autoencoders (Jang et al., Maddison et
 turns semantic hashing technique (Salakhutdinov Hinton, be improved yields good results context.
 introduce measure efﬁciency discrete autoencoders sequence models show improved semantic hashing has efﬁciency.
 cases, decipher latent code, showing latent symbols correspond words phrases.
 practical side, sampling latent code running beam search allows get valid diverse samples, important problem beam search (Vijayakumar
 leave number questions open future work.
 does architecture function c(s) affect latent code?
 improve discrete sequence autoencoding efﬁciency?
 remaining questions, see potential applications discrete sequence autoen- coders.
 is training multi-scale generative models end-to-end, opening way generating realistic images, audio video.
 application is reinforcement learning.
 Using la- tent code allow agents plan larger time scales explore sampling high-level latent actions atomic moves.
 REFERENCES Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hinton.
 Layer normalization.
 arXiv preprint arXiv:1607.06450,
 Samuel R.
 Bowman, Luke Vilnis, Oriol Vinyals, Andrew M.
 Dai, Rafal J´ozefowicz, Samy Bengio.
 Generating sentences continuous space.
 Proceedings SIGNLL’16, pp.

 Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, Tony Robinson.
 word benchmark measuring progress statistical language modeling.
 CoRR, abs/1312.3005,
 URL http://arxiv.org/abs/1312.3005.
 Geoffrey E.
 Hinton Ruslan Salakhutdinov.
 Reducing dimensionality data neural networks.
 Science,
 Eric Jang, Shixiang Gu, Ben Poole.
 Categorical reparameterization gumbel-softmax.
 CoRR, abs/1611.01144,
 URL http://arxiv.org/abs/1611.01144.
 Łukasz Kaiser Samy Bengio.
 active memory replace attention?
 Advances Neural Information Processing Systems, (NIPS),
 Łukasz Kaiser Ilya Sutskever.
 Neural GPUs learn algorithms.
 International Conference Learning Representations (ICLR),
 Diederik P.
 Kingma Max Welling.
 Auto-encoding variational bayes.
 CoRR, abs/1312.6114,
 Chris J.
 Maddison, Andriy Mnih, Yee Whye Teh.
 tinuous relaxation discrete random variables.
 http://arxiv.org/abs/1611.00712.
 concrete distribution: con- URL CoRR, abs/1611.00712,
 Luke Metz, Julian Ibarz, Navdeep Jaitly, James Davidson.
 Discrete sequential prediction con- tinuous actions deep rl.

 URL https://arxiv.org/abs/1705.05035.
 Ruslan Salakhutdinov Geoffrey E.
 Hinton.
 Deep Boltzmann machines.
 Proceedings AISTATS’09, pp.

 Ruslan Salakhutdinov Geoffrey E.
 Hinton.
 Semantic hashing.
 Int.
 J.
 Approx.
 Reasoning,
 Rico Sennrich, Barry Haddow, Alexandra Birch.
 Neural machine translation rare words subword units.
 Proceedings ACL’16,
 Ilya Sutskever, Oriol Vinyals, Quoc VV Le. Sequence sequence learning neural net- Advances Neural Information Processing Systems, pp.

 URL works.
 http://arxiv.org/abs/1409.3215.
 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.
 Gomez, CoRR,
 URL Lukasz Kaiser, Illia Polosukhin.
 Attention is need.
 http://arxiv.org/abs/1706.03762.
 Ashwin K.
 Vijayakumar, Michael Cogswell, Ramprasath R.
 Selvaraju, Qing Sun, Ste- Diverse beam search: Decoding di- URL fan Lee, David J.
 Crandall, Dhruv Batra.
 verse solutions neural sequence models.
 http://arxiv.org/abs/1610.02424.
 CoRR, abs/1610.02424,
 Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol.
 Stacked denoising autoencoders: Learning useful representations deep network local denoising criterion.
 Journal Machine Learning Research,
 Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, Taylor Berg-Kirkpatrick.
 Improved variational autoencoders text modeling using dilated convolutions.
 Proceedings ICML’17, pp.


 Healthcareassociatedinfectionsrepresentamajorcauseofmorbidityandmortal-ityintheUnitedStatesandothercountries[1].Althoughmanycanbetreated,theseinfectionsaddgreatlytohealthcarecosts[2].Furthermore,theemergenceofmultidrugresistantbacteriahavegreatlycomplicatedtreatmentofhealthcareas-sociatedinfections[3],makingthepreventionoftheseinfectionsevenmoreimpor-tant.Oneofthemosteﬀectiveinterventionsforpreventinghealthcareassociatedinfectionsishandhygiene[4].Yet,despiteinternationalprogramsaimedatincreas-inghandhygiene[4,5,6],ratesremainlow,lessthan50%inmostcases[4,6,7].Becauseoftheimportanceofhandhygieneinpreventinghealthcareassociatedinfections,infectioncontrolprogramsareencouragedtomonitorratestoencour-ageprocessimprovement[6,8,9].Inmostcases,handhygienemonitoringisdoneexclusivelybyhumanobservers,whicharestillconsideredthegoldstandardformonitoring[7].Yet,humanobservationsaresubjecttoanumberoflimitations.Forexample,humanobserversincurhighcostsandtherearediﬃcultiesinstan-dardizingtheelicitedobservations.Also,thetimingandlocationofobserverscangreatlyaﬀectthediversityandthequantityofobservations[10,11].Furthermore,thedistanceofobserverstohealthcareworkersunderobservationandtherelativebusynessofclinicalunitscanadverselyaﬀecttheaccuracyofhumanobservers[11].Thepresenceofhumanobserversmayartiﬁciallyincreasehandhygieneratestem-porarilyjustasthepresenceofotherhealthcareworkerscaninducepeereﬀectstoincreaserates[12,13].Finally,thenumberofhumanobservationspossibleisquitesmallincomparisontothenumberofopportunities[7,12].Asaconsequence,severalautomatedapproachestomonitoringhavebeenpro-posed[8,14,15,16].Manyofthesemeasurehandhygieneuponenteringandleavingapatient’sroom.Thesubsequentactivationofanearbyhandhygienedispenserisrecordedasahandhygieneopportunityfulﬁlledwhereas,ifnosuchactivationisobserved,theopportunityisnotsatisﬁed.Suchapproaches,whilenotcapturingallﬁvemomentsofhandhygiene,doprovideaneasyandconvenientmeasureofhandhygienecompliance.Withautomatedapproachesbecomingmorecommon,amoreongoingandcomprehensivepictureofhandhygieneadherenceshouldemerge,providingnewinsightsintowhyhealthcareworkersabstainfrompracticinghandhygiene.
 21MillionOpportunities3Inthiswork(anextensionof[17]),weprovideanin-depthexplorationoffactorsaﬀectinghandhygienecomplianceacrossmultiplehospitalfacilitiesusinglinearpredictivemodels.2DataandMethods2.1HandHygieneEventDataOurhandhygieneeventdataisaproprietarydatasetprovidedbyGojoIndustries.Thedatawereobtainedfromanumberofinstallationsconsistingofdoorcountersensors1,whichincrementacounteranytimeanindividualgoesinoroutofaroom,andhandhygienesensors,whichincrementacounterwhensoaporalcoholrubaredispensed.Additionalsupportingtechnologywasalsoinstalledtocollectandrecordtimestampedsensor-reportedcounts.WeprovideasimpleillustrationofhowthesetechnologiesareusedinFigure1andapictureofaninstrumentedroomentranceinFigure2.Inthispaper,wewillusethetermdispensereventtodesignatetriggeringanduseofaninstrumentedhandhygienedispenseranddooreventtodesignatethetriggeringofacountersensorlocatedononeoftheinstrumenteddoors.(a)(b)(c)Fig.1:Asimpleillustrationofthesensorsandcorrespondinginfrastructure.In(a),healthcareworkersenterandexitpatientroomsthatareﬁttedwithsensors,interactingwithinstrumenteddispensersastheydo;notethatthesensoronthehandhygienedispenserisinternal,andnotvisible.In(b),thesedooranddispensercountsareintermittentlysenttoawirelesstransmitter.In(c),thesecountsarerelayedviatransmitterandstoredinadatabase,alongwithotherinformation,suchastheroomthecountscamefromandthetimeanddateinwhichtheyweresent.1Practicallyspeaking,thesesensorscanbeﬁttoanysortofpatiententrance/exitarea,asdepictedinFigure2.
 4MichaelT.LashMSetal.Fig.2:Anurseapplyinghandhygienerubuponleavinganinstrumentedpatientarea.Notethedoorsensorhighlightedbytheredbox.Atotalof19facilitiesin10stateswereoutﬁttedwithsensors;becauseofprivacyconcerns,wereportonlythestateandCDCDivisionforeach.Thefacilitiescompriseawiderangeofgeographies,spanningbothcoasts,themidwest,andthesouth.Atotalof1851doorsensorsand639dispensersensorsreportedatotalof24,525,806dooreventsand6,140,067dispensereventsacrossthese19facilitiesbetweenOctober21,2013andJuly7,2014.Eachfacilitycontributedanaverageof172.3reportingdays,makingthisstudythelargestinvestigationofhandhygienecompliancetodate(i.e.,largerthanthe13.1millionopportunitiesreportedin[18]).Assumingeachdooreventcorrespondstoahandhygieneopportunity,weestimateanaveragefacilitycompliancerateof25.03%,inlinewithifnotjustbelowthereportedlow-endratefoundin[19].Theoriginaldata,consistingoftimestampedcountsreportedfromindividualsensorsovershortintervals,werere-factoredtosupportouranalysis.First,datafromeachsensorwerebinnedbytimestamp,t,into12hourintervals,correspondingtotraditionaldayandnightshifts,asindicatedbyanadditionalvariable,night,deﬁnedasfollows:nightShift=(1t[7pm,7am)0t[7am,7pm)Second,dooranddispensercountswereaggregatedbasedondayandnightshiftsoastoproduceaseriesofshift-levelrecords.Foreachsuchrecordwecomputehandhygienecompliance,orjustcompliance,bydividingthenumberofreported 21MillionOpportunities5dispensedeventsbythenumberofdoorevents:compliance=#dispenser#doorSuchadeﬁnitionofcomplianceassumesthateachdooreventcorrespondstoasinglehand-hygieneopportunityandeachdispensereventcorrespondstoasinglehand-hygieneeventwhereas,inreality,ahealthcareworkermightwellbeexpectedtoperformhandhygienemorethanonceperentry,resultinginratesthatexceedone,ifonlyslightly.Thisestimatoralsoignorestheplacementofdoorswithrespecttodispensers:multipledispensersmaywellbeassociatedwithasingledoorway,andsomedispensersmaybeinroomshavingmultipledoors.Thus,simplyaddingnewdispenserswillraiseapparentcomplianceratescomputedinthisfashion,whileaddingnewdoorsensorswillappeartoreducecompliance.Evenso,whenappliedconsistentlyandifsystemlayoutsareﬁxed,thisestimatorisareasonableapproximationoftruehandhygienecompliance,andsupportssoundcomparisonswithinafacility(butnotacrossfacilities).Becausemalfunctioningsensorsordeadbatteriescanproduceoutliers(i.e.,veryloworveryhighvalues),shiftswithfewerthan10doorordispensereventsreportedperday(possiblyindicatinganinstallationundergoingmaintenance),zerocompliance,orcompliancevaluesgreaterthan1wereremovedpriortoanalysis(atthecostofpossiblyexcludingsomelegalrecords).Theremainingdataconsistof5308shiftsfromtheoriginal5647records,having21,273,980handhygieneopportunitiesand5,296,749handhygieneevents(seeTable1).FacilityStateCDCDivTotDispTotDoorDaysRep91OHENC234292518772252101OHENC3509012021665260105TXWSC2388991940024260119MNWNC123877242939156123TXWSC3256181112198243127NMMnt13068554546171260135OHENC125731264331258144CAPac3989611744642260145CAPac5670962073566260147CAPac5009792462900260149CAPac5907082306392260153CTNewE169564603482208155NYM-At171275619507117156NCS-At43813820015157OHENC39455313396101163OHENC344102335168PAM-At304218690920170ILENC11260435363147173OHENC47881512232Total1085296749212739803274Table1:Descriptivestatisticsforallreportingfacilitiesintermsofstate,CDCdivision,handhygieneevents,peopleevents,andreportingdays.
 6MichaelT.LashMSetal.2.2FeatureDeﬁnitionsInthissubsectionwedeﬁnethefeatures(factors)thatwillbeexamined,andhoweachisderived.Fig.3:Assigning(redbox)NOAAweatherdata,reportedintermsofageographicgrid,tohealthcarefacilities(reddots),wherethebluecolorgradientmightrep-resenttemperature.2.2.1LocalWeatherDataBecausehealthcareworkersfrequentlyciteskindrynessandirritationasafactorindecreasedcompliance(particularlyincoldweathermonthswhereenvironmen-talhumidityisreduced),weassociatedailyairtemperature(denotedtemp)andrelativehumidity(denotedhumid)toeachtimestampedrecordbasedoneachfa-cility’sreportedzipcode.Spatiallyassimilatedweathervalues(σ=0.995)fortheentireglobewereobtainedfromtheNationalOceanicandAtmosphericAdminis-tration(NOAA)[20].Givenintermsofgridelements(atessilationofboundingboxescovering2.5°latitudeby2.5°longitude),theworldisthusdeﬁnedasa144by73gridhaving10512distinctgridelements.Weatherdataareavailableataﬁneleveloftemporalgranularity(ontheorderof4timesdailyforeachgridunit)fortheentireperiodofinterest.Thegeographicalassignmentofweatherdatawasob-tainedbyﬁrstmappingeachfacility’snumericalzipcodetothezipcode’scentroid(2010USCensusdata),andthensubsequentlymappingzipcodecentroid(lat,lon)tothecorrespondingNOAAgridelement.AnexampleofthisassignmentcanbeobservedinFigure3.Weassociateweatherinformationfromtheobservationtemporallyclosesttothestartofeachshift.
 21MillionOpportunities72.2.2InﬂuenzaSeverityWeconjecturethatthelocalseverityofcommonseasonalinfectiousdiseasessuchasinﬂuenzamayalsoaﬀecthandhygienecompliancerates.Wedeﬁneinﬂuenzaseverity(denotedflu)asthenumberofinﬂuenza-relateddeathsrelativetoalldeathsoveraspeciﬁedtimeinterval.InﬂuenzaseveritydatawereobtainedfromtheCDC’sMorbidityandMortalityWeeklyReport(MMWR),whichalsoreportsdataatweeklytemporalgranularity.RatherthanreportingdatabyCDCregion,however,dataareprovidedbyreportingcity(oneof122participatingcities,mostlylargemetropolitanareas).Wemapeachfacilityinourdatasettotheclosestreportingcityinordertoassociatetheappropriateseverityvaluetoeachrecord.InotherwordsrepCity=argmin{dist(facility,cityi):i=1,...,122}wheredist(fac,city),k(faclat,faclon),(citylat,citylon)k2,theEuclideandis-tancebetweentwoentitiesgivenintermsof(lat,lon)coordinates.Eightof19facilitieswerelocatedinareportingcity(i.e.,dist(fac,city)=0).Theremaining11facilitiesweremappedtoareportingcitythatwas,onaverage,66.2milesaway(only3of19facilitiesweremappedtoareportingcityfurtherthanthisaverage,withthelargestdistancebeing142miles).2.2.3TemporalFactorsWealsoconjecturethatexternalfactorsassociatedwithspeciﬁcholidaysoreventsmayaﬀecthandhygienecompliancerates.Holidaysmaychangestaﬃngratesoraﬀecthealthcareworkerbehaviors.Thenumberofvisitors(aﬀectingdoorcounterrates)mayalsobegreaterthanduringregularweekdays.Holidayssuchasthe4thofJulyareoftenassociatedwithalcohol-relatedaccidents,andmayincreasehealthcarefacilityworkloads(similarfactorsmayalsoapplyonweekends).Wedeﬁneanewvariableholidaythatreﬂectswhetheragivenshiftoccursononeofthe10federalholidays(NewYear’sEve,MartinLutherKingDay,Presi-dent’sDay,MemorialDay,the4thofJuly,LaborDay,ColumbusDay,Veteran’sDay,ThanksgivingorChristmas)where,ifanypartoftheshift(day/night)fallsontheholidayinquestion,theindicatorissetto1.Moreformally:holiday=(0t/∈{holidays}1t∈{holidays}Similarly,inordertoascertaintheimpactofweekendsoncompliance,wedeﬁneanewvariableweekdayasfollows:weekday=(0t∈{Sat,Sun}1t∈{Mon,Tues,Weds,Thurs,Fri}Noteherethatifashiftspanstheweekdayintoaweekend(orviceversa),itisencodedasaweekend.Arelatedconceptisthepresenceofnewresidentphysicians,whotraditionallystartworktheﬁrstofJuly.Wedeﬁneanewvariablethatcorrespondswiththis 8MichaelT.LashMSetal.timeperiodinordertoseeifthedatarevealthepresenceofaJulyeﬀect(denotedJuly):July=(0t/∈July1−71t∈July1−72.3ExploringFactorsAﬀectingHandHygiene2.3.1M5RidgeRegressionforFeatureExaminationWithcovariatesdeﬁnedandassociatedwiththecollectedsensordata,wewishtobuildalinearhypothesishthat(a)accuratelyestimateshandhygieneand(b)reportsthedirectionanddegreeofeﬀectofourdeﬁnedfeatures.Inaccomplishing(b)webearinmindtwothings:(1)Theremaybemulti-collinearityamongfeatures,whichmayadverselyaﬀecttheoutput.(2)That(a)and(b)maybeatoddswithoneanother;i.e.,obtaininggoodpre-dictionsmayentaildiscardingsomeprediction-inhibitingfeaturesforwhichwewouldliketoobtaineﬀectestimates(inpractice,weﬁndthatthisisnotactuallythecase).Therefore,weproposeanM5RidgeRegressionforFeatureExaminationmethoddesignedtoaccomplish(a)and(b),whilebearing(1)and(2)inmind.Thismethodisgivenbyh∗=argminh∈HlkΛ(X)h−yk22+λkhk22s.t.ρ(hj)≤.05∀j(1)whereX∈Rn×pisadesignmatrix,histhehypothesis,yisthetargetvectorconsistingofcomplianceratesinwhichaparticularyi∈[0,1],λisaregularizationterm,k·k2isthe‘2-norm,andρ(·)isafunctionthatreportsthep-valueofahypothesisterm(thisconstraintisensuredviasequentialbackwardselimination[21]).ThefunctionΛ(X)canbedeﬁnedasΛ(X),argmin{t∈THl}(2)wheretishypothesisselectedfromatreeofhypothesesconstructedusingtheM5method[22].Eﬀectively,(2)onlyreducesthepdimension,actingasafeatureselectionmethod,andhavingnobearingonthendimension.Thereareafewbeneﬁtsoftheabovemethodworthnoting.First,thehypothe-sisclassHlislinearandcommontoboth(1)and(2).Suchtwo-stageoptimizationapproaches,wheretheﬁrstobjectiveisoptimizedtakingintoaccountthehypoth-esisclassbeforethehypothesisitselfisoptimizedforpredictiveaccuracy(orsomeothersuchmeasure),havebeenshowntoworkwellinothercontexts[23].Sec-ondly,suchamethodisspeciﬁcallygearedtowardproducingahypothesisthatmakesuseoffeaturesthathaveanimmediatebearingupontheproblem,while 21MillionOpportunities9eliminatinginterpretabilityobscuringeﬀects,suchasmulti-collinearity.Moreover,thesedesirablesareobtainedwhileattemptingtoproducethemostaccuratehy-pothesis:anhthatelicitsfeatureindicativeness,producesaccurateresults,andcontrolsforconfoundingeﬀectsisthegoalofthistwo-stepoptimizationprocedure.Ultimately,weconductouranalysisbyobservingthesignandmagnitudeofthevaluesinthehypothesisvectorinordertodeterminethefactorsthatinﬂuencehandhygienecompliance,andwhethersuchfactorsaﬀectcomplianceinapositiveornegativemanner.WealsoobservecorrelationandRMSEvaluestodeterminehowwellourpredictivemodelworks,andwhetherthecorrespondingresultscanbetrusted.Allresultsandareobtainedviak-foldcross-validation(k=10).2.3.2SupportingMethodologyWealsousetwoestablished/standardtechniques–RReliefFfeaturerankingandmarginaleﬀectsmodeling–thatwillserveasapointofcomparisonbetweenourmethod,andalsohelpinformthediscussionoftheresultsobtained2.Featureranking:First,weproposetheuseoftheRReliefFalgorithm[26],amodiﬁcationoftheoriginalReliefalgorithmofKiraandRendell[27].RReliefFﬁndsafeaturej’sweightbyrandomlyselectingaseedinstancexifromdesignma-trixXandthenusingthatinstance’sknearestneighborstoupdatetheattribute.Thisdescriptionconsistsofthreeterms:theprobabilityofobservingadiﬀerentrateofhandhygienecompliancethanthatofthecurrentvaluegiventhatofthenearestneighbors,givenbyA=p(rate6=ratexi|kNN(xi)),(3)theprobabilityofobservingthecurrentattributevaluegiventhenearestneighbors,givenbyB=p(xi,j|kNN(xi)),(4)andtheprobabilityofobservingadiﬀerenthandhygieneratethanthecurrentvaluegivenadiﬀerentfeaturevaluevandthenearestneighbors,givenbyC=p(rate6=ratexi|kNN(xi)∧j=v).(5)Attributedistanceweightingisusedinordertoplacegreateremphasisoninstancesthatareclosertotheseedinstancewhenupdatingeachterm;ﬁnalweightsareobtainedbyapplyingBayes’ruletothethreetermsmaintainedforeachattribute,whichcanbeexpressedC∗BA−(1−C)∗B1−A.(6)Byusingthismethodwecouldthenrankattributesintermsoftheirimportance.Weagainreportrankingsusingk-fold(k=10)crossvalidation.MarginalEﬀectsModeling:Toprovideadditionalinsightintothefeaturesthatarerelevanttohandhygieneweanalyzedtheirmarginaleﬀects[28].Marginaleﬀects,alsoreferredtoasinstantaneousratesofchange,arecomputedbyﬁrst2NotethatboththeLASSO[24]andElasticNet[25]wouldhavealsomadeappropriatesupportingmethods.
 10MichaelT.LashMSetal.trainingahypothesish,then,usingthetestingdata,theeﬀectofeachcovariatecanbeestimatedbyholdingallothersconstantandobservingthepredictions.Suchamethodcanbeexpressedbyˆratei,j=h>[xi,j,¯x6=j](7)where,withaslightabuseofnotation,xi,j,thevalueofinstancei’sjthfeature,isaddedtothevector¯x6=j,whichconsistsoftheaverageofeachnon-jfeature,attheappropriatelocation(namely,thejthposition).Here,thenotation6=jisusedtoreinforcethefactthatthevectorofaverages¯xhasit’sjthelementreplacedbyxi,j.Othernon-jentriesaregivenby¯xk=µ(Xk),foranarbitraryindexpositionk.3Results3.1GlobalModelInthissectionweexaminetheresultsobtainedbycross-validatingglobalmodels,whereallfacilityrecordsareused,andafacility-identifyingfeatureisincluded.3.1.1PredictivePower:M5RidgeRegressionWelearnedahypothesisusingallavailablefeatures,includinganominalizedfa-cilityidentiﬁer.OurpredictiveresultscanbeobservedinTable2.WenotethattheRMSEisnotlargeandthecorrelationismoderate,implyingrelativelygoodpredictiveperformance.MeasureValueCorrelation0.3441RMSE0.1702Table2:CorrelationcoeﬃcientandRMSEofcross-validatedmodelpredictions.3.1.2ExaminingHypothesish∗Wenextexaminethetermsofthelearnedhypothesish∗(seeTable3).Themodelincludesuniqueidentiﬁersforall19facilities,12ofwhichhadpositivecorre-spondingvalues,indicatingrelativelyhigherratesofcompliance.Theremainingfacilities’h∗termshadrelativelysmallnegativevalues,indicatinglowerratesofcompliance.Amongotherfeatures,holidaysareassociatedwithlowercomplianceratesandinﬂuenzaseveritywithhighercompliance.Weekdaysareassociatedwithhighercompliancerates,asarehighertemperaturesandhumidity.Interestingly,theM5RidgeRegressionmodelappearstohaveeliminatedsomeholidays(MartinLutherKingday,Memorialday,Laborday,Columbusday,andThanksgiving),aswellasFacility163(thefacilitywiththelowestamountofhand-hygienedata).Thismeansthatthesefeaturesdonotcontributetohand-hygienecomplianceratesinanymeaningfulway.
 21MillionOpportunities11Featurehjfacility−={1,105,147,156,157,170}hj∈Fac−∈[−0.103,−0.016]facility+={91,119,123,127,135,144,hj∈Fac+∈145,149,153,155,168,173}[0.008,0.261]temp0.022humid0.0079weekday0.0069night−0.0218holiday={IndepDay,Pres.Day,hj∈HolVetDay,NewYear’s,Christmas}[−0.017,−.006]flu0.014July−0.0106Table3:Featurespeciﬁchjterms,whereredhighlightsfeatureswithanegativeassociationandbluehighlightsthosewithapositiveassociation.3.1.3RReliefFUsingRReliefFwecanrankfeaturesintermsoftheirimportanceinordertosupportandsupplementtheresultobtainedusingM5RidgeRegression.TheseresultsarereportedinTable4,whererankingsshownareaveragesfor10-foldcross-validation.Notethatherefacilitywasrepresentedasasinglediscretely-valuedfeatureinordertodeterminetheimportanceoffacilityasawhole(insteadoftreatingeachfacilityasitsownfeature),aswasholiday.AttributeAvgValAvgRankfacility0.029(±.001)1flu0.0072temp0.0053.3(±0.46)weekday0.0025humid.0016.3(±0.64)July≈0.07.2(±0.4)holiday≈0.07.8(±1.08)night≈0.08.7(±0.46)Table4:RReliefFattributeweights.3.1.4MarginalEﬀectsTheresultsobtainedfrommodelingthemarginaleﬀectscanbeobservedinFigure4.Figures4aand4bshowthemarginaleﬀectsoftworandomlyselectedfacilities;oneidentiﬁedasbeingassociatedwithlowerratesofcomplianceandoneidentiﬁedashavinghigherratesofcompliance(fromTable3).Notethat,becausethesearebinaryfeatures(takingonvaluesofeitherzeroorone),thekerneldensityoftheunderlyingdataisnotreadilyvisible(unliketheotherﬁgures,whichshowresultsfornon-binaryfeatures).AswecanseethemarginaleﬀectssupporttheresultobtainedusingbothM5RidgeRegressionandRReliefF,andalsoseemtosuggest 12MichaelT.LashMSetal.(a)Facility91.(b)Facility101.(c)flu.(d)humid.(e)temp.Fig.4:Themarginaleﬀectsofseveralselectcovariates,whereblueshowsthekerneldensityoftheoriginaldataandtheredlinesshowtheestimation.Rate(y-axis)vs.feature(x-axis).Notethatin4aand4bnokerneldensityestimateisprovided,astheseplotsareforbinaryfeatures.
 14MichaelT.LashMSetal.highertemperaturesandlevelsofhumidity(particularlytemperature)arestatis-ticallyassociatedwithhigherratesofhandhygiene.However,weﬁndthatsomefacilitiesco-locatedinthesamegeographicregionhaveconﬂictingstatisticalre-sults(e.g.,Facs.91,173).Weconjecturethatsucharesultmayattributabletodiﬀerencesinsensordeploymentlocation,butweleavesuchaninvestigationasfuturework.3.2Facility-SpeciﬁcModelingThefullM5RidgeRegressionmodels’relianceonfacilityidentities,coupledwiththeRReliefFfeaturerankingresult,suggeststhatcompliancedepends,atleastinpart,uponfacility-speciﬁchealthcareworkerattitudes,administrativeculture,orevensimplythedispositionofsensorsandthearchitectureofthefacility.Therefore,weproposetoconstructandanalyzefacility-speciﬁcmodelsinthesamemannerasourglobalmodel.Therefore,inthissection,wepresentacomprehensivesetoffacility-speciﬁcresultsobtainedusingfacility-speciﬁcmodels:weexplore10ofthe19facilitiesdisclosedinTable1,whichcomprehensivelyrepresentalargegeographicdispersion(whichmayproducegeographic-speciﬁcsimilaritiesanddiﬀerencesintheobtainedresults),whichwillfurtherillustratethefacility(andlocation)-speciﬁcfactorsaﬀectinghandhygienecompliance.3.2.1PredictivePower:M5RidgeRegressionThefacility-speciﬁcM5RidgeRegressionmodelingresultsarereportedinTable6.Fac#CorrelationRMSE1550.59070.06581530.20890.09911490.1168.04891230.61930.111270.7133.0313910.53840.09391010.37510.04421700.06450.06071680.3620.0794Table6:Facility-speciﬁcM5RidgeRegressioncross-validationresults.ComparingTable2,showingtheperformanceoftheglobalmodel,withTable6,wecanseethatthereisuniformlylowerRMSEamongthefacility-speciﬁcmodels(Table6)ascomparedtothatoftheglobalmodel.Thisresultisnotunexpected.Ontheotherhand,weobservearangeofcorrelationvalues,someofwhicharebetterthantheglobalmodel(theﬁrsteightfacilitiesinTable6),andsomeofwhichthatareworse.Wenotethatthelasttwofacilities,whichhadworsecorrelationresultsthantheglobalmodel,arealsothefacilitiesthathavecomparablylittledata.
 21MillionOpportunities15Wenowturntoexaminingthetermsofeachfacility-speciﬁchypothesisvector,whichcanbeobservedinTable7.Notethat,forthesakeofsimplicityinanalyzingthesefeatures,wehavecreatedasingle,binaryholidayfeature(asopposedtohavingafeatureforeachholiday,asinourglobalmodel).facility#temphumidweekdayfluholidaynightJuly1470.42370.0594NA−0.937NA−0.0176NA1550.2721NA0.04910.1847NA-0.178NA153NA0.0480.0168-0.0638NA-0.0514-0.0779149NANA0.0184-0.0543NA0.0093NA1230.419NA-0.0572-0.2392NA0.0787NA1270.0672NA0.03150.0383-0.0232-0.0499NA91-0.05460.13290.0683NA-0.1207-0.1012NA101-0.04370.02190.0219NA-0.0234-0.0169-0.0617170NANANA-0.1518NANANA168NA0.1414NA0.207NA-0.0742-0.0729Table7:Hypothesisvectortermsforeachfacility-speciﬁcmodel.InexaminingTable7,wewishtoﬁrstpointoutthat,relativetotheglobalmodelresultreportedinTable3,thatallfacility-speciﬁcmodelshadatleastonetermthatwasremovedviasequentialbackwardselimination.Moreover,theseeliminatedtermsdiﬀerbyfacility,demonstratingthatlocalmodelsaresensitivetodiﬀerentfeaturesindiﬀerentways.Inexaminingthehypothesisterms,someinterestingﬁndingsemerge.Withrespecttoourweather-basedfeatures–temperatureandhumidity–wecanseethat,forthemostpart,thesefactorswerepositivelyassociatedwithhigherratesofhandhygienecomplianceand,forcertainfacilities(147,155,123),thesefeaturesappeartobefairlyimportant(basedonthemagnitudeofthecoeﬃcients).Twofacilities,however,haveanegativeassociationwithtemperatureandcompliance.Thesecoeﬃcients,however,arerelativelysmallandareoﬀsetbypositiveassoci-ationsamonghumidity:inotherwords,theeﬀectsoftemperatureoncomplianceratesatthesefacilitiesappeartobesomewhatnegligible.Inexaminingweekdayandholiday,wecanseethatinallbutonefacility,weekdayhasapositiveinﬂuenceonhandhygienerates.Thissuggeststhatem-ployeesthatworkduringweekendsatthesefacilitiesmaybewashingtheirhandsless;thismaybeattributabletoanumberoffactors(increasedworkload,etc.).Theholidayfeature,ontheotherhand,tendstobeindicativeoflowerratesofcomplianceamongthethreefacilitiesreportinganon-zerotermintheirhypothesisvector(i.e.,facilities91,101,127).ThenightandJulyfeaturesalsotendtobenegativelyassociatedwithhandhygienecompliance,withJulyEffectbeinguniversallyassociatedwithnegativeratesofcompliance(amongthethreefacilitiesforwhichthistermwasnotelimi-nated).night,bycontrast,hadtwofacilitieswhichwerefoundtohaveapositivetermforthisfeature.Thesemaybehospitalswherethereisrelativelylessactiv-ityatnight(lessbusy);however,furtherinvestigationisneededtoteaseoutthereasonsindividualfacilitiesexperiencethesediﬀeringrates.Finally,fluappearstohaveamixofpositiveandnegativeassociationsamongfacilities.Inthosefacilitiesthathavenegativeassociations,acampaignfocusing 16MichaelT.LashMSetal.onﬂuawarenessmaybebeneﬁcial;however,lowerratesmaybeattributabletoincreasedactivityduringpeakﬂuseason,whichmayalsosuggesttheneedforhigherstaﬃnglevels–furtherinvestigationisneededtouncoverthereasonsbehindtheseassociations.3.2.2RReliefFInthissubsectionwediscusstheresultsofRReliefFfeaturerankingobtainedforeachofthe10facilitiesbeinginvestigated;theresultsarepresentedinTable8.Fac#temphumidweekdayfluholidaynightJuly14725.73.815.246.3155124.84.85.23.271532.25.84.514.73.16.71494.35.11.22.166.62.712314.15.434.373.21272.83.56.54.413.36.5913.935.52.115.571013.175.3434.611701.94.43.91.24.36.95.41681.53.82.91.56.36.61.8Table8:Facility-speciﬁcRReliefFfeaturerankings.Theﬁrstobservationwewishtomakeisthatthereisnosinglefeaturethatcompletelydominatesthefeaturerankingsamongthediﬀerentfacilities.Thissug-geststhatfacilities’complianceratesareaﬀecteddiﬀerentlybyourselectedfea-tures.However,wecanalsothatsomefeaturesareoftenrankedasbeingmoreimportant,whileothersaslessimportant.Forinstance,tempisfrequentlyoneofthetopthreefeatures,whileJulymoreoftenappearstowardthebottomoftheranking.Itisimportanttonotehere,however,thatwhileJuly,weekday,night,andholidayappeartowardtheendofthefeaturerankingforsomefacilities,theyappeartowardsthetopforothers.Theflufeaturealsofrequentlyappearsinthetopthreefeaturerankingsamongfacilities,whilehumidoftenappearssomewherenearthemiddleoftherankings.3.2.3MarginalEﬀectsThefacility-speciﬁcmarginaleﬀectsmodelingresultsarepresentedinFigure5.Notethatwearereportingonlyasubsetofresults,whichincludetemp,humid,weekday,andflu.Cumulatively,theseresultsfurthersupportwhatwehavealreadydiscussed,withafewobservationalcaveats.First,temperatureisfoundtobeuniversallyindicativeofhigherratesofcompliance,whichwasfoundtonotbeentirelytrueforfacilities91and101;thesecoeﬃcientsarelikelyobscuredbysomedegreeofmulticollinearitywithotherfeatures–thesameistrueofhumid.weekdayandflu,asintheotherresults,arefoundtobemostlyindicativeofhigherratesofcompliance,withtheexceptionofafewfacilities.
 21MillionOpportunities17Facilitytemptempweekdayflu14715515314912312791101170168Fig.5:Facility-speciﬁcmarginaleﬀectsmodelingresults.
 18MichaelT.LashMSetal.4DiscussionandFutureWorkInthissectionwediscussthebroaderimplicationsofourﬁndings,aswellasdirectionsforfuturework.Theglobalresults,includingthefullM5RidgeRegressionmodel,marginaleﬀectsmodels,andRReliefFfeatureranking,provideseveralinsights.First,wefoundthatfacilityidentitiesarestronglyrelatedtocompliance,suggestingthatfacility-wideattitudestowardshandhygieneexist,persistintime,andarepre-dictiveofcompliancerates.Ontheotherhand,thisobservationmayalsoreﬂectdiﬀerencesinsensorinstallation,wherediﬀerentfacilitiesmayhavesensorsinstru-mentedindiﬀerentdepartments,thusaﬀectingreportedrates.Second,increasesininﬂuenzaseveritywereassociatedwithanincreaseincompliance,whichisen-couragingbecauseitimpliesthathealthcareworkersarerespondingpositively(i.e,morehandhygiene)toanincreasedpresenceofinfectiousdisease.Thisartifactalsosurfacedinourfacility-speciﬁcmodels,whichalsorevealedthatdiﬀerentfa-cilitieshavediﬀerentmagnitudesintheeﬀectofﬂuseverityonhandhygiene.Third,ourconjectureregardinglowerweekendandholidaycomplianceappearstohavesomemerit,althoughthespeciﬁcholidaysassociatedwithnegativecom-plianceweresomewhatsurprising.Weagainacknowledgethatthisresultmaybeaﬀectedbyincreasedvisitorsduringthesetimes,dilutingtheperceivedcompliancerate.Furthermore,ourfacility-speciﬁcmodelingshowedthat,forsomehospitals,bothweekdayandholidayhadalargebearingonhandhygienecompliancepre-dictions(i.e.,thesefactorswereimportantpredictorsofcompliance).Fourth,ourconjecturesthathigherhumidityandtemperatureareindicativeofhigherratesofcompliancewereconﬁrmedbythefullmodel,marginaleﬀectsmodel,andsta-tisticalanalysis.Thisﬁndingisimportantashealthcareworkersoftenciteskinirritationordryskinasreasonsforreducedfrequencyofhandhygiene.Thesesamefactorswerealsostronglysuggestedbyourfacility-speciﬁcmodeling.Fifth,wefoundthatcomplianceduringtheﬁrstweekofresidents’attendancerancon-trarytoouroriginalconjecture:theJulywasessentiallyunobservable.Howeverwedidﬁndthatselectfacilities(153,101,and168)hadthisasaninﬂuencingfac-tor(particularly101and168).Finally,wefoundthatnightwasassociatedwithslightlylowercompliancerates.However,asourfacility-speciﬁcmodelingexposed,somefacilities(149,123)appeartohaveslightlyhigherratesofcomplianceduringtheevening;although,itisworthnotingthat,forthesefacilities,nightwasatthebottomoftheRReﬂiefFfeatureranking(indicatingrelativelylowimportance).Diﬀerentfacilitieshavediﬀerentfactorsthataﬀectcomplianceratesdiﬀerently:notwofacilitiesarealike.Whilemanyofthefacilitieshavefactorsthatinﬂuencecomplianceratesinsimilarways–positiveornegative(e.g.,temperature)–theydiﬀerindegree(howmuchthesecommonfactorsinﬂuencecompliance)andcom-position(thespeciﬁcsetofnon-zerotermsinthehypothesisvectorh∗).Cumula-tively,wecanseethatfactorsaﬀectinghandhygienecomplianceamongfacilitiesisacomplicatedtopicrequiringfurtherinvestigation.Thisworkhasseverallimitations.First,therearediﬀerencesamonginstalla-tions:notalldoorsanddispensersmaybeinstrumentedand,therefore,wecannottrack,forexample,theuseofpersonalalcoholdispensers(wecanonlyassumestablepracticeswithinfacilities).Thusourcomplianceestimatesmaybebasedonpartialinformationandarecertainlynotcomparableacrossfacilities.Second,ourcomplianceestimatesarefacilitywide,meaningthatwedonotexploittheco- 21MillionOpportunities19locationofdispensersanddooreventsensors,butonlythetemporalcorrelationoftheindividualevents.Thus,ourassumptionthateachdooreventcorrespondstoahand-hygieneopportunitymaybefundamentallyﬂawed,evenasitallowsforconsistentintra-facilitycomparisons.Third,weacknowledgethepossibilityoflocationandsamplingbiaswithregardtoboththesensorsandfacilities.IfsensorsweretobeplacedinonlytheICUofonefacilityandintheemergencyroomofanother,wemayobservediﬀerentrates,whichmaybeentirelyreasonableandexpectedinclinicalpractice.Additionally,thoughfacilitiesaredistributedacrosstheUnitedStates,theyarebynomeansmeanttobearepresentativesampleoffacilitytypesorclimaticconditions.Inourfutureendeavorswewouldﬁrstliketoconsideralternativedeﬁnitionsofcomplianceandexaminecomplianceatﬁner-grainedtemporallevels,perhapsex-ploringtime-seriesanalyses.Weintendtoalsoexploreframingtheproblemasoneofclassiﬁcation,ratherthanonlyregression,whichmayhelpteaseoutadditionalartifacts.Finally,datapertainingtocomplianceratesundercertaininterventionswouldgivewaytoexplorationofinterventioneﬃcacybothingeneralandus-ingprediction-basedmethodology,suchasinverseclassiﬁcation,torecommendfacility-speciﬁcinterventionpolicies[29,30].Handhygienecomplianceisasimpleyeteﬀectivemethodofpreventingthetransmissionofdisease,bothamongthepopulationatlarge,andwithinhealthcarefacilities,yettherehavebeenfewattemptstostudythefactorsthatcanaﬀectcompliance.Thisstudypresentsaﬁrstlookatfactorsthatunderliehealthcareworkerhand-hygienecompliancerates,includingweatherconditions,holidaysandweekends,andinfectiousdiseaseprevalenceandseverity,andservesasamodelforfuturestudiesthatwillexploittheavailabilityoftemporallyandspatiallyrichcompliancedatacollectedbythesophisticatedsensorsystemsnowbeingputintopractice.5ConﬂictsofInterestPhilipM.PolgreenhasreceivedresearchfundingfromCompanyGOJOIndustries,Inc.AuthorJasonSlaterisanemployeeofGOJOIndustries,Inc.References1.R.Klevens,J.Edwards,C.Richards,andT.Horan,“Estimatinghealthcare-associatedinfectionsanddeathsinushospitals,”PublicHealth,no.122,pp.160–166,2007.2.R.Roberts,R.Scott,B.Hota,L.Kampe,F.Abbasi,S.Schabowski,I.Ahmad,G.Ciavarella,R.Cordell,S.Solomon,R.Hagtvedt,andR.Weinstein,“Costsattributabletohealthcare-acquiredinfectioninhospitalizedadultsandacomparisonofeconomicmeth-ods,”MedicalCare,vol.48,no.11,pp.1026–1035,November2010.3.R.Roberts,B.Hota,I.Ahmad,R.Scott,S.Foster,F.Abbasi,S.Schabowski,L.Kampe,G.Ciavarella,M.Supino,J.Naples,R.Cordell,S.Levy,andR.Weinstein,“Hospitalandsocietalcostsofantimicrobial-resistantinfectioninachiagoteachinghospital:implicationsforantibioticstewardship,”ClinicalInfectiousDiseases,vol.49,no.8,pp.1175–1184,October2009.4.J.M.BoyceandD.Pittet,“Guidelinesforhandhygieneinhealth-caresettings:rec-ommendationsofthehealthcareinfectioncontrolpracticesadvisorycommitteeandthehicpac/shea/apic/idsahandhygienetaskforce,”InfectionControlandHospitalEpidemi-ology,no.23,pp.S3–S41,2002.
 20MichaelT.LashMSetal.5.B.Allegranzi,H.Sax,L.Bengaly,H.Richet,D.Minta,M.Chraiti,F.Sokona,A.Gayet-Ageron,P.Bonnabry,andD.Pittet,“Worldhealthorganization”pointg”projectmanage-mentcommittee.successfulimplementationoftheworldhealthorganizationhandhygieneimprovementstrategyinareferralhospitalinmali,africa,”InfectionControlandHospitalEpidemiology,vol.31,no.2,pp.133–141,February2010.6.D.Pittet,B.Allegranzi,andJ.Boyce,“Worldhealthorganizationworldallianceforpa-tientsafetyﬁrstglobalpatientsafetychallengecoregroupofexperts.theworldhealthorganizationguidelinesonhandhygieneinhealthcareandtheirconsensusrecommen-dations,”InfectionControlandHospitalEpidemiology,vol.30,no.7,pp.611–622,July2009.7.J.P.HassandL.E.L.,“Measurementofcompliancewithhandhygiene,”JournalofHospitalInfection,no.66,pp.6–14,2007.8.J.BoyceandM.Cooper,TandaDolan,“Evaluationofanelectronicdeviceforreal-timemeasurementofalcohol-basedhandrubuse,”InfectionControlandHospitalEpidemiol-ogy,vol.30,no.11,pp.1090–1095,November2009.9.JointCommissionofAccreditationofHealthcareOrganizations,“Patientsafetygoals,”JointCommissionofAccreditationofHealthcareOrganizations,Tech.Rep.,2017.[Online].Available:http://www.jcaho.org/accredited+organizations/patient+safety/npsg.htm10.J.Fries,A.Segre,G.Thomas,T.Herman,K.Ellingson,andP.Polgreen,“Monitoringhandhygieneviahumanobservers:Howshouldwebesampling?”InfectionControlandHospitalEpidemiology,vol.33,no.7,pp.689–695,Jul.2012,[PMID:22669230].11.D.Sharma,G.Thomas,E.Foster,J.Iacovelli,K.Lea,J.Streit,andP.Polgreen,“Theprecisionofhuman-generatedhand-hygieneobservations:acomparisonofhumanobserva-tionwithanautomatedmonitoringsystem,”InfectionControlandHospitalEpidemiology,vol.33,no.12,pp.1259–1261,December2012.12.T.Eckmanns,J.Bessert,M.Behnke,andH.Gastmeier,PandaRuden,“Compliancewithantiseptichandrubuseinintensivecareunits:Thehawthorneeﬀect,”InfectionControlandHospitalEpidemiology,no.27,pp.931–934,2006.13.M.Monsalve,S.Pemmaraju,G.Thomas,T.Herman,andP.Segre,AMandaPolgreen,“Dopeereﬀectsimprovehandhygieneadherenceamonghealthcareworkers?”InfectionControlandHospitalEpidemiology,vol.35,no.10,pp.1277–1285,October2014.14.V.Boscart,K.McGilton,A.Levchenko,G.Hufton,P.Holliday,andG.Fernie,“Ac-ceptabilityofawearablehandhygienedevicewithmonitoringcapabilities,”JournalofHospitalInfection,vol.70,no.3,pp.216–222,November2008.15.A.Venkatesh,M.Lankford,D.Rooney,T.Blachford,C.Watts,andG.Noskin,“Useofelectronicalertstoenhancehandhygienecomplianceanddecreasetransmissionofvancomycin-resistantenterococcusinahematologyunit,”AmericanJournalofInfectionControl,vol.36,no.3,pp.199–205,April2008.16.P.M.Polgreen,C.S.Hlady,M.a.Severson,A.M.Segre,andT.Herman,“Methodforautomatedmonitoringofhandhygieneadherencewithoutradio-frequencyidentiﬁcation.”Infectioncontrolandhospitalepidemiology:theoﬃcialjournaloftheSocietyofHospitalEpidemiologistsofAmerica,vol.31,no.12,pp.1294–1297,2010.17.M.T.Lash,J.Slater,P.M.Polgreen,andA.M.Segre,“Alarge-scaleexplorationoffactorsaﬀectinghandhygienecomplianceusinglinearpredictivemodels,”inHealthcareInformatics,2017IEEEInternationalConferenceon(ICHI),2017,pp.66–73.[Online].Available:http://ieeexplore.ieee.org/document/8031133/18.H.Dai,K.L.Milkman,D.A.Hofmann,andB.R.Staats,“TheImpactofTimeatWorkandTimeOﬀfromWorkonRuleCompliance:TheCaseofHandHygieneinHealthcare,”JournalofAppliedPsychology,vol.100,no.3,pp.846–862,2014.[Online].Available:http://papers.ssrn.com/sol3/papers.cfm?abstractid=242300919.C.JarrinTejadaandG.Bearman,“HandHygieneComplianceMonitoring:theStateoftheArt,”CurrentInfectiousDiseaseReports,vol.17,no.4,2015.[Online].Available:http://link.springer.com/10.1007/s11908-015-0470-020.E.Kalnay,M.Kanamitsu,R.Kistler,W.Collins,D.Deaven,L.Gandin,S.Iredell,S.Saha,G.White,Y.Zhu,a.Leetmaa,R.Reynolds,M.Chelliah,W.Ebisuzaki,W.Higgins,J.Janowiak,K.Mo,C.Ropelewski,J.Wang,R.Jenne,andD.Joseph,“TheNCEP/NCAR40-YearReanalysisProject,”pp.437–471,1996.[Online].Available:21.N.R.Draper,H.Smith,andE.Pownell,AppliedRegressionAnalysis.WileyNewYork,1966,vol.3.22.J.R.Quinlan,“Learningwithcontinuousclasses,”in5thAustralianJointConferenceonArtiﬁcialIntelligence,vol.92,1992,pp.343–348.
 21MillionOpportunities2123.F.D.Johansson,U.Shalit,andD.Sontag,“Learningrepresentationsforcounterfactualinference,”in33rdInternationalConferenceonMachineLearning(ICML),2016.24.R.Tibshirani,“Regressionshrinkageandselectionviathelasso,”JournaloftheRoyalStatisticalSociety.SeriesB(Methodological),pp.267–288,1996.25.H.ZouandT.Hastie,“Regularizationandvariableselectionviatheelasticnet,”JournaloftheRoyalStatisticalSociety:SeriesB(StatisticalMethodology),vol.67,no.2,pp.301–320,2005.26.M.Robnik-ˇSikonjaandI.Kononenko,“Anadaptationofreliefforattributeestimationinregression,”inMachineLearning:ProceedingsoftheFourteenthInternationalConference(ICML97),1997,pp.296–304.27.K.KiraandL.A.Rendell,“Apracticalapproachtofeatureselection,”inProceedingsoftheninthinternationalworkshoponMachinelearning,1992,pp.249–256.28.R.Williamsetal.,“Usingthemarginscommandtoestimateandinterpretadjustedpre-dictionsandmarginaleﬀects,”TheStataJournal,vol.12,no.2,p.308,2012.29.M.T.Lash,Q.Lin,W.N.Street,J.G.Robinson,andJ.Ohlmann,“Gen-eralizedinverseclassiﬁcation,”inProceedingsofthe2017SIAMInternationalConferenceonDataMining(SDM’17),2017,pp.162–170.[Online].Available:https://doi.org/10.1137/1.9781611974973.1930.M.T.Lash,Q.Lin,W.N.Street,andJ.Robinson,“Abudgetconstrainedinverseclas-siﬁcationframeworkforsmoothclassiﬁers,”inDataMiningWorkshops(ICDMW),2017IEEEInternationalConferenceon,2017,pp.1184–1193.
 Cluster analysis is essential many applications com- puter vision pattern recognition.
 Given fact recent advent deep learning, is increasing interest learning deep unsupervised representations clustering analysis [1], [2], [3], [4].
 Most methods perform clustering deep representations, make use auto-encoder representations encoder part) deﬁne clus- tering losses them.
 focus previous works have been choice auto-encoder type architecture clustering loss.
 DEC [1], ﬁrst dense auto-encoder is trained minimizing reconstruction error.
 Then, clustering optimization stage, method iterates computing auxiliary target distribution auto-encoder representations minimizing Kullback-Leibler diver- gence it.
 IDEC [2], is argued clustering loss DEC corrupts feature space, IDEC proposes optimize clustering loss reconstruction loss auto-encoder.
 DCEC [4] argues inefﬁciency using dense auto-encoders image clustering, adopts convolutional auto-encoder shows improves clustering accuracy DEC IDEC.
 GMVAE [3] adopts variational auto-encoder order learn unsupervised representations applies K-means clustering representations.
 manuscript, show regardless auto- encoder type (dense convolutional), constraining auto- encoder representations be unit-ball, i.e. be l2 normalized, auto-encoder training, improves clustering accuracy.
 show simple k-means clustering auto-encoder representations trained constraint gives improved accuracy large margin compared baselines additional clustering losses.
 Motivated high performance clustering method deep representations, propose unsupervised anomaly detection method based clustering.
 show anomaly detection method improves other deep anomaly detection strategies such reconstruction error based ones.
 investigate effect l2 normalization constraint training anomaly detection accuracy show leads superior results compared applying constraint.
 II.
 RELATED WORK A.
 Deep Unsupervised Anomaly Detection Unsupervised anomaly detection tries ﬁnd anomalies data using annotation [7].
 Recently, deep learning methods have been used task [5], [6].
 works train auto-encoders entire data use reconstruction loss indicator anomaly.
 DRAE [5] trains auto-encoders uses reconstruction error anomaly indicator.
 Moreover, DRAE proposes method make reconstruction error distributions normal abnormal classes separable is easier detect anomalies.
 AVAE [6] trains conventional variational auto-encoders use reconstruction error anomaly indicator.
 general assumption above works is anomaly data is smaller ratio normal data, auto-encoder learn reconstruct
 above assumption seems work speciﬁc deﬁnition anomaly normal samples are drawn single class anomaly classes have been selected many other classes [5].
 However, assumption fails anomaly type normal samples are drawn multiple classes anomaly class is sampled speciﬁc class [6].
 paper propose unsupervised anomaly detection method based clustering deep auto-encoder represen- tations show gives superior performance reconstruction error based anomaly.
 (a) normalization (b) l2 normalization training (c) l2 normalization training Fig.
 Illustration t-SNE encoding auto-encoder representations MNIST dataset dimensions.
 Best viewed color.
 B.
 Regularization Normalization Neural Networks III.
 PROPOSED METHOD Due high number parameters, neural networks have risk over-ﬁtting training data.
 sometimes reduces generalization ability learned network.
 order deal over-ﬁtting, regularization methods are employed.
 used regularization technique is weight norm regularization.
 aim is add additional regularization loss neural network error, gives high penalty weights have high norms.
 l1 l2 norm be exploited.
 normalization techniques neural networks emerged such [8], [9].
 Batch normalization [8], aims ﬁnd statistical mean variance activations are calculated updated according batch statistics.
 activations are normalized according statistics.
 layer normalization [9], mean variance are computed summed inputs neurons layer single training sample.
 overcomes batch-size dependency drawback batch-normalization.
 methods were proposed tricks make neural network training faster conditioning layer’s input, is argued have regularization effect due varying estimations parameters standardization epoch.
 proposed unit ball constraint put activations is normalization technique.
 However, layer batch unit ball constraint is parameter-free sets norm activation vector
 Therefore, is free parameter estimation stochasticity.
 act regularization method due hard constraint activations be ﬁxed norm.
 resembles l2 norm regularization.
 key difference is l2 norm regularization, norm is weights, case is applied activations.
 key difference is ﬁx activation norms whereas l2 norm regularization penalizes large weight norms does ﬁx norms value.
 A.
 Clustering l2 Normalized Deep Auto-Encoder Repre- sentations represent auto-encoder representations input E(I) reconstructed input D(E(I)).
 representations are obtained several dense convolutional layers applied input I.
 layer, are ﬁltering activation operations, pooling operations.
 Let be computations applied input layer i, encoded representations n-layer encoder are obtained Eq.
 E(I) = fn(fn−1(...f1(I))) (1) reconstruction part auto-encoder applies E(I) is obtained several dense deconvolutional layers.
 layer, are ﬁltering activation operations un-pooling up-sampling operations.
 Let gi be computations applied auto-encoder representa- tions, reconstructed signal m-layer decoder is obtained Eq.
 D(E(I)) = gm(gm−1(...g1(E(I)))) (2) auto-encoder training is conducted order reduce reconstruction error (loss) given Eq.
 (cid:88) j∈J L = |J| (Ij − D(E(Ij)))2 (3) Here, propose additional step conducted auto-encoder representations E(I).
 particular, apply l2 normalization E(I).
 corresponds adding hard constraint representations be unit ball.
 loss function introduced constraint be written Eq. Lc Ec are loss encoded representations introduced constraint.
 (a) Clustering Method (b) Normality Score Method Fig.
 Illustration proposed methods inference phase.
 (cid:88) j∈J Lc = |J| (Ij − D(Ec(Ij)))2, Ec(I) = E(I) (cid:107)E(I)(cid:107)2 (4) believe l2 normalized features are suitable clustering purposes, methods use Euclidean distance such conventional k-means.
 is distances vectors be independent length, depend angle vectors.
 positive side effect, enforcing unit norm representations act regularization entire auto-encoder.
 Fig.
 illustrate t-SNE [10] encoding (to dimen- sions) auto-encoder representations networks same architecture.
 auto-encoders were trained MNIST [11] dataset training split.
 representations corresponding Fig.
 are auto-encoder was trained loss Eq.
 same representations l2 normalization applied training are illustrated Fig.

 representations l2 constraint training training loss Eq. are illustrated Fig.

 is observed ﬁgures l2 normalization training (Fig.
 results separable clusters.
 example is distributions digit MNIST dataset.
 Note numbers are indicated color codes color bar is available Fig.

 is observed normalization training (Fig.
 digit is divided parts small part is surrounded digits.
 normalization applied training (Fig.
 effect becomes evident.
 So, observe applying normalization training does help all.
 But, l2 normalization constraint training (Fig.
 see clear separation digit single cluster rest numbers.
 be observed clusters are more compact Fig.
 compared others.
 training auto-encoder loss function Eq. clustering is performed k-means algorithm.
 clustering loss is applied.
 clustering method is illustrated Fig.

 B.
 Unsupervised Anomaly Detection using l2 Normalized Deep Auto-Encoder Representations Here, propose clustering based unsupervised anomaly detection.
 train auto-encoder entire dataset including normal abnormal samples annotation supervision is used.
 auto-encoder is trained loss Eq.
 l2 normalized auto- encoder representations are clustered k-means algorithm.
 assume anomaly cases be smaller number normal clusters.
 Note assumption does put constraint dataset, follows general deﬁnition anomaly.
 centroids obtained k-means method be considered be representations normal clusters errors are caused anomaly samples dataset.
 sample assign normality score vi Eq.
 (Ec(Ii) · Cj(cid:107)Cj(cid:107)2 (5) Eq. Cj is cluster centroid · is dot product oper- ator.
 Notice l2 normalize cluster centroids.
 representations Ec(Ii) are l2 normalized, vi ∈ holds.
 measure Eq. is intuitive considering expect high similarities normal samples normal classes.
 normality scoring method is illustrated Fig.

 vi = max normality score be used anomaly detection straightforward manner.
 Simply, abnormal samples be detected ones having vi < τ τ ∈ [0, is threshold.
 IV.
 EXPERIMENTAL RESULTS A.
 Clustering Evaluation Metrics: use used evaluation unsupervised clustering accuracy [1] given Eq.
 (cid:80)n i=1 = m(ci)} acc = max (6) Eq. li is ground truth labeling sample i, ci is cluster assignment according one-to-one mapping deﬁned m, m ranges possible one-to-one mappings clusters generated algorithm ground truth labels.
 maximization Eq. be performed Hungarian algorithm [12].
 compare clustering accuracy auto-encoder rep- resentations l2 normalization constraint.
 make comparison dense convolutional auto- encoders.
 dense auto-encoder, use MNIST [11] convolutional auto-encoders, use MNIST [11] USPS [15] datasets.
 is due availability results works use dense convolutional auto-encoders.
 dense auto-encoder, use network structure is used DEC [1] IDEC [2].
 encoding part are layers − − hidden neurons decoding part are − − d neurons, d is dimension input.
 re-implement auto-encoder training leaky relu [13] activations hidden layer last trained auto- encoder end end epochs.
 select best model lowest reconstruction error.
 be observed Table I, obtain similar clustering accuracy apply k-means auto-encoder representations compared original paper DEC [1].
 Note results indicated * corresponds own implementation.
 Other results baselines are borrowed original papers.
 Table shows train auto-encoder l2 normalization constraint representations, achieve clustering accuracy apply k-means representations.
 denote method AE-l2 stands auto-encoder l2 normalization.
 Moreover, clustering accuracy is better methods deﬁne designed clustering loss representations (DEC IDEC).
 Next, make experiments convolutional auto- encoder.
 this, make use model structure in- troduced DCEC [4].
 model consists convolutional ﬁlters encoding layers respectively.
 are ﬁlters encoding layers respectively.
 Convolutions are applied strides relu [14] activations.
 convolutional layers, activations are ﬂattened is dense layer dimension
 is followed dense layer reshaping.
 Decoder part consists deconvolutional ﬁlters size respectively.
 Relu activations were applied convolution, last one.
 network was trained epochs original paper DCEC [4].
 Table II, show clustering accuracy k-means applied convolutional autoencoder representations.
 were able obtain similar results original paper (DCEC).
 Note results indicated * corresponds own implementation.
 Other results baselines are borrowed original papers.
 be observed Table II train convolutional autoencoder l2 normaliza- tion constraint representations, achieve better performance.
 denote method CAE-l2 stands convolutional auto-encoder l2 normalization.
 performance is superior DCEC introduces additional clustering loss.
 TABLE I: Clustering Dense Auto-Encoder Representations AE* k-means AE k-means DEC IDEC AE-l2 k-means MNIST TABLE II: Clustering Convolutional Auto-Encoder Repre- sentations CAE* k-means MNIST USPS CAE k-means DCEC CAE-l2 k-means TABLE III: Comparison Normalization Methods MNIST USPS batch-norm layer-norm l2-norm l2 versus Batch Layer Normalization: Due l2 nor- malization step clustering method, compare applying other normalization techniques training.
 particular train separate networks using batch [8] layer [9] l2 normalization.
 other setup experiments are same.
 Batch size is used methods order have large enough batch batch normalization.
 method performs superior baselines large margin, accuracies Table III indicate.
 is noticed batch layer normalization provides noticeable accuracy increase baseline (CAE+k-means).
 MNIST dataset, layer batch normalization results signiﬁcant accuracy decrease.
 is important performance upgrade method is result input conditioning, is result speciﬁc normalization type is more ﬁt clustering Euclidean space.
 indicator showing B.
 Anomaly Detection Evaluation Metrics: anomaly detection method generates anomaly score, hard classiﬁcation result.
 Therefore, common evaluation strategy anomaly detec- tion is threshold anomaly score form receiver operating curve point is true positive false positive rate anomaly detection result corresponding threshold.
 area curve (AUC) RoC curve is used evaluation anomaly detection method [7].
 Here, evaluate method introduced Section III-B.
 evaluation setup implementation method are follows.
 MNIST training dataset, select digit class anomaly class keep random class dataset remaining is ignored.
 leave rest classes is.
 Then, use convolutional autoencoder structure DCEC [4] train l2 normalization constraint representations.
 apply k-means clustering representations keep centroids.
 experiments use k=9 k- means, assume know number normal classes data.
 MNIST test dataset, calculate auto-encoder representations.
 normality measure sample, calculate corresponding representation’s maximum similarity pre-calculated cluster centroids
 be noted repeat above procedure choosing different class be anomaly class, possible classes.
 evaluate baselines.
 ﬁrst baseline, ex- repeat above procedure, l2 normalization constraint representations.
 second baseline, train auto-encoder l2 normalization constraint representations.
 Then, test set, calculate reconstruction error sample deﬁne anomaly score.
 Using reconstruction error based anomaly detection follows works AVAE [6] DRAE [5].
 setups AVAE DRAE are different ours.
 AVAE, training is conducted normal data, method is unsupervised sense.
 DRAE, anomaly deﬁnition is different: single class is kept normal samples other classes are treated anomaly.
 setup presents easier case therefore reconstruction error based anomaly detection produces acceptable results.
 show setup is case.
 method plot RoC curve thresholding normality (or anomaly) score multiple thresholds.
 Then, evaluate area RoC curve (AUC) measuring anomaly detection performance.
 training test datasets methods are same.
 Due random selection anomaly class be kept, performance change according partition is selected.
 Therefore, run method times different random partitions report mean AUC.
 be observed Table IV clustering based anomaly detection method outperforms reconstruction error based anomaly detection CAE neural network structure.
 is worth noting interesting observation Table IV: digits reconstruction error based anomaly detection gets inferior performance.
 is evident digit
 reason is digit is easy reconstruct stroke) auto- encoder is trained much less examples digit, reconstruct well.
 shows clear drawback reconstruction error based anomaly detection.
 However, clustering based method, achieve high accuracy digits.
 effect proposed l2 normalization constraint representations training be observed Table IV.
 cases, i.e. digits selected anomaly, anomaly detection network trained l2 normal- ization constraint representations performs better without.
 digit observe inferior accuracy method.
 Compared other digits, observe less performance digits
 argue be happening due similar appearance digits handwritings.
 method confuse TABLE IV: Anomaly Detection Auto-Encoder Represen- tations Anom.
 Digit CAE.
 CAE (cluster) CAE-l2 (cluster) TABLE V: Anomaly Detection Auto-Encoder Represen- tations Anom.
 (Digit) AE.
 VAE.
 CAE-l2 cluster numbers other clustering.
 Table V, compare method method [6] performs reconstruction error based anomaly detection, using dense auto-encoders.
 is variational auto-encoder based version method.
 be noted method trains auto-encoders normal data.
 presents easier task compared case include anomalous samples training.
 case is unsupervised.
 Still, cases, method outperforms variants method large margin.
 digit observe inferior performance method compared VAE method.
 V.
 CONCLUSION paper, have applied l2 normalization constraint deep autoencoder representations autoencoder train- ing observed representations obtained way clusters Euclidean space.
 Therefore, applying simple k-means clustering representations gives high clustering accuracies works methods deﬁning additional clustering losses.
 have shown high performance is due conditioning applied representations is due selection particular nor- malization leads separable clusters Euclidean space.
 have proposed unsupervised anomaly detection method l2 normalized deep auto-encoder representations.
 have shown proposed l2 normalization constraint increases anomaly detection method’s perfor- mance.
 have shown adopted deep anomaly detection method based reconstruction error performs weak deﬁnition anomaly, method performs superior.
 REFERENCES [1] J.
 Xie, R.
 Girshick A.
 Farhadi, Unsupervised deep embedding clustering analysis, International Conference Machine Learning, pp.
 478-487, June,
 [2] X.
 Guo, L.
 Gao, X.
 Liu J.
 Yin, Improved embedded clustering local structure preservation, International Joint Conference Artiﬁcial Intelligence, pp.
 1753-1759, June,
 [3] N.
 Dilokthanakul, P.
 A.
 Mediano, M.
 Garnelo, M- C- Lee, H.
 Sal- imbeni, K- Arulkumaran M.
 Shanahan, Deep unsupervised clus- tering gaussian mixture variational autoencoders, arXiv preprint arXiv:1611.02648,
 [4] X.
 Huo, X.
 Liu, E.
 Zheand J.
 Yin, Deep Clustering Convolutional Autoencoders, International Conference Neural Information Process- ing, pp.
 373-382,
 [5] Y.
 Xia, X.
 Cao, F.
 Wen, G.
 Hua J.
 Sun, Learning discriminative reconstructions unsupervised outlier removal, Proceedings IEEE International Conference Computer Vision, pp.
 1511-1519,
 [6] J.
 S.
 Cho, Variational autoencoder based anomaly detection using reconstruction probability, SNU Data Mining Center, Tech.

 [7] M.
 Goldstein S.
 A comparative evaluation unsupervised anomaly detection algorithms multivariate data, PloS one, no.

 [8] S.
 Ioffe C.
 Szegedy, Batch Accelerating deep network training reducing internal covariate shift, International conference machine learning, pp.
 448-456,
 [9] J.
 L.
 Ba, J.
 R.
 Kiros G.
 E.
 Hinton, Layer arXiv preprint arXiv:1607.06450,
 [10] L.
 V.
 D.
 Maaten G.
 Hinton, Visualizing data using t-SNE, Journal machine learning research, pp.
 2579-2605,
 [11] Y.
 LeCun, L.
 Bottou, Y.
 Bengio P.
 Haffner, Gradient-based learning applied document recognition, Proceedings IEEE, vol.
 no.
 pp.
 2278-2324,
 [12] H.
 W.
 Kuhn, Hungarian method assignment problem, Naval Research Logistics, pp.

 [13] V.
 Nair G.
 E.
 Hinton, Empirical evaluation rectiﬁed activations convolutional network, arXiv preprint arXiv:1505.00853,
 [14] B.
 Xu, N.
 Wang, T.
 Chen M.
 Li, Rectiﬁed linear units improve restricted boltzmann machines, International Conference Machine Learning, pp.

 [15] Y.
 LeCun, O.
 Matan, B.
 Boser, J.
 D.
 Denker, D.
 Henderson, R.
 E.
 Howard, W.
 Hubbard, L.
 D.
 Jacket H.
 S.
 Baird, Handwritten zip code recognition multilayer networks.
 International Conference Pattern Recognition, vol.
 pp.
 35-40,
 last few decades, many initialization, optimiza- tion, regularization, many other techniques have been invented (Bengio et al., Hinton make deep neural networks (DNNs) applicable solv- ing challenging artiﬁcial intelligence tasks (Lecun
 Nevertheless, classical DNNs VGG networks (Simonyan Zisserman, have problem degra- dation, i.e., network goes deeper training testing errors increase sufﬁcient training data (He et
 Deep residual networks (ResNets), pre-activated ones et proposed et al.
 employ shortcut connections learn residuals keep clean information path solve aforementioned degradation problem.
 Furthermore, deep Mathematics, UCLA, Los Angeles, California, USA Mathematical Sciences, Yau Mathematical Sciences Center, Tsinghua University, Beijing, China Mathematics, Duke University, Durham, North Carolina, USA.
 Correspondence to: Bao Wang <wangbaonj@gmail.com>.
 ResNets enable generalization accuracy improvement networks layers.
 Many advances have been made emergence deep ResNets.
 include theoretical analysis algorithmic development.
 original work et al (He pre-activated ResNets are formulated discrete dynamical systems.
 dynamical system point view leads elegant analysis optimality identity map ResNets.
 Hardt Ma (Hardt Ma, use matrix factorization techniques analyze landscape linear ResNets representation power.
 E considers deep ResNets control problem class continuous dynamical systems (E,
 New network structures development is thriving theoretical analysis.
 using single shortcut connect consecutive residual blocks, connect convolutional networks employ shortcut connections connect distinct blocks (Huang
 wide residual networks (Zagoruyko Komodakis, increase width layers original ResNets.
 dense nets wider ResNets have certain amount improvement compared ResNets.
 Dual path networks is family interesting improvement ResNets dense nets (Chen et al.,
 accuracy DNN depends massive amounts training data.
 lack sufﬁcient training data leads degradation problem.
 Sig- niﬁcant accuracy reduction tends occur network goes deeper, be demonstrated pa- per.
 Many regularization techniques explored attempt tackle challenge(Zhu et Srivastava Wen al., satisfactory results are rare.
 Most existing strategies be classiﬁed loss func- tion regularization network structure regularization.
 None considered speciﬁcity data is critical importance data
 paper, try solve issue lacking enough training data using information data prior train DNNs. ﬁrst build connection be- deep ResNets partial differential equation (PDE) control problems.
 Well-posedness theories PDE control problems suggest take data dependent acti- vations DNNs. make data dependent activation Deep Learning Data Dependent Implicit Activation Function control problems, let consider terminal value problem linear transport equation Rd: (cid:40) ∂u ∂t + v(x, t) · ∇u = x ∈ Rd, t u(x, = f (x) x ∈ Rd, (2) v(x, t) is given velocity ﬁeld, d is dimension ﬂattened input tensor, f is composition activation function connected layer.
 use softmax activation function, f (x) = softmax(WF C · x), (3) WF C is weight connected layer, softmax function is given softmax(x)i = (cid:80) exp(xi) j exp(xj) models posterior probability instance be- longing class.
 is well-known (Evans, solution t be solved characteristics: dX(t; x) dt = v (X(t; x), t) X(0; x) = x.
 know characteristics, u is constant (Set T = below): u(x, = u (X(1; x), T = f (X(1; x)).
 Let {tk}L k=0 t0 tL be partition
 characteristics transport equation Eq.(4) be solved using simple forward Euler discretization X0(x) = x: Xk+1(x) = Xk(x) + ∆tv(Xk(x), tk), (5) ∆t is time step.
 choose velocity ﬁeld such ∆t v(x, t) = W(2)(t) · σ W(1)(t) · σ(x) (6) (cid:16) (cid:17) W(1)(t) W(2)(t) corresponds “weight” layers residual block, σ = ReLU ◦ BN, step forward Euler discretization Eq.(5) is equivalent advancing layer deep ResNets, (see Fig.

 numerical solution transport equation Eq.(2) t = is given u(x, = f (XL(x)), (7) is output get ResNets.
 Let be point training data label g(x).
 Training ResNet is equivalent ﬁnding parameters velocity ﬁeld Eq.(6) terminal value output Eq.(7) matches label g(x) Figure
 building block pre-activated ResNets.
 trainable, propose surgeries existing DNNs construct new data dependent activated DNNs. Efﬁcient algorithms train test model are investi- gated.
 Numerical results CIFAR10 dataset limited selected instances show great suc- cess paradigm solving challenge insufﬁcient data.
 successful achievement framework is regarding generalization error reduction.
 CI- FAR10 CIFAR100 datasets receive error reduction, respectively, compared base DNNs includes VGG, ResNet, pre-activated ResNet families.
 method provides alternative towards model compression is important applications mobile devices.
 paper is structured follows: section present connection PDE control problems deep ResNets, improvements deep ResNets moti- vated PDE theory.
 Data interpolation general manifold harmonic extension manner is reviewed sec- tion
 DNN surgeries, training/testing procedures, be presented section
 validate algorithms, large variety numerical results are demon- strated section
 summary work future directions are discussed section

 Deep Residual Networks PDE Control Problem Deep ResNets, pre-activated ResNets (Pre- ActResNets), are realized adding shortcut connections connect consecutive residual blocks classical convolutional neural networks (CNN).
 be re- garded cascade residual block, shown Fig.1, followed ﬁnal ﬂatten activation layers.
 Mathe- matically, residual block is formulated as: y = F(x,{Wi}) + x, (1) x y are input output tensors block, function F(x,{Wi}) represents residual mapping be parametrized.
 build connection deep ResNets PDE BNxlxl+1ReLUConv.BNReLUConv.+ Deep Learning Data Dependent Implicit Activation Function summary, training process deep ResNets be regarded solving following control problem transport equation Rd: ∂t + v(x, t) · ∇u(x, t) x ∈ Rd, t u(x, = f (x) u(xi, = g(xi) x ∈ Rd xi ∈ T, (8)  ∂u T denotes training set, g(xi) is label in- stance xi.
 control problem is determined terminal value u(x, = f (x) velocity ﬁeld v(x, t).
 Conventionally, corresponding terminal value transport equation is selected be softmax activation function shown (3).
 control problem point view, softmax function be good terminal condition, is pre-determined real value
 ideal terminal function be smooth function close labeled value training set
 Based observation, weighted nonlocal Laplacian (Shi et seems provide good choice terminal function.

 Manifold Interpolation-A Harmonic Extension Approach section, brieﬂy discuss smooth interpo- lation general smooth manifold, give sufﬁ- cient condition number samples needed make sure interpolation has enough representation diver- sity.
 Consider following interpolation problem: Let P = {p1, p2,··· pn} be set points manifold M ⊂ Rd S = {s1, s2,··· be subset P
 Sup- pose have labels data S, want extend label function u entire dataset P
 Har- monic extension is natural approach, minimizes following Dirichlet energy functional: (cid:88) x,y∈P E(u) = boundary condition: u(x) = g(x), x ∈ S, (cid:40)(cid:80) w(x, y) is weight function, chosen be Gaussian weight w(x, y) = exp(−||x−y||2 ), σ is scaling parameter.
 Euler-Lagrange equation Eq.(9) is: σ2 u(x) = g(x) y∈P (w(x, y) + w(y, x)) (u(x) − u(y)) = x ∈ P/S x ∈ S, (10) is boundary value problem graph Laplacian (GL).
 is observed work (Shi et (cid:17)(cid:80) adding scale GL tames issue unbalanced data, leads following weighted nonlocal Laplacian (WNLL):  (cid:80) (cid:16)|P| y∈P (w(x, y) + w(y, x)) (u(x) − u(y)) + |S| − u(x) = g(x) y∈S w(y, x) (u(x) − u(y)) = x ∈ P/S x ∈ S, (11) order guarantee representability WNLL, labeled data cover types instances data pool.
 this, give necessary condition Theorem

 Suppose have data pool formed N classes data uniformly, number instances class be large.
 want classes data be sampled least once, average (cid:1) data need be sampled least N(cid:0)1 + + + ··· + data pool.
 case, number data sampled class is + Proof.
 Let Xi, i = N, be number addi- tional data needed obtain i-type (i − distinct types have been sampled.
 total number instances needed is: N expectation.
 + ··· + + X = X1 + X2 + ··· + XN = Xi. N(cid:88) i=1 i, i− distinct types instances have been sampled.
 follows probability new instance being different type is i−1 N = N−i+1 N
 Essentially, obtain i-th distinct random variable X follows geometric distribution p = N−i+1 E[Xi] = N−i+1.
 Thus, have N(cid:88) N(cid:88) N − i + Asymptotically, E[X] ≈ N ln N large N.

 Network Structure Training Algorithms Deep ResNets enable hierarchical representation learning leads fabulous performance many artiﬁcial intel- ligence tasks.
 WNLL is harmonic extension approach manifold extension adaptable labeled instances are scarce.
 illustrated connection deep ResNets PDE control problems, data dependent terminal value control problem, i.e., data dependent activation function deep ResNets, be better ad hoc activation func- tions, e.g., softmax linear activations.
 section, w(x, y) (u(x) − u(y))2 (9) E[X] = E[Xi] = i=1 i=1 Deep Learning Data Dependent Implicit Activation Function discuss perform label extension harmonic extension manner put WNLL deep ResNets.
 new framework has following ben- deep ResNet learn optimal representations WNLL interpolation; WNLL activation layer, learned deep representations be better utilized classical activation functions.
 show on-the-ﬂy coupling information feed- forward error propagation solves lack data issue achieves great accuracy improvement compared existing DNNs. Let ﬁrst discuss numerical approach solve WNLL interpolation given Eq.(11).
 numerical ap- proach is straight computational burdens involved: ﬁnding weights w(x, y) x, y ∈ P solving resulting linear system.
 ﬁnd pairwise weights, need perform nearest neighbor searching.
 brute-force approach is quadratic scaling, however, are many fast algorithms sub-linear scaling purpose, e.g., KD-Tree, Ball-Tree, etc.
 adopt approximate nearest neighbor (ANN) searching algo- rithm is scalable large scale high dimensional data(Muja Lowe,
 resulted linear system is sparse positive deﬁnite is solved conjugate gradient method work.
 is worth emphasizing order guarantee WNLL interpolation is suitable represent classes instances, labeled instances be least N ln N N be number classes.
 important component algorithm is put WNLL activation layer DNNs, design ef- ﬁcient algorithms information feed-forward propaga- tion error back-propagation.
 information error propagation paths are demonstrated Fig.2. standard DNN, e.g., VGG, ResNet, is plotted Fig.2 (a), ’DNN’ block represents layers last softmax ac- tivation function.
 naive approach place WNLL DNN is replace softmax function WNLL.
 However, case information be feed-forwarded, error cannot be back-propagated, WNLL deﬁnes activation function learned representation, gradient is explic- available.
 train network WNLL activation, introduce new structure inherited standard DNN depicted chart (c) Fig.2. structure is ﬂexible be inherited existing DNN.
 equip new blocks standard DNN, buffer block WNLL activation function, buffer block is chosen be composition connected layer preserves dimension input tensor followed ReLU function(Nair Hinton,
 buffer block be made complicated.
 buffer block, tensor is passed activations, linear activation WNLL function, parallel.
 training algorithm network Fig.2(c) is iterative procedure following steps: • Step
 Train network linear activa- tion functions steady state.
 purpose, do feed data WNLL activation.
 • Step
 Run few training epochs network freeze “DNN” ”Linear Activation” blocks, ﬁne tune ’Buffer Block’.
 order back-propagate error ground-truth WNLL interpolated results, feed data pre-trained linear activation function, use corresponding computational graph perform er- ror back-propagation.
 • Step
 entire network, train network data feeding linear activation steady state again.
 trained network, generalization step, use WNLL activation get ﬁnal inference.
 algorithm is designed greedy fashion.
 following numerical results validate efﬁciency training al- gorithm superiority network structure.
 training testing procedures proposed network are summarized Algorithms respectively.
 Remark
 back-propagation, use com- putational graph linear function approximate WNLL function linear functions are simplest nontrivial harmonic functions.
 Mixed Gaussian seems be appealing approximation, is com- patible WNLL.
 continue explore better approximations subsequent work.

 Numerical Results validate accuracy, efﬁciency, robustness proposed model, present numerical results dif- ferent tests CIFAR10 CIFAR100 (Krizhevsky, MNIST(LeCun, SVHN dataset(Netzer et al.,
 is believe difﬁculty datasets are ranked CIFAR100 followed CI- FAR10, SVHN, easiest one is MNIST.
 numerical experiments, take standard data aug- mentation is used CIFAR datasets (He et al., Huang et Zagoruyko Komodakis,
 MNIST SVHN, use raw data data augmentation.
 order use computational graph linear function approximate WNLL, need dynamical computational graph.
 pur- pose, implement algorithm PyTorch platform (Paszke automatic differentiation is used Deep Learning Data Dependent Implicit Activation Function (a) (b) (c) Figure
 Illustration DNN architectures.
 Panel (a) depicts standard DNN, “DNN” block represents layers last activation layer network.
 Panel (b) plots standard network last layer replaced WNLL layer.
 Panel (c) shows network structure used work, detailed explanation is presented paper.
 symbolic computation used TensorFlow (Abadi et al., many other systems.
 compu- tations are done machine using single Nvidia Titan Xp graphics card.
 diving performance DNNs, ﬁrst compare performance WNLL interpolation other shallow classiﬁers different datasets.
 Table lists performance k-nearest neighbors (KNN) (the optimal k is listed table), support vector machine (SVM) RBF softmax regression WNLL interpolation function.
 WNLL interpolation, order speed computation, keep nearest neighbors, neighbor’s distance is used normalize weight matrix.
 KNN WNLL be regarded nonpara- metric approaches.
 outperforms other methods SVM.
 KNN, general, is better softmax regres- sion demonstrates importance manifold structure data.
 results show potential using WNLL softmax activation function DNNs. Table
 Accuracy simple classiﬁers different datasets SVM Softmax WNLL (k=5) (k=1) (k=1) KNN Dataset Cifar10 MNIST SVHN run epochs training vanilla DNN, i.e., standard DNN, initial learning rate being halved epochs Cifar10 Cifar100 datasets.
 train epochs WNLL DNN, i.e., WNLL activated DNN.
 stage, DNN is al- ready use smaller learning rate
 Cifar datasets, hyper-parameters are chosen based cross validation.
 keep alternating above steps learning rate being ﬁfth previous stages.
 batch size training vanilla DNN is set experiments.
 SVHN experiments, use same hyperparameters reported (Huang
 batch size is set training WNLL DNN.
 Theorem number is big sample types instances CIFAR100 classes images.
 optimizations are carried simple SGD solver default Nesterov momentum acceleration.
 Resolving Challenge Lack Training Data do have sufﬁcient training data, generaliza- tion accuracy decays network goes deeper.
 phenomenon is illustrated Fig.3. left right panels plot cases ﬁrst data training set CIFAR10 are involved training vanilla WNLL DNNs. believe increase generalization error is due data being sufﬁcient parametrize deep networks.
 suitable regularization deep networks be parametrized small amount training data.
 WNLL activation involves information data’s geometric structures is such regularizers.
 WNLL activation, generalization error rate decays net- work goes deeper.
 generalization accuracy vanilla WNLL DNN differ percent testing regime.
 have built connection deep ResNets PDE control problems, test performance surgeries algorithms other base DNNs, e.g., VGG networks.
 table list generalization error rates different DNNs VGG, ResNet, Pre-activated ResNet families entire, ﬁrst instances CIFAR10 training set.
 is easy see WNLL activated DNNs has accuracy improvement ResNets pre- DNNX(x;Θ)OutputSoftmaxDNNX(x;Θ)WNLLOutputDNNX(x;Θ)BuﬀerBlockFC(linearapprox.)WNLLOutput Deep Learning Data Dependent Implicit Activation Function Algorithm DNN Data Dependent Activation: Training Procedure.
 Input: Training set (T, lT ), T is set features, lT is label set.
 Output: optimized DNN surgeries, denoted DNNs. iter


 N (where let N = entire data, N = selected subset training set.
 do Train DNN linear activation starting previous iteration.
 ﬁrst step use default initialized one.
 Denote temporary model DNN1.
 Split T training validation parts, T model.
 Partition T T T V nbatch1 nbatch2 mini-batches, denoted {T T nbatch1 nbatch2 are given integers.
 i = do = T T(cid:83) T V
 treat instances T V unlabeled data training i }nbatch2 i }nbatch1 {T V i=1 i=1 j = do Apply model DNN1 T T Apply WNLL interpolation ˆT T (cid:83) T V (cid:83) ˆT V get ˆT T i = DNN1(T T i solving linear system (cid:83) T V i ).
 (cid:83) ˆT V (cid:83) T V i | V i | |T T (cid:88) xn∈T V (cid:88) (cid:83) T V xn∈T T (wmn + wnm)(um − un) + wnm(um − ln) ∀xm ∈ T T (cid:91) T V i
 obtain inferred label uj T V T V j=1 }nbatch1 end Voting {uj Back-propagate loss(lT V using computational graph linear activation function, update DNN1.
 classiﬁcation loss is selected be cross entropy exact predicted labels.
 end get inferred label uT V uT V end Table
 Generalization error rate test set vanilla DNNs WNLL activated ones trained entire, ﬁrst instances training set CIFAR10.
 (Median independent trials) Network Whole VGG11 VGG13 VGG16 VGG19 ResNet20 ResNet32 ResNet44 ResNet56 ResNet110 ResNet18 ResNet34 ResNet50 PreActResNet18 PreActResNet34 PreActResNet50 Vanilla WNLL Vanilla WNLL Vanilla WNLL Vanilla WNLL activated ResNets.
 Signiﬁcant accuracy improvement be observed surgeries training algorithms are applied VGG networks.
 VGG achieve testing error rate reduction.
 results presented rest paper are median independent trials reduce Deep Learning Data Dependent Implicit Activation Function Algorithm DNN Data Dependent Activation: Testing Procedure.
 Input: Testing set W training set (T, lT ), W T are features, lT are labels instances T
 trained DNN surgeries, denoted DNNs. Output: predicted labels test set W
 Partition T W nbatch1 nbatch2 number mini-batches, denoted {Ti}nbatch1 nbatch1 nbatch2 are given integers.
 i = do {Wi}nbatch2 i=1 i=1 j = do Apply model DNNs Tj Apply WNLL interpolation ˆTj (cid:88) (cid:83) Wi xn∈Tj (cid:83) Wi get ˆTj (cid:83) Wi) (cid:83) ˆWi solving linear system (cid:83) ˆWi = DNNs(Tj (cid:88) (cid:83) Wi| |Tj |Wi| xn∈Wi (wmn + wnm)(um − un) + wnm(um − ln) ∀xm ∈ Tj (cid:91) Wi. obtain inferred label uj Wi end Voting {uj Output predicted label uW =(cid:83)nbatch2 }nbatch1 j=1 Wi i=1 get inferred label uWi. uWi. end ter epochs, accuracies vanilla DNNs plateaued cannot improve more.
 However, stage use WNLL activation, is jump generalization accuracy; stage is accuracy reduction, training con- accuracy keeps climbing while.
 gener- alization accuracy increases use WNLL activation function.
 (a) (c) (b) (d) Figure
 evolution generation accuracy training procedure.
 Charts (a) (b) are accuracy plots ResNet50 training data, (a) (b) are plots epoch v.s. accuracy vanilla WNLL activated DNN.
 Pan- els (d) correspond case training data PreActResNet50.
 tests are done Cifar10 dataset.
 (a) (b) Figure
 Taming degeneration problem vanilla DNN WNLL activated DNN.
 Panels (a) (b) plot generation error training data are used train vanilla WNLL activated DNN, respectively.
 plot, test different PreActResNet18, PreActResNet34, PreActResNet50.
 is easy see vanilla network becomes deeper, generation error does decayed, WNLL activation resolves degeneracy.
 tests are done Cifar10 dataset.
 inﬂuence stochasticity.
 Error Rate Reduction Base DNNs next present superiority deep network terms generalization accuracy compared base network.
 Figure.
 plots generalization accuracy evolu- tion training procedure.
 Panels (a) (b) plot cases ResNet50 WNLL activated ResNet50 ﬁrst CIFAR10 training data are utilized.
 Charts (c) (d) are cases ﬁrst CIFAR10 training instances are used train vanilla pre-activated ResNet50 WNLL activated version.
 Af- Deep Learning Data Dependent Implicit Activation Function street view house number recognition (SVHN) task, test performance full training data are used.
 test performance ResNets pre-activated ResNets.
 is 7%-10% error rate reduction DNNs. is rela- improvement pre-activated ResNets ResNets, is consistent basic PDE control problem ansatz.
 Table
 Error rate vanilla DNN v.s. WNLL activated DNN whole SVHN dataset.
 (Median independent trials) Network ResNet20 ResNet32 ResNet44 ResNet56 ResNet110 ResNet18 ResNet34 PreActResNet18 PreActResNet34 Vanilla DNN WNLL DNN Tables list error rate different vanilla net- works WNLL activated networks.
 CIFAR10, WNLL activated DNNs outperformed vanilla ones absolute, relative error rate reduction.
 reproduced results vanilla DNNs datasets.
 results are consistent original reports other researchers’ reproductions (He et al., Huang et
 Interestingly, task become harder, improvement becomes more signiﬁcant.
 builds conﬁdent trying harder tasks future.
 Reducing sizes DNN models is important direction make DNN applicable gen- eralize purpose, e.g., auto-drive, mobile intelligence, etc.
 successful attempt is DNN weights quan- tization(Bengio David,
 approach is new direction reducing size model: achieve same level accuracy, compared vanilla model’s size be tens times smaller.

 Concluding Remarks Motivated connection deep ResNets PDE control problems, propose novel DNN struc- ture be inherited existing DNN.
 end-to-end greedy styled, multi-staged training algorithm is proposed train novel networks.
 order propagate errors, utilized computational graph linear function approximate Table
 Error rate vanilla DNN v.s. WNLL activated DNN whole CIFAR100 dataset.
 (Median independent trials) Network VGG11 VGG13 VGG16 VGG19 ResNet20 ResNet32 ResNet44 ResNet56 ResNet110 ResNet18 ResNet34 ResNet50 PreActResNet18 PreActResNet34 PreActResNet50 Vanilla DNN WNLL DNN manifold interpolation function.
 hand, new framework resolves issue lack big training data, other hand, provides great accuracy improve- ment compared base DNNs. improvement is consistent networks different depths.
 Utilizing structure, is easy get state-of-the-art results small model, has great potential mobile device applications.
 Nevertheless, are many di- rections improvement: current manifold interpolation is computational bottlenecks, according representability theorem data many classes.
 batch size need be large ImageNet dataset (J.
 poses memory challenges.
 important issue is approximation gradient WNLL activation function.
 Linear function is option is optimal.
 believe better harmonic function approximation lift model’s performance.

 Acknowledgement material is based, part, work supported U.S. Department Energy, Ofﬁce Science National Science Foundation, National Science Foun- dation China, Grant Numbers doe-sc0013838 DMS-1554564, (STROBE), NSFC
 Deep Learning Data Dependent Implicit Activation Function LeCun, Yann.
 mnist database handwritten digits.

 Lecun, Yann, Bengio, Yoshua, Hinton, Geoffrey.
 Deep learning.
 Nature,
 Muja, Marius Lowe, David G.
 Scalable nearest neighbor algorithms high dimensional data.
 Pattern Analysis Machine Intelligence (PAMI),
 Nair, Vinod Hinton, Geoffrey.
 Rectiﬁed linear units improve restricted boltzmann machines.
 ICML,
 Netzer, Yuval, Wang, Tao, Coates, Adam, Bissacco, Alessan- dro, Wu, Bo, Ng, Andrew Y.
 Reading digits nat- ural images unsupervised features learning.
 NIPS Workshop Deep Learning Unsupervised Feature Learning,
 Paszke, Adam, Gross, Sam, Chintala, Soumith, Chanan, Gregory, Yang, Edward, DeVito, Zachary, Lin, Zeming, Desmaison, Alban, Antiga, Luca, Lerer, Adam.
 Au- tomatic differentiation pytorch.

 Shi, Z., Osher, S., Zhu, W.
 Weighted nonlocal Laplacian interpolation sparse data.
 Journal Scientiﬁc Computing, pp.
 appear,
 Simonyan, Karen Zisserman, Andrew.
 deep con- volutional networks large-scale image recognition.

 Srivastava, N., Hinton, G.
 E., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.
 Dropout: simple way prevent neural networks overﬁtting.
 Journal Machine Learning Research,
 Wen, Yandong, Zhang, Kaipeng, Li, Zhifeng, Qian, Yu. A discriminative feature learning approach deep face recognition.

 Zagoruyko, S.
 Komodakis, N.
 Wide residual networks.
 BMVC,
 Zhu, Wei, Qiu, Qiang, Huang, Jiaji, Carderbank, Robert, Sapiro, Guillermo, Daubechies, Ingrid.
 Low dimen- sional manifold regularized neural networks.
 UCLA CAM Report:
 References Abadi, M., Agarwal, A., et al.
 Tensorﬂow: Large-scale machine learning heterogeneous distributed systems.

 Bengio, M.
 Courbariaux Y.
 David, J.
 Binaryconnet: Training deep neural networks binary weights.

 Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H.
 Greedy layer-wise training deep networks.

 Bishop, C.
 M.
 Pattern recognition machine learning.

 Chen, Yunpeng, Li, Jianan, Xiao, Huaxin, Jin, Xiaojie, Yan, Shuicheng, Feng, Jiashi.
 Dual path networks.

 E, Weinan.
 proposal machine learning dynamical systems.
 Communications Mathematics Statistics,
 Evans, L.C. Partial differential equations.
 American Mathe- matical Soc,
 Hardt, Moritz Ma, Teng Yu. Identity matters deep learning.
 ICLR,
 He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, Sun, Jian.
 Deep residual learning image recognition.
 CVPR,
 He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, Sun, Jian.
 Identity mappings deep residual networks.

 Hinton, G.
 E., Osindero, S., Teh, T.
 W.
 fast learning algorithm deep belief nets.
 Neural Computation, (7):1527–1554,
 Huang, Gao, Sun, Yu, Liu, Zhuang, Sedra, Daniel, WeinBerger, Kilian.
 Deep networks stochastic depth.

 Huang, Gao, Liu, Zhuang, Weinberger, K.
 Q., van der Maaten, Laurens.
 connected convolutional net- works.
 CVPR,
 J., Deng, W., Dong, R., Socher, L.-J., Li, K., Li, L., Fei Fei.
 ImageNet: Large-Scale Hierarchical Image Database.
 CVPR09,
 Krizhevsky, Alex.
 Learning multiple layers features tiny images.

 Langley, P.
 Crafting papers machine learning.
 Langley, Pat (ed.), Proceedings 17th International Confer- ence Machine Learning (ICML pp.
 Stanford, CA,
 Morgan Kaufmann.
 wide variety problems be reduced computing sum (many) non-negative numbers.
 include calculat- ing partition function graphical model, propositional model counting (#SAT), calculating permanent non-negative matrix.
 Equivalently, be viewed computing discrete integral non-negative weight func- tion.
 Exact summation, however, is intractable due curse dimensionality (Bellman
 alternatives exact computation, variational methods (Jordan al.
 Wainwright, Jordan, others sampling (Jerrum Sinclair Madras are popular approaches approximate summation.
 However, do guarantee estimate’s quality.
 emerging line work estimates bounds propositional model counts generally, discrete in- tegrals (Ermon al.
 Chakraborty, Meel, Vardi Ermon al.
 Zhao al.

 approaches reduce problem integration solving small number optimization problems involving same weight function subject additional random constraints introduced random hash function.
 results approximating Copyright c(cid:13) Association Advancement Artiﬁcial Intelligence (www.aaai.org).
 rights reserved.
 #P-hard problem exact summation (Valiant using solutions NP-hard optimization problems.
 Optimization be performed certain classes weight functions, such involved computation permanent non-negative matrix.
 summing (permanent computation) max- imize same weight function, obtain maximum weight matching problem, is fact solvable poly- nomial time (Kuhn
 However, adding hash-based con- straints makes maximum matching optimization prob- lem intractable, limits application random- ized hashing approaches (Ermon al.

 other hand, do exist polynomial-time random- ized approximation schemes (FPRAS) non-negative per- manent computation (Jerrum, Sinclair, Vigoda Bezáková al.

 gives hope approximation schemes exist other counting problems optimization hash-based constraints is intractable.
 present new method approximating bound- ing size general weighted set (i.e., sum weights elements) using geometric arguments based set’s shape.
 approach, relying hash-based techniques, establishes novel connection Rademacher complexity (Shalev-Shwartz Ben-David
 generalizes geometric approaches developed unweighted case weighted setting, such work Barvinok (1997) uses similar reasoning with- connecting Rademacher complexity.
 particular, ﬁrst generalize Rademacher complexity weighted sets.
 Rademacher complexity is deﬁned maximum sum Rademacher variables set, weighted Rademacher complexity accounts weight element set.
 Rademacher complexity is related size set, show weighted Rademacher complexity is related total weight set.
 be estimated solving multiple instances maximum weight optimization problem, subject random Rademacher perturbations.
 Notably, resulting optimization problem turns be much simpler re- quired aforementioned randomized hashing schemes.
 particular, weight function is log-supermodular, corresponding weighted Rademacher complexity be estimated perturbation does change original optimization problem’s complexity (Orlin Bach others
 approach resembles recent line work involving Gumbel distribution (Hazan Jaakkola Hazan, Maji, Jaakkola Hazan al.
 Balog al.
 Mussmann Ermon Mussmann, Levy, Ermon
 Gumbel-max idea is used bound partition function performing MAP inference model unnormalized probability state is perturbed random noise variables sampled Gumbel distribution.
 powerful, exact application Gumbel method is requires many independent random perturbations.
 uses local approximations technique.
 Empirically, spin glass models show tech- nique yields upper bounds similar lower bounds compared Gumbel method, given similar computa- tional resources.
 suite #SAT model counting instances approach produces comparable tighter upper lower bounds given limited computation.
 Rademacher complexity is important tool used learn- ing theory bound generalization error hypothesis class (Shalev-Shwartz Ben-David
 Deﬁnition
 Rademacher complexity set A ⊆ Rn is deﬁned as: R(A) := Ec sup a∈A ciai (1) Ec denotes expectation c, c is sampled uni- formly
 name suggests, is measure complexity set A (which, learning theory, is hypothesis class).
 measures “expressive” is evaluating “ﬁt” random noise vector c choosing closest vector (or hypothesis) A.
 Intuitively, Rademacher com- plexity is related |A|, number vectors A, crude notion complexity A.
 However, depends vectors are arranged ambient space Rn. A central focus paper be establishing quantitative relationships R(A) |A|.
 key property Rademacher complexity makes useful learning theory is be estimated using small number random noise samples c mild conditions (Shalev-Shwartz Ben-David
 result follows McDiarmid’s Proposition (McDiarmid,
 Let X1, ..., Xm ∈ X be independent random variables.
 Let f X m (cid:55)→ R be function satisﬁes bounded differences condition ∀i ∈ {1, ..., m} ∀x1, ..., xm, x(cid:48) i ∈ X |f (x1, ..., xi, ..., xm) − f (x1, ..., x(cid:48) (cid:35) (cid:34) i=1  > (cid:104)(cid:12)(cid:12)f (X1, ..., Xm) − E(cid:2)f (X1, ..., Xm)(cid:3)(cid:12)(cid:12) ≥  Pr i, ..., xm)| ≤ di.
 (cid:32) −22(cid:80) (cid:105) ≤ exp (cid:33) j d2 McDiarmid’s inequality says bound, high probability, function f random variables deviate expected value, given function does change value single random variable is changed.
 function Eq. (1) satisﬁes property (Shalev-Shwartz Ben-David use Eq. (1) bound R(A) high probability computing supremum small number noise samples c.
 Problem Setup section deﬁne problem intro- duce optimization oracle central solution.
 Let w {−1, → be non-negative weight function.
 consider problem computing sum (cid:88) Z(w) = w(x).
 Many problems, including computing partition func- tion undirected graphical model, w(x) is unnormalized probability state x (see Koller Fried- man (2009)), propositional model counting (#SAT), com- puting permanent non-negative matrix be reduced calculating sum.
 problem is challenging explicit calculation requires summing states, is intractable cases interest.
 Due general intractability calculating Z(w), focus efﬁcient approach estimating Z(w) provides upper lower bounds hold high probability.
 method depends following assumption: Assumption
 assume existence optimization ora- cle output value {(cid:104)c, x(cid:105) + log w(x)} x∈{−1,1}n δ(c, w) = max (2) vector c ∈ {−1, weight function w {−1, → [0,∞).
 Note paper denote log2 log, loge ln, assume log = −∞.
 Assump- tion is reasonable, are many classes models oracle exists.
 instance, polynomial time algorithms exist ﬁnding maximum weight match- ing weighted bipartite graph (Hopcroft Karp Jonker Volgenant
 Graph cut be ap- plied maximize class energy functions (Kol- mogorov Zabin
 generally, MAP inference be performed log-supermodular weight function (Orlin Chakrabarty, Jain, Kothari Fujishige
 perturbation preserves submodular- ity − log w(x), (cid:104)c, x(cid:105) be viewed n independent single variable perturbations, have efﬁcient opti- mization oracle original weight function is log-supermodular.
 Further, notice is weaker assumption compared optimization oracle required randomized hashing methods (Chakraborty, Meel, Vardi Ermon al.
 Zhao al.

 approximate optimization oracle exists ﬁnd value known bound maximum, modify bounds use approximate oracle.
 improve efﬁciency algorithm extend use additional problem classes.
 class log-supermodular distributions, approximate MAP inference is equivalent performing approximate submodular minimization (Jegelka, Lin, Bilmes
 note efﬁcient optimization oracle exists, problem calculating Z(w) is gener- ally hard.
 example, polynomial time algorithms ex- ist ﬁnding maximum weight perfect matching weighted bipartite graph.
 However, computing permanent bipartite graph’s adjacency matrix, equals sum weights perfect matchings Z(w), is #P- complete(Jerrum, Sinclair, Vigoda
 poly- nomial randomized approximation scheme (FPRAS) exists (Jerrum, Sinclair, Vigoda Bezáková al.
 based Markov chain Monte Carlo sample per- fect matchings.
 However, polynomial time complexity algorithm suffers large degree, limiting practical use.
 approach estimating sum Z(w) = (cid:80) Weighted Rademacher Bounds Z(w) x w(x) is based idea Rademacher complexity set is related set’s size.
 particular, Rademacher complexity is monotonic sense R(A) ≤ R(B) A ⊆ B.
 Note monotonicity does hold |A| ≤ |B|, is, R(A) is monotonic contents size.
 estimate sum arbi- trary non-negative elements generalizing Rademacher complexity deﬁnition
 Deﬁnition
 deﬁne weighted Rademacher complexity weight function w {−1, → [0,∞) R(w) := Ec max x∈{−1,1}n {(cid:104)c, x(cid:105) + log w(x)} (3) c sampled {−1,
 notation Eq. (2), weighted Rademacher com- plexity is R(w) = Ec[δ(c, w)].
 set A ⊆ {−1, let IA {−1, → {0, denote indicator weight function A, deﬁned IA(x) = ⇐⇒ x ∈ A.
 R(IA) = R(A), is, weighted Rademacher com- plexity is identical standard Rademacher complexity indicator weight functions.
 general weight function, weighted Rademacher complexity extends standard Rademacher complexity giving element (hypothesis) own weight.
 Algorithmic Strategy key idea paper is use weighted Rademacher complexity R(w) provide probabilistic estimates Z(w), total weight w.
 is reasonable strategy have seen before, indicator weight function IA {−1, → {0, R(IA) reduces standard Rademacher complex- ity R(A), Z(IA) = |A| is cardinality set.
 use known quantitative relation- ships R(A) |A| learning theory estimate (cid:20) (cid:21) k(cid:88) i=1 |A| = Z(IA) terms R(A) = R(IA).
 for- mulated framework Rademacher complexity, is strategy used Barvinok (1997).
 Here, generalize results general weight func- tions w show is, fact, possible use R(w) obtain estimates Z(w).
 observation be turned algorithm observing R(w) is expectation random variable concentrated mean.
 Therefore, show Proposition small number samples suf- ﬁces reliably estimate R(w) (and hence, Z(w)) high probability.
 w is nice’ have access optimization estimation algorithm is efﬁcient.
 Rademacher Estimate log Z(w) Inputs: A positive integer k weight function w {−1, → [0,∞).
 A number ¯δk(w) approximates log Z(w) = log (cid:16)(cid:80) (cid:17) x∈{−1,1}n w(x)
 Sample k vectors c1, c2,


 ck uni- formly

 Apply optimization oracle assumption vector c compute mean ¯δk(w) = max x∈{−1,1}n {(cid:104)ci, x(cid:105) + log w(x)}.

 Output ¯δk(w) estimator R(w) log Z(w).
 Bounding Weighted Rademacher Complexity weighted Rademacher complexity is expectation optimization problems.
 optimization problem is deﬁned sampling vector, direction have length n, {−1, ﬁnding vector x is aligned (largest dot product) adding log w(x).
 ﬁrst objective is derive bounds weighted Rademacher complexity terms sum Z(w).
 begin observation is impossible de- rive bounds Rademacher complexity terms set size are tight sets shapes.
 gain intuition, note high dimensional spaces dot product par- ticular vector chosen random {−1, is high probability.
 distribution weight vectors space take geomet- ric form.
 extreme conﬁguration is vectors large weights are packed together, forming Ham- ming ball.
 other extreme, vectors large weights be distributed space.
 Figure illustrates, large set packed vectors small set well-distributed vectors have similar Rademacher complexity.
 Thus, bounds Rademacher complexity are based underlying set’s size cannot be tight distributions.
 Nevertheless, lower upper bounds derive next are tight be useful practice.

 λ γ > weight functions w, wγ {−1, → [0,∞) wγ(x) = w(x)γ, weighted Rademacher complexity wγ is upper bounded R(wγ) ≤ (cid:40) log w∗(λ, γ) + λ λγ − log Z(w) + (4) wmax = maxx w(x), wmin = minx{w(x) w(x) > (λ, γ) = Note indicator weight function recover λγ ≥ λγ ≤ log Z(w) bound Massart’s Lemma setting λ = γ =
 Corollary large γ (cid:115) λ = log Z(w) wmax recover bound wmax ≤ Z(w) Lemma
 Lemma holds λ > γ >
 general set γ optimize λ make bound tight possible, comparing result trivial bound given Corollary More sophisticated optimization strategies λ γ result tighter bound.
 Please see appendix further details proofs.
 Bounding Weighted Sum Z(w) bounds weighted Rademacher complexity previous section, present method bounding sum Z(w).
 Proposition states estimate weighted Rademacher complexity using optimization oracle assumption
 Proposition
 c ∈ {−1, sampled ran- dom, bound R(w) − ≤ δ(c, w) ≤ R(w) + (5) holds probability greater .95.
 Proof.
 applying Proposition function fw(c) = δ(c, w), noting constant di have (cid:104)|δ(c, w) − R(w)| ≥ (cid:105) ≤ e−3 .05.
 ﬁnishes proof.
 bound Z(w) use optimization oracle solve perturbed optimization problem, giving estimate weighted Rademacher complexity, R(w).
 invert bounds R(w) (Lemmas obtain bounds Z(w).
 optimize parameters λ β (from equations make bounds tight possible.
 applying optimization oracle reduce slack introduced ﬁnal bound estimating R(w) (by Lemma arrive bounds sum Z(w), stated (cid:80) following theorem.

 probability least sum Z(w) = x∈{−1,1}n w(x) weight function w {−1, → [0,∞) is bounded outputs algorithms ψLB < log Z(w) < ψU B.
 Figure Illustration mapping set vectors high di- mensional space unit circle.
 Red regions correspond regions space have large dot prod- uct vector set.
 Left: size set is small, few regions have large dot product vector set, Rademacher complexity be small.
 Right: large set vectors is packed small region space, Rademacher complexity remain small.
 left right ﬁgures have similar (small) Rademacher complexities, different set sizes.
 illustrates tight bounds set size based Rademacher complexity are difﬁcult achieve.
 Lower bound.
 lower weighted Rademacher complexity adapt technique lower bounding standard Rademacher complexity.
 high level idea is space {−1, be mapped leaves binary tree.
 following path root leaf, are dividing space half n times, arrive leaf corresponds single element (with ﬁxed weight).
 choosing half space (branch tree) recurse step derive bound Lemma proof is given appendix.

 β ∈ (0, weighted Rademacher complexity weight function w {−1, → [0,∞) is lower bounded R(w) ≥ log w n log (1 − β) + log Z(w) − log w∗(β) (β) + (cid:16) (cid:17) log (cid:40) (β) = wmax = maxx w(x), wmin = minx{w(x) w(x) > β ≥ β ≤ Upper bound.
 unweighted setting, standard up- bound Rademacher complexity is used learn- ing theory show Rademacher complexity small hypothesis class is prove PAC- learnability.
 Massart’s Lemma (see (Shalev-Shwartz Ben-David lemma upper bounds Rademacher complexity terms size set.
 result is intuitive since, have noted, dot product vector x ∈ {−1, is small most other vectors c ∈
 Therefore, set is small Rademacher complexity be small.
 Adapting proof technique Massart’s Lemma weighted setting arrive following bound:
 log wmin was provided input λ ≤
 wmax was provided input, calculate ln Z(w) = Eγ max x∈{−1,1}n {ln w(x) + γ(x)} w∗ = (cid:19) (cid:18) − βopt (cid:26)wmin, βopt wmax, βopt < βopt > Algorithm Rademacher Lower Bound log Z(w) Inputs: estimator ¯δk(w) output algorithm k used compute ¯δk(w), wmin wmax.
 A number ψLB lower bounds Z(w).

 log wmin was provided input, calculate k − log wmin λ = ¯δk(w) −(cid:113) (¯δk(w) −(cid:113) ψLB =
 Otherwise, + log wmin.
 k − log wmin)2 ψLB = ¯δk(w) − (cid:114) −
 lower bound max{ψLB, log wmax}.
 Experiments closest line work paper showed partition function be bounded solving optimization problem perturbed Gumbel random variables (Hazan Jaakkola Hazan, Maji, Jaakkola Hazan al.
 Kim, Sabharwal, Ermon Balog al.

 approach is based fact (cid:21) (cid:40) (cid:40) random variables are sampled Gumbel distribution scale shifted Euler- Mascheroni constant have mean
 Perturbing states IID Gumbel random variables is intractable, leading authors bound ln Z(w) perturbing states combi- nation low dimensional Gumbel perturbations.
 upper bound ln Z(w) ≤ ΘU B = Eγ max x∈{−1,1}n ln w(x) + γi(xi) (Hazan al.
 lower bound (cid:41)(cid:35) (cid:41)(cid:35) n(cid:88) i=1 i=1 ln Z(w) ≥ ΘLB = Eγ max x∈{−1,1}n ln w(x) + γi(xi) (Balog al.
 p.
 hold expectation, γi(x) i =


 are sampled Gumbel distribution scale shifted Euler-Mascheroni constant have mean
 obtain bounds hold high probability using Gumbel perturbations calculate slack term (Hazan et al.
 p.
 (cid:33)2 ln (cid:40) max (cid:114) (cid:41)(cid:41) ln ln (cid:32) (cid:40) g = min + (cid:20) (cid:34) (cid:34) Algorithm Rademacher Upper Bound log Z(w) Inputs: estimator ¯δk(w), k used compute ¯δk(w), wmin wmax.
 A number ψU B upper bounds log Z(w).

 wmin was provided input, calculate (cid:113) k − log wmin (cid:113) k − log wmax ¯δk(w) + βmin = ¯δk(w) + βmax =
 Set value βopt =  βmin, βmax, < βmin < βmax < < βmax
 upper bound ψU B: (a) βopt = (b) βopt = (c) Otherwise, ψU B = ¯δk(w) + ψU B = n + log wmax.
 k + log(cid:0) (cid:1).
 ψU B = log log − βopt)+log w giving upper lower bounds θU B = ΘU B + g θLB = n hold probability − α k samples ΘLB − g are used estimate expectation bounds.
 note Gumbel expectation upper bound takes same form weighted Rademacher complexity, differences.
 perturbation is sampled Gum- bel distribution dot product vector Rademacher random variables and, bounds are written different log bases.
 compare bounds ob- tained Gumbel perturbations models.
 bound partition function spin glass model (Hazan et al.

 problem weight function is given unnormalized probability distribution spin glass model.
 Second bound propositional model counts (#SAT) variety SAT problems.
 problem falls unweighted category weight is satisfying assignment has weight bound total number satisfying assign- ments.
 question many assignments x underlying boolean variables result F evaluating true.
 weight function is given w(x) F (x) evaluates true, otherwise.
 performed MAP inference perturbed problem using weighted partial MaxSAT solver MaxHS (Davies
 Ground truth was obtained variety models1 us- ing exact propositional model counters (Thurley Sang al.
 Oztok Darwiche
 Table shows bounds hold probability .95 k =
 Gumbel lower bounds are produce non- trivial lower bounds several model instances.
 upper bounds are comparable tighter Gumbel upper bounds.
 Analysis bounds are looser computed random- ized hashing schemes (Chakraborty, Meel, Vardi Ermon al.
 Ermon al.
 Zhao al.
 require less computation (Ermon et al.
 Achim, Sabharwal, Ermon
 approach provides polynomial runtime guarantees MAP inference spin glass model random perturbations have been applied, randomized hashing approaches do
 proposi- tional model counting, found method is computa- cheaper orders magnitude results reported Zhao al.
 (2016).
 tried reducing runtime accuracy randomized hashing schemes running code Zhao al.
 (2016) f values .01, .02, .03, .04, .05.
 set maximum time limit hour method required .01 seconds computation reported results).
 experiments models reported Table approach re- quired orders magnitude computation found tighter bounds instances.
 Empirically, lower bounds were comparable tighter obtained Gumbel perturbations models.
 weighted Rademacher complexity is least good estimator log Z Gumbel up- bound, is estimator upper bound.
 upper bound using weighted Rademacher complexity, holds expectation, is weaker corresponding Gumbel expectation bound.
 However, slack term needed transform expecta- tion bound high probability bound is tighter corresponding Gumbel slack term.
 slack terms approach limit inﬁnite computation (k = ∞, number samples used estimate expectation result trade-off produce tighter upper bound value k, Gumbel bound becomes tighter.
 models used experiments be downloaded http://reasoning.cs.ucla.edu/c2d/results.html counts were model downloaded https://sites.google.com/site/marcthurley/sharpsat/benchmarks/ collected-model-counts Figure Bounds spin glass model k = (for methods), hold probability .95.
 bounds estimator are scaled match Gumbel log base e bounds.
 Spin Glass Model Following (Hazan et al.
 bound partition func- tion spin glass model variables xi ∈ i =


 variable represents spin.
 spin has local ﬁeld parameter θi corresponds local potential function θi(xi) = θixi.
 performed experi- ments grid shaped models spin variable has neighbors, occupies grid edge.
 Neighboring spins interact coupling parameters θi,j(xi, xj) = θi,jxixj.
 potential function spin glass model is θ(x1, x2,


 = θixi + θi,jxixj, (cid:88) i∈V (cid:88) (i,j)∈E (cid:88) i∈V (cid:88) (i,j)∈E 
 corresponding weight function w(x) = exp θixi + θi,jxixj compare bounds spin glass model.
 sam- pled local ﬁeld parameters θi random coupling parameters random [0, c) c varying.
 Non-negative coupling parameters make possible perform MAP inference us- ing graph-cuts algorithm (Kolmogorov Zabin Greig, Porteous, Seheult
 used python maxﬂow module wrapping implementation Boykov Kolmogorov (2004).
 Figure shows bounds hold probability .95, bounds are computed k
 value approach produces tighter upper bounds using Gumbel perturbations.
 crossover tighter Gumbel per- turbation bound occurs k ≈
 Lower bounds are equivalent, note is trivial recover bound calculating largest weight states.
 Propositional Model Counting Next evaluate method problem proposi- tional model counting.
 Given boolean formula F poses Model Name log-1 log-2 log-3 log-4 tire-1 tire-2 tire-3 tire-4 ra rb sat-grid-pbl-0010 sat-grid-pbl-0015 sat-grid-pbl-0020 sat-grid-pbl-0025 sat-grid-pbl-0030 c432 c499 c880 c1355 c1908 c2670 #Variables #Clauses ln(Z) ¯δ1(w) (20.8) (20.7) (22.3) (26.6) (11.2) (14.2) (17.1) (17.4) (15.5) (12.6) (4.9) (6.6) (9.0) (9.4) (13.0) (5.8) (6.2) (8.4) (12.2) (12.2) (14.6) ψU B θU B ψLB (46.2) (60.3) (65.3) (77.7) (17.6) (27.7) (36.1) (38.9) (0.0) (0.0) (0.0) (0.0) (0.0) (0.0) (0.0) (1.0) (0.4) (4.7) (14.2) (19.3) (10.8) (43.0) (45.1) (42.3) (58.8) (23.7) (29.6) (29.1) (35.3) (45.7) (43.9) (13.6) (18.8) (26.5) (33.1) (36.7) (18.2) (16.6) (21.3) (28.7) (32.8) (39.6) (0.6) (0.4) (0.4) (0.5) (0.4) (0.4) (0.6) (0.3) (10.1) (7.7) (2.2) (3.8) (5.6) (6.2) (8.7) (0.8) (1.2) (1.4) (1.3) (0.9) (3.4) θLB -0.3 (0.0) -0.3 (0.0) -0.3 (0.0) -0.2 (0.0) -0.5 (0.1) -0.4 (0.1) -0.4 (0.1) -0.3 (0.0) (0.0) (0.0) -0.5 (0.1) -0.1 (0.1) (0.1) (0.1) (0.0) -0.5 (0.1) -0.4 (0.1) -0.3 (0.1) -0.3 (0.1) -0.3 (0.0) -0.1 (0.0) Table Empirical comparison estimate (¯δ1(w)) bounds (ψ) propositional model counts bounds based Gumbel perturbations (θ).
 mean runs is shown standard deviation parentheses.
 Bounds hold probability .95 k = methods.
 Tighter bounds are bold.
 Meta column descriptions, left right: model name information, natural logarithm ground truth model counts estimator, upper bounds, lower bounds.
 Conclusion introduced weighted Rademacher complexity, novel generalization Rademacher complexity.
 showed quantity be used estimator size weighted set, gave bounds weighted Rademacher complexity terms weighted set size.
 allowed bound sum non-negative weight function, such partition function, terms weighted Rademacher complexity.
 showed weighted Rademacher complexity be approximated efﬁcient optimization oracle exists, is case variety practical problems including calculating partition function certain graphical models per- manent non-negative matrices.
 Experimental evaluation demonstrated approach provides tighter bounds competing methods certain conditions.
 future work estimator R(w) bounds Z(w) be generalized other forms randomness.
 sampling c {−1, conceiv- sample element ci other distribution, such uniform distribution Gaussian, Gumbel.
 bounds adapt continuous uniform gaussian distributions, derivations be more complex general.
 line future weighted Rademacher complexity be useful approximate inference learning theory.
 Acknowledgments acknowledge funding Ford, FLI NSF grants #1651565, #1522054, #1733686.
 thank Tri Dao, Aditya Grover, Rachel Luo, anonymous reviewers.
 References [2016] Achim, T.; Sabharwal, A.; Ermon, S.

 Be- yond parity constraints: Fourier analysis hash functions inference.
 International Conference Machine Learning,
 [2013] Bach, F., al.

 Learning submodular functions: A convex optimization perspective.
 Foundations Trends R(cid:13) Machine Learning 6(2-3):145–373.
 [2017] Balog, M.; Tripuraneni, N.; Ghahramani, Z.; Weller, A.

 Lost relatives Gumbel trick.
 International Conference Machine Learning,
 [1997] Barvinok, A.
 I.

 Approximate counting random optimization.
 Random Structures Algorithms
 [1961] Bellman, R.
 E.

 Adaptive control processes: guided tour.
 Princeton university press.
 [2006] Bezáková, I.; Štefankoviˇc, D.; Vazirani, V.
 V.; Vigoda, E.

 Accelerating simulated annealing permanent combinatorial counting problems.
 Proceed- ings seventeenth annual ACM-SIAM symposium Discrete algorithm,
 [2004] Boykov, Y., Kolmogorov, V.

 experimen- tal comparison min-cut/max-ﬂow algorithms energy minimization vision.
 IEEE transactions pattern analy- sis machine intelligence
 [2014] Chakrabarty, D.; Jain, P.; Kothari, P.

 Prov- able submodular minimization using wolfe’s algorithm.
 Advances Neural Information Processing Systems,
 [2013] Chakraborty, S.; Meel, K.
 S.; Vardi, M.
 Y.

 scalable approximate model counter.
 International Confer- ence Principles Practice Constraint Programming,
 Springer.
 [2013] Davies, J.

 Solving MAXSAT Decoupling Optimization Satisfaction.
 Ph.D. Dissertation, University Toronto.
 [2013a] Ermon, S.; Gomes, C.; Sabharwal, A.; Selman, B.

 Taming curse dimensionality: Discrete Proceedings integration hashing optimization.
 30th International Conference Machine Learning (ICML-13),
 [2013b] Ermon, S.; Gomes, C.
 P.; Sabharwal, A.; Selman, B.

 Embed project: Discrete sampling univer- sal hashing.
 Advances Neural Information Processing Systems (NIPS),
 [2013c] Ermon, S.; Gomes, C.
 P.; Sabharwal, A.; Selman, B.

 Optimization parity constraints: binary codes discrete integration.
 Proc.
 Conference Uncertainty Artiﬁcial Intelligence (UAI).
 [2013d] Ermon, S.; Gomes, C.
 P.; Sabharwal, A.; Selman, B.

 Taming curse dimensionality: Discrete integration hashing optimization.
 Proc.
 30th International Conference Machine Learning (ICML).
 [2014] Ermon, S.; Gomes, C.; Sabharwal, A.; Selman, B.

 Low-density parity constraints hashing-based discrete integration.
 International Conference Machine Learning,
 [1980] Fujishige, S.

 optimal base polymatroid respect weight vector.
 Mathematics Operations Research
 [1989] Greig, D.
 M.; Porteous, B.
 T.; Seheult, A.
 H.

 Exact maximum posteriori estimation binary images.
 Journal Royal Statistical Society.
 Series B (Method- ological)
 [2012] Hazan, T., Jaakkola, T.
 S.

 partition function random maximum a-posteriori perturbations.
 Langford, J., Pineau, J., eds., Proceedings International Conference Machine Learning (ICML-12),
 New York, NY, USA: ACM.
 [2016] Hazan, T.; Orabona, F.; Sarwate, A.
 D.; Maji, S.; Jaakkola, T.

 High dimensional inference ran- dom maximum a-posteriori perturbations.
 arXiv preprint arXiv:1602.03571.
 [2013] Hazan, T.; Maji, S.; Jaakkola, T.

 sam- pling Gibbs distribution random maximum a-posteriori perturbations.
 Advances Neural Informa- tion Processing Systems,
 [1971] Hopcroft, J.
 E., Karp, R.
 M.

 n5/2 algo- rithm maximum matchings bipartite graphs.
 Switch- ing Automata Theory, Annual Symposium on,
 IEEE.
 [2011] Jegelka, S.; Lin, H.; Bilmes, J.
 A.

 fast approximate submodular minimization.
 Advances Neural Information Processing Systems,
 [1996] Jerrum, M., Sinclair, A.

 markov chain monte carlo method: approach approximate counting integration.
 Approximation algorithms NP-hard prob- lems
 [2004] Jerrum, M.; Sinclair, A.; Vigoda, E.

 polynomial-time approximation algorithm permanent matrix nonnegative entries.
 Journal ACM (JACM)
 [1987] Jonker, R., Volgenant, A.

 shortest aug- menting path algorithm dense sparse linear assign- ment problems.
 Computing
 [1998] Jordan, M.
 I.; Ghahramani, Z.; Jaakkola, T.
 S.; Saul, L.
 K.

 introduction variational methods graphical models.
 NATO ASI SERIES D BEHAVIOURAL AND SOCIAL SCIENCES
 [2016] Kim, C.; Sabharwal, A.; Ermon, S.

 Exact sampling integer linear programs random perturba- tions.
 Proc.
 AAAI Conference Artiﬁcial Intelli- gence.
 [2009] Koller, D., Friedman, N.

 Probabilistic graphical models: principles techniques.
 MIT press.
 [2004] Kolmogorov, V., Zabin, R.

 energy functions be minimized graph cuts?
 IEEE transac- tions pattern analysis machine intelligence
 [1955] Kuhn, H.
 W.

 hungarian method assignment problem.
 Naval Research Logistics (NRL)
 [2002] Madras, N.
 N.

 Lectures monte carlo methods, volume
 American Mathematical Soc.
 [1989] McDiarmid, C.

 method bounded differences.
 Surveys combinatorics
 [2016] Mussmann, S., Ermon, S.

 Learning inference maximum inner product search.
 International Conference Machine Learning,
 [2017] Mussmann, S.; Levy, D.; Ermon, S.

 Fast amortized inference learning log-linear models randomly perturbed nearest neighbor search.
 UAI.
 [2009] Orlin, J.
 B.

 polynomial time algorithm submodular function minimization.
 Mathemat- ical Programming
 [2015] Oztok, U., Darwiche, A.

 top-down compiler sentential decision diagrams.
 IJCAI,
 [2004] Sang, T.; Bacchus, F.; Beame, P.; Kautz, H.
 A.; Pitassi, T.

 Combining component caching clause learning effective model counting.
 SAT.
 [2014] Shalev-Shwartz, S., Ben-David, S.

 Un- derstanding machine learning: From theory algorithms.
 Cambridge university press.
 [2006] Thurley, M.

 sharpsat-counting models advanced component caching implicit bcp.
 SAT,
 [1979] Valiant, L.
 G.

 complexity enumera- tion reliability problems.
 SIAM Journal Computing
 [2008] Wainwright, M.
 J.; Jordan, M.
 I.; al.

 Graphi- cal models, exponential families, variational inference.
 Foundations Trends R(cid:13) Machine Learning
 [2016] Zhao, S.; Chaturapruek, S.; Sabharwal, A.; Ermon, S.

 Closing gap short long xors model counting.
 AAAI,
 present formal proofs bounds sum Z(w) non-negative weight function w {−1, → [0,∞).
 readability restate results main paper.
 format proof is follows.
 bound weighted Rademacher complexity, R(w), output optimization oracle (δ(c, w), described Assumption refer slack bound.
 lower sum Z(w) R(w) apply slack bound obtain lower bound Z(w) terms δ(c, w).
 Similarly, upper sum Z(w) R(w) apply slack bound obtain upper bound Z(w) terms δ(c, w).
 tighten bounds applying optimization oracle.
 use McDiarmid’s bound (Proposition bound difference output optimization oracle (δ(c, w), described Assumption expectation, is weighted Rademacher complexity R(w).
 function constant di = McDiarmid’s bound, giving fw(c) = δ(c, w) = max x∈{−1,1}n P [|δ(c, w) − R(w)| ≥ ] ≤ exp P [|δ(c, w) − R(w)| ≥ ≤ exp {(cid:104)c, x(cid:105) + log2 w(x)}, (cid:33) (cid:32) −22(cid:80) (cid:33) j c2 ≤ .05.
 choosing c ∈ {−1, random say probability greater .95 R(w) − ≤ δ(c, w) ≤ R(w) +
 (6) Lower Bound section lower sum Z(w) R(w) apply slack bound obtain lower bound Z(w) terms δ(c, w).
 extend Massart lemma (Shalev-Shwartz Ben-David lemma weighted setting accounting log2 w(x) weight term weighted Rademacher complexity.
 lower bound Z(w) is given following Lemma: Lemma
 c ∈ {−1, sampled random, following bound holds probability greater Proof.
 begin upper bounding R(w) terms Z(w).
 Deﬁne c ∈ {−1, generated random x ∈ {−1,
 λ γ > weight functions w, wγ {−1, → [0,∞) wγ(x) = w(x)γ have (cid:40) (δ(c,w)−√ δ(c, w) − √ wmin)2 − log2 Z(w) ≥ (cid:104) Ec λ(cid:104)c, x(cid:105) + λγ log2 w(x) max (cid:105) + log2 wmin, δ(c,w)−√ wmin ≤ (cid:104) R(wγ) = Ec Ec (cid:34) Ec log2 log2 max max (cid:104) (cid:105) log2 w(x))(cid:105) ≤ (cid:104)c, x(cid:105) + γ log2 w(x) (cid:35) (cid:88) (cid:35) (cid:34)(cid:88) log2 w(x)) log2 w(x)) Jensen≤ log2 Ec have used Jensen’s inequality.
 linearity expectation independence elements ci random vector c, (cid:88) (cid:16) (cid:104) R(wγ) ≤ log2 log2 w(x)Ec log2 log2 w(x) i=1 n(cid:89) Eci (cid:2)2λcixi(cid:3)(cid:33) (cid:32) (cid:88) Using Lemma A.6 (Shalev-Shwartz Ben-David (cid:32) (cid:88) λ2||x||2 (cid:19) R(wγ) ≤ log2 (cid:18) (cid:88) log2 w(x)2 log2 log2 w(x) + log2 λ2 (cid:33) log2 (cid:33) (cid:32) (cid:88) log2 log2 w(x) (cid:32)(cid:88) i=1 (λxi)2 (cid:33) (cid:33) λn log2 w(x) log2 w(x) (cid:32)(cid:88) λn log2 (cid:33) λn w(x)w(x)λγ−1 (cid:88) (cid:33) (cid:32) i=1 (cid:32)(cid:88) (cid:33) log2 w(x)λγ log2 max w(x) λn log2 Z(w) + log2 Next, R(wγ) ≤ (cid:8)w(x)λγ−1(cid:9)(cid:88) w∗(λ, γ) = log2 Z(w) + (cid:26)wmax = maxx w(x), wmin = minx{w(x) w(x) > λγ ≥ λγ ≤ {wλγ−1 max min + λ log2 w∗(λ, γ) + λ λγ − (7) Note λγ = have valid inequalities hold choice w∗(λ, γ).
 Having bounded weighted Rademacher complexity terms Z(w), apply slack bound equation have that probability greater .95 (8) upper bound δ(w) holds λ optimize λ γ make bound tight possible.
 However, is non-trivial changing γ changes weight function supply optimization oracle.
 set γ = optimize λ.
 end section derive bound different choice γ.
 bound is trivial derive, illustrates other choices γ result meaningful bounds.
 log2 Z(w) +
 Rewriting bound Equation γ = w∗(λ) = w∗(λ, have log2 w∗(λ, γ) + λ δ(c, w) λγ − (9) optimal value λ makes bound tight possible occurs maximum quadratic function − (λ − log2 w∗(λ) − λ2 + λ(δ(c, w) − h(λ) = −λ2 − λ(log2 w∗(λ) − (δ(c, w) − h(cid:48)(λ) = −λn − log2 w∗(λ) ≤ log2 Z(w), (cid:17) (cid:16) + log2 w∗(λ) δ(c, w) h(cid:48)(cid:48)(λ) = −n, stated derivatives are valid λ (cid:54)= w∗(λ) is piecewise constant discontinuity λ
 maximum h(λ) occur λ = −∞, +∞, value λ makes h(cid:48)(λ)
 inspection maximum does occur λ = ±∞, maximum occur h(cid:48)(λ) λ derivative is zero.
 have h(cid:48)(λ) = λ = (δ(c, w) − √ (cid:17) − log2 w∗(λ))/n.
 Rearranging equation have + log2 w∗(λ) ≤ log2 Z(w).
 Depending value δ(c, w) have separate regimes optimal lower bound Z(w).
 − log2 w∗(λ) δ(c, w) − −λ2 (cid:16) + λ + log2 wmin: case h(cid:48)(λ) λ = (δ(c, w) − √ − log2 wmin)/n (note λ <
 δ(c, w) < + w∗(λ) = wmin) optimal lower bound is (δ(c, w) − √ − log2 wmin)2 + log2 wmin ≤ log2 Z(w).
 require λ > note discard bound recompute new c ﬁnd λ < computed value δ(c, w), happen low probability slack bound is violated have estimated R(w) δ(c, w).
 Note R(w) = Ec Ec (cid:2)maxx∈{−1,1}n{(cid:104)c, x(cid:105) + log2 wmin}(cid:3) = n + log2 wmin, R(w) − log2 wmin = n >
 (cid:2)maxx∈{−1,1}n{(cid:104)c, x(cid:105) + log2 w(x)}(cid:3)
 n + log2 wmin < δ(c, w) < + lower bound
 δ(c, w) > n + log2 wmax: case h(cid:48)(λ) is zero, λ have optimal δ(c, w) − ≤ log2 Z(w), − + log2 wmax: case cannot occur δ(c, w) ≤ + log2 wmax deﬁnition.
 illustrate alternative choices γ result meaningful bounds.
 Equation have (cid:20) max x∈{−1,1}n Ec (cid:21) (cid:20) {(cid:104)c, x(cid:105) + γ log2 w(x)} R(wγ) ≤ ≤ (cid:104)c, x(cid:105) + γ log2 (cid:26) Ec max x∈{−1,1}n log2 Z(w) + log2 Z(w) + w(x) w∗(λ, γ) λγ − λγ − ≤ (cid:27)(cid:21) log2 w∗(λ, γ) + λ log2 w∗(λ, γ) + λ log2 Z(w) w∗(λ, γ) + λ large γ (γ ≥ large γ, λ) have w∗(λ, γ) = wmax, makes log2 w(x) w∗(λ,γ) ≤ x.
 Further, (for c) single element x ∈ {−1, has unique largest weight (w(x) > w(y)∀x (cid:54)= y) arg x∈{−1,1}n (cid:104)c, x(cid:105) + γ log2 w(x) w∗(λ, γ) = arg x∈{−1,1}n {w(x)} (cid:27) (cid:27)(cid:21) w(x) w∗(λ, γ) =
 (cid:26) Ec max x∈{−1,1}n (cid:26) (cid:20) (cid:114) (cid:26) Therefore, large γ λ = log2 Z(w) wmax (cid:20) Ec max x∈{−1,1}n (cid:104)c, x(cid:105) + γ log2 (cid:104)c, x(cid:105) + γ log2 (cid:27)(cid:21) have w(x) w∗(λ, γ) ≤ log2 Z(w) w∗(λ, γ) + λ (cid:115) ≤ Z(w) log2 wmax wmax ≤ Z(w).
 (cid:114) λ γ ≤ λ.
 Note have used λ = λ requires bound Z(w).
 leave joint optimization λ γ future work.
 λ make γ big possible subject γ bound is trivial, picked γ poorly.
 tighten bound make γ small possible subject γ ≥ gauranteeing γ Upper Bound section upper sum Z(w) R(w) apply slack bound obtain upper bound Z(w) terms δ(c, w).
 proof technique is inspired developed method bounding sum Z(w) weight function values
 generalize proof weighted setting weight function w {−1, → [0,∞).
 principle underlying method is dividing space {−1, half n times arrive single weight.
 choosing half space recurse step bound upper bound Z(w).
 Deﬁne j dimensional space Ij = {x x ∈ {−1, j ≥ I0 = {(0)}.
 vector x ∈ Ij−1 j deﬁne x+, x− ∈ Ij single element (0) ∈ I0, deﬁne (0)+ = (1) (0)− = (−1).
 x+ = (x1, x2,


 xj−1, x− = (x1, x2,


 xj−1,−1).
 Given weight function wj Ij → [0,∞) (in section write wj denote weight function has j Z(w) wmax log2 dimensional domain denotes wn), weight functions j−1 {−1, → [0,∞), w+ w+ j−1(x) = wj(x+) have split weights original weight function new weight functions, j−1 dimensional domains (two disjoint half spaces original j dimensional domain).
 relate expectation R(wj) expectations R(w+ Lemma
 j one has j−1 {−1, → [0,∞), w− w− j−1) Lemmas
 j−1(x) = wj(x−) j−1) R(w− R(w+ j−1),R(w− j−1) ≤ R(wj) Proof.
 Given x, c ∈ Ij−1 have (cid:104)c, x(cid:105) + log2 w+ j−1(x) = ((cid:104)c+, x+(cid:105) + log2 wj(x+)) + ((cid:104)c−, x+(cid:105) + log2 wj(x+)) ≤ maxy∈Ij {(cid:104)c+, y(cid:105) + log2 wj(y)} + maxy∈Ij {(cid:104)c−, y(cid:105) + log2 wj(y)} δ(c+, wj) + δ(c−, wj) (10) inequality holds x, maximize left hand side Equation x get δ(c, w+ j−1) = max average c ∈ Ij−1 get j−1(x)(cid:9) ≤ δ(c+, wj) + δ(c−, wj) (cid:8)(cid:104)c, x(cid:105) + log2 w+ (cid:88) (cid:88) c∈Ij−1 ≤ R(w+ j−1) = δ(c, w+ j−1) δ(c+, wj) + δ(c−, wj) (cid:88) j−1) ≤ R(wj) follows same structure.
 c∈Ij c∈Ij−1 δ(c, wj) = R(wj) proof R(w− Lemma
 j one has Proof.
 Let c, x ∈ Ij−1.
 R(w− j−1) + R(w+ j−1) ≤ R(wj) − (cid:104)c, x(cid:105) + log2 w+ j−1(x) = (cid:104)c+, x+(cid:105) − + log2 w+ j−1(x) = (cid:104)c+, x+(cid:105) + log2 wj(x+) ≤ δ(c+, wj) inequality (cid:104)c, x(cid:105) + log2 w+ j−1(x) ≤ δ(c+, wj) − holds x.
 Maximizing x get δ(c, w+ j−1) ≤ δ(c+, wj)
 Similarly, maximizing x get (cid:104)c, x(cid:105) + log2 w− j−1(x) = (cid:104)c−, x−(cid:105) − + log2 w− j−1(x) = (cid:104)c−, x−(cid:105) + log2 wj(x−) ≤ δ(c−, wj) δ(c, w− j−1) ≤ δ(c−, wj)
 δ(c, w− j−1) + δ(c, w+ j−1) ≤ δ(c−, wj) + δ(c+, wj) − averaging c ∈ Ij−1 get R(w− j−1) + R(w+ j−1) ≤ j−1) c∈Ij−1 δ(c, w− j−1) + δ(c, w+ (cid:88) (cid:19) (cid:18) δ(c−, wj) + δ(c+, wj) (cid:88)  − = R(wj) −
 (cid:88) δ(c, wj) − c∈Ij−1 c∈Ij j−1) R(w− Equipped relations expectation R(wj) expectations R(w+ j−1) Lemmas are prepared upper bound Z(wn) R(wn).
 understand strategy is helpful view weight function wj {−1, → [0,∞) binary tree leaf corresponding weight.
 weight functions w+ j−1 w− j−1 correspond subtrees root nodes are children root node complete tree representing wj.
 strategy is divide original weight function wn half, picking subtrees based relative sizes (as measured sum subtree’s weights weighted Rademacher complexities).
 arrive leaf, corresponding single weight.
 allows relate weighted Rademacher complexity original weight function entire tree) Z(wn) weight single leaf.
 problem choosing subtree pick step based relative sizes is intractable.
 avoid difﬁculty do ﬁnd leaf, pick largest smallest weight entire tree, gives upper bound Z(wn) terms R(wn).
 upper bounding Z(wn) R(wn) apply slack bound obtain upper bound Z(wn) terms δ(c, wn).

 Assuming Z(w) c ∈ {−1, sampled random, following bound holds probability greater − log2 − log2 wmax (cid:1) ≈ δ(c, w) + (cid:1) − log2 (1 − βopt) + log2 wmin, (cid:1) − log2 (1 − βopt) + log2 + .58n, < βopt < < βopt < βopt = βopt = log2 Z(w) ≤  (cid:16) (cid:16) βopt log2 βopt log2 δ(c, w) + log2 wmax + n, + n log2 (cid:17)(cid:0)δ(c, w) + (cid:17)(cid:0)δ(c, w) + (cid:0)  δ(c,w)+ δ(c,w)+ βopt = wmin wmax wmin < δ(c,w)+ wmax < δ(c,w)+ wmax < δ(c,w)+ < < Proof.
 Let be parameter, be set later, such < β ≤
 construct sequence weight functions wn, wn−1,


 w1, w0 wj {−1, → [0,∞).
 Starting original weight function use rules decide wj−1 = w− j−1 wj−1 = w+ Rule Given wj, min{(cid:80) Rule Given wj, min{(cid:80) y w+ y w+ j−1(y),(cid:80) j−1(y),(cid:80) j−1.
 y w− y w− j−1(y)} < β(cid:80) (cid:40)(cid:88) j−1(y)} ≥ β(cid:80) (cid:8)R(w+ w+ wj−1 = arg max j−1 j−1,w w+ wj−1 = arg j−1 j−1,w w+ (cid:88) let j−1(y), w− j−1(y) let j−1),R(w− j−1)(cid:9) (cid:41) Note R(w0) = log2 wn(x) x ∈ In. is, dividing original space states half n times, are left single state.
 Now, given Z(w) > rule guarantees wn(x)
 proof contradiction, assume wn(x)
 requires integer i (with < i ≤ have <(cid:80) following rule makes impossible.
 y wi−1(y), ﬁrst step is relate weighted Rademacher complexity R(wn) ﬁnal leaf weight wn(x) based number y wi(y) =(cid:80) times use rule dividing original tree.
 time use rule R(wj−1) = min w+ j−1,w j−1 {R(w+ j−1),R(w− j−1)}.
 Lemma R(wj) is least large average R(w− minimum R(w− R(wj) ≥ R(wj−1) regardless use rule
 Let be number times have used Rule j−1) one, making least large j−1) one.
 R(wj) ≥ R(wj−1) + whenever use rule
 lemma have j−1) R(w+ j−1) R(w+ z∈Ij R(w) = R(wn) ≥ R(w0) + m = log2 wn(x) + m.
 relate number times use rule sum Z(wn).
 Observe that(cid:80) (cid:80) wj(z); have used rule have(cid:80) y wj−1(y) ≥ (1 − β)(cid:80) subtrees.
 have used rule then(cid:80) y wj−1(y) ≥ β(cid:80) (cid:18) β w0(y) ≥ (1 − β)n−m βm (cid:88) subtrees carry least fraction β total weight.
 wn(y) = (1 − β)n (cid:88) wn(x) = j−1(y) +(cid:80) (cid:19)m w− j−1(y) = z wj(z) picked largest z wj(z), deﬁnition use rule y∈Ij−1 y∈Ij−1 w+ − β y∈I0 y∈In Taking logarithms (cid:16) (cid:17) Note log2 (cid:19) (cid:18) β − β log2 wn(x) ≥ n log2 (1 − β) + m log2 + log2 Z(wn) −m log2 ≥ n log2 (1 − β) + log2 Z(wn) − log2 wn(x) (cid:18) β (cid:18) − β − β (cid:19) (cid:19) ≥ log2 (1 − β) + log2 Z(wn) − log2 wn(x).
 m log2 > < β < m ≥ n log2 (1 − β) + log2 Z(wn) − log2 wn(x) (cid:17) log2 n log2 (1 − β) + log2 Z(wn) − log2 wn(x) Z(wn).
 (11) (12) (13) (14) combining Equations have R(w) ≥ log2 wn(x) + Applying slack bound Equation have3 log2 (cid:17) (cid:16) (cid:16) (cid:17) log2 δ(c, w) ≥ log2 wn(x) + n log2 (1 − β) + log2 Z(wn) − log2 probability greater .95.
 choose β optimize bound.
 Note bound Equation contains term wn(x), is intractable ﬁnd leaf following procedure outlined above.
 use smallest largest weight depending value β.
 weights cannot be computed pick β eliminate wn(x) bound.
 Depending value β have cases outlined
 β = quantity log2 (cid:17) δ(c, w) ≥ log2 Z(wn) − log2(3/2) − =
 bound is scale invariant; scale weight function constant w(cid:48)(x) = aw(x) R(w(cid:48)) = log2 + R(w), log2 Z(w(cid:48) = log2 + log2 Z(wn), log2 w(cid:48) n(x) = log2 + log2 wn(x) bound remains unchanged.

 < β < quantity − log2( β > δ(c, w) ≥ log2 (1 − β) + log2 Z(wn)
 < β < .5, quantity − log2( β < δ(c, w) ≥ log2 (1 − β) + log2 Z(wn) (cid:16) (cid:17) (cid:16) (cid:17) log2 log2 (cid:32) − + log2 wmin − + log2 wmax (cid:17)(cid:33) (cid:17)(cid:33) (cid:16) (cid:16) log2 log2

 β = .5, quantity β = recover trivial bound log2 Z(wn) ≤ log2 wmax + n equation
 choose best value β minimize upper bound log2 Z(wn) respect β.
 upper bound log2 Z(wn) is (cid:18) − β (cid:19)(cid:16) δ(c, w) + (cid:17) − log2 (1 − β) + log2 w∗(β) − log2 w∗(β) (cid:18) − β − log2 (1 − β) + log2 w∗(β), log2 log2 Z(wn) ≤ L(β) = log2
 Differentiating = δ(c, w) + w∗(β) = wmin = minx{w(x) w(x) > β < get (cid:16)− L(cid:48)(β) = (cid:17) β2 − (1 − β)2 ﬁrst derivative has root β = L(cid:48)(cid:48)(β) = aβ (cid:16)− − log2 w∗(β), (cid:17) w∗(β) = wmax = maxx w(x) β > β2 − (cid:17) − β β2 − − β n = n3 β3 + β2 − β (1 − β)2 − β (cid:17) n, L(cid:48)(cid:48)( a(n−a).
 deﬁnition only meaningful values β are < β ≤ .5, < < n/2 second derivative is positive making root minimum n is valid range β minimum occurs endpoint range.
 Note > have limβ→0 L(β) = ∞, minimum occurs endpoint β =
 slack bound has been violated have estimated R(w) δ(c, is appropriate sample new c recompute δ(c, w).
 (To see this, note R(w) = Ec R(w) − log2 wmin = n > Also, lim→0 L( − ) (at β = term w∗(β) doesn’t appear bound doesn’t matter w∗(β) = wmin w∗(β) wmax).
 Assuming have sampled c such means optimal value β minimizes upper bound is: (cid:2)maxx∈{−1,1}n{(cid:104)c, x(cid:105) + log2 wmin}(cid:3) = n + log2 wmin, (cid:2)maxx∈{−1,1}n{(cid:104)c, x(cid:105) + log2 w(x)}(cid:3) ≥ Ec + ) = lim→0 L( δ(c,w)+ δ(c,w)+  (cid:17)(cid:0)δ(c, w) + (cid:17)(cid:0)δ(c, w) + (cid:0) + n log2 wmin wmax wmin < δ(c,w)+ wmax < δ(c,w)+ wmax < δ(c,w)+ < < − log2 wmin − log2 wmax (cid:1) ≈ δ(c, w) + (cid:1) − log2 (1 − βopt) + log2 wmin, (cid:1) − log2 (1 − βopt) + log2 + .58n, βopt = upper bound Z(wn) is given  (cid:16) (cid:16) βopt log2 βopt log2 δ(c, w) + log2 wmax + n, log2 Z(wn) ≤ < βopt < < βopt < βopt = βopt = Tightening Slack improve high probability bounds δ(c, w) terms Z(w) generating k independent vectors c1, c2,


 ck ∈ {−1, applying optimization oracle Assumption each, taking mean ¯δk(w) = (δ(c1, w) + ··· + δ(ck, w))/k.
 gives bounds log2 w∗(β)+ n log2 (1 − β) + log2 Z(w) − log2 w∗(β) log2 w∗(λ, γ)+λ (cid:114) (cid:114) log2 Z(w)+ λγ − ≤ ¯δk(w) ≤ last sections inverted bounds optimized β λ obtain high probability bounds Z(w) terms δ(c, w).
 process is unchanged, exceptions replace δ(w, c) (computed single c ∈ {−1, ¯δk(w) term w(cid:48)(c1, ..., ck) = w(c1) × ··· × w(ck) c1,


 ck ∈ {−1,
 sum new function’s weights is Proof: recall weight function w {−1, → [0,∞).
 Let’s deﬁne new weight function w(cid:48) {−1, → [0,∞) k
 log2 (cid:17) (cid:16) (cid:113) (cid:88) w(cid:48)(x) (cid:88) (cid:88) x1∈{−1,1}n Z(w(cid:48)) = ··· (cid:88) w(x1) × ··· × (cid:88) xk∈{−1,1}n x∈{−1,1}kn w(x1) × ··· × w(xk) w(xk) xk∈{−1,1}n x1∈{−1,1}n note largest smallest non-zero weights w(cid:48) are w(cid:48) c(cid:48) = (c1, .., ck).
 value δ(w(cid:48), c(cid:48)) new weight function is max = wk = Z(w)k.
 max w(cid:48) min = wk min.
 Deﬁne c(cid:48) ∈ {−1, δ(w(cid:48), c(cid:48)) = max {log2 w(x1) + (cid:104)c1, x1(cid:105)} + ··· + max x(cid:48)∈{−1,1}nk xk∈{−1,1}n = max x1∈{−1,1}n {log2 w(cid:48)(x(cid:48)) + (cid:104)c(cid:48), x(cid:48)(cid:105)} {log2 w(xk) + (cid:104)ck, xk(cid:105)} = k¯δk(w).
 lower ¯δk(w) applying bound Equation w(cid:48) (recalling w∗(β) = wmin w∗(β) = wmax) ﬁnd ¯δk(w) = δ(w(cid:48), c(cid:48)) ≥ log2 w∗(β)k nk log2 (1 − β) + log2 Z(w)k − log2 w∗(β)k = log2 w∗(β) + log2 (1 − β) + log2 Z(w) − log2 w∗(β) k log2 (cid:17) (cid:16) (cid:17) (cid:16) log2 Similarly, upper bound ¯δk(w) apply bound Equation w(cid:48) ﬁnd nk log2 w∗(λ, γ) + λ log2 w∗(λ, γ)k + λ λγ − log2 Z(w)k + log2 Z(w) + δ(w(cid:48), c(cid:48)) λγ − ¯δk(w) = ≤ (cid:18) (cid:114) (cid:19) (cid:114)
 Convolutional networks are able detect local patterns position image.
 patterns planar image, patterns sphere move case “move” is rotation translation.
 analogy planar CNN, like build network detect patterns are rotated sphere.
 shown Figure is good way use translational convolution cross-correlation1 analyze spherical signals.
 obvious approach, then, is change deﬁnition cross- correlation replacing ﬁlter translations rotations.
 Doing so, run subtle important difference plane sphere: space moves plane (2D translations) is isomorphic plane, space moves sphere (3D rotations) is different, three-dimensional manifold called SO(3)2.
 follows result spherical correlation (the output feature map) is be considered signal SO(3), signal sphere, S2.
 reason, deploy SO(3) group correlation higher layers spherical CNN (Cohen Welling,
 Figure Any planar projec- tion spherical signal re- sult distortions.
 Rotation spherical signal cannot be emu- lated translation planar projection.
 ∗Equal contribution name, CNNs use cross-correlation convolution forward pass.
 paper use term cross-correlation, correlation short.
 be more precise: symmetry group plane contains more translations, translations form subgroup acts plane.
 case sphere is coherent way deﬁne composition points sphere, sphere cannot act (it is group).
 reason, consider whole SO(3).
 Published conference paper ICLR implementation spherical CNN (S2-CNN) involves major challenges.
 square grid pixels has discrete translation symmetries, symmetrical grids sphere exist.
 means is simple way deﬁne rotation spherical ﬁlter pixel.
 Instead, order rotate ﬁlter need perform kind interpolation.
 other challenge is computational efﬁciency; SO(3) is three-dimensional manifold, naive implementation SO(3) correlation is O(n6).
 address problems using techniques non-commutative harmonic analysis (Chirikjian Kyatkin, Folland,
 ﬁeld presents far-reaching generalization Fourier transform, is applicable signals sphere rotation group.
 is known SO(3) correlation satisﬁes Fourier theorem respect SO(3) Fourier transform, same is true deﬁnition S2 correlation.
 Hence, S2 SO(3) correlation be implemented using generalized FFT algorithms.
 are ﬁrst use cross-correlation continuous group multi-layer neural network, evaluate degree mathematical properties predicted continuous theory hold practice discretized implementation.
 demonstrate utility spherical CNNs rotation invariant classiﬁcation regression problems experiments datasets.
 show spherical CNNs are much better rotation invariant classiﬁcation Spherical MNIST images planar CNNs. Second, use CNN classifying shapes.
 third experiment use model molecular energy regression, important problem computational chemistry.
 CONTRIBUTIONS main contributions work are following:
 theory spherical CNNs.
 ﬁrst differentiable implementation generalized Fourier transform S2 SO(3).
 PyTorch code is easy use, fast, memory efﬁcient.

 ﬁrst empirical support utility spherical CNNs rotation-invariant learning problems.
 RELATED WORK is understood power CNNs stems large part ability exploit (translational) symmetries combination weight sharing translation equivariance.
 becomes natural consider generalizations exploit larger groups symmetries, has been subject several recent papers Gens Domingos (2014); Olah (2014); Dieleman et al.
 Cohen Welling (2016); Ravanbakhsh al.
 (2017); Zaheer et al.
 (2017b); Guttenberg et al.
 (2016); Cohen Welling (2017).
 exception SO(2)-steerable networks (Worrall et al., Weiler et networks are limited discrete groups, such discrete rotations acting planar images permutations acting point clouds.
 Other recent work is concerned analysis spherical images, does deﬁne equivariant architecture (Su Grauman, Boomsma Frellsen,
 work is ﬁrst achieve equivariance continuous, non-commutative group (SO(3)), ﬁrst use generalized Fourier transform fast group correlation.
 preliminary version work appeared Cohen et al.
 (2017).
 perform cross-correlations sphere rotation group, use generalized FFT algorithms.
 Generalized Fourier analysis, called abstract- noncommutative harmonic analysis, has long history mathematics many books have been written subject (Sugiura, Taylor, Folland,
 good engineering-oriented treatment covers generalized FFT algorithms, see (Chirikjian Kyatkin,
 Other important works include (Driscoll Healy, Healy Potts Kunis Potts, Drake Maslen, Rockmore, Kostelec Rockmore, Potts Makadia Gutman et
 Published conference paper ICLR CORRELATION ON THE SPHERE AND ROTATION GROUP explain S2 SO(3) correlation analogy classical planar Z2 correlation.
 planar correlation be understood follows: value output feature map translation x ∈ Z2 is computed inner product input feature map ﬁlter, shifted x.
 Similarly, spherical correlation be understood follows: value output feature map evaluated rotation R ∈ SO(3) is computed inner product input feature map ﬁlter, rotated R.
 output feature map is indexed rotation, is modelled function SO(3).
 discuss issue more detail shortly.
 above deﬁnition refers various concepts have deﬁned mathematically.
 follows, go required concepts provide precise deﬁnition.
 goal section is present mathematical model spherical CNNs. Generalized Fourier theory implementation details be treated later.
 Unit Sphere S2 be deﬁned set points x ∈ R3 norm
 is two-dimensional manifold, be parameterized spherical coordinates α ∈ β ∈ [0, π].
 Spherical Signals model spherical images ﬁlters continuous functions f S2 → RK, K is number channels.
 Rotations set rotations dimensions is called SO(3), “special orthogonal group”.
 Rotations be represented × matrices preserve distance (i.e. ||Rx|| = ||x||) orientation (det(R) = +1).
 represent points sphere unit vectors perform rotation using matrix-vector product Rx. rotation group SO(3) is three-dimensional manifold, be parameterized ZYZ-Euler angles α ∈ [0, β ∈ [0, π], γ ∈ [0,
 Rotation Spherical Signals order deﬁne spherical correlation, need know rotate points x ∈ S2 rotate ﬁlters (i.e. functions) sphere.
 end, introduce rotation operator LR takes function f produces rotated function LRf composing f rotation R−1: [LRf ](x) = f (R−1x).
 Due inverse R, have LRR(cid:48) = LRLR(cid:48).
 Inner products inner product vector space spherical signals is deﬁned as: (1) (2) K(cid:88) S2 (cid:104)ψ, f(cid:105) = (cid:90) S2 f (Rx)dx =(cid:82) (cid:90) (cid:90) (cid:104)LRψ, f(cid:105) = ψk(x)fk(x)dx, k=1 measure ensures integration measure dx denotes standard rotation invariant integration measure sphere, be expressed dα sin(β)dβ/4π spherical coordinates (see Appendix A).
 invariance S2 f (x)dx, rotation R ∈ SO(3).
 is, volume spherical heightmap does change rotated.
 Using fact, show K(cid:88) LR−1 is adjoint LR, implies LR is unitary: K(cid:88) ψk(R−1x)fk(x)dx (3) k=1 S2 ψk(x)fk(Rx)dx S2 = (cid:104)ψ, LR−1 f(cid:105).
 k=1 Spherical Correlation ingredients place, are ready state was stated words before.
 spherical signals f ψ, deﬁne correlation as: [ψ (cid:63) f ](R) = (cid:104)LRψ, f(cid:105) = ψk(R−1x)fk(x)dx.
 (4) (cid:90) K(cid:88) S2 k=1 Published conference paper ICLR mentioned output spherical correlation is function SO(3).
 is counterintuitive, conventional deﬁnition spherical convolution gives output function sphere.
 However, shown Appendix B, conventional deﬁnition restricts ﬁlter be symmetric Z axis, limit expressive capacity network.
 Rotation SO(3) Signals deﬁned rotation operator LR spherical signals (eq.
 used deﬁne spherical cross-correlation (eq.

 deﬁne SO(3) correlation, need generalize rotation operator act signals deﬁned SO(3).
 show, reusing eq.
 is way go.
 is, f SO(3) → RK, R, Q ∈ SO(3): [LRf ](Q) = f (R−1Q).
 (5) Note argument R−1x Eq. denotes rotation x ∈ S2 R−1 ∈ SO(3), analogous term R−1Q Eq. denotes composition rotations (i.e. matrix multiplication).
 Rotation Group Correlation Using same analogy before, deﬁne correlation signals rotation group, f, ψ SO(3) → RK, follows: (cid:90) K(cid:88) [ψ (cid:63) f ](R) = (cid:104)LRψ, f(cid:105) = ψk(R−1Q)fk(Q)dQ.
 (6) SO(3) k=1 integration measure dQ is invariant measure SO(3), be expressed ZYZ-Euler angles dα sin(β)dβdγ/(8π2) (see Appendix A).
 Equivariance have seen, correlation is deﬁned terms rotation operator LR.
 operator acts input space network, justiﬁcation do have using second layer beyond?
 justiﬁcation is provided important property, shared kinds convolution correlation, called equivariance.
 layer Φ is equivariant Φ ◦ LR = TR ◦ Φ, operator TR.
 Using deﬁnition correlation unitarity LR, showing equivariance is liner: [ψ (cid:63) [LQf ]](R) = (cid:104)LRψ, LQf(cid:105) = (cid:104)LQ−1Rψ, f(cid:105) = [ψ (cid:63) f ](Q−1R) = [LQ[ψ (cid:63) f ]](R).
 derivation is valid spherical correlation rotation group correlation.
 FAST SPHERICAL CORRELATION WITH G-FFT is known correlations convolutions be computed using Fast Fourier Transform (FFT).
 is result Fourier theorem, states (cid:91)f ∗ ψ = ˆf · ˆψ.
 FFT be computed O(n log n) time product · has linear complexity, implementing correlation using FFTs is naive O(n2) spatial implementation.
 functions sphere rotation group, is analogous transform, refer generalized Fourier transform (GFT) corresponding fast algorithm (GFFT).
 transform ﬁnds roots representation theory groups, due space constraints go details refer interested reader Sugiura (1990) Folland (1995).
 Conceptually, GFT is nothing linear projection function set orthogonal basis functions called “matrix element irreducible unitary representations”.
 circle (S1) line are familiar complex exponentials exp(inθ).
 SO(3), have Wigner D- mn(R) indexed l ≥ −l ≤ m, n ≤ l.
 S2, are spherical harmonics3 functions Dl m(x) indexed l ≥ −l ≤ m ≤ l.
 Y l Denoting manifold (S2 SO(3)) X corresponding basis functions U l (which is vector-valued (Y l) matrix-valued write GFT function f X → R ˆf l = f (x)U l(x)dx.
 (8) S2 is group does have irreducible representations, is quotient groups SO(3)/ SO(2) have relation Y l m = Dl Published conference paper ICLR b(cid:88) l=0 l(cid:88) l(cid:88) m=−l integral be computed using GFFT algorithm (see Section
 inverse SO(3) Fourier transform is deﬁned as: f (R) = (2l + ˆf l mnU l mn(R), (9) S2.
 maximum frequency b is known bandwidth, is related resolution spatial grid (Kostelec Rockmore,
 Using well-known (in fact, deﬁning) property Wigner D-functions Dl(R)Dl(R(cid:48)) = Dl(RR(cid:48)) Dl(R−1) = Dl(R)†, be shown (see Appendix D) SO(3) correlation satisﬁes Fourier theorem4: (cid:91)ψ (cid:63) f = ˆf · ˆψ†, · denotes matrix multiplication block matrices ˆf ˆψ†.
 derive analogous S2 convolu- Similarly, using Y (Rx) = D(R)Y (x) Y l tion theorem: (cid:91)ψ (cid:63) f = ˆf l · ˆψl†, ˆf l ˆψl are
 says SO(3)-FT S2 correlation spherical signals be computed taking outer product S2-FTs signals.
 is shown ﬁgure
 m = Dl Figure Spherical correlation spectrum.
 signal f locally-supported ﬁlter ψ are Fourier block-wise tensored, summed input channels, inverse transformed.
 Note ﬁlter is supported, is use matrix multiplication (DFT) FFT algorithm it.
 parameterize sphere using spherical coordinates α, β, SO(3) ZYZ-Euler angles α, β, γ.
 IMPLEMENTATION G-FFT AND SPECTRAL G-CONV sketch implementation GFFTs. details, see (Kostelec Rockmore,
 input SO(3) FFT is spatial signal f SO(3), sampled discrete grid stored array.
 axes correspond ZYZ-Euler angles α, β, γ.
 ﬁrst step SO(3)-FFT is perform standard translational FFT α γ axes.
 FFT’ed axes correspond m, n axes result.
 second last step is linear contraction β axis FFT’ed array precomputed array samples Wigner-d (small-d) functions dl mn(β).
 shape dl depends l (it is (2l (2l linear contraction is implemented mn l ≥ m ≥ −l custom GPU kernel.
 output is set Fourier coefﬁcients ˆf l l =


 Lmax.
 algorithm S2-FFTs is similar, case FFT α axis only, do linear contraction precomputed Legendre functions β axis.
 code is available https://github.com/jonas-koehler/s2cnn.
 EXPERIMENTS ﬁrst sequence experiments, evaluate numerical stability accuracy algorithm.
 second sequence experiments, showcase new cross-correlation layers have result is valid real functions.
 complex functions, conjugate ψ left hand side.
 SO(3) IFFTS² FFTS² DFT Published conference paper ICLR introduced are useful building blocks several real problems involving spherical signals.
 examples are recognition shapes predicting atomization energy molecules.
 EQUIVARIANCE ERROR ReLU layer act.
 res.
 act.
 layer paper have presented ﬁrst in- stance group equivariant CNN con- tinuous, non-commutative group.
 dis- crete case, prove network is prove [LRf ]∗ ψ = LR[f ∗ ψ] continuous functions f ψ sphere rotation group, is true discretized version compute.
 Hence, is reasonable ask are signiﬁcant discretization ar- tifacts affect equivariance properties network.
 equivariance be maintained many layers, ex- pect weight sharing scheme become much less effective.
 tested equivariance SO(3) correlation various resolutions b.
 do ﬁrst sampling n = ran- (cid:80)n dom rotations Ri n feature maps fi K = channels.
 i=1 std(LRi Φ(fi) − compute ∆ = Φ(LRifi))/ std(Φ(fi)), Φ is composi- tion SO(3) correlation layers initialized ﬁlters.
 case perfect equivariance, expect quantity be zero.
 results (ﬁgure (top)), show approximation error ∆ grows resolution number layers, stays manageable range resolutions interest.
 repeat experiment ReLU activation function correlation operation.
 shown ﬁgure error is higher stays ﬂat.
 indicates error is due network layers, due feature map rotation, is exact bandlimited functions.
 Figure ∆ function resolution number layers.
 resolution ReLU res.
 layers ROTATED MNIST ON THE SPHERE experiment evaluate generalization performance respect rotations input.
 testing propose version MNIST dataset projected sphere (see ﬁg.

 created instances dataset: digit is projected northern hemisphere projected digit is randomly rotated.
 Architecture Hyperparameters baseline model, use simple CNN lay- ers conv-ReLU-conv-ReLU-FC-softmax, ﬁlters size × k = chan- nels, stride layers.
 compare spherical CNN layers S2conv-ReLU- SO(3)conv-ReLU-FC-softmax, bandwidth b = k = channels.
 models have parameters.
 Figure MNIST digits projected sphere using stereographic projection.
 Mapping plane results non-linear distortions.
 Results trained model non- rotated (NR) rotated (R) training set evaluated non-rotated rotated test set.
 See table
 planar CNN achieves high accuracy NR / NR regime, performance R / R regime is much worse, spherical CNN is unaffected.
 trained Published conference paper ICLR Figure ray line is cast surface sphere direction center.
 ﬁrst intersection model gives values signal sphere.
 images right represent spherical signals (α, β) representation.
 contain distance sphere cosine ray normal model.
 red dot corresponds pixel set red line.
 non-rotated dataset evaluated rotated dataset (NR / R), planar CNN does better random chance.
 spherical CNN shows slight decrease performance compared R/R, performs well.
 planar spherical NR / NR R / R NR / R Table Test accuracy networks evaluated spherical MNIST dataset.
 R = rotated, NR = non-rotated X / Y denotes, network was trained X evaluated Y.
 RECOGNITION SHAPES Next, applied S2CNN shape classiﬁcation.
 SHREC17 task (Savva contains models taken ShapeNet dataset (Chang have be classiﬁed common categories (tables, airplanes, persons, etc.).
 is aligned regular dataset version models are perturbed rotations.
 concentrate latter test quality rotation equivariant representations learned S2CNN.
 Representation project meshes enclosing using straightforward ray casting scheme (see Fig.

 point sphere send ray origin collect types information intersection: ray length cos / sin surface angle.
 further augment information ray casting information convex hull model, total gives channels signal.
 signal is discretized using Driscoll-Healy grid (Driscoll Healy, bandwidth b =
 Ignoring non-convexity surfaces assume projection captures enough information shape be useful recognition task.
 Architecture Hyperparameters network consists initial S2conv-BN-ReLU block followed SO(3)conv-BN-ReLU blocks.
 resulting ﬁlters are pooled using max pooling layer followed last batch normalization fed linear layer ﬁnal classiﬁcation.
 is important note max pooling happens group SO(3): fk is k-th ﬁlter ﬁnal layer (a function SO(3)) result pooling is maxx∈SO(3) fk(x).
 used features S2 SO(3) layers, respectively.
 Further, layer reduce resolution b, ﬁnal layer.
 ﬁlter kernel ψ SO(3) has non-local support, ψ(α, β, γ) (cid:54)= iff β = π γ = number points discretization is proportional bandwidth layer.
 ﬁnal network contains ≈ parameters, takes memory batch size takes hours train.
 ray castingfrom sphere origindistance sphere-impactnormal impact Published conference paper ICLR P@N Method Tatsuma_ReVGG Furuya_DLAN SHREC16-Bai_GIFT Deng_CM-VGG5-6DB Ours (3rd) R@N (2nd) F1@N (3rd) mAP (2nd) NDCG (2nd) Table Results best competing methods SHREC17 competition.
 Results evaluated trained model using ofﬁcial metrics compared top competitors category (see table results).
 precision F1@N, model ranks is runner other metric.
 main competitors, Tatsuma_ReVGG Furuya_DLAN use input representations network architectures are specialized SHREC17 task.
 Given task agnostic architecture model lossy input representation use, interpret models performance strong empirical support effectiveness Spherical CNNs. PREDICTION OF ATOMIZATION ENERGIES FROM MOLECULAR GEOMETRY Finally, apply S2CNN molecular energy regression.
 QM7 task (Blum Reymond, Rupp atomization energy molecules has be predicted geometry charges.
 Molecules contain N = atoms T = types (H, C, N, O, S).
 are given list positions pi charges zi atom i.
 Representation Coulomb matrices Rupp et al.
 (2012) propose rotation translation invariant representation molecules deﬁning Coulomb matrix C ∈ RN×N (CM).
 pair atoms i (cid:54)= j set Cij = (zizj)/(|pi − pj|) Cii =
 Diagonal elements encode atomic energy nuclear charge, other elements encode Coulomb repulsion atoms.
 representation is permutation invariant.
 end Rupp et al.
 (2012) propose distance measure Coulomb matrices used Gaussian kernels whereas Montavon al.
 (2012) propose sorting C random sampling index permutations.
 functions Uz(x) =(cid:80) Representation spherical signal utilize spherical symmetries geometry deﬁning sphere Si pi atom i.
 radius is kept uniform atoms molecules chosen minimal such intersections spheres training set happen.
 Generalizing Coulomb approach deﬁne possible z point x Si potential zi·z |x−pi| producing T channel spherical signal atom molecule (see ﬁgure
 representation is invariant respect translations equivariant respect rotations.
 However, is permutation invariant.
 signal is discretized using Driscoll-Healy (Driscoll Healy, grid bandwidth b representing molecule sparse N × T × × tensor.
 j(cid:54)=i,zj =z Architecture Hyperparameters use deep ResNet style S2CNN.
 ResNet block is made S2/SO(3)conv-BN-ReLU-SO(3)conv-BN input is added result.
 share weights atoms making ﬁlters permutation invariant, pushing atom dimension batch dimension.
 layer downsample bandwidth, increasing number features F
 integrating signal SO(3) molecule becomes N × F tensor.
 permutation invariance atoms follow Zaheer et al.
 (2017a) embed resulting feature vector atom latent space using MLP φ.
 sum latent representations atom dimension get ﬁnal regression value molecule mapping MLP ψ.
 φ ψ are
 Training simple MLP frequencies atom types molecule gives RMSE ∼
 Thus, train S2CNN residual only, improved convergence speed stability direct training.
 ﬁnal architecture is sketched table
 has parameters, memory batch size takes hours train.
 Published conference paper ICLR Figure ﬁve potential channels z ∈ {1, molecule containing atoms H (red), C (green), N (orange), O (brown), S (gray).
 Method Author RMSE MLP / random CM (a) LGIKA(RF) (b) RBF kernels / random CM (a) RBF kernels / sorted CM (a) MLP / sorted CM (a) Ours S2CNN Layer Input ResBlock ResBlock ResBlock ResBlock ResBlock DeepSet Layer φ (MLP) ψ (MLP) Features Bandwidth Input/Hidden Table Left: Experiment results QM7 task: (a) Montavon al.
 (2012) (b) Raj al.
 (2016).
 Right: ResNet architecture molecule task.
 Results evaluate RMSE compare results Montavon al.
 (2012) Raj et al.
 (2016) (see table
 learned representation kernel-based approaches MLP trained sorted Coulomb matrices.
 Superior performance be achieved MLP trained permuted Coulomb matrices.
 However, sufﬁcient sampling random permutations grows N, method is unlikely scale large molecules.
 DISCUSSION CONCLUSION paper have presented theory Spherical CNNs evaluated important learning problems.
 have deﬁned S2 SO(3) cross-correlations, analyzed properties, implemented Generalized FFT-based correlation algorithm.
 numerical results conﬁrm stability accuracy algorithm, deep networks.
 have shown Spherical CNNs generalize rotations, achieve state-of-the-art results competitive Model Recognition Molecular Energy Regression challenges, excessive feature engineering task-tuning.
 volumetric tasks model recognition, believe further improvements be attained generalizing further SO(3) roto-translation group SE(3).
 development Spherical CNNs is important ﬁrst step direction.
 interesting generalization is development Steerable CNN sphere (Cohen Welling, make possible analyze vector ﬁelds such global wind directions, other sections vector bundles sphere.
 exciting future application Spherical CNN is omnidirectional vision.
 little omnidirectional image data is available public repositories, increasing prevalence omnidirectional sensors drones, robots, autonomous cars makes compelling application work.
 Published conference paper ICLR REFERENCES L.
 C.
 Blum J.-L.
 Reymond.
 small molecules virtual screening chemical universe database GDB-13.
 J.
 Am. Chem.
 Soc.,
 W.
 Boomsma J.
 Frellsen.
 Spherical convolutions application molecular modelling.
 Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, R Garnett, editors, Advances Neural Information Processing Systems pages
 Curran Associates, Inc.,
 A.X. Chang, T.
 Funkhouser, L.
 Guibas, P.
 Hanrahan, Q.
 Huang, Z.
 Li, S.
 Savarese, M.
 Savva, S.
 Song, H.
 Su, al.
 Shapenet: information-rich model repository.
 arXiv preprint arXiv:1512.03012,
 G.S. Chirikjian A.B. Kyatkin.
 Engineering Applications Noncommutative Harmonic Analysis.
 CRC Press, edition, 2001.
 ISBN
 T.S. Cohen M.
 Welling.
 Group equivariant convolutional networks.
 Proceedings International Conference Machine Learning (ICML), volume pages
 T.S. Cohen M.
 Welling.
 Steerable CNNs. ICLR,
 T.S. Cohen, M.
 Geiger, J.
 Koehler, M.
 Welling.
 Convolutional networks spherical signals.
 ICML Workshop Principled Approaches Deep Learning,
 S.
 Dieleman, K.
 W.
 Willett, J.
 Dambre.
 Rotation-invariant convolutional neural networks galaxy morphology prediction.
 Monthly Notices Royal Astronomical Society,
 S.
 Dieleman, J.
 De Fauw, K.
 Kavukcuoglu.
 Exploiting Cyclic Symmetry Convolutional Neural Networks.
 International Conference Machine Learning (ICML),
 J.B. Drake, P.H. Worley, E.F. D’Azevedo.
 Spherical harmonic transform algorithms.
 ACM Trans.
 Math.
 Softw.,
 doi:
 J.R. Driscoll D.M. Healy.
 Computing Fourier transforms convolutions 2-sphere.
 Advances applied mathematics,
 G.B. Folland.
 Course Abstract Harmonic Analysis.
 CRC Press,
 R.
 Gens P.
 Domingos.
 Deep Symmetry Networks.
 Advances Neural Information Processing Systems (NIPS),
 B.
 Gutman, Y.
 Wang, T.
 Chan, P.M. Thompson, others.
 Shape registration spherical cross correlation.
 MICCAI workshop,
 N.
 Guttenberg, N.
 Virgo, O.
 Witkowski, H.
 Aoki, R.
 Kanai.
 Permutation-equivariant neural networks applied dynamics prediction.

 D.
 Healy, D.
 Rockmore, P.
 Kostelec, S.
 Moore.
 FFTs 2-Sphere – Improvements Variations.
 journal Fourier analysis applications,
 P.J. Kostelec D.N. Rockmore.
 SOFT: SO(3) Fourier Transforms.

 URL http://www.
 cs.dartmouth.edu/~geelong/soft/soft20_fx.pdf.
 P.J. Kostelec D.N. Rockmore.
 FFTs rotation group.
 Journal Fourier Analysis Applications,
 S.
 Kunis D.
 Potts.
 Fast spherical Fourier algorithms.
 Journal Computational Applied Mathematics,
 A.
 Makadia, C.
 Geyer, K.
 Daniilidis.
 Correspondence-free structure motion.
 Int.
 J.
 Comput.
 Vis., December
 D.K. Maslen.
 Efﬁcient Computation Fourier Transforms Compact Groups.
 Journal Fourier Analysis Applications,
 Published conference paper ICLR G.
 Montavon, K.
 Hansen, S.
 Fazli, M.
 Rupp, F.
 Biegler, A.
 Ziehe, A.
 Tkatchenko, O.A. von Lilienfeld, K.
 Müller.
 Learning invariant representations molecules atomization energy prediction.
 P.
 Bartlett, F.C.N. Pereira, C.J.C. Burges, L.
 Bottou, K.Q. Weinberger, editors, Advances Neural Information Processing Systems pages

 L.
 Nachbin.
 Haar Integral.

 C.
 Olah.
 Groups Group Convolutions,
 URL https://colah.github.io/posts/ 2014-12-Groups-Convolution/.
 D.
 Potts, G.
 Steidl, M.
 Tasche.
 Fast stable algorithms discrete spherical Fourier transforms.
 Linear Algebra Applications,
 D.
 Potts, J.
 Prestin, A.
 Vollrath.
 fast algorithm nonequispaced Fourier transforms rotation group.
 Numerical Algorithms, pages
 A.
 Raj, A.
 Kumar, Y.
 Mroueh, P.T. Fletcher, al.
 Local group invariant representations orbit embeddings.
 arXiv preprint arXiv:1612.01988,
 S.
 Ravanbakhsh, J.
 Schneider, B.
 Poczos.
 Deep learning sets point clouds.
 International Conference Learning Representations (ICLR) – workshop
 D.N. Rockmore.
 Recent Progress Applications Group FFTS.
 NATO Science Series II: Mathematics, Physics Chemistry,
 M.
 Rupp, A.
 K.-R.
 Müller, O.
 A.
 von Lilienfeld.
 Fast accurate modeling molecular atomization energies machine learning.
 Physical Review Letters,
 M.
 Savva, F.
 Yu, H.
 Su, A.
 Kanezaki, T.
 Furuya, R.
 Ohbuchi, Z.
 Zhou, R.
 Yu, S.
 Bai, X.
 Bai, M.
 Aono, A.
 Tatsuma, S.
 Thermos, A.
 Axenopoulos, G.
 Th. Papadopoulos, P.
 Daras, X.
 Deng, Z.
 Lian, B.
 Li, H.
 Johan, Y.
 Lu, S.
 Mk. Large-Scale Shape Retrieval ShapeNet Core55.
 Ioannis Pratikakis, Florent Dupont, Maks Ovsjanikov, editors, Eurographics Workshop Object Retrieval.
 Eurographics Association,
 ISBN
 doi:
 Y.C. Su K.
 Grauman.
 Learning spherical convolution fast features imagery.
 Adv.
 Neural Inf.
 Process.

 M.
 Sugiura.
 Unitary Representations Harmonic Analysis.
 John Wiley Sons, New York, London, Sydney, Toronto, edition,
 M.E. Taylor.
 Noncommutative Harmonic Analysis.
 American Mathematical Society,
 ISBN
 M.
 Weiler, F.A. Hamprecht, M.
 Storath.
 Learning steerable ﬁlters rotation equivariant CNNs.
 D.E. Worrall, S.J. Garbin, D.
 Turmukhambetov, G.J. Brostow.
 Harmonic networks: Deep translation rotation equivariance.
 CVPR,
 M.
 Zaheer, S.
 Kottur, S.
 Ravanbakhsh, B.
 Poczos, R.
 Salakhutdinov, A.
 Smola.
 Deep sets.
 arXiv preprint arXiv:1703.06114,
 M.
 Zaheer, S.
 Kottur, S.
 Ravanbakhsh, B.
 Poczos, R.R. Salakhutdinov, A.J. Smola.
 Deep sets.
 Advances Neural Information Processing Systems pages
 Published conference paper ICLR APPENDIX A: PARAMETERIZATION OF AND INTEGRATION ON S2 AND SO(3) use ZYZ Euler parameterization SO(3).
 element R ∈ SO(3) is written R = R(α, β, γ) = Z(α)Y (β)Z(γ), (10) α ∈ [0, β ∈ [0, π] γ ∈ [0, Z resp.
 Y are rotations Z Y axes.
 Using parameterization, normalized Haar measure is dγ dβ sin(β) dR have (cid:82) is called invariant measure has property that(cid:82) (cid:82) SO(3) f (R)dR (this is analogous more familiar property(cid:82) SO(3) dR =
 Haar measure (Nachbin, Chirikjian Kyatkin, SO(3) f (R(cid:48)R)dR = R f (x)dx R f (x + y)dx = (cid:82) dα (11) functions line).
 invariance property allows do many useful substitutions.
 have related parameterization sphere.
 element x ∈ S2 is written x(α, β) = Z(α)Y (β)n (12) n is north pole.
 parameterization makes explicit fact sphere is quotient S2 = SO(3)/ SO(2), H = SO(2) is subgroup rotations Z axis.
 Elements subgroup H leave north pole invariant, have form Z(γ).
 point x(α, β) ∈ S2 is associated coset representative ¯x = R(α, β, ∈ SO(3).
 element represents coset ¯xH = {R(α, β, γ)|γ ∈ [0,
 normalized Haar measure sphere is dx = dα dβ sin β normalized Haar measure SO(2) is dh = dγ (13) (14) have dR = dx dh, reﬂecting quotient structure.
 think function S2 γ-invariant function SO(3).
 Given function f S2 → C associate function ¯f (α, β, γ) = f (α, β).
 using normalized Haar measures, have: (cid:90) (cid:90) SO(3) (cid:90) (cid:90) (cid:90) (cid:90) π (cid:90) π (cid:90) π dα f (x)dx ¯f (R)dR = (cid:90) dα sin βdβ dγ ¯f (α, β, γ) sin βdβf (α, β) (cid:90) dα sin βdβf (α, β) dγ (15) allow deﬁne Fourier transform S2 Fourier transform SO(3), viewing function S2 γ-invariant function SO(3) taking SO(3)-Fourier transform.
 S2 APPENDIX B: CORRELATION EQUIVARIANCE have deﬁned S2 correlation [ψ (cid:63) f ](R) = (cid:104)LRψ, f(cid:105) = (cid:90) K(cid:88) S2 k=1 ψk(R−1x)fk(x)dx.
 Published conference paper ICLR Without loss generality, analyze single-channel case K =
 operation is equivariant: (cid:90) (cid:90) (cid:90) S2 S2 (cid:90) [ψ (cid:63) [LQf ]](R) = ψ(R−1x)f (Q−1x)dx ψ(R−1Qx)f (x)dx ψ((Q−1R)−1x)f (x)dx S2 = [ψ (cid:63) f ](Q−1R) = [LQ[ψ (cid:63) f ]](R) (17) A similar derivation be made SO(3) correlation.
 spherical convolution deﬁned Driscoll Healy (1994) is: [f ∗ ψ](x) = f (Rn)ψ(R−1x)dR (18) SO(3) n is north pole.
 Note deﬁnition, output spherical convolution is function sphere, function SO(3) deﬁnition cross-correlation.
 Note deﬁnition, deﬁnition involves integral SO(3).
 write integral terms Euler angles, noting north-pole n is invariant Z-axis rotations γ, R(α, β, γ)n = Z(α)Y (β)Z(γ)n = Z(α)Y (β)n, see deﬁnition integrates γ factors ψ), making invariant wrt γ rotation.
 other words, ﬁlter is ﬁrst (making circularly symmetric) is combined f (This was observed Makadia al.
 (2007)).
 consider be limited purpose pattern matching spherical CNNs. APPENDIX C: GENERALIZED FOURIER TRANSFORM compact topological group (like SO(3)) is associated discrete set orthogonal functions arise matrix elements irreducible unitary representations groups.
 circle (the group SO(2)) are complex exponentials (in complex case) sinusoids (for real functions).
 SO(3), functions are known Wigner D-functions.
 discussed paper, Wigner D-functions are parameterized degree parameter l order parameters m, n ∈ [−l,


 l].
 other words, have set matrix-valued functions Dl SO(3) → C(2l+1)×(2l+1).
 Wigner D-functions are orthogonal: (cid:104)Dl mn, Dl(cid:48) m(cid:48)n(cid:48)(cid:105) = (cid:90) (cid:90) π dα dβ sin β (cid:90) dγ mn(α, β, γ)Dl(cid:48) Dl m(cid:48)n(cid:48)(α, β, γ) = δll(cid:48)δmm(cid:48)δnn(cid:48) + (19) Furthermore, are complete, meaning behaved function f SO(3) → C be written linear combination Wigner D-functions.
 is idea Generalized Fourier Transform F SO(3): f (R) = [F−1 ˆf ](R) = (2l + ˆf l mnDl mn(R) (20) l(cid:88) l(cid:88) m=−l n=−l ∞(cid:88) l=0 Published conference paper ICLR ˆf l mn are called Fourier coefﬁcients f.
 Using orthogonality property Wigner D-functions, see Fourier coefﬁcients be retrieved computing inner product Wigner D-functions: [Ff ]l mn = (cid:90) (cid:90) ∞(cid:88) SO(3) SO(3) l(cid:48)=0 = ˆf l mn  ∞(cid:88) l(cid:48)=0 f (R)Dl mn(R)dR l(cid:48)(cid:88) l(cid:48)(cid:88) (2l(cid:48) + l(cid:48)(cid:88) l(cid:48)(cid:88) (cid:90) m(cid:48)=−l(cid:48) n(cid:48)=−l(cid:48)  Dl mn(R)dR ˆf l(cid:48) (2l(cid:48) ˆf l(cid:48) m(cid:48)n(cid:48) Dl(cid:48) m(cid:48)n(cid:48)(R)Dl mndR m(cid:48)=−l(cid:48) n(cid:48)=−l(cid:48) SO(3) APPENDIX D: FOURIER THEOREMS Fourier convolution theorems SO(3) §2 be found Kostelec Rockmore (2008); Makadia et al.
 (2007); Gutman et al.
 (2008).
 derive completeness.
 derive convolution theorems, use deﬁning property Wigner D-matrices: are (irreducible, unitary) representations SO(3).
 means satisfy: Dl(R)Dl(R(cid:48)) = Dl(RR(cid:48)), (22) R, R(cid:48) ∈ SO(3).
 Notice complex exponentials satisfy analogous criterion circle group S1 ∼= SO(2).
 is, einxeiny = ein(x+y), x + y is group operation SO(2).
 Unitarity means Dl(R)Dl†(R) = I.
 Irreducibility means, essentially, set matrices {Dl(R)| R ∈ SO(3)} cannot be block-diagonalized.
 derive Fourier theorem SO(3), use invariance integration measure dR: (cid:82) SO(3) f (R(cid:48)R)dR =(cid:82) SO(3) f (R)dR.
 facts proceed derive: (cid:91)ψ (cid:63) f (cid:90) (cid:90) (cid:90) (cid:90) (cid:90) = ˆf l ˆψl† SO(3) ψ(R−1R(cid:48))f (R(cid:48))dR(cid:48)Dl(R)dR ψ(R−1)f (R(cid:48))Dl(R(cid:48)R)dR(cid:48)dR SO(3) SO(3) SO(3) SO(3) SO(3) (ψ (cid:63) f )(R)Dl(R)dR (cid:90) (cid:90) f (R(cid:48))Dl(R(cid:48))dR(cid:48) (cid:90) f (R(cid:48))Dl(R(cid:48))dR(cid:48) (cid:90) SO(3) ψ(R−1)Dl(R)dR ψ(R)Dl(R) dR SO(3) SO(3) (23) SO(3)-Fourier transform SO(3) convolution ψ f is equal matrix product SO(3)-Fourier transforms ˆf ˆψ.
 sphere, derive analogous transform is called spherical harmonics m S2 → C are complete orthogonal family functions.
 transform.
 spherical harmonics Y l spherical harmonics are related Wigner D functions relation Dl mn(α, β, γ) = m0(α,
 m(α, β)einγ, Y l Y l m(α, β) = Dl Published conference paper ICLR S2 convolution f1 f2 is equivalent SO(3) convolution associated right- invariant functions ¯f1, ¯f2 (see Appendix A): (cid:90) (cid:90) (cid:90) (cid:90) S2 SO(2) f1(R−1x)f2(x)dx f1(R−1x)f2(x)dxdh S2 ¯f1(R−1R(cid:48)) ¯f2(R(cid:48))dR(cid:48) SO(3) = [ ¯f1 (cid:63) ¯f2](R) (cid:90) dγ (cid:90) dβ sin β dβ sin β (cid:90) π dβ sin β ¯f (α, β, γ)Dl mn(α, β, γ) f (α, β) dγ Dl mn(α, β, γ) f (α, β)Dl m0(α, [f1 (cid:63) f2](R) = dα (cid:90) (cid:90) (cid:90) (cid:90) dα (cid:90) π (cid:90) π dα = δn0 [F ¯f ]l mn = Fourier transform right invariant function SO(3) equals (24) (25) = δn0 f (x)Y l m(x)dx S2 think S2 Fourier transform function S2 n = column SO(3) Fourier transform associated right-invariant function.
 is beautiful result have been able ﬁnd reference for, seems likely has been observed before.

 wide range ﬁelds, music advertising recommendations healthcare wide range other consumer learning users’ personal tendencies judgements is essential.
 Many current approaches demand centralized data storage computation aggregate learn globally.
 Such central models, features known given user, make predictions appear personal user.
 such global models have proven be bring inherent conﬂicts privacy.
 User data leave device, training central model requires regular communication given user remote model.
 Further, users are way unique, exhibit diﬀerence preferences similar users, large centralized models have trouble adapting behavior.
 disadvantages mind, present deﬁnition personalization allows direct sharing centralization user data.
 see personalization balance generalization global information specialization given user’s quirks biases.
 make deﬁnition concrete, show simple baseline model’s performance changes sentiment analysis task function user bias, way information is shared models.
 hope work contribute framing discussion personalization provide metric evaluating ways model is providing user personal recommendations.
 discuss related areas such diﬀerential privacy, federated learning, have been motivated similar considerations.
 work ﬁt frameworks federated learning diﬀerential privacy.
 Personalized Models.
 has been long history research personalization machine learning.
 is wealth work using Bayesian hierarchical models
 Related Work Date: February
 REUBEN BRASHER, NAT ROTH, JUSTIN WAGLE learn mixes user global parameters data.
 works have achieved success areas health care [8], recommendation systems dealing mix implicit explicit feedback [30].
 has been increasing work helping practitioners integrate Bayesian techniques deep learning models [24].
 Many approaches personalization deep learning have relied combining personal features, hand written learned, global features make pre- dictions.
 example, deep recommender systems, feature user is certain gender, has seen certain movie.
 deep model learn embed features, combine linear model [5] order make recommenda- tions speciﬁc user.
 is common learn vector describing user end end task, doing featurization hand.
 such scenarios input be sentence user id prediction be next sentence [2], user is featurized learned vector.
 Similarly, Park al [20], learn vector representation user’s context generate image captions are personal user, [4] learn user vector language model determine set answers question satisfy user.
 approaches have beneﬁt requiring manual description important traits user.
 Here, discuss focus personalization work deep learning.
 general, deep learning models are large, complicated, non-linear.
 makes hard reason incorporating new user, set training examples aﬀect state model phenomenon known “catastrophic forgetting” [9], topic has seen large amount research [12], [10], [3].
 general, means add new user behavior is diﬀerent previous training examples, need take extra steps preserve performance previous users.
 makes online personalization models outlier users open problem deep learning.
 Federated Learning.
 other key personalization constraint is privacy related; get users trust model personal data, is believe is becoming necessary, mandated, guarantee degree privacy [21], [19].
 Research federated learning has demonstrated intelligence users be aggregated centralized models trained storing user data central location, alleviating part privacy concerns.
 research focuses training models data is distributed large number devices, further assumes device does have access representative sample global data [17], [13].
 believe federating learning is key part personalization strategy.
 Federated learning is concerned training central model does users globally.
 However, contribution individual user tends be washed update global model.
 Konecny al., [13] admit much, saying issues personalization are separate federated learning.
 Instead, current research focus improving communication speed [14] maintain stability models communication drops lags [25].
 [16] comes closest concerns, hypothesizes system user has personal set knowledge global mechanism aggregating knowledge similar users.
 do propose exact mechanism do aggregating determine users are similar.
 hope contribute conversation best compromise privacy decentralization learning, enforcing models cohere synchronize.
 is important note federation does guarantee privacy.
 practice aggregation gradients, place storing raw data, obscure user leak information users.
 example, attacker observes non-zero gradient feature representing location, be trivial infer users group live location.
 Making strong guarantees YOU WANT TO GO WHERE EVERYBODY KNOWS YOUR NAME extent data gives information individual users is domain diﬀerential privacy [7], [1].
 future work, hope incorporate stronger notions privacy discussion well, believe federated learning is good ﬁrst step towards greater user privacy.

 Personalization Definition problems mind, deﬁne personalization relative weighting performance model large, multi-user, global dataset, performance model data single user.
 deﬁnition implies several things.
 extent model be personalized depends model itself, spread user behavior.
 task users behave same, is little room personalization, global model trained user data be optimal locally.
 However, task user behavior varies individuals, is possible model trained users perform speciﬁc user.
 Nonetheless, speciﬁc user beneﬁt global data; example, user less training data see better performance use model trained global data.
 Therefore, best personalization strategy have ability incorporate global knowledge, distorting predictions given user.
 addition, add constraint user speciﬁc data be private user, cannot be shared models.
 particular, means user data is drawn same distribution, cannot train data.
 determine other ways share knowledge, such federating ensembling user models.
 paper, establish simple benchmarks evaluating model respects deﬁnition personalization.
 Formally, suppose have number users, N user have user speciﬁc data, {Xi i =


 N }, user speciﬁc models, {Mi i =


 N }.
 Let global data be D = S Xi, suppose have loss function, L is function Xi Mi, Li = L (Xi, Mi).
 deﬁne success personalization as: αL (Xi, Mi) + (1 − α) L (D, Mi) (1) α is determines much weight local user global data performance.
 case Xi follows same distribution D, deﬁnition collapses optimizing L (D, Mi), familiar, non personal objec- tive function dataset.
 However, α increases Xi diverges D, introduce tension optimizing speciﬁc user, ignoring whole dataset.
 Finally, enforce deﬁnition privacy, model, Mi, has access Xi, weights other models, Mj j


 N does have access other datasets, Mj, j =


 N j i.

 Personalization Motivation Implications One question be bother adding global data equation, is intuitive think personalization using model does single user’s data, data alone.
 However, intuition ignores fact have observed small amount behavior given user.
 ﬁt speciﬁc user’s data, risk overﬁtting performing new data same user.
 Bayesian interpretation deﬁnition is view global data term repre- senting prior belief user behavior.
 interpretation is view α much “catastrophic forgetting” allow model do order personalize user.
 REUBEN BRASHER, NAT ROTH, JUSTIN WAGLE Bayesian perspective, global data serves type regularization penalizes local model moving prior user data order ﬁt new user.
 think α hyperparameter representing strength prior belief.
 smaller α is, allow model deviate global state.
 be perfect rule choosing α, depend task, rate want adapt user.
 strategy be slowly increase α given user observe more data them.
 strategy, data rich users have large α data poor users have small α.
 data rich users be penalized less moving global state.
 is close treating loss maximum posteriori estimate users data distribution, observe more data.
 rate changing α be chosen minimize loss approach held user data, following normal cross validation strategy choosing hyperparameters.
 have domain speciﬁc intuition much personalization matters, α provides easy way express this.
 catastrophic forgetting perspective, deﬁnition is similar work [3], penalizes weights moving were previous task.
 work upweights penalty weights have high average gradient previous task, reasoning such weights are likely be important.
 penalize loss accuracy other users, penalizing change, gradient based approach does.
 indirect approach [3] has beneﬁts being scalable, be expensive recalculate total global loss adapting unlabeled data.
 Still, see common motivation, cases, have weighting much want allow model change response new examples.
 calculate L (D, Mi) do need gather data central location (which violate privacy constraint).
 is enough share Mi other user, sampling other users, gather summary statistics model performs.
 aggregate summary statistics evaluate Mi does D.
 However, sharing user’s model other users compromises original user’s privacy, model weights oﬀer insight user behavior.
 practice, have subset user data centralize users have opted in, large public curated dataset is relevant task interest.
 treat such dataset stand users behave.
 approach does compromise user privacy.
 Alternately, L (D, Mi) is meant regularize stabilize local models, be other approaches achieve global objective measuring performance global data.
 future work, explore best measure global loss violating user privacy.

 Experiments run experiment simple model demonstrate trade-oﬀs personal global performance choice α aﬀect way make future user predictions.
 Setup Data.
 use Stanford Sentiment Treebank (SSTB) [26] dataset evaluate learn models sentiment.
 ﬁrst step, take positive negative words dataset, ﬁnd training simple logistic regression model train set.
 run experiments simulating existence users.
 experiment, words are partitioned amongst users, users are assigned sentences containing words validation, train, test sets.
 Sentences contain words top are randomly assigned, sentences contain multiple are assigned relevant users.
 results split dataset model has subset words are enriched them, are underrepresented other models.
 YOU WANT TO GO WHERE EVERYBODY KNOWS YOUR NAME split is meant simulate pathological case user style; try simulate users train set are biased non-overlapping terms word choice use express sentiment.
 be case speciﬁc review dataset, general be natural language tasks users have speciﬁc slang, jokes, acronyms use be used others.
 such users, ideal setup adapt personal slang, leveraging global models help understand common language use.
 Architecture.
 user train separate models same archi- tecture.
 following baseline original SSTB paper, classify sentences using simple two-layer neural network, use average word embeddings input tanh non-linearity.
 use dimensional word embeddings, dropout [27], use ADAM optimize [11].
 start initial learning rate slowly decay, multiplying validation accuracy has decreased ﬁxed number batches, is same experiments.
 Finally, use early stopping case validation accuracy does decrease ﬁx number batches, equivalent epochs full train set.
 trained evaluate ways combining ﬁxed models, averaging model predictions, taking conﬁdent models, conﬁdence is deﬁned absolute diﬀerence models prediction.
 Evaluation Metrics.
 evaluate use train, validation, test splits provided dataset, use pytreebank [22] parse data.
 evaluate sentence level data test validation sets.
 model is state art.
 However, have experience putting models similar size low powered memory constrained devices, believe model be deployed.
 Nevertheless, model is complicated give sense happens try combine trained models.
 tables, report accuracy, test accuracy user speciﬁc data is evaluated sentences contain words none other users’ speciﬁc words.
 Global data scores represents whole test set.
 report results averaged independent trials.

 Results Analysis Single User Performance User-Speciﬁc Data Vs. Single User Perfor- mance Global Data.
 Unsurprisingly, second third columns Table show, single user models perform better own biased user-speciﬁc test set global data.
 makes sense model has been trained more words biased test set.
 words were selected be polarizing, gap makes concrete extent varying word usages hurt model performance task.
 Num.
 Users Single user model (global dataset) Average aggregation (global dataset) Single user model (user-speciﬁc dataset) Conﬁdence aggregation (global dataset) Table
 Accuracy number users.
 second column reports accu- racy single user model user-speciﬁc datasets.
 last columns report performance global dataset single user model ensemble models.
 REUBEN BRASHER, NAT ROTH, JUSTIN WAGLE Single User Performance User-Speciﬁc Data Vs. Ensembled Models User-Speciﬁc Data.
 number users increases, single user model outperforms aggregation methods user-speciﬁc data (Table
 is pronounced conﬁdence aggregation method: ensembling hurts performance experi- ments, eﬀect increasing add users.
 number users increases, given prediction are likely choose speciﬁc user’s model, performs own dataset.
 averaging aggregation method outperforms conﬁdence aggre- gation method is competitive single user model ﬁve users.
 However, more ﬁve users, averaging approach starts perform worse user’s own data, suggesting start drown personal judgment rely global knowledge.
 Num.
 Users Diﬀerence (Average Aggregation) Diﬀerence (Conﬁdence Aggregation) -0.001 -0.001 Table
 Comparison ensemble model performance single user model performance user-speciﬁc data.
 “Diﬀerence” columns denote single user model accuracy ensemble model accuracy.
 number users increases, user-speciﬁc models ensemble models in- creasingly wide margins.
 User Performance Global Data Vs. Ensembled Models Global Data.
 be easy conclude use single user model, Table demonstrates average-aggregated ensembled models single user model global data, number users increases.
 Again, is expect, aggregated models have been trained more words more examples, generalize better unbiased unseen data.
 global knowledge is important, contain insights phrases user has used few times.
 be true user has data.
 divide whole dataset users, number users increases, user-speciﬁc model is trained less data.
 case lack word training set indicate user use word.
 be user has interacted system enough individual model have learned language.
 Num.
 Users Diﬀerence (Average Aggregation) Diﬀerence (Conﬁdence Aggregation) -0.019 -0.029 -0.055 -0.075 -0.006 -0.005 -0.007 -0.006 Table
 Comparison ensemble model performance single user model performance global data.
 “Diﬀerence” columns denote single user model accuracy ensemble model accuracy.
 number users increases, average-aggregated ensemble model outperforms single user model.
 Choosing Approach Based α.
 experiments demonstrate tensions performing global user data, terms loss Equation
 apply Equation vary α, see point prefer diﬀerent strategies.
 YOU WANT TO GO WHERE EVERYBODY KNOWS YOUR NAME Speciﬁcally, suppose have approaches choose from, personalized L0 − L1 < ﬁrst losses p0 p1, global losses g0 g1 respectively.
 approach is superior.
 solve α such L0 = L1, loss term comes Equation
 Plugging deﬁnition in, see αp0 + (1 − α)g0 = αp1 + (1 − α)g1, αp0 + (1 − α)g0 − αp1 − (1 − α)g1 =
 equivalently, Rearranging yields α = g1 − g0 (p0 − p1) − (g0 − g1) (2) break personalization point.
 value α, see models valid solutions problem personalization.
 L0−L1 is linear respect α, L0−L1 ≥ α cutoﬀ, be greater cutoﬀ, vice versa.
 yields rule approach making decision multiple types models.
 requires choosing single hyperparameter α representing one’s belief much personalization matters task hand.
 illustrate apply ideas experimental results.
 see Tables compare using single model averaging predictions models, have: gaverage − gsingle = −0.054 psingle − paverage = −0.00523 So, cutoﬀ value α, single model averaged models yield equivalent losses are indiﬀerent them, is −0.0545/(−0.0545 =
 compute ranges α prefer model.
 Because, explained Lsingle − Laverage is linear α, evaluating single point cutoﬀ suﬃces: choose α computational convenience, have Lsingle − Laverage = psingle − paverage = −0.00523 ≤ Consequently, prefer single model values α cutoﬀ, averaged model values α cutoﬀ.

 Conclusion deﬁnition personalization allows complete decoupling models train time, requiring aggregate knowledge other models inference order po- beneﬁt global knowledge.
 addition, gives practitioner simple, parameter way, deciding choose amongst models have diﬀerent strengths weaknesses.
 have shown approach look sim- pliﬁed dataset model, na¨ıve approach using single model, aggregating models, sometimes be optimal.
 future, work develop better methods combining aggregate global knowledge, hurting performance.
 protect user privacy, consider alternate methods regularizing models global loss term, L (D, Mi).
 hope work provide useful framing future work personalization, learning decentralized architectures, such Ethereum Bitcoin [18], [28], serve guideline situations normal single loss centralized training paradigm cannot be used.
 REUBEN BRASHER, NAT ROTH, JUSTIN WAGLE References
 Mart´ın Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, Li Zhang, Deep learning diﬀerential privacy, Proceedings ACM SIGSAC Conference Computer Communications Security, ACM, pp.


 Rami Al-Rfou, Marc Pickett, Javier Snaider, Yun-hsuan Sung, Brian Strope, Ray Kurzweil, Con- versational contextual cues: case personalization history response ranking, arXiv preprint arXiv:1606.00372 (2016).

 Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, Tinne Tuytelaars, Mem- ory aware synapses: Learning (not) forget, arXiv preprint arXiv:1711.09601 (2017).

 Zheqian Chen, Ben Gao, Huimin Zhang, Zhou Zhao, Haifeng Liu, Deng Cai, User personalized satisfaction prediction multiple instance deep learning, Proceedings International Con- ference World Wide Web, International World Wide Web Conferences Steering Committee, pp.


 Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, al., Wide deep learning recommender systems, Proceedings Workshop Deep Learning Recommender Systems, ACM, pp.


 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, Imagenet: A large-scale hierar- chical image database, Computer Vision Pattern Recognition,
 CVPR
 IEEE Conference on, IEEE, pp.


 Cynthia Dwork, Diﬀerential privacy: A survey results, International Conference Theory Applications Models Computation, Springer, pp.


 Kai Fan, Allison E Aiello, Katherine A Heller, Bayesian models heterogeneous personalized health data, arXiv preprint arXiv:1509.00110 (2015).

 Robert M French, Catastrophic forgetting connectionist networks, Trends cognitive sciences (1999), no.


 Ronald Kemker, Angelina Abitino, Marc McClure, Christopher Kanan, Measuring catastrophic forgetting neural networks, arXiv preprint arXiv:1708.02072 (2017).

 Diederik P Kingma Jimmy Ba Adam, A method stochastic optimization.
 arXiv preprint arXiv:1412.6980.

 James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, al., Overcoming catastrophic forgetting neural networks, Proceedings National Academy Sciences

 Jakub Koneˇcn`y, H Brendan McMahan, Daniel Ramage, Peter Richt´arik, Federated optimization: distributed machine learning on-device intelligence, arXiv preprint arXiv:1610.02527 (2016).

 Jakub Koneˇcn`y, H Brendan McMahan, Felix X Yu, Peter Richt´arik, Ananda Theertha Suresh, Dave Bacon, Federated learning: Strategies improving communication arXiv preprint arXiv:1610.05492 (2016).

 Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, C Lawrence Zitnick, Microsoft coco: Common objects context, European conference computer vision, Springer, pp.


 Bernd Malle, Nicola Giuliani, Peter Kieseberg, Andreas Holzinger, more merrier-federated learning local recommendations, International Cross-Domain Conference Machine Learn- ing Knowledge Extraction, Springer, pp.


 H Brendan McMahan, Eider Moore, Daniel Ramage, Blaise Aguera y Arcas, Federated learning deep networks using model averaging, (2016).

 Satoshi Nakamoto, Bitcoin: A peer-to-peer electronic cash system,

 State California Department Justice Oﬃce Attorney General, Privacy laws — state califor- department justice oﬃce attorney general, https://oag.ca.gov/privacy/privacy-laws, Accessed:2018-01-22.

 Cesc Chunseong Park, Byeongchang Kim, Gunhee Kim, Attend you: Personalized image cap-
 European Parliament tioning context sequence memory networks, arXiv preprint arXiv:1704.06485 (2017).
 Council European Union, parliament (eu) http://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32016R0679&from=en, Accessed:2018-01-23.
 european council april Regulation
 Jonathan Raiman, Stanford sentiment treebank loader python, https://github.com/JonathanRaiman/pytreebank, Accessed:2018-01-05.

 Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, al., Imagenet large scale visual recognition chal- lenge, International Journal Computer Vision (2015), no.

 YOU WANT TO GO WHERE EVERYBODY KNOWS YOUR NAME
 Jiaxin Shi, Jianfei Chen, Jun Zhu, Shengyang Sun, Yucen Luo, Yihong Gu, Yuhao Zhou, Zhusuan: A library bayesian deep learning, arXiv preprint arXiv:1709.05870 (2017).

 Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, Ameet Talwalkar, Federated multi-task learning, arXiv preprint arXiv:1705.10467 (2017).

 Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, Christopher Potts, Recursive deep models semantic compositionality sentiment treebank, Pro- ceedings conference empirical methods natural language processing, pp.


 Nitish Srivastava, Geoﬀrey E Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, Dropout: simple way prevent neural networks overﬁtting., Journal machine learning re- search (2014), no.


 Gavin Wood, Ethereum: A secure decentralised generalised transaction ledger, Ethereum Project Yellow Paper (2014).

 Yi Zhang Jonathan Koren, Eﬃcient bayesian hierarchical user modeling recommendation system, Proceedings 30th annual international ACM SIGIR conference Research development information retrieval, ACM, pp.


 Philip Zigoris Yi Zhang, Bayesian adaptive user proﬁling explicit implicit feedback, Proceed- ings ACM international conference Information knowledge management, ACM, pp.

 paper studies Restricted Isometry Property (RIP) random projections sub- spaces.
 reveals distance low-dimensional subspaces remain unchanged being projected Gaussian random matrix overwhelming proba- bility, ambient dimension projection is large comparison dimension subspaces.
 Motivation era data deluge, labeling huge amount large-scale data be time-consuming, costly, intractable, unsupervised learning has attracted increasing attention ∗The authors are Department Electronic Engineering, Tsinghua University, Beijing China.
 corresponding author paper is Yuantao Gu (gyt@tsinghua.edu.cn).
 recent years.
 such methods emerging recently, subspace clustering (SC) [1, depicts latent structure variety data union subspaces, has been shown be powerful wide range applications, including motion segmentation, face clustering, anomaly detection.
 shows great potential explored datasets, such network data, gene series, medical images.
 Traditional subspace clustering methods, however, suﬀer deﬁciency simi- larity representation, be expensive adapt large-scale datasets.
 order alleviate high computational burden, variety works have been done address crucial problem eﬃciently handle large-scale datasets.
 Compressed Subspace Clustering (CSC) [5] known Dimensionality-reduced Subspace Clustering [6] is method performs SC compressed data points.
 random compression reduces dimension ambient space, computational cost ﬁnding self-representation SC be reduced.
 Based con- cept subspace aﬃnity, characterizes similarity subspaces, mathematical tools introduced conditions several popular al- gorithms cluster compressed data have been studied veriﬁed
 data points are projected high-dimensional ambient space RN new medium-dimensional ambient space Rn, worry is similarity low-dimensional subspaces increases SC algorithms are likely per- form well.
 Inspired well-known Johnson-Lindenstrauss (JL) Lemma [9, Restricted Isometry Property (RIP) [11, allows use random projection reduce space dimension keeping Euclidean distance data points leads boom sparse signal processing including Compressed Sensing (CS) [14, speculate similarity (or distance) given subspaces remain unchanged, dimension latent sub- space data lie is small compared ambient space projection n.
 be highlighted conjecture is conﬁned SC problem, believe beneﬁt future studies other subspace related topics.
 Motivated conjecture similarity given sub- spaces remain unchanged random projection, study RIP Gaussian random projections ﬁnite set subspaces.
 order give more solid guarantees more precise insight law magnitude dimensions CSC other sub- space related derive optimum probability bound RIP Gaussian random compressions subspaces paper.
 Compared previous work [21], probability bound has been improve − O(1/n) − e−O(n), is optimum consider state-of-the-art statistical probability theories Gaussian random matrix.
 Main Results projection Frobenius (F-norm short) distance is adopted work measure distance subspaces.
 be noted generalize deﬁnition [22] situation dimensions subspaces are diﬀerent.
 Deﬁnition ([21]) (Projection Frobenius norm distance generalized projection F-norm distance subspaces X1 X2 is deﬁned D(X1,X2) := √2kU1UT − U2UT kF, Ui denotes arbitrary orthonormal basis matrix subspace Xi, i =
 focus change distance low-dimensional sub- spaces being projected RN Rn (n < N ).
 projection low-dimensional subspace using Gaussian random matrix is deﬁned below.
 Deﬁnition (Gaussian random projection subspace) Gaussian random pro- jection d-dimensional subspace X ⊂ RN Rn (d < < N is deﬁned below, −→ Y = {y|y = Φx,∀x ∈ X}, projection matrix Φ ∈ Rn×N is composed entries drawn Gaussian distribution N (0,
 notice dimensions subspaces remain unchanged random pro- jection probability one.
 Based deﬁnitions above, main theoretical result work is stated follows.
 Suppose X1,


 ,XL ⊂ RN are L subspaces dimension less d.
 −→ Yi ⊂ Rn, i = random projection using Gaussian random matrix Φ ∈ Rn×N Xi L, < N
 exist constants c1(ε), c2(ε) depending ε such subspaces Xi Xj, n > c1(ε) max{d, ln L}, (1 − ε) D2(Xi,Xj) < D2(Yi,Yj) < (1 + ε) D2(Xi,Xj) (1) holds probability least − e−c2(ε)n.
 reveals distance subspaces remains unchanged random projection overwhelming probability, ambient dimension projection n is large.
 Contribution paper, study RIP Gaussian random matrices projecting ﬁnite set subspaces.
 problem is challenging random projections preserve orthogonality normalize vectors deﬁning orthonormal bases subspaces.
 order measure change subspace distance induced random projections, eﬀects have be quantiﬁed.
 Based building metric space subspaces projection F-norm distance, is connected subspace aﬃnity, start verifying aﬃnity subspaces concentrates estimate overwhelming probability Gaussian random projection.
 reach RIP subspaces generalize situation ﬁnite set subspaces, stated Theorem
 main contribution work is provide mathematical tool, shed light many problems including CSC.
 direct result Theorem solving SC problem large scale, conduct SC compressed samples alleviate high computational burden have theoretical performance guarantee.
 distance subspaces remains unchanged clustering error rate SC algorithm keep small conducting original space.
 Considering theory is independent SC beneﬁt future studies other subspace related topics.
 previous work [21] be compared Section know, is relevant work study distance preserving property subspaces random projection.
 Comparison JL Lemma RIP Sparse Signals famous Johnson-Lindenstrauss Lemma illustrates exists map higher- dimensional space lower-dimensional space such distance ﬁnite set data points change being mapped.
 Lemma (JL Lemma) [9, set V L points RN exists map f RN → Rn, < N such x1, x2 ∈ V, (1 − ε)kx1 − x2k2 ≤ kf (x1) − f ≤ (1 + ε)kx1 − x2k2 n is positive integer satisfying n − ε3/3), < ε < is constant.
 RIP random matrix illustrates distance sparse vectors change little high probability random projection.
 Deﬁnition [11, projection matrix Φ ∈ Rn×N n < N satisﬁes RIP order k exists δk ∈ such (1−δk)kx1−x2k2 ≤ kΦx1−Φx2k2 ≤ (1+δk)kx1−x2k2 Table Comparison other dimension-reduction theories including JL Lemma, RIP sparse signals, previous results [21] JL Lemma RIP sparse signals RIP low-dimensional subspaces [21] work object set L points RN k-sparse signals RN set L d-dimensional subspaces RN metric compression method error condition success probability Euclidean distance projection F-norm distance kxi − xjk2 − PjkF map f Gaussian random matrix (1 − ε, + ε) n ε2/2−ε3/3 (1 − δk, + δk) ≥ c1kln(cid:0) N k(cid:1) − e−c2n (1 − ε, + ε) large n > c1max{d, ln L} (ε−d/n)2n − e−c2n holds k-sparse vectors x1, x2 ∈ RN
 A Gaussian random matrix Φ ∈ Rn×N n < N has RIP order k n ≥ c1kln(cid:0) N k(cid:1) probability − e−c2n, c1, c2 are constants depending smallest nonnegative constant satisfying Deﬁnition
 above works focus change distance points determinate mapping random projection.
 comparison, work views subspace whole studies distance subspaces, best knowledge has been studied before.
 Moreover, above works study points Euclidean space l2-norm, work study subspaces Grassmannian manifold F-norm metric, is nonlinear complex.
 detailed comparison explain diﬀerences work related works is presented Table
 Comparison RIP Signals UoS are literatures studying distance preserving properties compressed data points, be sparse speciﬁc basis lie couple subspaces surfaces [23,
 authors [23] extended RIP signals are sparse compressible respect certain basis Ψ, i.e., x = Ψα, Ψ is represented unitary N×N matrix α is k-sparse vector.
 work [24] proves high probability random projection Φ preserve distance signals belonging Union Subspaces (UoS).
 [25], is shown random projection preserves structure surfaces.
 Given collection L surfaces linearization dimension d, are embedded space O(dδ2 log(Ld/δ)) dimension, surfaces are preserved sense pair points surfaces distance are preserved.
 main contribution [26] is stated follows.
 S is n point subset RN < δ < n = log is mapping RN Rn volumes sets size most d do change more factor + δ, distance points aﬃne hulls sets size most k − is preserved relative error δ.
 According survey, works study embedding Euclidean distances tween points subspaces, discuss embedding ﬁnite set subspaces terms projection F-norm distance.
 related works paper, same mathematical tool concentration inequalities random matrix theory are adopted derive RIP diﬀerent i.e., data points Euclidean space subspaces Euclidean space (or points Grassmann manifold), respectively.
 comparison, Euclidean space random projection are linear, Grassmannian is linear, let projection it, new problem is diﬃcult existing one, core contribution work is dealing above challenges brand-new geometric technique has been used derive RIP data points.
 Organization rest paper is organized follows.
 Based introduction principal angles, aﬃnity, connection projection F-norm distance, study RIP subspaces top level Section
 main result Theorem is proved using core propositions Lemma Theorem
 Section focus probability concentration inequalities Gaussian random matrix prepare necessary mathematical tools be used work.
 Section prove ﬁrst core proposition Lemma states aﬃnity line subspace concentrate estimate overwhelming probability random projection.
 Section prove second core proposition Theorem provides general theory aﬃnity subspaces arbitrary dimensions demonstrates concentration random projection.
 Section compare theories previous results highlight novelty.
 conclude work Section
 Most proofs lemmas remarks are included Appendix
 Notations Vectors matrices are denoted lower-case upper-case letter, respectively, boldface.
 AT denotes transposition.
 kak kAkF denote ℓ2 norm vector Frobenius norm matrix A.
 smax(A) smin(A) denote largest smallest singular value matrix A, respectively.
 Subspaces are denoted X ,Y, S.
 C(A) denotes column space matrix A.
 use S⊥ denote orthonormal complement space S.
 PS (v) denotes projection vector v subspace S.
 RIP Gaussian Random Projection Subspaces Preliminary starting theoretical analysis, ﬁrst introduce deﬁnition principal angles aﬃnity.
 concepts have been adopted describe relative position measure similarity subspaces.
 theoretical analysis ﬁrst focus estimation quantities random projection.
 using connection aﬃnity projection F-norm distance derived [21], derive result Theorem
 principal angles (or canonical angles) subspaces provide robust way characterize relative subspace positions [27,
 Deﬁnition principal angles θ1,··· θd1 subspaces X1 X2 dimen- sions d1 ≤ d2, are deﬁned cos θk = max x1∈X1 orthogonality constraints xT x2 =: kx1kkx2k max kx1kkkx2kk x2∈X2 i xil l = k − i =
 Beside deﬁnition, alternative way computing principal angles is use singular value decomposition [29].
 Let columns Ui be orthonormal bases subspace Xi dimension di, i = suppose d1 ≤ d2.
 Let λ1 ≥ λ2 ≥ ··· ≥ λd1 ≥ be singular values UT U2, cos θk = λk, k = d1.
 Based principle angles, aﬃnity is deﬁned measure similarity subspaces [2].
 Deﬁnition aﬃnity subspaces X1 X2 dimension d1 ≤ d2 is deﬁned aﬀ (X1,X2) :=(cid:18) d1 Xk=1 cos2 θk(cid:19)1/2 = kUT U2kF, columns Ui are orthonormal bases Xi, i =
 relationship distance aﬃnity is revealed Lemma
 concise deﬁnition easy computation aﬃnity, start theoretical analysis aﬃnity, present results distance using Lemma
 Lemma [21] distance aﬃnity subspaces X1 X2 dimension d1, d2, are connected D2(X1,X2) = d1 + d2 − aﬀ
 Theoretical Results section, present main theoretical results aﬃnity distance subspaces.
 that, let introduce basic notations be used.
 denote random projection subspaces X1 X2 Y1 Y2, respectively.
 denote DX = D(X1,X2) DY = D(Y1,Y2) distances random projection.
 Similarly, use aﬀX = aﬀ(X1,X2) aﬀY = aﬀ(Y1,Y2) denote aﬃnities projection.
 loss generality, suppose d1 ≤ d2.
 simplicity, refer aﬃnity (distance) random projection projected aﬃnity (projected distance).
 begin focus special case subspace is degenerated line (one-dimensional subspace).
 following lemma provides estimation aﬃnity line subspace Gaussian random projection.
 dimensionality new ambient space is large real projected aﬃnity concentrate estimation overwhelming probability.
 Lemma Suppose X1,X2 ⊂ RN are line d-dimension subspace, d ≥ respectively.
 Let λ = denote aﬃnity.
 are projected Rn, < N, Gaussian random matrix Φ ∈ Rn×N Xi −→ Yi, i = projected aﬃnity, aﬀY be estimated exist constants c1(ε), c2(ε) depending ε such n > c1(ε)d, Y − aﬀ holds probability least − e−c2(ε)n.
 (cid:12)(cid:12)(cid:12) Y(cid:12)(cid:12)(cid:12) aﬀ Y = λ2 + − λ2(cid:1) < (1 − λ2)ε (2) (3) Then, study general case projecting subspaces arbitrary dimensions.
 mentioned last subsection, begin estimation aﬃnity restate result terms distance.
 following theorem reveals concentration aﬃnity arbitrary sub- spaces random projection.
 Suppose X1,X2 ⊂ RN are subspaces dimension d1 ≤ d2, respectively.
 Take aﬀ Y = aﬀ X + d2 (d1 − aﬀ X (4) −→ Yi, i = estimate aﬃnity subspaces random projection, Xi
 exist constants c1(ε), c2(ε) depending ε such n > c1(ε)d2, < (d1 − aﬀ X )ε (5) Y − aﬀ holds probability least − e−c2(ε)n.
 (cid:12)(cid:12)(cid:12) Y(cid:12)(cid:12)(cid:12) concision evaluating relative position Deﬁnition present concentration using aﬃnity Lemma Theorem play essential role RIP subspaces.
 proofs, unfurl main text work, are postponed Section Section respectively.
 Using Lemma Theorem derive estimation projected distance.
 prove true projected distance concentrate estimate overwhelming probability.
 Corollary Suppose X1,X2 ⊂ RN are subspaces dimension d1 ≤ d2, respectively.
 use Y = D2 X − d2 n (cid:18)D2 X − (cid:19) d2 − d1 −→ estimation distance subspaces random projection, Xi Yi, i =
 exist constants c1(ε), c2(ε) depending ε such n > c1(ε)d2, D2 Y − D <(cid:18)D2 X − (cid:19) ε d2 − d1 (cid:12)(cid:12)(cid:12) Y(cid:12)(cid:12)(cid:12) holds probability least − e−c2(ε)n.
 Proof Combining (4) (6) using Lemma get |D2 aﬀ (5).
 Y|.
 Using Lemma again, have D2 = d1 − aﬀ X − d2−d1 Y − X
 Therefore, (7) is identical Y| = |aﬀ Y − D (6) (7) Proof Theorem are ready prove RIP Gaussian random matrix projecting ﬁnite set subspaces using results above.
 loss generality, assume di ≤ dj ≤ d.
 According Corollary exist constants c1,1, c2,1 depending ε such n > c1,1dj, D2(Xi,Xi) −(cid:18) dj − (cid:19) < D2(Yi,Yi) dj − di < D2(Xi,Xi) +(cid:18)− − dj (cid:19)
 dj − di holds probability least − e−c2,1n.
 n have dj/n ≤ d/n < ε/2.
 case, have D2(Yi,Yi) > D2(Xi,Xi) −(cid:18) dj D2(Xi,Xi) =(cid:18)1 − dj n − D2(Xi,Xi) > (1 − ε) D2(Xi,Xi), D2(Yi,Yi) < D2(Xi,Xi) +(cid:18)− dj D2(Xi,Xi) =(cid:18)1 − dj D2(Xi,Xi) < (1 + ε) D2(Xi,Xi), (8) (9) hold probability least e−c2,1n.
 Note (8) (9) hold ≤ i < j ≤ L.
 probability is least L(L−1) exists constant c2 depending ε, such L(L−1) c2,1}, n > c1 max{d, ln L}, conditions n > c1,1d, n n > are required are satisﬁed, probability is least − e−c2n c2 >
 reach ﬁnal conclusion.
 e−c2,1n.
 n > c2,1 e−c2,1n < e−c2n.
 Take c1 := max{c1,1, ε ln L(L−1) ln L(L−1) c2,1 Concentration Inequalities Gaussian Distribution proving main results, ﬁrst introduce useful concentration inequalities Gaussian distribution.
 Most are proved using following lemma, provides strict estimation singular values Gaussian random matrix.
 Lemma [30] Let A be N × n matrix elements aij are independent Gaussian random variables.
 t ≥ one has P(cid:16)smax(A) ≥ P(cid:16)smin(A) ≤ √N + √n + t(cid:17) ≤ e− t √N − √n − t(cid:17) ≤ e− t
 (10) (11) Based Lemma are ready prove useful lemmas be used prove theories RIP random projection subspaces.
 doing that, ﬁrst deﬁne standard Gaussian random matrix verify function satisfying certain condition be written single exponential function.
 Deﬁnition A Gaussian random matrix (or vector) has i.i.d. zero-mean Gaussian random entries.
 standard Gaussian random matrix A ∈ Rn×N has i.i.d. zero-mean Gaussian random entries variance
 column A is standard Gaussian random vector.
 Lemma Given k, holds f (ε, τ Xk=1 ak(ε, n)e−gk(ε,n,τ ), hk(ε) := lim τ→0 bk(ε) := lim n→∞ lim n→∞ ln ak(ε, n) gk(ε, τ > < hk(ε), (12) (13) (14) exist universal constants n0, c1 c2 > depending ε, such n > n0, τ < c1, satisﬁes f (ε, τ e−c2n.
 Proof proof is postponed Appendix Remark Lemma illustrates summation ﬁnite multiple exponential decay functions be bounded single exponential function.
 following lemma illustrates norm standard Gaussian random vector concentrates high probability, dimensionality is high.
 Lemma Assume ∈ Rn is standard Gaussian random vector.
 ε have hold n > n0, n0 c are constants dependent ε.
 P(cid:0)(cid:12)(cid:12)kak2 − > ε(cid:1) < e−c(ε)n (15) Proof proof is postponed Appendix Furthermore, Corollary generalizes Lemma case project standard Gaus- sian random vector orthonormal matrix.
 Corollary Let ∈ Rn be standard Gaussian random vector.
 given orthonormal matrix V = [v1,··· vd] ∈ Rn×d ε > have hold n > c1d, c1, c2 are constants dependent ε.
 kVTak2 − > ε(cid:19) < e−c2(ε)n (16) P(cid:18)(cid:12)(cid:12)(cid:12)(cid:12) Proof proof is postponed Appendix Corollary extends Lemma column-normalized standard Gaussian random matrix.
 reveals column-normalized Gaussian random matrix is high-quality approxima- tion orthonormal matrix, singular values are
 Corollary Let A = [a1,··· ak] ∈ Rn×k be standard Gaussian random matrix.
 column ¯A is normalized corresponding column A, is ¯A =(cid:20) a1 ka1k ,··· ak kakk(cid:21)
 get bound minimum maximum singular value ¯A P(cid:0)s2 P(cid:0)s2 min(cid:0) ¯A(cid:1) < − ε(cid:1) < e−c2,1(ε)n, ¯A(cid:1) + ε(cid:1) < e−c2,2(ε)n, ∀n > c1,1k, ∀n > c1,2k, (17) (18) c1,1 c2,1, c1,2 c2,2 are constants dependent ε (17) (18), respectively.
 Proof proof is postponed Appendix Remark ∈ R(n−d0)×k be standard Gaussian random matrix, have (18) hold n > c1,2 max{k, d0}, c1,2 c2,2 are constants dependent ε.
 following lemma studies property Gaussian random projection.
 Intuitively, illustrates line subspace are perpendicular other, be perpendicular Gaussian random projection.
 Lemma Assume u1 ∈ RN is unit vector, U2 ∈ RN×d is orthonormal matrix, u1 is perpendicular U2.
 Let Φ ∈ Rn×N be standard Gaussian random matrix.
 use a1 = Φu1 A2 = ΦU2 denote projection u1 U2 using Φ.
 V2 is arbitrary orthonormal basis C(A2), ε > have > ε(cid:17) ≤ e−c2(ε)n (19) hold n > c1(ε)d, c1, c2 are constants determined ε.
 P(cid:16)(cid:13)(cid:13)VT a1(cid:13)(cid:13) Proof proof is postponed Appendix Corollary Lemma further deﬁne ¯a1 = a1/ka1k normalized projection u1, ε > have hold n > c1(ε)d, c1, c2 are constants determined ε.
 P(cid:16)(cid:13)(cid:13)VT ¯a1(cid:13)(cid:13) > ε(cid:17) ≤ e−c2(ε)n (20) Proof proof is postponed Appendix Remark Using same notations Corollary take Φ ∈ R(n−d0)×N ¯a1 ∈ Rn−d0 V2 ∈ R(n−d0)×d, have have P(cid:16)(cid:13)(cid:13)VT ¯a1(cid:13)(cid:13) n > c1(ε) max{d, d0}, c1 c2 are constants dependent ε.
 veriﬁed replacing n n − d0 (20) applying Lemma
 detailed proof is postponed Appendix > ε(cid:17) ≤ e−c2(ε)n state independence random vectors, matrices, column- spanned subspaces.
 Deﬁnition random vectors are independent, distribution does have inﬂuence other.
 random matrices are indepen- dent, columns are independent.
 introduce independence random matrix subspace, holds true subspace is spanned columns random matrix is independent ﬁrst one.
 Finally, subspaces are independent, are spanned columns independent random matrices, respectively.
 Lemma Assume U V are matrices satisfying UTV Φ is Gaussian random matrix.
 ΦU ΦV are independent.
 be veriﬁed calcu- lating correlation entries ΦU ΦV, respectively.
 are zero.
 Proof Lemma proof Lemma is made steps.
 ﬁrst, derive accurate expression error estimating aﬀY
 estimate error is bounded utilizing concentration inequalities Gaussian distribution derived previous section.
 Step Let begin choosing bases line X1 subspace X2 calculating aﬃnity projection.
 According deﬁnition aﬃnity, λ = cos θ, θ is only principal angle X1 X2.
 use u u1 denote, respectively, basis X1 unit vector X2, constructs principal angle u.
 Therefore, u be rewritten following form u = λu1 +p1 − λ2u0, u0 denotes unit vector orthogonal X2.
 Based above deﬁnition, choose U = [u1, ..., ud] basis X2.
 Notice {u2,··· ud} be chosen orthonormality is satisﬁed.
 projecting X1 random Gaussian matrix, get subspace Y1, basis vector is (21) = Φu = λΦu1 +p1 − λ2Φu0 = λa1 +p1 − λ2a0, a1 := Φu1 a0 := Φu0 are orthogonal other.
 Y2, considering ΦU is set orthonormal basis, do using Gram-Schmidt process.
 Denote orthonormalized matrix V = [v1,··· vd].
 deﬁnition Gram-Schmidt ﬁrst column V be does change direction orthogonalization.
 v1 = a1 ka1k (23) Remark Consider aﬃnity subspaces S1 S2, dimension d
 Let v1 V2 be orthonormal basis S1 S2, respectively.
 aﬃnity equals norm projection v1 S2, i.e., λ = kPS2(v1)k =(cid:13)(cid:13)VT deﬁnition aﬃnity calculate aﬃnity Y1 Y2 v1(cid:13)(cid:13).
 aﬀ VT kak(cid:13)(cid:13)(cid:13)(cid:13) Y =(cid:13)(cid:13)(cid:13)(cid:13) λVTa1 +p1 − λ2VTa0(cid:13)(cid:13)(cid:13) kak2 (cid:13)(cid:13)(cid:13) kak2 (cid:16)λ2kVTa1k2 + (1 − λ2)kVTa0k2 + λp1 − λ2kaT a0k(cid:17)
 a1 lies Y2, have kVTa1k = ka1k.
 taking norm sides (22), write kak2 = λ2ka1k2 + (1 − λ2)ka0k2 + − λ2ka0kka1k.
 (24) (25) (26) Recalling (2) inserting estimation estimate error is deduced (cid:12)(cid:12)(cid:12) aﬀ aﬀ Y(cid:12)(cid:12)(cid:12) aﬀ Y − aﬀ (cid:18)1 − RHS (28) parts using triangle inequality.
 kak2 (cid:19)−(cid:18)1−(1−λ2)(cid:18)1− kak2 − kVTa0k2 kak2 (cid:19)(cid:12)(cid:12)(cid:12)(cid:12) Eliminating kVTa1k ka1k inserting (25) (26) (24), get kak2(cid:0)kak2−(1−λ2)ka0k2 +(1−λ2)kVTa0k2(cid:1) Y = kak2 − kVTa0k2 − (1 − λ2)(cid:18)ka0k2 kak2 (cid:19)
 =(cid:12)(cid:12)(cid:12)(cid:12) (1 − λ2)(cid:19)(cid:12)(cid:12)(cid:12)(cid:12) Y −(cid:18)λ2 + kak2 − kVTa0k2 =(cid:12)(cid:12)(cid:12)(cid:12) −(cid:18)ka0k2 =(1 − λ2)(cid:12)(cid:12)(cid:12)(cid:12) −(cid:18)ka0k2 kak2 − kVTa0k2 kak2 (cid:19)(cid:12)(cid:12)(cid:12)(cid:12) (cid:18)1 − kVTa0k2 ka0k2 kak2 − +(cid:12)(cid:12)(cid:12)(cid:12) kak2 − kak2 (cid:12)(cid:12)(cid:12)(cid:12) kak2 − probability least − have (cid:12)(cid:12)kak2 < ε1 (cid:12)(cid:12)ka0k2 − < ε1.
 n(cid:12)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:12)(cid:13)(cid:13)VTa0(cid:13)(cid:13) kak2 − (cid:12)(cid:12)(cid:12)(cid:12) ≤(cid:12)(cid:12)(cid:12)(cid:12) ≤(cid:12)(cid:12)(cid:12)(cid:12) kVTa0k2 − ka0k2 < ε2.
 Using (30), (31), (32) (29), n > max{n0, c1,2}d, probability least − − e−c2,2(ε2)n, have (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) (cid:18)1 − − ka0k2 − Xi=1 cos2 θi!(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) − ε1 ≤(cid:18) ε2 − ε1 + ε1 + ε1 − u0 is orthogonal ui, i = d, Corollary n > c1,2(ε2)d, probability least − e−c2,2(ε2)n, have Step bounding estimate error concentration inequalities, ﬁrst split (28) a0 are standard Gaussian random vectors, Lemma n > n0, (29) (27) (30) (31) (32) (33) (34) last inequality holds ε1 <
 complete proof, need formulate (34) (33) shape (3) single exponential function, respectively.
 Letting ε1 = ε2 =: ε/6 inserting (34) (28), have (cid:12)(cid:12)(cid:12) aﬀ Y − aﬀ Y(cid:12)(cid:12)(cid:12) ≤(cid:0)1 − λ2(cid:1)(cid:18)(cid:18) ≤(cid:0)1 − λ2(cid:1)(cid:18)(cid:18)3 =(cid:0)1 − λ2(cid:1) ε.
 Remark know exist constants c1, c2, such (33) is greater − e−c2(ε)n n > c1(ε)d close proof.
 ε1 + + ε1 + (cid:19) (cid:19) Proof Theorem proof Theorem is divided parts.
 subsection complete main body proof using important lemma, be proved subsection start, let introduce auxiliary variables.
 Remark Assume are subspaces S1 S2, dimension d1 ≤ d2.
 Let ˜Ui = [˜ui,1,··· ˜ui,di] denote orthonormal matrix subspace Si, i =
 do singular value decomposition ˜UT singular values λk = cos θk, ≤ k ≤ d1 are located diagonal Λ, θ1 ≤ θ2 ≤ ··· ≤ θd1 denote principal angles S1 S2.
 reshaping, have ˜U1 = Q2ΛQT UT U1 :=(cid:16) ˜U2Q2(cid:17)T ˜U1Q1 = Λ = λ1 


 λd1  Ui := ˜UiQi = [ui,1,··· ui,di] i = are orthonormal basis, have closest connection aﬃnity subspaces.
 Deﬁnition (principal orthonormal bases) refer U1 U2 principal orthonor- mal bases S1 S2, are derived using method Remark
 According Remark subspaces X1 X2, get principal orthonormal bases U1 U2, respectively.
 projection multiplying standard Gaussian random matrix Φ, original basis matrix changes Ai = ΦUi = [ai,1,··· ai,di] columns are unitary orthogonal other.
 normalize columns kai,diki columns are unitary ¯Ai = [¯ai,1,··· ¯ai,di] = h ai,1 orthogonal other.
 However, Corollary know ¯Ai be used good approximation orthonormal basis Yi. see ¯Ai plays important role estimating aﬃnity projection.
 ,··· kai,1k need deﬁne accurate orthonormal basis projected subspace.
 eﬃcient way is process ¯Ai using Gram-Schmidt orthogonalization, result is deﬁned Vi = [vi,1,··· vi,di] i =
 Main Body order prove Theorem need calculate ¯Ai is orthonormal matrix, use kVT aﬃnity projection.
 using triangle inequality, have Y estimate bias aﬀ Y
 ¯A1k estimate (35) aﬀ Y − aﬀ (cid:12)(cid:12)(cid:12) Y(cid:12)(cid:12)(cid:12) ≤(cid:12)(cid:12)(cid:12) aﬀ Y −(cid:13)(cid:13)VT ¯A1(cid:13)(cid:13) F(cid:12)(cid:12)(cid:12) +(cid:12)(cid:12)(cid:12)(cid:13)(cid:13)VT ¯A1(cid:13)(cid:13) F − aﬀ Y(cid:12)(cid:12)(cid:12) Therefore, following proof be divided steps.
 ﬁrst step is bound error caused using ¯A1 approximation V1 compute aﬃnity.
 do that, introduce important lemma, is essence proof.
 second step is bound diﬀerence approximated aﬃnity estimate, be derived using Lemma
 combine bounds complete proof.
 Step ﬁrst item RHS (35), according deﬁnition aﬃnity, have (cid:12)(cid:12)(cid:12) aﬀ Y −(cid:13)(cid:13)VT ¯A1(cid:13)(cid:13) F(cid:12)(cid:12)(cid:12) d1 V1k2 =(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) Xk=1(cid:16)kVT Xk=1(cid:12)(cid:12)(cid:12)kVT F(cid:12)(cid:12)(cid:12) ¯A1(cid:13)(cid:13) F −(cid:13)(cid:13)VT v1,kk2 −(cid:13)(cid:13)VT ¯a1,k(cid:13)(cid:13) v1,kk2 −(cid:13)(cid:13)VT ¯a1,k(cid:13)(cid:13) d1 (36) Lemma exist constants c1,1(ε1) c2,1(ε1) > depending ε1, such n > c1,1(ε1)d2, have (cid:12)(cid:12)(cid:12)kVT v1,kk2 ¯a1,k(cid:13)(cid:13) hold probability least − e−c2,1(ε1)n.
 Proof proof is postponed Section ≤(cid:0)1 − λ2 k(cid:1) ε1, ∀k = d1 (37) Plugging (37) (36), have (cid:12)(cid:12)(cid:12) aﬀ Y −(cid:13)(cid:13)VT ¯A1(cid:13)(cid:13) F(cid:12)(cid:12)(cid:12) ≤ ε1 d1 Xk=1(cid:0)1 − λ2 k(cid:1) (38) hold probability least − d1e−c2,1(ε1)n n > c1,1(ε1)d2.
 Step second estimation error RHS (35), convert problem subspace Y1 dimension d1 d1 subproblems, is 1- dimensional subspace, use Lemma estimate error.
 Denote X1,k := C{u1,k}, Y1,k := C{a1,k}, ≤ k ≤ d1.
 According deﬁnition aﬃnity Remark have aﬃnity X1,k X2 is kUT u1,kk = λk, aﬃnity Y1,k Y2 is equal (cid:13)(cid:13)VT Y1,k Y2 are projected subspaces, have Yk(cid:12)(cid:12)(cid:12) ≤(cid:0)1 − λ2 k(cid:1) ¯a1,k(cid:13)(cid:13).
 Using Lemma X1,k X2 are original subspaces (cid:12)(cid:12)(cid:12)(cid:13)(cid:13)VT ¯a1,k(cid:13)(cid:13) hold probability least − d1e−c2,2(ε2)n n > c1,2(ε2)d2, k = d1 − aﬀ (39) Plugging deﬁnition aﬀ have F − aﬀ (cid:12)(cid:12)(cid:12)(cid:13)(cid:13)VT ¯A1(cid:13)(cid:13) Y(cid:12)(cid:12)(cid:12) aﬀ Yk := λ2 k + d2 n (cid:0)1 − λ2 k(cid:1)
 Y (4), (39), (40) second estimation error, d2 k + (d1 − aﬀ d2 (1 − λ2 X )(cid:19)(cid:12)(cid:12)(cid:12)(cid:12) k)(cid:19)(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) d1 =(cid:12)(cid:12)(cid:12)(cid:12)(cid:13)(cid:13)VT F −(cid:18)aﬀ ¯A1(cid:13)(cid:13) X + =(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) −(cid:18)λ2 Xk=1(cid:18)(cid:13)(cid:13)VT ¯a1,k(cid:13)(cid:13) Xk=1(cid:12)(cid:12)(cid:12)(cid:13)(cid:13)VT Yk(cid:12)(cid:12)(cid:12) ¯a1,k(cid:13)(cid:13) − aﬀ Xk=1 (1 − λ2 k).
 ≤ ε2 d1 d1 (41) (40) (42) Step Combining (38) (41) (35), have n > max{c1,1(ε1), c1,2(ε2)}d2, aﬀ Y − aﬀ (cid:12)(cid:12)(cid:12) d1 Y(cid:12)(cid:12)(cid:12) ≤ (ε1 + ε2) Xk=1(cid:0)1 − λ2 k(cid:1) = (ε1 + ε2)(cid:0)d1 − aﬀ X(cid:1) hold probability least − d1e−c2,1(ε1)n − d1e−c2,2(ε2)n.
 Let ε1 = ε2 =: ε/2, according Remark verify exist constants c1, c2 depending ε, n > c1(ε)d2, holds probability least − e−c2(ε)n.
 complete proof.
 Y − aﬀ (cid:12)(cid:12)(cid:12) Y(cid:12)(cid:12)(cid:12) ≤(cid:0)d1 − aﬀ X(cid:1) ε Proof Lemma order improve readability proof, deﬁne variables required advance.
 variables deﬁned are summarized make part self-contained.
 use X1 X2 denote subspaces projection, dimensions d1 d2.
 principal orthonormal bases X1 X2 are denoted U1 U2, respectively.
 kth column Ui is denoted ui,k, spans 1-dimensional subspace denoted Xi,k, k = di, i =
 addition, deﬁne Ui,1:k matrix composed ﬁrst k columns Ui. That is Ui,1:k = [ui,1,··· ui,k] ≤ k ≤ di, i =
 subspace spanned columns Ui,1:k is denoted Xi,1:k = C(Ui,1:k).
 use Y1 Y2 denote subspaces projection, respectively, X1 X2 using standard Gaussian random matrix Φ.
 dimensions Y1 Y2 stay be d1 d2 probability
 Ai = ΦUi is basis Yi kth column is denoted ai,k, spans 1-dimensional subspace denoted Yi,k = C(ai,k).
 deﬁne Ai,1:k = [ai,1,··· composition ﬁrst k columns Ai. subspace spanned columns Ai,1:k is denoted Yi,1:k = C(Ai,1:k).
 use ¯A1 ¯A2 denote column-normalized result A1 A2, respectively.
 V1 V2 are deﬁned orthonormalized result ¯A1 ¯A2, respectively, using Gram-Schmidt orthogonalization.
 consequence, Vi provides orthonormal basis Yi. Similarly, Vi,1:k denotes matrix composed ﬁrst k columns Vi. Let’s start proof Lemma LHS (37).
 According deﬁnition V2, ¯a1,k, Remark have kVT v1,kk2 = kPY2(v1,k)k2 = −(cid:13)(cid:13)(cid:13) ¯a1,kk2 = kPY2(¯a1,k)k2 = −(cid:13)(cid:13)(cid:13) PY ⊥ PY ⊥ (v1,k)(cid:13)(cid:13)(cid:13) (¯a1,k)(cid:13)(cid:13)(cid:13) (43) (44) consequence, LHS (37) is derived diﬀerence squared norm projection v1,k ¯a1,k orthogonal complement Y2, v1,kk2 − kVT (cid:12)(cid:12)kVT ¯a1,kk2(cid:12)(cid:12) =(cid:12)(cid:12)(cid:12)(cid:12)(cid:13)(cid:13)(cid:13) PY ⊥ (v1,k)(cid:13)(cid:13)(cid:13) −(cid:13)(cid:13)(cid:13) PY ⊥ (¯a1,k)(cid:13)(cid:13)(cid:13) order analyze v1,k, take close look Gram-Schmidt orthogonalization process.
 introduce αk := kPY1,1:k−1(¯a1,k)k = kVT cosine only principal angle Y1,k Y1,1:k−1, bk := αk PY1,1:k−1(¯a1,k) (46) (47) unit vector direction projection a1,k Y1,1:k−1.
 consequence, Gram-Schmidt orthogonalization process is represented introduce ¯a1,k = PY1,1:k−1(¯a1,k) + PY ⊥ kv1,k.
 = αkbk +q1 − α2 (¯a1,k) ˆλk := kPY2(¯a1,k)k = kVT βk := kPY2(bk)k = kVT ¯a1,kk, bkk (48) (49) (50) denote, respectively, cosine only principal angle Y1,k Y2 C{bk} Y2.
 projecting side (48) orthogonal complement Y2, have (51) (bk).
 q1 − ˆλ2 k ¯a⊥1,k = αkq1 − β2 kb⊥k +q1 − α2 kPY ⊥ (v1,k), ¯a⊥1,k b⊥k denotes, respectively, unit vectors PY ⊥ (¯a1,k) PY ⊥ Moving ﬁrst item RHS (51) LHS taking norm sides, get (1 − α2 k)kPY ⊥ (v1,k)k2 = − ˆλ2 k + α2 k(cid:0)1 − β2 k(cid:1) − ˆλ2 kq1 − β2 kh¯a⊥1,k, b⊥k i.
 (52) addition, norm projection ¯a1,k Y⊥2 be represented using ˆλk Inserting (52) (53) (45), write kPY ⊥ (¯a1,k)k2 = − ˆλ2 k.
 (53) k + α2 v1,kk2 − kVT (cid:12)(cid:12)kVT =(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) − ˆλ2 k(cid:0)1 − β2 k + − β2 k(cid:16)1 − ˆλ2 α2 ¯a1,kk2(cid:12)(cid:12) k(cid:1) − ˆλ2 − α2 k(cid:17) αkq1 − ˆλ2 − α2 kq1 − β2 kh¯a⊥1,k, b⊥k i − (1 − ˆλ2 kq1 − β2 kh¯a⊥1,k, b⊥k i(cid:12)(cid:12)(cid:12)(cid:12) k)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) (54) Using fact geometric mean is more arithmetic mean α2 be veriﬁed α2 k is small quantity, further k < k +(cid:12)(cid:12)(cid:12) k(cid:17)(cid:16)α2 following, estimate quantities ˆλ2 (cid:12)(cid:12)(cid:12)kVT v1,kk2 −(cid:13)(cid:13)VT ≤ ¯a1,k(cid:13)(cid:13) − ˆλ2 k + − β2 αkh¯a⊥1,k, b⊥k i(cid:12)(cid:12)(cid:12)(cid:17)
 k, β2 k, α2 k, h¯a⊥1,k, b⊥k i (55) separately.
 Using basic algebra, (56) is reshaped λ2 k + d2 n (cid:0)1 − λ2 − ˆλ2 k ≤1 − λ2 k − =(cid:0)1 − λ2 <(cid:0)1 − λ2 d2 k(cid:1) − ˆλ2 (cid:0)1 − λ2 k(cid:1)(cid:18)1 − k(cid:1) (1 + ε1) k ≤(cid:0)1 − λ2 k(cid:1) ε1, k(cid:1) +(cid:0)1 − λ2 + ε1(cid:19) k = d1.
 k(cid:1) ε1 d2 (56) (57) Let ﬁrst consider − ˆλ2 k.
 Recalling deﬁnition (49), have estimate ˆλ2 k (39).
 Inserting (40) (39), probability least e−c2,1(ε1)n, have n > c1,1(ε1)d2 bound (57) looks direct ˆλk denotes aﬃnity compressed λk.
 According Lemma former be estimated latter.
 Second let check αk = kVT
 Intuitively, X1,1:k−1 is orthogonal X1,k, new subspaces Y1,1:k−1 (projected X1,1:k−1) Y1,k (projected X1,k) are orthogonal other.
 Actually, V1,1:k−1 ¯a1,k satisfy conditions Corollary
 consequence, exist constants c1,2(ε2), c2,2(ε2), such ε2 k < ε2 hold probability least − e−c2,2(ε2)n.
 n > c1,2(ε2)d1 > c1,2(ε2)(k − have α2 consider − β2 k, is bounded − λ2 k, Notice bk lies Y1,1:k−1 ⊂ Y1,1:k βk is norm projection bk Y2.
 minimum norm projection unit vector Y1,1:k Y2 approximates − β2 be k.
 diﬀerence be bounded following lemma.
 Lemma n > c1,3(ε3)d2, have − β2 k ≤(cid:0)1 − λ2 k(cid:1) (1 + ε3) holds probability least − e−c2,3(ε3)n.
 Proof proof is postponed Appendix (58) Finally, last term be estimated, h¯a⊥1,k, b⊥k i is proved be small quantity Lemma
 Intuitively, ¯a⊥1,k b⊥k is unit projections ¯a1,k ∈ Y1,k bk ∈ Y1,1:k−1, respectively, Y⊥2
 Consequently, inner product ¯a⊥1,k b⊥k be small Y1,k Y1,1:k−1, are independent other, are independent Y⊥2
 exist constants c1,4(ε4), c2,4(ε4), such n > c1,4(ε4)d2, have (cid:12)(cid:12)(cid:12)h¯a⊥1,k, b⊥k i(cid:12)(cid:12)(cid:12) Proof proof is postponed Appendix < ε4 holds probability least − e−c2,4(ε4)n.
 Now, are ready complete proof using concentration properties derived < ε4 (55), have above.
 Plugging (57), (58), α2 > max{c1,l(εl)}d2, l = k < ε2, (cid:12)(cid:12)(cid:12)h¯a⊥1,k, b⊥k i(cid:12)(cid:12)(cid:12) ≤ ¯a1,k(cid:13)(cid:13) hold probability least −P4 v1,kk2 −(cid:13)(cid:13)VT − λ2 have k(cid:1) (2 + ε1 + ε3) (ε2 + √ε2ε4) l=1 e−c2,l(εl)n.
 Let ε1 < ε3 < ε2 = ε4 =: ε/12, (cid:12)(cid:12)(cid:12)kVT v1,kk2 −(cid:13)(cid:13)VT ¯a1,k(cid:13)(cid:13) ≤(cid:0)1 − λ2 k(cid:1) ε.
 According Remark claim exist constants c1, c2, such n > c1(ε)d2, (59) holds probability least − e−c2(ε)n.
 Related Works earlier results RIP subspaces [21] are cited below.
 Suppose X1,X2 ⊂ RN are subspaces dimension d1 ≤ d2, respectively.
 X1 X2 are projected Rn Gaussian random matrix Φ ∈ Rn×N Xk −→ Yk, k = have (1 − ε)D2 X ≤ D2 Y ≤ (1 + ε)D2 X probability least is large enough.
 − (ε − d2/n)2n Theorem set composed L subspaces X1,··· ,XL ∈ RN dimension more d, are projected Rn Gaussian random matrix Φ ∈ Rn×N Xk −→ Yk, k = L, d ≪ n < N have (1 − ε)D2(Xi,Xj) ≤ D2(Yi,Yj) ≤ (1 + ε)D2(Xi,Xj), ∀i, j probability least is large enough.
 − − (ε − d/n)2n Compared previous results, paper has following main improve- ments.
 Firstly, use advanced random matrix theories deal error skillfully, probability e−O(n) derived paper is tighter − O(1/n) previous work, used Chebyshev inequality.
 Such improvement provides accurate law magnitude dimensions random projection problem, improved probability bound is optimum, compares analogical conclusions theory Compressed Sensing.
 Secondly, Theorem requires n ≫ d, does specify large n be connection ε, d, L, lower bound n.
 comparison, Theorem paper clariﬁes conclusion hold n is larger c1(ε) max{d, ln L}.
 Conclusion paper, utilize random matrix theory prove RIP Gaussian random compressions low-dimensional subspaces.
 Mathematically, demonstrate dimension compression n is larger c1(ε) max{d, ln L}, proba- bility less − e−c2(ε)n, distance subspaces compression remains unchanged.
 probability e−O(n) is optimum asymptotic sense, comparison analogical optimum theoretical result RIP Compressed Sensing.
 work provide solid theoretical foundation Compressed Subspace Clustering other low-dimensional subspace related problems.
 Appendix Proof Lemma ﬁrst prove special case K = i.e., f = ae−g short.
 According (13), (14), deﬁnition limitation, exist constants n0 c1 > depending ε.
 n > n0, τ < c1, have g > depending have
 Let c2 := h−b n > h − h−b ln n < b + h−b f = ae−g = e−(g−ln a) = exp(cid:18)−n(cid:18) g n − ≤ exp(cid:18)−n(cid:18)h (cid:19)(cid:19) h − b = exp(cid:18)− h − b h − b − b − n(cid:19) = e−c2n.
 ln n (cid:19)(cid:19) consider general case arbitrary K.
 According above analysis, have that, term f exist constants n0,k, c1,k c2,k > depending ε.
 n > n0,k, τ < c1,k, satisﬁes ake−gk < e−c2,kn.
 Let n0 := maxk n0,k, c1 := mink c1,k > c2 := mink c2,k >
 n > n0, τ < c1, have f = Xk=1 ake−gk < Xk=1 e−nc2,k ≤ e−c2n, complete proof.
 Proof Lemma Regarding √na matrix belonging Rn×1, using Lemma have probability least − t √n − − t ≤ smin(√na) = √nkak = smax(√na) ≤ √n + + t.
 Taking square subtracting n sides, have −(cid:16)2√n(1 + t) + (1 + t)2(cid:17) ≤ −(cid:16)2√n(1 + t) − (1 + t)2(cid:17) ≤ nkak2−n ≤ + t)2 probability least − t choosing ε satisfying nε + t) + (1 + get
 t = √n(cid:0)√1 + ε − −
 =: n0,1, have t
 Substituting equation expression (60) (61) n >(cid:16) probability, have P(cid:16)(cid:12)(cid:12)(cid:12)kak2 − > ε(cid:17) < exp(cid:16)−n(cid:0)√1 + ε − − /2(cid:17)
 According Lemma exist constants n0,2 c dependent ε, such RHS (61) is smaller e−cn.
 Taking n0 = max{n0,1, n0,2}, complete proof.
 Proof Corollary nε d have according (61) proof Lemma replacing ε nε d VTa ∈ Rd is standard Gaussian random vector.
 consequence, Proof Notice thatp n P (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) /2! r n VTa(cid:13)(cid:13)(cid:13)(cid:13) (cid:13)(cid:13)(cid:13)(cid:13) d ε+1−1(cid:19)2 d ε+1−1(cid:19)2 d >(cid:18) n n ≤ d, need n ε =: c1,1d.
 According Lemma exist constants c1,2, c2 de- pendent ε, such RHS (62) is smaller e−c2n.
 Taking c1 := max{c1,1, c1,2} dividing sides expression P(·) (62) complete proof.
 ε − is required.
 order satisfy requirement, i.e.,(cid:18) d exp −d(cid:18)r1 + (62) Proof Corollary order bound s2 min(cid:0) ¯A(cid:1), noticing min(cid:0) ¯A(cid:1) ≥ s2 s2 min (A) max i kaik2 turn estimate s2 min (A) maxi kaik2 separately.
 begin estimating s2 min (A).
 According Lemma probability least − e−t2/2, have s2 min (A) ≥ Let − ε1 be RHS (64) have =(cid:16)1 −pk/n − t/√n(cid:17)2 √k − t(cid:17)2 n(cid:16)√n − t = √n(cid:16)1 − √1 − ε1 −pk/n(cid:17)
 (64) (65) n > (1−√1−ε1)2 =: ˆc0,1k, have t
 Plugging (65) e−t2/2, get /2(cid:19)
 According probability (65) violates exp(cid:18)−n(cid:16)1 −pk/n − √1 − ε1(cid:17)2 Lemma above probability be bounded e−ˆc2,1(ε1)n n ˆc1,1k.
 have s2 min (A) − ε1 (66) hold n > max{ˆc1,1, ˆc0,1}k =: ˆc3k probability least − e−ˆc2,1(ε1)n.
 estimate maxi kaik2.
 According Lemma probability least − ke−ˆc2,2(ε2)n, n > n0,1, have (67) (68) Plugging (66) (67) (63), have i kaik2 + ε2.
 max s2 min(cid:0) ¯A(cid:1) ≥ − ε1 + ε2 = − ε1 + ε2 + ε2 Let ε1 = ε2 =: ε/2, probability least − e−ˆc2,1(ε/2)n − ke−ˆc2,2(ε/2)n, have s2 min(cid:0) ¯A(cid:1) ≥ − (ε/2 + ε/2) = − ε.
 Take c1,1 := max{ˆc3, n0,1} prove ﬁrst part corollary.
 order bound counterparts (63), (66), (67), respectively, max(cid:0) ¯A(cid:1), following same approach, derive step step s2 s2 max (A) min i kaik2 max(cid:0) ¯A(cid:1) ≤ s2 max (A) ≤ + ε1 n > ˆc4k probability least − exp(cid:18)−n(cid:16)1 +pk/n − √1 + ε1(cid:17)2 /2(cid:19) > − e−ˆc2,3n, i kaik2 − ε2 min (69) (70) (71) n > n0,2 probability least − ke−ˆc2,2(ε2)n − e−ˆc2,3(ε1)n.
 have s2 max(cid:0) ¯A(cid:1) ≤ + ε1 − ε2 = + ε1 + ε2 − ε2 (72) Similarly reshaping (72) letting ε2 ε1 = ε2 =: ε/4, taking c1,2 := max{ˆc4, n0,2}, prove second part corollary.
 Proof Lemma Using orthogonality u1 U2, C(A2) is independent a1.
 or- thonormal basis such subspace, V2 is independent a1.
 Then, according Deﬁnition a1 conditioned V2 is standard Gaussian random vector.
 conse- quence, a1 ∈ Rd×1, entries are independent standard Gaussian random variables, satisﬁes condition Lemma
 probability t have (73) (cid:13)(cid:13) Let ε :=(cid:16)√d + + t(cid:17)2 √nVT = s2 a1(cid:13)(cid:13) get max(cid:0)√nVT t = √nε − a1(cid:1) ≥(cid:16)√d + + t(cid:17)2 √d −
 n > ε =: c1,1d, have t > √d − ≥
 Plugging (74) e− t (73) holding is least exp(cid:18)−(cid:16)√nε − √d − /2(cid:19)
 According Lemma exist constants c1,2, c2, such n > c1,2d, probability is smaller e−c2n.
 Taking c1 := max{c1,1, c1,2} dividing sides (73) n, conclude lemma.
 probability Proof Corollary According deﬁnition ¯a1 basic probability, have P(cid:16)(cid:13)(cid:13)VT ¯a1(cid:13)(cid:13) > ε(cid:17) =P (cid:13)(cid:13)VT ka1k2 > ε! a1(cid:13)(cid:13) ka1k2 < ε! =1 − P (cid:13)(cid:13)VT a1(cid:13)(cid:13) ≤1 − P(cid:16)ka1k2 > − ε (cid:13)(cid:13)VT < ε (1 − ε)(cid:17) a1(cid:13)(cid:13) =P(cid:16)ka1k2 < − ε (cid:13)(cid:13)VT > ε (1 − ε)(cid:17) a1(cid:13)(cid:13) ≤P(cid:0)ka1k2 < − ε(cid:1) + P(cid:16)(cid:13)(cid:13)VT > ε (1 − ε)(cid:17)
 a1(cid:13)(cid:13) estimate items RHS (75), separately.
 using Lemma n > n0, have P(ka1k2 < − ε) < e−c2,1(ε)n.
 using Lemma ε < n > c1,2d, have Plugging (76) (77) (75), get P(cid:16)(cid:13)(cid:13)VT a1(cid:13)(cid:13) P(cid:16)(cid:13)(cid:13)VT ¯a1(cid:13)(cid:13) n > c1(ε)d, have (cid:13)(cid:13)VT ¯a1(cid:13)(cid:13) Proof Remark > ε(1 − ε)(cid:17) < e−c2,2(2ε/3)n.
 > ε(cid:17) < e−c2,1(ε)n e−c2,2(2ε/3)n.
 According Remark claim exist constants c1, c2, such > ε probability e−c2(ε)n.
 (77) (78) Replacing n n − d0 (20), get P(cid:16)(cid:13)(cid:13)VT ¯a1(cid:13)(cid:13) > ε(cid:17) ≤ e−ˆc2(ε)(n−d0).
 need prove exist constants c1, c2, n > c1 max{d, d0}, have n according Lemma have P(cid:16)(cid:13)(cid:13)VT ¯a1(cid:13)(cid:13) > ε(cid:17) ≤ e−ˆc2(ε)(n−d0) e−c2n.
 Let τ := d0 ˆc2(ε) (n − d0) = lim τ→0 h = lim τ→0 b = lim n > n0, τ = d0 choosing c1 := max{n0, condition (78) holding, is n > ˆc1d, is satisﬁed.
 lim n→∞ ln n ≤ τ0, exists constant c2, such e−ˆc2(ε)(n−d0) ≤ e−c2n.
 n ≤ τ0.
 ˆc1}, n > c1d0, have n > n0, τ = d0 ˆc2 (1 − τ = ˆc2 lim n→∞ = < h.
 Proof Lemma Using deﬁnition βk (50) fact Y1,1:k−1 ⊂ Y1,1:k, have β2 k bk(cid:13)(cid:13) = min kbk=1 b∈Y1,1:k−1 kVT bk2 ≥ min kbk=1 b∈Y1,1:k kVT bk2.
 Removing side (79) one, write − β2 k ≤ − min kbk=1 b∈Y1,1:k kVT bk2 = − min kbk=1 b∈Y1,1:k kPY2(b)k2 = max kbk=1 b∈Y1,1:k kPY ⊥ (b)k2.
 (79) (80) loose condition rewrite expression maximization problem step step, convert problem extreme singular value random matrix.
 vector b Y1,1:k, be spanned columns ¯A1,1:k b = ¯A1,1:kx = xj ¯a1,j, Xj=1 (81) x = [x1,··· xk]T denotes weight vector.
 Consequently, condition kbk be loosen condition x, i.e., kxk2 ≤ (cid:13)(cid:13) s2 ¯A1,1:kx(cid:13)(cid:13) min(cid:0) ¯A1,1:k(cid:1) Inserting (81) (82) (80), have − β2 k ≤ max s2 s2 kbk2 min(cid:0) ¯A1,1:k(cid:1) kxk2≤xu(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)  Xj=1 PY ⊥ kxk2≤xu(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) Xj=1 xjPY ⊥ kxk2≤xu(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) Xj=1 min(cid:0) ¯A1,1:k(cid:1) (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) xj ¯a1,j (¯a1,j)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) j ¯a⊥1,j(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) xjq1 − ˆλ2 = max = max =: xu.
 (82) (83) ˆλj is deﬁned (49).
 According (57) decreasing order λ1 ≥ ··· ≥ λd1, have − ˆλ2 j ≤ (1 − λ2 j )(1 + ε) ≤ (1 − λ2 k)(1 + ε), ∀j = < d1, (84) hold probability least − e−c2,1(ε1)n n > c1,1(ε1)d2.
 Inserting (84) (83), have − β2 k ≤ (1 − λ2 k)(1 + ε1) max ≤ (1 − λ2 k)(1 + ε1)s2 = (1 − λ2 k)(1 + ε1) Xj=1 xj ¯a⊥1,j(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) kxk2≤xu(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) max(cid:16) ¯A⊥1,1:k(cid:17) xu max(cid:16) ¯A⊥1,1:k(cid:17) min(cid:0) ¯A1,1:k(cid:1) s2 s2 (85) ¯A⊥1,1:k =h¯a⊥1,1,··· ¯a⊥1,ki
 need bound denominator numerator RHS (85), separately.
 denominator, according Corollary have n > c1,2(ε2)d1, P(cid:0)s2 min(cid:0) ¯A1,1:k(cid:1) > − ε2(cid:1) > − e−c2,2(ε2)n.
 (86) estimating numerator, A1,1:k is correlated Y2, apply available lemmas concentration inequalities independent Gaussian random matrix.
 However, using following techniques, manage convert problem estimating s2 normalized random matrix satisfying independence condition.
 max(cid:16) ¯A⊥1,1:k(cid:17) problem singular value Remark Recalling Remark characteristics principal orthonormal bases U1, U2 subspaces X1,X2, following decomposition way proof Lemma decompose column U1 projections X2 orthogonal complement get U1 = U2Λ + U0Λ⊥, (87) Λ⊥ = p1 − λ2


   d1 q1 − λ2 U0 =
 C(U0) ∈ RN×d1 is subspace X ⊥2 is UT random projection, decomposition Remark changes A1 = A2Λ + A0Λ⊥, (88) A0 = ΦU0.
 Projecting side (88) orthogonal complement Y2, have (A1) = PY ⊥ means normalized column PY ⊥ identical normalized column PY ⊥ That is PY ⊥ (A0)Λ⊥, (A1), i.e., ¯a⊥1,k deﬁned (51) are (A0), is denoted ¯a⊥0,k, k = d1.
 ¯A⊥1:1:k =h¯a⊥0,1,··· ¯a⊥0,ki =: ¯A⊥0,1:k.
 Considering property isotropy, Gaussian random vector remains Gaussian distri- bution is projected independent subspace.
 is demonstrated Remark
 Remark Let A1 ∈ Rn×d1 A2 ∈ Rn×d2, d1 ≤ be Gaussian random matrices.
 denote V2 orthonormal basis C(A2).
 projection A1 = [a1,1,··· C(A2) is denoted B1 = [b1,1,··· b1,d1], i.e., b1,k = PC(A2)(a1,k).
 A1 A2 are independent, have B1 = V2Ω, Ω ∈ Rd2×d1 is Gaussian random matrix.
 be veriﬁed using fact B1 = V2VT A1 =: V2Ω, Ω = (ωi,j) := VT A1.
 A1 is independent C(A2), orthonormal basis V2, distribution A1 is inﬂuenced, ﬁrst condition V2 regard given matrix.
 check ωi,j are i.i.d. mean Gaussian random variables.
 Recalling UT U2 = means uT = ≤ i ≤ d1, ≤ j ≤ d2.
 According Lemma have a0,i a2,j are independent.
 Moreover, a0,i is independent Y2 independent orthogonal complement, Y⊥2
 according Remark projection A0,1:k ∈ Rn×k Y⊥2 be written A⊥0,1:k :=ha⊥0,1,··· a⊥0,ki = V⊥2 Ω1:k, (a0,j), V⊥2 ∈ Rn×(n−d2) is arbitrary orthonormal basis Y⊥2 a⊥0,j := PY ⊥ Ω1:k ∈ R(n−d2)×k is Gaussian random matrix.
 According orthonormality V⊥2 normalize sides (90) (90) ¯A⊥0,1:k = V⊥2 ¯Ω1:k, (91) ¯Ω1:k denotes column-normalized Ω1:k.
 left multiplying orthonormal matrix does change singular value, have Combining (89) (92), using Remark have smax(cid:16) ¯A⊥0,1:k(cid:17) = smax(cid:0) ¯Ω1:k(cid:1)
 P(cid:16)s2 max(cid:16) ¯A⊥1,1:k(cid:17) < + ε3(cid:17) = P(cid:0)s2 max(cid:0) ¯Ω1:k(cid:1) < + ε3(cid:1) > − e−c2,3(ε3)n (92) (93) hold n > c1,3(ε3)d2.
 Plugging bounds denominator numerator, i.e., (86) (93), (85), get − β2 k(cid:1) (1 + ε1) k(cid:1)(cid:18)1 + probability least −P3 ε3 ≤ ε1 = ε2 = ε3 =: ε/7, have ε2+ε3 Remark reshape probability, complete proof.
 k ≤(cid:0)1 − λ2 =(cid:0)1 − λ2 l=1 e−c2,l(εl)n n > max{c1,l(εl)}d2.
 Let ε2 ≤ ≤ (ε2 + ε3) + = ε.
 using − ε2(cid:19) + ε3 − ε2 ε2 + ε3 − ε2 + ε3 + ε1 + ε1 (94) Proof Lemma calculate inner product ¯a⊥1,k b⊥k
 Recalling bk is projection a1,k C(A1,1:k−1), is obvious ¯a⊥1,k b⊥k are independent, aggravate problem estimate inner product directly.
 order solve this, therefore, have ﬁnd relationship product projection convert problem situation described Remark
 Recalling previous result ¯a⊥1,k = ¯a⊥0,k (89) using fact b⊥k ∈ C( ¯A⊥1,1:k−1), write (cid:12)(cid:12)(cid:12)h¯a⊥1,k, b⊥k i(cid:12)(cid:12)(cid:12) =(cid:12)(cid:12)(cid:12)h¯a⊥0,k, b⊥k i(cid:12)(cid:12)(cid:12) ≤(cid:13)(cid:13)(cid:13) =(cid:13)(cid:13)(cid:13) PC( ¯A⊥ PC( ¯A⊥ (95) need construct orthonormal basis C( ¯A⊥0,1:k−1) build connection ¯a⊥0,k.
 Recalling Remark deduction proof Lemma reshape h ¯A⊥0,1:k−1, ¯a0,ki = V⊥2 (cid:2) ¯Ω1:k−1, ¯ωk(cid:3) ¯ωk denotes last column ¯Ω1:k ∈ R(n−d2)×k.
 next apply Gram-Schimidt orthogonalization Ω1:k−1 get W1:k−1, is orthonormal basis C( ¯Ω1:k−1).
 orthonormality V⊥2 V⊥2 W1:k−1 is orthonormal basis C( ¯A⊥0,1:k−1).
 consequence, are able calculate RHS (95) (cid:13)(cid:13)(cid:13) PC( ¯A⊥ =(cid:13)(cid:13)(cid:13)(cid:13)(cid:16)V⊥2 W1:k−1(cid:17)T =(cid:13)(cid:13)(cid:13)(cid:13)(cid:16)V⊥2 W1:k−1(cid:17)T =(cid:13)(cid:13)WT ¯ωk(cid:13)(cid:13)
 ¯a⊥0,k(cid:13)(cid:13)(cid:13)(cid:13) V⊥2 ¯ωk(cid:13)(cid:13)(cid:13)(cid:13) (96) Recalling ¯Ω1:k is column-normalized Gaussian random matrix, be inde- pendent column ¯Ω1:k−1, independent C( ¯Ω1:k−1) = C(W1:k−1).
 Combining (95) (96), using Remark have P(cid:18)(cid:12)(cid:12)(cid:12)h¯a⊥1,k, b⊥k i(cid:12)(cid:12)(cid:12) > ε4(cid:19) ≤ P(cid:16)(cid:13)(cid:13)WT ¯ωk(cid:13)(cid:13) n ≥ c1,4d2.
 proof is completed.
 > ε4(cid:17) < e−c2,4(ε4)n (97) References [1] E.
 Elhamifar R.
 Vidal, “Sparse subspace clustering,” IEEE Conference Com- puter Vision Pattern Recognition (CVPR), pp.

 [2] M.
 Soltanolkotabi E.
 J.
 Candes, “A geometric analysis subspace clustering Annals Statistics, vol.
 no.
 pp.

 [3] E.
 Elhamifar R.
 Vidal, “Sparse subspace clustering: Algorithm, theory, ap- plications,” IEEE Transactions Pattern Analysis Machine Intelligence, vol.
 no.
 pp.

 R.
 Heckel H.
 B¨olcskei, “Robust subspace clustering thresholding,” IEEE Trans- actions Information Theory, vol.
 no.
 pp.

 [5] X.
 Mao Y.
 Gu, “Compressed subspace clustering: case study,” IEEE Global Conference Signal Information Processing, pp.

 [6] R.
 Heckel, M.
 Tschannen, H.
 Bolcskei, “Subspace clustering dimensionality- reduced data,” IEEE International Symposium Information Theory, pp.

 [7] R.
 Heckel, M.
 Tschannen, H.
 B¨olcskei, “Dimensionality-reduced subspace cluster- ing,” arXiv preprint arXiv:1507.07105,
 [8] Y.
 Wang, Y.-X.
 Wang, A.
 Singh, “A theoretical analysis noisy sparse subspace clustering dimensionality-reduced data,” arXiv preprint arXiv:1610.07650,
 [9] W.
 B.
 Johnson J.
 Lindenstrauss, “Extensions lipschitz maps hilbert space,” Contemporary mathematics, vol.
 pp.

 [10] S.
 Dasgupta A.
 Gupta, “An elementary proof johnson-lindenstrauss lemma,” International Computer Science Institute, Technical Report, pp.

 [11] E.
 J.
 Candes T.
 Tao, “Decoding linear programming,” IEEE Transactions Information Theory, vol.
 no.
 pp.

 [12] E.
 J.
 Cand`es, “The restricted isometry property implications compressed sensing,” Comptes Rendus Mathematique, vol.
 no.
 pp.

 [13] R.
 Baraniuk, M.
 Davenport, R.
 Devore, M.
 Wakin, “A simple proof restricted isometry property random matrices,” Constructive Approximation, vol.
 no.
 pp.

 [14] D.
 L.
 Donoho, “Compressed sensing,” IEEE Transactions Information Theory, vol.
 no.
 pp.

 [15] E.
 J.
 Candes, J.
 Romberg, T.
 Tao, “Robust uncertainty principles: exact signal reconstruction incomplete frequency information,” IEEE Transactions Information Theory, vol.
 no.
 pp.

 [16] S.
 Aeron, V.
 Saligrama, M.
 Zhao, “Information theoretic bounds compressed sensing,” IEEE Transactions Information Theory, vol.
 no.
 pp.

 [17] E.
 Candes J.
 Romberg, “Sparsity incoherence compressive sampling,” In- verse problems, vol.
 no.
 p.

 Y.
 C.
 Eldar G.
 Kutyniok, Compressed sensing: theory applications.
 Cam- bridge University Press,
 [19] A.
 Eftekhari M.
 B.
 Wakin, “New analysis manifold embeddings signal recov- ery compressive measurements,” Applied Computational Harmonic Analysis, vol.
 no.
 pp.

 [20] G.
 Kutyniok, A.
 Pezeshki, R.
 Calderbank, T.
 Liu, “Robust dimension reduction, fu- sion frames, grassmannian packings,” Applied Computational Harmonic Anal- ysis, vol.
 no.
 pp.

 [21] G.
 Li Y.
 Gu, “Restricted isometry property gaussian random projection ﬁnite set subspaces,” IEEE Transactions Signal Processing (to appear), arXiv preprint arXiv:1704.02109,
 [22] A.
 Edelman, T.
 A.
 Arias, S.
 T.
 Smith, “The geometry algorithms orthogo- nality constraints,” SIAM Journal Matrix Analysis Applications, vol.
 no.
 pp.

 [23] M.
 A.
 Davenport, P.
 T.
 Boufounos, M.
 B.
 Wakin, R.
 G.
 Baraniuk, “Signal pro- cessing compressive measurements,” IEEE Journal Selected Topics Signal Processing, vol.
 no.
 pp.

 [24] T.
 Blumensath M.
 E.
 Davies, “Sampling theorems signals union ﬁnite-dimensional linear subspaces,” IEEE Transactions Information Theory, vol.
 no.
 pp.

 [25] P.
 K.
 Agarwal, S.
 Har-Peled, H.
 Yu, “Embeddings surfaces, curves, moving points euclidean space,” Proceedings twenty-third annual symposium Computational geometry.
 pp.

 [26] A.
 Magen, “Dimensionality reductions preserve volumes distance aﬃne spaces, algorithmic applications,” Randomization approximation tech- niques computer science, pp.

 [27] C.
 Jordan, “Essai sur Bulletin Soci´et´e France, vol.
 pp.

 [28] A.
 Gal´antai H.
 C.
 J., “Jordan’s principal angles complex vector spaces,” Nuner- ical Linear Algebra Applications, vol.
 pp.

 [29] A.
 Bj¨orck G.
 H.
 Golub, “Numerical methods computing angles linear subspaces,” Mathematics Computation, vol.
 pp.

 [30] K.
 R.
 Davidson S.
 J.
 Szarek, “Local operator theory, random matrices banach spaces,” Handbook Banach Spaces, pp.


 successful identification drug-target interactions (DTI) is critical step drug discovery.
 field drug discovery expands discovery new drugs, repurposing existing drugs identification novel interacting partners approved drugs is gaining interest [40].
 recently, DTI pre- diction was approached binary classification problem [4,7,8,14,23,41,50,55], neglecting important piece information protein-ligand interactions, binding affinity values.
 Binding affinity provides information strength interaction drug-target (DT) pair is ex- pressed measures such dissociation constant (Kd), inhibition constant (Ki), half maximal inhibitory concentration (IC50).
 IC50 depends con- centration target ligand [9] low IC50 values signal strong binding.
 Similarly, low Ki/Kd values indicate high binding affinity (i.e. good inhibitors have around Ki lower).
 Ki/Kd values are represented terms pKd pKi, negative logarithm binding inhibition constants.
 binary classification based DTI prediction studies, construction data sets constitutes major problem, negative (not-binding) information is provided.
 most cases, DT pairs binding infor- mation is known are treated negative (not-binding) samples.
 lack true-negative samples study deals generation synthetic negative samples affects performance prediction algorithms.
 other hand, formulating DT prediction problem binding affinity prediction, enables creation realistic data sets, binding affinity scores are used, obviating need generation syn- thetic negative samples.
 Prediction protein-ligand interaction binding affinities has been focus protein-ligand scoring, is used virtual screening docking campaigns order predict putative strengths proposed ligands target identifying active inactive compounds [43].
 Non-parametric machine learning methods such Random Forest algo- rithm have been used successful alternative parametric scoring functions last decade order prevent dependency parameters [3,36,45].
 Later, Gabel al.
 showed RF-score failed virtual screening docking tests, speculating using features such co-occurrence atom-pairs over- simplified description protein-ligand complex led loss information raw interaction complex provide [20].
 same time study was published, deep learning started become popu- lar architecture powered increase data high capacity computing machines challenging machine learning methods.
 Inspired remarkable success rate image processing [13, speech recognition [15,25,30], learning methods are being used many other research fields, including bioinformatics such genomics studies [35, quantitative-structure activity relationship (QSAR) studies drug discovery [37].
 major advantage deep learning architectures is enable better representations raw data non-linear transformations layer [34] thus, learning hidden patterns data.
 few studies employing Deep Neural Networks (DNN) have been proposed DTI binary class prediction using different input models proteins drugs [10, ones employ stacked auto-encoders [52] deep-belief networks [53].
 Similarly, stacked auto-encoder based models Recurrent Neural Networks (RNNs) Convolutional Neural Networks (CNNs) were applied represent chemical genomic structures real-valued vector forms [22,31].
 Deep learning approaches have been applied protein-ligand interaction scoring common application has been use CNNs learn structures protein-ligand complexes [21,
 However, approach is limited known protein-ligand complex structures, ligands reported PDB [44].
 Recently, SimBoost method was proposed predict binding affinity scores gradient boosting machine using feature engineering repre- sent drug-target interactions [28].
 utilized similarity-based information DT pairs features were extracted network-based interactions pairs.
 Pahikkala al., other hand, employed Kronecker Reg- ularized Least Squares (KronRLS) algorithm utilized similarity-based representations drugs targets using 2D-based compound similar- ity method Smith-Waterman algorithm, [42].
 studies used traditional machine learning algorithms utilized 2D-representations compounds order provide similarity information.
 study, propose approach predict binding affinities protein-ligand interactions deep learning models using sequences (1D representations) proteins ligands.
 end, sequences pro- teins SMILES (Simplified Molecular Input Line Entry System) representa- tions compounds are used external features 3D-structures binding complexes limit data set.
 employ CNN blocks learn better representations raw protein sequences SMILES strings combine representations feed fully-connected layer block termed DeepDTA.
 used Davis Kinase binding affinity data set [16] evaluate performance model compared results KronRLS algorithm [42].
 results showed model uses separate CNN-based blocks represent proteins drugs performed KronRLS algorithm.
 model uses CNN-block learn SMILES S-W similarity based representation, achieved highest performance Concor- dance Index (CI) outperforming KronRLS algorithm (0.871) task predicting binding affinities DT pairs.
 performed better KronRLS task binding affinity prediction novel drugs known proteins.
 Materials Methods Data set evaluated proposed model Kinase data set [16] suggested [42] be used benchmark data set binding affinity prediction evaluation.
 Davis data set contains selectivity assays kinase protein family relevant inhibitors respective disassociation constant (Kd) values.
 data set comprises interactions proteins ligands, reported Table
 final aim model is predict binding affinity values.
 Table
 Data set Davis (Kd) Proteins Compounds Interactions Pahikkala al.
 used Kd values Davis data set directly, used values transformed log space pKd [28] explained Equation
 Kd pKd = −log10( (1) Figure illustrates distribution binding affinity values pKd form.
 observe peak pKd value (10000nM constitutes more half data set (20931
 values correspond negative pairs have weak binding affinities (Kd are observed primary screen [42].
 Figure
 Distribution binding affinity (pKd) values Davis data set compound SMILES strings were extracted Pubchem compound database based Pubchem CIDs [5].
 Figure illustrates distribution lengths SMILES strings compounds Davis data set.
 maximum length SMILES is average length is equal
 protein sequences Davis data set were extracted UniProt protein database based gene names/RefSeq accession numbers [2].
 Figure shows lengths sequences proteins Davis data set.
 maximum length protein sequence is average length is characters.
 Input Representation experimented input representation approaches have been com- used deep-learning based studies: one-hot encoding integer/label encoding.
 One-hot encoding is way representing categorical variables binary vector form.
 given set categories, entry binary vector is set corresponding label is set otherwise.
 scanned Figure
 Summary Davis data set.
 A) Distribution lengths SMILES strings B) Distribution lengths protein sequences SMILES sequences collected Pubchem com- piled labels letters).
 protein sequences, scanned protein sequences UniProt categories (unique letters) were extracted.
 example illustrates one-hot representation example SMILES belongs methyl isocyanate, “CN=C=O”.
 character SMILES, corresponding position is set
    C H N O


 n c = +

















  popular form input representation is use integers cate- gories (label/integer encoding).
 represent label corresponding integer (e.g. “C”:1, “H”:2, ‘N”:3 etc.).
 Label encoding example SMILES, “CN=C=O”, is given below.
 (cid:2)C N = C = O(cid:3) = (cid:2)1 Similar SMILES, protein sequences are encoded same fashion using one-hot label encodings.
 SMILES protein sequences have varying lengths.
 Hence, order create effective representation form, decided fixed maximum lengths SMILES protein sequences.
 chose maximum lengths based distributions illus- trated Figure maximum lengths cover most data set.
 sequences are longer maximum length are truncated, whereas shorter sequences are 0-padded.
 Proposed Model study treated protein-ligand interaction prediction regression problem aiming predict binding affinity scores.
 prediction model, adopted popular deep learning architecture, Convolutional Neural Network (CNN).
 CNN is architecture contains more convolutional layers followed pooling layer.
 pooling layer down-samples output previous layer provides way generalization features are learned filters.
 top convolutional pooling layers, model is completed more connected layers (FC).
 powerful feature CNN models is ability capture local dependencies help filters.
 Therefore, number size filters CNN affects kind features model learns input.
 is reported number filters increases, model becomes better recognizing patterns.
 proposed CNN-based prediction model comprises separate CNN blocks, aims learn representations SMILES strings protein sequences.
 CNN block, used consecutive 1D- convolutional layers increasing number filters.
 second third convolutional layers had double triple number filters first had, respectively.
 convolutional layers were followed max-pooling layer.
 final features max-pooling layers were concatenated fed fully-connected (FC) layers, named DeepDTA.
 used nodes first FC followed dropout layer rate Dropout is regularization technique is used avoid over-fitting setting activation neurons [48].
 third layer consisted nodes was followed output layer.
 proposed model combines CNN blocks is illustrated Figure
 activation function, used Rectified Linear Unit (ReLU) [38], g(x) = max(0, x), has been used deep learning studies [34].
 learning model tries minimize difference expected (real) value prediction training.
 work regression task, employed mean squared (MSE) loss function, P is prediction vector, Y corresponds vector actual outputs.
 indicates number samples.
 M SE = (Pi − Yi)2 i=1 (2) Figure
 DeepDTA model CNN blocks learn compound SMILES protein sequences.
 learning was completed epochs mini-batch size was used update weights network.
 Adam was used optimization algorithm train networks [33] default learning rate
 compared input representation one-hot label encoding, experimented ways feeding data prediction sys- tem.
 one-hot encoding, fed encoded data model, whereas label-encoding used Keras’ Embedding layer represent charac- ters 128-dimensional dense vectors.
 input consisted (85,128) dimensional matrices compounds proteins, respectively.
 Results Baseline baseline chose model presented Pahikkala coworkers employed Kronecker Regularized Least Squares (KronRLS) algorithm bind- ing affinity prediction [42].
 KronRLS aims minimize following function, f is prediction function [42]: J(f = i=1 (yi − f (xi))2 + λ||f ||2 (3) ||f ||2 k is norm f is related kernel function k, λ > is regularization hyper-parameter defined user.
 minimizer Equation be defined follows [32]: f (x) = i=1 aik(x, xi) (4) k is kernel function.
 order represent compounds, utilized similarity matrix was computed using SIMCOMP, tool utilizes properties compounds [27].
 Smith-Waterman algorithm was used construct protein similarity matrix [47].
 Evaluation evaluate performance model outputs continuous Concor- dance Index (CI) was used [24]: CI = Z X δi>δj h(bi − bj) (5) bi is prediction value larger affinity δi, bj is prediction value smaller affinity δj, Z is normalization constant, h(m) is step function [42]: h(x) = x > x = x < (6)   used paired-t test statistical significance tests confidence interval.
 Experiment Setup evaluated performance proposed model Davis data set [16] [42].
 used nested-cross validation decide best parameters test set.
 order learn generalized model, divided data set equal parts part is selected independent test set.
 remaining parts data set were used determine hyper- parameters five-fold cross validation.
 Figure illustrates partitioning data set.
 same setting was run [42] fair comparison.
 Figure
 Experiment setup.
 decided hyper-parameters model, number filters (same proteins compounds), length filter size compounds, length filter size proteins.
 chose experiment different filter lengths compounds proteins common one, due fact have different alphabets terms characters.
 hyper-parameter combination provided best average CI score five-folds was chosen best combination order model test set.
 experimented hyper-parameters chosen wide range fine-tuned model.
 example, determine number filters performed search
 explained Proposed Model subsection, second convolution layer was set contain number filters first layer, third was set contain times number filters first layer.
 filters obtained best results cross-validation experiments.
 Therefore, final model, CNN block consisted convolutions filters, respectively.
 test results reported Table used same structure summarized Table lengths filters were used compound CNN-block protein CNN-block.
 Table
 Parameters setting DTA model Parameters Number filters Filter length (compounds) Filter length (proteins) epoch hidden neurons batch size dropout learning rate (lr) Range [4,5,6,8] [4,6,8,12] Adam order provide robust performance measure, evaluated performance independent test set, model was trained learned parameters Table training sets used five-fold cross validation (note validation sets were used).
 final CI score was reported average results.
 Keras [12] Tensorflow [1] back-end was used development framework.
 experiments were run OpenSuse (3.50GHz Intel(R) Xeon(R) GeForce GTX (8GB)).
 work was accelerated running GPU cuDNN [11].
 Performance study, proposed deep-learning based model uses CNN- blocks learn representation drugs targets using sequences.
 baseline comparison, KronRLS algorithm uses similarity matrices proteins compounds input was chosen.
 Smith-Waterman (S-W) SIMCOMP algorithms were used compute pairwise similarities proteins ligands, respectively.
 illustrated CNN blocks are able represent proteins ligands.
 first, used S-W SIMCOMP similarity scores inputs fed combination scores FC part model (DeepDTA), consists hidden layers output layer.
 experimented alternative combinations: (i) learning compound representation CNN block using S-W similarity protein representation (ii) learning protein sequence representation CNN block using SIMCOMP describe compounds.
 reported performance models use CNN blocks one-hot categorical representations.
 Table reports average MSE CI scores independent test set models trained same parameters (shown Table using different training sets.
 Table
 average CI MSE scores test set different training sets.
 Proteins Smith-Waterman KronRLS [42] Smith-Waterman DeepDTA CNN DeepDTA (label) DeepDTA (one-hot) CNN DeepDTA (label, hot) CNN CNN DeepDTA (label) CNN DeepDTA (one-hot) Smith-Waterman CNN DeepDTA (label) DeepDTA (one-hot) Smith-Waterman CNN Compounds CI (std) SIMCOMP (0.0008) SIMCOMP (0.003) (0.006) CNN CNN (0.003) CNN (0.005) SIMCOMP (0.004) SIMCOMP (0.004) (0.004) (0.003) MSE Using fully-connected part neural networks (DeepDTA) S-W SIMCOMP similarity scores describe proteins drugs was formed baseline, KronRLS algorithm.
 combined CNN model proposed, other hand, performs baseline one-hot label encoded inputs.
 model compound represen- tation was built CNN block, however, achieved best CI score statistical significance baseline one-hot label encoding (p-value=0.0004 p-value=0.0014, respectively).
 MSE values models were less MSE baseline model.
 model protein representations were built CNN block, one-hot encoding label-encoding, model performed poorly.
 be due reasons: i) CNN model learn amino-acid sequences, ii) SIMCOMP represent compounds SMILES based CNN representation.
 One-hot encoding is used is ordered relationship be- variables, label encoding brings ordinal relationships data don’t exist.
 model compounds were repre- sented CNN-based learning proteins were represented S-W simi- larity scores, difference performances one-hot encoding label encoding was considered significant p-value
 performing better one-hot encoding CI score (ranking) based evaluation metric, observed model produced smallest MSE value label-encoded SMILES inputs.
 used one-hot encoded CNN S-W based DeepDTA model evaluate performance harder DTI prediction problem, was predict new drugs known proteins.
 model (optimized using Stochastic Gradient Descent [6], lr=0.01) produced average CI score KronRLS algorithm SIMCOMP S-W had average CI score outperforming baseline statistical significance (p-value=0.035).
 CNN-based protein sequence SIMCOMP-similarity based com- pound representation however, see label-encoding model performed better one-hot encoding model statistical signifi- cance (p-value=0.016).
 results were complementary existing knowledge amino-acid substitution matrices [29] indicating order amino-acids is important protein sequences.
 Therefore, tested com- bined CNN based model amino-acid sequences were represented label encoding SMILES were represented one-hot encoding.
 results indicated CI score (0.878) improved homogeneous models, significantly.
 Table
 average CI scores test set different training sets one-hot encoding.
 Proteins Compounds CI (std) DeepDTA (SMIlen=103) Smith-Waterman CNN Smith-Waterman CNN DeepDTA (SMIlen=64) DeepDTA (SMIlen=85) Smith-Waterman CNN (0.004) (0.003) (0.003) MSE obtained best performance one-hot encoded CNN S-W combined DTA model, decided observe maximum length SMILES string affected performance prediction.
 Table reports performances CNN S-W combined models maximum length SMILES was chosen length longest SMILES (103), average SMILES Davis (64) choice (85).
 observed is significant difference CI scores.
 Discussion study, proposed deep-learning based approach predict drug-target binding affinity using only sequences proteins drugs.
 used Convolu- tional Neural Networks learn representations raw sequence data proteins drugs.
 compared performance proposed model recent study employed KronRLS regression base- line.
 model CNN-blocks performed baseline, model uses CNN learn compound representations SMILES S-W compute protein similarity amino-acid sequences achieved performance KronRLS based algorithm statistical significance.
 showing SMILES based compound representation coupled S-W protein similarity had highest score prediction drug target interactions drugs targets were present training data set, tested effectiveness methodology data set proteins were encountered drugs were novel.
 model performed baseline KronRLS algorithm pre- diction task, requires better representation drugs aims predict affinities novel compounds.
 successful performance model task predicting affinities novel drugs supports effectiveness CNN architecture describing compounds using SMILES strings.
 investigated effect use different input representation tech- niques SMILES amino-acid sequences performance proposed models.
 SMILES strings, one-hot encoding based SMILES model produced highest CI score, whereas label encoding based SMILES model produced lowest mean square error (MSE) value.
 other hand, amino-acid sequences were represented label-encoding, considers or- dinal information integers, one-hot encoding.
 be indication amino-acids require structure handle ordered relationships, CNN architecture failed capture successfully.
 Long-Short Term Memory (LSTM), is special type Recurrent Neural Networks (RNN), be suitable approach learn protein se- quences, architecture has memory blocks allow effective learning long sequence.
 major contribution study is presentation novel deep learning-based model drug target affinity prediction uses character representations proteins drugs.
 SMILES representation compounds was shown be effective predicting affinities novel compounds.
 future focus building effective representation protein sequences.
 large percentage proteins remains untargeted due bias drug discovery field select group proteins due undruggability untapped pool proteins has gained interest protein deorphanizing efforts [18,
 methodology be extended predict affinity known compounds novel protein targets identified ligands prediction affinity novel drug-target pairs.
 Acknowledgments TUBITAK-BIDEB 2211-E Scholarship Program (to HO) BAGEP Award Science Academy (to AO) are acknowledged.
 thank Ethem Alpaydın, Attila G¨ursoy Pınar Yolum helpful discussions.
 Funding work is funded Bogazici University Research Fund (BAP) Grant Number
 References
 M.
 Abadi, A.
 Agarwal, P.
 Barham, E.
 Brevdo, Z.
 Chen, C.
 Citro, G.
 S.
 Corrado, A.
 Davis, J.
 Dean, M.
 Devin, al.
 Tensorflow: Large-scale machine learning heterogeneous distributed systems.
 arXiv preprint arXiv:1603.04467,

 R.
 Apweiler, A.
 Bairoch, C.
 H.
 Wu, W.
 C.
 Barker, B.
 Boeckmann, S.
 Ferro, E.
 Gasteiger, H.
 Huang, R.
 Lopez, M.
 Magrane, al.
 uni- versal protein knowledgebase.
 Nucleic acids research, D119,

 P.
 J.
 Ballester J.
 B.
 Mitchell.
 machine learning approach predict- ing protein–ligand binding affinity applications molecular docking.
 Bioinformatics,

 K.
 Bleakley Y.
 Yamanishi.
 Supervised prediction drug–target in- teractions using bipartite local models.
 Bioinformatics,

 E.
 E.
 Bolton, Y.
 Wang, P.
 A.
 Thiessen, S.
 H.
 Bryant.
 Pubchem: integrated platform small molecules biological activities.
 Annual reports computational chemistry,

 O.
 Bousquet L.
 Bottou.
 tradeoffs large scale learning.
 Advances neural information processing systems, pages

 D.-S.
 Cao, S.
 Liu, Q.-S.
 H.-M.
 Lu, J.-H.
 Huang, Q.-N.
 Hu, Y.-Z.
 Liang.
 Large-scale prediction drug–target interactions using protein sequences drug topological structures.
 Analytica chimica acta,

 D.-S.
 Cao, L.-X.
 Zhang, G.-S.
 Tan, Z.
 W.-B.
 Zeng, Q.-S.
 Xu, A.
 F.
 Chen.
 Computational prediction drug- target interactions us- ing chemical, biological, network features.
 Molecular Informatics,

 R.
 Z.
 Cer, U.
 Mudunuri, R.
 Stephens, F.
 J.
 Lebeda.
 Ic 50-to-k i: web-based tool converting ic k i values inhibitors enzyme activity ligand binding.
 Nucleic acids research, W445,

 K.
 C.
 Chan, Z.-H.
 You, al.
 Large-scale prediction drug-target in- teractions deep representations.
 Neural Networks (IJCNN), International Joint Conference on, pages
 IEEE,

 S.
 Chetlur, C.
 Woolley, P.
 Vandermersch, J.
 Cohen, J.
 Tran, B.
 Catanzaro, E.
 Shelhamer.
 cudnn: Efficient primitives deep learning.
 arXiv preprint arXiv:1410.0759,

 F.
 Chollet al.
 Keras,

 D.
 Ciregan, U.
 Meier, J.
 Schmidhuber.
 Multi-column deep neural networks image classification.
 Computer Vision Pattern Recog- nition (CVPR), IEEE Conference on, pages
 IEEE,

 M.
 C.
 Cobanoglu, C.
 Liu, F.
 Hu, Z.
 N.
 Oltvai, I.
 Bahar.
 Predicting drug–target interactions using probabilistic matrix factorization.
 Journal chemical information modeling,

 G.
 E.
 Dahl, D.
 Yu, L.
 Deng, A.
 Acero.
 Context-dependent pre-trained deep neural networks large-vocabulary speech recognition.
 IEEE Transactions Audio, Speech, Language Processing,

 M.
 I.
 Davis, J.
 P.
 Hunt, S.
 Herrgard, P.
 Ciceri, L.
 M.
 Wodicka, G.
 Pallares, M.
 Hocker, D.
 K.
 Treiber, P.
 P.
 Zarrinkar.
 Comprehensive analysis kinase inhibitor selectivity.
 Nature biotechnology,

 J.
 Donahue, Y.
 Jia, O.
 Vinyals, J.
 Hoffman, N.
 Zhang, E.
 Tzeng, T.
 Darrell.
 Decaf: A deep convolutional activation feature generic visual recognition.
 ICML, pages

 A.
 M.
 Edwards, R.
 Isserlin, G.
 D.
 Bader, S.
 V.
 Frye, T.
 M.
 Willson, H.
 Y.
 Frank.
 many roads taken.
 Nature,

 O.
 Fedorov, S.
 M¨uller, S.
 Knapp.
 (un) targeted cancer kinome.
 Nature chemical biology,

 J.
 Gabel, J.
 Desaphy, D.
 Rognan.
 Beware machine learning-based scoring functions danger developing black boxes.
 Journal chemical information modeling,

 J.
 Gomes, B.
 Ramsundar, E.
 N.
 Feinberg, V.
 S.
 Pande.
 Atomic con- volutional networks predicting protein-ligand binding affinity.
 arXiv preprint arXiv:1703.10603,

 R.
 G´omez-Bombarelli, D.
 Duvenaud, J.
 M.
 Hern´andez-Lobato, J.
 Aguilera-Iparraguirre, T.
 D.
 Hirzel, R.
 P.
 Adams, A.
 Aspuru-Guzik.
 Automatic chemical design using data-driven continuous representation molecules.
 arXiv preprint arXiv:1610.02415,

 M.
 G¨onen.
 Predicting drug–target interactions chemical genomic kernels using bayesian matrix factorization.
 Bioinformatics,

 M.
 G¨onen G.
 Heller.
 Concordance probability discriminatory power proportional hazards regression.
 Biometrika,

 A.
 Graves, A.-r.
 Mohamed, G.
 Hinton.
 Speech recognition deep IEEE international conference recurrent neural networks.
 acoustics, speech signal processing, pages
 IEEE,

 M.
 Hamanaka, K.
 Taneishi, H.
 Iwata, J.
 Ye, J.
 Pei, J.
 Hou, Y.
 Okuno.
 Cgbvs-dnn: Prediction compound-protein interactions based deep learning.
 Molecular Informatics,

 M.
 Hattori, N.
 Tanaka, M.
 Kanehisa, S.
 Goto.
 Simcomp/subcomp: chemical structure search servers network analyses.
 Nucleic acids re- search,

 T.
 He, M.
 Heidemeyer, F.
 Ban, A.
 Cherkasov, M.
 Ester.
 Simboost: read-across approach predicting drug–target binding affinities using gradient boosting machines.
 Journal cheminformatics,

 S.
 Henikoff J.
 G.
 Henikoff.
 Amino acid substitution matrices protein blocks.
 Proceedings National Academy Sciences,

 G.
 Hinton, L.
 Deng, D.
 Yu, G.
 E.
 Dahl, A.-r.
 Mohamed, N.
 Jaitly, A.
 Se- nior, V.
 Vanhoucke, P.
 Nguyen, T.
 N.
 Sainath, al.
 Deep neural networks acoustic modeling speech recognition: shared views re- search groups.
 IEEE Signal Processing Magazine,

 S.
 Jastrzkeski, D.
 Lesniak, W.
 M.
 Czarnecki.
 Learning smile (s).
 arXiv preprint arXiv:1602.06289,

 G.
 Kimeldorf G.
 Wahba.
 results tchebycheffian spline func- tions.
 Journal mathematical analysis applications,

 D.
 Kingma J.
 Ba. Adam: A method stochastic optimization.
 arXiv preprint arXiv:1412.6980,

 Y.
 LeCun, Y.
 Bengio, G.
 Hinton.
 Deep learning.
 Nature,

 M.
 K.
 Leung, H.
 Y.
 Xiong, L.
 J.
 Lee, B.
 J.
 Frey.
 Deep learning tissue-regulated splicing code.
 Bioinformatics,

 H.
 Li, K.-S.
 Leung, M.-H.
 Wong, P.
 J.
 Ballester.
 Low-quality struc- tural interaction data improves binding affinity prediction random forest.
 Molecules,

 J.
 Ma, R.
 P.
 Sheridan, A.
 Liaw, G.
 E.
 Dahl, V.
 Svetnik.
 Deep neural nets method quantitative structure–activity relationships.
 Journal chemical information modeling,

 V.
 Nair G.
 E.
 Hinton.
 Rectified linear units improve restricted boltz- mann machines.
 Proceedings 27th international conference machine learning (ICML-10), pages

 M.
 J.
 O’Meara, S.
 Ballouz, B.
 K.
 Shoichet, J.
 Gillis.
 Ligand similarity complements sequence, physical interaction, co-expression gene function prediction.
 PloS one,

 T.
 Oprea J.
 Mestres.
 Drug repurposing: new targets old drugs.
 AAPS journal,

 H.
 ¨Ozt¨urk, E.
 Ozkirimli, A.
 ¨Ozg¨ur.
 comparative study smiles- based compound similarity functions drug-target interaction predic- tion.
 BMC bioinformatics,

 T.
 Pahikkala, A.
 Airola, S.
 Pietil¨a, S.
 Shakyawar, A.
 Szwajda, J.
 Tang, T.
 Aittokallio.
 realistic drug–target interaction predic- tions.
 Briefings bioinformatics, page bbu010,

 M.
 Ragoza, J.
 Hochuli, E.
 Idrobo, J.
 Sunseri, D.
 R.
 Koes.
 Protein– ligand scoring convolutional neural networks.
 J.
 Chem.
 Inf.
 Model,

 P.
 W.
 Rose, A.
 Prli´c, A.
 Altunkaya, C.
 Bi, A.
 R.
 Bradley, C.
 H.
 Christie, L.
 D.
 Costanzo, J.
 M.
 Duarte, S.
 Dutta, Z.
 Feng, al.
 rcsb protein data bank: integrative view protein, gene structural information.
 Nucleic acids research, page gkw1000,

 P.
 A.
 Shar, W.
 Tao, S.
 Gao, C.
 Huang, B.
 Li, W.
 Zhang, M.
 Shahen, C.
 Zheng, Y.
 Bai, Y.
 Wang.
 Pred-binding: large-scale protein–ligand binding affinity prediction.
 Journal enzyme inhibition medicinal chemistry,

 K.
 Simonyan A.
 Zisserman.
 deep convolutional networks large-scale image recognition.
 arXiv preprint arXiv:1409.1556,

 T.
 F.
 Smith M.
 S.
 Waterman.
 Identification common molecular subsequences.
 Journal molecular biology,

 N.
 Srivastava, G.
 E.
 Hinton, A.
 Krizhevsky, I.
 Sutskever, R.
 Salakhut- dinov.
 Dropout: simple way prevent neural networks overfitting.
 Journal Machine Learning Research,

 K.
 Tian, M.
 Shao, S.
 Zhou, J.
 Guan.
 Boosting compound-protein in- teraction prediction deep learning.
 Bioinformatics Biomedicine (BIBM), IEEE International Conference on, pages
 IEEE,

 T.
 van Laarhoven, S.
 B.
 Nabuurs, E.
 Marchiori.
 Gaussian interac- tion profile kernels predicting drug–target interaction.
 Bioinformatics,

 I.
 Wallach, M.
 Dzamba, A.
 Heifets.
 deep convolutional neural network bioactivity prediction structure-based drug discovery.
 arXiv preprint arXiv:1510.02855,

 L.
 Wang, Z.-H.
 You, X.
 Chen, S.-X.
 Xia, F.
 Liu, X.
 Yan, Y.
 Zhou, K.-J.
 Song.
 computational-based method predicting drug–target interactions using stacked autoencoder deep neural network.
 Journal Computational Biology,

 M.
 Wen, Z.
 Zhang, S.
 Niu, H.
 Sha, R.
 Yang, Y.
 Yun, H.
 Lu. Deep- learning-based drug–target interaction prediction.
 Journal Proteome Research,

 H.
 Y.
 Xiong, B.
 Alipanahi, L.
 J.
 Lee, H.
 Bretschneider, D.
 Merico, R.
 K.
 Yuen, Y.
 Hua, S.
 Gueroussov, H.
 S.
 Najafabadi, T.
 R.
 Hughes, al.
 human splicing code reveals new insights genetic determinants disease.
 Science,

 Y.
 Yamanishi, M.
 Araki, A.
 Gutteridge, W.
 Honda, M.
 Kanehisa.
 Prediction drug–target interaction networks integration chemical genomic spaces.
 Bioinformatics,
 decades development, artificial neural network (ANN) shows powerful ability inference, approaching surpassing human level several specific scenarios such image recognition [1, chess games [3].
 reason such success is ANN manage great number parameters deep networks learn complex functions.
 However, interpretability ANN is criticized ANN inferences are difficult be explained concise interaction parameters network.
 Several researches have attempted open “black box” ANN interpret inferences.
 get insight features generated hidden layer ANN is key issue opening “box”.
 Schwartz-Ziv Tishby proposed visualize ANN Information Plane, revealed function efficient representation hidden layers [4].
 Zeiler al.
 investigated activations neural units feature layer ANN, highlighted regions input data are responsible activations [5,
 approach is useful dissect ANN models suggest ways improve [5].
 Szegedy al.
 analyzed ANN models synthesizing samples lead high activations neural units [7, found ANN models are hacked adding certain structured noise image space, revealing supervised learning ANN models ignore common sense target, e.g. mammals have legs generally.
 Recently, neural units ANN feature layer have been interpreted human-interpretable semantic concepts help labeled dataset [9, modeled explanatory graph reveals knowledge hierarchy hidden pre-trained model ANN [11].
 techniques have improved interpretability ANN, cannot interpret inference human, e.g. telling sample is dog cat.
 main reason is ANN is hard learn knowledge common sense single limited empirical dataset.
 Compared ANN, human make inferences interpret inferences based foundations, i.e. experiences knowledge.
 knowledge learned summarized development science technologies, human make better inferences smaller empirical dataset.
 Therefore, is vital “teach” ANN models knowledge improve ANN several aspects: reducing model complexity cost learn new problem, adding interpretability knowledge increasing accuracy.
 Knowledge representation have been used describe richness world computer systems, be understood artificial intelligence used reasoning inferring [12,
 Rule-based representation is common formalisms knowledge representation.
 example, expert systems based set rules have been used computer-aided medical diagnosis data processing analyzing [16, fault diagnosis [18], etc.
 subsequent question is knowledge representation be combined ANN.
 knowledge-based ANN is answer designed topological structure ANN according knowledge input variables represented knowledge dependency structures rules [19-21].
 However, answer is appropriate deep ANN input variables are high-dimensional raw data designers ANN have little idea dependency structures high-dimensional raw data.
 paper proposes Rule-embedded Neural Network (ReNN) makes use knowledge improve performance ANN.
 Rules are used represent domain knowledge common sense computable manner.
 ReNN disassembles “black box” ANN parts: local- Figure
 Computational graph ReNN based inference global-based inference.
 local-based inference handles local patterns be learned empirical datasets, global-based inference introduces rules local patterns have been accumulated human long time.
 Accordingly, knowledge human teachers experiences empirical datasets are combined, contribute global-based inference improve performance.
 differentiate contributions local patterns rules final inferences, improve interpretability neural networks.
 example, apply ReNN time- series detection problem experiments.
 METHOD Figure
 Block design feature-mapping global-mapping FCN architecture time-series data.
 consists convolution layers, batch normalization layers, max pooling (2×) deconvolution layers (2×), concatenation layers, are ordered hollow arrows show.
 filter size last convolution layer is others are
 solid arrows show shallower features output batch normalization layers are fused deeper layers concatenation layers.
 A.
 Basic computation graph Rule-embedded Neural Network (ReNN) is defined rule-modulated map input data, ReNN generates estimation targets follows: (1) (4) f, r g represent feature-mapping block, rule- modulating block, global-mapping block, respectively.
 computational graph ReNN is shown Fig.

 Given supervised-learning problem input data X targets Y.
 ReNN first extracts feature map input data feature-mapping block: (2) output feature map F is designed reflect features targets.
 features be subcomponents targets.
 task is detect face occurrences two- dimensional image, feature map consist neural activations face organs, mouth on.
 features be instances targets.
 task is detect R-peaks electrocardiograph (ECG) seconds, feature map consist neural activations multiple R-peaks.
 Then, ReNN applies rules feature map rule- modulating block, generates rule-modulated map.
 rules are designed according domain knowledge, activating nodes rule-modulated map rule- based evidences be found activations feature map.
 scenario face detection, relative positions face subcomponents be utilized analyze candidate positions scales face occurrences.
 scenario R-peak detection ECG, periodicity variability heart rate be used estimate possibility R- peak-occurrences candidate positions.
 feature map F be used input g accelerate model training procedure.
 Rule-modulating plays important role global mapping ReNN.
 However, rules make ReNN model non- differentiable, leading failure model training gradient-descend-based optimization methods.
 two-stage optimization strategy is adopted overcome limitation.
 feature-mapping block is optimized training dataset global-mapping block is optimized dataset fixing feature-mapping block.
 Same traditional ANN, gradient-descend-based optimization methods are used stage strategy [22].
 B.
 Neural Block design further design ReNN blocks, consider characteristics specific task, such data dimension target scales.
 task R-peak detection one- dimensional ECG be considered example following sections.
 ECG consists time-series data electrical activity heart, are recorded noninvasive electrodes placed skin.
 R-peak ECG is key time point procedure rapid depolarization right left ventricles.
 R-peaks are useful need estimate exact time heartbeat, e.g. calculating heart rate variability [23] wave transit time [24,
 sampling rate ECG is Hz paper, scale R-peak morphology is time-points (0.64 seconds).
 use fully-convolutional network (FCN) [26] design feature-mapping block global-mapping block.
 ReNN{,,}frgFfXRrFˆ,YgXR,XF,XYFeature mapping blockRule modulating blockGlobal mapping blockInput dataLocal-based inferenceGlobal-based inferenceRulesConvolutionBatch normalizationMax pooling (2x)Deconvolution (2x)ConcatenationLayers representation FCN architecture time series data is shown Fig.

 convolution layer is used capture local patterns, max-pooling layer is used summarize local patterns enlarge receptive field neural note deeper layers.
 batch normalization layer is used overcome vanishing gradient problem accelerate training deep neural networks.
 deconvolution layer is used upsample pooled data, pointwise classification be achieved feedforward computation [26].
 concatenation layer is used reserve fine features shallower deeper layers features are coarser due pooling operations.
 know, neural node layers convolution has receptive filed five, max pooling deconvolution layers double receptive field.
 sum, output FCN architecture has receptive field time-points (0.64 seconds).
 receptive field covers local information R-peak morphology, P-Q-R-S-T patterns electrical activity heartbeat.
 receptive field be enlarged max-pooling layers.
 centers are set k represents k-th center time point t.
 li be estimated according SDNN three-sigma rule thumb as: (5) constraint supporting regions be overlapped other, determine many supporting regions be used voting.
 simplicity, set maximum number supporting regions six.
 Rt is calculated as: (6) wi denotes confidence vote region: number output channels convolution layers deconvolution layers are important hyper-parameters control complexity feature layers.
 set output channels layers be same experiments simplicity.
 supporting regions be found due arrhythmia (7) C.
 Rule-modulating block rule-modulating block is designed according knowledge R-peaks ECG.
 distinguish R-peaks noises, human beings model piece ECG signals knowledge heart beats ask questions.
 example, is owner ECG normal sinus rhythm arrhythmias?
 is heart rate?
 is normal sinus rhythm, heart rate be used pick R-peaks noises according time distance neighboring R-peaks.
 knowledge be represented rules.
 first step is analyze knowledge-based concepts feature map.
 concepts are heart rate (HR) standard deviation (SDNN).
 help local-based inference, most R-peaks are prominent easy detect feature map Softmax function.
 refer set time intervals R-peaks Z, constraint seconds is applied intervals according domain knowledge heart rhythm common users.
 assumption Gaussian distribution Z, eliminate abnormal intervals caused noises.
 average standard deviation Z are calculated HR SDNN.
 Second, apply voting algorithm generate rule- Rt is defined modulated map possibility R-peaks time point t according votes supporting regions.
 supporting region is defined according knowledge heart rhythm.
 know, R-peak occurrences, are approximately-integral multiples HR current time point, be used supporting evidences possibility R-peak occurrence current time point.
 supporting regions consist time points are approximately-integral multiples HR away.
 denote supporting regions ci li are centers widths region.
 insufficient time intervals R-peaks, set Rt =
 SETTING section sets experiments evaluate ReNN task R-peak detection ECG.
 ECG has been used firsthand information analyze structure function heart years is becoming popular smart wearable devices monitor status users [28,
 However, is challenging detect R-peaks are several types noises affecting occurrences, such motion muscular activation interference AC interference [29].
 A.
 Dataset experiments, use ECG dataset constructed LOHAS Tech., ECG data are measured sampling rate Hz single-lead device electrodes placed left right arms.
 dataset includes ECG measurements users labels R-peaks ECG measurement.
 Most users are healthy normal sinus rhythm.
 ECG measurement is seconds long, is filtered high-pass filter low-pass filter remove irrelative signal components.
 dataset includes labels R-peaks.
 labels are labeled PT algorithm [30], suspicious labels are checked re-labeled human experts.
 standard deviation R-peak intervals (referred SDNN) is used estimate heart rate variability ECG measurement.
 (97%) ECG measurements dataset are low SDNN (<0.06s), ECG measurements are high SDNN (>0.1s).
 split dataset training dataset testing dataset include ECG measurements, respectively.
 are used model training evaluation.
 tRR,iicltkHR6ilkSDNNmax,1iiiiitiiwcllRl2argmax,212iiiiclcliiwel Table
 Detection performances Local-based inferences Global- based inferences test dataset.
 #C represents number channels FCN model.
 Type Local Local Local Local Local Global Global Global Global Global #C TP FP FN F1-score dataset.
 According F1-score, ReNN achieves low error rate number channels is local- based inference achieves comparable performance channels.
 However, computational cost local-based inference channels is several times ReNN.
 results reveal ReNN make use rule- modulated map enhance accuracy detect R-peaks.
 results reveal ReNN has potential reduce computational cost deep neural networks.
 Fig.
 illustrates example ECG measurements ReNN detection involving output feature-mapping block, rule-modulating block, global-mapping block.
 voltage range mV, signal ECG measurement is weaker standard ECG voltage range is mV.
 are noises be recognized R-peaks.
 Intuitively, is hard distinguish R-peaks signal local patterns.
 Nevertheless, have knowledge heart rhythm user.
 ECG is measured 25-year-old female is healthy history heart diseases.
 Accordingly, be premature beat other abnormal beat ECG measurement, discriminate R-peaks noisy signal heart rate neighboring R-peaks.
 above analyzing process human local patterns global patterns (HR) has been reflected detection process ReNN.
 feature-mapping block detects potential R-peaks according local patterns receptive fields.
 Secondly, rule-modulating block analyzes heart rhythm feature maps, estimates distributions heartbeats based heart rhythm features neighboring heartbeats.
 global-mapping block synthesizes original ECG rule-modulated map refine detection.
 example shown Fig.
 find noisy peak (false positive local-based model) has been suppressed successfully, distorted R-peak has been enhanced global-based inference.
 Figure
 Loss curves training process local based model ReNN model.
 Horizontal axis represents number training epochs, vertical axis represents value loss function.
 B.
 Model training train ReNN model FCN block, use Adam optimizer weighted cross-entropy labels model output loss function.
 feed ECG measurement labels R-peaks apply optimizer training step.
 ECG measurements are fed epochs, measurement is fed times.
 learning rate is set beginning, decayed steps rate
 total number training step is million.
 C.
 Model Evaluation training process.
 evaluate use value loss function analyze convergence model detection performance, use following metrics: number R- peaks detected model (true-positives, TP), number R-peaks missed (false negatives, FN), number noise samples detected R-peaks (false-positives, FP).
 calculate F1-score is harmonic mean precision sensitivity detection model.
 (8) Given exact location R-peak be unstable neighboring sampling-points due set tolerance interval detection.
 is, detected R-peak is judged model outputs positive tolerance interval labeled R-peak.
 tolerance interval is set length sampling points,
 RESULTS A.
 Convergence Fig.
 shows convergence loss function early training phase.
 loss curves local-based model ReNN model decrease first training epochs, tend be stable gradually.
 final values loss function are 8.904e-4 7.648e-4 local-based model ReNN model, respectively.
 results reveal loss functions have converged training dataset, ReNN model achieve better fitting performance.
 B.
 Detection performance Table lists detection performances local-based inference global-based inference (ReNN) test C.
 Interpretability interpret inference, ask answer following questions: 2F1-score2TPTPFPFN Figure
 ReNN detection ECG measurement.
 top line is input time-series data ECG measurement, ECG signals are weak.
 other lines top bottom are outputs feature-mapping block (line-F), rule-modulating block (line-R) global-mapping block respectively.
 lines are aligned according time axis.
 solid circles anchored lines are time points labeled R-peaks.
 downward triangle shows false positive (FP) local-based inference (high value line-F), global-based inference (lower value line-O) reduces probability R-peak time point due little support heart rhythm (low value line-R).
 first R-peak front triangle is distorted due noise.
 is detected higher probability support heart rhythm (high value line-R).
 ➢ are local patterns, are rules them?
 ➢ are local-based inferences local patterns?
 ➢ are global-based inferences help rules?
 ➢ explain conflicts local-based global-based inferences?
 answer questions ECG example Fig.
 local patterns are local morphologies R-peaks, are detected feature-mapping block.
 rules R-peaks are designed according HR SDNN.
 disassemble local morphologies more details, P-Q-R-S-T peaks, train feature- mapping block details.
 Then, rules order time constraints peaks be applied.
 ✓ Line-F shows local-based inferences use local patterns.
 suspicious R-peaks are detected according local-based inference.
 ✓ Line-O shows global-based inferences combine local patterns rules.
 Line-O has less jitters Line-F, indicating combination local patterns rules have reduced uncertainty global-based inferences.
 check analyze conflicts local- based global-based inferences carefully.
 be possibilities.
 Firstly, indicate exceptions rules, such premature beat heart, have value diagnosis is new case.
 indications conflicts have been diagnosed, add label user suppress similar conflicts, take actions such medication.
 Secondly, conflicts noise cause conflicts, indicate current ECG measuring analyzing system is efficient discover real abnormalities such kind.
 usage be checked, system be improved.
 above question-answering process is similar inference procedure human, leading better interpretability inference ReNN.
 DISCUSSION paper has proposed ReNN overcome limitation current ANNs – lack knowledge, validated approach ECG R-peak detection problem.
 introducing knowledge rule-embedding approach, ReNN improve detection accuracy reduce model complexity.
 needs big dataset model training be mitigated model complexity is reduced.
 Besides, ReNN has improved interpretability neural-network-based technologies.
 global-based inference ReNN be interpreted interaction local patterns rules local patterns.
 model long-term dependencies is difficult problem machine learning area, uncertainty grows input data become higher-dimensional.
 current popular solution is train deep neural networks big dataset, such LSTM [31] ResNet [1].
 argue rule-embedding approach provide new solution make use long-term dependencies efficiently, have accumulated knowledge long-term dependencies.
 become new way human-computer interaction (HCI) local-pattern-rules are contents interaction.
 Computers tell local patterns have learnt observed, teach rules are local patterns.
 seems are teaching kid student, computers seem more intelligent.
 be question rules be embedded ANN be placed behind.
 Rules are placed most knowledge is semantic features extracted raw data.
 hidden layers ANN play role extract features.
 Rules are placed want model dependencies local patterns rules make inferences synthetically.
 Rules play role extract global features, i.e. rule-modulated map, add rules worry conflicts rules combinatorial explosion multiple rules.
 Besides, Rules ReNN are independent optimization procedure, human experts domain knowledge design rules constraints differentiability.
 makes combination artificial neural network existing knowledge much easier.
 Furthermore, rules be organized accumulated cross-validation latest probabilistic graphical model based reasoning systems leading rule base rules are marked applicable tasks domains.
 rule computers know apply rules.
 be helpful transfer learning few samples be used unsupervised learning labels be used.
 local- pattern-rule HCI rule base lead new thinking artificial intelligence.
 author thanks Prof.
 Jue Wang Institute Automation, Chinese Academic Sciences introducing philosophy “structure + average” artificial intelligent, have inspired idea research.
 author thanks Dr. Jidong Liu Shandong Provincial Hospital help build ECG dataset, thanks Yao Wang Yudong Zhu LOHAS Tech supports, thanks Zongbo Zhang, Shuyu Han, Dongyang Mei, Ran Yan, Yuzhi Mu Shuai Ma weekly discussion machine learning problems.
 REFERENCES [1] K.
 He, X.
 Zhang, S.
 Ren, J.
 Sun, "Deep residual learning image recognition," Proceedings IEEE conference computer vision pattern recognition, pp.
 770-778.
 [2] A.
 Krizhevsky, I.
 Sutskever, G.
 E.
 Hinton, "Imagenet classification deep convolutional neural networks," Advances neural information processing systems, pp.
 1097-1105.
 [3] D.
 Silver, J.
 Schrittwieser, K.
 Simonyan, I.
 Antonoglou, A.
 Huang, A.
 Guez, "Mastering game go human knowledge," Nature, vol.
 p.

 [4] R.
 Shwartz-Ziv N.
 Tishby, "Opening Black Box Deep Neural Networks Information," arXiv preprint arXiv:1703.00810,
 [5] M.
 D.
 Zeiler R.
 Fergus, "Visualizing Understanding Convolutional Networks," ECCV, European Conference, Proceedings, Part I, Zurich, Switzerland, pp.
 818-833.
 [6] J.
 Yosinski, J.
 Clune, T.
 Fuchs, H.
 Lipson, "Understanding neural networks deep visualization," ICML Workshop Deep Learning,
 [7] C.
 Szegedy, W.
 Zaremba, I.
 Sutskever, J.
 Bruna, D.
 Erhan, I.
 Goodfellow, al., "Intriguing properties neural networks," arXiv preprint arXiv:1312.6199,
 [8] A.
 Nguyen, J.
 Yosinski, J.
 Clune, "Deep neural networks are High confidence predictions unrecognizable images," Proceedings IEEE Conference Computer Vision Pattern Recognition, pp.
 427-436.
 [9] B.
 Zhou, A.
 Khosla, A.
 Lapedriza, A.
 Oliva, A.
 Torralba, "Object detectors emerge deep scene cnns," ICLR (arXiv preprint arXiv:1412.6856),
 [10] D.
 Bau, B.
 Zhou, A.
 Khosla, A.
 Oliva, A.
 Torralba, "Network Dissection: Quantifying Interpretability Deep Visual Representations," arXiv preprint arXiv:1704.05796,
 [11] Q.
 Zhang, R.
 Cao, F.
 Shi, Y.
 N.
 Wu, S.-C.
 Zhu, "Interpreting CNN knowledge Explanatory Graph," AAAI
 [12] R.
 Davis, H.
 Shrobe, P.
 Szolovits, is knowledge representation?," AI magazine, vol.
 p.

 [13] J.
 F.
 Sowa, Principles semantic networks: Explorations representation knowledge: Morgan Kaufmann,
 [14] K.
 Polat S.
 Güneş, "An expert system approach based principal component analysis adaptive neuro-fuzzy inference system diagnosis diabetes disease," Digital Signal Processing, vol.
 pp.
 702-
 [15] S.
 S.
 A.
 Naser A.
 O.
 Mahdi, "A proposed Expert System Foot Diseases Diagnosis," American Journal Innovative Research Applied Sciences, vol.
 pp.
 155-168,
 [16] E.
 Avci, "An expert system based Wavelet Neural Network-Adaptive Norm Entropy scale invariant texture classification," Expert Syst.
 Appl., vol.
 pp.

 [17] G.
 Winter, "xia2: expert system macromolecular crystallography data reduction," Journal Applied Crystallography, vol.
 pp.
 186-190,
 [18] M.
 B.
 Jain, M.
 B.
 Srinivas, A.
 Jain, "A novel Web based Expert System Architecture on-line off-line fault diagnosis control (FDC) transformers," TENCON IEEE Region Conference, pp.
 1-5.
 [19] S.
 I.
 Gallant, "Connectionist expert systems," Commun.
 ACM, vol.
 pp.
 152-169,
 [20] J.
 W.
 Shavlik, "Combining Symbolic Neural Learning," Machine Learning, vol.
 pp.
 321-331, March
 [21] G.
 G.
 Towell J.
 W.
 Shavlik, "Knowledge-based artificial neural networks," Artificial Intelligence, vol.
 pp.
 119-165,
 [22] Y.
 LeCun, Y.
 Bengio, G.
 Hinton, "Deep learning," Nature, vol.
 pp.
 436-444,
 [23] J.
 F.
 Thayer, S.
 S.
 Yamamoto, J.
 F.
 Brosschot, "The relationship autonomic imbalance, heart rate variability cardiovascular disease risk factors," International Journal Cardiology, vol.
 p.

 [24] C.
 Ahlstrom, A.
 Johansson, F.
 Uhlin, T.
 Länne, P.
 Ask, "Noninvasive investigation blood pressure changes using pulse wave transit time: novel approach monitoring hemodialysis patients," Journal Artificial Organs Official Journal Japanese Society Artificial Organs, vol.
 p.

 [25] C.
 Lin, Y.
 Zhou, H.
 Wang, Y.
 Wang, "Pulse waveform indicator baseline offset pulse transit time based blood pressure estimation," IEEE Healthcare Innovations Point Care Technologies (HI- POCT), pp.
 26-31.
 [26] J.
 Long, E.
 Shelhamer, T.
 Darrell, "Fully convolutional networks semantic segmentation," Proceedings IEEE Conference Computer Vision Pattern Recognition, pp.
 3431-3440.
 [27] N.
 J.
 Mehta I.
 A.
 Khan, "Cardiology's greatest discoveries 20th century," Texas Heart Institute Journal, vol.
 p.

 [28] B.
 Yu, L.
 Xu, Y.
 Li, "Bluetooth Low Energy (BLE) based mobile electrocardiogram monitoring system," Information Automation (ICIA), International Conference on, pp.
 763-767.
 [29] P.
 Rajpurkar, A.
 Y.
 Hannun, M.
 Haghpanahi, C.
 Bourn, A.
 Y.
 Ng, "Cardiologist-Level Arrhythmia Detection Convolutional Neural Networks," arXiv preprint arXiv:1707.01836,
 [30] J.
 Pan W.
 J.
 Tompkins, "A Real-Time QRS Detection Algorithm," IEEE Transactions Biomedical Engineering, vol.
 BME-32, pp.
 230-236,
 [31] S.
 Hochreiter J.
 Schmidhuber, "Long Short-Term Memory," Neural Computation, vol.
 pp.
 1735-1780,
 [32] L.
 Bottou, "From machine learning machine reasoning," Machine learning, vol.
 pp.
 133-149,
 [33] D.
 Koller N.
 Friedman, Probabilistic graphical models: principles techniques: MIT press,
 Various models signals images have been studied recent years including dictionary models, tensor manifold models.
 Dictionaries represent signals are used applications such compression, denoising, medical image reconstruction.
 Dictionaries learned training data sets outperform analytical models are adapted signals (or signal classes).
 goal dictionary learning is ﬁnd matrix D input matrix P, representing data set, be written P ≈ DZ, Z denoting (unknown) sparse representation matrix.
 learning synthesis dictionary sparsifying trans- form models has been studied several works [1]–[11].
 convergence speciﬁc learning algorithms has been studied recent works [6], [12]–[16].
 learning problems are non-convex works prove conver- gence critical points problems [6], [17].
 Others (e.g., [15], [18]) prove recovery generative models speciﬁc (often expensive) algorithms, many restrictive assumptions.
 recent work considers structured dictionary learning objective showing high probability are spurious local minimizers, provides speciﬁc convergent algorithm [19], [20].
 work, analyze convergence properties structured (unitary) dictionary transform learning involves cheap updates works applications such image denoising magnetic resonance image reconstruction [10], [16], [21].
 goal is simulta- n × sparsifying transformation matrix W n× N sparse coefﬁcients (representation) matrix Z training data represented columns given n× N matrix P, solving following constrained optimization problem: F s.t. WT W = Id,(cid:13)(cid:13)Z(.,j) (cid:13)(cid:13)0 ≤ s∀j.
 (cid:107)WP − Z(cid:107)2 arg W,Z (1) columns Z(.,j) Z have most s non-zeros (corre- sponding (cid:96)0 “norm”), s is given parameter.
 Alternatives Problem (1) replace column-wise sparsity constraint constraint sparsity entire matrix Z (i.e., aggregate sparsity), use sparsity penalty (e.g., (cid:96)p penalties ≤ p
 Problem (1) corresponds learning (synthesis) dictionary WT representing data P WT Z.
 Optimizing Problem (1) alternating updating W (operator update step) Z (sparse coding step) generate following updates.
 tth Z update is given (.,j) = Hs(Wt−1P(.,j)) ∀ j, thresholding operator Zt Hs(·) zeros s largest magnitude elements vector (leaving s entries unchanged).
 subsequent W update is based full singular value decomposition (SVD) ZtPT (cid:44) VΣUT Wt = VUT
 method is shown Algorithm
 Recent works have shown convergence Algorithm variants critical points equivalent unconstrained problems [16], [22], [23].
 Here, further prove local linear convergence method underlying generative (data) model mild assumptions depend properties underlying/generating sparse coefﬁcients.
 experiments show method is robust insensitive initial- ization practice.
 II.
 CONVERGENCE ANALYSIS main contribution work is local convergence analysis Algorithm
 particular, show mild iterates Algorithm converge underlying (generating) data model.
 Alternating Optimization (1) Input: Training data matrix P, maximum iteration count L, sparsity s Output: WL, ZL Initialize: W0 t t ≤ L do Zt (.,j) = Hs PZtT =UtΣtVtT Wt = VtUtT t = t + (cid:0)Wt−1P(.,j) (cid:1) ∀ j end A.
 Notation remainder work, adopt following nota- tion.
 unitary transformation matrix, sparse representation matrix, training data matrix are denoted W ∈ Rn×n, Z ∈ Rn×N P ∈ Rn×N respectively.
 matrix X, denote jth column, ith row, entry (i, j) X(·,j), X(i,·), X(i,j) respectively.
 tth iterate approximation algorithm is denoted using (·)t, lower-case t, (·)T denotes transpose.
 function S(y) returns support (i.e., set indexes) vector y ∈ Rn, locations nonzero entries y.
 Matrix Dk denotes n × n diagonal matrix ones zero location k).
 Additionally, ˜Dk denotes N × N diagonal matrix has ones entries (i, i) i ∈ S(Z∗ (k,·)) zeros elsewhere, matrix Z∗ is deﬁned Section II-B (see assumption (A1)).
 Frobenious norm, denoted (cid:107)X(cid:107)2 F is sum squared elements X, (cid:107)X(cid:107)2 denotes spectral norm.
 Lastly, Id denotes sized identity matrix.
 B.
 Assumptions presenting main results, brieﬂy discuss assumptions explain implications: (A1) Generative model: exists Z∗ unitary W∗ such W∗P = Z∗, (cid:107)P(cid:107)2 = (normalized).
 (A2) Sparsity: columns Z∗ are s−sparse, i.e., (cid:107)Z∗ (A3) Spectral property: underlying Z∗ satisﬁes bound κ4(cid:0)Z∗(cid:1) max1≤k≤n (cid:107)DkZ∗Z∗T Z∗ ˜Dk(cid:107)2 < κ(·) (·,j)(cid:107)0 ≤ s ∀j.
 denotes condition number (ratio largest smallest singular value).
 (A4) Initialization: (cid:107)W0 − W∗(cid:107)F ≤  appropriate small  >
 generative model assumption states exists underlying transform representation matrix data set P.
 like investigate Algo- rithm ﬁnd such underlying models (minimizers (1)).
 (cid:18) Z∗ do specify best (largest permissible)  explicitly,  < β(·) denoting smallest nonzero magnitude minj β vector, arise proof steps.
 actual permissible  is dictated (convergence of) Taylor series expansions discussed proof.
 (·,j) (·,j) (cid:107)Z∗ (cid:107)2 Assumption (A2) states columns Z∗ have most s nonzeros.
 assume coefﬁcients are “structured” assumption (A3), satisfying spectral property, be used establish theorems.
 discuss conjecture states property holds speciﬁc probabilistic model.
 Assumption (A4) states initial sparsifying transform is solution W∗.
 assumption simpliﬁes proof has been made other works such [15], [18], address issue speciﬁc (good) initialization main algorithm.
 work, show effect general initializations Section III.
 C.
 Main Results Proofs state convergence results Theorems II.1 II.2. Theorem II.1 establishes Algorithm converges underlying generative model assumptions discussed Section II-B.
 makes additional assumption Z∗ Z∗Z∗T = Id, simpliﬁes assumption (A3).
 Theo- rem II.2 presents general result based Assumption (A3).
 include (simpler) proof Theorem II.1, full details general Theorem II.2’s proof be found [24].
 Following Theorem II.2, Conjecture states Assumption (A3) holds used probabilistic assumption sparse representation matrix Z∗.
 Theorem II.1. Assumptions (A1)−(A4) assuming Z∗Z∗T = Id, Frobenius error iterates generated Algorithm underlying generative model Assumption (A1) is bounded follows: (cid:107)Zt − Z∗(cid:107)F ≤ qt−1, (cid:107)Wt − W∗(cid:107)F ≤ qt, (2) q (cid:44) max1≤k≤n (cid:107)DkZ∗ ˜Dk(cid:107)2  is ﬁxed based initialization.
 Z∗Z∗T = Id, Assumption (A3) follows q < above, Theorem establishes iterates converge linear rate underlying generative model.
 following, prove Theorem II.1 using induction approximation error iterates respect Z∗ W∗.
 Let series {Et} {∆t} be deﬁned Et := Wt − W∗, ∆t := Zt − Z∗.
 (3) (4) Assumption (A4), (cid:107)E0(cid:107)F ≤ .
 ﬁrst provide proof base case t
 proof involves main parts.
 show error Z1 Z∗ is bounded (in norm) .
 show (cid:107)W1 − W∗(cid:107)F ≤ q, q is iteration-independent.
 ﬁrst part, use Assumptions (A1), (A2), (A4).
 Assumption (A4) ensures superset support Z∗ Z1 is recovered ﬁrst iteration.
 particular, column sparse coefﬁcients matrix Z1 Algorithm satisﬁes = Hs(W∗P(·,j) + E0P(·,j)) (·,j) + E0P(·,j)) j E0P(·,j), (·,j) = Hs(W0P(·,j)) = Hs(Z∗ = Z∗ (·,j) + Γ1 (Eq.3) (5) (A1) (A4) j is diagonal matrix one (i, i)th entry Γ1 i ∈ S(Z1 (·,j)) zero E0 is deﬁned (3).
 last equality (5) follows fact support (·,j) small .
 particular, Z1 (·,j) includes Z∗ (cid:13)(cid:13)P(·,j) (cid:13)(cid:13)(cid:13)2 (cid:13)(cid:13)(cid:13)Z∗ (cid:13)(cid:13)2 = (cid:13)(cid:13)∞ ≤(cid:13)(cid:13)E0P(·,j) (cid:13)(cid:13)E0P(·,j) have (·,j) (cid:13)(cid:13)2 ≤(cid:13)(cid:13)E0(cid:13)(cid:13)F (cid:13)(cid:13)(cid:13)2 (cid:13)(cid:13)(cid:13)Z∗ (cid:16) Z∗ (·,j) (cid:17) Therefore, (cid:107)E0(cid:107)F ≤  < β(·) denoting smallest nonzero magnitude vector, support Z1 (·,j) (i.e., entries perturbation term E0P(·,j) are large change support).
 following results hold: (·,j) includes Z∗ minj β (·,j) (·,j)(cid:107)2 (cid:107)Z∗ (cid:107)Z1 − Z∗(cid:107)2 (Eq.5) = (cid:107)[Γ1 (i)≤ (cid:107)E0P(cid:107)2 (ii)≤ (cid:107)E0(cid:107)2 N E0P(·,N )](cid:107)2 = (cid:107)E0(cid:107)2 F
 ..., Γ1 F(cid:107)P(cid:107)2 j; step (ii) holds (A1) Step (i) follows deﬁnition Γ1 Frobenius norm product matrices; (cid:107)P(cid:107)2 last equality holds.
 conclude (cid:107)Z1 − Z∗(cid:107)F ≤ (cid:107)E0(cid:107)F (A4)≤ .
 (6) Next, analyze quality updated transform W1.
 bound (cid:107)W1 − W∗(cid:107)F q, rely Taylor Series expansions matrix inverse positive-deﬁnite square root functions.
 Denote full SVD Z∗Z1T U1 T
 zΣ1 Then, Assumption (A1) Algorithm have, zV1 W∗T Z∗Z1T (A1) W1 = V1U1T Then, W1 is expressed terms SVD Z∗Z1T = U1Σ1V1T = PZ1T W1 = V1 zU1 W∗.
 Using error W1 W∗ satisﬁes (cid:107)W1 − W∗(cid:107)F = (cid:107)V1 = (cid:107)(V1 zU1 zU1 W∗ − W∗(cid:107)F zU1 T − Id(cid:107)F (7) (8) matrix V1 T be rewritten follows: T − Id)W∗(cid:107)F = (cid:107)V1 zU1 (cid:0)Σ1 (cid:123)(cid:122) (a) (cid:1)−1 (cid:125) )−1 = V1 = (Z∗Z1T (cid:124) (cid:124) (cid:123)(cid:122) (b) U1 U1 (Z∗Z1T zΣ1 zU1 Z1Z∗T (cid:125) (9) V1 zU1 is easy show Z∗Z∗T = Id, Z∗Z1T is invertible  < (sufﬁcient condition).
 Using (4) assumption Z∗Z∗T = Id, Taylor Series expansions matrix inverse positive-deﬁnite square root (9), be written (a) = (Z∗Z1T )−1 = (Id + Z∗∆1T )−1 = Id − Z∗∆1T + O((∆1)2) (b) = (Z∗Z1T = Id + Z1Z∗T (Z∗∆1T + ∆1Z∗T + O((∆1)2).
 rewrite (9) V1 zU1 = (a)(b) = Id + (∆1Z∗T − Z∗∆1T + O((∆1)2), terms, is bounded norm C(cid:13)(cid:13)∆1(cid:13)(cid:13)2 constant O((∆1)2) denotes corresponding higher order series C (independent iterates).
 ﬁrst transform iterate W1 W∗ is bounded Substituting above expressions (8), error (cid:107)W1 − W∗(cid:107)F T − Id(cid:107)F zU1 (Eq.8) = (cid:107)V1 ≈ (cid:107)∆1Z∗T − Z∗∆1T(cid:107)F
 approximation error (10) is bounded norm C2, is negligible small .
 bound (dominant) term − Z∗∆1T(cid:107)F
 matrix ∆1Z∗T − Z∗∆1T has zero diagonal, have following inequalities: (cid:107)W1 − W∗(cid:107)F ≈ (cid:107)∆1Z∗T − Z∗∆1T(cid:107)F (cid:118)(cid:117)(cid:117)(cid:116) (cid:118)(cid:117)(cid:117)(cid:116) n(cid:88) (cid:107)DkZ∗ ˜Dk∆1 T(cid:107)2 ≤ (cid:107)DkZ∗ ˜Dk(cid:107)2 (k,·)(cid:107)2 k=1 ≤ max (cid:107)DkZ∗ ˜Dk(cid:107)2 k=1 (cid:107)∆1 (k,·)(cid:107)2 = q(cid:107)Z1 − Z∗(cid:107)F (6)≤ q, (k,·) (cid:118)(cid:117)(cid:117)(cid:116) n(cid:88) k=1 q (cid:44) maxk (cid:107)DkZ∗ ˜Dk(cid:107)2.
 Thus, have shown results t = case.
 complete proof Theorem II.1 observing subsequent iteration t = τ + same steps be repeated induction hypothesis (IH) show (cid:107)Zτ +1 − Z∗(cid:107)F = (cid:107)∆τ +1(cid:107)F ≤ (cid:107)Eτ(cid:107)F = (cid:107)Wτ − W∗(cid:107)F (IH)≤ qτ  (cid:107)Wτ +1 − W∗(cid:107)F ≤ q(cid:107)Zτ +1 − Z∗(cid:107)F ≤ q(qτ ).
 next result generalizes Theorem II.1 removing assumption Z∗Z∗T = Id. Theorem II.2. Assumptions (A1) − (A4), iterates Algorithm converge underlying generative (cid:4) Fig.
 performance Algorithm recovering W∗ s = s
 model Assumption (A1), i.e., Frobenius error iterates generative model satisﬁes (cid:107)Zt − Z∗(cid:107)F ≤ qt−1, (cid:107)Wt − W∗(cid:107)F ≤ qt, q (cid:44) κ4(cid:0)Z∗(cid:1) max1≤k≤n (cid:107)DkZ∗Z∗T Z∗ ˜Dk(cid:107)2 < (11)  is ﬁxed based initialization.
 Note dropping unit spectral norm (normalization) condition P Assumption (A1) does affect (cid:107)Wt− W∗(cid:107)F bound Theorem II.2 creates scaling (cid:107)Zt − Z∗(cid:107)F bound,  gets replaced (cid:107)P(cid:107)2.
 Conjecture
 Suppose locations s nonzeros column Z∗ is chosen random, non- zero entries are i.i.d. Z∗ sN ).
 Then, ﬁxed, small large enough N.
 q (cid:44) κ4(cid:0)Z∗(cid:1) max1≤k≤n (cid:107)DkZ∗Z∗T Z∗ ˜Dk(cid:107)2 < (i,j) ∼ N (0, s√ Conjecture states assumed probabilistic model Z∗, N is large is sufﬁcient training data (or columns P), Algorithm is assured have rapid local linear iterate convergence underlying generative model.
 conjecture be veriﬁed simulations numerical results supporting be found [24].
 experiments presented paper focus illustrating local convergence Algorithm robustness initialization.
 III.
 EXPERIMENTS section, show numerical experiments support analytical conclusions.
 provide results illustrating robustness algorithm initializations.
 experiments, generated training data using generated W∗ Z∗, n N = s = {5,
 transform W∗ is generated case applying Matlab’s orth() function standard Gaussian matrix.
 representation matrix Z∗ is generated s described Conjecture i.e., support Fig.
 performance Algorithm various initial- izations s (top) s = (bottom).
 (cid:17) (·,j) (·,j)(cid:107)2 (cid:16) Z∗ ﬁrst experiment, column Z∗ is chosen random nonzero entries are drawn i.i.d. Gaussian distribution mean zero variance n/sN.
 initial W0 Algorithm is chosen satisfy (cid:107)W0 − W∗(cid:107)F ≤   = (see (5)).
 Fig.
 shows behavior minj β Frobenious error Wt W∗ algo- rithm.
 observed (linear) convergence iterates generative operator W∗ is accordance Theorem II.2 Conjecture
 Next, study performance Algorithm differ- ent initializations n N = s = {5,
 Fig.
 shows objective function Problem (1) algorithm iterations.
 training data satisfy Assump- tions (A1) (A2), minimum objective value (1) is
 different types initializations are considered.
 ﬁrst, labeled ‘eps’, denotes initialization Fig.

 other initializations are  = minj β follows: entries W0 drawn i.i.d. standard Gaussian distribution (labeled n × n identity matrix W0 labeled discrete cosine transform (DCT) initialization labeled ‘dct’; entries W0 drawn i.i.d. uniform distri- bution ranging (labeled ‘unif’); W0 = labeled ‘zero’.
 more general initializations ‘eps’), see behavior Algorithm is split (cid:16) Z∗ (cid:107)Z∗ (cid:17) (cid:107)Z∗ (·,j) (·,j)(cid:107)2 [9] S.
 Ravishankar Y.
 Bresler, “Learning sparsifying transforms,” IEEE Trans.
 Signal Process., vol.
 no.
 pp.

 [10] S.
 Ravishankar Y.
 Bresler, “Closed-form solutions sparsifying transform learning,” IEEE International Conference Acoustics, Speech Signal Processing (ICASSP), pp.

 [11] B.
 Wen, S.
 Ravishankar, Y.
 Bresler, “Structured overcomplete sparsi- fying transform learning convergence guarantees applications,” International Journal Computer Vision, vol.
 no.
 2-3, pp.

 [12] D.
 A.
 Spielman, H.
 Wang, J.
 Wright, “Exact recovery sparsely- used dictionaries,” Proceedings 25th Annual Conference Learning Theory, pp.

 [13] S.
 Arora, R.
 Ge, A.
 Moitra, “New algorithms learning incoherent overcomplete dictionaries,” Proceedings 27th Conference Learning Theory, pp.

 [14] Y.
 Xu W.
 Yin, “A fast patch-dictionary method whole image recovery,” Inverse Problems Imaging, vol.
 no.
 pp.

 [15] A.
 Agarwal, A.
 Anandkumar, P.
 Jain, P.
 Netrapalli, R.
 Tandon, “Learning used overcomplete dictionaries,” Journal Machine Learning Research, vol.
 pp.

 [16] S.
 Ravishankar Y.
 Bresler, “(cid:96)0 sparsifying transform learning IEEE Trans.
 efﬁcient optimal updates convergence guarantees,” Signal Process., vol.
 no.
 pp.
 May
 [17] C.
 Bao, H.
 Ji, Y.
 Quan, Z.
 Shen, “Dictionary learning sparse coding: Algorithms convergence analysis,” IEEE Transactions Pattern Analysis Machine Intelligence, vol.
 no.
 pp.
 July
 [18] A.
 Agarwal, A.
 Anandkumar, P.
 Jain, P.
 Netrapalli, “Learning used overcomplete dictionaries alternating minimization,” SIAM Journal Optimization, vol.
 no.
 pp.

 [19] J.
 Sun, Q.
 Qu, J.
 Wright, “Complete dictionary recovery sphere I: Overview geometric picture,” IEEE Transactions Information Theory, vol.
 no.
 pp.

 [20] J.
 Sun, Q.
 Qu, J.
 Wright, “Complete dictionary recovery IEEE sphere II: Recovery riemannian trust-region method,” Transactions Information Theory, vol.
 no.
 pp.

 [21] S.
 Ravishankar Y.
 Bresler, “Data-driven learning union IEEE sparsifying transforms model blind compressed sensing,” Transactions Computational Imaging, vol.
 no.
 pp.

 [22] S.
 Ravishankar Y.
 Bresler, “Efﬁcient blind compressed sensing using sparsifying transforms convergence guarantees application magnetic resonance imaging,” SIAM Journal Imaging Sciences, vol.
 no.
 pp.

 [23] C.
 Bao, H.
 Ji, Z.
 Shen, “Convergence analysis iterative data- driven tight frame construction scheme,” Applied Computational Harmonic Analysis, vol.
 no.
 pp.

 [24] A.
 Ma S.
 Ravishankar D.
 Needell, “Analysis structured dictionary learning,” Information Inference, preparation.
 phases.
 ﬁrst phase, iterates decrease objective.
 iterates are solution, second phase occurs phase, Algorithm enjoys rapid convergence (towards
 Note objective’s convergence rate second phase is comparable ‘eps’ case.
 behavior Algorithm is similar s = s = latter case taking more iterations enter second phase convergence.
 makes sense are more variables learn larger s.
 IV.
 CONCLUSION work presented analysis fast alternating min- imization algorithm unitary sparsifying operator learning.
 proved local linear convergence algorithm underlying generative model mild assumptions.
 Numer- ical experiments illustrated local convergence behavior, demonstrated algorithm is robust initialization practice.
 full version work, including proof Theorem II.2 numerical results support be found [24].
 theoretical analysis algorithm’s robustness observed Fig.
 is left future work.
 ACKNOWLEDGMENTS Saiprasad Ravishankar was supported part follow- ing grants: NSF grant CCF-1320953, ONR grant N00014-15- 1-2141, DARPA Young Faculty Award D14AP00086, ARO MURI grants W911NF-11-1-0391 2015-05174-05, NIH grants R01 EB023618 U01 EB018753, UM-SJTU seed grant.
 Anna Ma Deanna Needell were supported NSF DMS #1440140 were residence Mathematical Science Research Institute Berkeley, California, Fall semester), NSF CAREER DMS #1348721, NSF BIGDATA DMS #1740325.
 REFERENCES [1] M.
 Aharon, M.
 Elad, A.
 Bruckstein, algorithm designing overcomplete dictionaries sparse representation,” IEEE Transactions Signal Processing, vol.
 no.
 pp.

 [2] M.
 Yaghoobi, T.
 Blumensath, M.
 Davies, “Dictionary learning sparse approximations majorization method,” IEEE Transac- tions Signal Processing, vol.
 no.
 pp.

 [3] J.
 Mairal, F.
 Bach, J.
 Ponce, G.
 Sapiro, “Online learning matrix factorization sparse coding,” J.
 Mach.
 Learn.
 Res., vol.
 pp.

 [4] D.
 Barchiesi M.
 D.
 Plumbley, “Learning incoherent dictionaries sparse approximation using iterative projections rotations,” IEEE Transactions Signal Processing, vol.
 no.
 pp.

 [5] L.
 N.
 Smith M.
 Elad, “Improving dictionary learning: Multiple IEEE Signal Processing dictionary updates coefﬁcient reuse,” Letters, vol.
 no.
 pp.
 Jan
 [6] C.
 Bao, H.
 Ji, Y.
 Quan, Z.
 Shen, “L0 norm based dictionary learning proximal methods global convergence,” IEEE Conference Computer Vision Pattern Recognition (CVPR), pp.

 [7] S.
 Ravishankar, R.
 R.
 Nadakuditi, J.
 A.
 Fessler, “Efﬁcient sum outer products dictionary learning (SOUP-DIL) application inverse problems,” IEEE Transactions Computational Imaging, vol.
 no.
 pp.
 Dec
 [8] S.
 Ravishankar, B.
 E.
 Moore, R.
 R.
 Nadakuditi, J.
 A.
 Fessler, “Efﬁcient learning dictionaries low-rank atoms,” IEEE Global Conference Signal Information Processing (GlobalSIP), Dec pp.

 Data acquisition analysis is ubiquitous, data contains errors be incomplete.
 example, data is obtained user surveys, people choose answer subset questions.
 Ideally, want eliminate surveys are complete, contain useful information.
 many tasks, such certain regression classiﬁcation require complete completed data [SG02].
 Alternatively, consider problem collaborative ﬁltering, made popular classic Netﬂix problem [BL07], [BK07], [KBV09], one aims predict user ratings unseen movies based available user-movie ratings.
 setting, accurate data completion is goal, opposed data pre-processing task.
 Viewing users rows matrix movies columns, like recover unknown entries resulting matrix subset known entries.
 is goal many types other applications, ranging systems identiﬁcation [LV09] sensor networks [BLWY06], [Sch86], [Sin08].
 task is known matrix completion [Rec11].
 underlying matrix is low-rank observed entries are sampled random, achieve exact recovery high probability mild additional assumptions using nuclear norm minimization (NNM) [CT10], [RFP10], [CR09], [Gro11], [CP10].
 many applications, however, expect structural dif- ferences observed unobserved entries, violate classical assumptions.
 structural differences, mean entry is observed unobserved be random occur uniform selection mechanism.
 Consider Netﬂix problem.
 Popular, well-received movies are likely have been rated many users, A.
 Nuclear Norm Matrix Completion Let M ∈ Rn1×n2 be unknown matrix like recover Ω be set indices observed entries.
 Let PΩ Rn1×n2 → Rn1×n2, (cid:40) [PΩ]ij = Mij (i, j) ∈ Ω (i, j) (cid:54)∈ Ω [CT10].
 many applications, is reasonable assume matrix M is low-rank.
 example, expect few factors contribute user’s movie preferences compared number users number movies con- sidered.
 Similarly, health data, few underlying features contribute many observable signs symptoms.
 minimization, (cid:99)M = argmin rank(A) s.t. PΩ(A) = PΩ(M recovers lowest rank matrix matches observed entries exactly.
 minimization problem is NP-hard, uses convex relaxation ||A||∗ s.t. PΩ(A) = PΩ(M ), (cid:99)M = singular values, i.e. ||X||∗ := (cid:80) || · ||∗ is nuclear norm, given sum i σi(X) [CT10], [RFP10], (1) [CP10], [CR09].
 course, applications tend have higher values missing entries, case methods be scaled accordingly.
 B.
 Matrix Completion Structured Observations propose adding regularization term unobserved entries promote adherence structural assumption expect entries be
 solve (cid:102)M = argmin L1 norm ||M||1 =(cid:80) ||A||∗ + α||PΩC (A)|| s.t. PΩ(A) = PΩ(M ), (2) α > || · || is appropriate matrix norm.
 example, expect most unobserved entries be few be large magnitude, entrywise ij |Mij| is reasonable choice.
 C.
 Matrix Completion Noisy Observations reality, expect data is corrupted amount noise.
 assume matrix M, like recover, satisﬁes PΩY = PΩM + PΩZ, PΩY are observed values, M is low-rank PΩZ represents noise observed data.
 [CP10], Cand´es Plan suggest using following minimization recover unknown matrix: ||A||∗ s.
 t.
 ||PΩ(M − A)||F < δ.
 (3) (cid:99)M = argmin (cid:113)(cid:80) alent (cid:99)M = argmin Recall, ||X||F = ij X ij.
 formulation is equiv- ||PΩ(M − A)||F + ρ||A||∗ (4) ρ = ρ(δ).
 latter minimization problem is easier solve practice [CP10].
 order account assumption unobserved entries are likely be zero, propose adding regularization term unobserved entries aim solve(cid:102)M = argmin ||PΩ(M − A)||F + ρ||A||∗ + α||PΩC (A)||.
 (5) II.
 NUMERICAL RESULTS A.
 Recovery Noise ﬁrst investigate performance (2) ob- served entries are exact, i.e. is noise errors observed values.
 Figure consider low-rank matrices M ∈ R30×30.
 generate M rank r, take M = MLMR, ML ∈ R30×r MR ∈ Rr×30 are sparse matrices (with density respectively) nonzero entries are distributed random zero one.
 subsample zero nonzero entries data matrix various rates generate matrix missing entries.
 compare performance (2) using L1 regularization unobserved entries standard NNM report error ratio ||(cid:102)M −M||F /||(cid:99)M −M||F various sampling rates, (cid:102)M (cid:99)M are solutions (2) (1), respectively.
 regularization parameter α used is selected set {10−1, (discussed below).
 Values Figure indicate minimization L1 regularization outperforms standard NNM.
 Results are averaged ten trials.
 expected, ﬁnd sampling nonzero entries is high, modiﬁed method (2) is likely outperform standard NNM.
 choose parameter α, regularization term, be optimal α ∈ {10−1, report values used Figure
 large recovered matrix approach unobserved entries are predicted be zero, α becomes zero, recovery (2) approaches standard NNM.
 sampling rate zero entries is low sampling nonzero entries is high, addition (2) outperforming NNM, see larger value α is optimal, supporting claim regularization improves performance.
 Higher α values are optimal nonzero sampling rate is zero.
 are few nonzero entries sampled low-rank matrix recovered is likely be zero matrix.
 setting, expect standard NNM unobserved entries are likely be recovered zeros larger coefﬁcient regularization term harm performance.
 α is close zero, difference performance is minimal, regularization have little effect case.
 (cid:102)M (cid:99)M given (2) (1), respectively, L1 plot ||(cid:102)M − M||F /||(cid:99)M − M||F
 consider matrices various Fig.

 regularization recovered values unobserved entries, ranks average results ten trials, α optimal α ∈ {10−1,
 B.
 Recovery Noisy Observed Entries generate matrices previous section consider minimization given (4).
 Suppose entries noise matrix Z are i.i.d. N (0, σ2).
 set parameter ρ, done [CP10], be ρ = n1 + n2) (cid:115) |Ω| σ.
 n1n2 C.
 Matrix recovery health data Next, consider real survey data patients responding particular questions provided LymeDis- ease.org.
 Data used was obtained LymeDisease.org patient registry, MyLymeData, Phase June
 Ques- tion responses are integer values answering questions was required, is subset data survey is complete calculate reconstruction errors).
 patients have Lyme disease survey questions ask topics such current past symptoms, treatments outcomes.
 example, “I say general health is: 0-Poor, 1-Fair, 2-Good, 3-Very good, 4-Excellent.” Although, part data considered is complete, expect general, patients are likely record responses noticeable symptoms, missing response medical survey indicate lack symptoms.
 Thus, setting, L1 regularization unobserved entries is natural choice.
 Due computational constraints, ten trials executed, randomly sample patient surveys generate matrix.
 previous experiments, subsample zero nonzero entries data matrix various rates generate matrix missing entries.
 complete subsampled matrix NNM (1) (2) using L1 regularization unobserved entries report ||(cid:102)M −M||F /||(cid:99)M −M||F averaged ten trials Figure
 parameter α, regularization term, is chosen be optimal α ∈ {10−1, report values used Figure
 results Lyme disease data match found synthetic experiments done noise.
 Regularizing L1-norm unobserved entries improves performance sampling non-zero entries is high sampling zero entries is low.
 Fig.

 Average optimal α value α ∈ {10−1, minimization given (2) L1 regularization recovered values unobserved entries.
 matrices considered are same Figure
 consider low-rank matrices M ∈ R30×30 generated previous section noise matrix Z i.i.d. entries sampled N
 set ρ various sampling rates zero nonzero entries M (cid:113)|Ω| ·0.1. report ||(cid:102)M −M||F /||(cid:99)M −M||F Figure
 Here,(cid:99)M and(cid:102)M are given (4) (5) respectively.
 see improved performance regularization sampling rate zero entries is low sampling nonzero entries is high.
 (cid:102)M (cid:99)M given (2) (1), respectively, L1 ||(cid:102)M − M||F /||(cid:99)M − M||F
 consider matrices various ranks Fig.

 regularization recovered values unobserved entries, plot distributed i.i.d. noise standard deviation σ = added.
 average results ten trials α optimal α ∈ {10−1,
 (cid:102)M (cid:99)M given (2) (1), respectively, L1 ||(cid:102)M −M||F /||(cid:99)M −M||F
 consider patient surveys responses Fig.

 regularization recovered values unobserved entries, plot chosen patient surveys.
 average results ten trials α optimal α ∈ {10−1,
 A.
 Connection Robust Principal Component Analysis (RPCA) program (2) resembles method pro- posed [CLMW11], called Robust Principal Component Analysis (RPCA).
 RPCA is modiﬁed version traditional Principal Component Analysis is robust rare corruptions arbitrary magnitude.
 RPCA, assumes low- rank matrix has set entries corrupted.
 goal is recover true underlying matrix corruptions.
 simply, observed matrix Y ∈ Rn1×n2, have decomposition Y = L + S, L is low-rank matrix like recover S is sparse matrix corruptions.
 strategy ﬁnding decomposition proposed [CLMW11] is ||L||∗ + α||S||1 s.t. L + S = Y.
 (6) argmin L,S method be extended matrix completion setting, like recover unobserved values observed values, subset be corrupted.
 setting, [CLMW11] proposes solving following minimiza- tion problem ||L||∗ + α||S||1 s.t. PΩ(L + S) = PΩ(Y ).
 L,S return original matrix completion problem, assume observed entries be exact.
 Let M ∈ Rn1×n2 be matrix aim recover.
 expect unobserved entries M be sparse, is, small fraction be rewrite minimization (2) form similar RPCA know support corruptions is restricted set ΩC, i.e. S = PΩC (S).
 have, ||A||∗ + α||S||1 s.t. + S = PΩ(M ).
 (7) argmin A,S strategy differs traditional RPCA assume observed data be free errors know corruptions are restricted set unobserved entries.
 following result.
 applying Theorem [CLMW11], have Proposition Suppose M ∈ Rn1×n2 M = U ΣV ∗ gives singular value decomposition M.
 Suppose max ||U∗ei||2 ≤ µr ||V ∗ei||2 ≤ µr ||X||∞ = |Xi,j|, ei r is rank M, is ith standard basis vector µ is incoherence parameter deﬁned [CLMW11].
 Suppose set observed entries, Ω, is distributed sets cardinality m support set S0 non-zero unobserved entries is distributed sets cardinality s contained ΩC.
 is numerical ||U V ∗||∞ ≤ max (cid:114) µr Fig.

 Average optimal α value α ∈ {10−1, minimization given (2) L1 regularization recovered values unobserved entries Lyme patient data.
 III.
 ANALYTICAL REMARKS provide basic analysis regularization approach.
 First, simpliﬁed setting, unobserved entries are modiﬁed recovery given (2) perform least traditional NNM.
 Proposition Suppose M ∈ Rn1×n2 Ω gives set index pairs observed entries.
 Assume unobserved entries are zero, i.e. PΩC (M =
 (cid:99)M = argmin||A||∗ s.t. PΩ(A) = PΩ(M ), and(cid:102)M = argmin||A||∗ + α||PΩC (A)|| s.t. PΩ(A) = PΩ(M ), have matrix norm || · ||.
 Proof: deﬁnitions (cid:99)M (cid:102)M, ||(cid:102)M − M|| ≤ ||(cid:99)M − M|| ||(cid:99)M||∗ ≤ ||(cid:102)M||∗.
 α > have Using inequality above, ||(cid:102)M||∗ + α||PΩC ((cid:102)M )|| ≤ ||(cid:99)M||∗ + α||PΩC ((cid:99)M )|| ≤ ||(cid:102)M||∗ + α||PΩC ((cid:99)M )||.
 ||PΩC ((cid:102)M )|| ≤ ||PΩC ((cid:99)M )||.
 PΩ((cid:102)M = PΩ((cid:99)M = PΩ(M assumption PΩC (M = ||(cid:102)M − M|| = ||PΩC ((cid:102)M )|| ≤ ||PΩC ((cid:99)M )|| = ||(cid:99)M − M||.
 desired result follows constant c such probability least − cn−10 minimization (7) α = n achieves exact recovery, provided rank(L0) ≤ ρrn(2)µ−1(log n(1))−2 s ≤ ρsn(1)n(2), ρr ρs are positive numerical constants.
 proposition is direct application Theorem [CLMW11] program given (7).
 Note here, corruptions are unobserved entries are nonzero.
 Thus, s, number nonzero unobserved entries is small, result be stronger corresponding matrix completion results larger, number missing entries.
 depend authors [CLMW11] note RPCA be thought challenging version matrix completion.
 reasoning being, matrix completion aim recover set unobserved entries, locations are known, whereas RPCA setting, have set corrupted entries, locations are unknown, like identify erroneous determine correct values.
 Figure [CLMW11] provides numerical evidence practice RPCA does fact require more stringent conditions achieve exact recovery corresponding matrix completion problem.
 image completion repair, corruptions are correlated isolated speciﬁc regions image.
 authors provide exper- imental evidence incorporating estimate support corruptions aids recovery.
 same reasoning, expect stronger result suggested Proposition likely holds, do make use fact are able restrict locations corruptions (nonzero, unobserved entries) subset larger matrix.
 IV.
 DISCUSSION incomplete data expect unobserved entries are likely be ﬁnd regularizing values unobserved entries performing NNM improves performance various conditions.
 improvement performance holds synthetic data, noise, Lyme disease survey data.
 investigate performance L1 regularization unob- served entries is natural choice many applications.
 Testing validity methods, such (2), real data is challenging, setting hinges assumption unobserved data is different observed data require having access ground truth values unobserved entries.
 paper, choose take complete data partition observed unobserved entries.
 way manage challenge is examine performance various tasks, such classiﬁcation prediction, based data has been completed different ways.
 setting, relative performance different completion strategies depend speciﬁc task considered.
 However, many applications, like complete data order use further goal.
 setting, judging performance matrix completion algorithm effect performance ultimate goal is natural.
 offer preliminary arguments ex- pect approach (2) work structural assumption unobserved entries are likely be sparse small magnitude, however, stronger theoretical results are likely possible.
 example, show regularizing values unobserved entries performing NNM improves performance case unobserved entries are based empirical evidence expect improved performance general conditions.
 range papers, including [CT10], [RFP10], [CR09], [Gro11], discuss conditions exact matrix completion is possible assumption observed entries matrix are sampled random.
 reasonable structural assumptions unobserved entries be able specify conditions lead exact recovery?
 save such questions future work.
 authors like thank LymeDisease.org use data derived MyLymeData conduct study.
 like thank patients contributions MyLymeData, Anna Ma guidance working data.
 authors were supported NSF CAREER DMS #1348721, NSF BIGDATA DMS #1740325, MSRI NSF DMS #1440140.
 REFERENCES [BK07] [BL07] R.
 M.
 Bell Y.
 Koren.
 Lessons Netﬂix prize challenge.
 Acm Sigkdd Explorations Newsletter,
 J.
 Bennett S.
 Lanning.
 Netﬂix prize.
 Proceedings KDD cup workshop, volume page
 New York, NY, USA,
 [BLWY06] P.
 Biswas, T.-C.
 Lian, T.-C.
 Wang, Y.
 Ye. Semideﬁnite programming based algorithms sensor network localization.
 ACM Trans.
 Sensor Networks (TOSN),
 [CLMW11] E.
 J.
 Cand´es, X.
 Li, Y.
 Ma, J.
 Wright.
 Robust principal [CP10] [CR09] [CT10] [Gro11] [KBV09] component analysis?
 J.
 ACM,
 E.
 J.
 Cand`es Y.
 Plan.
 Matrix completion noise.
 Proceedings IEEE,
 E.
 J.
 Cand`es B.
 Recht.
 Exact matrix completion convex optimization.
 Found.
 Comput.
 Math.,
 E.
 J.
 Cand`es T.
 Tao.
 power convex relaxation: Near-optimal matrix completion.
 IEEE Trans.
 Inform.
 Theory,
 D.
 Gross.
 Recovering low-rank matrices few coefﬁcients basis.
 IEEE Trans.
 Inform.
 Theory,
 Y.
 Koren, R.
 Bell, C.
 Volinsky.
 Matrix factorization techniques recommender systems.
 Computer,
 [LV09] [Rec11] [RFP10] [LRZM12] X.
 Liang, X.
 Ren, Z.
 Zhang, Y.
 Ma. Repairing sparse low- rank texture.
 Computer Vision–ECCV pages
 Z.
 Liu L.
 Vandenberghe.
 Interior-point method nuclear norm approximation application system identiﬁcation.
 SIAM J.
 Matrix Analysis Appl.,
 B.
 Recht.
 simpler approach matrix completion.
 J.
 Machine Learning Research,
 B.
 Recht, M.
 Fazel, P.
 A.
 Parrilo.
 Guaranteed minimum rank solutions linear matrix equations nuclear norm minimization.
 SIAM Review,
 R.
 Schmidt.
 Multiple emitter location signal parameter estimation.
 IEEE Trans.
 Antennas Propagation,
 J.
 L.
 Schafer J.
 W.
 Graham.
 Missing data: view state art.
 Psychological methods,
 A.
 Singer.
 remark global positioning local distances.
 Proc.
 National Academy Sciences,
 [Sch86] [SG02] [Sin08]
 Lung cancer is common cause cancer-related death men.
 Low-dose lung CT screening provides ef- fective way early diagnosis, reduce lung cancer mortality rate.
 Advanced computer-aided diagnosis systems are expected have high sensi- tivities same time maintaining low false positive rates.
 Recent advances deep learning enable rethink ways clinician lung cancer diagnosis.
 1https://github.com/uci-cbcl/DeepLung.git Current lung CT analysis research includes nod- ule detection [6, nodule classiﬁcation [26,
 is few work building complete lung CT cancer diagnosis system automated lung CT can- cer diagnosis using deep learning, integrating nodule detection nodule classiﬁcation.
 is worth exploring whole lung CT cancer diagnosis system understanding performance current deep learning technol- ogy differs experienced doctors.
 best knowledge, is ﬁrst work automated complete lung CT cancer diagnosis system using deep nets.
 emergence large-scale dataset, LUNA16 [24], accelerated nodule detection related research.
 Typi- cally, nodule detection consists stages, region pro- posal generation false positive reduction.
 Traditional approaches require designed features such morphological features, voxel clustering pixel thresholding
 Recently, deep ConvNets, such Faster R-CNN [21, ConvNets [18, are employed generate candidate bounding boxes [5,
 second stage, advanced methods com- plex features, such designed texture features, are used remove false positive nodules.
 nature CT data effectiveness Faster R-CNN object detection natural images [13], design Faster R-CNN nodule detection convolu- tional kernels U-net-like encoder-decoder structure learn latent features [22].
 U-Net structure is convolutional autoencoder, augmented skip connections encoder decoder layers [22].
 Al- has been used context semantic segmentation, being able capture contextual lo- cal information be helpful nodule detections well.
 ConvNet has large number pa- rameters is difﬁcult train public lung CT datasets small sizes, dual path network is employed building block deep dual path network is more compact provides better performance deep resid- ual network same time [3].
 era deep learning, manual feature engi- Figure
 framework DeepLung.
 DeepLung ﬁrst employs Faster R-CNN generate candidate nodules.
 uses deep DPN extract deep features detected cropped nodules.
 Lastly, GBM deep features, detected nodule size, raw pixels is employed classiﬁcation.
 Patient-level diagnosis be achieved fusing classiﬁcation results detected nodules CT.
 neering followed classiﬁers was general pipeline nodule classiﬁcation [10].
 large-scale LIDC-IDRI [2] dataset became available, deep learning-based methods have become dominant framework nod- ule classiﬁcation research [25,
 Multi-scale deep Con- vNet shared weights different scales has been pro- posed nodule classiﬁcation [26].
 weight sharing scheme reduces number parameters forces multi-scale deep ConvNet learn scale-invariant features.
 Inspired recent success dual path network (DPN) ImageNet propose novel framework CT nodule classiﬁcation.
 design deep dual path network extract features.
 gradient boosting machines (GBM) are known have superb performance given effec- tive features, use GBM deep dual path features, nodule size, cropped raw nodule CT pixels nod- ule classiﬁcation [8].
 built automated lung CT cancer di- agnosis system, henceforth called DeepLung, combin- ing nodule detection network nodule classiﬁcation network together, illustrated Fig.

 CT im- age, ﬁrst use detection subnetwork detect candi- date nodules.
 Next, employ classiﬁcation subnet- work classify detected nodules malignant benign.
 patient-level diagnosis result be achieved whole CT fusing diagnosis result nodule.
 main contributions are follows: ex- CT images, deep ConvNets are de- signed nodule detection classiﬁcation respectively.
 ConvNet contains many parameters is difﬁcult train small public lung CT datasets, employ dual path networks neural network architecture DPN uses less parameters obtains performance residual network [3].
 Speciﬁcally, inspired effectiveness Faster R-CNN object detection propose Faster R-CNN nodule detection based dual path network U-net-like encoder-decoder structure, deep dual path network nodule classiﬁcation.
 classiﬁcation framework achieves performance compared state-of-the-art approaches, surpasses performance experienced doctors public dataset, LIDC-IDRI.
 au- tomated DeepLung system, nodule classiﬁcation based detection, is comparable performance experienced doctors nodule-level patient-level diagnosis.

 Related Work Traditional nodule detection involves hand-designed fea- tures descriptors [19] requiring domain expertise.
 Re- cently, several works have been proposed use deep Con- vNets nodule detection learn features, is proven be effective hand- designed features.
 Setio al.
 proposes multi-view Con- vNet false positive nodule reduction [23].
 Due nature CT scans, work proposed ConvNets handle challenge.
 ConvNet (FCN) is proposed generate region candidates, deep ConvNet weighted sampling is used false positive reduction [6].
 Ding et al.
 Liao et al.
 Faster R-CNN generate candidate nodules followed ConvNets re- move positive nodules [5,
 Due effective performance Faster R-CNN [13, design novel network, Faster R-CNN dual path blocks, nodule detection.
 U-net-like encoder-decoder scheme is employed Faster R-CNN learn features [22].
 Nodule classiﬁcation has been based segmentation [7] manual feature design [1].
 Several works designed contour feature, shape feature tex- ture feature CT nodule diagnosis [32,
 Recently, deep networks have been shown be effective medical images.
 Artiﬁcial neural network was implemented CT nodule diagnosis [28].
 effective net- work, multi-scale ConvNet shared weights differ- ent scales learn scale-invariant features, is proposed nection is exist redundancy ex- ploited features.
 dual path connection uses part fea- ture maps dense connection part resid- ual learning.
 dual path connection splits feature maps parts.
 part, F(x)[d :], is used residual learning, other part, F(x)[: d], is used dense connection shown Fig.

 d is hyper-parameter deciding many new features be exploited.
 dual path connection be formulated y = G([x[: d], F(x)[: d], F(x)[d :] + x[d :]), (1) y is feature map dual path connection, G is used ReLU activation function, F is convolutional layer functions, x is input dual path connection block.
 Dual path connection integrates advantages advanced frameworks, residual learning feature reuse dense connection ability exploit new features, uniﬁed structure obtained success Ima- geNet dataset[4].
 design deep neural nets based DPN compactness effectiveness.
 Faster R-CNN U-net-like encoder- decoder structure dual path blocks is illustrated Fig.

 Due GPU memory limitation, input Faster R-CNN is cropped reconstructed CT im- ages pixel size × ×
 encoder network is derived DPN [3].
 ﬁrst max-pooling, convolutional layers are used generate features.
 Af- ter that, dual path blocks are employed encoder subnetwork.
 integrate U-net-like encoder-decoder design concept detection learn deep nets efﬁ- [22].
 fact, region proposal generation, Faster R-CNN conducts pixel-wise multi-scale learning U-net is validated effective way pixel-wise labeling.
 integration makes candidate nodule gener- ation effective.
 decoder feature maps are processed deconvolution layers dual path blocks, are concatenated corre- sponding layers encoder network [34].
 convo- lutional layer dropout (dropout probability is used second last layer.
 last layer, design anchors, scale references are designed based distribution nodule sizes.
 anchor, are parts loss function, classiﬁcation loss Lcls current box is nodule not, regression loss Lreg nodule coordinates x, y, z nodule size d.
 anchor overlaps ground truth bounding box intersection union (IoU) higher con- sider positive anchor (p(cid:63) =
 other hand, anchor has IoU ground truth boxes less consider negative anchor (p(cid:63) =
 multi-task loss function anchor i is deﬁned L(pi, ti) = λLcls(pi, p(cid:63) i + p(cid:63) i Lreg(ti, ti (cid:63)), (2) Figure
 Illustration dual path connection [3], beneﬁts advantage residual learning [11] dense connection [12] network structure design intrinsically.
 nodule classiﬁcation [26].
 Deep transfer learning multi- instance learning is used patient-level lung CT diagno- sis [25,
 comparative study ConvNets is conducted ConvNet is shown be better ConvNet CT data [33].
 multi- task learning transfer learning framework is proposed nodule diagnosis [14].
 Different approaches, propose novel classiﬁcation framework CT nod- ule diagnosis.
 Inspired recent success deep dual path network (DPN) ImageNet [3], design novel DPN extract features raw CT nodules.
 part superior performance GBM complete features, employ GBM different levels granularity rang- ing raw pixels, DPN features, global features such nodule size nodule diagnosis.
 Patient-level diag- nosis be achieved fusing nodule-level diagnosis.

 DeepLung Framework automated lung CT cancer diagnosis system consists parts: detection classiﬁcation.
 design Faster R-CNN nodule detection, propose GBM deep DPN features, raw nodule CT pixels nodule size nodule classiﬁcation.
 Faster R-CNN Deep Dual Path Net Nodule Detection Inspired success dual path network Ima- geNet design deep DPN framework lung CT nodule detection classiﬁcation Fig.
 Fig.

 Dual path connection advantage residual learning dense connection [11,
 shortcut connection residual learning is effective way eliminate vanishing gradient phenomenon deep networks.
 learned feature sharing perspective, residual learning enables feature reuse, dense connec- tion has advantage exploiting new features [3].
 Addi- tionally, connected network has fewer parameters residual learning is need relearn redundant feature maps.
 assumption dual path con- Fig.

 main reason employ dual modules de- tection classiﬁcation is classifying nodules be- nign malignant requires system learn ﬁner-level features, be achieved focusing nod- ules.
 addition, allows introduce extra features ﬁnal classiﬁcation.
 ﬁrst crop CT data centered pre- dicted nodule locations size × ×
 convolutional layer is used extract features.
 dual path blocks are employed learn higher level fea- tures.
 Lastly, average pooling binary logistic regression layer are used benign malignant diagnosis.
 deep dual path network be used clas- siﬁer nodule diagnosis be em- ployed learn effective features.
 construct feature concatenating learned deep DPN features (the sec- ond last layer (2,560 dimension)), nodule size, raw cropped nodule pixels.
 Given complete effec- tive features, GBM is superb method build advanced classiﬁer [8].
 validate feature combining nodule size raw cropped nodule pixels combination GBM classiﬁer obtained average test accuracy.
 Lastly, employ GBM constructed feature achieve best diagnosis performance.
 DeepLung System: Fully Automated Lung CT Cancer Diagnosis DeepLung system includes nodule detection us- ing Faster R-CNN nodule classiﬁcation using GBM constructed feature (deep dual path features, nodule size raw nodule CT pixels) shown Fig.

 Due GPU memory limitation, ﬁrst split whole CT several × × patches, process detector, combine detected results to- gether.
 keep detected boxes detection prob- abilities larger (threshold -2 sigmoid function).
 that, non-maximum suppression (NMS) is adopted based detection probability intersection union (IoU) threshold expect miss many ground truth nodules.
 get detected nodules, crop nodule center detected center size ×
 detected nodule size is kept feature input later downstream classiﬁcation.
 deep DPN is em- ployed extract features.
 use GBM construct features conduct diagnosis detected nodules.
 pixel feature, use cropped size × × center detected nodule center experiments.
 patient-level diagnosis, detected nodules is positive patient is classiﬁed having cancer.
 Conversely, detected nodules are patient is considered non-cancer.
 Figure
 Faster R-CNN framework contains dual path blocks U-net-like encoder-decoder structure.
 design layers dual path network encoder subnetwork.
 model employs anchors multi-task learning loss, including coordinates (x, y, z) diameter d regression, candidate box classiﬁcation.
 numbers boxes are feature map sizes format (#slices*#rows*#cols*#maps).
 numbers connections are format (#ﬁlters #slices*#rows*#cols).
 Figure
 deep dual path network framework nodule classiﬁcation subnetwork, contains dual path con- nection blocks.
 training, deep dual path network feature is extracted gradient boosting machine do diagnosis.
 numbers are same formats Fig.

 pi is predicted probability current anchor i being nodule, ti is predicted relative coordinates nodule position, is deﬁned ti = x − xa da y − ya da z − za da log( da )), (3) (x, y, z, d) are predicted nodule coordinates diameter original space, (xa, ya, za, da) are coor- dinates scale anchor i.
 ground truth position, is deﬁned x(cid:63) − xa y(cid:63) − ya z(cid:63) − za log( )), (4) d(cid:63) da t(cid:63) i = da da da (x(cid:63), y(cid:63), z(cid:63), d(cid:63)) are nodule ground truth coordinates diameter.
 λ is set Lcls, used binary cross entropy loss function.
 Lreg, used smooth l1 regression loss function [9].
 Gradient Boosting Machine Dual Path Net Feature Nodule Classiﬁcation CT data, advanced method be effective ex- tract volume feature [33].
 design deep dual path network CT lung nodule classiﬁcation
 Experiments conduct extensive experiments validate DeepLung system.
 perform 10-fold cross validation us- ing detector LUNA16 dataset.
 nodule classiﬁ- cation, use LIDC-IDRI annotation, employ LUNA16’s patient-level dataset split.
 Finally, val- idate whole system based detected nodules patient-level diagnosis nodule-level diagnosis.
 training, model, use epochs total stochastic gradient descent optimization mo- mentum batch size parameter is limited GPU memory.
 use weight decay
 initial learning rate is total number epoch, epoch
 Datasets LUNA16 dataset is subset largest avail- able dataset pulmonary nodules, LIDC-IDRI [2,
 LUNA16 dataset has detection annotations, LIDC-IDRI contains related information low-dose lung CTs including several doctors’ annotations nodule sizes, locations, diagnosis results, nodule texture, nodule margin other informations.
 LUNA16 dataset removes CTs slice thickness greater slice spacing inconsistent missing slices LIDC-IDRI dataset, gives patient-level 10-fold cross validation split dataset.
 LUNA16 dataset contains low-dose lung CTs, LIDC-IDRI contains low-dose lung CTs. Note LUNA16 dataset removes annotated nodules size smaller
 nodule classiﬁcation, extract nodule annotations LIDC-IDRI dataset, ﬁnd mapping different doc- tors’ nodule annotations LUNA16’s nodule annota- tions, obtained ground truth nodule diagnosis averaging different doctors’ diagnosis (discarding score diagnosis corresponds N/A.).
 ﬁnal av- erage score is equal (uncertain malignant be- nign), remove nodule.
 nodules score greater label positive.
 label negative.
 CT slides were annotated anonymous doctors, identities doctors (referred Drs 1-4 1st-4th annotations) are con- sistent.
 such, refer “simulated” doctors.
 make results reproducible, keep CTs LUNA16 dataset, use same cross validation split LUNA16 classiﬁcation.
 Preprocessing Three automated preprocessing steps are employed input CT images.
 clip raw data
 Second, transform range [0,
 Finally, use LUNA16’s given segmentation ground truth remove background.
 Figure
 Faster R-CNN network residual blocks.
 contains several residual blocks.
 employ deep resid- ual network layers encoder subnetwork, is extension Res18 net [11].
 DeepLung Nodule Detection train evaluate detector LUNA16 dataset following 10-fold cross validation given patient-level split.
 training, augment dataset randomly ﬂip- ping image use cropping scale betweeb
 evaluation metric, FROC, is average recall rate average number false positives scan, is ofﬁcial evaluation metric LUNA16 dataset [24].
 test phase, use detec- tion probability threshold -2 sigmoid function), followed NMS IoU threshold validate performance proposed deep dual path network detection, employ deep residual network comparison Fig.

 encoder part baseline network is deep residual network layers, is extension Res18 net [11].
 Note Res18 Faster R-CNN contains trainable parameters, DPN26 Faster R-CNN employs trainable parameters, is Res18 Faster R-CNN.
 FROC performance LUNA16 is visualized Fig.

 solid line is interpolated FROC based true prediction.
 DPN26 Faster R-CNN achieves FROC score false positive nodule reduc- tion stage, is previous using two-stage training [6].
 DPN26 Faster R-CNN us- ing parameters performs better Res18 Faster R-CNN, demonstrates superior suit- ability DPN detection.
 Ding et al.
 obtains FROC using Faster R-CNN followed extra false positive reduction classiﬁer [5], employ enhanced Faster R-CNN deep dual path detec- tion.
 have applied model Alibaba Tianchi Medical AI nodule detection challenge were able achieve top accuracy hold-out dataset.
 DeepLung Nodule Classiﬁcation validate nodule classiﬁcation performance DeepLung system LIDC-IDRI dataset LUNA16’s split 10-fold patient-level cross vali- Table
 Nodule-level diagnosis accuracy (%) nodule classiﬁcation subnetwork DeepLung experienced doctors doctor’s conﬁdent nodules.
 Dr Dr Average Doctors DeepLung Dr Dr Table
 Statistical property predicted malignant probability borderline nodules (%) Prediction > Frequency < > < > < > Figure
 Sensitivity (Recall) rate respect false positives scan.
 FROC (average recall rate false positives Res18 Faster R-CNN is FROC DPN26 Faster R-CNN is parameters Res18 Faster R-CNN.
 Res18 Faster R-CNN has total recall rate detected nodules, DPN26 Faster R-CNN has recall rate
 Table
 Nodule classiﬁcation comparisons LIDC-IDRI dataset.
 Models Multi-scale CNN [26] Slice-level CNN [33] Nodule-level CNN [33] Vanilla CNN [33] Multi-crop CNN [27] Deep DPN Nodule Size+Pixel+GBM All feat.+GBM Accuracy Year dation.
 are nodules are positive.
 training, ﬁrst pad nodules size × × crop padded data, horizontal ﬂip, vertical ﬂip, z-axis ﬂip data augmentation, set patch zero, normalize data mean standard deviation obtained training data.
 total number epochs is
 initial learning rate is reduce epoch epoch
 Due time resource limitation training, use fold test, ﬁnal performance is average performance ﬁve test folds.
 nodule classiﬁcation performance is concluded Table
 table deep DPN achieves per- formance Multi-scale CNN [26], Vanilla CNN [33] Multi-crop CNN [27], strong power structure deep dual path network.
 GBM nodule size raw nodule pixels crop size achieves comparable performance multi- scale CNN [26] superior classiﬁcation per- formance GBM.
 Finally, construct feature deep dual path network features, Faster R-CNN detected nodule size raw nodule pixels, obtain ac- curacy, shows effectiveness deep dual path network features.
 Compared Experienced Doctors Their Individual Conﬁdent Nodules compare predictions “simulated” experienced doctors individually conﬁdent nodules (with individual score
 Note annota- tions are
 Comparison results are concluded Table
 Table doctors’ conﬁdent nodules are easy be diagnosed nodules performance comparison model’s performances Table Table
 average performance model is better experienced doctors individually conﬁdent diagnosed nodules.
 fact, model’s performance is better doctors (doc- tor conﬁdent nodule diagnosis task.
 re- sult validates deep network surpasses human-level perfor- mance image classiﬁcation [11], DeepLung is suited nodule diagnosis experienced doctors.
 employ Kappa coefﬁcient, is common approach evaluate agreement raters, test agreement DeepLung ground truth [16].
 kappa coefﬁcient DeepLung is is better average kappa coefﬁcient doctors (81.58%).
 evaluate performance nod- ules including borderline nodules (labeled uncertain malignant benign), compute log likeli- hood (LL) scores DeepLung doctors’ diagnosis.
 randomly times experienced doctors’ annotations “simulated” doctors.
 mean LL doctors is -2.563 standard deviation
 contrast, LL DeepLung is -1.515, showing performance DeepLung is standard deviation average performance doctors, is signiﬁcant.
 is important analysis sta- Table
 Comparison DeepLung’s classiﬁcation detected nodules doctors nodules.
 Method Acc.
 (%) TP Set FP Set Doctors 74.05-82.67 Table
 Patient-level diagnosis accuracy(%) DeepLung experienced doctors doctor’s conﬁdent CTs. Dr Average Doctors DeepLung Dr Dr Dr tistical property predictions borderline nodules cannot be classiﬁed doctors.
 Interestingly, borderline nodules are classiﬁed be malignant (with probability > benign (with proba- bility Table
 DeepLung classiﬁed most borderline nodules malignant probabilities closer one, showing potential tool assisted diagnosis.
 DeepLung Fully Automated Lung CT Can- cer Diagnosis validate DeepLung automated lung CT cancer diagnosis LIDC-IDRI dataset same protocol LUNA16’s patient-level split.
 employ Faster R-CNN detect suspicious nodules.
 retrain model nodule classiﬁcation model detected nodules dataset.
 center detected nodule is ground positive nodule, is pos- itive nodule.
 is negative nodule.
 mapping detected nodule ground truth evaluate performance compare performance experienced doctors.
 adopt test fold validate performance same nodule classiﬁcation.
 Different pure nodule classiﬁcation, auto- mated lung CT nodule diagnosis relies nodule detection.
 evaluate performance DeepLung detec- tion true positive (TP) set detection positive (FP) set Table
 detected nodule center ground truth regions, is TP set.
 detected nodule center ground truth regions, is FP set.
 Table DeepLung system using detected nodule region obtains accu- racy detected TP nodules.
 Note expe- rienced doctors obtain accuracy nodule diagnosis average.
 DeepLung system au- tomated lung CT nodule diagnosis achieves av- erage performance experienced doctors.
 FP set, nodule classiﬁcation subnetwork DeepLung reduce FP detected nodules, guarantees automated system is effective lung CT can- cer diagnosis.
 Compared Experienced Doctors Their Individually Conﬁdent CTs employ DeepLung patient-level diagnosis fur- ther.
 current CT has nodule is classiﬁed positive, diagnosis CT is positive.
 nod- ules are classiﬁed negative CT, diagnosis CT is negative.
 evaluate DeepLung doc- tors’ conﬁdent CTs benchmark comparison Table
 Table DeepLung achieves patient-level diagnosis accuracy.
 is average perfor- mance experienced doctors better Dr alto- gether.
 performance gives conﬁdence DeepLung be useful tool assist doctors’ diagon- sis.
 further validate method doc- tors’ individual conﬁdential CTs. Kappa coefﬁcient DeepLung is average Kappa coefﬁ- cient doctors is
 implies predictions DeepLung are good agreement ground truths patient-level diagnosis, are comparable experienced doctors.

 Discussion section, argue utility DeepLung visualizing nodule detection classiﬁcation results.
 Nodule Detection randomly pick nodules test fold visual- ize red circles ﬁrst row Fig.

 Detected nodules are visualized blue circles second row.
 Be- cause CT is voxel data, plot central slice visualization.
 third row shows detection prob- abilities detected nodules.
 central slice number is shown slice.
 diameter circle is relative nodule size.
 central slice visualizations Fig.
 ob- serve detected nodule positions including central slice numbers are consistent ground truth nodules.
 circle sizes are similar nodules ﬁrst row second row.
 detection probability is high nodules third row.
 shows Faster R-CNN works detect nodules test fold
 Nodule Classiﬁcation visualize nodule classiﬁcation results test fold Fig.

 choose nodules is predicted right, annotated doctors.
 ﬁrst nodules are benign nodules, remaining nod- ules are malignant nodules.
 numbers ﬁg- Figure
 Visualization central slices nodule ground truths detection results.
 randomly choose nodules (red circle boxes ﬁrst row) test fold
 Detection results are shown blue circles second row.
 center slice numbers are shown images.
 last row shows detection probability.
 DeepLung performs nodule detection.
 ures are DeepLung predicted malignant probabilities followed annotation doctors is wrong.
 DeepLung, probability is larger predicts malignant.
 predicts benign.
 experi- enced doctor, nodule is large has irregular shape, has high probability be malignant nodule.
 Figure
 Visualization central slices nodule classiﬁcation results test fold
 choose nodules are predicted DeepLung, annotated doctors.
 numbers nodules are predicted malignant proba- bilities followed annotation doctors is wrong.
 ﬁrst nodules are benign nodules.
 rest nodules are malignant nodules.
 DeepLung performs nodule classiﬁcation.
 Fig.
 observe doctors mis-diagnose nodules.
 reason be be humans are ﬁt process CT data are low signal noise ra- tio.
 doctors cannot ﬁnd weak irregular boundaries consider normal tissues nodule boundaries leading false negatives false pos- itives.
 addition, doctors’ own internal bias play role conﬁdent he/she predicts scans be- ing limited observing slice time.
 Machine learning-based methods overcome limitations are able learn complicated rules high dimensional features utilizing input slices much problem.
 perspective, DeepLung be great use doctors effort make consistent accurage diagonsis.

 Conclusion work, propose automated lung CT can- cer diagnosis system based deep learning.
 DeepLung consists parts, detection classiﬁcation.
 exploit CT images, propose deep convolutional networks based dual path networks, is more compact yield better performance residual networks.
 nodule detection, design Faster R-CNN dual path blocks U-net- encoder-decoder structure detect candidate nodules.
 detected nodules are fed nodule clas- siﬁcation network.
 use deep dual path network extract classiﬁcation features.
 Finally, gradient boost- ing machine combined features are trained classify candidate nodules benign malignant.
 Extensive ex- perimental results public available large-scale datasets, LUNA16 LIDC-IDRI datasets, demonstrate supe- rior performance DeepLung system.
 References [1] H.
 J.
 Aerts et al.
 Decoding tumour phenotype noninva- sive imaging using quantitative radiomics approach.
 Na- ture communications,
 [2] S.
 G.
 Armato al.
 lung image database consortium (lidc) image database resource initiative (idri): com- pleted reference database lung nodules ct scans.
 Medi- cal physics,
 [3] Y.
 Chen, J.
 Li, H.
 Xiao, X.
 Jin, S.
 Yan, J.
 Feng.
 Dual path networks.
 NIPS,
 [4] J.
 Deng, W.
 Dong, R.
 Socher, L.-J.
 Li, K.
 Li, L.
 Fei.
 Imagenet: large-scale hierarchical image database.
 CVPR.
 IEEE,
 [5] J.
 Ding, A.
 Li, Z.
 Hu, L.
 Wang.
 Accurate pulmonary nodule detection computed tomography images using deep convolutional neural networks.
 MICCAI,
 [6] Q.
 Dou, H.
 Chen, Y.
 Jin, H.
 Lin, J.
 Qin, P.-A.
 Heng.
 Automated pulmonary nodule detection convnets online sample ﬁltering hybrid-loss residual learning.
 MICCAI,
 [7] A.
 El-Baz et al.
 shape analysis early diagnosis malignant lung nodules.
 IPMI,
 [8] J.
 H.
 Friedman.
 Greedy function approximation: gradient boosting machine.
 Annals statistics,
 [9] R.
 Girshick.
 r-cnn.
 ICCV, pages
 [10] F.
 Han, G.
 Zhang, H.
 Wang, B.
 Song, H.
 Lu, D.
 Zhao, H.
 Zhao, Z.
 Liang.
 texture feature analysis di- agnosis pulmonary nodules using lidc-idri database.
 Medical Imaging Physics Engineering.
 IEEE,
 [11] K.
 He, X.
 Zhang, S.
 Ren, J.
 Sun.
 Deep residual learning image recognition.
 CVPR,
 [12] G.
 Huang, Z.
 Liu, K.
 Q.
 Weinberger, L.
 van der Maaten.
 connected convolutional networks.
 CVPR,
 [13] J.
 Huang al.
 Speed/accuracy trade-offs modern convo- lutional object detectors.
 CVPR,
 [14] S.
 Hussein, K.
 Cao, Q.
 Song, U.
 Bagci.
 Risk stratiﬁca- tion lung nodules using cnn-based multi-task learning.
 IPMI,
 [15] C.
 Jacobs al.
 Automatic detection subsolid pulmonary nodules thoracic computed tomography images.
 Medical image analysis,
 [16] J.
 R.
 Landis G.
 G.
 Koch.
 measurement observer agreement categorical data.
 biometrics,
 [17] F.
 Liao, M.
 Liang, Z.
 Li, X.
 Hu, S.
 Song.
 Evaluate malignancy pulmonary nodules using deep leaky noisy-or network.
 arXiv preprint arXiv:1711.08324,
 [18] J.
 Long, E.
 Shelhamer, T.
 Darrell.
 convolutional networks semantic segmentation.
 CVPR,
 [19] E.
 Lopez Torres et al.
 Large scale validation m5l lung cad heterogeneous ct datasets.
 Medical physics,
 [20] K.
 Murphy al.
 large-scale evaluation automatic pul- monary nodule detection chest ct using local image fea- tures k-nearest-neighbour classiﬁcation.
 Medical image analysis,
 [21] S.
 Ren, K.
 He, R.
 Girshick, J.
 Sun.
 r-cnn: Towards real-time object detection region proposal
 NIPS,
 [22] O.
 Ronneberger, P.
 Fischer, T.
 Brox.
 U-net: Convolu- tional networks biomedical image segmentation.
 MIC- CAI,
 [23] A.
 A.
 A.
 Setio al.
 Pulmonary nodule detection ct im- ages: false positive reduction using multi-view convolutional networks.
 IEEE TMI,
 [24] A.
 A.
 A.
 Setio al.
 Validation, comparison, combina- tion algorithms automatic detection pulmonary nod- ules computed tomography images: luna16 challenge.
 Medical Image Analysis,
 [25] W.
 Shen, M.
 Zhou, F.
 Yang, D.
 Dong, C.
 Yang, Y.
 Zang, J.
 Tian.
 Learning experts: Developing transferable deep features patient-level lung cancer prediction.
 MICCAI,
 [26] W.
 Shen, M.
 Zhou, F.
 Yang, C.
 Yang, J.
 Tian.
 Multi-scale convolutional neural networks lung nodule classiﬁcation.
 IPMI,
 [27] W.
 Shen, M.
 Zhou, F.
 Yang, D.
 Yu, D.
 Dong, C.
 Yang, Y.
 Zang, J.
 Tian.
 Multi-crop convolutional neural net- works lung nodule malignancy suspiciousness classiﬁca- tion.
 Pattern Recognition,
 [28] K.
 Suzuki, F.
 Li, S.
 Sone, K.
 Doi.
 Computer-aided diag- nostic scheme distinction benign malignant nodules thoracic low-dose ct use massive training artiﬁcial neural network.
 IEEE TMI,
 [29] Z.
 Wang al.
 Exploring vector deep networks action spotting.
 CVPRW,
 [30] Z.
 Wang al.
 Weakly supervised patchnets: Describing aggregating local patches scene recognition.
 IEEE TIP,
 [31] Z.
 Wang al.
 Structed triplets learning pos-tag guided attention visual question answering.
 WACV,
 [32] T.
 W.
 Way, L.
 M.
 Hadjiiski, B.
 Sahiner, H.-P.
 Chan, P.
 N.
 Cascade, E.
 A.
 Kazerooni, N.
 Bogot, C.
 Zhou.
 Computer-aided diagnosis pulmonary nodules ct scans: Segmentation classiﬁcation using active contours.
 Medical Physics,
 [33] X.
 Yan, J.
 Pang, H.
 Qi, Y.
 Zhu, C.
 Bai, X.
 Geng, M.
 Liu, D.
 Terzopoulos, X.
 Ding.
 Classiﬁcation lung nodule malignancy risk computed tomography images using con- volutional neural comparison strategies.
 ACCV,
 [34] M.
 D.
 Zeiler, D.
 Krishnan, G.
 W.
 Taylor, R.
 Fergus.
 De- convolutional networks.
 CVPR.
 IEEE,
 [35] W.
 Zhu al.
 Co-occurrence feature learning skeleton based action recognition using regularized deep lstm net- works.
 AAAI,
 [36] W.
 Zhu, Q.
 Lou, Y.
 S.
 Vang, X.
 Xie.
 Deep multi-instance networks sparse label assignment whole mammo- gram classiﬁcation.
 MICCAI,
 [37] W.
 Zhu, X.
 Xiang, T.
 D.
 T.
 Tran, G.
 D.
 H.
 Hager, X.
 Xie.
 Adversarial structured nets mass segmentation mammograms.
 IEEE International Symposium Biomedi- cal Imaging,
 Deep reinforcement learning is general method have been successful solving complex control problems.
 Mnih al.
 [Mnih combined Q learning deep neural networks proved be successful image based Atari games.
 Policy gradient methods have been proved ef- ﬁcient continuous control problems ([Sutton et al., [Silver et al., [Heess et al., discrete control problems ([Silver [Wang et al.,
 policy gradient methods, actor-critic algorithms are heart many signiﬁcant advances reinforcement learn- ing ([Bhatnagar et [Degris et al., [Lillicrap et al., [Mnih
 algorithms estimate state-action value functions independently, proved be efﬁcient policy optimization.
 However, enormous number online simulation data is required deep reinforcement learning.
 Hence attempt learn expert demonstrations decrease amount online data required deep reinforcement learning algo- rithms.
 representative method learning ex- pert demonstrations is inverse reinforcement learning.
 Ng al.
 proposed ﬁrst inverse reinforcement learning algo- rithm [Ng Russell, recovers reward function based assumption expert policy is global optimal policy.
 recovered reward function, Abbeel et al.
 are able propose apprenticeship learning ([Abbeel Ng, train policy expert demonstrations simulation environment does output reward.
 Appren- ticeship learning inspired many similar algorithms ([Syed Schapire, [Syed [Piot et al., [Ho Ho al.
 [Ho Ermon, proposed imitation learning method merges inverse reinforce- ment learning reinforcement learning, hence imitate expert demonstrations generative adversarial networks (GANs).
 algorithms proved successful solving MDP\R ([Abbeel Ng,
 However, MDP\R is different original MDP MDP\R environments do output task based reward data.
 reason, inverse reinforce- ment based algorithms attempt assume expert demon- strations be global optimal imitate expert demon- strations.
 order learn expert demonstrations MDP, state-of-the-art reinforcement learning algorithms, different frameworks are required.
 are prior work attempt make use ex- pert demonstrations reinforcement learning algorithms.
 [Lakshminarayanan al., Lakshminarayanan al.
 proposed training method DQN based assump- tion expert demonstrations are global optimal, pre- train state-action value function estimators.
 Cruz Jr al.
 [Cruz Jr focused feature ex- tracting high dimensional, image based simu- lation environments, proposed framework discrete control problems pretrains neural networks clas- siﬁcation tasks using supervised learning.
 purpose pretraining process is speed training process try- ing extract features high dimensional states.
 However, work is suitable image based, discrete action en- vironments, ignored fact expert demonstrations perform better current learned policies.
 ﬁrst published version AlphaGo [Silver et al., is important work pretrains neural networks human expert demonstrations.
 work, policy network value network is used.
 value net- work is trained on-policy reinforcement learning, policy network is pretrained expert demonstrations using supervised learning, trained policy gradient.
 work [Cruz Jr et al., are similar, role expert demonstrations is speed feature extraction, give policy warm start.
 fact expert demon- strations perform better is used, framework is extensive other problems other rein- forcement learning algorithms.
 paper, propose extensive framework pre- trains actor-critic reinforcement learning algorithms ex- pert demonstrations, use expert demonstrations policy functions value estimators.
 derive method computing policy gradient value estimators expert demonstrations.
 Experiments show method improves performance baseline algorithms continuous control environments high-dimensional- state discrete control environments.
 Background Preliminaries paper, deal inﬁnite-horizon discounted Markov Decision Process (MDP), is deﬁned tu- ple {S, A, P, r, ρ0, γ}.
 tuple, S is ﬁnite set states, A is ﬁnite set actions, P S × A × S → R is transition probability distribution, r S → R is reward function, ρ0 S → R is probability distribution initial state S0, γ ∈ (0, is discount factor.
 stochastic policy πs S × A → R returns probabil- ity distribution actions based states, deterministic policy πd S → A returns action based states.
 paper, deal stochastic policies deterministic policies, ∼ π(s) means ∼ πs(a|s) = πd(s) respectively.
 state-action value function Qπis: Qπ(st, at) Est+1,at+1,...
 γτ r(st+τ deﬁnitions value function V π advantage function Aπ are: (cid:34) ∞(cid:88) (cid:34) ∞(cid:88) τ =0 (cid:35) (cid:35) V π(st) = Eat,st+1,...
 γτ r(st+τ Aπ(st, at) = Qπ(st, at) − V π(st) τ =0 let η(π) denote discounted reward π: (cid:35) (cid:34) ∞(cid:88) t=0 η(π) = Es0,a0,...
 γtr(st) future convenience, let dπ(s) denote limiting dis- tribution states: dπ(s) = lim t→∞ P r(st = s) deﬁnitions above: s0 ∼ ρ0(s0), ∼ π(st), st+1 ∼ P (st+1|st, at) goal actor-critic reinforcement learning algorithms is maximize discounted reward, η(π), obtain op- timal policy, use parameterized policy πθ.
 estimating η(π) ∇θη(πθ) based simulated samples, many algorithms use state-action value estimator Qw, estimate state-value function Qπ policy function πθ.
 typical deterministic actor-critic algorithm DDPG (Deep Deterministic Policy Gradient) [Lillicrap et uses estimator Qw = ˆQπ estimate gradient off-policy deterministic discounted reward ηβ(πθ) = s∈S dβ(s)V π(s) [Degris et β is roll- (cid:80) policy: ∇θηβ(π) ≈ Est∼dβ (s) = Est∼dβ (s) (cid:2)∇θQw(s, a)|s=st,a∼πθ(st) (cid:3) (cid:2)∇aQw(s, a)|s=st,a∼πθ(st)π(cid:48) (cid:3) Where Qw is updated sampled data π using Bell- man equation, π(cid:48) θ = ∇θπθ(s)|s=st.
 off-policy algorithm has Qw estimator policy πθ is ACER (Actor-Critic Experience Replay) [Wang et optimizes stochastic policy.
 algorithm maximizes off-policy deterministic discounted re- ward ηβ(πθ) well, modiﬁes off-policy policy gra- dient ˆgacer = ∇θηβ(π) to: ˆgacer = ¯ρt∇θ log πθ(at|st)(cid:2)Qret(st, at) − V w(st)(cid:3) (cid:110) ∇θ log πθ(a|st)Aw(st, a) + Ea∼πθ(st) Where Aw(st, a) = Qw(st, a) − V w(st), ¯ρt = [x]+ = x x > is otherwise; min β(a,st) ∼ dβ(st) V w(st) = Ea∼πθ(st)(st, a); ρt(a) = π(a,st) ∼ β(st); Qret is Retrace estimator Qπ [Munos et al., be expressed follows: c, π(at,st) β(at,st) (cid:21) (cid:32)(cid:20) ρt(a) − c (cid:111) ρt(a) (cid:33) Qret(st, at) = rt + γ ¯ρt+1δQ(st+1, at+1) + γV w(s+1) δQ(st+1, at+1) = Qret(st+1, at+1) − Qw(st+1, at+1) ACER, state-action value function is updated using Qret target, gradient gQ: gQ = (Qret(st, at) − Qw(st, at))∇wQw(st, at) paper, apply methods expert demonstrations DDPG ACER.
 Expert Based Pretraining Methods Suppose exists expert policy π∗ performs better π.
 deﬁne perform better following straight- constraint: η(π∗) (cid:62) η(π) (1) deﬁnition perform above is based fact goal actor-critic RL algorithms is maximize η(π).
 expert policy π∗ is different IRL [Ng Russell, imitation learning [Ho Ermon, LfD [Hester et π∗ is optimum policy MDPs. (s, a) pairs, {(st, at)}t=0,1,2,..., sampled π.
 deﬁne demonstration policy π sequence Actor-critic RL algorithms tend optimize η(πθ) target.
 pretraining procedures algorithms need estimate η(πθ) optimization target using expert demonstrations.
 Also, deﬁnition need esti- mate η(π∗) well.
 However, only demonstrations expert policy π∗ black-box simulation environment, η(π∗) η(πθ) cannot be estimated.
 Hence introduce (see [Schulman et al., [Kakade Langford,

 policies π π∗: η(π∗) − η(π) = Es∗ ,a∗ ,...∼π∗ γtAπ(s∗ t a∗ t (2) (cid:35) Proof.
 (See [Schulman et al., [Kakade Langford, Note Aπ(s, a) = Es(cid:48)∼P (s(cid:48)|s,a) [r(s) + γV π(s(cid:48)) − V π(s)] (cid:34) ∞(cid:88) t=0 (cid:35) have: Es∗ ,a∗ ,...∼π∗ ,...∼π∗ (cid:34) ∞(cid:88) (cid:34) ∞(cid:88) t=0 γtAπ(s∗ t a∗ t γt(r(st) + γV π(st+1) − V π(st)) (cid:35) (cid:35) (cid:34) ∞(cid:88) t=0 γtr(st) t=0 [V π(s0)] + Es∗ ,a∗ ,...∼π∗ = − Es∗ = − η(π) + η(π∗) many actor-critic RL algorithms DDPG ACER, policy optimization is based accurate estima- tions state-action value functions value functions learned policy πθ.
 Typically, algorithms use data sam- pled πθ, {(st, at, rt)}t=0,1,2,..., estimate Qπ V π.
 estimating processes need large amount sim- ulations be accurate enough.
 Combine Theorem constraint (1), have: (cid:34) ∞(cid:88) t=0 (cid:35) Es∗ ,a∗ ,...∼π∗ γtAπ(s∗ t a∗ t (cid:62) (3) result links state-action value functions expert demonstrations, allowing apply constraint (1) training state-action value functions.
 constraint is value estimators, Qw V w.
 value estimators are accurate enough, constraint (3) be satis- ﬁed.
 algorithm update value estimators constraint (3), estimators be more accurate, result improve policy optimizing process.
 pretraining process is policy optimization using expert demonstrations.
 most actor-critic algorithms, suppose advantage function Aπ(s, a) is known conducting policy optimization.
 estimate up- date step expert demonstrations estimations value functions.
 Considering Theorem estimate policy gradient following: ∇θη(πθ) =∇θ(η(πθ) − η(π∗)) = − ∇θEs∗ ,...∼π∗ ,a∗ (cid:34) ∞(cid:88) (cid:35) γtAπ(s∗ t a∗ t (4) t=0 Equation (4) provides off-policy policy optimization procedure data expert demonstrations.
 turns perform better is procedure ex- pert policy π∗.
 Recently, people propose sample efﬁcient RL algo- rithms, ACER Q-Prop [Gu et RL al- gorithms need large amount simulation time train- ing.
 expert demonstrations, is reward data, cannot conduct sample efﬁcient policy optimization processes.
 However, update policies (4), simulation time is needed.
 call situation simulation ef- ﬁcient, means algorithms need large amount data, need few simulation data training.
 Note sample efﬁcient algorithms are simulation ef- ﬁcient methods intend decrease simulation time.
 paper, evaluate method simulation efﬁcient is.
 section, found pretraining methods actor- critic RL algorithms, (3) (4).
 are based Theorem
 theorem connects policy dis- counted reward η(πθ) expert demonstration data, requir- ing reward data expert trajectories.
 Equation (3) gives constraint value function estimators based deﬁnition perform better, equation (4) provides off- policy method optimize policy function regardless expert demonstrations perform.
 Algorithms Expert Demonstrations Theorem provides way satisfy constraint (1) update policies πθ demonstrations expert policy π∗, does need data sampled π∗.
 section, organize results Section piratical way, apply pretraining methods typical actor- critic RL algorithms, DDPG ACER.
 actor-critic RL algorithms use neural networks Qw(s, a) estimate state-action value functions pol- icy, Qπ(s, a), π is is current learned policy training, is parameterized function, πθ, form artiﬁcial neural networks.
 pretraining processes based Theorem need estimator advantage function policy πθ, Aπ(s, a).
 Based parameterized policy state-action value func- tion estimator Qw, obtain advantage function estima- tor Aw,θ: Aw,θ(s∗ t a∗ V w,θ(s∗ t a∗ t = Qw(s∗ t = Ea∼πθ(s)Qw(s∗ t − V w,θ(s∗ t t a) (5) (6) Figure Example screenshots MuJoCo simulation environments attend experiment DDPG baseline.
 tasks are: HalfCheetah (left), Hopper (middle), Walker2d (right).
 Considering training processes DDPG ACER, beginning processes policies are random estimators Qw(s, a) are accurate, is little data simulation.
 exist expert demonstrations perform better initial introduce data using constraint (3), order obtain accurate estimator Qw(s, a).
 Qw(s, a) is accurate fact π∗ performs better.
 Hence update estimator expert demonstrations following gradient, [x]+ = x x > otherwise is zero: constraint (3) is satisﬁed, (cid:34) Q = ∇w g∗ Es∗ ,a∗ ,...∼π∗ γtAw,θ(s∗ t a∗ t (cid:34) ∞(cid:88) t=0 (cid:34) ∞(cid:88) (cid:35)(cid:35) (7) (cid:35) From equation (4), optimize policy expert demon- strations.
 expert demonstrations do contain reward data, update policy parameters simple policy gradient: π = −∇θEs∗ g∗ ,a∗ ,...∼π∗ γtAw,θ(s∗ t a∗ t (8) t=0 reason π∗ is optimal policy MDPs, train expert demonstrations lim- ited period time beginning training process, guarantee π∗ performs πθ, hence call process pretraining.
 add gradients g∗ algorithms: pretrain actor-critic RL algorithms DDPG π original gradients Q g∗ Q = gQ + λQg∗ gpre π = gπ + λπg∗ gpre (9) (10) Q gpre Where gQ gπ are original gradients baseline actor- critic RL algorithms, andgpre are pretraining gradi- ents estimator Qw parameterized policy function πθ pretraining.
 introduce expert demon- strations base algorithms replacing them, state-action value functions are estimated baseline algorithms gradient g∗ Q makes Qw satisfy constraint (1).
 Pretraining DDPG DDPG is representative off-policy actor-critic deterministic RL algorithm.
 algorithm is continuous action space MDPs, optimizes policy using off-policy policy gra- dient.
 neural networks are used DDPG same time.
 is named critic network, is state-action value function estimator Qw, other is named actor network, is parameterized policy πθ.
 is algorithm deterministic control, input actor network is state MDPs, output is corresponding action.
 neural networks are trained simultaneously, gra- dients gQ gπ respectively.
 gQ is based Bellman equa- tion, gπ is off-policy policy gradient.
 order introduce expert demonstrations pretraining critic network actor network, apply (9) (10) pretrain neural networks.
 Note deterministic policy πθ, equation (6) be- comes V w,θ(s) = Qw(s, πθ(s)).
 Pretraining ACER ACER is off-policy actor-critic stochastic RL algorithm, modiﬁes policy gradient make process sam- ple efﬁcient.
 ACER solves discrete control problems continuous control problems.
 discrete control problems, double-output convolu- tional neural work (CNN) is used ACER.
 output is softmax policy πθ, other is Qw values.
 θ w share most parameters, are updated sepa- different gradients.
 stochastic control problems, new structure named Stochastic Dueling Networks (SDNs) is used value func- (cid:80)n tion estimation.
 network outputs deterministic value es- timation V w(s), stochastic state-action value estimation Qw,θ(s, a) ∼ V w(s) + Aw(s, a) i=1 Aw(s, ˙a)| ˙a∼πθ.
 t − Hence equation (5) becomes Aw,θ(s∗ t = Qw,θ(s∗ V w(s∗ t ).
 t a∗ t a∗ ACER, gradient gπ is modiﬁed policy gradient, gQ is based Retrace.
 gradients are explained Section
 paper, compute pretraining gradients g∗ g∗ π expert demonstrations.
 Policy gradient is estimated using trust region ACER, Q Figure Results pretraining based DDPG.
 ﬁgures is different task, are experimented HalfCheetah (left), Hopper (middle) Walker2d (right), vertical dashed black lines represent points pretraining end, horizontal dashed brown lines represent average episode reward expert demonstrations.
 transparent blue red lines are original training results, opaque lines are smoothed lines sliding windows.
 Experiments test algorithms based DDPG ACER various environments, order investigate simulation efﬁcient pretraining methods are.
 baselines are DDPG ACER pretraining.
 existence [x]+, g∗ Q deﬁned (7) be inﬁnity sometimes.
 Hence clip gradient pre- training.
 set λQ λπ = equations (9) (10).
 expert policies generate expert demonstrations are policies trained baseline algorithms, i.e. DDPG ACER.
 DDPG baseline, apply algorithm low dimensional simulation environments using MuJoCo physics engine [Todorov al., test tasks action dimensionality are: HalfCheetah (6D), Hopper (3D), Walker2d (6D).
 tasks are illustrated Figure
 setups DDPG baseline share same net- work architecture compute policies estimate value functions referring [Lillicrap
 Adam [Kingma Ba, is used learning parameters learn- ing rate actor network critic network are
 critic network, L2 weight decay is used γ
 actor network critic network have hidden layers units respectively.
 results pretraining method based DDPG are illustrated Figure
 ﬁgures, horizontal dashed brown lines represent average episode reward expert demonstrations.
 is obvious expert demonstrations are global optimal demonstrations, order guar- antee expert policies perform better learned pretraining process stops training steps simulation steps.
 shown Figure is obvious DDPG pretraining method outperforms initial DDPG.
 Results HalfCheetah (Figure left) is representative clear, pre- training process gives training warm start, pre- training stops, performance drops new learning gradient.
 However, pretraining, DDPG learns faster baseline, hence outperforms initial DDPG.
 results DDPG are unstable Hopper (Fig- ure middle) Walker2d (Figure right), smoothed results Figure Example screenshots Atari simulation environments attend experiment ACER baseline.
 tasks left right are: AirRaid, Breakout, Carnival, CrazyClimber Gopher.
 indicate DDPG pretraining processes learns faster DDPG.
 ACER baseline, apply algorithm image based Atari games.
 tested discrete control prob- lems ACER, environments tested are: Air- Raid, Breakout, Carnival, CrazyClimber Gopher.
 en- vironments are illustrated Figure
 experiment settings are similar [Wang et al., double-output network consists convolutional layer kernels stride convolutional layer × kernels stride convolutional layer × kernels stride followed connected layer units.
 network outputs softmax policy state-action value Q action.
 limitation memory, thread ACER have replay memory frames, is only different setting [Wang
 Entropy regular- ization weight is adopted, discount factor γ = importance weight truncation c
 Trust region updating is used described [Wang al., settings trust region update remain same.
 ACER trust region update is tested paper.
 results pretraining method based ACER trust region update is illustrated Figure
 envi- ronments are image based Atari games.
 lines have same meaning Figure is obvious ACER pretraining process outperforms initial ACER.
 DDPG, performance learned policies does fall pretraining process ends.
 is 0246810Training Steps /1050100020003000400050006000RewardDDPGDDPG+Pretrain0246810Training Steps /10505001000150020002500300035000246810Training Steps Figure Results pretraining based ACER trust region update.
 Similar Figure vertical dashed black lines are points pretraining end, horizontal dashed brown lines are average episode reward expert demonstrations.
 transparent red blue lines represent original training results, opaque ones are smoothed results sliding windows.
 Q deﬁned (7) is zero, g∗ stochastic discrete control, random policy random state-action value estimator satisﬁes constraint (1), hence g∗ π deﬁned (8) is policy gradient based expert demonstrations, similar original gπ baseline ACER, performance learned policies does fall pretraining.
 Note learning expert demonstrations use same amount simulation steps baseline pre- training method is more simulation efﬁcient baselines.
 Conclusion work, propose extensive method pretrains actor-critic reinforcement learning methods.
 Based The- orem design method takes advantage expert demonstrations.
 method does rely global op- timal assumption expert demonstrations, is key differences method IRL algorithms.
 method pretrains policy function state-action value estimators gradients (9) (10).
 experiments based DDPG ACER, demonstrate method outperforms raw RL algorithms.
 limitation framework is has estimate advantage function expert demonstrations, framework is suitable algorithms A3C [Mnih et al., TRPO [Schulman et main- tain value estimator V w(s).
 other hand, fact expert demonstrations perform better is considered pretraining policies (Equation (8)).
 left extensions future work.
 References [Abbeel Ng, Pieter Abbeel Andrew Y Ng. Apprenticeship learning inverse reinforcement learn- ing.
 Proceedings twenty-ﬁrst international con- ference Machine learning, page

 [Bhatnagar al., Shalabh Bhatnagar, Richard Sutton, Mohammad Ghavamzadeh, Mark Lee.
 Natural actor- critic algorithms.
 Automatica,
 [Cruz Jr Gabriel V Cruz Jr, Yunshu Du, Matthew E Taylor.
 Pre-training neural networks human demonstrations deep reinforcement learning.
 arXiv preprint arXiv:1709.04083,
 [Degris al., Thomas Degris, Martha White, Richard S Sutton.
 Actor-Critic.pdf.
 Icml,
 [Gu al., Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E Turner, Sergey Levine.
 Q- Prop Sample-Efﬁcient Policy Gradient Off Policy Critic.
 ICLR, pages
 [Heess al., Nicolas Heess, Gregory Wayne, David Silver, Tim Lillicrap, Tom Erez, Yuval Tassa.
 Learn- ing continuous control policies stochastic value gradi- ents.
 Advances Neural Information Processing Sys- tems, pages
 [Hester al., Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Andrew 0.00.20.40.60.81.0Training Steps /107020406080100RewardAirRaidACER+pretrainACER0.00.20.40.60.81.0Training Steps /107051015202530Breakout0.00.20.40.60.81.0Training Steps /10710152025303540Carnival0.00.20.40.60.81.0Training Steps /1070255075100125150175RewardCrazyClimber0.00.20.40.60.81.0Training Steps ence Machine Learning Knowledge Discovery Databases, pages

 [Schulman et al., John Schulman, Sergey Levine, Michael Jordan, Pieter Abbeel.
 Trust Region Policy Optimization.
 Icml-2015, page
 et al., David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller.
 Deterministic policy gradient algorithms.
 Proceedings International Conference Machine Learning (ICML-14), pages
 et al., David Silver, Aja Huang, Chris J Maddi- son, Arthur Guez, Laurent Sifre, George Van Den Driess- che, Julian Schrittwieser, Ioannis Antonoglou, Veda Pan- neershelvam, Marc Lanctot, al.
 Mastering game go deep neural networks tree search.
 Nature,
 [Sutton et Richard S.
 Sutton, David Mcallester, Satinder Singh, Yishay Mansour.
 Policy Gradient Methods Reinforcement Learning Function Ap- proximation.
 Advances Neural Information Processing Systems pages
 [Syed Schapire, Umar Syed Robert E Schapire.
 game-theoretic approach apprenticeship Advances neural information processing learning.
 systems, pages
 [Syed et al., Umar Syed, Michael Bowling, Robert E Schapire.
 Apprenticeship learning using linear Proceedings 25th international programming.
 conference Machine learning, pages

 [Todorov al., Emanuel Todorov, Tom Erez, Yu- val Tassa.
 Mujoco: A physics engine model-based control.
 Intelligent Robots Systems (IROS), IEEE/RSJ International Conference on, pages
 IEEE,
 [Wang al., Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, Nando Freitas.
 Sample efﬁcient actor-critic expe- rience replay.
 arXiv preprint arXiv:1611.01224,
 Sendonaris, Gabriel Dulac-Arnold, Ian Osband, John Aga- piou, et al.
 Learning demonstrations real world reinforcement learning.
 arXiv preprint arXiv:1704.03732,
 [Ho Ermon, Jonathan Ho Stefano Ermon.
 Advances Generative adversarial imitation learning.
 Neural Information Processing Systems, pages
 [Ho Ermon, Jonathan Ho Stefano Ermon.
 Generative Adversarial Imitation Learning.
 Nips, pages
 [Ho al., Jonathan Ho, Jayesh Gupta, Stefano Ermon.
 Model-free imitation learning policy opti- mization.
 International Conference Machine Learn- ing, pages
 [Kakade Langford, Sham Kakade John Lang- ford.
 Optimal Approximate Reinforce- ment Learning.
 Proceedings International Conference Machine Learning, pages
 [Kingma Ba, Diederik Kingma Jimmy Ba. arXiv Adam: A method stochastic optimization.
 preprint arXiv:1412.6980,
 [Lakshminarayanan al., Aravind Lakshmi- narayanan, Sherjil Ozair, Yoshua Bengio.
 Reinforce- ment Learning Few Expert Demonstrations.
 Neural Information Processing Systems Workshop Deep Learning Action Interaction,
 [Lillicrap al., Timothy P.
 Lillicrap, Jonathan J.
 Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yu- val Tassa, David Silver, Daan Wierstra.
 Continuous control deep reinforcement learning.
 arXiv preprint arXiv:1509.02971, pages
 [Mnih al., Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle- mare, Alex Graves, Martin Riedmiller, Andreas K Fidje- land, Georg Ostrovski, al.
 Human-level control deep reinforcement learning.
 Nature,
 [Mnih al., Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu.
 Asyn- chronous methods deep reinforcement learning.
 International Conference Machine Learning, pages
 [Munos et R´emi Munos, Tom Stepleton, Anna Harutyunyan, Marc G.
 Bellemare.
 Safe Efﬁcient Off-Policy Reinforcement Learning.
 arXiv, (Nips),
 [Ng Russell, Andrew Ng Stuart Russell.
 Al- gorithms inverse reinforcement learning.
 Proceedings Seventeenth International Conference Machine Learning,
 [Piot al., Bilal Piot, Matthieu Geist, Olivier Pietquin.
 Boosted bellman residual minimization han- Joint European Confer- dling expert demonstrations.
 Imagine are interested setting classiﬁcation program deciding given word belongs certain language.
 be seen mission supervised machine learning, machine experiences labeled data target language.
 label is datum is contained language otherwise.
 machines task is infer rule order generate words language interest generalize training samples.
 Inductive Inference provides model diﬀerent performance measures, allowing abstract likewise important questions concerning details implemantation focuses general properties learning process be achieved.
 formally, according Gold [1967] learner is modelled com- putable function, receiving sequences incorporating more more data.
 Thereby, updates current description target language hypothesis).
 Learning is considered successful, ﬁnite time Martin Aschenbach, Timo K¨otzing, Karen Seidel learner’s hypotheses yield good enough approximations target language.
 original most common learning success criterion is called explanatory (Ex-)learning (Lim-)learning limit requires learner settles correct hypothesis, recog- nizes words language be learned.
 allowing vacillation many descriptions admitting many anomalies, i.e., deviations described language target language, have been considered, see example Case [2016].
 Analyzing diﬀerent measures learning success answers natural questions re- garding learning process: Will learning power reduce inconsistent hypotheses are allowed be changed?
 learning proceed mono- tonically, is, hypothesis is subset hypotheses?
 successful more collections languages, case demand monotonoc- ity sets inferred words?
 does relate requiring cautious learner, guesses supersets hypotheses?
 Osherson, Stob, Weinstein [1986] several restrictions learning informant mention cautious learning, forbids con- jecture strict subset earlier conjecture, is restriction learning power; extend statement Proposition
 consider version conservativeness mind changes (changes hypothesis) are allowed is positive data contradict- ing current hypothesis, claim restrict learning power.
 paper, stick deﬁnition B¯arzdi¸nˇs [1977] Blum Blum [1975], deﬁne conservativeness learning informant such mind changes are allowed is negative data contradicting current hypoth- esis, is natural deﬁnition case learning informants.
 work Lange Zeugmann [1994] considers restricted learning indexable families, i.e., sets languages is uniform decision procedure, deal arbitrary collections recur- enumerable sets.
 See Lange, Zeugmann, Zilles [2008] survey learning indexable families.
 learning positive information, so-called texts, are en- tire maps displaying pairwise relations diﬀerent learning restrictions, see K¨otzing Palenta [2014] Jain, K¨otzing, Ma, Stephan [2016].
 paper give informative map case learning in- formant.
 derive this, Section normal forms regularity property learning informants are provided.
 Especially, is shown learners be assumed total presentation informant be canonical, is, presenting data labels following common well-ordering N.
 Gold [1967] was interested normal forms proved be assumed loss generality basic setting, re- sults apply called delayable learning success criteria be helpful generalize insights, are bounded Lim-learning.
 Section proceed analyze set delayable learning restrictions, share common feature hypotheses be delayed violat- Learning Families Formal Languages Informants ing learning restriction (by contrast, hypothesis is consistent be consistent due new data, restriction consistency is delayable).
 show promiment requirement conservativeness does restrict learning power, paired (strong) decisiveness, re- quiring learner return abandoned conjecture.
 is essential understanding relations diﬀerent delayable learning restictions, Lim-learning informants, number other learning criteria are non-restrictive.
 proof is intricate simulation combining diﬀerent techniques ﬁeld.
 complete map delayable learning restrictions showing afore- mentioned learning restrictions cautiousness, avoiding hypotheses proper subsets guesses, monotonicity, removing correct data conjecture, are incomparable terms learning power.
 yields pairwise relations learning criteria, being displayed Figure Figure
 Section generalize result Gold [1967], Lim-learning texts, negative information is implicit absence certain data, be harder Lim-learning informants.
 Relations presentation modi have been investigated Lange Zeugmann [1993], focussed interdependencies considering diﬀerent learning restriction concerning monotonicity.
 show observation monotonic Lim-learnability informants implying Lim-learnability texts is bound indexable families providing collection recursive languages separating learning criteria.
 Learning text has been studied, including many learning success criteria other variations, see Jain, Osherson, Royer, Sharma [1999] Case [2016].
 observation indexable family being Lim-learnable informants correct learnable, convergence hypothe- ses needs be semantic, fails arbitrary collections enu- merable sets are consideration.
 follows results B¯arzdi¸nˇs [1974] Case Smith [1983], observed Section
 Further, Section prove holds case delayable semantic restrictions are required.
 Additionally, strict hierarchy allowing increasing ﬁnite number anomalies learning power allowing vacillation many correct hypotheses, are provided sections, respectively.
 adress open questions arising paper other challenges related investigations.
 Informant Learning: Restrictions Success let N denote natural numbers including write inﬁnite cardinality.
 Moreover, function f write dompfq domain ranpfq range.
 deal (a subset cartesian product, are going refer projection functions ﬁrst second coordinate pr1 pr2, respectively.
 sets X, Y P N write X “a Y X equals Y anomalies, i.e., |pXzY q Y pY zXq| ď a, |.| denotes Martin Aschenbach, Timo K¨otzing, Karen Seidel cardinality function.
 spirit write X “˚ Y exists P N such X “a Y i.e., pXzY qYpY zXq is ﬁnite.
 Further, Xăø denotes ﬁnite sequences X, i.e., functions n P N X X ø stands countably inﬁnite sequences X, i.e., functions N X.
 Additionally, Xďø :“ Xăø Y X ø denotes set ﬁnite inﬁnite sequences X.
 f P Xďø t P N, let frts :“ tps, fpsqq | s ă tu denote restriction f t.
 Finally, sequences σ, τ P Xăø concatenation is τ write σ Ď τ σ is initial segment τ i.e., is denoted σ t P N such σ “ τrts.
 setting, have X “ N ˆ t0,
 denote set partial functions f dompfq Ď Nˆt0, Ñ N total functions f Ñ N P R, respectively.
 possible, notation terminology learning theoretic side fol- low Jain, Osherson, Royer, Sharma [1999], whereas computability theoretic side refer Odifreddi [1999], Rogers [1967] K¨otzing [2009].
 Backbone Learning Restrictions come introduce basic deﬁnitions Gold-Style learning theory important paper.
 Let L Ď N.
 L is enumerable, i.e., is (partial) computable function f such dompfq “ L, call L language.
 case L is recursive, i.e., characteristic function is computable, say is recursive language.
 Moreover, call L Ď PowpNq collection languages, L P L is language.
 L P L is recursive language, L is called collection recursive languages.
 case exists enumeration tLξ | ξ P Ξu L, Ξ Ď N is recursive computable function f ranpfq Ď t0, such x P Lξ ô fpx, ξq “ ξ P Ξ x P N, say L is indexable family recursive languages.
 ﬁx programming system ϕ introduced Royer Case [1994].
 Brieﬂy, ϕ-system, natural number p, denote ϕp par- tial computable function program code p.
 call p index Wp :“ dompϕpq.
 reference Blum complexity measure, p, t P N, p Ď Wp recursive set natural numbers equal denote W t t, machine executing p halts most t steps.
 are going make use recursive sets proof essential Proposition showing conservativeness does restrict (explanatory) Lim-learning informants.
 Moreover, s-m-n refer well-known recursion theoretic ob- servation, gives nice ﬁnite inﬁnite recursion theorems, Case’s Operator Recursion Theorem ORT.
 context language learning, Gold [1967], seminal paper, distin- guished major diﬀerent kinds information presentation.
 focus paper is methods presenting language learner, called informants.
 Intuitively, natural number x informant language L answers question x P L ﬁnite time.
 precisely, natural number x informant has px, px, range, ﬁrst is interpreted x P L second x R L, respectively.
 Learning Families Formal Languages Informants order grasp formally, have deal ﬁnite inﬁnite sequences pairs px, iq, x P N i P t0,
 f P pNˆt0, let pospfq :“ ty P N | Dx P N pr1pfpxqq “ y ^ pr2pfpxqq “ negpfq :“ ty P N | Dx P N pr1pfpxqq “ y ^ pr2pfpxqq “ denote sets natural numbers, f gives positive negative information, respectively.
 Lange, Zeugmann, Zilles [2008] according B¯arzdi¸nˇs [1977] Blum Blum [1975] A Ď N deﬁne Conspf, Aq :ô pospfq Ď A ^ negpfq Ď NzA say f is consistent f is compatible A case Conspf, Aq is true.
 Deﬁnition
 Let L be language.
 call function N Ñ Nˆt0, such pospIq Y “ N pospIq X negpIq “ ∅ informant.
 denote Inf set informants set informants language L is deﬁned InfpLq :“ tI P Inf | pospIq “ Lu. is immediate, negpIq “ NzL P InfpLq. informant time t P N reveals information t itself, short pr1pIptqq “ t, call canonical informant according Gold [1967] methodical informant.
 learner, be thought scientist trying infer theory machine making guess basis evaluated data, is modelled (partial) computable function M dompMq Ď pNˆt0, Ñ N.
 following introduce fundamental notion learning sequence clarify successful learning means.
 Deﬁnition
 Let M be learner L language.
 Further, let P InfpLq be informant L presented M
 (i) call h “ phtqtPN P Nø, ht :“ MpIrtsq t P N, learning sequence M I.
 h is sequence M ’s hypotheses observing I.
 (ii) P NYt˚u b P Ną0 Yt˚,8u say M learns L b -learns L bpM, Iq, is time t0 P N such anomalies vacillation number b limit, short M Lima I, shorter Lima t ě t0 have Wht “a L |t ht | t ě t0 u| ď b.
 intuition latter is that, sensing I, M vacillates b-many hypotheses, case b “ ˚ stands many diﬀerent hypotheses.
 convenience literature, ommit Martin Aschenbach, Timo K¨otzing, Karen Seidel superscript subscript
 Lim-learning, known explanatory (Ex-)learning, is common deﬁnition successful learning cor- responds notion identiﬁability limit Gold [1967], learner decides correct hypotheses.
 other end hierarchy convergence criteria is correct learning, short Bc- Lim8-learning, requires learner be correct, al- lows many diﬀerent hypotheses limit.
 correct learning was introduced Osherson Weinstein [1982].
 general b -learning P N Y t˚u b P Ną0 Y t˚u was ﬁrst men- deﬁnition Lima tioned Case [1999].
 setting, allow b subsume Lima notion convergence criterion, determine semi- topological sense learning sequence needs have L limit, order succeed learning L.
 following review so-called learning restrictions, i.e., potential properties learning sequence being investigated paper.
 Learning re- strictions incorporate certain desired properties learners’ behavior relative information being presented.
 this, employ notion consistency, stated above.
 Deﬁnition
 Let M be learner P Inf informant.
 denote h “ phtqtPN P Nø learning sequence M I.
 write (i) ConspM, Iq (Angluin [1980]), M is consistent I, i.e., t ConspIrts, Whtq.
 (ii) ConvpM, Iq (Angluin [1980]), M is conservative I, i.e., s, t s ď t ConspIrts, Whsq ñ hs “ ht.
 (iii) DecpM, Iq (Osherson, Stob, Weinstein [1982]), M is decisive I, i.e., r, s, t r ď s ď t Whr “ Wht ñ Whr “ Whs
 (iv) CautpM, Iq (Osherson, Stob, Weinstein [1986]), M is cautious I, i.e., s, t s ď t (cid:32)Wht Ĺ Whs.
 (v) WMonpM, Iq (Jantke [1991],Wiehagen [1991]), M is weakly monotonic I, i.e., s, t s ď t ConspIrts, Whsq ñ Whs Ď Wht
 (vi) MonpM, Iq (Jantke [1991],Wiehagen [1991]), M is monotonic I, i.e.,for s, t s ď t Whs X pospIq Ď Wht X pospIq. Learning Families Formal Languages Informants (vii) SMonpM, Iq (Jantke [1991],Wiehagen [1991]), M is monotonic I, i.e., s, t s ď t Whs Ď Wht.
 (viii) NUpM, Iq (Baliga, Case, Merkle, Stephan, Wiehagen [2008]), M is non-U-shaped I, i.e., r, s, t r ď s ď t Whr “ Wht “ pospIq ñ Whr “ Whs.
 (ix) SNUpM, Iq (Case Moelius [2011]), M is non-U-shaped I, i.e., r, s, t r ď s ď t Whr “ Wht “ pospIq ñ hr “ hs.
 (x) SDecpM, Iq (K¨otzing Palenta [2014]), M is decisive I, i.e., r, s, t r ď s ď t Whr “ Wht ñ hr “ hs.
 following lemma states implications above deﬁned learning restrictions, form foundation research.
 Figure page includes resulting backbone, is diﬀerent learning positive information, WMon does imply NU context learning informants.
 implications are represented black lines bottom top.
 backbone learning positive information consult K¨otzing Palenta [2014].

 Let M be learner P Inf informant.
 (i) ConvpM, Iq implies SNUpM, Iq WMonpM, Iq. (ii) SDecpM, Iq implies DecpM, Iq SNUpM, Iq. (iii) SMonpM, Iq implies CautpM, Iq, DecpM, Iq, MonpM, Iq WMonpM, Iq. (iv) DecpM, Iq SNUpM, Iq imply NUpM, Iq. (v) WMonpM, Iq does imply NUpM, Iq. Proof.
 Verifying claimed implications is straightforward.
 order verify (v), consider L “
 Fix p, q P N such Wp “ Y t1u Wq “ deﬁne learner M σ P Nˆt0, Mpσq “ P negpσq ^ R pospσq; otherwise.
 p, q, order prove WMonpM, Iq P InfpLq, let be informant L sIpxq :“ mintt P N | pr1pIptqq “ xu, i.e., sIp1q sIp2q denote ﬁrst occurance p1, p2, ranpIq, respectively.
 have t P N Y t1u, Wht “ sIp1q ă t ď sIp2q; otherwise.
 Martin Aschenbach, Timo K¨otzing, Karen Seidel have Whs “ WMpIrssq “ Y t1u P negpIrtsq s, t P N sIp1q ă s ď sIp2q t ą sIp2q.
 Therefore, (cid:32)ConspIrts, Whsq negpIrtsq Ę NzWhs.
 obtain WMonpM, Iq s ď t N are such ConspIrts, Whsq, know Whs “ Y t1u hold likewise sIp1q ă t ď sIp2q hence Wht “ Y t1u, yields Whs Ď Wht.
 Furthermore, Whs “ options Wht satisfy Whs Ď Wht.
 Otherwise, case M observes canonical informant L, have Wh0 “ Wh1 “ Wh2 “ Y t1u Wht “ t ą shows (cid:32)NUpM, Iq. Learning Sequences Success Simulated Learners deriving backbone hierarchy delayable learning restrictions learning languages come introduce general properties learning restrictions learning success criteria, allow gen- eral bound setting (explanatory) Lim-learning.
 Deﬁnition
 Let T :“ P ˆ Inf denote whole set possible learners informants.
 denote ∆ :“ t Caut, Cons, Conv, Dec, SDec, WMon, Mon, SMon, NU, SNU, Tu set admissible learning restrictions Γ :“ t Lima b | P N Y t˚u ^ b P Ną0 Y t˚,8uu set convergence criteria.
 Further, β P t nč i“0 δi X γ | P N,@i ď P ∆q γ P Γ u Ď P ˆ Inf say β is learning success criterion.
 Note convergence criterion is learning success criterion letting n δ0 “ T, latter stands restriction.
 liter- ature convergence criteria are called identiﬁcaton criteria denoted ID.
 Let sum introduce notation learning criterion.
 order observe way learning is powerful one, are going compare diﬀerent settings, denoted form rαInf βs rαTxtβs, Inf Txt have indices.
 Clearly, distinguish mode information presentation, namely, learner observes language positive information, text, Txt, informant, Inf
 refer results learning collections recursive functions, text graph respective function is presented learner.
 denote associated learning criteria form rαFnβs, indices Fn are allowed.
 Learning Families Formal Languages Informants Secondly, learning criterion speciﬁes, successful learning means.
 information is provided position β, learning restrictions meet are denoted alphabetic order, followed convergence criterion.
 Last least, position restrict set admissible learners re- quiring example totality.
 properties stated position α are independent learning success.
 Note is conventional require M ’s hypothesis sequence fulﬁll certain learning restrictions, asking success learning process.
 Deﬁnition
 Let be property partial computable functions set pNˆt0, N β learning success criterion.
 denote rαInf set collections languages are β-learnable informants learner M property α.
 case learner needs succeed canonical informants, denote corresponding set collections languages rαInf canβs.
 notations rαTxtβs rαFnβs are deﬁned similarly.
 case learning positive information, have been plenty investigations relation diﬀerent success criteria, e.g., relation rTxtLimbs rTxtNULimbs b P Ną0 Y t˚,8u Carlucci, Case, Jain, Stephan [2008], Baliga, Case, Merkle, Stephan, Wiehagen [2008] Fulk, Jain, Osherson [1994] summed Case [2016].
 Moreover, K¨otzing Palenta [2014] Jain, K¨otzing, Ma, Stephan [2016] give picture, learning restrictions Deﬁnition relate, learning languages texts restricting attention set-driven iterative learners.
 interesting understanding power learning informants are results function learning, as, next lemma, collections functions separating convergence criteria associated setting yield separating collection respective convergence criteria, learning languages informants.
 following make use computable bijection x.
 .y NˆN Ñ N computable inverses π1, π2 N Ñ N such x “ xπ1pxq, π2pxqy x P N.

 f P R let Lf :“ txx, fpxqy | x P Nu denote language encoding graph.
 Let P NYt˚u b P Ną0 Yt˚,8u.
 F Ď R F P rFnLima bs ô LF :“ t Lf | f P F u P rInf Lima bs.
 Proof.
 Let a, b F be stated.
 First, assume is learner M function sequences such F P FnLima bpMq. order deﬁne learner M1 acting informant sequences returning W -indices, employ following procedure obtaining W -code Gppq Lϕp given ϕ-code p: Given input interpreted xx, yy, let program encoded p run x “ π1pnq.
 halts returns y “ π2pxq, halt; loop.
 Martin Aschenbach, Timo K¨otzing, Karen Seidel learner M1 acts σ P pNˆt0, M1pσq :“ GpMpdecodeppospσqqqq, decodeppospσqq denotes σ computable sequence τ τpiq “ pπ1pniq, π2pniqq i ă |pospσq| “ |τ|, pniqiă|pospσq| denotes bpM1q enumeration pospσq according σ.
 construction, LF P Inf Lima G preserves number anomalies.
 other claimed direction let M be learner informant sequences bpMq. employ computable function LF P Inf Lima f P R transforms W -index p Lf ϕ-index Hppq such ϕHppq “ f
 interpret natural number i xxu, vy, ty check ϕp halts xu, vy most t steps computation.
 so, check u is argument want compute fpxq case answer is yes, return v.
 Given input x, i “ till do following: Φppπ1piqq ď π2piq π1pπ1piqq “ x, return π2pπ1piqq; increment i.
 Deﬁne learner M1 σ P pNˆNqăø M1pσq :“ HpMpˆσqq, pxxπ1piq, π2piqy, pxxπ1piq, π2piqy, transform σ “ ppx0, fpx0qq,


 ,px|σ|´1, fpx|σ|´1qqq informant sequence ˆσ length ˆ|σ| :“ maxtj | @i ă j π1piq ă |σ|u letting σpπ1piqq “ pxπ1piq, π2piqq ˆσpiq :“ i ă ˆt.
 Note f P R T P Txtpfq letting IT :“ Trts, obtain informant Lf
 show pxx, fpxqy, P IT x P N leave other details reader.
 Let x P N s minimal, such px, fpxqq P ranpTrssq, i.e., xs´1 “ x.
 Further, let t be such s ď ˆt.
 ITpxs ´ fpxqyq “ y Trtspxs ´ fpxqyq “ pxx, fpxqy,
 tPN Again, claim follows, H preserves number anomalies.
 obtain hierarchy learning restrictions.
 Proposition
 Let b P t1,8u.
 (i) P N holds rInf Lima bs Ĺ rInf Lima`1 bs, bs Ĺ rInf Lim˚ (ii) (iii) rInf Lim˚s Ĺ rInf Lim8s Proof.
 Lemma results transfer corresponding observations function learning B¯arzdi¸nˇs [1974] Case Smith [1983].
 aPNrInf Lima s, Learning Families Formal Languages Informants rInf Lims Ĺ


 Ĺ rInf Limas Ĺ rInf Lima`1s Ĺ


 rInf Limas Ĺ rInf Lim˚s Ĺ rInf Lim8s Ĺ


 Ĺ rInf Lima8s Ĺ rInf Lima`18 s Ĺ


 rInf Lima8s Ĺ rInf Lim˚
 particular, have aPN lemma holds considering TxtLima b -learning languages, construction text sequence informant sequence is folklore.
 next deﬁnition provides properties learning restrictions playing cen- tral role most proofs, applies learning restrictions introduced Deﬁnition
 analog learning text has been introduced K¨otzing Palenta [2014] generalization is studied K¨otzing, Schirneck, Seidel [2017], relations rTxtδLims rTxtδLim8s diﬀerent δ P ∆ are investigated, respectively.
 Deﬁnition
 Denote set unbounded non-decreasing functions S, i.e., S :“ t s N Ñ N | @x P NDt P N sptq ě x @t P N spt ě sptqu.
 s P S is called admissible simulating function.
 predicate β Ď P ˆ Inf is delayable, s P S, I, I1 P Inf partial functions M, M1 P P holds: Whenever have pospI1rtsq Ě pospIrsptqsq, negpI1rtsq Ě negpIrsptqsq M1pI1rtsq “ MpIrsptqsq t P N, βpM, Iq conclude βpM1, I1q.
 name refers tricks order delay mind changes learner were used obtain polynomial computation times learners hypothesis updates discussed Pitt [1989] Case K¨otzing [2009].
 Moreover, be confused notion δ-delay Akama Zeugmann [2008], allows satisfaction considered learning restriction δ steps un-δ-delayed version.
 order give intuition, think β learning restriction learning success criterion imagine M be learner.
 is delayable carries M informant learners M1 informants I1 representing delayed version M I.
 More concretely, learner M1 conjectures hsptq “ MpIrsptqsq time t has, form I1rts, least much data available was used M hypothesis, M1 I1 is considered delayed version M I.
 Note, simulating function’s unboundedness particular guarantees pospIq “ pospI1q negpIq “ negpI1q.
 next result guarantees arguing deﬁned properties covers considered learning restrictions consistency.
 Martin Aschenbach, Timo K¨otzing, Karen Seidel Lemma
 (i) Let δ P ∆.
 is delayable δ ‰ Cons.
 (ii) convergence criterion γ P Γ is delayable.
 intersection many delayable predicates P ˆ Inf is i“0 δi X γ delayable.
 Especially, learning success criterion β “ δi P ∆ztConsu i ď n γ P Γ β is delayable.
 Proof.
 approach piq showing, Cons is delayable.
 do so, con- uqq sider s P S sptq :“ t t I1pxq :“ px, stands characteristic function natural numbers.
 s-m-n are learners M M1 such σ P pNˆt0, I, I1 P Inf deﬁned Ipxq :“ pt x u, x WMpσq “ tx P N | px ^ x ď t WM1pσq “ tx P N | px ^ x ď t uq _ px odd ^ x ą t uq _ px odd ^ x ą t |σ| |σ| |σ| |σ| uqu uqu.
 Further, ConspM, Iq is veriﬁed t P N pospIrtsq “ tx P N | x ^ x ď t t ´ uu Ď WMpIrtsq “ tx P N | x ^ x ď t t ´ uqu Ď NzWMpIrtsq other hand (cid:32)ConspM1, I1q t pospI1rtsq “ tx P N | x ^ x ă tu Ę tx P N | px ^ x ď t t uq _ px odd ^ x ą t t uqu “ WM1pI1rtsq.
 remaining proofs piq piiq are straightforward.
 Basically, Dec, SDec, SMon Caut, simulating function s being non-decreasing M1pI1rtsq “ MpIrsptqsq t P N suﬃce, NU, SNU Mon further needs informants I1 satisfy pospIq “ pospI1q.
 proof WMon Conv be delayable, requires assumptions, s’s unboundedness.
 Last least, order prove convergence criterion γ “ Lima delayed needs characterizing properties s course M1pI1rtsq “ MpIrsptqsq.
 Finally, piiiq is obvious.
 b P N Y t˚u b P Ną0 Y t˚,8u, carries Normal Forms: Canonical Informants Totality facilitate smooth proofs, section discuss normal forms learning informants.
 consider notion set-drivenness, restricts set admissible learners considering order presentation number occurances certain datum.
 Lemma show delayable learning success criteria, collection languages is learnable Learning Families Formal Languages Informants canonical informants is learnable arbitrary informants.
 Moreover, Lemma observe considering total learners does alter learnability collection languages case delayable learning success criterion.
 line, provide regularity property learners, called syntactic decisiveness, Lim-learning Lemma
 Deﬁnition (Wexler Culicover [1980]).
 learner M is set-driven, σ, τ P Nˆt0, p pospσq “ pospτq ^ “ negpτqq ñ Ipσq “ Ipτq.
 Intuitively, M is set-driven, does care order information is presented are repetitions.
 Sch¨afer-Richter [1984] Fulk [1985] showed set-drivenness is restriction learning positive information relation learning restrictions diﬀer observed K¨otzing Palenta [2014].
 next Lemma observe that, contrast, set-drivenness is re- striction setting learning informants.
 Concurrently, generalize Gold [1967]’s stating considering canonical informants determine learning success does give learning power, arbitrary delayable learning success criteria.

 Let be delayable learning success criterion.
 lan- guage collection L is β-learnable learner canonical informants be β-learned set-driven learner arbitrary informants, i.e., rInf canβs “ rSdInf βs Proof.
 Clearly, have rInf canβs Ě rSdInf βs.
 other inclusion, let L be β-learnable learner M canonical informants.
 Let L P L I1 P InfpLq. f P pNˆt0, I1 initial segments, deﬁne sf P S t frts is deﬁned, sfptq “ suptx P N | @w ă x w P pospfrtsq Y i.e., largest natural number x such w ă x know, w P pospfrtsq.
 following f be I1 initial segments, case ensures pospfrtsq Ď L appropriate t.
 construction, sf is non-decreasing consider informant P Inf pospIqYnegpIq “ N, sI is unbounded.
 order employ delayability β, deﬁne operator Σ pNˆt0, Ñ pNˆt0, such f P pNˆt0, form Σpfq obtain sound version f
 Σpfq is deﬁned t ă sfp|f|q case f is ﬁnite t P N Σpfqptq :“ pt, P ranpfq; pt, pt, otherwise.
 Martin Aschenbach, Timo K¨otzing, Karen Seidel Intuitively, Σpfq repetitions sum information contained f largest initial segment N, f interruption informs about.
 ﬁnite sequence σ canonical version Σpσq has length sσp|σ|q.
 consider set-driven learner M1 deﬁned M1pσq “ MpΣpσqq.
 :“ is canonical informant L, have βpM, Iq. Moreover, t P N holds pospIrsI1ptqsq Ď pospI1rtsq negpIrsI1ptqsq Ď negpI1rtsq deﬁnitions sI1 using Σ.
 Finally, M1pI1rtsq “ MpΣpI1rtsqq “ MpΣpI1qrsI1ptqsq “ MpIrsI1ptqsq delayability β yields βpM1, I1q.
 Therefore, considering delayable learning informants, looking canonical informants yields full picture set-driven learners.
 make use reduction other proofs.
 Note construction canonical sound version ΣpIq informant corresponds construction corresponding one-one text T1´1 arbitrary text T employed K¨otzing, Schirneck, Seidel [2017].
 Clearly, similar result be obtained, learning recursive functions graphs being presented canonical arbitrary order.
 next proposition answers arising question, Lemma holds, requiring non-delayable learning restriction consistency, negatively.
 following let K :“ t p P N | ϕpppqÓu denote halting problem.
 Proposition
 collection languages L :“ t2K Y Y txuq | x P Nu is consistently, conservatively, Lim-learnable canonical informants.
 Further, L is Lim- learnable arbitrary informants.
 short have L P rInf canConsConvSDecSMonLimszrInf ConsLims.
 Proof.
 Let p N Ñ N such Wppxq “ Y Y txuq x P N let k be W -index Y
 Consider learner M deﬁned Mpσq “ ppxq, k, x P negpσq P pospσq exists; otherwise.
 σ P pNˆt0,
 Clearly, M conservatively, Lim-learns L informants canonical infor- mants languages L is consistent.
 Learning Families Formal Languages Informants Now, assume is learner M such L P Inf ConsLimpMq. Lemma is locking sequence σ Y
 s-m-n is computable function χpxq “ Mpσq “ Mpσ otherwise.
 (cid:97)p2x consistency M L, obtain χ is characteristic function K, contradiction.
 Note, be indexable family witnessing diﬀerence stated previous proposition, indexable family is con- Lim-learnable enumeration.
 connatural observation rConsFnLims Ĺ rConsFncanLims Jantke Beick [1980], learning collections recursive functions, be helpful reprove result generalization Lemma consistent learning.
 Gold [1967] introduces request informants M L.
 name suggests, is interaction learner informant sense learner decides, natural number informant inform next.
 observation rInf Lims “ rInf canLims “ rInf reqLims seems hold true facing arbitrary delayable learning success criteria, fails context non-delayable learning restriction consistency.
 Moreover, following lemma, most proofs remainder paper are going consider total learners.

 Let be delayable learning success criterion.
 lan- guage collection β-learnable learner informants be β-learned total learner informants, i.e., rInf βs “ rRInf βs.
 Proof.
 Let L P rInf βs M be learner witnessing this.
 loss generality assume ∅ P dompMq. deﬁne total learner M1 letting sM pNˆt0, Ñ N, σ ÞÑ supts P N | s ď |σ| M halts σrss most |σ| stepsu M1pσq :“ MpσrsMpσqsq.
 convention supp∅q yields sM is total is computable, M ﬁrst |σ|-many steps have be evaluated σ’s many initial segments.
 employ Blum complexity measure here.
 Hence, M1 is total computable function.
 order observe, M1 Inf β-learns L, let L P L be informant L.
 letting sptq :“ sMpIrtsq, obtain unbounded non-decreasing Martin Aschenbach, Timo K¨otzing, Karen Seidel function, s P S.
 Moreover, t P N sptq ď t follows pospIrsptqsq Ď pospIrtsq, negpIrsptqsq Ď negpIrtsq M1pIrtsq “ MpIrsMpIrtsqsq “ MpIrsptqsq.
 delayability β I1 “ I, obtain βpM1, Iq. following transfer employed observation Blum Blum [1975] setting learning informants generalize conver- gence criteria introduced Deﬁnition
 Deﬁnition
 Let M be learner, L language P N Y t˚u b P Ną0 Y t˚,8u.
 call σ P pNˆt0, Lima b -locking sequence M L, Conspσ, Lq ˘˘˘ DD Ď N p |D| ď b ^ @τ P pNˆt0, Conspτ, Lq ^ Mpσ WMpσ(cid:97)τq “a L ^ Mpσ τq P D τqÓ (cid:97) (cid:97) Further, locking sequence M L is Lim-locking sequence M L.
 Intuitively, learner M is locked sequence σ language L sense presentation consistent L circumvent M guessing admis- sible approximations L guesses based extension σ are captured ﬁnite set size most b.
 Note deﬁnition implies MpσqÓ, WMpσq “a L Mpσq P D.

 Let M be learner, P NYt˚u, b P Ną0Yt˚,8u L language Lima b -identiﬁed M
 is Lima b -locking sequence M L.
 Proof.
 is straightforward contradictory argument.
 loss gen- erality M is deﬁned ∅.
 Assume σ Conspσ, Lq, MpσqÓ WMpσq “a L ﬁnite D Ď N most b elements exists sequence τ D σ P pNˆt0, σ qÓ ^ τ D σ Lq ^ Mpσ (cid:97) Conspτ D Let IL denote canonical informant L.
 obtain informant L M does Lima b -converge letting (cid:97) σ q R D τ D σ q “a L _ Mpσ (cid:32)WMpσ(cid:97)τ D :“ σ0 :“ ILr1s, (cid:97) τ Dn σn i q | maxt0, ď i ď collect M ’s σn, nPN (cid:97) ILpn σn`1 :“ σ n P N, Dn :“ t Mpσ´ most b-many last relevant hypotheses.
 is informant L having interlaced canonical informant L, learner M Lima b -converges I.
 n0 Ď Irts have Wht “a L.
 Therefore, let be such t σ´ t Mpσ´ i q | n0 ď i ď n0 bu has cardinality b contradiction.
 Learning Families Formal Languages Informants Obviously, appropriate version holds learning text is consid- ered.
 determine relations introduced learning restrictions (explanatory) Lim-learning informants, introduce further beneﬁ- cial property, requiring learner return abandoned hypothesis.
 Deﬁnition (K¨otzing Palenta [2014]).
 Let M be learner, L lan- guage informant L.
 write SynDecpM, Iq, M is decisive I, i.e., @r, s, t pr ď s ď t ^ hr “ htq ñ hr “ hs.
 following easy observation shows variant decisiveness be assumed setting Lim-learning informants.
 is employed proof Proposition

 language collection Lim-learnable informants be Lim-learned informants, i.e., rInf Lims “ rInf SynDecLims Proof.
 rInf SynDecLims Ď rInf Lims, suﬃces show Inf Lim-learnable collection languages is Inf SynDecLim-learnable.
 For, let L P rInf Lims M witnessing this.
 deﬁnition learner M1, make use one-one computable padding function pad NˆN Ñ N such Wp “ dompϕpq “ dompϕpadpp,xqq “ Wpadpp,xq p, x P N.
 Now, consider M1 deﬁned M1pσq :“ padpMpσq,|σ|q, M1pσq, Mpσ´q ‰ Mpσq; M1 behaves M crucial diﬀerence, M performs mind change, M1 guesses same language M did, syn- hypothesis is diﬀerent former ones.
 padding function’s deﬁning property assumption M Inf Lim-learns L yield Inf SynDecLim-learnability L M1.
 Note SDec implies SynDec, is delayable learning restriction afsoep.
 Thus, proof Lemma have restricted attention canonical informants.
 Relations Delayable Learning Restrictions order reveal relations delayable learning restrictions (explanatory) Lim-learning informants, Proposition acquire Martin Aschenbach, Timo K¨otzing, Karen Seidel conservativeness decisiveness do restrict informant learning.
 this, Propositions provide cautious monotonic learning are incomparable, implying learning settings are stronger monotonic learning weaker unrestricted learning.
 overall picture is summarized Figure stated Theorem
 Proposition
 collection languages Lim-learnable methodical informants be Lim-learned total, set-driven learner informants, i.e., rInf Lims “ rRSdInf ConvSDecLims Especially, rInf Lims “ rInf ConvSDecLims Proof.
 rInf Exs Ě rRSdInf ConvSDecExs Lemmas suﬃces show rRInf SynDecExs Ď rInf mConvSDecExs.
 Now, let L P rRInf SynDecExs M learner witnessing this.
 particular, M is total informants languages L have M returns withdrawn hypothesis.
 set X t P N, let Xrts denote methodical informant sequence ﬁrst t elements X.
 want deﬁne learner M1 uses hypotheses hypotheses mimic hypotheses Mpσq, poisoned, i.e., modiﬁed such that, σ is locking sequence, ppσq is codes same language Mpσq.
 However, σ is locking sequence, hypothesis include data want change mind on.
 order do computable way, use following deﬁnitions.
 given σ, D Ď W s Mpσq s, let Dq “ mintz ď s | D Ď W z Mpσqu.
 given σ, D0 s, let‹ Qpσ, D0, sq “ tD Ď W s Mpσq | maxpDq ă minpW s MpσqzDq D0 Ă D Mpσq “ MpW s Mpσqrzspσ, Dqsqu.
 Intuitively, given σ, ppσq enumerate more more elements enumerated Mpσq.
 have enumerated set D0 consider time bound s, Q gives candidate sets use extending D0.
 Note that, σ, D0 s, Qpσ, D0, sq is ordered Ď.
 consider following auxiliary sets σ.
 A0pσq “ pospσq; $’&’%W s @s P N As`1pσq “ Mpσq, maxĎ Qpσ, Aspσq, sq, Aspσq, Mpσq ‰ H; negpσq X W s Qpσ, Aspσq, sq ‰ H; otherwise.
 ‹ suppose minH convenience.
 Learning Families Formal Languages Informants Furthermore, using s-m-n, deﬁne p σ such sPN Wppσq “ Aspσq.
 $’&’%ppσq, Finally, deﬁne new learner M1 such M1pσq “ ppσq, M1pσ´q, otherwise.
 |σ| “ Mppσ´q1q ‰ Mpσq ^ (cid:32)Conspσ, A|σ|ppσ´q1qq; is, M1 follows mind changes M suitably inconsistent hypoth- esis has been seen.
 hypotheses M are poisoned way ensure decide inconsistency.
 Claim Let L be language Inf Ex-learned M
 M1 Inf Ex-learns L.
 Let be minimal such that, t1 ě t, MpLrtsq “ MpLrt1sq.
 Thus, MpLrtsq is correct hypothesis L, denote e.
 Case M1 does make mind change t.
 M1 converged mind change M is t0 ă t minimal with, t1 ě t0, e1 :“ M1pLrt0sq “ M1pLrt1sq.
 deﬁnition M1 get, t1 ě t, ConspLrt1s, At1pLrt0sqq.
 Thus, We1 contains elements L other, i.e., We1 “ L desired.
 Case M1 makes mind change t.
 Let ě be time mind change.
 Clearly, M1 converge ppLrt1sq, denoted e1.
 get We1 Ď L immediately.
 is minimal element x P LzWe1.
 We1 is ﬁnite equal AspLrt1sq next added elements contain x deﬁnition).
 Let “ maxpAspLrt1sq Y txuq let s1 ą s be large such Lry e ry i.e., element L (and including) y are enumerated equals W s1 time s1.
 Let :“ pospLry be time window considered third condition pospLry` P QpLrt1s, AspLrt1sq, s1q, s1 ě s1.
 Let rzs “ Lrzs, i.e., elements s2 ě s1 be large such W s2 time window are enumerated.
 pospLry P QpLrt1s, AspLrt1sq, s2q, M is converged t ă t1 L, contradiction x (which is pospLry` being included We1.
 Claim Let L be language Inf Ex-learned M
 M1 conservative L.
 Let be such M1pLrtsq ‰ M1pLrt
 Let :“ M1pLrtsq let t0 ď t be minimal such M1pLrt0sq “ e1.
 mind change M1 get was mind change M MpLrt0sq ‰ MpLrt
 Suppose, way contradiction, We1 is consistent Lrt i.e. ConspLrt We1q.
 (1) Martin Aschenbach, Timo K¨otzing, Karen Seidel fact M1 made mind change get (cid:32)ConspLrt At`1pLrt0sqq.
 was element x P At`1pLrt0sq is listed Lrt x P At`1pLrt0sq Ď We1, contradicting Equation (2).
 particular, obtain negpLrt1sq X WMpLrt1sq “ H ﬁrst case deﬁnition A.
 Thus, satisfy Equation (3), is x such x P pospLrt
 (4) pospLrt Ď We1 Equation (2), is t1 ą t minimal such pospLrt Ď At1pLrt0sq.
 implies pospLrt P QpLrt At1´1pLrt0sq, t1q.
 Thus, zt1pLrt0s, pospLrt ă t using mind change M see Equa- tion (1).
 get pospLrt P QpLrt AtpLrt0sq, t leading Lrt Ď At`1pLrt0sq, contradiction Equation (4).
 Finally, M1 behaves methodical informant L M1 is consistent mind change makes mind change in- consistent (i.e., M1 is conservative).
 auxiliary sets transfer technique poisoning conjecture introduced K¨otzing Palenta [2014] tailor special setting.
 next propositions show monotonic cautious learning are in- comparable level indexable families.
 ﬁrst proposition learner be assumed cautious languages does identify.
 Thus, according Deﬁnition write success independent property learner left side mode presentation.
 Proposition
 indexable family L :“ t2X Y p2pNzXq | X Ď N ﬁnite X “ Nu is Lim-learnable cautious learner informants.
 Further, L is mono- Lim8-learnable informants.
 short have L P rCautInf LimszrInfMonLim8s.
 Particularly, rInfMonLims Ĺ rInf Lims.
 Proof.
 ﬁrst show L R rInfMonLim8s.
 Let M be Inf Lim8-learner L.
 Further, let I0 be methodical informant L0 :“ P L.
 exists Learning Families Formal Languages Informants t0 such WMpI0r2t0sq “
 Moreover, consider methodical informant I1 L1 :“


 Y p2pNzt0,


 P L let t1 ą t0 such WMpI1r2t1sq “ L1.
 Similarly, let I2 be methodical informant L2 :“


 t1 Y p2pNzt0,


 t1 P L choose t2 ą t1 WMpI2r2t2sq “ L2.
 P pL0 X L2qzL1 construction I2r2t0s “ I0r2t0s I2r2t1s “ I1r2t1s, obtain R WMpI2r2t1sq X L2 P WMpI2r2t0sq X L2 M does learn L2 I2.
 Let adress L P rCautInf Lims.
 Fix p P N such Wp “
 Further, s-m-n is computable function q N Ñ N WqpxXyq “ X Yp2NzXq` xXy stands canonical code ﬁnite set
 deﬁne learner M σ P Nˆt0, Mpσq “ p, qpxpospσq X otherwise.
 pospσq Ď Intuitively, M guesses odd number is known be language L be learned.
 sure L ‰ M assumes numbers known be L are only numbers therein.
 is easy verify M is computable construction learns L.
 establishing cautiousness, let L be language, informant L s ď t.
 Furthermore, assume WMpIrssq ‰ WMpIrtsq.
 case pospIrssq have x P ppospIrtsq X x R ppospIrssq X desired WMpIrtsqzWMpIrssq ‰ ∅.
 again, pospIrssq implies WMpIrssq “ WMpIrtsqzWMpIrssq ‰ ∅.
 Corollary
 exists indexable family Lim-learnable informants, Lim-learnable informants.
 particular, rInf SMonLims Ĺ rInf CautLims.
 following proposition extends overservation Osherson, Stob, We- instein [1986] cautious learning restrict learning power.
 Proposition
 indexable family L :“ tNzX | X Ď N ﬁniteu is Lim-learnable informants.
 Further, L is correct learnable informants.
 short have L P rInfMonLimszrInf CautLim8s.
 Particularly, rInf CautLims Ĺ rInf Lims.
 Martin Aschenbach, Timo K¨otzing, Karen Seidel Proof.
 order approach L R rInf CautLim8s, let M be GInf Lim8- learner L I0 methodical informant N.
 Moreover, let t0 be such WMpI0rt0sq “ N.
 Let I1 be methodical informant L1 :“ Nztt0
 M learns L1, is t1 ą t0 such WMpI1rt1sq “ L1.
 have I1rt0s “ I0rt0s M is learning L1 I1.
 s-m-n is computable function p N Ñ N such ﬁnite sets X holds WppxXyq “ NzX, xXy denotes canocical code X employed proof Proposition
 deﬁne learner M letting σ P Nˆt0, Mpσq “ ppxnegpσqyq.
 corresponding intuition is M includes natural number guess, excluded σ.
 Clearly, M learns L behaves L, D Ď N ﬁnite, informant NzD t P N, have WMpIrtsq Ě NzD WMpIrtsq X NzD “ NzD.
 reproves following result Lange, Zeugmann, Kapur [1996].
 Corollary
 exists indexable family Lim-learnable informants, Lim-learnable informants.
 particular, rInf SMonLims Ĺ rInfMonLims.
 sum preceding results next theorem represent Figure black lines denote backbone given Lemma
 therein claimed proper inclusions were stated Propositions Corollaries thereafter.

 learning restrictions introduced deﬁnition Caut, Mon SMon do restrict informed Lim-learning informants.
 Further, kind learning Caut Mon are incomparable.
 short have (i) @δ P tConv, Dec, SDec, WMon, NU, SNUu rInf δLims “ rInf Lims (ii) rInfMonLims K rInf CautLims Proof.
 ﬁrst part is immediate consequence Proposition is second part Propositions
 Outperforming Learning Texts following relate informant learning prominent concept learning positive information, i.e., texts.
 Learning Families Formal Languages Informants Fig.

 Relations delayable learning restrictions full-information (explana- tory) Lim-learning languages informants.
 Deﬁnition
 Let L be language.
 set texts language L is TxtpLq :“ t T P pN Y t#uqø | cntpTq “ Lu, cntpTq “ ranpTqzt#u is content T
 Thus, text is enumeration language, pause symbol is interpreted new information necessary empty language.
 case L is presented learner M form text, M know sure natural number x is L.
 learner, was thought child language acquisition, is setting modelled (partial) computable function M dompMq Ď Năø Ñ N.
 Gold [1967] observed rTxtLims Ĺ rInf Lims lateron Lange Zeugmann [1993] investigated interdependencies considering diﬀerent monotonicity learning restrictions.
 instance, showed exists indexed family L P rInfMonLimszrTxtLims ‰ ∅ contrast indexed families Inf SMonLim-learnability implies TxtLim- learnability.
 show inclusion fails level families recursive languages.
 InfLimSdInfLimTNUDecSMonMonWMonCautSDecSNUConv Martin Aschenbach, Timo K¨otzing, Karen Seidel Proposition
 class recursive languages L :“ t2pL Y txuq Y | L is recursive ^ WminpLq “ L ^ x ě minpLqu is Lim-learnable informants.
 Moreover, L is Lim-learnable texts, i.e., L P rInf SMonLimszrTxtLims.
 Proof.
 Let pm denote index Y pm,x index
 learner M look minimum possible L-generating language presented enumerable set moreover try detect exception x, case exists.
 Thus, checks m such P pospσq P pospσq k ă m holds P negpσq P negpσq.
 case m has property relative σ, write σL minpmq.
 Further, M tries ﬁnd x such P pospσq P negpσq excpxq x is wished.
 Consider learner M deﬁned abbreviate σL $’&’%indp∅q, pm, pm,x, Mpσq “ is m σL σL σL minpmq is x σL minpmq σL excpxq.
 minpmq; excpxq; σ P pNˆt0,
 Clearly, M Lim-learns L.
 observe L R rTxtLims, assume exists M such L P TxtLimpMq. s-m-n exists e P N such i P N Aσpiq “ t k P N | Mpσq ‰ Mpσ Bσpiq “ t k P N | Mpσq ‰ Mpσ (cid:97)p2e (cid:97)p2e σ0 “ p2e, σi, (cid:97) i p2e (cid:97) i p2e tn | P ranpσiqu.
 Aσipiq “ Bσipiq “ ∅ i ą ^ σi´1 “ σi; Aσipiq ‰ ∅ ^ infpAσipiqq ď infpBσipiqqq; Bσipiq ‰ ∅ ^ infpBσipiqq ă infpAσipiqq; $’’’’’’’’&’’’’’’’’% iPN σi`1 “ “ is recursive, is ﬁnite decide construc- tion σi.
 P L.
 index i holds σi`1 “ σi, M fails learn
 other hand, is such i, letting T :“ iPN σi obtain text Y M performs many mindchanges.
 Note learner witnessing SMonLim-learnability L is conser- vative decisive, was stated Theorem
 Learning Families Formal Languages Informants Duality Vacillatory Hierarchy having investigated relations diﬀerent delayable learning restrictions setting Lim-learning informants relation learning texts, link other convergence criteria.
 Proposition observed hierarchy, varying number anomalies show allowing learner vacillate many correct hypothesis limit does give learning power.
 contrary, requiring semantic convergence, i.e., allowing many correct hypotheses limit, does allow learn more collections languages arbitrary semantic learning restriction hand.
 indexable family recursive languages is Lim-learnable infor- mants enumeration, vacillatory hierarchy collapses such collections bs languages, i.e., indexable families L have L P rInf Lima P N Y t˚u b P Ną0 Y t˚,8u.
 contrast, strengthen Proposition (iii) separating Inf Lim- Inf Lim8-learning level families recursive languages, requiring Lim8-learning sequence meet introduced delayable semantic learning restrictions.
 Proposition
 collection recursive languages L :“ tL Y txu | L Ď N is recursive ^ WminpLq “ L ^ x ě minpLqu is Lim8-learnable informants.
 Moreover, L is Lim-learnable informants, i.e., L P rInf SMonLim8szrInf Lims.
 Especially, δ P tCaut, Dec, Mon, SMon, WMon, NUu holds L P rInf δLim8szrInf Lims.
 Proof.
 Lemmas suﬃces show L P rInf canSMonLim8szrInf canLims.
 s-m-n are p Nˆt0, ˆ N Ñ N learner M such σ P Nˆt0, x P N Wppσ,xq “ Wminppospσqq Y txu $’&’%indp∅q, minppospσqq, ppσ, xq, Mpσq “ pospσq “ ∅; pospσqzW x “ minppospσqzW |σ| minppospσqq “ ∅; |σ| minppospσqqq; Martin Aschenbach, Timo K¨otzing, Karen Seidel indp∅q refers canonical index empty set.
 L Inf canSMonLim8pMq, let L Y txu P L L Ď N recursive, WminpLq “ L x ě minpLq let be methodical informant LYtxu.
 t ą minpLq have WminppospIrtsqq “ WminpLq “ L.
 Further, let be minimal such ty P L | y ă xu Ď W m minpLq. x ě minpLq construction yields t P N $’&’%∅, Wht “ L, L Y txu, otherwise.
 t ď minpLq; minpLq ď t ă maxtx mu; be veriﬁed, case y P L have L “ L Y tyu shows Inf canSMonLim8-learnability L M
 order approach L R rInf canLims, assume contrary is learner M Inf canLim-learns L.
 are going deﬁne recursive language L WminpLq “ L helpful showing L is Inf canLim-learned M
 order do so, methodical σ P Nˆt0, deﬁne sets σ stand methodical informant pospσq, whereas σ, A1 A0 σ collect t ą |σ| σ denotes methodical informant pospσqYt|σ|u.
 A0 M ’s hypothesis σrts makes guess diﬀerent Mpσq.
 capture t ą |σ| such M reads follows σrts is diﬀerent Mpσq.
 Similarly, A1 σ Ď N.
 let σ :“ t t P N | t ą |σ| ^ MpI A0 σ :“ t t P N | t ą |σ| ^ MpI A1 σrtsq ‰ Mpσqu, σrtsq ‰ Mpσqu.
 Note t ą |σ| σrts “ σ σrts “ σ (cid:97)pp|σ|,


 ,pt ´ (cid:97)pp|σ|,


 ,pt ´
 s-m-n exists p P N such (we use convention infpHq “ @i P N σi`1 “ σ0 “ pp0,


 ,pp ´ σi “ A1 A0 infpA0 σiqs, σiqs, σirminpA0 σirminpA1 pospσiq.
 Wp “ $’&’%σi, iPN σi “ H; σiq ď infpA1 σiq; Learning Families Formal Languages Informants construction p “ minpWpq Wp is recursive, yields L :“ Wp P L.
 Further, i P N σi ‰ σi`1 follows Mpσiq ‰ Mpσi`1q.
 Aiming contradiction, let be methodical informant L, implies iPN σi Ď I.
 M explanatory learns L does make many mind changes I, exists i0 P N such i ě i0 have σi “ σi0
 t ą |σi0| holds MpI σi0 rtsq “ Mpσi0q “ MpI σi0 rtsq, M does learn least L “ pospσi0q L Y t|σi0|u methodical informants.
 other hand lie L therefore, M had existed beginning.
 allowing many diﬀerent correct hypotheses limit gives learning question many hypotheses al- ready allow learn more collections languages.
 following proposition shows observed B¯arzdi¸nˇs Podnieks [1973] Case Smith [1983] function learning hierarchy vacillatory learning collapses learning languages informants.
 Note contrasts results lan- guage learning texts Case [1999], observing P N Y t˚u hierarchy rTxtLimas Ĺ


 Ĺ rTxtLima rTxtLima bs Ĺ rTxtLima bs Ĺ rTxtLima˚s Ď rTxtLima8s.
 b`1s Ĺ


 bPNą0 Proposition
 Let P N Y t˚u.
 rInf Limas “ rInf Lima˚s.
 Proof.
 Clearly, rInf Limas Ď rInf Lima˚s.
 other inclusion let L be rInf Lima˚s M learner witnessing this.
 Lemma assume M is total.
 construction Lima-learner M1, employ recursive function Ξ pNˆt0, ˆ N Ñ N, given σ P pNˆt0, p P N |σ| Ξpσ,pq X “ ∅ moreover, σ Ď τ are such alters p such W |τ| |σ| p X negpτq, Ξpσ, pq “ Ξpτ, pq.
 way do is p X negpσq “ W letting Ξpσ, pq denote unique program, given x checks, x “ yi, pyiqiă|negpσq| is increasing enumeration negpσq.
 answer is positive, program goes loop.
 executes program encoded p x, yields ϕΞpσ,pqpxq “ x P negpσq; Ò, ϕppxq, otherwise.
 Now, M1 works follows: I.
 Compute pi :“ Mpσrisq i ď |σ|.
 II.
 Withdraw pi property |negpσq X W |σ| pi | ą a.
 Martin Aschenbach, Timo K¨otzing, Karen Seidel III.
 Deﬁne M1pσq be code program coresponding union vote Ξpσ, piq, pi was withdrawn previous step: Given input x, n till do following: i :“ π1pnq ď |σ|, |σ| |negpσqXW pi | ď ΦΞpσ,piqpxq ď π2pnq, return increment n.
 guarantees ϕM1pσqpxq Ò, D i ď |σ| p|negpσq X W otherwise.
 |σ| pi | ď ^ ϕΞpσ,piqpxqÓq; Intuitively, M1pσq eliminates commission errors guesses M initial segments σ, violating allowed number anomalies, asks converges input, implies WM1pσq “ iď|σ|,|negpσqXW |σ| pi |ďa WΞpσ,Mpσrisqq.
 order show L P Inf LimapMq, let L P L P InfpLq. L P Lima˚pMq, is t0 such M ’s hypotheses are ths | s ď t0u | W t0 hs X NzL| ą s ď t0 | Whs X NzL| ą a.
 assume s ď t0 | Whs X NzL| ď have observed commission errors most t0 steps, reads Whs X NzL “ W t0 t ě t0 obtain same set indices hs X NzL.
 :“ t ΞpIrts, piq | i ď t ^ |negpIrtsq X W t pi| ď au X NzL “ ∅.
 Further, ϕh1 M1 return same hypothesis, namely, h1 “a L.
 construction choice t0 remains argue Wh1 pxq exists are commission i.e., Wh1 case is least p P A such ϕppxq are most t0 arguments, ϕh1 t0 Similar results language learning texts b P t1,8u Case Lynes [1982], gain strict hierarchy bounding number occuring correct hypotheses target ﬁxed number P N Y t˚u observed Proposition
 is undeﬁned.
 t0 t0 Conclusion Future Work paper investigates learning formal languages limit informants, i.e., inferring growing sample set positive negative data more (almost) correct enumeration procedure(s) target.
 Whereas Section provides necessary deﬁnitions back-bone delayable learning restrictions setting, Section establishes normal form being to- tal that, delayable learning success criterion, Learning Families Formal Languages Informants need consider learning canonical informants.
 Thereafter, Section complete picture (set-driven) Lim-learning informants de- layable learning restriction, depicted diagram Figure is derived.
 is valid case indexable families are considered.
 Important examples are levels Chomsky hierarchy type-0, i.e. enumerable, formal languages.
 proofs combine diﬀerent techniques em- ploy connections provided beforehand.
 contrast observed hierarchies learning success allows deducing approximations target language diﬀerent quality Section Section provides requiring learner output correct enumeration procedure is powerful allowing ﬁnite number correct descriptions limit.
 show that, learning informants, facing semantic learning restrictions hand, gain more learning power, case learning is considered successful inﬁnite number correct descriptions limit.
 Inf Ex SdInf Ex NU SNU WMon Dec SDec Conv Mon Caut SMon Fig.

 Diagram depicting relations delayable learning restrictions (ex- planatory) Lim-learning languages informants.
 Further research investigate relationships diﬀerent de- layable learning restrictions other convergence criteria, general results Section be helpful.
 Further, requiring learner be total restricts consistent learning informants, seems appropri- ate indicator concerning conjecture delayability being right structural property gain deeper insights connections diﬀerences available deﬁnitions learning success.
 end, results Akama Zeugmann [2008] Case K¨otzing [2008] be helpful.
 open question regards relation learning recursive functions texts graphs learning languages informants texts.
 seems delayability plays crucial role order obtain normal forms investigate learning restrictions relate setting.
 is clear, delayability is right assumption generalize Lemma
 Consult survey Zeugmann Zilles [2008] standard textbook Martin Aschenbach, Timo K¨otzing, Karen Seidel Jain, Osherson, Royer, Sharma [1999] more results setting function learning transfer learning collections languages informants generalization.
 According Osherson, Stob, Weinstein [1986] requiring learner base conjecture previous current datum, makes Lim- learning harder.
 relations delayable learning restrictions called iterative learners presentation mode positive information has been investigated Jain, K¨otzing, Ma, Stephan [2016], has been done learning informants.
 indexable families, was interest Lange Grieser [2003] oﬀer surprising observations.
 automatic structures alternative approach model learner, have been investigations diﬀerent types text aﬀect explanatory learnability, see Jain, Luo, Stephan [2010] H¨olzl, Jain, Schlicht, Seidel, Stephan [2017].
 latter started investigating learning canonical informants learning text relate automatic setting.
 natural question seems be eﬀect other kinds informants learning success criteria have.
 Last least, rating model’s value other research aiming under- standing capability human machine learning seems challeng- ing task tackle.
 work was supported German Research Foundation (DFG) Grant KO (SCL).
 Bibliography Y.
 Akama T.
 Zeugmann.
 Consistent coherent learning δ-delay.
 Information Computation,
 D.
 Angluin.
 Inductive inference formal languages positive data.
 Infor- mation control,
 G.
 Baliga, J.
 Case, W.
 Merkle, F.
 Stephan, W.
 Wiehagen.
 unlearning helps.
 Information Computation,
 J.
 B¯arzdi¸nˇs.
 theorems limiting synthesis functions.
 Theory Algorithms Programs, Latvian State University, Riga,
 J.
 B¯arzdi¸nˇs.
 Inductive inference automata, functions programs.
 Amer.
 Math.
 Soc.
 Transl., pages
 J.
 B¯arzdi¸nˇs K.
 Podnieks.
 theory inductive inference.
 Mathematical Foundations Computer Science,
 L.
 Blum M.
 Blum.
 mathematical theory inductive inference.
 Information Control,
 L.
 Carlucci, J.
 Case, S.
 Jain, F.
 Stephan.
 Non-U-shaped vacillatory team learning.
 Journal Computer System Sciences,
 J.
 Case.
 power vacillation language learning.
 SIAM Journal Com- puting,
 J.
 Case.
 Gold-style learning theory.
 Topics Grammatical Inference, pages

 J.
 Case T.
 K¨otzing.
 delayed postdictive completeness consistency learning.
 Proc.
 ALT (Algorithmic Learning Theory), pages
 J.
 Case T.
 K¨otzing.
 Diﬃculties forcing fairness polynomial time in- ductive inference.
 Proc.
 Algorithmic Learning Theory, pages
 J.
 Case C.
 Lynes.
 Machine inductive inference language identiﬁcation.
 Proc.
 ICALP (International Colloquium Automata, Languages Programming), pages
 J.
 Case S.
 Moelius.
 Optimal language learning positive data.
 Infor- mation Computation,
 J.
 Case C.
 Smith.
 Comparison identiﬁcation criteria machine inductive inference.
 Theoretical Computer Science,
 M.
 Fulk.
 Study Inductive Inference Machines.
 PhD thesis, SUNY Buﬀalo,
 M.
 Fulk, S.
 Jain, D.
 Osherson.
 Open problems Systems Learn.
 Journal Computer System Sciences, December
 E.
 Gold.
 Language identiﬁcation limit.
 Information Control,
 R.
 H¨olzl, S.
 Jain, P.
 Schlicht, K.
 Seidel, F.
 Stephan.
 Automatic learning repetitive texts.
 Proc.
 Algorithmic Learning Theory, pages
 Martin Aschenbach, Timo K¨otzing, Karen Seidel S.
 Jain, D.
 Osherson, J.
 Royer, A.
 Sharma.
 Systems Learn: Intro- duction Learning Theory.
 MIT Press, Cambridge, Massachusetts, second edition,
 S.
 Jain, Q.
 Luo, F.
 Stephan.
 Learnability automatic classes.
 LATA, pages
 S.
 Jain, T.
 K¨otzing, J.
 Ma, F.
 Stephan.
 role update constraints text-types iterative learning.
 Information Computation,
 K.
 Jantke H.
 Beick.
 Combining postulates naturalness inductive infer- ence.
 Humboldt-Universit¨at zu Berlin.
 Sektion Mathematik,
 K.
 P.
 Jantke.
 Monotonic nonmonotonic inductive inference functions patterns.
 Nonmonotonic Inductive Logic, International Workshop, Proc., pages
 T.
 K¨otzing.
 Abstraction Complexity Computational Learning Limit.
 PhD thesis, University Delaware,
 T.
 K¨otzing R.
 Palenta.
 map update constraints inductive inference.
 Algorithmic Learning Theory, pages
 T.
 K¨otzing, M.
 Schirneck, K.
 Seidel.
 Normal forms semantic lan- guage identiﬁcation.
 Proc.
 Algorithmic Learning Theory, pages

 S.
 Lange G.
 Grieser.
 Variants iterative learning.
 Theoretical computer science,
 S.
 Lange T.
 Zeugmann.
 Monotonic versus non-monotonic language learning.
 Proc.
 Nonmonotonic Inductive Logic, pages
 S.
 Lange T.
 Zeugmann.
 Characterization language learning in- formant various monotonicity constraints.
 Journal Experimental Theoretical Artiﬁcial Intelligence,
 S.
 Lange, T.
 Zeugmann, S.
 Kapur.
 Monotonic dual monotonic language learning.
 Theoretical Computer Science,
 S.
 Lange, T.
 Zeugmann, S.
 Zilles.
 Learning indexed families recursive languages positive data: survey.
 Theoretical Computer Science, (1):194–232,
 P.
 Odifreddi.
 Classical Recursion Theory, volume II.
 Elsivier, Amsterdam,
 D.
 Osherson S.
 Weinstein.
 Criteria language learning.
 Information Control,
 D.
 Osherson, M.
 Stob, S.
 Weinstein.
 Learning strategies.
 Information Control,
 D.
 Osherson, M.
 Stob, S.
 Weinstein.
 Systems Learn: Introduc- tion Learning Theory Cognitive Computer Scientists.
 MIT Press, Cambridge, Mass.,
 L.
 Pitt.
 Inductive inference, DFAs, computational complexity.
 Proc.
 AII (Analogical Inductive Inference), pages
 H.
 Rogers.
 Theory Recursive Functions Eﬀective Computability.
 McGraw Hill, New York,
 Reprinted, MIT Press,
 J.
 Royer J.
 Case.
 Subrecursive Programming Systems: Complexity Suc- cinctness.
 Research monograph Progress Theoretical Computer Science.
 Birkh¨auser Boston,
 Learning Families Formal Languages Informants G.
 Sch¨afer-Richter.
 ¨Uber Eingabeabh¨angigkeit Komplexit¨at von Inferenzs- trategien,
 Dissertation, RWTH Aachen.
 K.
 Wexler P.
 Culicover.
 Formal Principles Language Acquisition.
 MIT Press, Cambridge, Massachusetts,
 R.
 Wiehagen.
 thesis inductive inference.
 Nonmonotonic Inductive Logic, International Workshop, Proc., pages
 T.
 Zeugmann S.
 Zilles.
 Learning recursive functions: survey.
 Theoretical Computer Science,
 system security analysis Guided Dropout Benjamin Donnot‡ †∗, Isabelle Guyon‡•, Marc Schoenauer‡, Antoine Marot†, Patrick Panciatici† ‡ UPSud Inria TAU, Université Paris-Saclay, France.
 • ChaLearn, Berkeley, California.
 † RTE France.
 Abstract.
 propose new method compute load-ﬂows (the steady-state power-grid given productions, consumptions grid topology), substituting conventional simulators based diﬀer- ential equation solvers.
 use deep feed-forward neural network trained load-ﬂows precomputed simulation.
 architecture permits train network so-called “n-1” problems, load ﬂows are evalu- ated possible line disconnection, generalize “n-2” problems re-training (a clear advantage combinatorial nature problem).
 end, developed technique bearing similarity “dropout”, named “guided dropout”.
 Background motivations Electricity is commodity consumers take granted and, govern- ments relaying public opinion (rightfully) request renewable energies be used increasingly, little is known entails scenes additional complexity Transmission Service Operators (TSOs) operate power transmission grid security.
 Indeed, renewable energies such wind solar power are predictable conventional power sources thermal power plants).
 power grid is considered be operated “security” (i.e. secure state) is zone “constraints”, includes power ﬂowing line does exceed given limits.
 end, is standard practice operate grid real time so-called “n-1” criterion: is preventive measure requiring times grid remain safe state component (generators, lines, transformers, etc.) were disconnected.
 Today, complex task dispatchers, are trained engineers, consists analyzing situations checking eﬀect using sophisticated (but slow) high-end simulators.
 part larger project assist TSOs daily operations [3], goal paper is emulate power grid neural network provide fast estimations power ﬂows lines given “injections” (electricity productions consumptions).
 guided dropout method Due combinatorial nature changes power grid topology, is imprac- tical (and slow) train neural network topology.
 idea is ∗Benjamin Donnot corresponding authors: benjamin.donnot@inria.fr train single network architecture variants capture elementary grid topology variants (occurring willful accidental line disconnections) reference topology lines are service.
 train simultane- samples obtained reference topology elementary topology changes, limited line disconnection (“n-1” cases), are encoded neural network activating “conditional” hidden units.
 Regular generalization is evaluated testing neural network additional {injections, power ﬂows} input/output pairs “n-1” cases.
 evaluate super-generalization “n-2” cases, pair lines is disconnected, activating simultane- corresponding conditional hidden units network was trained such cases).
 work was inspired “dropout” [6] relates other eﬀorts literature learn “sparsify”[2] increase network capacity increasing computational time (used automatic translation [5]) “mixed models” (used e.g. person identiﬁcation [7] source identiﬁcation believe idea encode topological changes grid network architecture is novel is type application address.
 Baseline methods compared performance proposed Guided Dropout (GD) method multiple baselines (Figure Model: neural network is trained grid topology.
 Var (OV): One single input variable encodes line is disconnected (0 line disconnected, line is disconnected, line etc.) Hot (OH): is number lines power grid, n extra binary input variables are added, coding connection/disconnection.
 DC approximation: A standard baseline power systems.
 is approximation AC (Alternative Current) non-linear powerﬂow equations.
 Model is brute force approach does scale size power grid.
 power grid n lines require training n(n−1)/2 neural networks implement “n-2” cases.
 Variable is simplest encoding allowing train network “n-1” cases.
 does allow generalize “n-2” cases.
 Hot is reference architecture, allows generalize “n-2” cases.
 DC approximation power ﬂows neglects reactive power permits compute approximation power ﬂows using matrix inversion, given detailed physical model grid.
 See supplemental material details neural network architectures DC calculations.
 conduct fair comparison DC baseline, use input variables active power injections.
 perform better were using additional variables such voltages used AC power ﬂow, considering constant DC approximation1.
 supplemental material be available https://hal.archives-ouvertes.fr/hal- github repository FPSSA-GuidedDropout.
 Fig.
 Neural network architectures being compared.
 overlay types architectures consideration.
 Lines denoting trainable parameters are present models, indicated legend.
 Biases repre- sented.
 injection inputs are split submodules productions consumptions.
 Experiments goal section is demonstrate multiple grid topolo- gies be modeled single neural network purpose provid- ing fast current ﬂow predictions (measured Amps), given injections given topologies, anticipate lines exceed ther- mal limit contingency occur.
 show phenomenon call super-generalization: “guided dropout” topology encoding, neural network, trained “n-1” topology cases only, generalizes “n-2” cases.
 demonstrate approach be viable grid large French Extra High Voltage power grid.
 ﬁrst conducted systematic experiments small size benchmark grids Matpower [8], library used test power system algorithms [1].
 report results largest case studied: 118-node grid n lines.
 used variants grid topologies disconnected line (“n-1” dataset) randomly sampled cases pairs disconnected lines (“n-2” dataset),
 Training test data were obtained generating topology considered input vectors (including active reactive injections).
 generate semi-realistic data, used knowledge French gri, mimic spatio-temporal behavior real data [3].
 example, enforced spatial correlations productions consumptions mimicked production ﬂuctuations, are disconnected main- tenance economical reasons.
 Target values were obtained computing (a) Regular generalization.
 Super-generalization.
 Fig.
 Mini grid buses (nodes).
 show L2 error Amperes log scale function training epochs.
 neural network cases is trained “n-1” cases multiple examples injections.
 (a) Regular generalization.
 Test set made (all) test injections “n-1” cases.
 Super-generalization.
 Test set made subset test injections “n-2” cases.
 Error bars are 25-75% quantiles runs having converged.
 resulting ﬂows lines AC power ﬂow simulator Hades2,2.
 resulted “n-1” dataset samples (we include “n-1” dataset samples reference topology) “n-2” dataset samples.
 used “n-1” dataset training, hyper-parameter selection, testing.
 “n-2” data were used testing.
 experiments, input output variables were standardized.
 opti- mized “L2 error” (mean-square error) using Adam optimizer Tensor- ﬂow.
 Figure shows generalization super-generalization learning curves various methods.
 reasons given super-generalization be achieved Hot Guided Dropout, explains Figure has curves.
 DC approximation is represented horizontal dashed line (since does involve training).
 test error is represented log scale.
 be seen Figure 2-a neural networks are pow- erful making load ﬂow predictions “One Model” approach (yellow curve) outperforms DC approximation order magnitude.3 “One Model” approach is impractical larger grid sizes.
 “One Hot” approach is worse “Guided Dropout” “DC approx- imation”.
 neural network approaches, “Guided Dropout” gives best results, beats DC approximation “n-2” cases (super-generalization).
 super-generalization capabilities neural networks trained “Guided Dropout” are obtained combining single network “shared” units trained freeware version Hades2 is available http://www.rte.itesla-pst.org/ note yellow curve slight over-ﬁtting evidenced test error increase training epochs, be alleviated early stopping.
 error (log10.
 scale)L2 error n-1 dataset (training n-1)DC approxOne ModelOne VarOne hot*G.
 Dropout050100150200250300Epoch1.00.80.60.40.20.0L2 error (log10.
 scale)L2 error n-2 dataset (training n-1)DC approx.One hot*G.
 available data many similar (yet diﬀerent) grid topologies spe- cialized units activated speciﬁc topologies.
 economy resources, similar spirit weight sharing convolutional neural networks, performs kind regularization.
 Obtaining good performance new “unseen” grid topologies (not available training) is biggest practical advantage “Guided Dropout”: Acquiring data train model “n-2” cases Extra High Voltage French power grid (counting nodes, (cid:39) lines) require computing power ﬂow simulations, take year, given computing full AC power ﬂow simulation takes ms RTE current production software.
 Conversely, RTE stores “n-1” cases part “security analyses” conducted minutes, “n-1” dataset is available.
 check method scales size real power transmis- sion grid, conducted preliminary experiments using real data French Extra High Voltage power grid nodes.
 end, extracted grid state data September October
 represents: grid states, lines, generations units individual loads (accounting dynamic node splitting, represented − power nodes period time).
 did simulate line disconnections preliminary experiments do apply method).
 is evaluation computational performance run time.
 trained network following dimensions (its architecture remains be optimized): units (ﬁrst) encoder layer, conditional units second (guided dropout) layer, units last (decoder) layer.
 Training takes order day.
 architecture, data are loaded computer RAM memory, are able perform more load-ﬂows second, enables compute more load-ﬂows same amount time current AC power ﬂow simulators.
 Conclusions future work comparison various approaches approximate “load ﬂows" using neural networks has revealed superiority “Guided Dropout”.
 novel method introduced allows train single neural network predict power ﬂows variants grid topology.
 Speciﬁcally, trained variants single disconnected line (“n-1” scenarios), network generalizes variants TWO disconnected lines (“n-2” scenarios).
 Given combinatorial nature problem, presents signiﬁcant computational advantages.
 target application is pre-ﬁlter serious grid contingencies such com- binations line disconnections lead equipment damage service discontinuity.
 demonstrated standard benchmarks AC power ﬂows method compares several reference baseline methods including DC approximation, terms predictive accuracy computational time.
 daily “n-1” situations are examined RTE computational cost AC simulations.
 “Guided Dropout” allow pre-ﬁlter alarming “n-2” situations, investigate AC simulation.
 Preliminary computational scaling simu- lations performed Extra High Voltage French grid indicate viability such hybrid approach: neural network be (cid:39) times deployed AC power ﬂow simulator.
 Given new method is data – knowledge used physics system (e.g. reactance, resistance admittance lines) topology grid –, empirical results presented paper are encouraging, DC approximation DOES make use such knowledge.
 prompted RTE management commit additional eﬀorts pursue line research.
 Further work include incorporating such prior knowledge.
 ﬂip side using data driven method (compared DC approxima- tion) is need massive amounts training data, representative actual scenarios situations, changing environment, loss explain- ability model.
 ﬁrst problem be addressed ﬁne tuning/adapting model.
 overcome second one, are working theoretical foundations method.
 public version code release Github is preparation.
 References [1] O Alsac B Stott.
 Optimal load ﬂow steady-state security.
 IEEE transactions power apparatus systems, (3):745–751,
 [2] Y.
 Bengio et al.
 Estimating propagating gradients stochastic neurons conditional computation.

 [3] B.
 Donnot et al.
 Introducing machine learning power system opera- tion support.
 IREP Symposium, Espinho, Portugal, August
 [4] S.
 Ewert M.
 B.
 Sandler.
 Structured dropout weak label multi- instance learning application score-informed source separation.
 Proc.
 ICASSP, pages
 [5] N.
 Shazeer et al.
 large neural sparsely-gated mixture-of-experts layer.

 [6] N.
 Srivastava et al.
 Dropout: simple way prevent neural networks overﬁtting.
 JMLR,
 [7] T.
 Xiao et al.
 Learning deep feature representations domain guided dropout person re-identiﬁcation.
 Proc.
 CVPR, pages
 [8] R.
 D.
 Zimmerman et al.
 Matpower.
 IEEE Trans.
 Power Systems, pages
 deep learning penetrates more application is natural demand adapt deep learning techniques area task-speciﬁc requirements constraints.
 immediate consequence is expectation perform respect task-speciﬁc performance measures.
 be challenging, performance measures be complex structure be motivated legacy, algorithmic convenience.
 Examples include F-measure is popular retrieval tasks, various ranking performance measures such area-under-the-ROC-curve, Kullback-Leibler divergence is popular class-ratio estimation problems.
 Optimizing performance measures application areas has proved be challenging learning linear models, is evidenced recent surge progress optimizing “non-decomposable” loss functions learning linear models, review Section
 challenge becomes hard ∗amartya18x@gmail.com †kpawan@cse.iitk.ac.in ‡purushot@cse.iitk.ac.in §schawla@qf.org.qa ¶fsebastiani@gmail.com trying do training neural network architectures such multi-layer perceptrons convolutional recurrent neural networks.
 vast majority training techniques used neural network present consist using simple per-sample loss functions such least-squares loss cross-entropy.
 use has allowed research directions focus developing more evolved network architectures, developing optimized implementations training routines GPU show is suboptimal sound eﬀort towards training task-speciﬁc loss functions pays handsomely.
 Contributions work advances state-of-the-art training neural networks wide variety non-decomposable performance measures.

 show train neural networks respect performance measures are concave, pseudolinear, nested concave functions.

 algorithms are adapted neural architectures such multi-layered perceptrons recurrent networks, be integrated popular symbolic gradient frameworks such Theano, TensorFlow, PyTorch.

 methods superior performance traditional cross-entropy based training routines – F-measure maximization task benchmark dataset method achieves F-measure less mini-batch iterations takes traditional cross-entropy based training more iterations reach similar performance levels.

 methods outperform proposed techniques training deep networks ranking performance [17].
 benchmark dataset IJCNN, technique Song al.
 is able oﬀer min-TPR/TNR performance whereas technique is able reach performance few iterations.

 apply techniques end-to-end sentimental analysis quantiﬁcation network achieve perfect quantiﬁcation scores challenge dataset using less number training iterations.

 oﬀer formal stabilization guarantees algorithms.
 Related Work recent years have seen much interest, progress, training task-speciﬁc per- formance measures ﬁeld classiﬁcation ranking.
 notable works include investigate statistical properties plug-in classiﬁers various non-decomposable objectives in- cluding F-measure, [7, propose stochastic gradient-style algorithms optimizing non-decomposable performance measures such F-measure, KL-divergence, area ROC curve (AUC), precision recall curve (AUCPR), recall ﬁxed precision (R@P), etc.
 However, works cited focus training linear models.
 allows simple algorithms works provide detailed analyses theoretical guarantees, approaches do extend deep networks.
 Algorithms deep learning optimize non-decomposable performance measures are unexplored.
 be attributed de-facto use backprop- agation algorithm training neural networks depends loss function being decom- posable.
 are aware signiﬁcant eﬀorts training deep networks non-decomposable perfor- mance measures.
 discuss put contributions perspective.

 Song et.
 al.
 [17] introduce algorithm training neural networks ranking tasks average precision performance measure.
 key contribution [17] is result shows behaved non-decomposable loss functions, expected gradient loss function respect network weights be expressed terms standard decomposable loss functions such cross-entropy least squares loss.

 Eban et.
 al.
 [3] introduce algorithms optimizing ranking objectives e.g. area precision- recall curve precision ﬁxed recall rate.
 works are focussed ranking measures work addresses classiﬁcation class-ratio estimation (quantiﬁcation) measures.
 applications classiﬁcation are various machine learning data analysis.
 problem quantiﬁcation expects accurate estimation relative prevalence class labels (e.g. fraction positive vs negative reviews) is useful social engineering epidemiology.
 work [17] only considers average precision performance measure does address performance measures study such F-meaure KL divergence.
 Moreover, adapted method proposed [17] performance measures study experiments show precise primal dual techniques outperform method [17].
 work [3] does consider F-measure study, do report exper- imentation F-measure.
 possible reason be algorithm requires constrained optimization problem be solved is challenging deep networks.
 We, other hand, provide generic methods solving classes performance measures include large number used measures e.g. H-mean, G-mean, Jaccard coeﬃcient, Q-measure etc [3] cannot handle.
 Furthermore, convergence guarantees proposed algorithms do oﬀer stabilization ﬁrst order stationarity guarantees methods.
 concluding remark, note methods do techniques were earlier proposed training linear models, such [14].
 work diﬀers existing works, including [14], signiﬁcant manner constitutes independent contribution.
 Previous works, such [14] consider linear models lead convex problems.
 note paper, naive direct application existing techniques deep networks yields poor results.
 techniques [14] cannot be integrated modern deep learning frameworks Theano, TensorFlow, PyTorch scalable manner.
 techniques show do so.
 Moreover, provide formal stationarity guarantees algorithms applied deep networks [14] cannot provide assume convexity problems.
 Problem Setting sake simplicity, restrict binary classiﬁcation problems.
 Let X ⊂ Rd be space feature vectors Y = {−1, +1} be label set.
 training data set S be sampled i.i.d. ﬁxed unknown distribution D X × Y.
 proportion positives population sample S be denoted p = P (x,y)∼D [y = +1] ˆpS respectively.
 sharp contrast previous work multivariate optimization considers linear models, concentrate non-linear models, induced deep neural networks.
 assume neural architecture (number layers, nodes, activation functions connectivity) has been ﬁxed let W denote space models (weights network edges).
 perform learning, use neural model, edge weights are indexed w ∈ W, assign score data point x ∈ X be converted labels, class probability estimates etc).
 Linear models assign score computing (cid:104)w, x(cid:105).
 However, use general notation f (x; w) denote score given data point x neural model indexed weights w.
 function f be seen encoding neural connections activations.
 stress function f is, general, convex concave.
 note lack structure scoring function precludes large body work linear multivariate optimization quantiﬁcation being applied deep models.
 consider performance measures be expressed terms true positive rate (TPR) true negative rate (TNR) model.
 TPR TNR are count-based are unsuitable numerical optimization algorithms.
 reason, consider use reward functions surrogates Table List performance measures Ψ(P, N p, n denote TPR TNR values obtained model.
 Name Type Min [18] Q-Mean [9] Concave Concave Fβ [11] KLD [1] Pseudolinear Nested Concave Expression (P, N −(cid:113) (1−P )2+(1−N )2 min{P, N} (1+β2)·P β2+n/p+P−n/p·N see text TPR TNR values.
 reward function r assigns reward r(ˆy, y) true label is y ∈ Y prediction is ˆy ∈ R.
 Given reward function r, model w ∈ W, data point (x, y) ∈ X × Y, scoring function f use r+(w; x, y) = r−(w; x, y) = − p · r(f (x; w), y) · I{y = · r(f (x; w), y) · I{y = −1} calculate rewards positive negative points denotes indicator function).
 expected value rewards be treated surrogates TPR TNR.
 Note E [r+(w; x, y)] = E [r(f (x; w), y)|y = setting r0-1(ˆy, y) = I{y · ˆy > i.e. classiﬁcation accuracy reward function yields E [r+(w; x, y)] = TPR(w).
 use shorthand P (w) = E [r+(w; x, y)] denote population averages reward function and, given sample n data points S = {(x1, y1),


 yn)}, denote sample average ˆPS(w) i=1 r+(w; xi, yi) deﬁne N (w), ˆNS(w).
 previous work restrict ourselves concave surrogate reward functions.
 particular utilize sigmoidal reward, is used activation function neural networks is non-concave: rsigmoid(ˆy, y) = (1 + exp(−y · ˆy))−1 (cid:80)n Performance Measures consider general classes performance measures, namely, (i) Concave Performance Measures, (ii) Pseudo-linear Performance Measures (iii) Nested Concave Performance Measures.
 experi- ments, present results selection performance measures are listed Table
 Concave Performance Measures: measures be written concave function TPR TNR values: PΨ(w) = Ψ (TPR(w), TNR(w)) concave link function Ψ R2 → R.
 measures are used cost-sensitive classiﬁ- cation cases severe label imbalance, example detection theory [18].
 popularly used member family is so-called Min-function assigns value min{TPR(w), TNR(w)} model w.
 Note compels model pay equal attention classes.
 Other examples include Q-mean H-mean measures.
 Pseudo-linear Performance Measures: measures be written ratio linear functions TPR TNR values model, i.e. have fractional linear link function.
 speciﬁcally, given given coeﬃcients b ∈ R3, P(a,b)(w) = a0 + a1 · TPR(w) + a2 · TNR(w) b0 + b1 · TPR(w) + b2 · TNR(w) used F-measure [11] is pseudo-linear performance measure terms TPR, TNR values model is represented harmonic mean precision recall.
 Other members include Jaccard coeﬃcient Gower-Legendre measure.
 Nested Concave Performance Measures: Recent works [1, problem areas such quantiﬁcation class ratio estimation problems, have brought focus performance measures be written concave combinations concave performance measures.
 formally, given concave functions Ψ, ζ1, ζ2 R2 → R, deﬁne performance measure P(Ψ,ζ1,ζ2)(w) = Ψ(ζ1(w), ζ2(w)), ζi(w) := ζi(TPR(w), TNR(w)), i =
 used measure quantiﬁcation tasks is KLD: Kullback-Leibler Divergence [1, be shown be sum concave functions TPR TNR.
 p ∈ R2 is vector true class priors binary classiﬁcation task ˆp estimate thereof, KLD(p, ˆp) = p(y) log p(y) ˆp(y) (1) (cid:88) y∈Y KLD(p, ˆp) = indicates perfect quantiﬁcation.
 note are several other performance measures techniques handle do discuss due lack space.
 include measures class-imbalanced classiﬁcation such H-mean, G-mean, Jaccard coeﬃcient (see [14]), quantiﬁcation measures such Q-measure, NSS CQB (see [7]).
 Deep Optimization Algorithms task training deep models quantiﬁcation performance measures requires address problem optimizing concave, nested concave, pseudolinear performance measures discussed Section is challenging due several reasons: measures are non-decomposable do lend straightforward training methods such gradient descent backpropagation, deep models convenience convexity, existing methods optimizing such measures e.g. fail apply deep models.
 fact, see Section direct application traditional techniques yields poor results.
 section show overcome challenges arrive scalable methods training deep networks complex non-decomposable measures.
 desirable trait methods is enjoy local convergence guarantees.
 techniques oﬀer superior empirical performance compared typical training methods deep models.
 following, procedure NN-init(din, dout, conf) initializes neural network din input output nodes, internal conﬁguration (hidden layers, number internal nodes, connectivity) speciﬁed conf.
 DUPLE: A Deep Learning Technique Concave Performance Measures present DUPLE (Algorithm scalable stochastic mini-batch primal dual algorithm training deep models concave performance measures.
 ﬁnd convenient deﬁne (concave) Fenchel conjugate link functions performance measures.
 concave function Ψ α, β ∈ R, deﬁne Ψ∗(α, β) = inf concavity Ψ, have, u, v ∈ R, u,v∈R{αu + βv − Ψ(u, v)}
 (2) (3) Ψ(u, v) = inf α,β∈R{αu + βv − Ψ∗(α, β)}
 DUPLE: Dual UPdates Learning dEep-models Require: Primal step sizes ηt, network conﬁguration {din, conf}, batch size b w0 ← NN-init(din, conf) (cid:8)α0, β0, r+, r−, n+, n−(cid:9) ← t =


 T do i )}i=1,...,b i, yt St ← SAMPLE mini-batch b data points {(xt wt ← wt−1 + ηt · ∇wg(wt; St, αt−1, βt−1) r+ ← r+ + r− ← r− + n+ ← n+ + n− ← n− + (αt, βt) ← arg (cid:80)b (cid:80)b (cid:80)n i=1 r+(wt; xt (cid:80)n i=1 r−(wt; xt (cid:20) − Ψ∗(α, β) i, yt i i, yt i i = I{yt i = −1} I{yt r− r+ + β n+ (cid:21) i=1 i=1 (cid:46) Primal Step (cid:46) Tot.
 reward +ves (cid:46) Tot.
 reward -ves (cid:46) Total positives (cid:46) Total negatives (cid:46) Dual Step (α,β) end return wT motivation DUPLE comes realization application Danskin’s theorem, gradient respect Ψ function be found obtaining maximizer α, β values (3).
 be expensive, is much cheaper update “dual” variables using gradient descent techniques instead.
 results DUPLE algorithm, primal dual stochastic-gradient based technique maintains primal model w ∈ W dual variables α, β ∈ R updates using stochastic gradient steps.
 time step uses current estimates dual variables update model, ﬁx model update dual variables.
 note DUPLE draws SPADE algorithm proposed [14].
 However, application deep models requires non-trivial extensions.

 SPADE enjoys fact gradient updates are rapid linear models is carefree performing updates individual data points.
 Doing neural models is expensive.

 Deep model training frameworks are optimized compute gradients deep networks, espe- cially GPU platforms.
 However, assume objective function respect compute gradients is static iterations.
 SPADE violates principle be seen taking gradients respect diﬀerent cost-weighted classiﬁcation problem iteration.

 theoretical convergence guarantees oﬀered SPADE assume reward surrogate functions being used are concave functions respect model.
 noted Section neural models, scoring function f (x; w) is concave/convex function w.
 DUPLE addresses above issues makes crucial design changes make optimized use deep networks.

 DUPLE overcomes issue expensive gradients amortizing gradient computation costs mini-batches.
 found improve stability properties algorithm.

 overcome issue changing objective functions, DUPLE works augmented objective function.
 Given model w ∈ W, set S labeled data points, scalars α, deﬁne g(w; S, α, β) = α · ˆPS(w) + β · ˆNS(w).
 (see Section notation).
 time steps, DUPLE takes gradients respect augmented objective function instead.
 exploit symbolic computation capabilities oﬀered frameworks such Theano [2] allow scalars α, β be updated train network diﬀerent objective function time step.

 analysis DUPLE makes assumptions convexity/concavity reward scoring functions.
 requires functions r+, r− be diﬀerentiable almost-everywhere.
 Thus, DUPLE assumes bare minimum allow take gradients.
 are able show following convergence guarantee DUPLE (see Appendix A) assuming reward functions r(f (x; w), y) are L-smooth functions model w.
 is satisﬁed reward functions consider.
 Note, however, assume reward functions are concave model parameters.
 use shorthand ∇t = ∇wg(wt; St, αt, βt) F (wt, αt) = g(wt; St, αt, βt).
 Notice result assures DUPLE procedure stabilize oscillate indeﬁnitely.

 Consider concave performance measure deﬁned using link function Ψ is concave L(cid:48)-smooth.
 Then, executed uniform step length satisfying η L DUPLE -stabilizes (cid:101)O(cid:0) (cid:1) iterations.
 speciﬁcally, T iterations, DUPLE identiﬁes model wt such (cid:18)(cid:113) (cid:19) 2 (cid:107)∇t(cid:107)2 ≤ O L(cid:48) log T DENIM: Deep Learning Nested Concave Performance Measures extend DUPLE algorithm performance measures involve nesting concave functions.
 reiterate, KLD performance measure is used quantiﬁcation, falls category.
 measures are challenging optimize using DUPLE due nested structure prevents closed form solution Fenchel conjugates.
 address challenge, present DENIM (Algorithm nests update parallel nesting performance measures.
 DENIM follows similar principle DUPLE is based NEMSIS algorithm [7].
 However, NEMSIS algorithm faces same drawbacks SPADE algorithm is unsuitable training deep models.
 Due complex nature performance measure, DENIM works diﬀerent augmented objective function.
 h(w; S, α, β, γ) = (γ1α1 + γ2β1) · ˆPS(w) + (γ1α2 + γ2β2) · ˆNS(w) Note DENIM performs inner outer dual updates are nested.
 DENIM enjoys similar convergence results DUPLE omit lack space.
 DAME: A Deep Learning Technique Pseudolinear Performance Mea- sures present DAME (Algorithm algorithm training deep models pseudolinear perfor- mance measures such F-measure are popular several areas direct optimization routines are sought after.
 recall work [3] does discuss F-measure optimization, do have access scalable implementations same.
 algorithm DAME, other hand, is based alternating strategy, is scalable gives superior performance tasks datasets.
 sake simplicity, represent pseudolinear performance measure P(a,b)(w) = Pa(w) Pb(w) a0 + a1 · TPR(w) + a2 · TNR(w) b0 + b1 · TPR(w) + b2 · TNR(w) deﬁne notion valuation function.
 Deﬁnition (Valuation Function).
 valuation pseudolinear measure P(a,b)(w) level v is deﬁned be V (w, v) = Pa(w) − v · Pb(w) DAME makes use simple observations operation: A model w has good performance i.e. P(a,b)(w) > v iﬀ satisﬁes V (w, v) > valuation function is performance measure xt i, yt i = (r+(wt; xt i, yt i ), r−(wt; xt i, yt i )) Algorithm DENIM: A DEep Nested prImal-dual Method Require: Primal step sizes ηt, network conﬁguration {din, conf}, batch size b w0 ← NN-init(din, conf) (cid:8)r0, q0, α0, β0, γ0(cid:9) ← (0, t =


 T do i )}i=1,...,b i, yt St ← SAMPLE mini-batch b data points {(xt wt ← wt−1 + ηt · ∇wh(wt; St, αt, βt, γt) qt ← (t − · qt−1 + (αt−1 qt ← qt + (αt−1 βt−1 i=1 r−(wt; xt )(cid:80)b qt ← t−1(cid:0)qt − (ζ∗ rt ← t−1(cid:16) )(cid:80)b (βt))(cid:1) (t − · rt−1 +(cid:80)b βt−1 (αt), ζ∗ (cid:17) i, yt i i=1(r(wt; xt i, yt i )) i=1 r+(wt; xt i, yt i αt = arg βt = arg γt = arg (α)} {α · rt − ζ∗ {β · rt − ζ∗ (β)} {γ · qt − Ψ∗(γ)} (cid:46) Primal Step (cid:46) Inner Dual Step (cid:46) Inner Dual Step (cid:46) Outer Dual Step end return wT decomposable one, corresponding cost-weighted binary classiﬁcation problem costs given weights a, b v.
 use notation P(a,b),S(w) VS(w, v) denote respectively, performance measure, valuation function deﬁned data sample S.
 time step DAME looks vt = P(a,b)(wt) attempts approximate task optimizing F-measure (or other pseudolinear measure) using cost weighted classiﬁcation problem described valuation function level vt.
 updating model respect approximation, DAME reﬁnes approximation again, on.
 note similar alternating strategies have been studied literature context F-measure [10, oﬀer provable convergence guarantees linear models.
 However, direct implementation methods gives poor results see next section.
 complex nature performance measures, are convex concave, make challenging train deep models.
 solve problem, DAME utilizes two-stage training procedure, involving pretraining en- tire network (i.e. upper lower layers) standard training objective such cross-entropy least squares, followed ﬁne tuning upper layers network optimize F-measure.
 pretraining is done using standard stochastic mini-batch gradient descent.
 sake simplicity let (w1, w2) denote stacking neural networks described models w1 w2.
 w2 denotes network input dimensionality din output dimensionality dint whereas w1 denotes network input dimensionality dint output dimensionality dout.
 ensure diﬀerentiability, DAME uses valuation functions appropriate reward functions replacing TPR TNR functions.
 are able show stronger ﬁrst order stationary convergence guarantee DAME.
 sake simplicity, present proof batch version algorithm Appendix B.
 assume valuation functions are L-smooth functions upper model w1.
 is noteworthy present guarantee ﬁne-tuning phase pre-training phase enjoys local convergence guarantees standard arguments.
 reason, omit lower network analysis.
 assume performance measure satisﬁes Pa(w) ≤ M w ∈ W ·Pb(w) ≥ m w ∈ cW
 note assumptions are standard [7, satisﬁed F-measure, Jaccard coeﬃcient etc have m, M = Θ(1) (see [14]).
 Let κ = + M/m.
 have following result.

 executed uniform step length satisfying η Lκ DAME discovers -stable model lengths ηt, network conﬁguration (cid:46) New features dataset Algorithm DAME: A Deep Alternating Maximization Require: Training step w−1 w−1 (w0,0 {din, dint, dout, conf1, conf2}, batch size b ← NN-init(dint, conf1) ← NN-init(din, dint, conf2) w0 Create new dataset ˜T =(cid:8)(f (xi, w0 ← Pre-train cross-entropy dataset T yi)(cid:9)n {(xi, yi)}n i=1, i=1 t =


 T do St,0 ← SAMPLE mini-batch b data points (zt,t(cid:48) vt ← P(a,b),St,0 (wt−1 t(cid:48) =


 T (cid:48) do w0 St,t(cid:48) ← SAMPLE mini-batch b data points (zt,t(cid:48) wt−1,t(cid:48) VSt,t(cid:48) ((wt−1,t(cid:48)−1 end ← wt−1,T (cid:48) wt ← wt−1,t(cid:48)−1 + ηt · ∇ wt−1,t(cid:48)−1 yt,t(cid:48) vt) end return (wT w0 Data Set Points Feat.
 Positives KDDCup08 PPI CoverType Letter IJCNN-1 Adult Twitter NA Source KDDCup08 [16] UCI UCI UCI UCI SEMEVAL16 (cid:1) inner iterations.
 speciﬁcally, t ≤ κ2 O(cid:0) that(cid:13)(cid:13)∇wP(a,b)(w)(cid:13)(cid:13) ≤ .
 2 Table Statistics data sets used.
 η(1− Lκη )2 DAME identiﬁes model wt such Experimental Results performed extensive evaluation DUPLE, DENIM DAME benchmark real-life challenge datesets found outperform traditional techniques training neural networks, nuanced task-driven training techniques proposed work [17].
 Datasets: use datasets listed Table
 Twitter refers dataset revealed part SEMEVAL sentiment detection challenge [4].
 Competing Methods: implemented adapated several benchmarks past literature attempt assess performance methods.

 ANN 0-1 refers benchmark multi-layer perceptron model trained using cross-entropy loss functions minimize misclassiﬁcation rate.

 STRUCT-ANN refers adaptation structured optimization algorithm [17] various performance measures (implementation details Appendix C).

 ANN-PG refers implementation plug-in classiﬁer F-measure suggested [10].

 DENIMS-NS refers variant NEMSIS algorithm uses count based reward sigmoidal rewards.
 similar benchmark was constructed DUPLE well.
 (a) PPI (b) KDD08 (c) COVT (d) IJCNN Figure Experiments maximizing MinTPRTNR, concave performance measure lack space, experimental results DAME algorithm are included Appendix B.
 hyper-parameters including model architecture were kept same algorithms.
 Learning rates were optimized give best results.
 Experiments Concave Measures results DUPLE (Figures optimizing MinTPRTNR QMean performance measures, show DUPLE oﬀers convergence comparison ANN 0-1.
 is be noted MinTPRTNR, ANN 0-1 has hard time obtaining non-trivial score.
 experiment IJCNN1, ran experiment longer time allow ANN 0-1 STRUCT-ANN converge observe are time intensive, compared DUPLE.
 experiments show DUPLE variant DUPLE-NS competitors terms speed accuracy.
 is be noted DUPLE takes lesser iterations STRUCT-ANN iteration DUPLE is least faster STRUCT-ANN.
 (a) PPI (b) KDD08 (c) IJCNN1 (d) A9A Figure Experiments maximizing QMean, concave performance measure (a) KDD08 (b) COD-RNA (c) LETTER (d) A9A Figure Experiments maximizing F-measure, pseudolinear performance measure TPR TNRppiDUPLEANN-0-1Struct-ANN0510152025303540Iterations0.00.10.20.30.40.50.60.70.80.9Min TPR TNRkdd08DUPLEANN-0-1Struct-ANN05101520Iterations0.00.10.20.30.40.50.60.70.80.9Min TPR TNRcovtypeDUPLEANN-0-1Struct-ANN0100200300400500Iterations0.00.20.40.60.81.0Min TPR TNRijcnn1DUPLEANN-0-1Struct-ANN0510152025303540Iterations0.20.30.40.50.60.70.8QMeanppiDUPLEDUPLE-NSANN-0-1Struct-ANN051015202530Iterations0.20.30.40.50.60.70.80.9QMeankdd08DUPLEDUPLE-NSANN-0-1Struct-ANN01020304050Iterations0.20.30.40.50.60.70.80.91.0QMeanijcnn1DUPLEDUPLE-NSANN-0-1Struct-ANN01020304050Iterations0.20.30.40.50.60.70.80.9QMeana9aDUPLEDUPLE-NSANN-0-1Struct-ANN05101520Iterations0.00.10.20.30.40.50.6F-Measurekdd08DAMEANN01ANN-PGStruct-ANN05101520Iterations0.00.10.20.30.40.50.60.70.80.9F-Measurecod-rnaDAMEANN01ANN-PGStruct-ANN05101520Iterations0.00.10.20.30.40.5F-MeasureletterDAMEANN01ANN-PGStruct-ANN020406080100Iterations0.00.10.20.30.40.50.60.7F-Measurea9aDAMEANN01ANN-PGStruct-ANN Experiments Nested Performance Measures Figure see results obtained DENIM optimizing KLD performance measure.
 shows rapid convergence near-perfect quantiﬁcation (class ratio estimation) scores.
 experiments show DENIM DENIMS-NS require less iterations competitor ANN (whenever ANN 0-1 is successful all).
 STRUCT-ANN benchmark is shown experiments got value predicting data point negative datasets are biased.
 (a) PPI (b) Letter (c) COVT (d) IJCNN Figure Experiments minimizing Kullback Leibler divergence, nested concave performance measure Experiments Pseudolinear Measures Figure (in Supplementary material) shows performance DAME optimizing F1-Measure.
 naive training misclassiﬁcation loss yields poor F-measure performance.
 Moreover, plug-in methods such proposed [10] linear models perform poorly.
 DAME other hand is able oﬀer good F-measure scores looking fraction total data.
 be seen, STRUCT-ANN oﬀers poor performance seems perform linear case.
 is implementation Struct ANN is minibatch method gradient obtained structual method has real information due this.
 other variants application [17] STRUCT-ANN algorithm, full batch methods were used.
 like point use entire training dataset update is expensive respect memory computation time, working GPU architectures.
 Case Study: Quantiﬁcation Sentiment Analysis report results experiments comparing performance DENIM Twitter sentiment detection challenge problem.
 task challenge was ascertain fraction tweets exhibiting various sentiments.
 performance was measured using Kullback-Leibler divergence (see (1)).
 trained end-to-end DeepLSTM model trained using DENIM.
 trained attention- enabled network same task using DENIM.
 models accepted raw text standard one-hot encoding format performed task speciﬁc optimization generated task speciﬁc vocabulary embeddings.
 representations were 64-dimensional were learnt other network parameters.
 Implementation details: LSTM models used single hidden layer hidden gave rise 64-dimensional hidden state representations.
 LSTM model, ﬁnal label was obtained applying linear model logistic wrapper function.
 attention models (referred AM), decoder hidden states were set be 64-dimensional well.
 alignment model was set be feed-forward model softmax layer.
 Step lengths were tuned using standard implementations ADAM method.
 Training was done adapting DENIM method.
 DENIM is able obtain perfect quantiﬁcation LSTM (KLD = AM (KLD = models (see
 contrast, classical cross-entropy method attention model (AM-CE) is unable obtain satisfactory performance.
 DENIM converges optimal test KLD 0510152025303540Iterations0.500.450.400.350.300.250.200.150.100.05NegKLDppiDENIMDENIM-NSANN-0-10510152025Iterations2.01.51.00.50.0NegKLDletterDENIMDENIM-NSANN-0-10510152025303540Iterations0.400.350.300.250.200.15NegKLDcovtypeDENIMDENIM-NSANN-0-10510152025303540Iterations2.52.01.51.00.50.0NegKLDijcnn1DENIMDENIM-NSANN-0-1 performance lesser iterations, using less data samples.
 note AM models trained DENIM give KLD losses are orders magnitude smaller LSTMs oﬀer trained DENIM.
 (a) Convergence optimal test KLD performance diﬀerent RNN models.
 (b) Change Quantiﬁcation performance dis- tribution drift.
 Figure Results Twitter Sentiment Analysis Task experiment changing fraction positive negative examples order see performance model distribution drift (see Figure
 fraction negatives positives test set was distorted original values resampling.
 test distribution priors are distorted more, AM-CE (Attention Model trained Cross Entropy) performs poorly.
 DENIM LSTMs displays degree robustness drift succumbs high level drift.
 DENIM AM models other hand, remains robust high degree distribution drift, oﬀering near-zero KLD error.
 beneﬁts attention models employed DENIM allow identify critical words tweet signal polarity.
 highlighted words (see Figure are DENIM assigned attention score α ≈
 TGIF!! Make great day, Robbie!! Monsanto’s Roundup good be love Snoop anyone having problems Windows
 be coincidental i downloaded, WiFi keeps dropping
 @NariahCFC barca pre season stand player half..
 @alias8818 Hey there! We’re excited have part T-Mobile family! listening Fleetwood Mac having candles is perfect Sunday evening Figure Figuring attention is.
 Highlighted words got high attention scores.
 red (green) highlight indicates tweet was tagged negative (positive) sentiment.
 References [1] Barranquero, J., D´ıez, J., Coz, J.J.: Quantiﬁcation-oriented learning based reliable classiﬁers.
 Pattern Recognition (2015) [2] Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde- Farley, D., Bengio, Y.: Theano: A CPU GPU math compiler Python.
 In: Proceedings Python Science Conference (SciPy pp.

 Austin, USA (2010) [3] Eban, E., Schain, M., Mackey, A., Gordon, A., Saurous, R., Elidan, G.: Scalable Learning Non- Decomposable Objectives.
 In: Proceedings 20th International Conference Artiﬁcial Intelligence Statistics (AISTATS) 051015202530354045Iterations876543210NegKLDTwitterLSTM-DENIMAM-CEAM-DENIM [4] Esuli, A.: ISTI-CNR SemEval-2016 Task Quantiﬁcation ordinal scale.
 In: Proceedings 10th International Workshop Semantic Evaluation (SemEval
 San Diego, US (2016) [5] Esuli, A., Sebastiani, F.: Optimizing text quantiﬁers multivariate loss functions.
 ACM Transactions Knowledge Discovery Data Article (2015) [6] Gao, W., Sebastiani, F.: Tweet sentiment: From classiﬁcation quantiﬁcation.
 In: Proceedings International Conference Advances Social Network Analysis Mining (ASONAM pp.

 Paris, FR (2015) [7] Kar, P., Li, S., Narasimhan, H., Chawla, S., Sebastiani, F.: Online Optimization Methods In: Proceedings ACM International Conference Knowledge Quantiﬁcation Problem.
 Discovery Data Mining (SIGKDD pp.

 San Francisco, USA (2016) [8] Kar, P., Sriperumbudur, B.K., Jain, P., Karnick, H.: Generalization Ability Online Learning Algorithms Pairwise Loss Functions.
 In: International Conference Machine Learning (ICML) (2013) [9] Kennedy, K., Namee, B.M., Delany, S.J.: Learning study one-class classiﬁca- tion low-default portfolio problem.
 In: International Conference Artiﬁcial Intelligence Cognitive Science (ICAICS), Lecture Notes Computer Science, vol.
 pp.
 (2010) [10] Koyejo, O.O., Natarajan, N., Ravikumar, P.K., Dhillon, I.S.: Consistent binary classiﬁcation generalized performance metrics.
 In: Proceedings Annual Conference Neural Information Processing Systems (NIPS pp.

 Montreal, CA (2014) [11] Manning, C.D., Raghavan, P., Sch¨utze, H.: Introduction Information Retrieval.
 Cambridge University Press (2008) [12] Narasimhan, H., Agarwal, S.: SVMtight pAUC: New Support Vector Method Optimizing Partial AUC Based Tight Convex Upper Bound.
 In: ACM SIGKDD Conference Knowledge, Discovery Data Mining (KDD) (2013) [13] Narasimhan, H., Kar, P., Jain, P.: Optimizing non-decomposable performance measures: A tale classes.
 In: Proceedings International Conference Machine Learning (ICML pp.

 Lille, FR (2015) [14] Narasimhan, H., Kar, P., Jain, P.: Optimizing Non-decomposable Performance Measures: A Tale Classes.
 In: Proceedings International Conference Machine Learning (ICML pp.

 Lille, FR (2015) [15] Narasimhan, H., Vaish, R., Agarwal, S.: Statistical Consistency Plug-in Classiﬁers Non- decomposable Performance Measures.
 In: Annual Conference Neural Information Processing Systems (NIPS) (2014) [16] Qi, Y., Bar-Joseph, Z., Klein-Seetharaman, J.: Evaluation diﬀerent biological data computational classiﬁcation methods use protein interaction prediction.
 Proteins (2006) [17] Song, Y., Schwing, A.G., Zemel, R.S., Urtasun, R.: Training Deep Neural Networks Direct Loss In: Proceedings International Conference Machine Learning (ICML) Minimization.
 (2016) [18] Vincent, P.: Introduction Signal Detection Estimation.
 Springer-Verlag New York, Inc.
 A Proof Theorem Theorem
 Consider concave performance measure deﬁned using link function Ψ is concave L(cid:48)-smooth.
 Then, executed uniform step length satisfying η L DUPLE -stabilizes (cid:101)O(cid:0) (cid:1) iterations.
 speciﬁcally, T iterations, DUPLE identiﬁes model wt such (cid:18)(cid:113) (cid:19) 2 (cid:107)∇t(cid:107)2 ≤ O L(cid:48) log T Proof.
 Recall assume reward functions r(f (x; w), y) are L-smooth functions model w.
 is satisﬁed reward functions consider.
 Note, however, assume reward functions are concave model parameters.
 use shorthand ∇t = ∇wg(wt; St, αt, βt) F (wt, αt) = g(wt; St, αt, βt).
 prove result batch version DUPLE algorithm sake simplicity present key ideas.
 extension mini-batch version is straightforward introduce additional error order batch version DUPLE makes following model update wt+1 = wt + η · ∇t.
 Using b is batch size.
 smoothness reward functions, get F (wt+1, αt) ≥ F (wt, αt) +(cid:10)∇t, wt+1 − wt(cid:11) − L (cid:13)(cid:13)wt+1 − wt(cid:13)(cid:13)2 rearranging, give (cid:107)∇t(cid:107)2 (cid:13)(cid:13)∇t(cid:13)(cid:13)2 ≤ T(cid:88) t=1 − ηL η(1− ηL ≤ F (wt+1,αt)−F (wt,αt) T(cid:88) F (wT +1, αT + (cid:17)(cid:32) t=2 summing up, gives (cid:33) F (wt, αt−1) − F (wt, αt) However, forward regret-analysis dual updates execute follow-the-leader algorithm fact due L(cid:48)-smoothness Ψ, functions F (w, α) are L(cid:48) convex, T(cid:88) F (wt, αt−1) − F (wt, αt) ≤ O (L(cid:48) log T
 completes proof applying averaging argument.
 t=2 B DAME: A Deep Learning Technique Pseudolinear Perfor- mance Measures present algorithm training deep models pseudolinear performance measures such F-measure.
 are popular several areas direct optimization routines are sought after.
 know proposed algorithm training deep models F-measure work [3].
 However, algorithm involves constrained optimization routines deep models authors do discuss details implementing same.
 algorithm DAME, other hand, is based alternating strategy, is scalable gives superior performance tasks datasets.
 sake simplicity, represent pseudolinear performance measure P(a,b)(w) = Pa(w) Pb(w) a0 + a1 · TPR(w) + a2 · TNR(w) b0 + b1 · TPR(w) + b2 · TNR(w) Given above, deﬁne notion valuation function.
 (a) CT (b) IJCNN (c) IJCNN1 (d) IJCNN1 (e) IJCNN1 (f) IJCNN1 Figure Experiments DAME Deﬁnition (Valuation Function).
 valuation pseudolinear measure P(a,b)(w) level v is deﬁned V (w, v) = Pa(w) − v · Pb(w) use notation P(a,b),S(wt) VS(w, v) denote respectively, performance measure, valuation function deﬁned data sample S.
 time step DAME looks vt = P(a,b)(wt) attempts approximate task optimizing F-measure (or other pseudolinear measure) using cost weighted classiﬁcation problem described valuation function level vt.
 making updates model respect approximation, DAME reﬁnes approximation again, on.
 note similar alternating strategies have been studied literature context F-measure [10, oﬀer provable convergence guarantees linear models.
 However, direct implementation methods gives poor results see next section.
 complex nature performance measures, are convex concave, make challenging train deep models.
 solve problem, DAME utilizes two-stage training procedure, involving pretraining en- tire network (i.e. upper lower layers) standard training objective such cross-entropy least squares, followed ﬁne tuning upper layers network optimize F-measure.
 pretraining is done using standard stochastic mini-batch gradient descent.
 details algorithm are given Algorithm
 sake simplicity let (w1, w2) denote stacking neural networks described models w1 w2.
 w2 denotes network input dimensionality din output dimensionality dint whereas w1 denotes network input dimensionality dint output dimensionality dout.
 ensure diﬀerentiability, DAME uses valuation functions appropriate reward functions replacing TPR TNR functions.
 are able show stronger local convergence guarantees DAME.
 Due lack space, present sketch proof batch version St,i = ˜T time steps i, constant step lengths.
 continue assume valuation functions are L-smooth functions upper model.
 is noteworthy present guarantee ﬁne-tuning phase pre-training phase 05101520Iterations0.000.050.100.150.200.25F-MeasurecovtypeDAMEANN01ANN-PGStruct-ANN05101520Iterations0.00.10.20.30.40.50.6F-Measurekdd08DAMEANN01ANN-PGStruct-ANN05101520Iterations0.00.10.20.30.40.50.60.70.8F-Measureijcnn1DAMEANN01ANN-PGStruct-ANN05101520Iterations0.00.10.20.30.40.50.60.70.80.9F-Measurecod-rnaDAMEANN01ANN-PGStruct-ANN05101520Iterations0.00.10.20.30.40.5F-MeasureletterDAMEANN01ANN-PGStruct-ANN020406080100Iterations0.00.10.20.30.40.50.60.7F-Measurea9aDAMEANN01ANN-PGStruct-ANN enjoys local convergence guarantees standard arguments.
 reason, omit lower network analysis.
 assume performance measure satisﬁes Pa(w) ≤ M w ∈ W ·Pb(w) ≥ m w ∈ cW
 note assumptions are standard [7, satisﬁed F-measure, Jaccard coeﬃcient etc have m, M = Θ(1) (see [14]).
 Let κ = + M/m.
 ∇wV (w,P(a,b)) prove Theorem ﬁrst show following result.
 have ∇wP(a,b)(w) = Pb(w) model O(cid:0) Pb(w) ≥ m, Theorem follow Theorem
 executed uniform step length satisfying η (cid:1) inner iterations.
 speciﬁcally, κ2 (cid:13)(cid:13)(cid:13)∇wt V ˜St((wt ≤ .
 2 such vt−1) wt model wt Proof.
 is easy see V (wt−1 vt) = V (w1, v) is Lκ-smooth function model parameter w1 realizable valuation v = P(a,b)(w) w ∈ W.
 Now, batch version DAME algorithm makes following model updates inner loop Lκ DAME discovers -stable )2 iterations, DAME identiﬁes η(1− Lκη ∇(t−1,t(cid:48)) = ∇ V (wt−1,t(cid:48)−1 wt−1,t(cid:48)−1 V (wt−1,t(cid:48) vt) ≥ V (wt−1,t(cid:48)−1 wt−1,t(cid:48) = wt−1,t(cid:48)−1 + η · ∇(t−1,t(cid:48)), = V (wt−1,t(cid:48)−1 (cid:69) − Lκ vt).
 Using smoothness reward functions, get (cid:68)∇(t−1,t(cid:48)), wt−1,t(cid:48) (cid:18) (cid:13)(cid:13)(cid:13)2 vt) + wt−1,t(cid:48)−1 wt−1,t(cid:48)−1 (cid:13)(cid:13)(cid:13)wt−1,t(cid:48) (cid:13)(cid:13)(cid:13)∇(t−1,t(cid:48))(cid:13)(cid:13)(cid:13) > , valuation model θ(t+1,i) goes (cid:19)(cid:13)(cid:13)(cid:13)∇(t−1,t(cid:48))(cid:13)(cid:13)(cid:13)2 − Lκη vt) + η vt) ≥ c P(wt ≥ P(wt−1 M
 m putting results tell such )2 inner iterations encountering model wt,t(cid:48) ≤  (cid:13)(cid:13)(cid:13)∇wP(wt,t(cid:48) m + c shows step 2.
 least η maximum value performance measure model is M DAME cannot more M is easy see V (wt η(1− Lκη (cid:17) − Lκη (cid:16) (cid:13)(cid:13)(cid:13)∇(t−1,t(cid:48))(cid:13)(cid:13)(cid:13)2 well.
 ≤ .
 easy calculation shows model have following experiments Figure:[7] show performance DAME F1-Measure.
 naive training misclassiﬁcation loss yields poor F-measure performance.
 Moreover, naive implementation methods proposed linear models such plug-in method performs poorly.
 DAME other hand is able oﬀer good F-measure scores looking fraction total data.
 be seen, struct ANN provides consistent poor performance seems perform linear case.
 is implementation Struct ANN is minibatch method gradient obtained structual method has real information due this.
 other variants application[17] structual ANN, people have used full batch methods.
 like point case is intractable respect memory computation time.
 C Details implementation Structual ANN [17] assume ∆ is loss function are looking input is dimensional confusion matrix.
 Keeping is mind, deﬁne following functions.
 a(ˆy, y) = I{yi = = (cid:88) (cid:88) (cid:88) (cid:88) b(ˆy, y) = c(ˆy, y) = d(ˆy, y) = I{yi = = I{yi = = I{yi = = Finally, m(·) is artiﬁcial neural network (cid:40) f (w) = max ˆy ∆ (a(ˆy, y), b(ˆy, y), c(ˆy, y), d(ˆy, y)) + (cid:41) (ˆyi − yi)m(xi) i=1 ˜y ∈ arg ˆy Hence, ﬁnd need solve following g(w) = ∂m(xi) + C · f (w) ∂g(w) (cid:51) w + ∂f (w) min (cid:40) ∆ (a(ˆy, y), b(ˆy, y), c(ˆy, y), d(ˆy, y)) + (ˆyi − yi)m(xi) i=1 (˜yi − yi)∂m(xi) ∈ ∂f (w) i=1 (cid:40) ∆ (p, q, r, s) + (cid:41) (cid:41) (cid:41) (cid:41) (ˆyi − yi)m(xi) i=1 n(cid:88) (cid:40) (cid:40) n(cid:88) i=1 arg max ˆy such a(ˆy,y)=p,b(ˆy,y)=q,c(ˆy,y)=r,d(ˆy,y)=s (ˆyi − yi)si arg max (p,q,r,s) arg max (p,q,r,s) a(ˆy,y)=p,b(ˆy,y)=q,c(ˆy,y)=r,d(ˆy,y)=s arg max ˆy such ∆ (p, q, r, s) + ∆ (p, q, r, s) + (˜yi − yi)∂m(xi) arg (p,q,r,s) arg max ˆy such ˆyisi i=1 a(ˆy,y)=p,b(ˆy,y)=q,c(ˆy,y)=r,d(ˆy,y)=s is amiable symbolic gradient operation, need ﬁnd gradient looks i=1 However, linearity, is same n(cid:88) i=1 (˜yi − yi)m(xi) Therefore need do forward pass symbolic graph get value m(xi) feed solver violated constraint, give deﬁne symbolic gradient n(cid:88) (˜yi − yi)m(xi) i=1
 wide selection clustering methods are available [1, most assume concurrent access data being clustered.
 interest is clustering datum becomes available, applications require unsupervised learning real time.
 Links approach is estimate probability distribution cluster based current constituent vectors, use estimates assign new vectors clusters, update estimated distributions added vector.
 update step includes ﬁx- ing cluster assignments indicated taking additional data account, is improve internal model time, typical online usage scenarios, clus- ter assignment is provided once, time new vector is made available.
 Prior work [3] addressing online clustering unit vectors employs small-variance approximation is applied low- dimensional problems such segmentation surface normals
 approach is complementary uses high- dimensional approximation, has been applied problems high variance.
 Links has been used cluster CNN-based FaceNet embeddings [4] LSTM-based voice embeddings [5].
 results latter experiment are presented separate paper [6].
 current paper focuses technical details algorithm.

 MODEL Generative model cluster Let X = {xi} be set unit-length vectors RN
 are conﬁned submanifold SN−1, determine proximity purpose clustering use natural metric submanifold, is angle vectors: ∠(x, x ′) = arccos(x · x ′).
 address problem cluster distributions sub- manifold following properties:
 cluster has center vector µ member vectors x are generated probability density is isotropic sense depends distance center, x ∼ ρ(x; µ) = f (∠(x, µ)).

 function f is same cluster, probabil- ity densities different clusters are related isometry.

 f (θ) decreases θ; example, Gaus- sian normalized SN−1: f (θ) ∝ e θ2
 (2) ensures distribution is localized, exponential decrease compensates polynomial factor marginal distribution θ: ρ(∠(x, µ) = θ) = A(sin θ)N−2f (θ) (3) A is constant equal hypersurface area SN−2, A = (4) Γ(cid:0) N−1 (cid:1)
 prior distribution ρ(µ) center cluster µ is constant SN−1 (no unit vector is preferred).
 Estimated distribution Given set X = {xi} chosen same cluster, knowledge center cluster, like estimate cluster’s probability distribution.
 likelihood center value µ is L(µ; X) =Yi ρ(xi; µ) Pi(∠(xi,µ))2 ∝ e (5) prior ρ(µ) is constant, posterior ρ(µ|X) is pro- portional expression equation
 maximum likelihood (and maximum posteriori) center is ˆµ = argmin µ Xi (cid:0)∠(xi, µ)(cid:1)2 (6) is same centroid vectors {xi} deﬁned hypersphere according [7].
 estimated probability distribution cluster is ˆρ(x; X) ∝ZSN −1 ∝ZSN −1 L(µ; X)ρ(x; µ)dµ (∠(x,µ))2+Pi(∠(xi,µ))2 (7) dµ.
 probability new vector x belongs same cluster be estimated cumulative amount increasing function k, variance estimated distri- bution decreases k.
 Z{y∈SN −1 | ˆρ(y;X)≥ ˆρ(x;X)} ˆρ(y; X)dy.
 (8) Similarly, assess clusters are same, de- termine threshold cosine similarity centroids µc · µ c ≥ s(k, k′) where, N ≫ k N ≫ k′, High-dimensional approximation primary interest is problems large N
 example, typical embedding vectors have N
 large enough N following are true: Lemma randomly chosen vectors x, x′ are perpendicular, i.e., s(k, k′) = q(cid:0)1 + s(cid:18)1 + k tan2 θc(cid:1)(cid:0)1 + k (cid:16) − + k′ tan2 θc(cid:1) k′ (cid:16) T T (16) − P (x · x ′ > δ) < ǫ (9) Note equation is special case k′ = positive numbers δ ≪ ǫ ≪
 angle θ cluster center random vector cluster is equal global constant θc, i.e., P (|θ − θc| > δ) < ǫ (10) positive numbers δ ≪ π ǫ ≪
 Lemma Given chosen vectors cluster center µ, components perpendicular µ al- ways be perpendicular other, i.e., P(cid:16)(cid:0)x − (x · µ)µ(cid:1) ·(cid:0)x ′ − (x ′ · µ)µ(cid:1) > δ(cid:17) < ǫ positive numbers δ ≪ ǫ ≪
 (11) assess add new vector x existing cluster known include k vectors {xi}k i=1, determine threshold x · ˆµ ≥ s(k) cosine similarity new vector centroid ˆµ existing vectors.
 Using approximation lemmas assuming N ≫ compute vector (x−cos θcµ) components orthonormal basis including µ, sin θc andn sin θc (xi − cos θcµ)ok i=1
 yields ˆµ = threshold pk2 cos2 θc + k sin2 θc xi Xi=1 (12) (13) s(k) = k cos2 θc T pk2 cos2 θc + k sin2 θc q k +(cid:0)1 − k(cid:1) T Tc = cos θc, call cluster similarity threshold.
 Note lim k→∞ ˆµ = µ lim s(k) = Tc, (14) (15) conﬁrms accumulate more vectors given clus- center cosine similarity threshold estimated dis- tribution center cosine similarity threshold generative distribution estimate improves).
 s(k) is = s(k), lim k,k′→∞ s(k, k′)
 (17) (18) latter conﬁrms centers estimated sets cluster points converge.

 Online clustering new input vector is assigned cluster is pro- duced, knowledge future vectors backtracking.
 unique ID cluster is returned.
 clusterer keeps statisti- cal information vectors received far.
 cannot change previous answer, change internal representation cluster statistics, such improvements estimated distributions cluster splits merges indicated new informa- tion.
 Internal representation Links algorithm’s internal representation is two-level hierar- chy: clusters are collections subclusters, subclusters are col- lections input vectors.
 subclusters are represented nodes graph edges join ‘nearby’ nodes (meaning subclusters belong same cluster given data far), clusters are deﬁned connected components graph.
 sub- clusters are indivisible, clusters become split graph edges response changes subcluster estimated probability distribu- tions new data is added.
 Alternatively, subclusters joined edge become merged response changes.
 reasons maintaining two-level hierarchy (rather than, say, arbitrary number levels) are efﬁciency practi- cality.
 is efﬁcient algorithm scales number subclusters number vectors.
 is practical key cluster substructure affect future cluster IDs is set potential split points.
 Assessing cluster membership new vector x is available, compute cosine similarity subcluster centroid ˆµj, add most-similar subcluster similarity is ﬁxed threshold Ts. other words, let J = argmax {x · ˆµj}.
 (19) Hyperparameter Tuning similarity thresholds Tc, Ts Tp need be tuned best rep- resent data source.
 is done labeling dataset cluster IDs, running clusterer data, adjusting hyperparameters improve accuracy output cluster IDs. Accuracy is fraction correct IDs. Prior evaluation, Hungarian algorithm [8] is used map subset output cluster IDs subset ground truth cluster IDs such way produces best possible accuracy.
 applications alternate objective has been used; example, gives dif- ferent weights conﬂating IDs vs.
 fracturing IDs, reﬂect seriousness type error practise.

 authors like thank Dr. Brian Budge Dr. Navid Shiee help APIs evaluation frameworks used im- plementation Links algorithm.
 x · ˆµJ ≥ Ts (20) add x subcluster J.
 Ts, called subcluster similarity threshold, is hyperparameter determining granularity cluster substructure appropriate data.
 inequality does hold, start new subcluster con- taining x.
 Next, use estimated probability distribution subcluster J determine include new subcluster same cluster J, thresholding cumulative probability expression
 high-dimensional approximation, means subcluster is included cluster x · ˆµJ ≥ s(kJ (21) kJ is number vectors subcluster J.
 ﬁrst approximation, s(k) is given equation
 be reﬁned section inequality does hold, add edge graph joining new subcluster subcluster J.
 Updating clusters new vector is added existing subcluster, subclus- ter’s centroid change.
 brings subcluster simi- larity threshold centroid subcluster joined ﬁrst edge, are merged.
 other words, ˆµi · ˆµj ≥ Ts, nodes i j are replaced single node con- taining vectors both, edge connections both.
 merging process results new subcluster centroid, check is continued affected subclusters.
 Next, edges joining affected nodes are checked validity.
 edge joining subclusters j is removed following does continue hold: ˆµi · ˆµj ≥ s(ki, kj) (22) s(ki, kj) is given equation improvements follow section severing cluster removing edge, attempt is made re-join parts adding edge affected node new partner node does satisfy inequality
 such partner is found, cluster remains split.
 Anisotropy Equations were used determine thresholds member- ship same cluster given subcluster, treating subcluster’s members randomly chosen cluster correlated other.
 were properly take account intra-subcluster correlations, consequence is limit equation be reduced positive number Tp < call pair similarity maximum, lim k,k′→∞ s(k, k′) = Tp, (23) value s(1, is T c remain unchanged.
 implicit anisotropy cluster distribution, such elon- gation preferred axis, reduce value Tp changing s(1,
 simple approximate way in- corporate adjustments algorithm is replace s(k, k′) s(k) following interpolated versions: ˜s(k, k′) = T c + ˜s(k) = ˜s(k,
 Tp − T − T c (cid:0)s(k, k′) − T c(cid:1) (24) (25)
 REFERENCES [1] Brian Everitt, Cluster Analysis, John Wiley Sons,
 [2] Christian Hennig, Marina Meila, Fionn Murtagh, Roberto Rocci, Handbook Cluster Analysis, Chapman Hall/CRC, December
 [3] Julian Straub, Trevor Campbell, Jonathan P.
 How, John W.
 Fisher, “Small-variance nonparametric clustering hyper- sphere,” IEEE Conference Computer Vision Pat- tern Recognition (CVPR), June pp.

 [4] F.
 Schroff, D.
 Kalenichenko, J.
 Philbin, “Facenet: uni- ﬁed embedding face recognition clustering,” IEEE Conference Computer Vision Pattern Recognition (CVPR), June pp.

 [5] Li Wan, Quan Wang, Alan Papir, Ignacio Lopez Moreno, “Generalized end-to-end loss speaker veriﬁcation,” arXiv preprint arXiv:1710.10467,
 [6] Quan Wang, Carlton Downey, Li Wan, Philip Andrew Mans- “Speaker diarization ﬁeld, Ignacio Lopez Moreno, lstm,” arXiv preprint arXiv:1710.10468,
 [7] Samuel R.
 Buss Jay P.
 Fillmore, “Spherical averages applications spherical splines interpolation,” ACM Trans- actions Graphics, vol.
 no.
 pp.

 [8] Harold W.
 Kuhn, “The hungarian method assignment problem,” Naval Research Logistics Quarterly, vol.
 pp.

 Artiﬁcial intelligence (AI)-enabled systems are playing in- creasingly prominent role society dis- rupting commercial government organizations oper- ate.
 rate advances machine learning (ML) using Deep Neural Networks (DNNs) is staggering, en- abling AI-enabled systems master complex tasks includ- ing Go al., autonomous driving [Bojarski predicting census data [Gebru et
 incredible advances, however, have come signiﬁcant cost: DNNs are complex, opaque, derive power millions parameters be trained large data sets.
 Thus, understanding explaining DNN came particular conclusion is difﬁcult task.
 AI-enabled systems become prevalent lives, lack explainability intepretability comes serious societal consequences.
 example, ML is being used predict recidivism criminal justice system [Berk, interventions child welfare system [Cuccaro-Alamin et al., cancer radiology images [Esteva et al.,
 Lack explainability applications have life death consequences.
 need explainability interpretability has cer- gone unnoticed AI social science com- munities.
 Important strides are being made imbue DNNs inherent explainability achieve predic- tive power DNNs using explainable methods.
 par- ticular, has been signiﬁcant amount progress ex- plaining Convolutional Neural Networks (CNNs) im- age domain, likely due ease visualizing expla- nations.
 example, GradCAM [Selvaraju et al., LRP [Binder et al., are popular methods gen- erating saliency maps indicate relevance pixels image output CNN.
 methods have improved explainabil- ity landscape, lack critical element needed true understanding humans: have limited causal interpre- tations.
 Causality means explanation has deep roots AI community [Pearl,
 causal explana- tion DNN’s operation provides end–consumer DNN output (i.e., human) understanding be changed DNN input re- sults impactful change output.
 sensitive domains credit scoring money laundering, causal ex- planation is critical system implementers justify operation ML models government regulators.
 major challenge, however, is causal explanation be formulated terms concepts variables are under- standable human; explanation end obfuscated original model.
 hypothesis explore paper posits human–understandable causal model operation DNN allows arbitrary causal interventions queries is effective, necessary, tool explain- ability interpretability.
 Arbitrary interventions allows user understand chain causal effects DNN input, low–level features domain, high–level human–understandable concepts, DNN outputs.
 Critically, model, constructed accurately, support intro- spective queries DNN supported other methods such counterfactual queries.
 be able ask is probability car have turned had been pedestrian present input im- age?” capability is powerful tool debugging, un- derstanding bias, ensuring safe operation AI systems.
 paper, explore hypothesis demonstrate causal approach DNN explanation is possible yields valuable information various classiﬁcation sys- tems.
 part approach, extract low–dimensional concepts DNNs generate human–understandable “vocabulary” variables.
 perform interventional experiments learn graphical causal model (i.e., Bayesian network [Pearl, relates DNN’s inputs concepts, concepts DNN’s outputs.
 demonstrate explanatory power model identifying concepts several networks highest expected causal effect outputs.
 contributions be summarized follows: human–understandable concepts aid explainability A causal model DNN’s operation formulated • unsupervised technique extract concepts DNNs have high likelihood being human– understandable • proposed way measure causal effects inputs concepts DNN’s outputs remainder paper is organized follows.
 Sec- tion discuss related explainability work.
 Section formulate notions causality DNNs. discuss extraction human–understandable concepts DNNs Section
 run several experiments present examples results Section conclude Section Related Work Many recent works employ saliency visual explanations.
 Several previous works have visualized predictions em- phasizing pixels have inﬂuential values (i.e., pixel values change output network changes signiﬁcantly).
 early work domain [Erhan et input image acti- vated neuron interest was found gradient ascent image domain; work was extended [Simonyan et al., obtain class-speciﬁc saliency maps.
 manipulating image space monitoring impact output, other works have considered analysis learned features network order glean understand- ing network functions.
 seminal work [Zeiler Fergus, multi-layer deconvolutional net- work was used project features activations (the output convolution maps images) input pixel space.
 Gra- dient information ﬂowing penultimate convolutional layer was used GradCAM [Selvaraju et iden- tify discriminating patterns input images.
 manipulating input images activa- tions network, methods have been explored generate images produce desired response net- work; constructed result help explain input has maximal desired response network.
 Techniques have been developed invert network constructing images activate network [Mahendran Vedaldi, [Erhan et al.,
 Fascinating images be constructed activate deep convolu- tional ﬁlters network [Mordvintsev illustrate ellaborate features generated larger deep net architectures.
 Methods do rely gradients have gained traction visual explanations.
 Layer-Wise Rele- vance Propagation [Bach et relies conservation principle redistribute prediction network back- wards relevance score is computed element input space, has been shown produce interpretable heatmaps explain individual classiﬁcations [Binder et al.,
 method reported [Xie aims re- late human understandable concepts network outputs, employing deconvolution masking- based technique ﬁnd score strength distributed representations input concepts late stage feature maps.
 Other meth- ods do consider network all, approximate ”blackbox” models simpler, ex- plainable ones have shown generate results inspire trust users [Ribeiro et al.,
 Causal Modeling Causality has long history AI numerous ML efforts have focused building realistic accurate causal mod- els world statistical models [Pearl,
 has resulted plethora causal formalisms se- mantics various ﬁelds [Granger,
 However, work, frame semantics causality terms in- terventional effects Pearl’s do-calculus [Pearl,
 directed graphical model G deﬁning causal diagram set variables X = x1, ..., xn, deﬁne joint probability distribution X (from Pearl Eqn P (xi|pai) P (x1, ..., xn) = (cid:89) (1) denoted do(x(cid:48) pai are parents xi.
 intervention G setting xi = x(cid:48) i), induces modiﬁed graph G(cid:48) edges pai xi are removed, resulting postintervention distribution Pearl Eqn.
 xi = x(cid:48) xi (cid:54)= x(cid:48) P (x1, ..., xn|do(x(cid:48) P (xj|paj)  (cid:81) j(cid:54)=i i)) = (2) semantics causality are important explain- ing DNNs because, essence, explanations be causal models.
 is, seeks explanation net- work’s decisions, is asking changes be made input output change stay same?”.
 formulation causation explanation is supported literature [Woodward,
 consider causal model deﬁnes joint distribution P (O, P, X) set DNN outputs O, inputs P, intermediate vari- ables X.
 importantly, notion interventions provides clear sound mechanism user understand DNN produces different output values.
 put way, explanation observed DNN out- put be formulated intervention.
 example, say “the DNN recognized pedestrian image saw head”, implies DNN thinks is pedestrian head image.
 DNN had detected head, intervene input remove head image, probability detecting pedestrian have changed.
 Existing explanation methods DNNs, particular image classiﬁcation tasks, lack ability provide con- crete causal interpretation user.
 example, gradient– based methods explanation such layerwise relevance propagation (LRP) [Bach et al., Grad–CAM [Sel- varaju et al., attempt explain output activation class C terms input P activations, realized speciﬁc input Pj. methods are use- ful, don’t provide causal intervention semantics are sufﬁcient robust explanation.
 Due discontinuous saturated gradients, indicate causality restricted domain function deﬁned network be approximated linear function.
 Adversarial in- stances generated using gradient descent [Goodfellow et provide indication local behavior functions deﬁned trained DNNs does have semantic relevance, suggests addition interventions deﬁned gradient based methods being restricted small domain, are uninterpretable are considering se- dubious aspect DNNs. Methods LRP avoid practical issues gradient based methods “redistributing” activation levels relevant pix- els, do provide explicit causal intervention semantics desired effective explanation.
 Causal Representation DNNs Given want causal model reﬂects interven- tion semantics, question arises is represented joint distribution P (O, P, X).
 full access in- ternals DNN, have causal representation DNN terms structure weights.
 is, given DNN, deﬁne X = R, R is set neu- rons network, learn joint distribution ex- intervening network representations [Pearl,
 user ask counterfactual questions network, i.e. P (O, P, X|do(x(cid:48) i)) input, output, internal neuron network.
 method is, technical level, correct, serves model explanation.
 is due lack human–level concepts underlie arbitrary neuron network: saying ri caused network detect pedestrian be correct does satisfy needs eventual human ingest explanations.
 language interventions human use understand network are represented individ- ual neurons.
 user cannot inquire causal impact head detection pedestrian only method intervention available is neuron gran- ularity.
 result, posit DNN causal model be constructed conceptual granularity is meaningful humans.
 propose causal model DNNs be rep- resented joint distribution O, P, set concepts C.
 process deriving C is described function fR R → C speciﬁc DNN transforms repre- sentation neurons activations set concept variables.
 Ideally, fR have following properties: (cid:90) (cid:90) P (O, P, R) = P (O, P, C) i)) = P (O, P|C, do(p(cid:48) i)) P (O, P|R, do(p(cid:48) (3) (4) is, want joint distribution inputs out- puts be same neuron–level concept– level causal models.
 Furthermore, want same causal dependencies hold respect input interven- tions.
 semantics C are open range sim- ple groups neuron high–level human concepts arms, legs, heads.
 subjectivity concepts is powerful, presents method explain DNN operation compromising true causal semantics net- work, provides ability allow users propose human– understandable interventions queries.
 Computing Causal Effects Given causal model deﬁned P (O, P, C), are num- ber interesting queries ask better understand explain operation DNN.
 work, propose measure call expected causal effect.
 deﬁne causal effect intervention x(cid:48) i Xj = xj given evidence Z as: Eﬀect(xi → xj, Z) = P (xj|do(x(cid:48) i), ZXi) − P (xj|ZXi (5) ZXi is evidence is descendant Xi. deﬁnition is similar traditional measures causal ef- fect [Rosenbaum Rubin, key difference is are comparing effect intervention Xi intervention all.
 Given effect, deﬁne expected causal effect as: EXi[Eﬀect(xi → xj, Z)] = P (Xi = xi|Z) Eﬀect(xi → xj, Z) (6) (cid:88) xi∈Xi Note compute expectation, use ev- idence Z, want consider effects outcomes are possible given evidence.
 example, ob- serve output DNN is true, causal effect variable false output is (for binary DNN output).
 Using formulation, have simple effec- tive measure quantify impact various DNN inputs concepts outputs.
 Concept Extraction order constuct causal model ﬁrst create set interpretable concepts C satisfy above causal semantics.
 way construct concepts sat- isfy semantics be consider network activa- tions.
 goal be learn causal model relating activations.
 concepts chooses, however, do need be restricted represented network activations satisfy semantics.
 simple example, consider inserting linear dense layers deep neural network such weight matrices W1 W2 were inverses other.
 ac- tivations multiplication W1 take form linear combination prior activations, ﬁnal network output be unaffected.
 speciﬁc repre- sentation instance features given activation values does have special relevance.
 choose ﬁnd concept representation, is, transformation activations, that’s interpretable.
 addition satisfying causal intervention criteria described section interpretable concepts satisfy few additional criteria:
 Concepts be low-dimensional minimize amount investigation human need employ.

 Concepts be interpretable case images, like activations be restricted contiguous areas containing consistent, interpretable visual features.

 Concepts contain relevent information needed achieving target network’s task (image classiﬁcation cases consider).
 create auxilliary neural network model con- structs concept representations satisfying properties training designed loss function.
 Speciﬁcally, form model be au- toencoder, speciﬁc compression interpretability losses be applied, retention information required classiﬁcation be ensured application recon- struction losses.
 approach has been employed construct interpretable representations learned features used classiﬁcation [Qi Li,
 approach differs main ways.
 First, training autoencoder match output classiﬁcation based linear function coded features, employ different reconstruction losses autoencoder.
 Second, train multiple autoen- coders deep neural network construct in- terpretable representations activations network.
 elaborate loss function employ, ”shallow” reconstruction loss is applied is L1 norm difference input output activations autoencoder.
 employ ”deep” reconstruction loss ensures reconstructed activations result same classiﬁcation output being passed rest network.
 loss takes form KL-divergence be- tween output probability distributions original net- work copy network autoencoder inserted given activation.
 is, target activations ai, coding function cθ, decoding function dθ, function r describing application rest network layers following autoencoded activation layer: Lshallow(θ; ai) = |dθ(cθ(ai)) − ai|1 Ldeep(θ; ai) = KL(r(ai)||r(dθ(cθ(ai)))) (7) (8) deep reconstruction loss is enforced large loss weighting hyperparameter λdeep >> λshallow.
 allows decoder reconstruct altered network activations ensuring im- portant downstream network activations are unaffected.
 added ﬂexibility ﬁtting deep loss shallow loss enables autoencoder learn interpretable rep- resentations relaxing requirement activations are decoded precisely.
 apply ”interpretabil- ity” loss serves quantify properties associate interpretable concept much prior work [Qi Li, particular em- sparsity loss, cross-entropy loss total-varation loss encourage smooth, independent coded con- cept features.
 total autoencoder loss is then: L(θ; xi) =λshallowLshallow(θ; xi)+ λdeepLdeep(θ; xi)+ λinterpretabilityLinterpretability(θ; xi) (9) weighting hyperparameters were chosen inspect- ing results tuning factors output seemed reasonable only took iterations manual reﬁne- ment dataset.
 trained autoencoders manner activations multiple layers network.
 deep reconstruction loss is particular beneﬁt shal- lower layers, one expect be able ﬁt linear classiﬁer based simple edge detectors.
 training process proceeds ﬁrst training autoencoder shal- lowest desired layer, inserting trained autoencoder network training next deepest autoencoder, iterating autoencoders have been trained.
 ex- periments train 3-4 autoencoders spaced through- convolutional layers target network.
 au- toencoder consists convolutional layers coding decoding networks.
 See Figure depiction archi- tecture used training.
 See Figures de- pictions resulting coded activations sample input image instances.
 Figure A schematic depiction training autoencoders activa- tion compression.
 depicts architecture used training autoencoder single layer.
 Having trained have plau- sible approach intervening network.
 were intervene network activations probe func- tion be difﬁcult maintain complex correlational statistics various components layer activations.
 Violating statistics result activation values be impossible recreate input sample, re- sulting misleading output.
 autoencoder, other hand, expect correlations features net- work activations be captured encoding network.
 Figure Sample display coded features instance VGG16 trained Inria pedestrian dataset.
 top left corner is original input image.
 row corresponds extracted coded feature images different autoencoder, col- umn corresponds different extracted feature image autoencoder.
 Figure Sample display coded features instance VGG19 trained birds200.
 Intervening resulting code ensure decoded activations retain statistics, reduce size possible interventions consider.
 experiments, autoencode few convolu- tional layers do encode way vector valued restrict interventions zeroing individual concept feature images coded activations.
 Experiments Having trained autoencoders network, have set concepts intervene (by changing autoencoders code) known causal structure represent- ing relationship concepts.
 Given con- struct causal model describing relationships concepts output prediction known methods
 order ﬁt causal construct large synthetic dataset containing training set in- put images values concept variables.
 randomly intervene coded images autoencoders zeroing entire feature image.
 has causal ef- fect downstream layers is captured pooled coded values downstream variables.
 intervene way coded feature image probability record resulting values.
 iden- tify active coded feature images simple variance threshold, many are result sparsifying loss terms.
 coded feature images are mean-pooled binned k ﬁnite bins.
 mean- pool concept images make construction bayes nets tractable future work intend improve approach.
 found bins were sufﬁcient maximize probability data model according techniques.
 feature values are treated variables Figure A graphical depiction learned causal Bayes net JN6 applied Inria pedestrian dataset.
 crossed boxes serve indicate nodes given level have edges incident nodes subsequent level.
 Figure A resulting sample feature image displaying head identi- ﬁcation birds200 shallowest autoencoder (level0 feat11) displayed nearest neighbors feature subset augmented dataset.
 query image is leftmost, rest ﬁgures.
 causal Bayes net layer autoencoded variables is dependent variables previous layer.
 shallowest autoencoded layer is treated dependent class label other labels associated data instance.
 Then, having built large synthetic dataset capturing interventions were made causal effect inter- ventions, construct bayes net described ﬁt CPDs node dataset.
 See Figure graphical depiction sample learned causal model.
 resulting model perform query individual input instances ranks variables network accord- ing maximum causal effect output classiﬁca- tion (see Eqn.

 is single example types causal queries perform constructed model.
 experiments were performed network architec- tures / datasets: (1) VGG applied Birds200 [Welinder et (2) VGG applied Inria pedestrian dataset [Dalal, (3) A small layer conv net refer JN6 applied Inria dataset.
 visualize top k variables according ex- Figure A resulting feature image displaying body color identiﬁ- cation birds200 autoencoder (level1 feat2).
 Expected Causal Effect 7.24E-04 Variable level4 feat6 level3 feat3 level2 feat10 level1 feat3 level1 feat17 level0 feat27 level2 feat27 level0 feat1 level4 feat3 level4 feat22 level0 feat14 level4 feat21 level1 feat28 level3 feat28 Figure resulting expected causal effect query entire dataset applied VGG16 Inria
 ’Level’ denotes autoen- coder (the shallowest being level0) ’feat’ indicates coded feature image channel refers (of are active prun- ing).
 Conclusion summarize, describe approach explaining predictions deep neural networks using causal semantics relate output prediction network concepts rep- resented within.
 use series autoencoders loss functions encouraging interpretable properties construct concepts representing information content activations target network.
 autoencoders are trained novel ”deep” loss allows increased ﬂexibility representation.
 pool features intervene autoencoded network construct variables use build causal bayesian network ﬁts causal rela- tionship deﬁned network structure.
 use network identify features signiﬁcant causal relevance individual classiﬁcations are visualized described approach.
 early investigation ideas domain.
 are number interesting possible directions future work.
 clear area potential improvement is use sophisticated methods construct variable obser- vations bayes net.
 future intend explore construction variational autoencoders image structure is encoded allow compression ir- relevant image structure.
 increase size bayes nets (in terms incident edges nodes), suggests be prudent consider structure learning reducing size bayes net skeleton.
 Additionally, we’d consider causal relationship rich in- put labels, network concept features ultimate classiﬁca- tion.
 enable direct identiﬁcation parts network identify relevant input concepts (eg.
 part network detects heads?) components contribute ultimate classiﬁcation, direct identiﬁ- cation confounding concepts result incorrect classiﬁcation (eg.
 classiﬁcations are incorrect is dark out).
 are interested extending ap- proaches non-image domains.
 Figure A feature image autoencoder trained vgg16 (level3 feat28) applied Inria dataset depicting iden- tiﬁcation feet.
 concept image has low average causal impact ﬁnal classiﬁcation (see Figure indicating visi- bility feet have major impact classiﬁcation.
 Figure A feature image autoencoder trained vgg16 (level3 feat3) applied Inria dataset depicting iden- tiﬁcation person’s outline.
 feature has large expected causal effect output
 input instance, con- cept image (level3 feat3) has individual causal effect largest causal effect concept images instance.
 i − C k pected causal effect displaying images cor- responding coded feature image.
 visualize nearest neighbors dataset according l1 distance concept feature images, |C j i |1 speciﬁed concept feature image Ci input instances j k.
 helps user better interpret feature image.
 See Figures instances nearest neighbor visualization.
 ﬁnal goal enable user inter- rogate input image instance interest identifying concepts network are relevant classiﬁcation (as measured causal effect) visu- alize context other instances contain concept similar manner.
 Figure list expected causal effect dataset VGG16 trained Inria pedestrian dataset.
 addition depicted average causal effect query causal model individual classiﬁcation instances.
 way identify feature images maximum causal effect instance question analyze represent nearest neighbor queries.
 be useful tool for, instance, debugging misclas- siﬁcations DNN.
 example, Figure depict concept feature image largest causal effect instance, is level3 feat3 individual causal effect
 intend develop interactive tool en- able queries type enable explanation instances interest.
 omit additional speciﬁc results due lack space, intend include more future release work.
 Acknowledgments material is based work supported United States Air Force Contract No. FA8750-17-C-0018 DARPA XAI program.
 Distribution A.
 Approved pub- lic release.
 Distribution is unlimited.
 References [Bach et al., Sebastian Bach, Alexander Binder, Gr´egoire Montavon, Frederick Klauschen, Klaus-Robert M¨uller, Wojciech Samek.
 pixel-wise explanations non-linear classiﬁer decisions layer-wise relevance propagation.
 PloS one,
 Richard Berk.
 impact assessment ma- chine learning risk forecasts parole board decisions Journal Experimental Criminology, recidivism.

 [Binder al., Alexander Binder, Sebastian Bach, Gregoire Montavon, Klaus-Robert M¨uller, Wojciech Samek.
 Layer-wise relevance propagation deep neural network architectures.
 Information Science Appli- cations (ICISA) pages

 [Bojarski Mariusz Bojarski et al.
 End arXiv preprint end learning self-driving cars.

 [Cuccaro-Alamin et al., Stephanie Cuccaro-Alamin, Regan Foust, Rhema Vaithianathan, Emily Putnam- Hornstein.
 Risk assessment decision making child protective services: Predictive risk modeling context.
 Children Youth Services Review,
 Navneet Dalal.
 Histograms oriented gradi- ents human detection.

 [Erhan al., Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pascal Vincent.
 Visualizing higher-layer features deep network.
 University Montreal,
 [Esteva et al., Andre Esteva, Brett Kuprel, Roberto A Novoa, Justin Ko, Susan M Swetter, Helen M Blau, Sebastian Thrun.
 Dermatologist-level classiﬁcation skin cancer deep neural networks.
 Nature,
 [Gebru al., Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Li Fei-Fei.
 Fine- grained car detection visual census estimation.
 AAAI, volume page
 [Goodfellow et al., Ian Goodfellow, Jonathon Shlens, Christian Szegedy.
 Explaining harnessing adver- sarial examples.
 arXiv preprint arXiv:1412.6572,
 Clive WJ Granger.
 Testing causality: personal viewpoint.
 Journal Economic Dynamics control,
 [Mahendran Vedaldi, Aravindh Mahendran Andrea Vedaldi.
 Understanding deep image representa- tions inverting them.
 Proceedings IEEE con- ference computer vision pattern recognition, pages
 [Mordvintsev al., Alexander Mordvintsev, Christo- pher Olah, Mike Tyka.
 Inceptionism: Going neural networks.
 Google Research Blog.
 Retrieved June,
 Judea Pearl.
 Causality.
 Cambridge University Press,
 Judea Pearl.
 Theoretical impediments ma- chine learning sparks causal revolu- tion.
 arXiv preprint arXiv:1801.04016,
 [Qi Li, Zhongang Qi Fuxin Li. Embedding deep networks visual explanations.
 arXiv preprint arXiv:1709.05360,
 [Ribeiro al., Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin.
 i trust you?: Explaining predictions classiﬁer.
 Proceedings ACM SIGKDD International Conference Knowledge Discovery Data Mining, pages

 [Rosenbaum Rubin, Paul R Rosenbaum Don- ald B Rubin.
 central role propensity score observational studies causal effects.
 Biometrika,
 [Selvaraju al., Ramprasaath R Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh, Dhruv Batra.
 Grad-cam: did say that?
 visual explanations deep networks gradient- arXiv preprint arXiv:1610.02391, based localization.

 et al., David Silver, Aja Huang, Christopher J.
 Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Do- Grewe, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, Demis Hassabis.
 Mas- tering game go deep neural networks tree search.
 Nature,
 John Nham, Nal Kalchbrenner, [Simonyan et Karen Simonyan, Andrea Vedaldi, Andrew Zisserman.
 Deep convolutional networks: Visualising image classiﬁcation models saliency maps.
 arXiv preprint arXiv:1312.6034,
 [Welinder al., P.
 Welinder, S.
 Branson, T.
 Mita, C.
 Wah, F.
 Schroff, S.
 Belongie, P.
 Perona.
 Caltech- ucsd birds
 Technical Report CNS-TR-2010-001, Cal- ifornia Institute Technology,
 James Woodward.
 Making things hap- pen: A theory causal explanation.
 Oxford university press,
 [Xie al., Ning Xie, Md Kamruzzaman Sarker, Derek Doran, Pascal Hitzler, Michael Raymer.
 Re- lating input concepts convolutional neural network de- cisions.
 arXiv preprint arXiv:1711.08006,
 [Zeiler Fergus, Matthew D Zeiler Rob Fergus.
 Visualizing understanding convolutional networks.
 European conference computer vision, pages

 Convolutional networks (CNNs) have acceler- ated progress many computer vision areas appli- cations recent years.
 powerful visual repre- sentational capabilities, CNNs are bottlenecked im- mense computational demands.
 Recent CNN architectures such Residual Networks (ResNets) [8, Inception [34] require billions ﬂoating-point operations (FLOPs) perform inference single input image.
 Fur- thermore, amount visual data grows, need in- higher-capacity (thus higher complexity) CNNs have shown utilize large visual data compared lower-capacity counterparts [33].
 have been works tackle efﬁciency is- sues deep CNNs, lowering numerical preci- sions (quantization) [14, pruning network weights [6, adopting separable convolutions [16,
 methods result efﬁcient models have ﬁxed inference costs (measured ﬂoating-point oper- ations FLOPs).
 Models ﬁxed inference costs cannot work certain resource-constrained vision sys- tems, computational budget be allocated CNN inference depends real-time resource avail- ability.
 system is lower resources, is prefer- able allocate lower budget efﬁcient cheaper inference, vice versa.
 Moreover, cases, ex- act inference budget cannot be known beforehand training time.
 simple solution concern, train several CNN models such has different inference cost, select one matches given budget inference time.
 However, is time-consuming train many models, mention computational stor- age required store weights many models.
 work, focus CNNs computational costs are adjustable inference time.
 CNN cost- adjustable inference has be trained al- lows users control trade-off inference cost network accuracy/performance.
 different inference in- stances (each different inference cost) are derived same model parameters.
 cost-adjustable inference CNNs, propose novel training method Stochastic Downsampling Point (SDPoint).
 SDPoint instance is network conﬁguration consisting unique downsampling point (layer index) network layer hierarchy unique downsam- pling ratio.
 illustrated Fig.
 training itera- tion, SDPoint instance is selected list instances), downsampling happens based down- sampling point ratio instance.
 earlier downsampling happens, lower total computational costs be, given smaller feature maps are cheaper process.
 SDPoint instance be determinis- Figure Progression feature map spatial sizes training (Left) conventional CNN, (Right) SDPoint.
 costs refer computational costs measured numbers ﬂoating-point operations (FLOPs).
 handpicked SDPoint instances seen dur- ing training) match given inference budget.
 Existing approaches [20, achieve cost-adjustable inference CNNs work evaluating subparts network (e.g., skipping layers skipping subpaths), network parameters are utilized cheaper in- ference.
 contrast existing approaches, SDPoint makes full use network parameters infer- ence costs, making use network represen- tational capacity.
 Moreover, (scale-related) parameter sharing SDPoint instances (each different downsampling downsampling ratio) provides signiﬁ- cant improvement terms model regularization.
 top advantages, SDPoint is architecture-neutral, adds parameter training overheads.
 carry ex- periments image classiﬁcation variety recent network architectures validate effectiveness SD- Point terms cost-accuracy performances regular- ization beneﬁts.
 code reproduce experiments be released.

 Related Work Cost-adjustable Inference: representative method achieve cost-adjustable inference is train “intermediate” classiﬁers [20, branch intermediate network layers.
 lower inference cost be attained early-exiting, based intermediate classiﬁers’ out- put conﬁdence [20] entropy threshold.
 lower threshold is, lower inference cost be, vice versa.
 [20], intermediate softmax classiﬁers are trained (second stage) base network has been com- trained (ﬁrst stage).
 downside [20] is intermediate classiﬁer losses are backpropagated ﬁne-tuning base network weights.
 make net- works aware intermediate classiﬁers, BranchyNet [36] has intermediate classiﬁers (each more layers branch [20]) ﬁnal classiﬁer trained jointly, us- ing weighted sum classiﬁcation losses.
 SDPoint method relies same ﬁnal clas- siﬁer different inference costs.
 FractalNets [18] are CNNs designed have many parallel subnetworks be dropped regulariza- tion training.
 cost-adjustable inference, FractalNet’s “paths” be left
 path-dropping regularization gives inconsistent/marginal improvements data augmentation is being used.
 line work related cost-adjustable inference is adaptive computation recurrent networks [5] CNNs [4].
 inference costs adaptive com- putation networks are adaptive given inputs harder examples more easier ones.
 learned policies choosing amount computation be modiﬁed inference cost-adjustable inference.
 Stochastic Regularization: work is related stochastic regularization methods apply certain stochastic operations network training regularization.
 Dropout [32] drops network activations, DropCon- nect [37] drops network weights.
 Stochastic Depth [13] allows nonlinear residual building blocks be dropped training.
 methods are similar way inference, dropped elements (activations, weight, residual blocks) are be present.
 methods, different stochastic instances seen training have comparable pass costs, making unﬁt cost-adjustable inference.
 Multiscale parameter-sharing: Multiscale training CNNs, ﬁrst introduced [7] is similar SDPoint.
 training algorithm [7], network is trained images (one scale epoch).
 same idea has been applied CNN train- ing other tasks [2,
 multiscale training down- samples input images different sizes, SDPoint downsamples feature maps (at feature level).
 Downsam- pling feature level encourages earlier network layers learn better preserve information, compensate loss spatial information caused stochastic downsampling later.
 does apply multiscale training, input images are downsampled interpolation oper- ations happen network training takes place.

 Preliminaries: Conventional CNNs Fixed Downsampling Points Conventionally, downsampling feature maps happens CNNs several predeﬁned ﬁxed locations/points layer hierarchy, depending architectural designs.
 example, ResNet-50, spatial pooling (happens af- ter ﬁrst ReLU layer, last residual block) strided convolutions (or convolution strides > happens residual blocks) are used achieve downsampling.
 downsampling layers are network stages.
 Downsampling CNNs trades low-level spatial information richer high- level semantic information (needed high-level visual tasks such image classiﬁcation) gradual fashion.
 network inference, ﬁxed downsampling points have be followed are conﬁg- ured training, optimal accuracy performance.
 work, go ﬁxed downsampling points develop novel stochastic downsampling method named Stochastic Downsampling Point (SDPoint) does restrict downsampling happen time same ﬁxed points layer hierarchy.
 proposed method is com- plementary ﬁxed downsampling points existing network architectures, do replace them.
 SDPoint be plugged existing network architectures, major architectural modiﬁcations are required.

 Stochastic Downsampling Point A Stochastic Downsampling Point (SDPoint) instance has unique downsampling point p ∈ Z unique down- sampling ratio r ∈ R are stochastically/randomly se- lected network training.
 p r are stochasti- selected beginning network training iter- ation, downsampling occurs selected point (based selected ratio) samples current training mini-batch.
 downsampling points downsampling ratios be discussed more upcoming sections.
 Downsampling is performed downsampling function D(·) makes use downsampling op- erations.
 selected point falls lower layer layer hierarchy, downsampling happens earlier forward propagation), causing quicker loss spa- tial information feature maps, more computation savings.
 Conversely, spatial information be better pre- served higher computational costs, stochastic down- sampling happens later.
 SDPoint turn feature map spatial sizes prediction layers be different original sizes, cause shape incompatibility prediction layer weights labels) con- volutional outputs prediction layers).
 prevent this, preserve feature map spatial size last net- work stage, regardless stochastic downsampling taking place not, adjusting convolution strides and/or pool- ing sizes accordingly.
 example, image classiﬁcation consider global average pooling layer [22] ﬁnal classiﬁcation layer be last network stage.
 Therefore, regardless spatial size (variable due SD- Point) incoming feature maps, pool have spatial size ×
 Downsampling Operation discussed Sect.
 downsampling operation employed D(·) be pooling [1] (average max variations) strided convolution.
 opt average pooling (the corresponding downsampling function is de- noted Davg(·)), strided convolutions max pooling several reasons.
 Strided convolutions are preferred way do downsampling recent network ar- chitectures, add extra parameters (convolution weights) improving representational capa- bility.
 work, want rule possible perfor- mance improvements increase parameter numbers SDPoint itself).
 Moreover, strided convo- lutions integer-valued strides cannot work arbitrary downsampling ratios (see Sect.

 other hand, average pooling is preferred max pooling paper due fact max pooling is form non- linearity.
 Using max pooling downsampling opera- tion either push greater non-linearity net- work (positive outcome) is unfair baselines, exacerbate vanishing gradient problem [11] com- associated deep networks (negative outcome).
 effectiveness average pooling has been val- idated extensive roles recent CNN architec- tures (e.g., global average pooling [22, DenseNets’ tran- sition [12]).
 Downsampling Points training iteration, downsampling point p SDPoint instance be drawn discrete uniform distribution set predeﬁned downsampling point in- dices P = {0, ...,N -1,N}, N + number points.
 work, downsampling point candidates are points consecutive CNN “basic build- ing blocks”, mirroring placements ﬁxed downsam- pling layers conventional CNNs. keep original network (without stochastic downsampling) instance assigning index p it, perform full-cost inference later.
 Let F (·) denote function car- ried i-th basic building block, wi denote net- work weights involved block.
 given input xi downsampling ratio r, downsampling is carried following: yi = Davg(F (xi; wi); si, r) (1) obtain output yi.
 downsampling switch denoted si ∈ {True, False} is turned p = i.
 non-residual CNNs (e.g., VGG-Net [30]), ba- sic building block comprises consecutive convolutional, Batch Normalization (BN) [15], non-linear activation lay- ers.
 other hand, residual networks, residual blocks are considered basic building blocks.
 downsampling point p be selected be point basic building blocks net- work, downsampling happens.
 residual block involves streams information (i.) identity skip connection (ii.) non-linear function consisting several network layers, apply stochastic downsampling function Davg(·) point residual addition operation.
 experiment Densely Connected Networks (DenseNets) [12] paper.
 DenseNets, SDPoint downsampling points are points right be- hind block concatenation mirroring ﬁxed downsampling DenseNets.
 principle, mini-batch sample have unique downsampling point pi (for stronger stochasticity), due practical reasons (e.g., training efﬁciency, ease implementation), resort using same pi samples mini-batch.
 is possible have more downsampling points training iteration, number possible combinations SDPoint instances become large.
 instances deviate much original network, terms computational cost accuracy performance.
 opt single stochastic downsampling point work.
 Downsampling Ratios consider set downsampling ratios R, SDPoint instance draw downsam- pling ratio r from, use current training iteration.
 Sect.
 downsampling ratios are drawn accord- ing discrete uniform distributions.
 ratios cannot be low hamper training convergence (due parameter-sharing unfeasibility).
 consider small number downsampling ratios R prevent excessive number SDPoint instances, cause great difﬁculty evaluating SDPoint in- stances cost-adjustable inference.
 recent experimental study [24] CNNs ﬁnds is sufﬁcient make quali- tative conclusions optimal network structure hold full-sized (224 × image resolution) ImageNet [29] classiﬁcation task, using × original resolution) input images.
 Conceivably, same network structure/architecture works certain image resolution is likely work resolu- tion double/half that.
 Motivated above-mentioned heuristics experimental ﬁnding, come downsampling ratio set R = {0.5,
 same ratios have been used [2] “multiscale-input” semantic segmentation.
 same hyperpameter R is used experiments paper.
 Downsampling such fractional downsampling ra- tios cannot be achieved integer-valued pooling hyperparameters.
 example, pooling × feature map × (with r minimal over- laps) cannot be done tuning pooling size stride.
 end, adopt spatial pooling strat- egy (which works pooling choice Sect.
 akin Spatial Pyramid Pooling [7] gener- ates ﬁxed-length representation adaptive calculations pooling sizes strides.
 Downsampling Points (cid:46) Downsampling Ratios (cid:46) Forward pass Randomly draw p P Randomly draw r R x1 = x i ∈ {1, ...,N -1,N} do Algorithm Training SDPoint P = {0, ...,N -1,N} R = {0.5, given training mini-batch x do end end Compute loss xN +1 Backward pass Parameter updates end xi+1 = Davg(F (xi; wi); si, r) si = False i = p si = True Training SDPoint SDPoint gives rise new training algorithm CNNs. training consolidating in- troduced SDPoint concepts is given Algorithm
 F (·) denotes generic nonlinear building network block CNNs. simplicity sake, omit other network lay- ers are basic building blocks start- ing ending layers.
 nutshell, Algorithm shows whenever building block index i is equal down- sampling point p, downsampling switch s is turned
 Stochastic downsampling happens output i-th building block, stochastic downsampling ratio r.
 is important point (stochastic) downsampling does happen, p is drawn be allowing network work original “unadulterated” form.
 Regularization SDPoint be seen regularizer CNNs. stochastic downsampling takes place, receptive ﬁeld size becomes larger causes sudden shrinkage spatial information feature maps.
 network has learn adapt such variations training, per- form parameter-sharing downsampled feature maps sized feature maps (when p
 addition robustness terms receptive ﬁeld size spatial shrinkage, SDPoint necessitates convolu- tional layers accommodate different “padded pixel non-padded pixel” ratios.
 example, applying × convolutional ﬁlter (with zero-padding × fea- ture map gives padded-pixel ratio compared ratio resulted applying same ﬁlter × feature map.
 Zero-padded pixels are similar zero-ed activations caused Dropout [32], sense are missing values.
 Thus, higher padded- pixel ratio is akin having higher number dropped-out activations, vice versa.
 form variation provides fur- ther regularization boost.
 Experimentally, ﬁnd use heavy data augmentation such “scale + aspect ratio” augmentation [35, SDPoint help.

 Cost-adjustable Inference A network perform inference different computational costs depending user requirements, is considered be capable cost-adjustable inference.
 Opting lower inference cost results lower prediction accuracy, vice versa.
 SDPoint supports cost-adjustable inference, given SDPoint instances have varying computational costs, given different downsampling point locations downsampling ratios.
 importantly, instances have been trained minimize same prediction loss, helps work inference.
 inference, handpick SDPoint instance (with downsampling point p downsampling ratio r) make inference cost particular inference budget.
 Instance-Speciﬁc Batch Normalization men- tioned Sect.
 SDPoint instances are trained such way training mini-batch iteration shares same SDPoint instance.
 SDPoint instance, pre- diction loss minimization training are based Batch Normalization (BN) statistics (means stan- dard deviations) particular instance.
 Therefore, us- ing BN statistics accumulated many training itera- tions (and many different SDPoint instances) infer- ence causes inference-training “mismatch”.
 similar form inference-training “mismatch” caused BN statistics has been observed [31] context.
 BN statistics required SDPoint instance differ instance.
 using same (accu- mulated) BN statistics perform cost-adjustable inference, inference accuracies be jeopardized.
 address “mismatch” issue, compute SD- Point instance-speciﬁc BN statistics, use cost- adjustable inference.
 Disentangling different SDPoint instances unsharing BN statistics makes inference more accurate.
 computational storage overhead re- sulted instance-speciﬁc BN statistics is low, BN statistics earlier layers be shared1 certain SDPoint instances downsample layers.

 Experiments Experiments are carried image classiﬁcation tasks evaluate SDPoint.
 consider image classiﬁcation datasets varying dataset scales terms numbers categories/classes sample counts: CIFAR-10 [17] (50k training images, validation images, classes), CIFAR- [17] (50k training images, validation images, classes), ImageNet [29] (1.2M training images, valida- tion images, classes).
 inference cost comparison, measure model costs terms ﬂoating-point op- eration numbers (FLOPs) needed forward propagation single image.
 treat addition multiplication separate operations.
 Implementations are PyTorch [25].
 CIFAR [40] CIFAR-10 CIFAR-100, baseline archi- tectures are Wide-ResNet (WRN-d28-w10 WRN-d40-w4) DenseNetBC-d40-g60 [12].
 ‘d’, ‘w’, ‘g’ stand network depth, widen factor WRN, growth rate DenseNetBC, respectively.
 training hyperparameters (e.g., learning rates, schedules, batch sizes, follow ones original papers, training epoch numbers ﬁx all.
 original learning rate schedules apply (e.g., learning rates are dropped total number training epochs).
 numbers SDPoint downsam- pling points (N {WRN-d28-w10, WRN-d40-w4, DenseNetBC-d40-g60} are {12, ,12} respectively.
 mentioned Sect.
 downsampling ratios are drawn R = {0.5,
 Baseline Comparison: compare SDPoint baseline methods related ours, terms cost-adjustable inference performance.
 classiﬁcation error-cost performance plots CIFAR-10 CIFAR-100 supplementary materials more storage overheads.
 Figure WRNs’ DenseNetBC’s cost-error plots CIFAR-10 (Top) CIFAR-100 (Bottom).
 is observed models trained SDPoint outperform non-SDPoint counterparts, given same computational budgets.
 Model ResNeXt-d29-c08 [38] ResNeXt-d29-c16 [38] DenseNetBC-d250-g24 [12] DenseNetBC-d190-g40 [12] WRN-d40-w4 [40] WRN-d40-w4 [40] SDPoint WRN-d28-w10 [40] WRN-d28-w10 [40] Dropout [32] WRN-d28-w10 [40] SDPoint DenseNetBC-d40-g60 [12] DenseNetBC-d40-g60 [12] SDPoint Params GFLOPs CIFAR-10 CIFAR-100 (↓ (↓ (↓ (↓ Table CIFAR-10 CIFAR-100 validation errors (%).
 GFLOPs values separated “/” are CIFAR- CIFAR-100 respectively.
 (↓ (↑ (↓ (↓ are shown Fig.

 Note SDPoint baseline methods, instances same model appear plots; higher-cost instance performs worse lower-cost instance, is shown.
 model (evaluated dataset) is trained obtain cost-error plot.
 (i) Early-Exits train models based WRN intermediate classiﬁers (branches) allow early- exits (EE), following design BranchyNet [36].
 network stage main network has spaced branches, branches have single-repetition building block branch network stage.
 blocks branches follow same hyperparameters (e.g., blocks original network.
 cost-adjustable inference, evaluate branch, make samples “exit” same branch.
 early-exit models have more parameters baseline models SDPoint-based models.
 conjecture worse performance EE is due lack full network parameter ultilization.
 Also, EE forces CNN features be classiﬁcation-ready early stage, causing higher layers rely classiﬁcation- ready features, learning better features own.
 (ii) Multiscale Training (MS) Multiscale (MS) training is baseline method inspired [7,
 input images are downsampled using bilinear interpolations, integer-valued size randomly chosen sizes ranging half (16 × full size (32 × step size pixel.
 is done training iteration, similar SDPoint.
 number “instances” (16) resulted multiscale training is downsampling point num- bers applying SDPoint WRNs DenseNetBC(s).
 ranges cost-adjustable inference costs are comparable.
 Instance-speciﬁc BN statistics are applied.
 cost-adjustable performance MS consis- trails SDPoint, input downsampling causes drastic information loss feature map downsampling (see Sect.

 (iii) Uniform Batch Normalization (UBN) validate effectiveness SDPoint instance-speciﬁc BN, show results SDPoint baseline BN statistics are averaged many training iterations, are uniform instances.
 are consistent classiﬁcation performance gaps using UBN statistics instance-speciﬁc BN statistics, suggesting is prefer- able keep instance-speciﬁc statistics inference.
 State-of-the-art Comparison: Table reports Model ResNeXt-d101-c64 [38] DenseNetBC-d264 [26] ResNeXt-d101-c32 [38] ResNeXt-d101-c32 [38] SDPoint PreResNet-d101 [9] PreResNet-d101 [9] SDPoint PreResNet-d101 [9] SACT [4] PreResNet-d101 [9] SDPoint Params GFLOPs ∼32 ∼26 ∼89M ∼73M Top-1 (↓ (↓ Top-5 (↓ (↓ Table ImageNet top-1 top-5 validation errors (%), model parameter numbers giga-FLOPs (GFLOPs).
 CIFAR validation results state-of-the-art (SOTA) ResNeXt [38] DenseNetBC [12] models, compar- ison ours.
 SDPoint-enabled model, show results (giga-FLOPs, classiﬁcation errors) best- performing SDPoint instance instances.
 Notably, WRN-d28-w10 SDPoint is competitive SOTA mod- els CIFAR-100, outperforms CIFAR-
 Overall, SDPoint improves classiﬁcation performance bringing additional parameters computational costs, SOTA models re- quire model complexity attain slight improve- ments.
 best SDPoint-enabled models CIFAR- have reduced inference costs (FLOPs).
 reckon prolonged preservation spatial details (i.e., early downsampling) CNN feature maps is crucial dataset low label complexity such CIFAR-
 reveals drawback current practice using CNNs “one-size-ﬁts-all” fashion.
 ImageNet consider ResNeXt-d101-c32 [38] PreResNet- d101 [9] baseline architectures.
 ‘c’ stands ResNeXt’s cardinality.
 SDPoint, are downsampling points model.
 train models ImageNet-1k [29] training set, evaluate validation set (224×224 center crops).
 models are trained using training hyperparameters “scale + aspect ratio” aug- mentation [35] identical [38].
 Note do allocate training epochs models SDPoint.
 cost-error plots are given Fig.
 PreResNet- d101 ResNeXt-d101-c32 respectively, ﬁxed-cost designed2 baseline models same architecture families.
 Overall, models trained SDPoint match performance baseline models lower-cost range, surpass upper-cost range.
 Notably, obtain cost-error plots, SDPoint-enabled models have be trained
 hyperparameters are chosen authors [9, optimize accuracy performances budget constraints.
 Figure PreResNets’ [9] cost-error plots ImageNet.
 PreResNet-d101 (SDPoint) has be trained (as single model), baseline models SDPoint) has be trained huge training storage costs.
 Figure ResNeXts’ [38] cost-error plots ImageNet.
 Fig.
 ResNeXt-d101-c32 (SDPoint..) has be trained (as single model).
 baseline models are trained separately, resulting huge total number epochs (#models × #epochs model) storage cost.
 Ablation Study: study effects choice SDPoint downsampling points downsampling ratios cost-adjustable inference performance.
 this, train ResNeXt-d101-c32 default SDPoint hyperparameters (downsampling points end residual block, downsampling ratios {0.5,0.75}), baseline models (i) downsampling points end other residual block dubbed alternate (ii) downsampling ratio {0.75} dubbed
 are shown Fig.

 removing downsampling ratio alternating blocks downsampling gives worse results, due reduced stochasticity (and regularization strengths).
 State-of-the-art Comparison: compare models SOTA ResNeXt-d101-c64 [38] DenseNetBC-d264-g48 [26] models Table
 SDPoint pushes top-1 top-5 validation errors ResNeXt- 51015202530GFLOPs22232425262728Top-1 Validation Error (%)PreResNet-d34PreResNet-d50PreResNet-d101PreResNet-d152PreResNet-d200PreResNet-d101 (SDPoint)51015202530GFLOPs21222324252627Top-1 Validation Error (%)ResNeXt-d50-c32ResNeXt-d101-c32ResNeXt-d101-c64ResNeXt-d101-c32 (SDPoint-075)ResNeXt-d101-c32 (SDPoint-alternate)ResNeXt-d101-c32 (SDPoint) Figure Imagenet validation examples grouped according minimum inference costs (FLOPs) required ResNeXt-d101-c32 (with SDPoint) classify correctly, terms top-5 accuracy.
 ground-truth label names are shown corresponding images.
 d101-c32 respectively, are (previously) attainable SOTA models inference costs parameter counts.
 display results (and mean FLOPs) Spatially Adaptive Computation Time (SACT) [4] paired PreResNet-d101, compare SDPoint instance PreResNet-d101 achieves similar classiﬁcation errors.
 SDPoint needs FLOPs needed SACT achieve similar results.
 SACT saves computation skipping layers (and network parameters) certain locations feature maps according learned policy inputs, SDPoint downsamples feature maps save computation (but makes full use network parameters capacity inference).
 contend cost- accuracy trade-off inference, reducing feature spatial sizes is harmful accuracy skipping network parameters/layers.
 Analysis: provide analyses ResNeXt- d101-c32 (trained SDPoint ImageNet) regards certain aspects downsampling SDPoint.
 Cost-dependent misclassiﬁcations: group ImageNet validation images are classiﬁed full inference cost) according minimum inference costs required classify correctly, present examples Fig.

 difﬁcult examples require higher inference costs (9.9, GFLOPs) be classi- ﬁed correctly, have size-dominant interfering objects/scenes (e.g., hair dryer, cab, caldron, cock, tench), contrast easier examples (4.3 GFLOPs).
 Intuitively, pooling-based downsampling causes more information loss smaller objects larger (size-dominant) objects, occurs early layer, semantic/context information is weak distinguish objects interest interfering objects.
 So, difﬁcult examples, makes sense preserve informative object details CNN layer hierarchy, downsample feature maps are rich enough.
 Scale sensitivity: Training CNNs SDPoint involves stochastic downsampling intermediate feature maps, hypothesize be beneﬁcial scale sensitiv- ity/invariance, mentioned Sect.
 validate hypothesis, vary pre-cropping3 sizes ImageNet validation images range ..., step size resulting pre-cropping sizes.
 pre- cropping size, × center image regions are cropped evaluation.
 models involved are SDPoint- enabled ResNeXt-d101-c32, baseline SD- Point.
 compute mean pairwise cosine simi- larities (a total pairs) resulted different pre- cropping sizes, terms ImageNet 1k-class probabil- ity scores.
 is done entire ImageNet validation set.
 pairwise cosine-similarity mean obtained baseline model is SDPoint-enabled model, is
 higher cosine similarity is strong indicator model being sensitive scales.
 demonstrates SDPoint beneﬁt CNNs, terms scale sensi- tivity.

 Conclusion propose Stochastic Downsampling Point (SDPoint), novel approach train CNNs downsampling inter- mediate feature maps.
 extra parameter training costs, SDPoint facilitates effective cost-adjustable inference improves network regularization (thus accuracy performance).
 experiments, ﬁnd SDPoint help identify more optimal less costly) sub-networks (Sect.
 sort input examples various levels classiﬁcation difﬁculties (Fig.
 mak- ing CNNs less scale-sensitive (Sect.

 is standard practice [8, resize images have shorter side (pre-cropping size) doing × center-cropping.
 References [1] Y.-L.
 Boureau, J.
 Ponce, Y.
 LeCun.
 theoretical analy- sis feature pooling visual recognition.
 International Conference Machine Learning (ICML),
 L.-C.
 Chen, G.
 Papandreou, I.
 Kokkinos, K.
 Murphy, A.
 L.
 Yuille.
 Deeplab: Semantic image segmentation deep convolutional nets, atrous convolution, con- nected crfs.
 IEEE Transactions Pattern Analysis Ma- chine Intelligence (TPAMI),
 [3] F.
 Chollet.
 Deep learning depthwise sepa- rable convolutions.
 Conference Computer Vision Pattern Recognition (CVPR),
 [4] M.
 Figurnov, M.
 D.
 Collins, Y.
 Zhu, L.
 Zhang, J.
 Huang, D.
 Vetrov, R.
 Salakhutdinov.
 adaptive compu- Conference Com- tation time residual networks.
 puter Vision Pattern Recognition (CVPR),
 [5] A.
 Graves.
 Adaptive computation time recurrent neural networks.
 arXiv preprint arXiv:1603.08983,
 S.
 Han, J.
 Pool, J.
 Tran, W.
 Dally.
 Learning weights connections efﬁcient neural network.
 Conference Neural Information Processing Systems (NIPS),
 K.
 He, X.
 Zhang, S.
 Ren, J.
 Sun.
 Spatial pyramid pooling deep convolutional networks visual recognition.
 European Conference Computer Vision (ECCV),
 [8] K.
 He, X.
 Zhang, S.
 Ren, J.
 Sun.
 Deep residual learning Conference Computer Vision image recognition.
 Pattern Recognition (CVPR),
 [9] K.
 He, X.
 Zhang, S.
 Ren, J.
 Sun.
 Identity mappings deep residual networks.
 European Conference Com- puter Vision (ECCV),
 [10] Y.
 He, X.
 Zhang, J.
 Sun.
 Channel pruning accelerat- ing deep neural networks.
 International Conference Computer Vision (ICCV),
 S.
 Hochreiter.
 Untersuchungen zu dynamischen neuronalen netzen.
 Diploma, Technische Universit¨at M¨unchen,
 G.
 Huang, Z.
 Liu, L.
 van der Maaten, K.
 Q.
 Weinberger.
 connected convolutional networks.
 Conference Computer Vision Pattern Recognition (CVPR),
 [13] G.
 Huang, Y.
 Sun, Z.
 Liu, D.
 Sedra, K.
 Q.
 Weinberger.
 Deep networks stochastic depth.
 European Confer- ence Computer Vision (ECCV, pages

 I.
 Hubara, M.
 Courbariaux, D.
 Soudry, R.
 El-Yaniv, Conference Y.
 Bengio.
 Binarized neural networks.
 Neural Information Processing Systems (NIPS),
 S.
 Ioffe C.
 Szegedy.
 Batch normalization: Accelerating deep network training reducing internal covariate shift.
 International Conference Machine Learning (ICML), pages
 [16] M.
 Jaderberg, A.
 Vedaldi, A.
 Zisserman.
 Speeding convolutional neural networks low rank expansions.
 British Machine Vision Conference (BMVC),
 A.
 Krizhevsky.
 Learning multiple layers features tiny images.

 G.
 Larsson, M.
 Maire, G.
 Shakhnarovich.
 Fractal- In- net: Ultra-deep neural networks residuals.
 ternational Conference Learning Representations (ICLR),
 C.-Y.
 Lee, S.
 Xie, P.
 Gallagher, Z.
 Zhang, Z.
 Tu. Deeply- supervised nets.
 Artiﬁcial Intelligence Statistics (AIS- TATS,
 S.
 Leroux, S.
 Bohez, T.
 Verbelen, B.
 Vankeirsbilck, P.
 Simoens, B.
 Dhoedt.
 Resource-constrained classiﬁ- cation using cascade neural network layers.
 Interna- tional Joint Conference Neural Networks (IJCNN),
 [21] H.
 Li, A.
 Kadav, I.
 Durdanovic, H.
 Samet, H.
 P.
 Graf.
 Pruning ﬁlters efﬁcient convnets.
 International Con- ference Learning Representations (ICLR),
 [22] M.
 Lin, Q.
 Chen, S.
 Yan.
 Network network.
 In- ternational Conference Learning Representations (ICLR),
 J.-H.
 Luo, J.
 Wu, W.
 Lin.
 Thinet: ﬁlter level pruning Interna- method deep neural network compression.
 tional Conference Computer Vision (ICCV),
 D.
 Mishkin, N.
 Sergievskiy, J.
 Matas.
 Systematic eval- uation convolution neural network advances ima- genet.
 Computer Vision Image Understanding (CVIU),
 A.
 Paszke, S.
 Gross, S.
 Chintala, G.
 Chanan.
 Pytorch.
 http://pytorch.org/.
 G.
 Pleiss, D.
 Chen, G.
 Huang, T.
 Li, L.
 van der Maaten, K.
 Q.
 Weinberger.
 Memory-efﬁcient implementation densenets.
 arXiv preprint arXiv:1707.06990,
 [27] M.
 Rastegari, V.
 Ordonez, J.
 Redmon, A.
 Farhadi.
 Xnor- net: Imagenet classiﬁcation using binary convolutional neu- ral networks.
 European Conference Computer Vision (ECCV),
 J.
 Redmon A.
 Farhadi.
 Yolo9000: Better, faster, stronger.
 Conference Computer Vision Pattern Recognition (CVPR),
 [29] O.
 Russakovsky, J.
 Deng, H.
 Su, J.
 Krause, S.
 Satheesh, S.
 Ma, Z.
 Huang, A.
 Karpathy, A.
 Khosla, M.
 Bernstein, al.
 Imagenet large scale visual recognition challenge.
 In- ternational Journal Computer Vision (IJCV,
 [30] K.
 Simonyan A.
 Zisserman.
 deep convolutional networks large-scale image recognition.
 International Conference Learning Representations (ICLR),
 S.
 Singh, D.
 Hoiem, D.
 Forsyth.
 Swapout: Learning ensemble deep architectures.
 Conference Neural Information Processing Systems (NIPS), pages
 [32] N.
 Srivastava, G.
 E.
 Hinton, A.
 Krizhevsky, I.
 Sutskever, R.
 Salakhutdinov.
 Dropout: simple way prevent neu- ral networks overﬁtting.
 Journal Machine Learning Research (JMLR),
 [33] C.
 Sun, A.
 Shrivastava, S.
 Singh, A.
 Gupta.
 Revisiting unreasonable effectiveness data deep learning era.
 International Conference Computer Vision (ICCV),
 [34] C.
 Szegedy, S.
 Ioffe, V.
 Vanhoucke, A.
 A.
 Alemi.
 Inception-v4, inception-resnet impact residual connections learning.
 AAAI Conference Artiﬁcial Intelligence,
 [35] C.
 Szegedy, W.
 Liu, Y.
 Jia, P.
 Sermanet, S.
 Reed, D.
 Anguelov, D.
 Erhan, V.
 Vanhoucke, A.
 Rabinovich.
 Going deeper convolutions.
 Conference Computer Vision Pattern Recognition (CVPR),
 [36] S.
 Teerapittayanon, B.
 McDanel, H.
 Kung.
 Branchynet: Fast inference early exiting deep neural networks.
 International Conference Pattern Recognition (ICPR),
 [37] L.
 Wan, M.
 Zeiler, S.
 Zhang, Y.
 L.
 Cun, R.
 Fergus.
 Reg- ularization neural networks using dropconnect.
 Inter- national Conference Machine Learning (ICML),
 S.
 Xie, R.
 Girshick, P.
 Doll´ar, Z.
 Tu, K.
 He. Ag- gregated residual transformations deep neural networks.
 Conference Computer Vision Pattern Recognition (CVPR),
 [39] T.-J.
 Y.-H.
 Chen, V.
 Sze.
 Designing energy- efﬁcient convolutional neural networks using energy-aware Conference Computer Vision Pattern pruning.
 Recognition (CVPR),
 S.
 Zagoruyko N.
 Komodakis.
 Wide residual networks.
 British Machine Vision Conference (BMVC),
 [41] C.
 Zhu, S.
 Han, H.
 Mao, W.
 J.
 Dally.
 Trained ternary quantization.
 International Conference Learning Rep- resentations (ICLR),

 network anomaly is deviation normal operation network, is learned observation is signified decreased network performance.
 paper, are interested network performance measurements: throughput, packet loss rate, one-way delay (OWD), traceroute.
 Throughput measures amount data be transferred time interval.
 interval was chosen be seconds is deemed cause undue stress link short have unreliable measurements.
 Packet loss is percentage lost packets total transferred packets.
 reach sensitivity 10​-5 measure Hz average result one-minute bins.
 One-way delay measures delay (in ms) direction path.
 Traceroute is path transition time source destination.
 Anomalies last hour multiple days (if be caused multitude factors.
 Possibilities include: full connectivity disruption, packets are lost; device path is close saturation, signified increase one-way delay packets spend more time device’s buffers; device path is saturated, packets are lost device’s buffers overflow, signified increase one-way delay; routing changes leading asymmetrical paths takes more time reorder packets, signified large variance one-way delays; dirty fibers other problems optics, signified increase packet loss rate.
 collect data perfSONAR [1] servers.
 perfSONAR is network measurement toolkit monitors stores network performance data pairs endpoints ("links").
 PerfSONAR is installed thousands servers, are interested ones are part WLCG (Worldwide LHC Computing Grid) OSG (Open Science Grid) meshes.
 network mesh size is large.
 ever-increasing amount links issues network performance, becomes difficult identify where, when, issues arise are significant enough ignore.
 Furthermore, due high variance quantity data collected, becomes difficult analyze develop models normal network behavior anomalies.
 Machine learning algorithms are favorable learn is normal Abstract behavior is anomalous behavior adapt changes structure normal data.
 optimal anomaly detection method satisfy following criteria: Have capability combine disparate data features data sources, e.g. links site, Packet loss, OWDs, paths, throughputs, FTS measurements, give information features (combinations features) are causing anomaly, alert appropriate persons responsible solving issue; have tunable sensitivity, are interested short duration (order hour) flukes action be taken time scales shorter that; perform practical level.
 Packet loss are interested packet loss due extraordinary influence throughput.
 seen Figure change packet loss cause magnitude change throughput.
 Figure relation packet loss (%) throughput.
 Related Work Machine learning techniques have been used anomaly detection [2].
 Su presented density method using k-nearest neighbors algorithm (kNN) detect denial-of-service attacks [3].
 Sakurada al.
 published reconstruction method using auto autoencoders detect anomalies artificial data generated Lorenz system real data spacecraft’s telemetry data [4].
 Rajasegarar al.
 described boundary method using one-class support vector machines anomaly detection sensor networks [5].
 Recently, Catmore proposed split-sample classification method other ideas anomaly detection [6].
 paper presents new methods based split-sample classification reconstruction [6].

 Datasets use different datasets test functionality new machine learning algorithms applied network anomaly detection.
 Figure shows example simulated data.
 Simulated Dataset A simulation actual data was generated test functionality methods.
 defined time series (features) spanning seven-day period 08-01-2017
 Data time series was assigned value was generated second.
 Normal data was generated time series generating random value less generating random values normal distribution number.
 Normal data was flagged
 Table is example such generated data.
 Anomalous data was standard deviations normal noise distribution.
 Anomalous data was generated times entire time period had maximum duration hours.
 Table is example generated anomalous data.
 Figure is example simulated data.
 Table first seconds generated normal data.
 Time 2017-08-03 2017-08-03 2017-08-01 2017-08-01 2017-08-05 2017-08-05 2017-08-02 2017-08-02 2017-08-05 2017-08-05 2017-08-03 2017-08-03 Affected [2, [2, [1, [5, [2, [3, Table time period features affected anomaly generated simulated data.
 Feature numbers were listed descending order significance data.
 Figure
 figure is example simulated data.
 link, assigned specific color, is different feature dataset pink columns signify anomalies.
 vertical axis is value data point feature has arbitrary units.
 Real-World Datasets data collected various links measuring one-way delay, throughput, packet loss was used test functionality methods see work practical level.
 Figures show examples real-world data be analyzed.
 Figure
 graph shows packet loss measured day period 2017-05-10 2017-05-30 link source is PIC destination is CERN.
 CERN is Tier-0 site Geneva, Switzerland, PIC is Spanish Tier-1 center.
 Figure
 graph shows square root packet loss (%) six-day period 05-14-2017 05-20-2017 sites: CERN-PROD, FZK-LCG2, UFlorida-HPC, UKI-NORTHGRID-MAN-HEP, CSCS-LCG2.

 New Anomaly Detection Method are numerous algorithms anomaly detection time series.
 general methods used are eg.
 ARIMA, SVM, specific ones eg.
 Bayesian inspired approaches [7].
 have annotated historical data limits unsupervised methods.
 amount data has be processed excludes several promising approaches.
 Similar split-sample classification method [6], following compare time-dependent features period examination (subject period) preceding period (referent period).
 do trying train Boosted Decision Tree (BDT)[8] Neural Network (NN) correctly assign unlabeled samples subject referent periods.
 expect significant difference data periods be exploited BDT/NN.
 follows accuracy classification be proportional dissimilarity samples.
 selected referent periods hours be sensitive short duration incidents short capture long-term changes link performance.
 Subject period hour is minimal period expect human intervention happen case alert was received.
 approaches, flag reference data zero subject data one.
 Further, data reference subject periods are combined used train machine learning models.
 Training effectiveness was tested remaining data.
 A.
 Boosted Decision Trees decision tree is rule-based learning method [2] creates classification model, predicts value target variable learning simple decision rules inferred data features.
 decision tree depth is known decision stump.
 Decision trees make split based Gini impurity, measure randomly chosen element data set be labeled were labeled randomly.
 higher Gini impurity suggests pure split.
 set items ​A​ classes, ​i​ ϵ {1, … ​A​}.
 Let be fraction items labeled class ​i ​in set.
 Gini impurity be calculated follows [9]: Ig (p) = − ∑ i=1 pi examples decision trees stumps, Figures show decision trees different depths.
 Figure diagram is example decision tree depth
 Figure
 diagram is example different decision stumps decision trees depth
 Taking first stump example, tree splits data feature Gini impurity taking pieces sample data.
 was able separate data whereby right column has Gini impurity signifying was categorized.
 apply AdaBoost [10], boosted decision tree algorithm, datasets.
 Boosting is family machine learning algorithms start weak classifiers, i.e. classifiers label examples better random guessing, return weighted vote them, result is accurate labeling.
 Boosted decision trees train decision tree dividing data features.
 Then, validates decision tree training data find misclassified data values.
 value is assigned weight, determining importance.
 weight misclassified values is increased weight classified values is decreased, new tree is built.
 uses original tree formed tree tests training data find misclassified values.
 tree is formed, process is repeated many weak classifiers, i.e. estimators, desired.
 tree produce weighted vote something is anomaly.
 majority vote is used determine classification (e.g., anomaly not) datasets.
 B.
 Simple neural network recent advances hardware performance availability (GPUs), software stack (Keras, Tensorflow), became possible train neural networks large number trainable parameters.
 chose start neural network consisting input layer many ReLU activation neurons number time series data investigation, hidden layer many ReLU neurons sigmoid activated output neuron.
 selected topology sufficient capture effects training data.
 future work, investigate performance simpler, layer network simplify finding time series contributed period being flagged anomalous.
 order increase performance, optimize learning rates, try different neuron activations, optimizers, etc.
 Training testing data were prepared same way BDT model, training data gets shuffled epoch.
 train epochs batches samples using Adam optimizer [11] loss function binary cross-entropy.
 Training period takes seconds Tesla NVidia GPU times NVidia GTX How trained network performed test samples is given binary accuracy.
 anomaly is flagged based likely is accuracy magnitude happen pure chance.

 Experimental Results A.
 Boosted Decision Trees section, study effectiveness BDT detecting unusual events network performance data.
 analyze packet loss one-way delay.
 estimators were used testing data.
 AUC score was determined, score was certain threshold result was determined be anomaly.
 A.1. Decision Stumps vs.
 Decision Trees tested effectiveness boosted decision trees utilizing trees depth vs.
 boosted decision trees utilizing decision stumps simulated data.
 be seen Figures using decision stumps using decision trees produced equal results.
 methods were able identify anomalies hour length.
 is interesting note methods were able detect first anomaly.
 is likely anomalies occurred first day (our method uses day’s worth data training data), such did have enough training data identify them.
 results were comparable, decision stumps took average seconds hour worth data run, whereas decision trees took seconds hour worth data, times longer.
 determined decision stumps applied BDT were practical efficient method anomaly detection were used remainder experiment.
 Figure
 ​This graph shows results BDT using decision stumps.
 anomaly detection was performed simulated data period days.
 Red lines indicate anomalous data was generated, blue shading indicates algorithm predicted anomaly was, green is AUC score hour.
 Figure
 ​This graph shows results BDT using decision trees depth
 A.2. Anomaly Duration
 Degree Anomalous Behavior wanted know had greater effect interval was determined be magnitude offset anomaly normal data, duration anomalous behavior, number time series affected.
 anomalies were generated same set normal data.
 Figure shows visual representation said generated anomalies.
 start anomaly was separated hours.
 fluctuated duration anomalous number features affected, anomaly offset one.
 degree anomalous behavior encompasses number features affected anomaly offset.
 Data considered have affected many features affected times many data was (3 features vs
 Data long duration anomalous behavior was times data (3 hours vs
 generated offsets amplitudes: small large shift.
 Normal data had accuracy
 threshold anomalous behavior was determined AUC score
 results shown Table Figure see change anomaly offset had significant effect anomaly was determined be so.
 Holding number features affected duration anomalous behavior constant, increase anomaly offset increased AUC score increase.
 number features affected had significant effect.
 Holding anomaly offset anomaly duration constant, increase number features affected increased AUC score increase.
 duration anomalous behavior had smallest effect determining anomaly.
 Holding other variables constant, increase duration anomalous behavior decreased AUC score change.
 results suggest extent anomaly is considered such is dependent anomaly offset features affected, is, degree anomalous behavior, anomaly duration.
 Anomaly Offset [𝝈] Duration Features Affected [h] AUC Hour Hour Hour Table
 table shows AUC scores different simulated anomalies having combination anomaly duration, amplitude, number features affected.
 Anomaly numbers come order anomaly was generated.
 Results cut level are shown bold letters.
 Figure figure shows anomalies generated day period.
 anomaly has combination anomaly duration, amplitude, number features affected.
 A.3 Application real data tested effectiveness using boosted decision tree packet loss one-way delay data 4-day time span sites: PIC​and CERN-PROD.
 AUC threshold was set
 Figure is evident AUC threshold is high, detects anomalies be practical.
 Figure
 graph shows boosted decision tree applied one-way delay packet loss data PIC CERN-PROD
 light blue columns signify areas algorithm detected anomalous activity, using AUC threshold
 anomaly was were able generate feature importance time series ROC curve, allowing tell time serie(s) caused anomaly.
 Figure shows example said importance curve.
 Figure
 ​This figure shows first anomaly detected, relative importance features, ROC curve.
 looking feature importance, see feature had greatest influence AUC score, feature feature feature did factor generating anomaly.
 Due high frequency anomalies were detected, AUC threshold was determined be impractical.
 adjusting AUC score result seen Figure became practical be used.
 Figure graph shows boosted decision tree applied one-way delay packet loss data PIC CERN-PROD 30-day period.
 AUC threshold was
 B.
 Simple neural network preliminary study, used hidden layers many ReLU neurons time series single sigmoid output neuron.
 results network trainable parameters.
 result test is binary classification accuracy defined ratio labeled total number samples.
 distribution accuracies be expected chance depends number samples referent subject intervals be seen Figure
 figure shows Figure
 distribution accuracies obtained chance (binomial distribution) corresponding referent subject intervals sample second (p=0.92308).
 Accuracy larger ​0.9285​ happen <1% chance.
 B1.
 Applied simulated data, impact duration, magnitude anomaly, number affected time series used simulated data described ​A2​.
 period subject period was used reference.
 choice gives accuracy pure chance, consider detected calculated accuracy has less chance appearing (0.9285).
 results shown Table Figure see last anomalies (with offsets have been detected, first hour anomaly appearance.
 Accuracy levels first anomalies last hours fourth anomaly were threshold equal pure chance accuracy.
 be seen Figure loss accuracy change stepwise manner case backpropagation optimization does find minima result be equal chance.
 does limit applicability method are interested small effect anomalies affecting single time series, repeated identification same anomaly.
 Anomaly Offset [𝝈] Duration Features Affected [h] Accuracy hour hour hour hour Table
 table shows accuracies different simulated anomalies having combination anomaly duration, amplitude, number features affected.
 Anomaly numbers come order time anomaly was generated.
 Results cut level are shown bold letters.
 Figure
 figure shows anomalies generated day period.
 Anomalies are described ​Table
 last anomalies have been detected.
 Figure Loss accuracy training epochs.
 Anomaly threshold was reached epoch
 possible optimization is early training termination threshold has been reached.
 B3.
 Performance actual data using method actual data use referent interval hours average possible anomalies referent data longer periods.
 Data is measured packet loss CERN other sites have values measured covering day period.
 is data point minute.
 Missing data was filled values.
 Figure
 shows results obtained training network epochs interval, using batch size having split training testing data.
 Mean chance accuracy distribution is chance accuracy more has less probability.

 Figure
 Packet loss CERN other sites given IP address PerfSONAR node).
 Shown is square root value better visualize small values.
 blue line shows binary classification accuracy improvement chance value.
 Thick black line (“Detected”) marks intervals flagged anomalous.

 Conclusion paper presents new methods detecting network performance anomaly based split-sample classification: AdaBoost Simple neural network.
 methods are tested simulated datasets check sensitivity respect duration amplitude anomaly.
 boosted decision tree method proved be fast (4 seconds evaluation hour data tested) detected simulated anomalies.
 added benefit is returns ordered list series according contribution anomaly being flagged.
 selected AUC threshold is possible tune desired sensitivity/false positive level.
 simple neural network model used was hyper-parameter optimized network tried proved sensitive short low amplitude changes.
 Given are looking significant anomalies is good feature.
 evaluation is slower seconds hour data tested, is enough be practical use.
 significant issue is requires GPU processing.
 is three-layer network is difficult get information importance different time series resulting decision.
 results actual data are encouraging, using production different network configurations be tested (two layers, fewer neurons layer, etc.) hyper-parameter tuned.
 Appendix codes test data be found repository: https://github.com/ATLAS-Analytics/AnomalyDetection References [1] ​Tierney B., Metzger J., Boote J., Brown A., Zekauskas M., Zurawski J., Swany M., Grigoriev M., “perfSONAR: Instantiating Global Network Measurement Framework”, Workshop Real Overlays Distributed Systems (ROADS’09) Co-located ACM Symposium Operating Systems Principles (SOSP), January LBL-1452E.
 [2] V.
 Chandola, A.
 Banerjee, V.
 Kumar.
 “Anomaly detection: survey.” ACM Computing Surveys (CSUR), Volume Issue.
 July
 [3] M.Y. Su. “Real-time anomaly detection systems Denial-of-Service attacks weighted k-nearest-neighbor classifiers.” Expert Systems Applications.
 April
 [4] M.
 Sakurada, T.
 Yairi.
 “Anomaly Detection Using Autoencoders Nonlinear Dimensionality Reduction.” MLSDA'14 Proceedings MLSDA Workshop Machine Learning Sensory Data Analysis.
 December
 [5] S.
 Rajasegarar, C.
 Leckie, J.
 C.
 Bezdek M.
 Palaniswami, "Centered Hyperspherical Hyperellipsoidal One-Class Support Vector Machines Anomaly Detection Sensor Networks," IEEE Transactions Information Forensics Security, vol.
 no.
 pp.
 518-533, Sept.

 [6] Catmore, James.
 “Ideas Anomaly Detection Data Quality Monitoring.” University Oslo.
 June
 ​http://bit.ly/2GtohJ7 [7] N.
 R.
 Zhang D.
 O.
 Siegmund.
 “Model selection high-dimensional, multi-sequence changepoint problems.” Statistica Sinica,
 [8] ​Friedman, J.
 H.
 "​Stochastic Gradient Boosting.​" March
 [9] D.
 Coppersmith, S.J. Hong, J.R. Hosking.
 “Data Mining Knowledge Discovery.” June
 [10] Freund Y., Schapire R.E. “A decision-theoretic generalization on-line learning application boosting.” In: Vitányi P.
 (eds) Computational Learning Theory.

 Lecture Notes Computer Science (Lecture Notes Artificial Intelligence), vol

 [11] ​Kingma, D.
 P., Ba, J.
 L.
 (2015).
 Method Stochastic Optimization.
 International Conference Learning Representations,

 T O reconstruct complete precise geometry object is essential many graphics robotics applications, AR/VR [1] semantic understand- ing, robot grasping [2] obstacle avoidance.
 Classic approaches use off-the-shelf low-cost depth sensing devices such Kinect RealSense cameras recover shape object captured depth images.
 approaches require multiple depth images different viewing angles object estimate complete structure [3] [4] [5].
 However, practice is feasible scan surfaces object reconstruction, leads incomplete shapes occluded regions large holes.
 addition, acquiring processing multiple depth views require more computing power, is ideal many applications require real-time performance.
 paper, aim tackle problem estimating complete structure object using single depth view.
 is challenging task, partial observation object depth image view- ing angle) be associated inﬁnite number possible models.
 Traditional reconstruction approaches use interpolation techniques such plane ﬁtting, Laplacian hole ﬁlling [6] [7], Poisson surface estimation [8] [9] infer underlying structure.
 How- ever, recover limited occluded missing regions, small holes gaps due quantization arti- facts, sensor noise insufﬁcient geometry information.
 Interestingly, humans are good solving such ambiguity leveraging prior knowledge.
 • Bo Yang, Stefano Rosa, Andrew Markham Niki Trigoni are Department Computer Science, University Oxford, UK.
 E-mail:{bo.yang,stefano.rosa,andrew.markham,niki.trigoni}@cs.ox.ac.uk • Corresponding Hongkai Wen, is Department Computer Science, University Warwick, UK.
 E-mail: hongkai.wen@dcs.warwick.ac.uk example, given view chair rear legs occluded front legs, humans are able guess likely shape visible parts.
 Recent advances deep neural networks data approaches show promising results dealing task.
 paper, aim acquire complete high- resolution shape object given single depth view.
 utilizing high performance convolutional neu- ral nets large open datasets models, approach learns smooth function map view complete dense shape.
 particular, train end-to-end model estimates full volumetric occupancy single depth view object.
 state-of-the-art deep learning approaches [10] [11] [2] shape reconstruction single depth view achieve encouraging results, are limited small resolutions, scale voxel grids.
 result, learnt structure tends be coarse inaccurate.
 However, increase shape resolution sacriﬁcing recovery accuracy is challenging, higher resolution increase search space potential mapping functions, resulting difﬁculties convergence neural nets.
 Recently, deep generative models achieve impressive success modeling complex high-dimensional data dis- tributions, Generative Adversarial Networks (GANs) [12] Variational Autoencoders (VAEs) [13] emerge powerful frameworks generative learn- ing, including image text generation [14] [15], latent space learning [16] [17].
 past few years, number works [18] [19] [20] [21] applied such generative models learn latent space represent object shapes, order solve simple discriminative tasks such new image gener- ation, object classiﬁcation, recognition shape retrieval.
 paper, propose 3D-RecGAN++, novel model combines skip-connected autoencoder adver- sarial learning generate complete ﬁne-grained structure conditioned single view.
 Particularly, model encodes view low-dimensional latent space vector represents general geometric structures, decodes recover likely full shape.
 rough shape is fed conditional discriminator is trained distinguish coarse structure is plausible not.
 autoencoder is able approximate corresponding shape, adversarial training tends add ﬁne details estimated shape.
 ensure ﬁnal generated shape corresponds input single partial view, adversarial training model is based conditional GAN [22] random guessing.
 above novel efﬁcient network design excels competing approaches [2] [11] [23], use single connected layer [2], low capacity decoder [11], multi-stage inefﬁcient LSTMs [23] estimate full shapes.
 contributions are follows: propose novel generative model reconstruct complete accurate structure using single ar- bitrary depth view.
 Particularly, model takes simple occupancy grid map input requiring object class labels annotations, predicting compelling shape high resolution voxel grid.
 drawing autoencoder GAN, approach is end-to-end trainable high level generality.
 best knowledge, is ﬁrst work reconstructs such high resolution shapes using single view.
 (2) exploit conditional GAN training reﬁne shape estimated autoencoder.
 contribu- tion is use mean value latent vector feature, single scalar, output discriminator stabilize GAN training.
 (3) conduct extensive experiments single category multi-category object reconstruction, outperforming state art.
 Importantly, approach is able generalize unseen object categories.
 last, model performances real-world dataset collected Kinect, being trained synthetic datasets.
 (4) best knowledge, are good open datasets have ground truth occluded/missing parts holes view real world scenarios.
 therefore contribute real world testing dataset community.
 preliminary version work has been published ICCV workshops [24].
 code data are available at: https://github.com/Yang7879/3D-RecGAN-extended RELATED WORK review different pipelines reconstruction shape completion.
 conventional geometry based techniques state art deep learning based approaches are covered.
 Model/Shape Completion.
 [25] uses plane ﬁtting complete small missing regions, [26] [27] [28] [29] [30] apply shape symmetry ﬁll holes.
 methods show good results, relying predeﬁned geometric regularities limits structure space hand-crafted shapes.
 Besides, approaches are likely fail missing occluded regions are big.
 similar ﬁtting pipeline is leverage database priors.
 Given partial shape input, [31] [32] [33] [34] [35] [36] try retrieve identical most likely model align partial scan.
 However, approaches assume database contains identical similar shapes, being unable generalize novel objects categories.
 (2) Multiple RGB/Depth Images Reconstruction.
 Tra- ditionally, dense reconstruction requires collection RGB images [37].
 Geometric shape is recovered dense feature extraction matching [38], minimiz- ing reprojection errors [39] color images.
 Recently, [40] [41] [42] [43] [44] [45] leverage deep neural nets learn shape multiple RGB images.
 However, resolution recovered occupancy shape is small scale
 advancement depth sensors, depth images are used recover object shape.
 Classic approaches fuse multiple depth images iterative closest point (ICP) algorithms [3] [46] [47], recent work [48] learns shape using deep neural nets multiple depth views.
 (3) Single RGB Image Reconstruction.
 Predicting complete object model single view is long- standing challenging task.
 reconstruct- ing speciﬁc object category, model templates be used.
 example, morphable models are exploited face recovery [50].
 concept was extended reconstruct simple objects [51].
 general complex object recon- struction single RGB image, recent works [52] [53] [54] aim infer shapes using multiple RGB images weak supervision.
 However, training procedure [54] is stage end-to-end, [53] uses sim- ple autoencoder designing sophisticated learn- ing frameworks shape learning, [52] requires shape priors constraints.
 Shape prior knowedge is required [55] [56] [57].
 recover high resolution shapes, [58] [59] use Octree representation, [60] proposed inverse discrete cosine transform (IDCT) tech- nique.
 Lin al.
 [61] designed pseudo-renderer predict dense shapes, [62] estimates sketches dense shapes single RGB image.
 (4) Single Depth View Reconstruction.
 task reconstruction single depth view is complete occluded structures visible parts.
 ShapeNets [10] is early work using deep neural nets estimate shapes single depth view.
 Firman et al.
 [63] trained random decision forest infer unknown voxels.
 designed shape denoising, VConv- DAE [1] be used shape completion.
 facilitate robotic grasping, Varley et al.
 proposed neural network infer full shape single depth view [2].
 However, approaches are able generate low resolution voxel grids are less unlikely capture ﬁne geometric details.
 Recent works [11] [64] [23] [65] infer higher resolution shapes.
 However, 3D- EPN [11] relies shape database synthesize higher resolution shapes learning small voxel grid depth view, SSCNet [64] requires strong voxel-level annotations supervised scene completion semantic Fig.
 Overview network architecture training.
 Fig.
 Overview network architecture testing.
 generating training pairs, feed network.
 ﬁrst part network follows idea autoencoder U-net architecture [74].
 skip-connected autoencoder serves initial coarse generator is followed up-sampling module further generate high resolution shape voxel grid.
 whole generator aims learn correlation partial complete structures.
 super- vision complete generator is able learn function f infer reasonable shape given brand new partial view.
 testing phase, however, results tend be grainy ﬁne details.
 address issue, training phase, re- constructed shape generator is further conditional discriminator verify plausibility.
 particular, partial input view is paired corresponding complete shape, is called ‘real reconstruction’, partial view is paired corresponding output shape generator, is called ‘fake reconstruction’.
 discriminator aims discriminate ‘fake reconstruction’ ‘real recon- struction’.
 original GAN framework [12], task discriminator is classify real fake in- put, Jensen-Shannon divergence-based loss function is difﬁcult converge.
 recent WGAN [75] leverages Wasserstein distance weight clipping loss function stabilize training procedure, whilst extended work WGAN-GP [76] improves training process using gradient penalty respect input.
 3D- RecGAN++, apply WGAN-GP loss function conditional discriminator, guarantees stable convergence.
 overall network architecture training is shown Figure testing phase needs well trained generator shown Figure
 main challenge reconstruction arbitrary single view is generate new information includ- ing ﬁlling missing occluded regions unseen views, keeping estimated shape corresponding speciﬁc input view.
 training phase, 3D-RecGAN++ leverages skip-connected autoen- coder up-sampling module generate reasonable ‘fake reconstruction’ high resolution occupancy grid, applies adversarial learning reﬁne ‘fake reconstruction’ make similar ‘real recon- struction’ updating parameters generator.
 Fig.
 t-SNE embeddings partial views complete shapes multiple object categories.
 label prediction.
 [23] [65] are designed shape inpainting reconstructing complete structure partial depth view.
 re- cent 3D-PRNN [66] predicts simple shape primitives using RNNs, estimated shapes do have ﬁner geometric details.
 (5) Deep Generative Frameworks.
 Deep generative frameworks, such VAEs [13] GANs [12], have achieved impressive success image super-resolution [67], image generation [15], text image synthesis [68], etc.
 Recently, [69] [70] [71] [21] applied generative networks structure generation.
 However, most generate shapes random noise reconstructing structures speciﬁc single image.
 3D-RECGAN++ Overview method aims estimate complete dense structure object, takes arbitrary single depth view input.
 output shape is auto- aligned corresponding partial view.
 achieve task, object model is represented high resolution voxel grid.
 use simple occupancy grid shape encoding, represents occupied cell empty cell.
 Speciﬁcally, input partial view, denoted x, is 643 occupancy grid, output shape, denoted y, is high resolution probabilistic voxel grid.
 input partial shape is calculated single depth image given camera parameters.
 use ground truth dense shape aligned orientation same input partial depth view supervise network.
 generate ground training evaluation pairs, scan objects ShapeNet [72].
 Figure is t-SNE visualization [73] partial views corresponding full shapes multiple general chair bed models.
 green dot represents t-SNE embedding view, whilst red dot is embedding cor- responding shape.
 be seen multiple categories have similar mapping relationships.
 Essentially, neural network is learn smooth function, denoted f, maps green dots red dots possible high dimensional space shown Equation
 function f is parametrized convolutional layers general.
 Z = {0, (1) (cid:16) y = f (x) x ∈ Z encoderdecoderconditional discriminatorU-netinput viewtrue full shapereal reconstructionfake reconstructionxconcatconcatlossup-samplingyencoderdecoderU-netinput viewup-samplingxy testing phase, given novel view input, trained generator is able recover full shape satisfactory accuracy, discriminator is used.
 Architecture Figure shows detailed architecture proposed RecGAN++.
 consists main generator Figure discriminator Figure
 generator consists skip-connected autoencoder up-sampling module.
 vanilla GAN gen- erator generates data arbitrary latent distribu- 3D-RecGAN++ generator synthesizes data latent distributions views.
 Particularly, encoder has ﬁve convolutional layers, has bank ﬁlters strides followed leaky ReLU activation function max pooling layer ﬁlters strides
 number output channels max pooling layer starts doubling sub- sequent layer ends
 encoder is followed fully-connected layers embed semantic information latent space.
 decoder is composed symmetric up-convolutional layers are followed ReLU activations.
 Skip-connections encoder decoder guarantee propagation local structures input view.
 skip-connected autoencoder is fol- lowed up-sampling module consists layers up-convolutional layers detailed Figure
 simple efﬁcient up-sampling module upgrades output shape high resolution requiring complex network design operations.
 be noted connected layers skip-connections, vanilla autoencoder be unable learn reasonable complete structures latent space is limited local structure is preserved.
 efﬁcient up-sampling module, is unable generate high resolution shapes.
 Al- complicated dedicated network design output shapes, be unlikely be trained single GPU ex- high computation consumption high resolution shape generation.
 loss function optimization methods are described Section discriminator aims distinguish esti- mated shapes are plausible not.
 Based condi- tional GAN, discriminator takes real reconstruction pairs fake reconstruction pairs input.
 Particularly, consists convolutional layers, ﬁrst concatenates generated shape voxel grid) input partial view (i.e. voxel grid) is reshaped tensor.
 reshaping process is done using Tensorﬂow ‘tf.reshape()’.
 Basically, is inject condition information matched tensor dimension, leave network learn useful features condition input.
 convolutional layer has bank ﬁlters strides followed ReLU activation function last layer is followed sigmoid activation func- tion.
 number output channels convolutional layers starts doubling subsequent layer ends
 early stage GAN high dimen- sional real fake distributions overlap, discriminator separate using single scalar output, is analyzed [77].
 experiments, original WGAN-GP crashes early epochs due high dimensionality (i.e. + dimensions).
 stabilize it, propose use mean feature (i.e. mean vector feature) discriminator.
 mean vector feature captures more information input is difﬁcult discriminator distinguish mean feature is fake real input.
 enables useful information back- propagate generator.
 theoretical study mean feature matching method GAN is [78]; mean feature matching is applied [79] stabilize GAN.
 Therefore, discriminator is distinguish distri- butions mean feature fake real reconstructions, generator is trained make distributions mean feature similar possible.
 apply WGAN-GP loss functions modiﬁed mean feature matching.
 Objectives objective function 3D-RecGAN++ includes main object reconstruction loss (cid:96)ae genera- tor; objective function (cid:96)gan conditional GAN.
 (1) (cid:96)ae generator, inspired [80], use mod- iﬁed binary cross-entropy loss function stan- dard version.
 standard binary cross-entropy weights false positive false negative results equally.
 How- ever, most voxel grid tends be empty, net- work gets false positive estimation.
 regard, impose higher penalty false positive results false negatives.
 Particularly, weight hyper-parameter α is assigned false positives, (1-α) false negative results, shown Equation
 −α ¯yi log(yi)−(1−α)(1− ¯yi) log(1−yi) (2) (cid:20) N(cid:88) i=1 (cid:96)ae = (cid:21) ¯yi is target value {0,1} speciﬁc ith voxel ground truth grid ¯y, yi is corresponding estimated value (0,1) same voxel autoen- coder output y.
 calculate mean loss total N voxels whole voxel grid.
 (2) (cid:96)gan discriminator, leverage state art WGAN-GP loss functions.
 original GAN loss function presents overall loss real fake inputs, represent loss function (cid:96)g gan Equation generating fake reconstruction pairs (cid:96)d gan Equation discriminating fake real reconstruction pairs.
 Detailed deﬁnitions derivation loss functions be found [75] [76], modify conditional GAN settings.
 (cid:96)g gan = −E(cid:2)D(y|x)(cid:3) gan = E(cid:2)D(y|x)(cid:3) − E(cid:2)D(¯y|x)(cid:3) (cid:17)2(cid:21) (cid:20)(cid:16)(cid:13)(cid:13)∇ ˆyD(ˆy|x)(cid:13)(cid:13)2 − +λE (cid:96)d (3) (4) (a) Generator shape estimation single depth view.
 (b) Discriminator shape reﬁnement.
 Fig.
 Detailed architecture 3D-RecGAN++, showing main building blocks.
 Note are shown separate modules, are trained end-to-end.
 ˆy = ¯y + (1 − )y,  ∼ U [0, x is input partial depth view, y is corresponding output autoencoder, ¯y is corresponding ground truth.
 λ controls trade-off optimizing gradient penalty original objective WGAN.
 epochs.
 do use dropout batch normal- ization, testing phase is same training stage.
 whole network is trained single Titan X GPU scratch.
 generator 3D-RecGAN++ network, are loss functions, (cid:96)ae (cid:96)g gan, optimize.
 discussed Section minimizing (cid:96)ae tends learn overall shapes, whilst minimizing (cid:96)g gan estimates plausible structures conditioned input views.
 minimize (cid:96)d gan is improve performance discrimi- nator distinguish fake real reconstruction pairs.
 optimize generator, assign weights β (cid:96)ae (1− β) (cid:96)g gan.
 loss functions generator discriminator are follows: (cid:96)g = β(cid:96)ae + (1 − β)(cid:96)g gan (cid:96)d = (cid:96)d gan (5) (6) Training adopt end-to-end training procedure whole network.
 optimize generator discriminator, alternate gradient decent step discriminator step generator.
 WGAN-GP, λ is set gradient penalty [76].
 α ends modiﬁed cross entropy loss function, β is joint loss function (cid:96)g.
 Adam solver [81] is used discriminator generator batch size
 other Adam parameters are set default values.
 Learning rate is set discriminator generator Data Synthesis task dense reconstruction single depth view, obtaining large amount training data is obsta- cle.
 Existing real RGB-D datasets surface reconstruction suffer occlusions missing data is ground truth complete high resolution shapes single view.
 recent work 3D-EPN [11] synthesizes data object completion, map encoding scheme is complicated TSDF is different network requirement.
 tackle issue, use ShapeNet [72] database generate large amount training testing data rendered depth images corresponding complete shape ground truth.
 Particularly, subset object categories is selected experiments.
 category, generate training data CAD models, synthesizing testing data CAD models.
 CAD model, create virtual depth camera scan different uni- sampled views roll, pitch yaw space.
 virtual scan, depth image corre- sponding complete voxelized structure are generated regard same camera angle.
 depth image is transformed partial voxel grid using virtual camera parameters.
 pair partial view complete shape is synthesized.
 Overall, concat channel channels channels channels channels32768 channels83256 channels163128 channels32364 channels channels12838 channels25631 channelconcatconcatconcatconcat43 convlrelu23 maxpool43 convlrelu23 maxpool43 convlrelu23 maxpool43 convlrelu23 maxpoolflattendensereludenserelu43 deconvrelureshape43 deconvrelu43 deconvrelu43 deconvrelu43 deconvrelu43 deconvreluyx256x256x41 channelfake reconstructionreal reconstructionground truth128x128x130 channels6431 channely25631 channel256x256x2601 channel43 convlrelu43 convlrelu43 convlrelu43 convlrelu43 convlrelu43 convlreluflatten64x64x65 channels32x32x33 channels16x16x17 channels channels channelsxloss20480reshape25631 channelup-sampling moduleconcat channel channels channels channels channels32768 channels83256 channels163128 channels32364 channels channels12838 channels25631 channelconcatconcatconcatconcat43 convlrelu23 maxpool43 convlrelu23 maxpool43 convlrelu23 maxpool43 convlrelu23 maxpoolflattendensereludenserelu43 deconvrelureshape43 deconvrelu43 deconvrelu43 deconvrelu43 deconvrelu43 deconvreluyx256x256x41 channelfake reconstructionreal reconstructionground truth128x128x130 channels6431 channely25631 channel256x256x2601 channel43 convlrelu43 convlrelu43 convlrelu43 convlrelu43 convlrelu43 convlreluflatten64x64x65 channels32x32x33 channels16x16x17 channels channels channelsxloss20480reshape25631 channelup-sampling module training pairs testing pairs are gener- ated object category.
 data are produced Blender.
 large quantity synthesized data, collect real world data order test proposed network.
 use Microsoft Kinect camera scan set common objects, such chairs, tables, etc., multiple angles.
 Then, use ElasticFusion [47] reconstruct full shapes objects, camera pose scan.
 objects are segmented background.
 extract ground truth information aligning full objects partial views.
 be noted that, due noise quantization artifacts low-cost RGB-D sensors, inaccuracy algorithm, full ground truth is accurate.
 EVALUATION section, evaluate 3D-RecGAN++ com- parison state art approaches ablation study investigate proposed network.
 Metrics evaluate performance reconstruction, con- sider mean Intersection-over-Union (IoU) pre- dicted voxel grids ground truth.
 IoU individual voxel grid is deﬁned follows: (cid:80)N (cid:104) (cid:80)N i=1 (cid:2)I(yi > p) ∗ I( ¯yi)(cid:3) I(cid:0)I(yi > p) + I( ¯yi)(cid:1)(cid:105) i=1 IoU = I(·) is indicator function, yi is predicted value ith voxel, ¯yi is corresponding ground truth, p is threshold voxelization, N is total number voxels whole voxel grid.
 experiments, p is set predicted value is is likely be occupied probabilistic aspect.
 higher IoU value, better reconstruction model.
 Competing Approaches compare state art deep learning based approaches single depth view reconstruction.
 compare generator network, i.e. GAN, named 3D-RecAE short.
 3D-EPN.
 [11], Dai al.
 proposed neural network reconstruct shape voxel grid, high resolution shape is retrieved existing shape database.
 fair comparison, compared neural network performance.
 Besides, occupancy grid representation is used network training testing.
 (2) Varley et al.
 [2], network was designed complete shape single depth view robot grasping.
 output network is voxel grid.
 (3) Han et al.
 global structure inference network local geometry reﬁnement network are proposed complete high resolution shape noisy shape.
 network is designed single depth view reconstruction, output shape is voxel grid is comparable network.
 fair comparison, same occupancy grid representation is used net- work.
 be noted network involves many convoluted designs, training procedure is slow inefﬁcient due many LSTMs involved.
 3D-RecAE.
 3D-RecGAN++, remove discriminator keep generator infer complete shape single depth view.
 comparison illustrates beneﬁts adversarial learning.
 Single-category Results (1) Results.
 networks are trained tested different categories same network conﬁgurations.
 compare IoU different approaches, results voxel grids using max pooling stride axes.
 Table shows IoU comparison methods voxel grids, Table shows IoU comparison [23] approaches higher resolution voxel grids.
 Figure shows qualitative results single category re- construction.
 paper, meshgrid function Matlab is used plot shapes better visualization.
 (2) Analysis.
 proposed 3D-RecGAN++ outperforms competing approaches terms IoU lower (323 voxel grids) higher resolutions (2563 voxel grids).
 shapes generated 3D- RecGAN++ are compelling others terms shape accuracy geometrical details.
 TABLE Per-category IoU (323 voxel grids).
 3D-EPN [11] Varley al.
 [2] Han et al.
 [23] 3D-RecAE (ours) 3D-RecGAN++ (ours) bench chair coach table TABLE Per-category IoU (up voxel grids).
 Han et al.
 [23] (643) Han et al.
 [23] (1283) Han et al.
 [23] (2563) 3D-RecAE (ours) (643) 3D-RecAE (ours) (1283) 3D-RecAE (ours) (2563) 3D-RecGAN++ (ours) (643) 3D-RecGAN++ (ours) (1283) 3D-RecGAN++ (ours) (2563) bench chair coach table Multi-category Results (1) Results.
 networks are trained tested multiple categories being given class labels.
 networks are trained categories: {bench, chair, coach, table}; tested individual category.
 Table shows IoU comparison methods resolution voxel grids, Table shows IoU comparison [23] methods higher resolution voxel grids.
 Figure shows qualitative results approaches multiple categories.
 (2) Analysis.
 proposed 3D-RecGAN++ outperforms state art large margin categories are trained single model.
 Fig.
 Qualitative results per-category reconstruction different approaches.
 performance network trained multiple categories, does degrade compared training network individual categories.
 conﬁrms network has enough capacity capability learn diverse features multiple categories.
 TABLE Multi-category IoU (323 voxel grids).
 3D-EPN [11] Varley al.
 [2] Han et al.
 [23] 3D-RecAE (ours) 3D-RecGAN++ (ours) bench chair coach table Cross-category Results (1) Results.
 investigate generality net- works, train networks {bench, chair, coach, table}, test different categories: {airplane, car, faucet, guitar, gun, monitor}.
 categories, has single arbitrary views random selected objects testing, is same data size used previous {bench, chair, coach, table}.
 Table Han al.
 [23] (643) Han et al.
 [23] (1283) Han et al.
 [23] (2563) 3D-RecAE (ours) (643) 3D-RecAE (ours) (1283) 3D-RecAE (ours) (2563) TABLE Multi-category IoU (up voxel grids).
 table 3D-RecGAN++ (ours) (643) 3D-RecGAN++ (ours) (1283) 3D-RecGAN++ (ours) (2563) bench chair coach shows IoU comparison approaches voxel grids, Table shows IoU comparison [23] approaches higher resolution voxel grids.
 Figure shows qualitative results methods unseen categories.
 further evaluate generality 3D-RecGAN++ speciﬁc category.
 Particularly, conduct groups experiments.
 ﬁrst group, train 3D- RecGAN++ bench, test remaining categories: {chair, coach, table}.
 second group, input(643)3D-EPN(323)Varley et al.(403)Han et al.
 (2563)3D-RecAE(2563)3D-RecGAN++(2563)Ground Truth(2563) Fig.
 Qualitative results multi-category reconstruction different approaches.
 TABLE Cross-category IoU (323 voxel grids).
 airplane faucet guitar gun monitor 3D-EPN [11] Varley al.
 [2] Han et al.
 TABLE Cross-category IoU (up voxel grids).
 car 3D-RecAE (ours) 3D-RecGAN++ (ours) airplane car faucetguitar gun monitor Han et al.
 [23] (643) Han et al.
 [23] (1283) Han et al.
 [23] (2563) 3D-RecAE (ours) (643) 3D-RecAE (ours) (1283) 3D-RecAE (ours) (2563) 3D-RecGAN++ (ours) (643) 3D-RecGAN++ (ours) (1283) 3D-RecGAN++ (ours) (2563) network is trained chair tested {bench, coach, table}.
 Similarly, groups experiments are conducted.
 Basically, experiment is investigate approach learns features category generalizes different category, vice versa.
 Table shows cross-category IoU 3D-RecGAN++ trained individual category voxel grids.
 (2) Analysis.
 proposed 3D-RecGAN++ achieves higher IoU unseen categories compet- ing approaches.
 network learns rich features different object categories, is able generalize new types categories.
 implies network learn geometric relationships such lines, planes, curves are common various object categories.
 be observed model trained bench tends be general others, bench is likely have more general features be learned, simple categories such coach are unlikely consist many general features are shared different categories.
 Real-world Experiment Results (1) Results.
 Lastly, order evaluate domain adap- tation capability networks, train networks synthesized data categories {bench, chair, coach, table}, input(643)3D-EPN(323)Varley et al.(403)Han et al.
 (2563)3D-RecAE(2563)3D-RecGAN++(2563)Ground Truth(2563) Fig.
 Qualitative results cross-category reconstruction different approaches.
 TABLE Cross-category IoU 3D-RecGAN++ trained individual category (2563 voxel grids).
 Group (trained bench) Group (trained chair) Group (trained coach) Group (trained table) bench chair coach table test real-world data collected Mi- crosoft Kinect camera.
 real-world data were collected different environments, including ofﬁces, homes, outdoor university parks, shown Figure
 Compared synthesized data, real-world partial views are nois- ier incomplete.
 object, selected different depth views testing.
 Table shows IoU performance approaches using voxel grids, Table compares IoU [23] approaches higher resolutions.
 Figure shows qualitative results methods.
 (2) Analysis.
 are reasons IoU is lower compared testing synthetic dataset.
 ground truth objects obtained Elastic- Fusion are empty solid, are occupied surface.
 However, networks predict dense solid voxel grids, interior bulky objects couches is matching.
 Secondly, input depth view real world dataset is noisy incomplete, due limitation RGB-D sensor (e.g., reﬂective surfaces, outdoor light).
 many cases, input view does capture whole object contains small part object, leads failure cases (e.g. row Figure lower IoU scores overall.
 However, proposed network is able reconstruct reasonable dense shapes given noisy incomplete input input(643)3D-EPN(323)Varley et al.(403)Han et al.
 (2563)3D-RecAE(2563)3D-RecGAN++(2563)Ground Truth(2563) Fig.
 Real world objects reconstruction.
 depth views, competing algorithms (e.g. Varley al.) are robust real world noise unable generate compelling results.
 TABLE Real-world multi-category IoU (323 voxel grids).
 3D-EPN [11] Varley al.
 [2] Han et al.
 [23] 3D-RecAE (ours) 3D-RecGAN++ (ours) bench chair coach table TABLE Real-world multi-category IoU (up voxel grids).
 Han et al.
 [23] (643) Han et al.
 [23] (1283) Han et al.
 [23] (2563) 3D-RecAE (ours) (643) 3D-RecAE (ours) (1283) 3D-RecAE (ours) (2563) 3D-RecGAN++ (ours) (643) 3D-RecGAN++ (ours) (1283) 3D-RecGAN++ (ours) (2563) bench chair coach table Impact Adversarial Learning (1) Results.
 proposed 3D- RecGAN++ tends outperform ablated network RecAE does include adversarial learning GAN part.
 visualization experiment results, shapes 3D-RecGAN++ are compelling 3D-RecAE.
 further investigate adversarial learning improves ﬁnal results comparing 3D- RecAE, calculate mean precision recall above multi-category experiment results.
 Table shows mean precision 3D-RecGAN++ 3D-RecAE individual categories using network trained multiple categories, Table shows mean recall.
 (2) Analysis.
 be seen results 3D- RecGAN++ have much higher precision scores RecAE, means has less false positive estimations, 3D-RecAE tends estimate false positives.
 Therefore, estimated shapes are likely be ’fatter’ ’big- ger’, 3D-RecGAN++ tends output ’thinner’ shapes more shape details being exposed.
 3D- RecGAN++ 3D-RecAE achieve high re- call scores (i.e. 3D-RecGAN++ has lower recall scores compared 3D-RecAE.
 means 3D-RecGAN++ 3D-RecAE are capable estimat- ing object shapes many false negatives.
 other ground truth shape tends be subset estimated shape result.
 TABLE Multi-category mean precision (up voxel grids).
 3D-RecAE (323) 3D-RecAE (643) 3D-RecAE (1283) 3D-RecAE (2563) 3D-RecGAN++ (323) 3D-RecGAN++ (643) 3D-RecGAN++ (1283) (2563) bench chair coach table TABLE Multi-category mean recall (up voxel grids).
 3D-RecAE (323) 3D-RecAE (643) 3D-RecAE (1283) 3D-RecAE (2563) 3D-RecGAN++ (323) 3D-RecGAN++ (643) 3D-RecGAN++ (1283) (2563) bench chair coach table Overall, regard experiments per-category, multi-category, cross-category experiments, 3D- RecGAN++ outperforms others large margin, other approaches reconstruct reasonable shapes.
 terms generality, Varley et al.
 [2] Han et al.
 [23] are inferior [2] uses single connected layers, ConvNets, shape generation is unlikely be general various shapes, [23] applies LSTMs shape blocks generation is inefﬁcient Fig.
 Qualitative results real world objects reconstruction different approaches.
 unable learn general structures.
 However, 3D- RecGAN++ is superior thanks generality simple efﬁcient autoencoder convolutional dis- criminator.
 3D-RecAE tends over estimate shape, adversarial learning 3D-RecGAN++ is likely remove over-estimated parts, leave estimated shape be clearer more shape details.
 DISCUSSION 3D-RecGAN++ achieves state art performance object reconstruction single depth view, has limitations.
 Firstly, network takes volumetric representation single depth view input, taking raw depth image.
 Therefore, preprocessing raw depth images is required network.
 However, many application scenarios such robot grasping, such preprocessing be trivial straightforward given depth camera parameters.
 Sec- input depth view network contains clean object information cluttered background.
 possible solution is leverage existing segmentation algorithm such Mask-RCNN [82] segment target object instance raw depth view.
 CONCLUSION work, proposed novel framework 3D- RecGAN++ reconstructs full structure object arbitrary depth view.
 leveraging generalization capabilities autoencoders generative adversarial 3D-RecGAN++ predicts dense accurate structures ﬁne details, outperforming state art single-view shape completion individual object category.
 tested network’s ability reconstruct multiple categories providing object class labels training testing, showed network is able predict precise shapes.
 investigated network’s recon- struction performance unseen categories, proposed approach predict satisfactory structures.
 Finally, model is robust real world noisy data infer accurate shapes model is trained synthesized data.
 conﬁrms network has capability learning general latent features objects, ﬁtting function training datasets, adversarial learning 3D-RecGAN++ learns add geometric details estimated shapes.
 summary, network requires single depth view recover dense complete shape ﬁne details.
 input(643)3D-EPN(323)Varley et al.(403)Han et al.
 (2563)3D-RecAE(2563)3D-RecGAN++(2563)Ground Truth(2563) [2] REFERENCES [1] A.
 Sharma, O.
 Grau, M.
 Fritz, “VConv-DAE Deep Volumetric Shape Learning Object Labels,” ECCV,
 J.
 Varley, C.
 Dechant, A.
 Richardson, J.
 Ruales, P.
 Allen, “Shape Completion Enabled Robotic Grasping,” IROS,
 [3] R.
 A.
 Newcombe, S.
 Izadi, O.
 Hilliges, D.
 Molyneaux, D.
 Kim, A.
 J.
 Davison, P.
 Kohli, J.
 Shotton, S.
 Hodges, A.
 Fitzgibbon, “KinectFusion: Real-time dense surface mapping tracking,” ISMAR,
 [4] M.
 Nießner, M.
 Zollh¨ofer, S.
 Izadi, M.
 Stamminger, “Real-time reconstruction scale using voxel hashing,” ACM Transactions Graphics, vol.
 no.
 pp.

 [5] F.
 Steinbrucker, C.
 Kerl, J.
 Sturm, D.
 Cremers, “Large-Scale Multi-Resolution Surface Reconstruction RGB-D Sequences,” ICCV,
 [6] A.
 Nealen, T.
 Igarashi, O.
 Sorkine, M.
 Alexa, “Laplacian Mesh Optimization,” SIGGRAPH,
 [7] W.
 Zhao, S.
 Gao, H.
 Lin, “A robust hole-ﬁlling algorithm triangular mesh,” Visual Computer, vol.
 no.
 pp.

 [8] M.
 Kazhdan, M.
 Bolitho, H.
 Hoppe, “Poisson Surface Recon- struction,” Symposium Geometry Processing,
 [9] M.
 Kazhdan H.
 Hoppe, “Screened poisson surface reconstruc- tion,” ACM Transactions Graphics, vol.
 no.
 pp.

 [10] Z.
 Wu, S.
 Song, A.
 Khosla, F.
 Yu, L.
 Zhang, X.
 Tang, J.
 Xiao, “3D ShapeNets: A Deep Representation Volumetric Shapes,” CVPR,
 [11] A.
 Dai, C.
 R.
 Qi, M.
 Nießner, “Shape Completion using 3D- Encoder-Predictor CNNs Shape Synthesis,” CVPR,
 [12] I.
 J.
 Goodfellow, J.
 Pouget-Abadie, M.
 Mirza, B.
 Xu, D.
 Warde- Farley, S.
 Ozair, A.
 Courville, Y.
 Bengio, “Generative Adver- sarial Nets,” NIPS,
 [13] D.
 P.
 Kingma M.
 Welling, “Auto-Encoding Variational Bayes,” ICLR,
 [14] Z.
 Hu, Z.
 Yang, X.
 Liang, R.
 Salakhutdinov, E.
 P.
 Xing, “Controllable Text Generation,” ICML,
 [15] T.
 Karras, T.
 Aila, S.
 Laine, J.
 Lehtinen, “Progressive Growing GANs Improved Quality, Stability, Variation,” ICLR,
 [16] X.
 Chen, Y.
 Duan, R.
 Houthooft, J.
 Schulman, I.
 Sutskever, P.
 Abbeel, “InfoGAN: Interpretable Representation Learning In- formation Maximizing Generative Adversarial Nets,” NIPS,
 [17] T.
 D.
 Kulkarni, W.
 F.
 Whitney, P.
 Kohli, J.
 B.
 Tenenbaum, “Deep Convolutional Inverse Graphics Network,” NIPS,
 [18] E.
 Grant, P.
 Kohli, M.
 V.
 Gerven, “Deep Disentangled Repre- sentations Volumetric Reconstruction,” ECCV Workshops,
 [19] R.
 Girdhar, D.
 F.
 Fouhey, M.
 Rodriguez, A.
 Gupta, “Learning Predictable Generative Vector Representation Objects,” ECCV,
 [20] H.
 Huang, E.
 Kalogerakis, B.
 Marlin, “Analysis synthe- sis shape families deep-learned generative models surfaces,” Computer Graphics Forum, vol.
 no.
 pp.

 [21] J.
 Wu, C.
 Zhang, T.
 Xue, W.
 T.
 Freeman, J.
 B.
 Tenenbaum, “Learning Probabilistic Latent Space Object Shapes Generative-Adversarial Modeling,” NIPS,
 [22] M.
 Mirza S.
 Osindero, “Conditional Generative Adversarial Nets,” arXiv,
 [23] X.
 Han, Z.
 Li, H.
 Huang, E.
 Kalogerakis, Y.
 Yu, “High- Resolution Shape Completion Using Deep Neural Networks Global Structure Local Geometry Inference,” ICCV,
 [24] B.
 Yang, H.
 Wen, S.
 Wang, R.
 Clark, A.
 Markham, N.
 Trigoni, “3D Object Reconstruction Single Depth View Adver- sarial Learning,” ICCV Workshops,
 [25] A.
 Monszpart, N.
 Mellado, G.
 J.
 Brostow, N.
 J.
 Mitra, “RAPter: Rebuilding Man-made Scenes Regular Arrangements Planes,” ACM Transactions Graphics, vol.
 no.
 pp.

 [26] N.
 J.
 Mitra, L.
 J.
 Guibas, M.
 Pauly, “Partial Approximate Symmetry Detection Geometry,” SIGGRAPH,
 [27] M.
 Pauly, N.
 J.
 Mitra, J.
 Wallner, H.
 Pottmann, L.
 J.
 Guibas, “Discovering structural regularity geometry,” ACM Transac- tions Graphics, vol.
 no.
 p.

 [28] I.
 Sipiran, R.
 Gregor, T.
 Schreck, “Approximate Symmetry Detection Partial Meshes,” Computer Graphics Forum, vol.
 no.
 pp.

 [29] P.
 Speciale, M.
 R.
 Oswald, A.
 Cohen, M.
 Pollefeys, “A Symme- try Prior Convex Variational Reconstruction,” ECCV,
 S.
 Thrun B.
 Wegbreit, “Shape symmetry,” ICCV,
 [31] Y.
 M.
 Kim, N.
 J.
 Mitra, D.-M.
 Yan, L.
 Guibas, “Acquiring Indoor Environments Variability Repetition,” ACM Transactions Graphics, vol.
 no.

 [32] Y.
 Li, A.
 Dai, L.
 Guibas, M.
 Nießner, “Database-Assisted Ob- ject Retrieval Real-Time Reconstruction,” Computer Graphics Forum, vol.
 no.
 pp.

 [33] L.
 Nan, K.
 Xie, A.
 Sharf, “A Search-Classify Approach Cluttered Indoor Scene Understanding,” ACM Transactions Graphics, vol.
 no.
 pp.

 [34] T.
 Shao, W.
 Xu, K.
 Zhou, J.
 Wang, D.
 Li, B.
 Guo, “An interactive approach semantic modeling indoor scenes RGBD camera,” ACM Transactions Graphics, vol.
 no.
 pp.

 [35] Y.
 Shi, P.
 Long, K.
 Xu, H.
 Huang, Y.
 Xiong, “Data-driven contextual modeling scene understanding,” Computers Graphics, vol.
 pp.

 [36] J.
 Rock, T.
 Gupta, J.
 Thorsen, J.
 Gwak, D.
 Shin, D.
 Hoiem, “Completing Object Shape Depth Image,” CVPR,
 [37] R.
 Hartley A.
 Zisserman, Multiple View Geometry Computer Vision.
 Cambridge University Press,
 [38] R.
 A.
 Newcombe, S.
 J.
 Lovegrove, A.
 J.
 Davision, “DTAM: Dense Tracking Mapping Real-time,” ICCV,
 [39] S.
 Baker I.
 Matthews, “Lucas-Kanade Years Unify- ing Framework Part International Journal Computer Vision, vol.
 no.
 pp.

 [40] C.
 B.
 Choy, D.
 Xu, J.
 Gwak, K.
 Chen, S.
 Savarese, “3D- R2N2: A Uniﬁed Approach Single Multi-view Object Reconstruction,” ECCV,
 [41] X.
 Di, R.
 Dahyot, M.
 Prasad, “Deep Shape Low Number Silhouettes,” ECCV,
 [42] Z.
 Lun, M.
 Gadelha, E.
 Kalogerakis, S.
 Maji, R.
 Wang, “3D Shape Reconstruction Sketches Multi-view Convolu- tional Networks,”
 [43] D.
 J.
 Rezende, S.
 M.
 A.
 Eslami, S.
 Mohamed, P.
 Battaglia, M.
 Jader- berg, N.
 Heess, “Unsupervised Learning Structure Images,” NIPS,
 [44] A.
 Kar, C.
 H¨ane, J.
 Malik, “Learning Multi-View Stereo Machine,” NIPS,
 [45] M.
 Ji, J.
 Gall, H.
 Zheng, Y.
 Liu, L.
 Fang, “SurfaceNet: End- to-end Neural Network Multiview Stereopsis,” ICCV,
 [46] T.
 Whelan, J.
 McDonald, M.
 Kaess, M.
 Fallon, H.
 Johannsson, J.
 J.
 Leonard, “Kintinuous: Spatially Extended Kinectfusion,” RSS Workshops,
 [47] T.
 Whelan, S.
 Leutenegger, R.
 F.
 Salas-moreno, B.
 Glocker, A.
 J.
 Davison, “ElasticFusion Dense SLAM A Pose Graph,” RSS,
 [48] G.
 Riegler, A.
 O.
 Ulusoy, H.
 Bischof, A.
 Geiger, “OctNetFusion: Learning Depth Fusion Data,”
 [49] V.
 Blanz T.Vetter, “Face Recognition based Fitting Morphable Model,” IEEE Transactions Pattern Analysis Ma- chine Intelligence, vol.
 no.
 pp.

 [50] P.
 Dou, S.
 K.
 Shah, I.
 A.
 Kakadiaris, “End-to-end face reconstruction deep neural networks,” CVPR,
 [51] A.
 Kar, S.
 Tulsiani, J.
 Carreira, J.
 Malik, “Category-speciﬁc object reconstruction single image,” CVPR,
 [52] J.
 Gwak, C.
 B.
 Choy, M.
 Chandraker, A.
 Garg, S.
 Savarese, “Weakly supervised Reconstruction Adversarial Con- straint,” arXiv,
 [53] S.
 Tulsiani, T.
 Zhou, A.
 A.
 Efros, J.
 Malik, “Multi-view Su- pervision Single-view Reconstruction Differentiable Ray Consistency,” CVPR,
 [54] X.
 Yan, J.
 Yang, E.
 Yumer, Y.
 Guo, H.
 Lee, “Perspective Transformer Nets: Learning Single-View Object Reconstruction Supervision,” NIPS,
 [55] C.
 Kong, C.-H.
 Lin, S.
 Lucey, “Using Corresponding CAD Models Dense Reconstructions Single Image,” CVPR,
 [56] A.
 Kurenkov, J.
 Ji, A.
 Garg, V.
 Mehta, J.
 Gwak, C.
 Choy, S.
 Savarese, “DeformNet: Free-Form Deformation Network Shape Reconstruction Single Image,” NIPS,
 [57] J.
 K.
 Murthy, G.
 V.
 S.
 Krishna, F.
 Chhaya, K.
 M.
 Krishna, “Reconstructing Vechicles Single Image Shape Priors Road Scene Understanding,” ICRA,
 [58] H.
 Christian, S.
 Tulsiani, J.
 Malik, “Hierarchical Surface Pre- diction Object Reconstruction,”
 [59] M.
 Tatarchenko, A.
 Dosovitskiy, T.
 Brox, “Octree Generat- ing Networks: Efﬁcient Convolutional Architectures High- resolution Outputs,” ICCV,
 [60] A.
 Johnston, R.
 Garg, G.
 Carneiro, I.
 Reid, A.
 v.
 d.
 Hengel, “Scaling CNNs High Resolution Volumetric Reconstruction Single Image,” ICCV Workshops,
 C.-H.
 Lin, C.
 Kong, S.
 Lucey, “Learning Efﬁcient Point Cloud Generation Dense Object Reconstruction,” AAAI,
 [62] J.
 Wu, Y.
 Wang, T.
 Xue, X.
 Sun, W.
 T.
 Freeman, J.
 B.
 Tenenbaum, “MarrNet: Shape Reconstruction Sketches,” NIPS,
 [63] M.
 Firman, O.
 M.
 Aodha, S.
 Julier, G.
 J.
 Brostow, “Structured Prediction Unobserved Voxels Single Depth Image,” CVPR,
 [64] S.
 Song, F.
 Yu, A.
 Zeng, A.
 X.
 Chang, M.
 Savva, T.
 Funkhouser, “Semantic Scene Completion Single Depth Image,” CVPR,
 [65] W.
 Wang, Q.
 Huang, S.
 You, C.
 Yang, U.
 Neumann, “Shape In- painting using Generative Adversarial Network Recurrent Convolutional Networks,” ICCV,
 [66] C.
 Zou, E.
 Yumer, J.
 Yang, D.
 Ceylan, D.
 Hoiem, “3D-PRNN: Generating Shape Primitives Recurrent Neural Networks,” ICCV,
 [67] C.
 Ledig, L.
 Theis, F.
 Huszar, J.
 Caballero, A.
 Cunningham, A.
 Acosta, A.
 Aitken, A.
 Tejani, J.
 Totz, Z.
 Wang, W.
 Shi, “Photo-Realistic Single Image Super-Resolution Using Genera- tive Adversarial Network,” CVPR,
 [68] S.
 Reed, Z.
 Akata, X.
 Yan, L.
 Logeswaran, B.
 Schiele, H.
 Lee, “Generative Adversarial Text Image Synthesis,” ICML,
 [69] M.
 Gadelha, S.
 Maji, R.
 Wang, “3D Shape Induction Views Multiple Objects,”
 [70] E.
 Smith D.
 Meger, “Improved Adversarial Systems Object Generation Reconstruction,” CoRL,
 [71] A.
 A.
 Soltani, H.
 Huang, J.
 Wu, T.
 D.
 Kulkarni, J.
 B.
 Tenen- baum, “Synthesizing Shapes Modeling Multi-View Depth Maps Silhouettes Deep Generative Networks,” CVPR,
 [72] A.
 X.
 Chang, T.
 Funkhouser, L.
 Guibas, P.
 Hanrahan, Q.
 Huang, Z.
 Li, S.
 Savarese, M.
 Savva, S.
 Song, H.
 Su, J.
 Xiao, L.
 Yi, F.
 Yu, “ShapeNet: Information-Rich Model Repository,” arXiv,
 [73] L.
 v.
 d.
 Maaten G.
 Hinton, “Visualizing Data using t-SNE,” Journal Machine Learning Research, vol.
 no.
 Nov, pp.

 [74] O.
 Ronneberger, P.
 Fischer, T.
 Brox, “U-Net Convolutional Networks Biomedical Image Segmentation,” MICCAI,
 [75] M.
 Arjovsky, S.
 Chintala, L.
 Bottou, “Wasserstein GAN,” ICML,
 tion,” ICLR,
 ICCV,
 [76] I.
 Gulrajani, F.
 Ahmed, M.
 Arjovsky, V.
 Dumoulin, A.
 Courville, “Improved Training Wasserstein GANs,” NIPS,
 [77] M.
 Arjovsky L.
 Bottou, “Towards Principled Methods Training Generative Adversarial Networks,” ICLR,
 [78] Y.
 Mroueh, T.
 Sercu, V.
 Goel, “McGAN: Mean Covariance Feature Matching GAN,” ICML,
 [79] J.
 Bao, D.
 Chen, F.
 Wen, H.
 Li, G.
 Hua, “CVAE-GAN: Fine- Grained Image Generation Asymmetric Training,” ICCV,
 [80] A.
 Brock, T.
 Lim, J.
 M.
 Ritchie, N.
 Weston, “Generative Discriminative Voxel Modeling Convolutional Neural Networks,” NIPS Workshops,
 [81] D.
 P.
 Kingma J.
 Ba, “Adam: A method stochastic optimiza- [82] K.
 He, G.
 Gkioxari, P.
 Dollar, R.
 Girshick, R-CNN,”
 Deep learning is attractive area research constant growth [1].
 partic- ular, neuro-computing ﬁeld, study deep neural networks composed multiple non-linear layers has proved able learn feature representations higher levels abstraction, leading eminent performance e.g. vision tasks.
 Extending beneﬁts depth recurrent neural networks (RNNs) is intriguing research direction is gaining increas- ing attention [2].
 context, study deep RNNs has pointed organized recurrent models have potentiality devel- oping multiple time-scales representations input history internal states, be great help, e.g., dealing text processing tasks [3].
 recently, studies area Reservoir Computing (RC) [4, have shown ability developing such structured state space organization is intrinsic property layered RNN architectures
 study deep RC networks hand allowed development trained deep models learning temporal domain, other hand paved way further studies properties deep RNNs dynamics absence (or prior to) learning recurrent connections.
 aspect prominent relevance study dynamical models is rep- resented analysis memory abilities.
 paper, exploiting ground provided deep RC framework, address problem analyzing short-term memory capacity individual higher) layers deep recurrent architectures.
 Contributing highlight intrinsic di- versiﬁcation transient state dynamics constructed recurrent investigation aims shedding more light bias layering RNN architectural design.
 Framed RC area, analysis is intended provide insights process reservoir network construction.
 Deep Stacked RNN consider deep RNNs [3] recurrent architecture is obtained stacked composition multiple non-linear recurrent hidden layers, illustrated Fig.

 state computation proceeds following hierarchical network organization, lowest layer highest one.
 Speciﬁcally, time step t ﬁrst recurrent layer network is fed external input successive layer is fed activation previous one.
 Fig.
 Hierarchical organization hidden layers deep RNN.
 dynamical system perspective, deep RNN implements input- driven discrete-time non-linear dynamical system, state evolution layer i is ruled state transition function F (i).
 denote input dimension NU assume, sake simplicity, hidden following, use u(t) x(i)(t), layer contains NR recurrent units.
 respectively, indicate external input state i-th hidden layer time step t.
 Based notation, state ﬁrst layer is updated according following equation: x(1)(t) = F (1)(u(t), x(1)(t − = tanh(W(1)u(t) + ˆW(1)x(1)(t − (1) W(1) ∈ RNR×NU is input weight matrix ˆW(1) ∈ RNR×NR is recurrent weight matrix ﬁrst layer.
 successive layer i > state is updated according to: x(i)(t) = F (i)(x(i−1)(t), x(i)(t−1)) = tanh(W(i)x(i−1)(t)+ ˆW(i)x(i)(t−1)), (2) W(i) ∈ RNR×NR collects weights inter-layer connections layer i − layer i ˆW(i) ∈ RNR×NR is recurrent weight matrix layer i.
 Note eq.
 tanh non-linearity is used element-wise applied activation function recurrent units bias terms are omitted ease notation.
 is worth observing that, deep recurrent dynamics evolve whole, layer i state information coming previous level i − acts independent input information encodes history external input present time step.
 Taking aspects related learning recurrent connections (and speciﬁc aspects involved diﬀerent training strategies), focus analysis case untrained deep recurrent dynamics.
 do so, resort introduced deep RC framework [6], according recurrent part deep RNN architecture is left untrained initialization subject stability constraints [7].
 Speciﬁcally, network is initialized weights uniform distribution re-scaled control layer i values kW(i)k2 ρ( ˆW(i)), ρ(·) denotes spectral radius matrix argument (i.e. maximum eigenvalues magnitudes).
 quantities are hyper-parameters model inﬂuence state dynamics are set small values order guarantee stable regime, standard initialization approach trained networks.
 Notice framework allows hand investigate ﬁxed characterization state dynamics successive levels deep RC network, other hand study bias due layering deep recurrent architectures.
 Output computation is implemented using output layer size NY
 diﬀerent choices are possible state-output connection settings (see e.g. following analysis aims consider output modules are applied layer recurrent network.
 enables study characteristics state behavior emerging diﬀerent levels architecture.
 use linear output modules, such (i) outx(i)(t), matrices layer i output is computed y(i)(t) = W (i) ∈ RNY ×NR are trained layer individually, using direct method such pseudo-inversion.
 RC framework setting ensures same training cost layer.
 Experiments investigate short-term memory abilities deep RNN architectures resorting Memory Capacity (MC) task [9].
 aims measuring extent past input events be recalled present state activations.
 Speciﬁcally, recurrent system is tested ability reconstruct delayed versions stationary uni-variate driving input signal (NU = NY = MC layer i computed squared correlation coeﬃcient, follows: M C(i) = k=1 M C(i) k = k=1 Cov2(u(t − k), y(i) V ar(u(t)) V ar(y(i) k (t)) k (t)) (3) (i) y k (t) is activation output unit trained reconstruct u(t−k) signal state layer i, Cov V ar denote covariance variance operators.
 order exercise memory capability systems, used i.i.d. input signals uniform distribution unstructured temporal stream u(t) does carry information previous inputs


 u(t−2), u(t−1).
 task, considered time-step long sequence, ﬁrst time steps were used training1 remaining MC assessment.
 ﬁrst time steps were considered transient washout initial conditions.
 considered deep RNNs NL = recurrent layers, containing NR recurrent units.
 Input inter-layer weights were re- scaled such kW(i)k2 i =


 NL.
 Weights recurrent connections were re-scaled same spectral radius layers, i.e. ρ = ρ( ˆW(i)) i =


 NL, ρ values ranging [0.1,
 Note that, considered experimental settings, higher values ρ > network dynamics tend exhibit chaotic behavior, shown previous works terms local Lyapunov exponents [10,
 recurrent dynamics chaotic regimes are interesting practical point view, paper consider cases (ρ > scope analysis.
 choice ρ generated networks realizations (with diﬀerent seeds random generation), averaged achieved results such realizations.
 practical assessment MC values, is useful recall basic the- oretical result provided [9], states MC NR-dimensional recurrent system driven i.i.d. uni-variate input signal is upper bounded NR.
 considered maximum value delay k eq.
 equal twice size state space, is suﬃcient account correlations are involved experimental settings.
 Fig.
 shows MC values achieved correspondence higher layers architecture, diﬀerent cases ρ considered network initialization.
 Results point recurrent networks ordered regime (ρ exceeding higher layers deep architecture are biased short-term memory abilities.
 networks chaotic regime (ρ higher layers tend show poorer MC.
 MC performance shown Fig.
 has peak correspondence Spectral Radius Fig.
 MC diﬀerent layers deep RNNs increasing values ρ.
 ρ case score improves layer, layer.
 Interestingly, results highlight eﬀectiveness layering striking advantage convenient process RC networks architectural design.
 memory NR-dimensional reservoir be improved using underlying stack recurrent layers ﬁlter external input signal.
 Note such improvement comes only price increased cost state computation increases number layers), cost output training remains same.
 layer layer layer layer Delay (k) Fig.
 k-delay memory capacity deep RNN layers increasing height.
 further inquire memory structure developed layers deep RNNs analyzing MC values increasing delays.
 Fig.
 shows forgetting curves individual higher) layers, i.e. values M C(i) function k, obtained case ρ = plot Fig.
 reveals diversiﬁcation memory spans components deep recurrent architecture: higher layers are able store information past inputs longer times.
 layer memory recall is null delay dynamics developed 10th layer lead value is delay
 see input signals smaller delays are reconstructed lower layers, higher layers are characterized peak tends shift right (more evident layer Fig.
 slope forgetting curve tends be smoother.
 highlighted diversiﬁcation short-term memory spans successive layers deep RNN architecture is interesting way characterizing quantitative way) intrinsic richness state representations developed deep recurrent system.
 Conclusions paper have provided computational analysis short-term memory deep RNNs. do so, have resorted MC task mean quantify memory state dynamics successive levels deep recurrent system.
 results showed higher layers organized RNN architecture are learning recurrent connections, improved ability latch input information longer time spans.
 analysis provided paper revealed interesting insights diversiﬁcation memory structure developed deep stacked RNN dynamics, showing higher layers tend forget past input history compared lower ones.
 Furthermore, framed deep RC framework, results provided evidence support practical beneﬁt layered recurrent organization way improve memory skills reservoir networks cost-eﬀective fashion.
 Overall, further studies research direction are de- manded (e.g. theoretical side), believe outcomes provided paper contribute better understand characterize bias due layering deep recurrent neural models.
 References [1] I.
 Goodfellow, Y.
 Bengio, A.
 Courville.
 Deep learning.
 MIT press,
 [2] P.
 Angelov A.
 Sperduti.
 Challenges deep learning.
 Proc.
 24th European Symposium Artiﬁcial Neural Networks (ESANN), pages
 i6doc.com,
 [3] M.
 Hermans B.
 Schrauwen.
 Training analysing deep recurrent neural networks.
 NIPS, pages
 [4] D.
 Verstraeten, B.
 Schrauwen, M.
 d’Haene, D.
 Stroobandt.
 experimental uniﬁca- tion reservoir computing methods.
 Neural networks,
 [5] M.
 Lukoˇseviˇcius H.
 Jaeger.
 Reservoir computing approaches recurrent neural network training.
 Computer Science Review,
 [6] C.
 Gallicchio, A.
 Micheli, L.
 Pedrelli.
 Deep reservoir computing: critical experi- mental analysis.
 Neurocomputing,
 [7] C.
 Gallicchio A.
 Micheli.
 Echo state property deep reservoir computing networks.
 Cognitive Computation,
 [8] R.
 Pascanu, C.
 Gulcehre, K.
 Cho, Y.
 Bengio.
 construct deep recurrent neural networks.
 arXiv preprint arXiv:1312.6026v5,
 [9] H.
 Jaeger.
 Short term memory echo state networks.
 Technical report, German National Research Center Information Technology,
 [10] C.
 Gallicchio, A.
 Micheli, L.
 Silvestri.
 Local lyapunov exponents deep rnn.
 Proc.
 25th European Symposium Artiﬁcial Neural Networks (ESANN), pages
 i6doc.com,
 [11] C.
 Gallicchio, A.
 Micheli, L.
 Silvestri.
 Local lyapunov exponents deep echo state networks.

 (Accepted).
 Kernel methods make use non-linear patterns data whilst being able use linear solution methods, non-linear transformation data examples fea- ture space inner products correspond appli- cation kernel function data examples (Hof- mann et al.,
 Many kernel methods have been conceived direct application well-known linear methods feature space, reformulated be expressed form inner products.
 is case kernel PCA, obtained ap- plication linear PCA feature space (Sch¨olkopf al., involving eigendecomposition kernel matrix.
 has been shown outperform linear PCA number applications (Chin Suter,
 Incremental algorithms, solution is updated additional data examples, are desirable.
 data ar- Department Statistical Science University College London London WC1E United Kingdom p.northrop@ucl.ac.uk rives time solution is required additional data efﬁcient incremen- tal algorithms are available repeated applica- tion batch procedure.
 Furthermore, incremental al- gorithms have lower memory footprint batch counterparts.
 paper, propose novel algorithm incremen- tal kernel PCA, accounts change mean covariance matrix additional data example.
 works writing expanded mean-adjusted kernel matrix additional data point terms num- ber rank updates, rank update al- gorithm eigendecomposition be applied.
 use rank update algorithm based work Golub (1973) Bunch et al.
 (1978).
 few previous exact incremental algorithms kernel PCA have been proposed, are based application incremental linear PCA method feature space (Kim Chin Suter, Hoegaerts et
 Rank update algorithms eigendecomposition have been applied kernel PCA, best knowledge.
 mean feature vectors is adjusted, algorithm corre- sponds incremental procedure eigendecom- position kernel matrix, be applied.
 algorithm has same time memory complex- ities existing algorithms incremental kernel PCA is efﬁcient com- parable algorithm Chin Suter (2007), allows change mean.
 be con- sidered ﬂexible, is straightforward apply different rank update algorithm one have used, improved efﬁciency.
 Approximate algorithms be example ran- domized linear algebra (Mahoney,
 usefulness kernel methods is limited large computational requirements time memory, scale number data points, dimension transformed variables is large, are available, therefore ex- press solution terms transformed data examples.
 is true kernel PCA requires eigendecomposition kernel matrix, expen- sive operation.
 remedy, various approximate meth- ods have been introduced, such Nystr¨om method (Williams Seeger, creates low-rank approximation kernel matrix based randomly sampled subset data examples.
 extend algorithm incremental kernel PCA incremental calculation Nystr¨om approxi- mation kernel matrix.
 add data examples subset used create Nystr¨om ap- proximation kernel PCA.
 allows evalu- ate accuracy Nystr¨om approxima- tion added data example.
 Rudi al.
 presented incremental updating procedure Nystr¨om approximation kernel ridge regression, based rank updates Cholesky decomposition.
 proposed incremental procedure be applied kernel method requiring eigendecomposition in- verse kernel matrix.
 Combining incremental algorithm Nystr¨om method leads further improvements memory efﬁciency, compared ei- ther method own.
 BACKGROUND KERNEL METHODS Kernel methods allow application linear meth- ods discover non-linear patterns variables, non-linear transformation data points φ(x) feature space linear algorithms be ap- plied (Hofmann et
 rely things.
 calculation inner products trans- formed data examples symmetric positive def- inite kernel k(x, second, expression solution space transformed data space transformed variables.
 have set n observations {xi}n i=1.
 Linear meth- ods scale dimension observa- tions.
 xi is real vector xi = (x(1) ), linear method scale number variables d.
 Let xi be element set X
 general, further restrictions need be placed set X is great beneﬁt kernel methods.
 example, X be collection text strings graphs (Lodhi et al., ..., x(d) x(2) i.e. Vishwanathan et
 Let H be Hilbert space real-valued functions X inner product (cid:104)· ·(cid:105)H.
 X is vector space, H is closed sub- space X ∗, dual space bounded linear functionals X
 Consider H(cid:48), dual space linear functionals H.
 x∈X is element δx∈H(cid:48) such δx(f = f (x), termed evaluation functional.
 δx continuous), Riesz rep- is bounded (i.e. resentation theorem is unique element gx∈H such δx(f =(cid:104)gx, f(cid:105)H
 con- sider gx function x, say k(x,·), k(x,·) (cid:104)k(x,·), f (·)(cid:105)H = has reproducing property, f (x).
 Furthermore, reproducing property, have (cid:104)k(x,·), k(y,·)(cid:105)H = k(x, y).
 k(x, y) is symmetric positive deﬁnite function symmetric positive deﬁ- nite property inner product.
 function k(x,·) is denoted φ(x), termed feature map.
 space H has uncountable dimension, ev- ery (separable) Hilbert space is isomorphic (cid:96)2, space square-summable sequences (Bol- lob´as, element φ(xi) has representation vector φ(xi) = (φ1(xi), φ2(xi), ..., φd(xi)) R φk(xi)φk(xj).
 call feature vectors.
 However, representation is known, d is be possible apply linear method vari- ables φ1(x), φ2(x), ..., φd(x).
 Thanks represen- ter theorem (Sch¨olkopf solution in- stead be expressed terms elements H, (cid:104)φ(xi), φ(xj)(cid:105)H =(cid:80)d f (x) =(cid:80)n i=1 αik(xi, x) coefﬁcients αi.
 arrange feature vectors rows data matrix Φ.
 kernel matrix is given K := (k(xi, xj))∈Rn×n = ΦΦT
 KERNEL PCA PCA ﬁnds set orthogonal linear combinations variables maximizes variance linear com- bination turn.
 PCA be used dimensionality re- duction, regression classiﬁcation problems, detect outliers, other applications (Jolliffe,
 principal components are obtained calculating eigendecomposition sample covariance matrix n X T X, data matrix (centred) observations C = X, observation occupies row.
 gives decomposition C = V ΛV T columns V are directions maximum variance.
 principal components be obtained related sin- gular value decomposition (SVD).
 Assuming centered data, kernel PCA performs eigen- decomposition covariance matrix feature space (Sch¨olkopf ΦT Φv = λv ΦT Φ = V ΣV T
 Hence- resulting decomposition forth ignore factor n be con- cerned eigendecomposition ΦT Φ.
 Noting span{ΦT} = span{V write v terms n-dimensinal vector u v = ΦT u.
 Left-multiplying eigenvalue equation Φ obtain Ku = λu decomposition K = U ΛU T
 data vectors feature space are assumed be centred, need subtract mean variable Φ calculate eigendecomposition K(cid:48) = (Φ− = K − − K1n + (1) is n× n matrix (1n)i,j = element equal n.
 n, i.e. INCREMENTAL KERNEL PCA Incremental algorithms update existing solution several additional data examples, referred online learning.
 goal is specialized algo- rithms achieve greater time memory performance repeated application batch procedures.
 are many use cases incremental versions batch al- gorithms, example memory capacity is con- strained, data examples arrive time, termed streaming data, solution is desired additional data example.
 few algorithms exact incremental kernel PCA have been proposed.
 algorithm Chin Suter (2007) is based incremental linear PCA algorithm Lim al.
 (2004).
 time complexity is O(n3) memory complexity O(n2).
 Hoegaerts et al.
 (2007) kernel matrix expanded additional data example terms rank updates, ad- justing change mean, propose al- gorithm update subset m dominant eigenvalues corresponding eigenvectors.
 algorithm is ap- plied update eigenpairs, complexities time memory are O(n3) O(n2), respectively.
 Iterative algorithms produce sequence improving ap- proximate solutions converges exact solution number steps increases (Golub Van Loan,
 iterative algorithm be made op- erate incremental fashion, expanding data set additional data examples restart- ing iterative procedure.
 example iterative method kernel PCA be made operate in- crementally is kernel Hebbian algorithm (Kim based generalized Hebbian algorithm (Oja, applied feature space.
 Various approximations incremental kernel PCA have been proposed.
 See example Tokumoto Ozawa (2011) Sheikholeslami et al.
 (2015).
 present exact algorithm incremental kernel PCA, describe similar works further.
 THE NYSTR ¨OM METHOD Nystr¨om method (Williams Seeger, ran- domly samples m data examples full dataset, uniformly, calculates low-rank approxima- tion ˜K full kernel matrix ˜K = Kn,mK−1 m,mKm,n Kn,m is n× m matrix obtained choosing m columns original matrix K, Km,n is trans- pose Km,m contains intersection same m columns rows.
 KERNEL PCA THROUGH RANK ONE UPDATES section present algorithm incremen- tal kernel PCA based rank updates eigen- decomposition kernel matrix K, mean- adjusted kernel matrix K(cid:48).
 incremental algorithm eigendecomposition kernel matrix K be applied explicit implicit inverse same is required, such kernel regression kernel SVM.
 Various methods other kernel PCA are based eigendecomposition kernel matrix, such kernel FDA (Mika et
 efﬁcient solution methods are available, access eigendecomposition be useful statisti- cal regularization controlling numerical stability.
 contrast covariance matrix linear PCA, kernel matrix expands size additional data point, needs be taken account, ef- fect eigensystem determined.
 write ker- nel matrix K(cid:48) m+1,m+1 created m+1 data examples terms expansion sequence symmetric rank updates kernel matrix K(cid:48) m,m, apply rank update algorithm eigendecomposition K(cid:48) m,m obtain eigendecomposition K(cid:48) m+1,m+1.
 number algorithms have been suggested perform rank modiﬁcation symmetric eigenproblem.
 Golub (1973) presented procedure determine eigenvalues diagonal matrix updated rank perturbation.
 Bunch al.
 (1978) extended re- sults determination eigenvalues eigen- vectors arbitrary perturbed matrix, including im- proved procedure determine eigenvalues.
 Stability issues calculation eigenvectors, including loss numerical orthogonality, motivated several improvements Sorensen, Sorensen Tang, Gu Eisenstat,
 Alternatively, employ update algorithms singular value decomposition, such algorithm sug- gested Brand (2006) thin singular value decom- position.
 use rank update algorithm eigenvalues Golub (1973) determine eigenvectors according Bunch al.
 (1978).
 experiments approach seems be stable accurate most use cases.
 assume kernel matrix remains non-singular update.
 algorithm has same time memory complex- ities competing methods.
 algorithm com- parable ours is one Chin Suter (2007), accounts change mean.
 addi- tional data example is added incrementally, eigen- pairs are retained, requires eigendecomposition m+2×m+2 matrix, eigendecomposition m×m unadjusted kernel matrix, multiplication m×m matrices step.
 multiplication m×m matrices requires ﬂops, state- of-the-art QR algorithm symmetric eigenproblem ﬂops (Golub Van Loan, algo- rithm requires ﬂops O(m3) factor.
 proposed algorithm requires ﬂops O(m3) fac- tor mean is adjusted, ﬂops multiplication m+1×m+1 matrices rank update.
 algorithm is more efﬁcient.
 RANK UPDATE PROCEDURE m write K(cid:48) know eigendecomposition K(cid:48) m+1,m+1 terms ex- UmΛmU T pansion number symmetric rank updates K(cid:48) m,m, apply rank update algo- rithm obtain eigendecomposition K(cid:48) Um+1Λm+1U T m+1.
 Zero-mean data assume data examples have zero mean feature space, mean does need be updated previous data points Km,m needs be ex- panded additional row column.
 case devise rank update procedure Km,m Km+1,m+1 steps.
 denote ki,j =k(xi,xj) = [k1,m+1 k2,m+1 ··· km,m+1]T column vector elements k1,m+1, k2,m+1, ..., km,m+1 let km+1,m+1]T σ=4/km+1,m+1 km+1,m+1]T have Km+1,m+1= (cid:20)Km,m (cid:21) km+1,m+1 −σv2vT m,m+σv1vT :=K +σv1vT −σv2vT (2) m,m corresponding expansion Km,m K rank updates, is column vector ze- m,m ros.
 Compared eigensystem Km,m, K have additional eigenvalue λm+1= km+1,m+1 corresponding eigenvector um+1=[0 ···
 matrix K m,m is symmetric positive deﬁnite (SPSD), eigenvalues are positive.
 remain SPSD ﬁrst update, is sum SPSD matrices, v1vT is Gram matrix, element is seen separate vector.
 resulting ma- trix second update be SPSD holds Km+1,m+1.
 algorithm updating iteration is described Algorithm given function rankoneupdate(σ,v,L,U updates eigenvalues L eigenvectors U rank additive perturba- tion σvvT
 (cid:20)U i=1 row vector eigenvalues L ma- Algorithm Incremental eigendecomposition kernel Input: Dataset {xi}m+1 trix eigenvectors U Km,m; kernel function k(·,·) Output: Eigenvalues L eigenvectors U Km+1,m+1 L←[L km+1,m+1/4] U← km+1,m+1/4 sigma ←4/km+1,m+1 k1←[k1,m+1 k2,m+1
 km+1,m+1/2] k0←[k1,m+1 k2,m+1
 km+1,m+1/4] L,U←rankoneupdate(sigma, k1, L, U L,U←rankoneupdate(−sigma, k0, L, U (cid:21) limit kernel functions k(x,x) is constant, loss generality set k(x,x)=1 above expression simpliﬁes.
 m+1,m+1, elements K(cid:48) Mean-adjusted data construct rank update procedure K(cid:48) m,m K(cid:48) m,m need be ad- justed addition expansion row column.
 ﬁrst rank updates adjust mean K(cid:48) m,m account additonal data ex- ample.
 expand resulting matrix perform symmetric updates set last row column required values, (2).
 Recall taking mean account, per- forms eigendecomposition adjusted kernel ma- trix K(cid:48)=K−1nK+K1n−1nK1n.
 elements K(cid:48) m,m be adjusted following for- mula m+1,m+1)1:m,1:m K(cid:48)(cid:48) m,m:=(K(cid:48) m,m+1mKm,m+Km,m1m−1mKm,m1m =K(cid:48) +(−1m+1Km+1,m+1−Km+1,m+11m+1 +1m+1Km+1,m+11m+1)1:m,1:m (· )1:m,1:m denotes ﬁrst m rows columns matrix.
 latter terms are rank matrices.
 matrices −(1m+1Km+1,m+1)1:m,1:m are constant columns, hence rows Km,m1m− sum, (Km+1,m+11m+1)1:m,1:m.
 matrix has constant entries, equal sum elements Km,m multiplied factor (1m+1Km+1,m+11m+1)1:m,1:m.
 Consequently, terms be written rank updates.
 have (1mKm,m−1maT Km,m1m−(Km+1,m+11m+1)1:m,1:m m+1 m+1 (Km,m1m−a1T m) section is column vector ones.
 Km,m is symmetric m, have (Km,m1m)T (1m+1Km+1,m+1)1:m,1:m= (Km+1,m+11m+1)T set u= m(m+1) Km,m1m− m+1 a+ C1m C=− m2 Σm+ (m+1)2 Σm+1 mKm,m1m, sum have denoted elements Km,m, obtain K(cid:48)(cid:48) m,m+1muT +u1T (1m+u)(1m+u)T − =K(cid:48) m,m+ is symmetric rank updates K(cid:48) m,m.
 Σm Km,m1m be updated iterations (1m−u)(1m−u)T Σm+1=Σm+2aT Km+1,m+11m+1=[Km,m1m+a; aT [b; c] denotes column vector b expanded additional element c.
 expand K(cid:48)(cid:48) m,m K(cid:48) m+1,m+1, (2), taking adjusted mean account.
 required last row column is given v := k− m+1 (1m+11T m+1k+Km+1,m+11m+1 − m+1 Σm+11m+1) k=[aT k(xm+1,xm+1)]T
 let v1=[(v)1:m; σ=4/(v)m+1 v2=[(v)1:m; (v)m+1] (v)m+1] (v)1:m is vector ﬁrst m elements v, (v)m+1 is last element, have K(cid:48) m+1,m+1= +σv1vT −σv2vT (cid:20)K(cid:48)(cid:48) :=K (cid:21) −σv2vT m,m m,m+σv1vT (v)m+1 (3) have devised procedure update K(cid:48) m,m K(cid:48) m+1,m+1 using symmetric rank updates, rank eigendecomposition update algorithm be applied.
 full procedure is described Algo- rithm
 Note matrix K(cid:48) m,m expansion do need be kept memory.
 procedure is linear time memory, constituent quantities are updated incrementally.
 UPDATE ALGORITHM THE EIGENDECOMPOSITION describe algorithm updating eigen- decomposition rank perturbation.
 Suppose Algorithm Incremental eigendecomposition adjusted kernel matrix Input: Dataset {xi}m+1 i=1 row vector eigenvalues L ma- trix eigenvectors U Km,m; kernel function k(·,·); sum elements Km,m, denoted S; sum rows Km,m, Km,m1m, denoted K1 Output: Eigenvalues L eigenvectors U Km+1,m+1 a←[k1,m+1 k2,m+1
 km,m+1] S2←S +2∗sum(a)+km+1,m+1 C←−S/m2 +S2/(m+1)2 u←K1/(m∗(m+1))2−a/(m+1)+0.5∗C∗ones(m) L,U←rankoneupdate(0.5, L, U L, U K1←[K1+a sum(a)+k] S←S2 m←m+1 v←k−(ones(m)∗(sum(a)+k)+K1−S/m)/m v0←v[m] v←v[1:m−1] L←[L v0/4] U← v0/4 sigma←4/v0 v1←[v v0/2] v2←[v v0/4] L,U←rankoneupdate(sigma, v1, L, U L,U←rankoneupdate(−sigma, v2, L, U (cid:20)U know eigendecomposition symmetric matrix A=U ΛU T
 Let B =U ΛU T +σvvT =U (Λ+σzzT )U T z=U T v, look eigendecomposition ˜B =Λ+σzzT := ˜U ˜Λ ˜U T (Bunch et al.,
 eigendecomposition B is given U ˜U ˜Λ ˜U T U T unchanged eigenvalues eigenvectors U B := U ˜U, product orthogonal matrices is orthogonal eigendecomposition is unique, provided eigenvalues are distinct.
 eigenvalues ˜B be calculated O(n2) time ﬁnding roots secular equation (Golub, n(cid:88) i=1 ω(˜λ):=1+σ z2 λi−˜λ (4) eigenvalues modiﬁed system are subject following bounds λi≤˜λi≤λi+1 λn≤˜λn≤λn +σzT z λi−1≤˜λi≤λi λ1 +σzT z≤˜λ1≤λ1 i=1,2,...,n−1, σ>0 σ>0 i=2,3,...,n, σ<0 σ<0 (5) be used supply initial guesses root ﬁnding algorithm.
 Note expanding eigen- system, described eigenpairs need be re- ordered bounds be valid.
 updated eigenvalues have been calculated eigenvectors perturbed matrix B are given (Bunch et al., uB i = U D−1 i z i z(cid:107) (cid:107)D−1 (6) Di :=Λ−˜λiI.
 U D−1 are m×m Di is diagonal denominator is O(m) numer- ator is O(m2), leading O(m3) time complexity up- date eigenvectors.
 number ﬂops full procedure is +O(n2).
 Equation (6) requires cre- ation additional n×n matrix, hence full proce- dure is quadratic memory.
 INCREMENTAL NYSTR ¨OM section extend proposed algorithm in- cremental calculation Nystr¨om approximation kernel matrix.
 Having access incremental pro- cedure Nystr¨om method be useful.
 Different sizes subsets used approximation be evaluated, determine suitable size problem hand empirical investigation characteristics Nystr¨om method subsets dif- ferent sizes.
 large datasets, combination Nystr¨om method incremental calculation results further gains memory efﬁciency.
 Rudi al.
 proposed incremental algorithm Nystr¨om approximation applied ker- nel ridge regression, based rank updates Cholesky decomposition.
 proposed procedure be seen generalization work.
 best knowledge, is ﬁrst incremental algorithm calculation full Nystr¨om approximation ker- nel matrix.
 Given eigenvalues Λ eigenvectors U matrix Km,m, corresponding approximate eigenval- ues eigenvectors K are given (Williams Seeger, Λnys := U := (cid:114) Kn,mU Λ−1 (7) obtain procedure incremental ˜K = U nysΛnysU nysT calculate U Λ using Algorithm (2), iteration extra column Kn,m corresponding additional data example, calculate rescaling (7).
 rescaling Figure Difference batch incremental calculation K(cid:48) size datasets.
 has O(m2n) time complexity matrix product (7).
 Note proposed incremental calculation Nystr¨om approximation reproduces batch com- putation m, save numerical differences.
 accuracy Nystr¨om approximation has been exten- studied, including comparisons other meth- ods (Gittens Mahoney, Yang et
 EXPERIMENTAL ANALYSIS section present results number experiments1.
 run experiments differ- ent datasets UCI Machine Learning Reposi- tory (Lichman, simulated Magic gamma tele- scope dataset Yeast dataset, containing cellular protein location sites.
 applicable, remove target variable is categorical continu- ous.
 experiments use radial basis functions kernel (cid:18) −(cid:107)x−y(cid:107)2 (cid:19) k(x,y)=exp σ is parameter.
 dataset, set σ be median distances pairs data ex- amples (in subset full dataset), common heuris- tic.
 code Python is available https://github.com/cfjhallgren/inkpca INCREMENTAL KERNEL PCA implement evaluate algorithm incremen- tal kernel PCA adjustment mean feature vectors.
 Numerical accuracy is good, adjusting mean not.
 slight loss orthogonality is discov- ered eigenvectors, measured close U U T is identity, mean-adjusted data requires updates step involves nu- merical operations.
 have assumed kernel matrix re- mains full rank added data example.
 be case theory data contains noise, numerical rank deﬁciency cause issues practice.
 Equation (4) lack required num- ber roots.
 instance deﬂate ma- trix (see e.g. Bunch al.
 (1978) details), purposes experiments have contended excluding speciﬁc data example algorithm.
 excluded data point does add time overhead O(n3) factor.
 numerical operation leads small loss ac- curacy, due ﬁnite representation ﬂoating-point numbers, is propagated, varying severity, subsequent operations.
 incremental procedure involves more operations batch procedure, leads worse accuracy compari- son, termed drift.
 illustrate plotting Frobenius, spectral trace norms difference be- m×m adjusted kernel K(cid:48) m,m reconstruction using calculated eigen- decomposition, different numbers data points m, i.e. (cid:107)K(cid:48) m (cid:107).
 plot difference (cid:48)T m,m−U(cid:48) mΛ(cid:48) mU meantrace meanspectral mean01020304050607080m0.00000.00050.00100.00150.0020NormyeastFrobeniustracespectralFrobenius meantrace meanspectral mean Figure Difference K ˜K size datasets.
 run algorithm mean difference value m runs.
 Please see Figure
 drift reconstruction unadjusted matrix is smaller is plotted.
 results show drift is small.
 INCREMENTAL NYSTR ¨OM implement proposed incremental calculation Nystr¨om approximation, using ﬁrst observa- tions dataset.
 Having access incremen- tal algorithm calculating Nystr¨om approximation lets investigate approximation im- proves additional data point speciﬁc data set.
 calculate Frobenius norm, spectral norm trace norm difference Nystr¨om ap- proximation full kernel matrix step algorithm.
 norm be interest downstream machine learning practitioner (Gittens Mahoney,
 plot results run algorithm average runs.
 Please see Figure
 seen plots, Nystr¨om approximation seems provide high degree accuracy approximating matrix K, small number basis points.
 CONCLUSION have paper presented algorithm incre- mental kernel PCA based rank updates eigendecomposition kernel matrix K mean- adjusted kernel matrix K(cid:48), extended incre- mental calculation Nystr¨om approximation kernel matrix.
 Rank update algorithms eigen- decomposition other chosen paper be applied kernel PCA problem, improved accuracy efﬁciency, including algorithms conceived.
 be straightforward adapt proposed algorithm incremental kernel PCA maintain subset eigenvectors eigenvalues.
 incremental procedure Nystr¨om method aid determining suitable size subset used approximation empirical evaluation.
 limited amount work has been dedicated de- termination hyperparameter equivalent hyper- parameters other approximate kernel methods.
 Var- ious bounds statistical accuracy Nystr¨om method related approximations have been derived, guide choice hyperparameter, be suitable strategy.
 Acknowledgements like thank Ricardo Silva Department Statistical Science UCL helpful comments guidance.
 References Bollob´as, B.
 (1999).
 Linear analysis.
 Cambridge Uni- versity Press, Cambridge, UK, edition.
 Brand, M.
 (2006).
 low-rank modiﬁcations thin singular value decomposition.
 Linear Algebra Applications,
 Bunch, J.
 R., Nielsen, C.
 P., Sorensen, D.
 C.
 (1978).
 meantrace meanspectral mean01020304050607080m050100150200250NormyeastFrobeniustracespectralFrobenius meantrace meanspectral mean Mika, S., R¨atsch, G., Weston, J., Sch¨olkopf, B., M¨uller, K.-R.
 (1999).
 Fisher discriminant analysis kernels.
 Neural Networks Signal Process- ing IX: Proceedings IEEE Signal Process- ing Society Workshop, pages
 IEEE.
 Oja, E.
 (1982).
 Simpliﬁed model principal component analyzer.
 Journal Mathematical Biol- ogy,
 Rudi, A., Camoriano, R., Rosasco, L.
 (2015).
 Less is more: Nystr¨om computational regularization.
 Advances Neural Information Processing Systems, pages
 Sch¨olkopf, B., Herbrich, R., Smola, A.
 (2001).
 Computational generalized representer theorem.
 Learning Theory (COLT), pages
 Springer.
 Sch¨olkopf, B., Smola, A., M¨uller, K.-R.
 (1998).
 Nonlinear component analysis kernel eigenvalue problem.
 Neural computation,
 Sheikholeslami, F., Berberidis, D., Giannakis, G.
 B.
 (2015).
 Kernel-based low-rank feature extraction budget big data streams.
 IEEE Global Confer- ence Signal Information Processing (Global- SIP), pages
 IEEE.
 Sorensen, D.
 C.
 Tang, P.
 T.
 P.
 (1991).
 or- thogonality eigenvectors computed divide-and- SIAM Journal Numerical conquer techniques.

 Tokumoto, T.
 Ozawa, S.
 (2011).
 fast incremen- tal kernel principal component analysis learning stream data chunks.
 International Joint Confer- ence Neural Networks (IJCNN), pages
 IEEE.
 Vishwanathan, S.
 V.
 N., Schraudolph, N.
 N., Kondor, R., Borgwardt, K.
 M.
 (2010).
 Graph kernels.
 Journal Machine Learning Research,
 Williams, C.
 Seeger, M.
 (2001).
 Using Nystr¨om method speed kernel machines.
 Advances Neural Information Processing Systems, pages
 Yang, T., Li, Y.-F., Mahdavi, M., Jin, R., Zhou, Z.-H.
 (2012).
 Nystr¨om method vs random Fourier features: A theoretical empirical comparison.
 Advances Neural Information Processing Systems, pages
 Rank-one modiﬁcation symmetric eigenprob- lem.
 Numerische Mathematik,
 Chin, T.-J.
 Suter, D.
 (2007).
 Incremental kernel prin- cipal component analysis.
 IEEE Transactions Im- age Processing,
 Dongarra, J.
 J.
 Sorensen, D.
 C.
 (1987).
 par- allel algorithm symmetric eigenvalue problem.
 SIAM Journal Scientiﬁc Statistical Computing,
 Gittens, A.
 Mahoney, M.
 W.
 (2016).
 Revisiting Nystr¨om method improved large-scale machine Journal Machine Learning Research, learning.

 Golub, G.
 H.
 (1973).
 modiﬁed eigenvalue problems.
 Siam Review,
 Golub, G.
 H.
 Van Loan, C.
 F.
 (2013).
 Matrix com- putations.
 John Hopkins University Press, Baltimore, MD, edition.
 Gu, M.
 Eisenstat, S.
 C.
 (1994).
 stable efﬁcient algorithm rank-one modiﬁcation sym- metric eigenproblem.
 SIAM Journal Matrix Analy- sis Applications,
 Hoegaerts, L., De Lathauwer, L., Goethals, I., Suykens, J.
 A., Vandewalle, J., De Moor, B.
 (2007).
 Ef- updating tracking dominant kernel principal components.
 Neural Networks,
 Hofmann, T., Sch¨olkopf, B., Smola, A.
 J.
 (2008).
 Kernel methods machine learning.
 Annals Statistics,
 Jolliffe, I.
 (2002).
 Springer, New York, NY, edition.
 Principal component analysis.
 Kim, K.
 I., Franz, M.
 O., Sch¨okopf, B.
 (2005).
 It- erative kernel principal component analysis image modeling.
 IEEE Transactions Pattern Analysis Machine Intelligence,
 Lichman, M.
 (2013).
 UCI machine learning repository.
 Lim, J., Ross, D.
 A., Lin, R.-S., Yang, M.-H.
 (2004).
 Incremental learning visual tracking.
 Advances Neural Information Processing Systems, pages
 Lodhi, H., Saunders, C., Shawe-Taylor, J., Cristianini, N., Watkins, C.
 (2002).
 Text classiﬁcation using string kernels.
 Journal Machine Learning Research,
 Mahoney, M.
 W.
 (2011).
 Randomized algorithms ma- trices data.
 Foundations Trends R(cid:13) Machine Learning,
 Sigma-Pi-Sigma neural networks (SPSNNs) [1,4,7,8] kind high-order neural networks provide more powerful mapping capability traditional feedforward neural networks (Sigma-Sigma neural networks).
 SPSNN, Pi layer (denoted Π layer hereafter) is inserted Sigma layers.
 Pi node (Π node) Π layer corresponds monomial, variables are outputs Sigma nodes (Σ nodes) ﬁrst Sigma layer (Σ1 layer).
 node second Sigma layer (Σ2 layer) implements linear combination outputs Π layer, therefore represents multinomial expansion output σ = (σ1,··· σN Σ1 layer.
 multinomial expansion is processed activation function Σ2 layer give ﬁnal output network.
 beginning development SPSNN, researchers have realized is good idea include possible monomials Π layer, i.e., get complete multinomial expansion Σ1 layer, results many Π nodes Π layer.
 existing literature, order reduce number Π special multinomial Ps (called multi-linear multinomial) is used SPSNNs. monomials Ps are linear respect particular variable σi taking other variables constants.
 monomials such σn i σj n > are included Ps. intuitive idea strategy be following: A Π node receive most signal, more signals, node.
 i σn general numerical approximation point view, monomial plays important role approximating nonlinear mappings using multinomial.
 Thus, special multi-linear multinomial Ps be best choice SPSNN approximate particular nonlinear mapping.
 end, propose adaptive approach ﬁnd better multinomial given problem.
 elaborate, start complete multinomial given order.
 employ regularization technique learning process given problem reduce number monomials used multinomial, end modiﬁed SPSNN (MSPSNN) involving ∗W.
 Wu is corresponding author (e-mail: wuweiw@dlut.edu.cn).
 F.
 Li, K.S. Mohamed W.
 Wu are School Mathematical Sciences, Dalian University Technology, Dalian China.
 Y.
 Liu is School Information Science Engineering, Dalian Polytechnic University, Dalian, China.
 same number monomials (= number nodes Π layer) Ps. particular, smoothing L1/2 regularization term [10,15] is used example method, has been applied various kinds neural network regularization.
 divide learning process MSPSNN phases.
 ﬁrst phase is structural optimization phase.
 Here, insert regularization term usual error function SPSNN involving complete set multinomials, perform usual gradient learning process.
 end, delete Π nodes smaller Π-Σ2 weights, obtain network same number Π nodes Ps. second learning phase is reﬁnement phase.
 re-start gradient learning process network obtained ﬁrst learning phase, use weights survived ﬁrst phase initial weights.
 aim reﬁnement phase is make loss caused deleted nodes ﬁrst learning phase.
 Numerical experiments are performed benchmark problems including approximation problems classiﬁcation problems.
 is shown new MSPSNN behaves better traditional SPSNN Ps. rest paper is arranged follows.
 proposed MSPSNN smoothing L1/2 Section Supporting numerical simulations are regularization term is described Section
 presented.
 conclusions are given Section
 MSPSNN method smoothing L1/2 regularization Sigma-Pi-Sigma neural network SPSNN is composed input layer, hidden layers summation layer (Σ1 layer) product node layer (Π layer), output layer (Σ2 layer).
 numbers nodes layers are M + N, Q respectively.
 Denote x = (x0,· · ·, xM )T ∈ RM +1 input vector, M components x0,··· are “real” input, xM is extra artiﬁcial input, ﬁxed -1.
 output vector σ ∈ RN Σ1 layer respect x be written σ = (σ1,··· σN = (g(w1 · x), g(w2 · x),· · ·, g(wN · x))T (1) g(·) is given nonlinear activation function, wn = (wn0,· · ·, wnM )T ∈ RM +1 (1 ≤ ≤ N is weight vector connecting n-th summation node Σ1 layer input layer, wn · x denotes inner product wn x.
 remark component wnM represents bias n-th summation node Σ1 layer.
 Π Π node connects certain nodes Σ1 receives signals nodes, outputs particular monomial such (2) Denote ∧q (1 ≤ q ≤ Q) index set nodes Σ1 layer are connected q-th Π node.
 instance, let assume above examples (2) correspond ﬁrst, third ﬁfth nodes Π layer, respectively.
 Then, have σ1, σ1σ2,
 ∧1 = {1},∧3 = {1, = {1,
 (3) output vector τ = (τ1,··· τQ)T ∈ RQ Π layer is computed τq = σi, ≤ q ≤ Q.
 (cid:89) i∈∧q make convention τq =(cid:81) i∈∧q σi ≡ ∧q = φ, i.e., q-th Π node is concentrate attention choice ∧q’s, let describe output Σ2 layer.
 connected node Σ1 layer.
 choice ∧q’s is main concern paper.
 (4) output single node Σ2 layer, i.e., ﬁnal output network, is (5) f (·) is given activation function, w0 = (w0,1, w0,2,··· ∈ RQ is weight vector connecting Π layer Σ2 layer.
 network is used approximation problems, y = f (w0 · τ ), set f (t) = t.
 other hand, network is used classiﬁcation problems, f (t) is chosen be Sigmoid function.
 cases, see (1), (4) (5) input · τ Σ2 layer is multinomial expansion output values Σ1 layer, components τ correspond monomials, components w0 are coeﬃcients, involved multinomial expansion.
 comparison, recall usual feedforward neural input Σ2 layer is linear combination output values Σ1 layer.
 discuss choice ∧q’s detail explain main idea paper.
 convenience clarity, take third order multinomial variables example introduction section.
 Therefore, have N = i.e., Σ1 layer has nodes.
 consider choices ∧q’s, resulting diﬀerent multinomial expansions: complete mutinomial, linear multinomial (the traditional approach), adaptive multinomial (our proposed approach).
 choice complete mutinomial means input Σ2 layer is complete multinomial follows: w0,1 + w0,2σ1 + w0,3σ2 + w0,4σ3 + w0,5σ1σ2 + w0,6σ1σ3 + w0,7σ2σ3 + w0,8σ2 + w0,13σ1σ2 + w0,14σ3σ2 + w0,11σ2σ2 + w0,12σ3σ2 + w0,10σ2 + w0,9σ2 + w0,15σ1σ3 + w0,16σ3σ2 + w0,17σ3 + w0,18σ3 + w0,19σ3 + w0,20σ1σ2σ3.
 (6) see are twenty monomials multinomial expansion, corresponding twenty Π nodes Π layer.
 generally, Σ1 layer has N number monomials is CN N +3, grows N increases.
 Therefore, complete multinomial approach is good choice practice.
 complete = C3 traditional choice existing literature is linear multinomial approach: A linear multinomial is linear respect particular variable σi, other variables taken constants.
 linear multinomial corresponds (6) is w0,1 + w0,2σ1 + w0,3σ2 + w0,4σ3 + w0,5σ1σ2 + w0,6σ1σ3 + w0,7σ2σ3 + w0,8σ1σ2σ3.
 (7) see are monomials (7), i.e., nodes left Π layer.
 Generally, Σ1 layer has N number monomials is CN N
 Table shows comparison CN linear diﬀerent N
 be seen diﬀerence becomes bigger N increases.
 complete CN linear = C0 N + C1 N + C2 N + C3 Table Comparison CN complete CN linear diﬀerent N
 CN complete CN linear Diﬀerence network structure corresponding (7) is shown Fig.

 corresponding ∧q’s are follows: ∧1 = {φ},∧2 = {1},∧3 = {2},∧4 = {3},∧5 = {1, = {1, = {2, = {1,
 (8) observe (6) (7), ﬁrst product node, corresponding bias w0,1, does connect node Σ1 layer, ∧1 = {φ}.
 notice are repeated indexes ∧q (8).
 proposed choice is follows: start complete multinomial given order.
 employ regularization technique learning process reduce number monomials used multinomial, end new SPSNN involving same number monomials traditional choice.
 instance, Example given Section below, new SPSNN is obtained following multimonial: w0,1 + w0,2σ1 + w0,3σ2σ3 + w0,4σ1σ2 + w0,5σ2σ2 + w0,6σ3 + w0,7σ3 + w0,8σ3
 correspondingly, ∧1 = (∅),∧2 = {1},∧2 = {2, = {1, = {2, ∧5 = {2, = {1, = {2, = {3,
 (9) (10) notice are repeated indexes ∧q’s.
 Error function L1/2 regularization Let training samples be {xj, Oj}J M )T is j-th input sample Oj is corresponding ideal output.
 Let ∈ R (1 ≤ j ≤ J) be network output input xj.
 aim training process is build network such errors |yj − Oj| (1 ≤ j ≤ J) are small possible.
 conventional square error function regularization term is follows: j=1 ⊂ RM +1 × R, xj = (xj xj J(cid:88) J(cid:88) ˜E(W) = W = (wT wT ,··· N ), gj(t) = (yj − Oj)2 = gj(w0 · τ j), j=1 j=1 (g(t) − Oj)2, t ∈ R, ≤ j ≤ J.
 Let derive gradient error function ˜E(W).
 Notice i ,· · ·, σj ,··· τ j τ j = (τ j Q)T = τ j σj i (cid:89) i∈∧1 (cid:89) i∈∧2 (cid:89) i∈∧Q σj i )T (11) (12) (13) (14) (15) (16) (17) (18) σj = (σj σj σj N )T = (g(w1 · xj), g(w2 · xj),··· g(wN · xj))T
 partial derivative ˜E(W) respect w0,q (1 ≤ q ≤ Q) is ˜Ew0,q (W) = j(w0 · τ j)τ j g(cid:48) q
 J(cid:88) j=1 Moreover, ≤ n ≤ N ≤ m ≤ M ≤ q ≤ Q, have ∂τq ∂wnm i∈∧q\n σi)g(cid:48)(wn · x)xm, q (cid:54)= n ∈ ∧q, q = n ∧q.
 According (4) (16), ≤ n ≤ N, ≤ m ≤ M have ˜Ewnm(W) = j(w0 · τ j) g(cid:48) j(w0 · τ j) g(cid:48) Q(cid:88) (cid:88) q∈(cid:87) q=1 w0,q ∂τ j ∂wnm (cid:89) w0,q( i∈∧Q\n i )g(cid:48)(wn · xj)xj σj m, (cid:40) ((cid:81) J(cid:88) J(cid:88) j=1 j=1 ∂τ j ∂wnm denotes value ∂τq ∂wnm σi = σj i x = xj (16).
 error function L1/2 regularization term is E(W) = ˜E(W) + λ[ |w0,q|1/2 + |wnm|)1/2].
 Q(cid:88) q=1 N(cid:88) M(cid:88) m=0 n=1 gradient method L1/2 regularization training network is: Starting arbitrary initial value W0, weights are updated by: Wk+1 = Wk − (cid:52)Wk. Here, (cid:52)Wk = ((cid:52)wk ,(cid:52)wk ,(cid:52)wk ,(cid:52)wk J(cid:88) = −ηEw0,q (Wk) = −η[ (cid:52)wk N M )T g(cid:48) j(wk · τ j)τ j q + j=1 (19) (20) λsgn(wk (cid:52)wk nm = −ηEwnm(Wk) = −η[ g(cid:48) j(wk · τ k,j) J(cid:88) j=1 (cid:88) q∈∨n (cid:89) i∈∧Q\n σk,j wk )g(cid:48)(wk n · xj)xj m λsgn(wk nm) n0| + ··· + |wk ].
 (21) Here, ≤ j ≤ J; ≤ n ≤ N ≤ m ≤ M ≤ q ≤ Q; k = η > is learning rate; λ > is regularization parameter.
 Error function smoothing L1/2 regularization note usual L1/2 regularization term (18) is non-diﬀerentiable function origin.
 previous studies [][], has been replaced smoothing function follows E(W) = ˜E(W) + λ[ |f (w0,q)|1/2 + |f (wnm)|)1/2], N(cid:88) M(cid:88) m=0 Q(cid:88) (cid:40)|x|, q=1 (22) (23) f (x) is following piecewise multinomial function: f (x) = is easy obtain − x4 + + |x| ≥ a, |x| <
 f (x) ∈ [ +∞), f(cid:48)(x) ∈ [−1, f(cid:48)(cid:48)(z) ∈ [0, ].
 gradient error function be written EW(W) = (Ew0,1(W), Ew0,2(W),··· Ew0,Q(W), Ew10(W), Ew11(W),··· EwNM (W))T (24) Ew0,q (W) = Ewnm(W) = J(cid:88) J(cid:88) j=1 j=1 q + j(w0 · τ j)τ j g(cid:48) (cid:88) j(w0 · τ j) g(cid:48) q∈∨n λf(cid:48)(w0,q) (w0,q))1/2 (cid:89) w0,q( i∈∧Q\n i )g(cid:48)(wn · xj)xj σj m + λf(cid:48)(w0,q) (wn0) + ··· + f (wnm))1/2 ).
 Starting arbitrary initial value W0, gradient method smoothing L1/2 regular- ization updates weights {Wk} Wk+1 = Wk − (cid:52)Wk (25) = −ηEw0,q (Wk) = −η[ J(cid:88) j=1 g(cid:48) j(wk · τ k,j)τ j q + λf(cid:48)(wk (wk (26) (cid:52)wk = −η[ = −ηEnm(Wk) J(cid:88) · τ k,j) g(cid:48) j(wk j=1 (cid:88) q∈∨n (cid:89) i∈∧Q\n σk,j wk )g(cid:48)(wk n · xj)xj m + λf(cid:48)(wk (wk n0) + ··· + f (wk nM ))1/2 ], (27) ≤ j ≤ J; ≤ n ≤ N ≤ m ≤ M ≤ q ≤ Q; k = η > is learning rate; λ > regularization parameter.
 Algorithm As mentioned Introduction, divide learning process phases: structural optimization phase choosing structure network, followed reﬁnement phase choosing weights.
 Detailed descriptions training phases are given following Algorithms respectively.
 Structural optimization Input.
 Input dimension M number N Σ1 nodes, number Q Π maximum iteration number I, learning rate regularization parameter λ, training samples {xj, Oj}J ∈ RQ Initialization.
 Initialize randomly initial weight vectors w0 w0 n = (w0 Training.
 k = · ·, do )T ∈ RM +1 (1 ≤ ≤ N ).
 j=1 ⊂ RM +1 × R.
 w0 n1,··· = (w0 Compute error function (22).
 Compute gradients (26) (27).
 Update weights wk wk n (1 ≤ ≤ N using (25).
 end Structural optimization.
 ˆQ = CN Output.
 ﬁnal weight vectors ˆw0 ˆwn = wI largest weights absolute value form vector ˆw0 = ˆw1, ˆw2,··· ˆw ˆQ}.
 = (wI n (1 ≤ ≤ N ).
 obtained weight vector wI w0,Q)T select Algorithm Reﬁnement training Input.
 Input dimension M number N Σ1 nodes, number ˆQ Π maximum iteration number K, learning rate η, training samples {xj, Oj}J j=1 ⊂ RM +1 × R.
 Initialization.
 Set w0 Reﬁnement Training.
 k = K do = ˆwn (1 ≤ ≤ N ), λ =
 = ˆw0 w0 Compute error function (22).
 Compute gradient weights (cid:52)wk Update weights wk (cid:52)wk n (1 ≤ ≤ N (25).
 wk n (1 ≤ ≤ N (26) (27), respectively.
 end Output.
 ﬁnal weight vectors wK wK n (1 ≤ ≤ N ).
 Numerical experiments section, proposed method is performed numerical benchmark problems: Mayas’ function problem, Gabor function problem, Sonar problem Pima Indians diabetes data clas- siﬁcation diﬀerent learning rates.
 Example Mayas’ function approximate example, network is considered approximate Mayas’ function below: HM (x, y) = + y2) −
 training samples network are input points selected × grid −0.5 ≤ x ≤ −0.5 ≤ y ≤ Similarly, test samples are input points selected grid −0.5 ≤ y ≤ −0.5 ≤ y ≤ performing Algorithms η = λ = iterationmax = select approximate complete multinomial.
 monomials, σ2σ3, σ1σ2 new structure corresponds Fig.

 new weighted linear combination is σ3σ2 σ3 σ3 σ3 w0,1 + w0,2σ1 + w0,3σ2σ3 + w0,4σ1σ2 + w0,5σ2σ2 + w0,6σ3 + w0,7σ3 + w0,8σ3 (29) Fig.
 ﬁrst product node, corresponding bias w0,1, does connect node Σ1 layer, ∧1 = φ.
 have ∧1 = ∅,∧2 = {1},∧2 = {2, = {1, = {2,
 ∧5 = {2, = {1, = {2, = {3, (30) Then, perform use test samples evaluate method.
 average error diﬀerent parameter η tests improvement performance have been shown Table
 persuasive comparison shows new structure attains best eﬀectiveness, i.e., smallest error.
 Fig.
 see surface Mayas’ error function new structures is decreasing converge
 Table Comparison average error Mayas’ approximate error function.
 Learning Rate Average Old Average New Improvement % Figure Comparison error Mayas approximation problem.
 Example Gabor function approximate example, MPSPNN is used approximate Gabor function as: (cid:18) x2 + y2 (cid:19) HG = exp cos(2π(x + y)) (31) training samples neural network are input points selected × grid −0.5 ≤ x ≤ −0.5 ≤ y ≤ Similarly, test samples are input points selected grid −0.5 ≤ y ≤ −0.5 ≤ y ≤ performing Algorithms η = λ = iterationmax = σ1, σ2σ3, σ1σ2 are selected approximate complete multinomial.
 new structure corresponds Fig.
 new weighted linear combination is σ3 w0,1 + w0,2σ1 + w0,3σ2σ3 + w0,4σ1σ2 + w0,5σ2σ2 + w0,6σ3 + w0,7σ3 + w0,8σ3 (32) have ∧1 = ∅,∧2 = {1},∧3 = {2, = {1, = {2, = {1, = {2, = {3, (33) Then, perform use test samples evaluate method.
 average error improvement performance have been shown Table
 results show new structure attains smallest error.
 Fig.
 see surface Gabor error function new structures is decreasing converge predicted Theorem
 Table Comparison average error Gabor approximate error function.
 Learning Rate Average Old Average New Improvement % Figure Comparison error Gabor approximation problem.
 Example Sonar data classiﬁcation Sonar problem is well-known benchmark dataset, aims classify reﬂected sonar signals categories (metal cylinders rocks).
 related data set comprises input vectors, components.
 example, 4-fold cross validation is used perform experiments, is, samples training samples testing are selected samples.
 performing method, σ3, σ1σ2, σ2 are selected approximate complete multinomial.
 new structure corresponds Fig.
 new weighted linear combination is σ2 w0,1 + w0,2σ3 + w0,3σ1σ2 + w0,4σ2 + w0,5σ2 + w0,6σ2σ2 + w0,7σ2σ2 + w0,8σ3 (34) Then, have ∧1 = ∅,∧2 = {2},∧3 = {1, = {1, = {3, = {1, = {2, = {2, (35) Table Comparison average classiﬁcation accuracy sonar problem.
 Round Old Train New Train Impprovment% Old Test New Test Improvement% Overall Table Comparison best classiﬁcation accuracy sonar problem.
 Round Old Train New Train Impprovment% Old Test New Test Improvement% Overall Table Comparison worst classiﬁcation accuracy sonar problem.
 Round Old Train New Train Impprovment% Old Test New Test Improvement% Overall structures, trials are carried learning algorithm.
 Tables compare average accuracy, best accuracy worst accuracy classiﬁcation structures, respectively.
 be seen new structure is advantageous old structure.
 show new structure is decreasing converge iterative learning predicted Theorem
 Example Pima Indians diabetes data classiﬁcation verify theoretical evaluation MSPSNNs, used Pima Indians Diabetes Database, comprises samples attributes.
 dataset is available UCI machine learning repository (http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes).
 4-fold cross validation is used perform method.
 σ1σ2, σ1σ3, σ1σ4, σ2σ3, σ2σ4, σ3σ4 σ2 σ2 σ2 σ2 σ1σ2 σ3σ2 σ1σ2 σ4σ2 are selected.
 new structure corresponds Fig.
 new weighted linear combination is w0,1 + w0,2σ1σ2 + w0,3σ1σ3 + w0,4σ1σ4 + w0,5σ2σ3 + w0,6σ2σ4 + w0,7σ3σ4 + w0,8σ2 + w0,13σ3σ2 + w0,15σ4σ2 + w0,14σ1σ2 + w0,10σ2 + w0,11σ2 + w0,12σ1σ2
 (36) Then, have ∧1 = ∅,∧2 = {1, = {1, = {1, = {2, = {2, = {3, = {1,
 ∧9 = {2, = {3, = {4, = {1, = {2, = {1, = {3, (37) results comparative analysis experiments using old new structure four-order are presented, paying particular attention average error, average best error average wort correct classiﬁcation shown Tables
 lead verify theoretical evaluation SPSNNs learning new structure is decreasing converge iterative learning predicted Theorem
 Conclusion study, use smoothing L1/2 regularization select appropriate terms approximate complete Kolmogorov-Gabor Multinomial product layer SP- SNNs.Numerical experiments are implemented Mayas’ function problem, Gabor function prob- lem, Sonar data classiﬁcation Pima Indians diabetes data classiﬁcation.
 numerical results (a) (b) (c) (d) Figure (a) Old structure; (b) New structure based Mayas’ function approximation; (c) New structure based Gabor function approximation; (d) New structure based Sonar problem ap- proximation.
 Table Comparison average error Pima Indians problem.
 Round Old Train New Train Impprovment% Old Test New Test Improvement% Overall Table Comparison best error Pima Indians problem.
 Round Old Train New Train Impprovment% Old Test New Test Improvement% Overall demonstrate terms, product nodes selected using smoothing L1/2 regularization have been proved capability providing more possibility powerful mapping, is diﬀerent old structure.
 References [1] C.K. Li, A sigma-pi-sigma neural network, Neural Process.
 Lett.
 pp.
 1-9.
 ∏ ∏ ∏ ∏ ∏ ∏ ∏ Input layer ∑1 ∏ x ∈ RM ∑2 τ ∈ RQ σ ∈ RN w0,8 w0,1 wNM w11 ∑ ∑ ∑ ∑ x1 xM x2 x3 ∏ ∏ ∏ ∏ ∏ ∏ ∏ ∏ ∏ ∏ ∏ Input layer ∑1 ∏ x ∈ RM ∑2 τ ∈ RQ σ ∈ RN w0,8 w0,1 wNM w11 ∑ ∑ ∑ ∑ x1 xM x2 x3 ∏ ∏ ∏ ∏ ∏ ∏ ∏ ∏ ∏ ∏ ∏ Input layer ∑1 ∏ x ∈ RM ∑2 τ ∈ RQ σ ∈ RN w0,8 w0,1 wNM w11 ∑ ∑ ∑ ∑ x1 xM x2 x3 ∏ ∏ ∏ ∏ ∏ ∏ ∏ ∏ ∏ ∏ ∏ ∏ Input layer ∑1 ∏ x ∈ RM ∑2 τ ∈ RQ σ ∈ RN w0,8 w0,1 wNM w11 ∑ ∑ ∑ ∑ x1 xM x2 x3 ∏ ∏ ∏ ∏ ∏ Figure Four-input three-order Old structure Figure Four-input three-order new structure based Pima Indians diabetes data classiﬁcation.
 Table Comparison wort error Pima indians problem.
 Round Old Train New Train Impprovment% Old Test New Test Improvement% Overall [2] Y.
 Liu, Z.
 Li, D.
 Yang, K.S. Mohamed, J.
 Wang, W.
 Wu, Convergence batch gradient learning algorithm smoothing L1/2 regularization Sigma-pi-sigma neural networks.
 Neurocom- puting (2015), pp.
 333-341.
 [3] M.M. Gupta, N.
 Homma, Z.G. Hou, M.G. Solo, I.
 Bukovsky, Higher order neural networks: fundamental theory applications.
 Artiﬁcial Higher Order Neural Networks Computer Science Engineering: Trends Emerging Applications pp.
 397-422,
 [4] D.E. Rumelhart, G.E. Hinton, R.J. Williams, (1985) Learning internal representations error propagation (No. ICS-8506).
 California Univ San Diego La Jolla Inst Cognitive Science.
 ∏ Input layer ∑1 ∏ x ∈ RM ∑2 τ ∈ RQ σ ∈ RN w0,15 w0,1 wNM w11 ∑ ∑ ∑ ∑ x1 xM x2 x3 ∏ ∏ ∏ ∏ ∏ x4 ∑ ∏ ∏ ∏ ∏ ∏ ∏ ∏ ∏ ∏ Input layer ∑1 ∏ x ∈ RM ∑2 τ ∈ RQ σ ∈ RN w0,15 w0,1 wNM w11 ∑ ∑ ∑ ∑ x1 xM x2 x3 ∏ ∏ ∏ ∏ ∏ x4 ∑ ∏ ∏ ∏ ∏ ∏ ∏ ∏ [5] G.P. Liu, (2012) Nonlinear identiﬁcation control: neural network approach.
 Springer Science Business Media.
 [6] M.Fallahnezhad,M.H. Moradi,S.
 Zaferanlouei, (2011) hybrid higher order neural classiﬁer handling classiﬁcation problems.
 Expert Systems Applications, 386-393.
 [7] A.
 R.
 barron, Predicted Squared Error: A criterion automatic model selection,” Self- Organizing Methods Modeling: GMDH Type Algorithms (S.J. Farlow, Ed.), Marcel Dekker, Inc., NY, Chap.4 (1984), pp.
 87-103.
 [8] V.
 S.
 Stometta B.
 A.
 Hubermann, improved three-layer propagation algorithm, Proc.
 IEEE IJCNN, vol.
 pp.637-643.
 [9] H.
 Zhang, Y.
 Tang, X.
 Liu, (2015) Batch gradient training method smoothing l0 regular- ization feedforward neural networks.
 Neural Computing Applications, 383-390.
 [10] W.
 Wu, Q.W. Fan, J.M. Zurada al., Batch gradient method smoothing L1/2 regularization training feedforward neural networks,Neural Networks (2014) 72-78.
 [11] R.
 Reed, Pruning algorithms-a survey,IEEE Transactionson Neural Networks (1997) 185-204.
 [12] S.
 Shalev-Shwartz, T.
 Zhang, (2014) Accelerated proximal stochastic dual coordinate ascent regularized loss minimization.
 International Conference Machine Learning (pp.
 64-72).
 [13] J.
 Tang, S.
 Alelyani, H.
 Liu, (2014) Feature selection classiﬁcation: A review.
 Data Classiﬁ- cation: Algorithms Applications, pp.

 [14] S.
 Scardapane, D.
 Comminiello, A.
 Hussain, al.
 (2017) Group sparse regularization deep neural networks.
 Neurocomputing 241:81-89.
 [15] Z.
 Xu, H.
 Zhang, Y.
 Wang, X.
 Chang, L1/2 regularization, Science ChinaInformation Sciences, Vol.
 No. (2010) pp.
 1159-1165.
 [16] W.
 Wu, H.
 Shao, Z.
 Li, Convergence batch BP algorithm penalty FNN training, Neural Information Processing, (2006) 562-569.
 [17] J.
 Wang, W Wu, J.
 M.
 Computational properties conver-gence analysis BPNN cyclic cyclic learning penalty, Neural Networks, Vol.
 pp.
 127-135.
 [18] A.
 S.
 Weigend, D.
 E.
 Rumelhart, B.
 Huberman, Generalization weight-elimination applied currency exchange rate prediction, Neural Networks, IJCNN International Joint Confer- ence on, Seattle, (1991), PP.837-841.
 [19] Z.
 Xu, X.
 Chang, F.
 Xu, H.
 Zhang, L1/2 Regularization: A Thresholding Representation Theory Fast Solver, Neural Networks Learning Sys-tems, IEEE Transactions Vol.
 No. (2012) pp.
 1013-1027.
 [20] M.
 Yuan, Y.
 Lin, Model selection estimation regression grouped variables, Journal Royal Statistical Society: Series B (Statistical Methodology), Vol.
 pp.
 49-67.
 Related Works






































 Problem Setting






































 Results








































 Techniques






































 (cid:101)O(k2)-Space Algorithm Chen’ Framework
















 (cid:101)O(k)-Space Algorithm (Nearly Optimal) Sensitivity-based Sampling

 Max-Cut Average-Distance

























 Meta Algorithms Roadmap




























 Concluding Remark



































 Acknowledgments





































 References Notation B Preliminaries B.1 Deﬁnitions Dynamic Streaming Model k-Clustering Problems









 B.2 Deﬁnitions k-means Clustering




























 B.3 Deﬁnitions M-estimator Clustering

























 B.4 Basic Probability Tools


































 B.5 Tools Previous Work
































 C Do Previous Techniques Fail?
 D Coreset Construction k-means Based Chen’s Framework D.1 Deﬁnitions Properties
































 D.2 Recursive Partition Approach






























 D.3 Bounding Close Parts
































 D.4 Bounding Far Parts

































 D.5 Coreset Construction
































 E Coreset Dynamic Data Stream E.1 Streaming Coreset Construction





























 E.2 Dynamic Point-Cell Storing Data Structure




















 E.3 Main Algorithm






































 F General Clustering Problem F.1 M-Estimator Clustering

































 F.2 Improvements k-median






























 G Applications G.1 A Dynamic Streaming Approximation Max-CUT


















 G.2 Dynamic Streaming Approximation Average Distance














 H (cid:101)O(k) Space Algorithm Based Sensitivity Sampling H.1 Deﬁnitions Preliminaries






























 H.2 Reducing Original Problem Important Points




















 H.3 Sampling Scores Important Points


























 H.4 Algorithm









































 Introduction Clustering is central problems modern research computation, including algorithmic design, machine learning many more.
 clustering methods, k-means is important approaches clustering geometric datasets.
 ﬁrst k-means algorithm dates 1950s.
 last half century, varies works have studied problem (see [Jai10] complete survey).
 inspired tremendous other variants clustering methods, e.g. aﬃnity propagation, mean-shift,spectral clustering, mini batch k-means many more (e.g., [Scu10, CM02, SM00, MS01, NJW02, VL07]).
 problem has been studied diﬀerent settings computational models, e.g. distributed computing, parallel computing, map-reduce, streaming setting quantum computing.
 k-means clustering has been used various research topics, e.g., database, data-mining, computer vision, geostatistics, agriculture, astrophysics (e.g., [SM00, GKL+17, LIY+15, MS01]).
 Indyk [Ind04] introduced model dynamic geometric data streams, set geometric points d-dimensional discrete space [∆]d are updated dynamically, large ∆, i.e., data stream is form insert(p) delete(p) p [∆]d.
 ﬁxed dimension d, Frahling Sohler [FS05] develop ﬁrst eﬃcient streaming (1 + )-approximation algorithms k-means, k-median, other geometric problems dynamic data streams.
 paper, propose algorithm maintain coreset size k−O(d) obtaining (1 ± ) approximation k-clustering, including k-means k-median.
 coreset clustering is small weighted point set summarizes original dataset.
 Solving clustering problem coreset provides approximate solution original problem.
 is fundamental tool solving k-means clustering problem.
 recent work Braverman, Frahling, Lang, Sohler Yang [BFL+17] provides data structure maintaining coreset k-median dynamic stream using space polynomial d.
 technique cannot be extended k-means, technique relies p∈P dist(p, C), dist(·,·) is Euclidean metric C is set points.
 show hard example (in Section C) error based design is unbounded k-means, is form sum squares, i.e., p dist2(p, C).
 paper, ﬁrst show combining coreset framework [Che09] grid structure constructed [BFL+17] gives data structure maintaining k-means coreset dynamic stream use space polynomial d.
 Such data structure uses fact k-median cost function is form sum absolutes, i.e.,(cid:80) (cid:80) space (cid:101)O(k2 poly(d)).
 space is optimal k, is ﬁrst (to best knowledge) kind obtaining space polynomial d.
 obtain algorithm space optimal k (i.e., linear k), new idea has be introduced.
 [FL11] introduced revolutionary coreset framework constructing coreset batch-setting insertion-only stream.
 coreset framework is sampling data points based “sensitivity” point.
 is deﬁned maximum percentage change cost function possible clustering solutions removing point dataset.
 recently, [BFL16] improves framework gives better insertion-only streaming algorithm.
 major contributions is show sensitivity-based sampling scheme is achievable dynamic update streams.
 Hence obtain algorithm is space optimal k polynomial dimension d.
 further show data structure maintaining coreset be extended uniﬁed approach general classes k-clustering, including k-median, M-estimator clustering, clusterings general set cost functions distance.
 space/time algorithm is similar k-means poly(d) factor diﬀerence.
 M- estimator represents speciﬁc function distance, e.g., Huber norm [Hub64] is speciﬁed parameter τ > measure function H is given H(x) = x2/2τ, |x| ≤ τ H(x) = |x| − τ /2 otherwise.
 Recently, M-estimators attracted interests variety computing ﬁelds (e.g., [NYWR09, CW15b, CW15a, SWZ17b]).
 Related Works is known ﬁnding optimal solution k-means is NP-hard k = d = [DFK+04, ADHP09, MNV09, Vat09].
 success algorithm used practice is Lloyd’s algorithm, is known “the” k-means method [Llo82].
 k-means can’t be solved polynomial time, has several works trying understand “local search method” k-means.
 Kanungo al.
 [KMN+02] proved simple local search heuristic is able obtain polynomial-time algorithm approximation ratio +  ﬁxed  > k- means Euclidean space.
 recently, groups improved ratio +  independently, Friggstad, Rezapour Salavatipour [FRS16] showed that, error parameter  > local search algorithm considers swaps dO(d) · (1/)O(d/) centers time produce solution using k centers cost is most (1 + )-factor greater optimum solution.
 Cohen-Addad, Klein Mathieu [CAKM16] proved number swapped centers is poly(1/).
 large number works [IKI94, Mat00, BHPI02, DLVKKR03, HPM04] proposed (1 + )-approximation algorithm ineﬃcient running time Rd. is line works targeting insertion-only streaming k-means k-median.
 example, [BS80, GMMO00, COP03, BDMO03, AHPV04, HPM04, HPK05, Che09, FL11, FS12, AMR+12, BFL16] many others have developed improved streaming algorithms computing so- lution k-means k-median approximately.
 Recent years, have been lots interest dynamic streaming algorithms other settings, e.g. [BYJK+02, FKM+05, Bas08, KL11, AGM12a, AGM12b, GKP12, GKK12, AGM12b, BKS12, CMS13, CMS13, AGM13, McG14, BGS15, BHNT15, BS16, ACD+16, ADK+16, BWZ16, KLM+17, SWZ17a, SWZ17b].
 addition, are many works related k-median k-means diﬀerent settings, e.g., [IP11, BIP+16, BCMN14, CCGG98].
 Problem Setting section, deﬁne problem interests.
 consider streaming setting, space algorithm is limited, i.e., cannot input points.
 dynamic data stream setting, allow points be deleted.
 Formally, let Q denote set points high dimensional grids initialized empty set.
 time t, observe tuple (pt, opt) pt ∈ [∆]d opt ∈ {insertion, deletion} meaning insert point delete point Q.
 Note points observed stream belong Q end stream deletion is allowed.
 one-pass data-stream, want output small multi-set points (i.e., size o(|Q|)) S (associated weight-function w point) summarizes important properties ground-truth points set Q.
 require algorithm have ﬁxed memory budget update time.
 rules naïve approach storing Q memory explicitly.
 speaking, require S be  k-means coreset Q, satisﬁes, ∀Z ⊂ [∆]d,|Z| = k (1 − ) cost(Q, Z) ≤ costw(S, Z) ≤ (1 + ) cost(Q, Z), restrict setting be integer grid sake representation simplicity.
 algorithm analysis be extended non-integer grid setting.
 (cid:88) p∈Q min z∈Z cost(Q, Z) = dist2(p, z) costw(S, Z) = (cid:88) p∈Q wp dist2(p, z) min z∈Z ⊂ [∆]d |C∗ | = k such function cost(C) :=(cid:80) other diﬀerent clustering objectives, e.g., M-estimator clustering, problem setting is same dist2(·) function is changed corresponding function.
 Results k-Means discrete geometric point set P ⊂ [∆]d, k-means problem is ﬁnd set C∗ p∈P dist2(p, C) is minimized, dist(p, C) = minc∈C dist(p, c) describing minimal distance p point C.
 de- ﬁrst (1 + )-approximation algorithm k-means clustering problem dynamic data streams uses space polynomial dimension d.
 best knowledge, previ- ous algorithms k-means dynamic streaming setting required space dimension.
 Formally, main theorem states, Theorem (k-means).
 Fix , δ ∈ (0, k, ∆ ∈ N+, let L = log ∆.
 is data struc- ture supporting insertions deletions point set Q ⊂ [∆]d, maintaining weighted set S positive weights point, such probability least − δ, time (cid:101)O(−2k2 · log2(1/δ) · poly(d, L)) bits worst case2.
 update input, algo- stream, S is -coreset k-means size −2k2 · log(1/δ) · poly(d, L).
 data structure uses rithm needs poly(d, L, log k) time process.
 pass, outputs coreset time poly(d, k, L, log
 Note that, k-means problem, [CEM+15] shows do random projection reduce d O(k/2).
 random projection does preserve grid- structure, remains caveat use random projection method dynamic geometric stream.
 Nevertheless, interesting setting be d ≤ O(k/2) d (cid:29) log k.
 theorem is restated Theorem E.5 Section E proof is presented therein.
 present one-pass algorithm diﬀerent trade-oﬀs k .
 algo- rithm has space linear k polynomial d.
 dependence k is optimal (for ﬁxed  d)! guarantee result is presented following theorem.
 Fix , δ ∈ (0, k, ∆ ∈ N+, let L = log ∆.
 is data structure supporting insertions deletions point set Q ⊂ [∆]d, maintaining weighted set S positive weights k-means size k· log(1/δ)· poly(d, L, data structure uses (cid:101)O(k· log2(1/δ)· poly(d, L, point, such probability least time stream, S is -coreset bits worst case.
 update input, algorithm needs poly(d, L, log k) time process.
 pass, outputs coreset time poly(d, k, L, log
 M-estimator Clustering More General Cost Functions Further show algorithm analysis k-means be extended general functions (including M-estimator) distances, i.e., function satisﬁes sub-additivity: is ﬁxed constant C > function f, deﬁne be f · logO(1)(f ).
 ∀x, y ≥ M (x + y) ≤ C · (M (x) + M (y)).
 exact dependence  is determined coreset framework [FL11] [BFL16], show −3 dependence is suﬃcient.
 See Section H details.
 Notice above equation holds M (·) is non-decreasing function satis- fying ∀x > M (x) > ∀c > M (cx) ≤ f (c)M (x) f (c) > is bounded function.
 case, are aiming solve (cid:88) z∈Q min Z⊂[∆]d:|Z|≤k min z∈Z M (dist(q, z)) given point set Q.
 M-estimator clustering problem has been studied [FS12, BFL16], algorithm be applied insertion streaming model.
 show ﬁrst dynamic streaming algo- rithms problem.
 data structure maintains coreset size similar k-means, diﬀerent dependence dimension d.
 precisely, extend algorithm analysis Theorem M-estimator clustering setting, get algorithm success probability least − δ output -coreset M-estimator clustering worst case, poly(d) factor depends M-estimator function.
 update input, algorithm needs poly(d, L, log k) time process outputs coreset time poly(d, k, L, log
 generalize algorithm analysis Theorem M-estimator clustering setting.
 case, get coreset size k · log(1/δ)· poly(d, L, Section F, show details generalize algorithm analysis Theorem size −2k2 · log(1/δ) · poly(d, L).
 data structure uses (cid:101)O(−2k2 · log2(1/δ) · poly(d, L)) bits data structure uses (cid:101)O(k · log2(1/δ) · poly(d, L, )) bits worst case.
 M-estimator clustering setting.
 Maintaining Approximate Solutions Max-Cut Average Distance show data-structure be used maintain approximate solution Max-Cut Average-Distance problems, ﬁrst problem asks ﬁnd cut streaming point set Q, such sum distances possible pairs cut are maximized; problem asks estimate average distance pairs.
 data structure supports maintaining 1/2- approximation Max-Cut times stream, estimating cost cut (1±) factor.
 Average-Distance, data structure supports maintaining (1±)-approximation times stream.
 Furthermore, data structure maintains approximate solutions generalized version problems, i.e., M-estimator distances.
 data structure problems uses space polynomial d, log ∆, log δ is failure probability.
 formal results are presented Section G.
 (cid:101)O(k2)-Space Algorithm Chen’ Framework Techniques ﬁrst show data structure design underlying framework is based [Che09].
 Denote Xk = {C ⊂ [∆]d |C| ≤ k} set k-sets.
 ﬁnite point set P ⊂ [∆]d, -coreset k-means P is weighted point set S ⊂ [∆]d such cost(S, C) =(cid:80) ∀C ∈ Xk | cost(P, C) − cost(S, C)| ≤  cost(P, C) s∈S ws dist(s, C)2, ws is weight point s.
 coreset algorithm be viewed combination several techniques clustering algorithms dynamic streaming algorithms.
 high level, apply oblivious grid structure point sets used [FS05] [BFL+17] form implicit partition point sets.
 partition satisﬁes crucial property required coreset framework Chen [Che09].
 build dynamic streaming algorithm simulate random sampling Chen’s framework.
 combinations are non-trivial.
 highlight several diﬃculties resolve paper.
 • Firstly, naïve application grid structure [FS05] gives coreset size depending d.
 resolve problem, randomly shift grid struture show exponential dependence d becomes polynomial.
 • Secondly, Chen’s framework, points are stored memory, do sampling dataset.
 However, us, memory algorithm is limited points be deleted.
 do know sampling parameters stream coming.
 parameters are crucial implement Chen’s framework.
 resolve problem applying clever data structure, is similar K-set data structure [Gan05].
 data structure has small memory budget, output set points insertions deletions size ﬁnal point set is smaller memory budget.
 knowing parameters true dataset, guess many possibilities parameters Chen’s framework.
 show guessed parameters are close true parameters, data sample is guaranteed be smaller memory budget.
 set points form coreset dataset be output sampling data structure.
 elaborate details framework have been applying.
 start introduction Chen’s coreset framework.
 t ≤ αk and(cid:80)t Chen’s Coreset Framework bottom level algorithm is framework Chen [Che09].
 shown Figure core idea is ﬁnd partition dataset P = P1∪ P2∪


 Pt, i=1 |Pi| diam(Pi)2 ≤ β OPT, OPT is optimal cost k-means P
 ple points random part Pi such error estimating denote partition (α, β)-partition.
 (α, β)-partition is known, sam- cost contribution Pi is |Pi| diam(Pi)2/β.
 formally, denote Xk = {C ⊂ [∆]d |C| ≤ k} set k centers Si set samples Pi. Assigning point Si weight |Pi|/|Si|, Hoeﬀding bound, expect, high probability, Notice size coreset is (cid:101)O(αk2β2).
 Combining samples part, obtain S = ∪t (cid:12)(cid:12) cost(Pi, C) − cost(Si, C) ∀C ∈ Xk (cid:12)(cid:12) ≤ |Pi| diam(Pi)2/β.
 i=1Si is coreset k-means P
 Obtaining (α, β)-partition Dynamic Stream oﬄine setting, i.e., case points are stored memory, obtaining (α, β)-partition is straightforward, i.e., using Indyk’s (α, β)-bi-criterion algorithm [Ind00a].
 becomes challenging algorithm has limited memory points be deleted.
 overcome diﬃculty applying similar grid structure used [FS05] [BFL+17].
 shown Figure build log ∆ many grids dataset.
 higher layer reﬁnes parent layer splitting cell many sub-cells.
 stress grid structure is oblivious point set.
 crucial property, are able insert delete points point set, grids stay
 Similar [FS05], show level i, number cells containing more O(OPT·4i/∆2/k) Figure (a) coreset framework Chen [Che09].
 point set is partitioned set sets, are called (α, β)-partition.
 Points are sampled partition.
 See texts details.
 (b)The grid structure point set.
 top bottom, have levels grids.
 higher level partition cell parent level many sub-cells, d is dimension d.
 points is bounded · k, independent number points P
 call cells heavy cells, form tree.
 Notice grid structure has log ∆ levels, number heavy cells is O(log ∆· · k).
 heavy cell has diameter √d· ∆/2i non-heavy children heavy cell level i contains most O(2d · OPT·4i/∆2/k) points, level, (number points non-heavy children) · (diameter heavy cell)2 is bounded O(2d OPT·4i/∆2) · (√d · ∆/2i)2 = O(2d OPT /k).
 Applying deﬁnition (α, β)-partition, show non-heavy children heavy cells form (α, β)-partition, α = O(2d·log ∆) β = O(2d·log ∆).
 majority work algorithm does apply Chen’s framework, ﬁnally -coreset is size (cid:101)O(k/O(d)) is showing dynamic streaming algorithm maintain such partition.
 stress algorithm uses (cid:101)O(k2/O(d)) space.
 Removing Exponential Dependence d last paragraph, show (α, β)- partition be obtained building grid structure dataset.
 However, size partition is exponential d.
 get rid exponential dependence, apply random shift grid structure, shown Figure
 explain random shift brings dependence d.
 shown last paragraph, heavy cell is deﬁned number points it.
 Let ﬁx optimal solution k-means, i.e., set k-centers z1, z2,


 zk.
 random shift grantees that, high probability, zis is “far” boundary cell.
 Conditioning event, cells do contain optimal center, point contributes cost least distance optimal center boundary cell, is “far”.
 are able bound number cells containing center containing many points
 k cells containing optimal center, total number P1P2P3P4P5P6 Figure Random shift grid brings number heavy cells.
 left panel, have worst case alignment points grids many cells contain lots points.
 right panel, random shift, cells are containing many points.
 “heavy cell”s is bounded O(α · k), α = O(d · log ∆).
 additional tricks, show implicit (α, β)-partition be constructed heavy cells, β = (poly(d log ∆)).
 Sample Maintainance Rejection Data Structure random-shifted grid struc- ture, aim sample points (α, β)-partition, does Chen’s construction.
 However, knowing parameters point sets, optimal cost OPT, have guess value.
 guess many possibilities OPT, i.e., oi = i =


 log(d∆d).
 guess, run sampler attempt get coreset.
 Notice that, guesses sampler based Chen’s construction are oblivious input point set.
 show guess oi ≤ OPT, above mentioned samples (with sampling probability heavy cells deﬁned oi) form -coreset point set.
 oi/ OPT is small, coreset is large.
 construct new dynamic set-point sampler data structure has ﬁxed memory budget rejects large data samples preserve data samples small number points.
 data structure, pick smallest oi, resulting samples are preserved data structure.
 result, samples are correct coreset.
 elaborates details data structure.
 data structure supports insertions deletions pairs form (C, p), C represents set p ∈ [∆]d is point.
 data structure supports querying points sets small number points, time stream.
 Suppose are most α non-empty sets most θ sets containing least β points rest sets containing γ points.
 memory budget [Θ(θβ + γ)], algorithm is able maintain number points cell, γ points cells less points.
 high level idea is use level pair-wise hash functions.
 ﬁrst level, hash ID set (i.e., name coordinate set is cell) universe [Θ(α)] point universe [Θ(β)], hash pair hash values set ID point universe [Θ(θβ + γ)].
 be shown point c set C less β points, third hash value is unique other pairs, least constant probability.
 Use unique hash value, sanity checker based parity recover bit point ID set.
 repeating log recover γ points sets less points.
 randomlyshiftheavycellonlysmallnumberofcellscontainingmanypoints (cid:101)O(k)-Space Algorithm (Nearly Optimal) Sensitivity-based Sampling show techniques improving space complexity (cid:101)O(k2) (cid:101)O(k).
 high level idea data structure is simulate sensitivity-based sampling using oblivious linear sketch.
 sampling scheme takes advantages most data structures constructed last section.
 begin, ﬁrst brieﬂy review coreset framework [FL11] [BFL16].
 [FL11] [BFL16], A Brief Review Coreset Framework [FL11] [BFL16] have proposed framework called sampling.
 techniques are similar distance functions other (cid:96)2 take k-means example.
 Let Q be set points, let X k ⊂ [∆]d×k be set possible k-centers.
 sensitivity point q ∈ Q is deﬁned s(q) = max Z∈X k dist2(q, Z)/ dist2(p, Z) (cid:18)(cid:88) p∈Q (cid:19) point q Q.
 be shown that(cid:80) Namely, s(q) denotes maximum possible change cost function k-set Z removing point gets probability s(cid:48)(q)/((cid:80) q s(q) ≈ k.
 Suppose designs uniform upper bound s(cid:48)(·) such ∀q ∈ Q s(cid:48)(q) ≥ s(q), denote probability distribution D Q let R be set i.i.d. samples sampling D |R| (cid:38) (cid:80) p∈Q s(cid:48)(p)).
 framework [FL11] [BFL16] shows that, q∈Q s(cid:48)(q)/2 assign point q Q weight (|R|s(cid:48)(q))−1, R is high probability (1 ± ) k-means coreset theory.
 If(cid:80) q∈Q s(q), (cid:101)O(k)-sized coreset is constructed.
 Q.
 proof is establishing connection VC-dimension theory coreset q∈Q s(cid:48)(q) is diﬀerent from(cid:80) speaking, reason sensitivity sampling removes k factor k2-construction is constructing new functional space (cid:38) k samples), VC-dimension is O(d) O(kd).
 Sensitivity Sampling Dynamic Data Stream Analogously framework described last section, describe algorithm simulates sensitivity sampling dynamic stream.
 have shown, assign sampling probability, need is upper bound sensitivity.
 point p ∈ [∆]d, p is set Q end stream, have corresponding sensitivity.
 Denote true sensitivity s(p).
 want design “good” upper bound s(cid:48)(p) sensitivity point p such
 sum s(cid:48)(p) is large;
 are able i.i.d. sample m points ﬁnal undeleted points such point is chosen probability proportional s(cid:48)(p) sample, magnitude m is inﬂuenced sum s(cid:48)(p);
 end stream, are able approximate s(cid:48)(p) given point p.
 ﬁrst property ensures m be large means size coreset is small.
 second property ensures obtain points coreset implementing such sampling procedure.
 third property shows obtain weights points coreset.
 previous section, gave concepts “heavy cell” (α, β) partition based random shifted grid.
 Precisely, cell ith level contains least Ti number points, say cell is “heavy” Ti is threshold parameter is set be Θ( d2 (∆/2i)2 k · OPT work.
 discussed previous section, non-“heavy” cells parent cell is “heavy” formed partition.
 call such non-“heavy” cell partition cell.
 cells formed partition, is easy see, undeleted point, belong unique partition cell.
 Furthermore, unique partition cell contains point p is ith level grid, say is partition point ith level.
 show points are partition points ith level have universal sensitivity upper bound is Θ(d3/Ti).
 Furthermore, p∈Q s(cid:48)(p) set sensitivity upper bound p s(cid:48)(p) = Θ(d3/Ti), is easy argue (cid:80) cannot be large.
 reason is (cid:88) p∈Q (cid:48) (p) = (# partition points ith level) · Θ(d3/Ti) (# heavy cells ith level) · Θ(d3) level i (cid:88) (cid:88) = (# heavy cells) · Θ(d3) ≤ α · k · Θ(d3), level i (cid:80) ﬁrst inequality follows level heavy cell contains least Ti number points.
 second inequality follows last section, α is same mentioned last section.
 p∈Q s(cid:48)(p) is large, know size coreset be small.
 notice know point p is partition point ith level, know s(cid:48)(p).
 last section, know have streaming algorithm ﬁnd heavy cells means know shape whole partition end stream.
 Thus, given point determine partition cell, know s(cid:48)(p) means point coreset, calculate weight.
 Now, problem remaining is get m i.i.d. sample points, sample, point p is chosen probability proportional s(cid:48)(p).
 challenge implement sampling procedure is sampling scheme is diﬀerent sampling procedure described previous section.
 Note previous section, sampling scheme is determine point be chosen not, need get m independent samples sample is point Q, probability p is chosen is proportional s(cid:48)(p).
 cannot use sampling scheme described previous section.
 handle issue two-stage sampling.
 good property s(cid:48)(p) is partition points ith level have same s(cid:48)(p).
 sample point p probability proportional s(cid:48)(p) is equivalent sample level i, level j is j = s(cid:48)(p) sampled probability proportional (# partition points jth level)·s(cid:48) point p is partition point jth level; uniformly sample partition point ith level.
 implement ﬁrst sampling stage, need know number partition points level.
 achieve using streaming algorithm described last section.
 let focus second stage.
 stage, want implement uniform sampling oracle partition points ith level.
 cannot apply traditional (cid:96)0 sampler do have information partition points level beginning stream.
 achieve goal, be more carefully.
 subsample points ith level, use streaming data structure described last section maintain survived points.
 show using data structure are able recover survived partition points.
 survived partition points ith level, choose survivor output.
 potential problem is ith level, number partition points is small, is possible none partition j s(cid:48) Algorithm A Meta-Algorithm Point Sampling Dynamic Data Stream procedure PointSampler(P Let O = {1, log(d∆)(cid:101) Choose shifted |L| layers grids Create |O| independent KSet instances (with limited memory budget) layer l ∈ L o, l, create set hash functions Hl,o, h ∈ Hl,o is function [∆]d → {0, update point p ∈ ∆d data-stream do },L = {−1, ,(cid:100)log ∆(cid:101)}.
 reads points set P data-stream Create |Hl,o| copies p form (p, i) hi ∈ Hl,o (o, l) ∈ O × L hi ∈ Ho,l do end Update (o, l)-th sketch: hi(p) = KSeto,l.update(p, i) end Choose smallest o∗ ∈ O such {KSeto∗,l}l∈L succeed.
 sampled point sets grid cells given {KSeto∗,l}l∈L end procedure point be survived.
 introduce problem number partition points ith level is small, probability level i is chosen ﬁrst stage is small.
 (cid:17) (cid:16)(cid:80) precisely, suppose need m i.i.d. samples total, high probability number times ﬁrst sampling stage samples level i is m · (# partition points ith level) · s(cid:48) j(# partition points jth level) · s(cid:48) (cid:17) (cid:16)(cid:80)
 Suppose level i, prepared many uniform sampling need guarantee number success oracle is least m·(# partition points ith level)·s(cid:48) j(# partition points jth level) · s(cid:48) is guaranteed have suﬃcient many samples.
 work, show that, uniform sampling oracle level i, need drop probability be poly(d, −1, log(∆), log(1/δ))· is enough achieve goal.
 Thus, have enough success uniform samplers level.
 i/ kTi Max-Cut Average-Distance use coreset data structure obtain solutions max-cut average distance dynamic dataset.
 basic idea is use 1-means coreset proxy estimating pairwise distance.
 reduce Max-Cut instance Average-Distance instance instance estimating distance point subset original point set.
 1-means coreset is suﬃcient case.
 Meta Algorithms Roadmap All algorithms share similar meta-structure sample points dynamic data stream.
 meta-algorithm is presented Algorithm
 meta algorithm is oblivious linear sketch input data set algorithm does need know actual data set).
 view data set binary vector meta-algorithm converts vector Θ(d log ∆) binary vectors {0, Θ(d log ∆) vectors Z∆d, i.e., gives Θ(d log ∆) level points samples counts number points grid cells level.
 output vectors are sparse, hence be stored limited memory budget.
 meta ﬁrst build random shifted grids G−1, G0, G1,··· GL L = log ∆ grid Gi reﬁnes parent splitting cell cells evenly.
 pair (o, l) o ∈ {1, }, l ∈ {−1, L}, choose set random hash Pr[hi(p) = is deﬁned according speciﬁc tasks (i.e., (cid:101)O(k2) (cid:101)O(k) algorithms, functions Hl,o,,δ, hi ∈ Hl,o,,δ is function hi [∆]d → such ∀p ∈ [∆]d, chose diﬀerent sampling probability), initialize point maintainer KSet (see Algorithm
 KSet data structure has limited memory budget.
 KSet data structure succeed number points grid cells sampled (by hash functions hi) is memory budget (otherwise, information stored KSet is treated garbage).
 scan stream, insert/delete operation, suppose point is ﬁrst check hi(p)
 is ignore operation.
 Otherwise, level l ﬁnd cell C ∈ Gl contains p, o, use information C, p operation type (ins/del) update point maintainer corresponds (o, l).
 build coreset succeeded KSet instances smallest parameter o.
 provide basic notation Section A.
 Section B states deﬁnitions useful tools previous works.
 Section D, explain construct coreset using randomized grid structure hash functions universe [∆]d.
 Section E, show maintain coreset dynamic stream.
 Section F presents general result M-estimator improvement compared previous k-median result.
 Section G shows extend k-means other geometric problems.
 provide improved one-pass algorithm k-means based sensitivity sampling Section H.
 Concluding Remark paper present algorithm obtaining -coreset k-means problem high- dimensional dynamic geometric data streams.
 ﬁrst algorithm is one-pass algorithm takes space (cid:101)O(k2−2 poly(d, log ∆)) based Chen’s framework [Che09].
 second algorithm is one- pass algorithm takes space (cid:101)O(k poly(d, log ∆, based sensitivity sampling.
 algorithm take space polynomial d.
 best knowledge, are ﬁrst results obtianing k-means coreset using space polynomial d.
 particular, second algorithm is optimal regard parameter k.
 coresets output algorithms consist positive weighted points.
 run favorite oﬄine algorithms obtain desired approximated solutions.
 algorithm be extended general set cost functions, e.g., M-estimator.
 Acknowledgments authors like thank Alexandr Andoni, Vladimir Braverman, Lijie Chen, Kenneth Clarkson, Harry Lang, Cameron Musco, Christopher Musco, Vasileios Nakos, Jelani Nelson, Eric Price, Aviad Rubinstein, Chen Shao, Christian Sohler, Cliﬀord Stein, Huacheng Yu, Zhengyu Wang, David P.
 Woodruﬀ, Hongyang Zhang useful discussions.
 LY like thank Professor Vladimir Braverman generous support.
 References [ACD+16] [ADHP09] [ADK+16] [AGM12a] [AGM12b] [AGM13] Ittai Abraham, Shiri Chechik, Daniel Delling, Andrew V Goldberg, Renato F Werneck.
 dynamic approximate shortest paths planar graphs worst- case costs.
 Proceedings Twenty-Seventh Annual ACM-SIAM Symposium Discrete Algorithms (SODA), pages
 Society Industrial Applied Mathematics,
 Daniel Aloise, Amit Deshpande, Pierre Hansen, Preyas Popat.
 Np-hardness euclidean sum-of-squares clustering.
 Machine learning,
 Ittai Abraham, David Durfee, Ioannis Koutis, Sebastian Krinninger, Richard Peng.
 dynamic graph sparsiﬁers.
 IEEE Annual Symposium Foundations Computer Science (FOCS), pages
 IEEE,
 Kook Jin Ahn, Sudipto Guha, Andrew McGregor.
 Analyzing graph structure linear measurements.
 Proceedings twenty-third annual ACM-SIAM sym- posium Discrete Algorithms, pages
 Society Industrial Applied Mathematics,
 Kook Jin Ahn, Sudipto Guha, Andrew McGregor.
 Graph sketches: sparsiﬁcation, spanners, subgraphs.
 Proceedings ACM SIGMOD-SIGACT-SIGAI symposium Principles Database Systems, pages

 Kook Jin Ahn, Sudipto Guha, Andrew McGregor.
 Spectral sparsiﬁcation dynamic graph streams.
 Approximation, Randomization, Combinatorial Op- timization.
 Algorithms Techniques, pages

 [AHPV04] Pankaj K Agarwal, Sariel Har-Peled, Kasturi R Varadarajan.
 Approximating extent measures points.
 Journal ACM (JACM),
 [AMR+12] Marcel R Ackermann, Marcus Märtens, Christoph Raupach, Kamil Swierkot, Chris- tiane Lammersen, Christian Sohler.
 Streamkm++: clustering algorithm data streams.
 Journal Experimental Algorithmics (JEA),
 [Bas08] [BCMN14] [BDMO03] [BFL16] [BFL+17] Surender Baswana.
 Streaming algorithm graph spanners-single pass constant processing time edge.
 Information Processing Letters,
 Sayan Bhattacharya, Parinya Chalermsook, Kurt Mehlhorn, Adrian Neumann.
 New approximability results robust k-median problem.
 Scandinavian Work- shop Algorithm Theory, pages

 Brain Babcock, Mayur Datar, Rajeev Motwani, Liadan O’Callaghan.
 Maintain- ing variance k-medians data stream windows.
 Proceedings twenty- second ACM SIGMOD-SIGACT-SIGART symposium Principles database sys- tems, pages

 Vladimir Braverman, Dan Feldman, Harry Lang.
 New frameworks oﬄine streaming coreset constructions.
 arXiv preprint arXiv:1612.00889,
 Vladimir Braverman, Gereon Frahling, Harry Lang, Christian Sohler, Lin F Yang.
 Clustering high dimensional dynamic data streams.
 ICML.
 https://arxiv.

 [BGS15] [BHNT15] [BHPI02] [BIP+16] [BKS12] [BR94] [BS80] [BS16] [BWZ16] [BYJK+02] [CAKM16] Surender Baswana, Manoj Gupta, Sandeep Sen.
 dynamic maximal match- ing o(log n) update time.
 SIAM Journal Computing,
 Sayan Bhattacharya, Monika Henzinger, Danupon Nanongkai, Charalampos Tsourakakis.
 Space-and time-eﬃcient algorithm maintaining dense subgraphs one-pass dynamic streams.
 Proceedings forty-seventh annual ACM sym- posium Theory computing, pages

 Mihai B¯adoiu, Sariel Har-Peled, Piotr Indyk.
 Approximate clustering core- sets.
 Proceedings thiry-fourth annual ACM symposium Theory com- puting, pages

 Arturs Backurs, Piotr Indyk, Eric Price, Ilya Razenshteyn, David P Woodruﬀ.
 Nearly-optimal bounds sparse recovery generic norms, applications k-median sketching.
 Proceedings Twenty-Seventh Annual ACM-SIAM Sym- posium Discrete Algorithms, pages

 Surender Baswana, Sumeet Khurana, Soumojit Sarkar.
 dynamic ran- domized algorithms graph spanners.
 ACM Transactions Algorithms (TALG),
 Mihir Bellare John Rompel.
 Randomness-eﬃcient oblivious sampling.
 Foun- dations Computer Science, Proceedings., Annual Symposium on, pages
 IEEE,
 Jon Louis Bentley James B Saxe.
 Decomposable searching problems i.
 static-to- dynamic transformation.
 Journal Algorithms,
 Aaron Bernstein Cliﬀ Stein.
 Faster dynamic matchings small approxi- mation ratios.
 Proceedings Twenty-Seventh Annual ACM-SIAM Symposium Discrete Algorithms, pages
 Society Industrial Applied Mathe- matics,
 Christos Boutsidis, David P Woodruﬀ, Peilin Zhong.
 Optimal principal compo- nent analysis distributed streaming models.
 Proceedings Annual ACM SIGACT Symposium Theory Computing (STOC), pages
 ACM, https://arxiv.org/pdf/1504.06729,
 Ziv Bar-Yossef, TS Jayram, Ravi Kumar, D Sivakumar, Luca Trevisan.
 Counting distinct elements data stream.
 International Workshop Randomization Approximation Techniques Computer Science, pages

 Vincent Cohen-Addad, Philip N Klein, Claire Mathieu.
 Local search yields ap- proximation schemes k-means k-median euclidean minor-free metrics.
 Foundations Computer Science (FOCS), IEEE Annual Symposium on, pages
 IEEE,
 [CCGG98] Moses Charikar, Chandra Chekuri, Ashish Goel, Sudipto Guha.
 Rounding trees: deterministic approximation algorithms group steiner trees k-median.
 Proceedings thirtieth annual ACM symposium Theory computing, pages

 [CEM+15] [Che09] [CM02] [CMS13] [COP03] [CW15a] [CW15b] [DFK+04] Michael B Cohen, Sam Elder, Cameron Musco, Christopher Musco, Madalina Persu.
 Dimensionality reduction k-means clustering low rank approxima- tion.
 Proceedings Forty-Seventh Annual ACM Symposium Theory Computing (STOC), pages
 ACM, https://arxiv.org/pdf/1410.6801,
 Ke Chen.
 coresets k-median k-means clustering metric euclidean spaces applications.
 SIAM Journal Computing,
 Dorin Comaniciu Peter Meer.
 Mean shift: robust approach feature space analysis.
 IEEE Transactions pattern analysis machine intelligence,
 Michael S Crouch, Andrew McGregor, Daniel Stubbs.
 Dynamic graphs sliding-window model.
 European Symposium Algorithms, pages

 Moses Charikar, Liadan O’Callaghan, Rina Panigrahy.
 streaming algo- rithms clustering problems.
 Proceedings thirty-ﬁfth annual ACM sym- posium Theory computing, pages

 Kenneth L Clarkson David P Woodruﬀ.
 Input sparsity hardness robust subspace approximation.
 IEEE Annual Symposium Foundations Computer Science (FOCS), pages
 IEEE, https://arxiv.org/pdf/1510.

 Kenneth L Clarkson David P Woodruﬀ.
 Sketching m-estimators: A uniﬁed approach robust regression.
 Proceedings Twenty-Sixth Annual ACM- SIAM Symposium Discrete Algorithms (SODA), pages

 Petros Drineas, Alan Frieze, Ravi Kannan, Santosh Vempala, V Vinay.
 Clustering large graphs singular value decomposition.
 Machine learning,
 [DLVKKR03] W Fernandez De La Vega, Marek Karpinski, Claire Kenyon, Yuval Rabani.
 Proceedings thirty-ﬁfth Approximation schemes clustering problems.
 annual ACM symposium Theory computing, pages

 [FIS05] [FKM+05] [FL11] [FRS16] Gereon Frahling, Piotr Indyk, Christian Sohler.
 Sampling dynamic data streams applications.
 Symposium Computational Geometry pages
 Joan Feigenbaum, Sampath Kannan, Andrew McGregor, Siddharth Suri, Jian Zhang.
 graph problems semi-streaming model.
 Theoretical Computer Science,
 Dan Feldman Michael Langberg.
 uniﬁed framework approximating clustering data.
 Proceedings ACM Symposium Theory Computing, STOC San Jose, CA, USA, 6-8 June pages
 Zachary Friggstad, Mohsen Rezapour, Mohammad R Salavatipour.
 Local search yields ptas k-means doubling metrics.
 Foundations Computer Science (FOCS), IEEE Annual Symposium on, pages
 IEEE,
 [FS05] [FS12] [Gan05] [GKK12] Gereon Frahling Christian Sohler.
 Coresets dynamic geometric data streams.
 Proceedings thirty-seventh annual ACM symposium Theory computing (STOC), pages

 Dan Feldman Leonard J Schulman.
 Data reduction weighted outlier- resistant clustering.
 Proceedings twenty-third annual ACM-SIAM sympo- sium Discrete Algorithms, pages
 Society Industrial Applied Mathematics,
 Sumit Ganguly.
 Counting distinct items update streams.
 International Sym- posium Algorithms Computation, pages

 Ashish Goel, Michael Kapralov, Sanjeev Khanna.
 communication streaming complexity maximum bipartite matching.
 Proceedings twenty- third annual ACM-SIAM symposium Discrete Algorithms, pages

 [GKL+17] Shalmoli Gupta, Ravi Kumar, Kefu Lu, Benjamin Moseley, Sergei Vassilvitskii.
 Local search methods k-means outliers.
 Proceedings VLDB Endow- ment,
 [GKP12] Ashish Goel, Michael Kapralov, Ian Post.
 Single pass sparsiﬁcation stream- ing model edge deletions.
 arXiv preprint arXiv:1203.4900,
 [GMMO00] Sudipto Guha, Nina Mishra, R.
 Motwani, L.
 O’Callaghan.
 Clustering data streams.
 FOCS, pages
 [HPK05] [HPM04] [Hub64] [IKI94] [Ind00a] [Ind00b] [Ind04] Sariel Har-Peled Akash Kushal.
 Smaller coresets k-median k-means clustering.
 Proceedings twenty-ﬁrst annual symposium Computational geometry, pages

 Sariel Har-Peled Soham Mazumdar.
 coresets k-means k-median clustering.
 Proceedings thirty-sixth annual ACM symposium Theory computing, pages

 Peter J.
 Huber.
 Robust estimation location parameter.
 Annals Mathe- matical Statistics,
 Mary Inaba, Naoki Katoh, Hiroshi Imai.
 Applications weighted voronoi dia- grams randomization variance-based k-clustering.
 Proceedings tenth annual symposium Computational geometry, pages

 Piotr Indyk.
 High-dimensional computational geometry.
 PhD thesis, stanford univer- sity,
 Piotr Indyk.
 Stable distributions, pseudorandom generators, embeddings data stream computation.
 Foundations Computer Science,
 Proceedings.
 Annual Symposium on, pages
 IEEE,
 Piotr Indyk.
 Algorithms dynamic geometric problems data streams.
 Proceedings thirty-sixth annual ACM symposium Theory computing, pages

 [IP11] [Jai10] [KL11] [KLM+17] [KMN+02] [LIY+15] [Llo82] [LNNT16] [Mat00] [McG14] [MNV09] [MS01] [NJW02] Piotr Indyk Eric Price.
 K-median clustering, model-based compressive sensing, sparse recovery earth mover distance.
 Proceedings forty-third annual ACM symposium Theory computing, pages

 Anil K Jain.
 Data clustering: years k-means.
 Pattern recognition letters,
 J.
 Kelner A.
 Levin.
 Spectral sparsiﬁcation semi-streaming setting.
 Symposium Theoretical Aspects Computer Science (STACS),
 Michael Kapralov, Yin Tat Lee, CN Musco, CP Musco, Aaron Sidford.
 Sin- gle pass spectral sparsiﬁcation dynamic streams.
 SIAM Journal Computing,
 Tapas Kanungo, David M Mount, Nathan S Netanyahu, Christine D Piatko, Ruth Silverman, Angela Y Wu. A local search approximation algorithm k-means clustering.
 Proceedings eighteenth annual symposium Computational geometry, pages

 Zaoxing Liu, Nikita Ivkin, Lin Yang, Mark Neyrinck, Gerard Lemson, Alexander Sza- lay, Vladimir Braverman, Tamas Budavari, Randal Burns, Xin Wang.
 Streaming algorithms halo ﬁnders.
 e-Science (e-Science), IEEE International Conference on, pages
 IEEE,
 Stuart Lloyd.
 Least squares quantization pcm.
 IEEE transactions information theory,
 Kasper Green Larsen, Jelani Nelson, Huy L Nguyen, Mikkel Thorup.
 Heavy hit- ters cluster-preserving clustering.
 Foundations Computer Science (FOCS), IEEE Annual Symposium on, pages
 IEEE, https://arxiv.org/ pdf/1604.01357,
 Jiri Matoušek.
 approximate geometric k-clustering.
 Discrete Computational Geometry,
 Andrew McGregor.
 Graph stream algorithms: survey.
 ACM SIGMOD Record,
 Meena Mahajan, Prajakta Nimbhorkar, Kasturi Varadarajan.
 planar k- means problem is np-hard.
 International Workshop Algorithms Computa- tion, pages

 Marina Meila Jianbo Shi.
 random walks view spectral segmentation.

 ., Andrew Y Ng, Michael Jordan, Yair Weiss.
 spectral clustering: Analysis algorithm.
 Advances neural information processing systems, pages
 [NYWR09] Sahand Negahban, Bin Yu, Martin J Wainwright, Pradeep K Ravikumar.
 uniﬁed framework high-dimensional analysis m-estimators decomposable regularizers.
 Advances Neural Information Processing Systems, pages
 [Scu10] [SM00] [SWZ17a] [SWZ17b] [Vat09] David Sculley.
 Web-scale k-means clustering.
 Proceedings international conference World wide web (WWW), pages

 Jianbo Shi Jitendra Malik.
 Normalized cuts image segmentation.
 Transactions pattern analysis machine intelligence,
 IEEE Zhao Song, David P Woodruﬀ, Peilin Zhong.
 Low rank approximation entrywise (cid:96)1-norm error.
 Proceedings Annual Symposium Theory Computing (STOC).
 ACM, https://arxiv.org/pdf/1611.00898,
 Zhao Song, David P Woodruﬀ, Peilin Zhong.
 Relative error tensor low rank approximation.
 arXiv preprint arXiv:1704.08246,
 Andrea Vattani.
 hardness k-means clustering plane.
 Manuscript, volume
 http://cseweb.ucsd.edu/avattani/papers/kmeans_hardness.pdf,
 [VL07] Ulrike Von Luxburg.
 tutorial spectral clustering.
 Statistics computing,
 Notation n ∈ N+, let [n] denote set {1, n}.
 Given integers m ≤ denote [m, n] = function f, deﬁne be f · logO(1)(f ).
 addition O(·) notation, {m, m + m +


 integer interval.
 functions f, g, use shorthand f (cid:46) g (resp.
 (cid:38)) indicate f ≤ Cg (resp.
 ≥) absolute constant C.
 use f (cid:104) g mean cf ≤ g ≤ Cf constants c, C.
 B Preliminaries B.1 Deﬁnitions Dynamic Streaming Model k-Clustering Problems section, give deﬁnition input form computational model.
 Deﬁnition B.1 (Dynamic streaming model k-clustering).
 Let P ⊂ [∆]d be empty set.
 dynamic streaming model, is stream update operations such qth operation has form (opq, pq) opq ∈ {ins, del}, pq ∈ [∆]d means set P add point pq remove point pq.
 algorithm is allowed 1-pass/2-pass stream.
 end stream, algorithm stores information regarding P.
 space complexity algorithm model is deﬁned total number bits required describe information algorithm stores stream.
 B.2 Deﬁnitions k-means Clustering representation simplicity, restrict dataset be discrete space [∆]d large integer ∆ d.
 extension results Rd is straightforward.
 discuss issue concluding remarks.
 use dist(·,·) Euclidean distance space [∆]d, i.e., p, q ∈ [∆]d, dist(p, q) = (cid:107)p − q(cid:107)2 (cid:107)·(cid:107) is (cid:96)2 norm.
 further extend distance deﬁnition set point, set set.
 Formally, let sets P, Q ⊂ [∆]d point p ∈ [∆]d, dist(p, q) dist(P, Q) = min dist(p, q).
 dist(p, Q) = dist(Q, p) = min q∈Q p∈P,q∈Q denote diam(Q) diameter Q, i.e., largest distance pair points Q.
 Deﬁnition B.2 (k-means clustering).
 Given input point set Q ⊂ [∆]d, k-means clustering problem is ﬁnd set k points Z ⊂ [∆]d, such following objective is minimized.
 (cid:88) q∈Q cost(Q, Z) = dist2(q, Z).
 point Z is called center.
 Note set Z deﬁnes partition point set Q assigning point q ∈ Q closest center Z (ties are broken arbitrarily).
 use denote minimum cost k-means problem.
 Deﬁnition B.3 (Coreset k-means).
 Given input point set Q ⊂ [∆]d.
 -coreset S Q is multiset, smaller size, summarizes important structures Q.
 so- lution optimization problem S is approximate solution Q.
 Formally, let S = {(s1, w1), (s2, w2),


 be -coreset Q, si ∈ [∆]d wi ∈ R is weight si.
 S satisﬁes (cid:12)(cid:12) cost(S, Z) − cost(Q, Z) (cid:12)(cid:12) ≤  cost(Q, Z), ∀Z ⊂ [∆]d,|Z| = k cost(S, Z) := wi dist2(si, Z).
 (cid:88) si∈S (cid:88) q∈Q Deﬁnition B.4 (Coreset k-means dynamic stream).
 Given point set P ⊂ [∆]d described dynamic stream, error parameter  ∈ (0, fail probability parameter δ ∈ (0, goal is design algorithm dynamic streaming model (Deﬁnition B.1) probability least − δ output -coreset k-means (Deﬁnition B.3) minimal space.
 B.3 Deﬁnitions M-estimator Clustering coreset framework be extended arbitrary clustering using M-estimators.
 Deﬁnition B.5 (M-estimator Clustering).
 deﬁne function M R → R be M-estimator.
 deﬁne costM (Q, Z) = min z∈Z M (dist(q, z)).
 goal M-estimator clustering solve following optimization problem.
 min Z⊂[∆]d:|Z|≤k costM (Q, Z).
 Deﬁnition B.6 (Coreset M-estimator).
 Given input point set Q ⊂ [∆]d.
 -coreset S Q is multiset, smaller size, summarizes important structures Q.
 solution optimization problem S is approximate solution Q.
 Formally, let S = {(s1, w1), (s2, w2),


 be -coreset Q, si ∈ [∆]d wi ∈ R is weight si.
 S satisﬁes ∀Z ⊂ [∆]d |Z| = k, holds, costM (S, Z) :=(cid:80) (cid:12)(cid:12) costM (S, Z) − costM (Q, Z) (cid:12)(cid:12) ≤  costM (Q, Z), si∈S wi minz∈Z M (dist(si, z)).
 Deﬁnition B.7 (Coreset M-estimator clustering dynamic stream).
 Given point set P ⊂ [∆]d described dynamic stream, error parameter  ∈ (0, failure probability parameter δ ∈ (0, goal is design algorithm dynamic streaming model (Deﬁnition B.1) probability least − δ output -coreset k-center M-estimator clusetering (Deﬁnition B.6) minimal space.
 dom variables.
 Suppose |Xi| ≤ b surely, i ∈ [n].
 Let σ2 denote (cid:80)n B.4 Basic Probability Tools Lemma B.8 (Bernstein inequality [HPM04]).
 Let X1, X2,··· Xn be independent zero-mean ran- j ].
 j=1 E[X positive t, (cid:34) i=1 Pr Xi > t (cid:35) ≤ exp(cid:0) (cid:1).
 + B.5 Tools Previous Work Theorem B.9 ([LNNT16]).
 Given parameters k  ∈ (0, δ ∈ (0,
 is random- ized (one-pass) algorithm uses O((k + log n/δ · log m) bits space, requires O(log n) time update, needs O(k + poly(log n) decoding time.
 i ∈ [n], let fi ∈ [− poly(n), poly(n)] denote frequency i end data stream.
 loss generality, assume f1 ≥ f2 ≥ ··· ≥ fn.
 algorithm is able output set H size O(k + such that, probability − δ, Property (II) i ∈ [n], f Property (I) (i,(cid:98)fi) ∈ H, f Property (III) (i,(cid:98)fi) ∈ H, |(cid:98)fi − fi| ≤ ((cid:80)n j (i,(cid:98)fi) ∈ H j /k − 2(cid:80)n j /k + 2(cid:80)n j f j=k+1 f j=k+1 f j
 i ≥ i ≥ (cid:80)n (cid:80)n j=1 f j=1 f C Do Previous Techniques Fail?
 [FIS05] is early works using sampling procedure solve dynamic streaming [FIS05] geometric problem.
 show is possible use point samples dynamic point set subroutine solve several geometric problem, e.g. Euclidean Minimum Spanning Tree.
 However, show implement uniform sampling using counting distinct elements subsampling procedure subroutines.
 work, require assigning points diﬀerent “importance”.
 bottom level sampling scheme ours is similar theirs, requires complicated framework implement importance sampling point sets.
 set centers, P be point sets cost(Z, P =(cid:82) ∞ [Ind04] paper uses critical observation estimate cost k-median, is: let Z be summation logarithmic levels, i.e.,(cid:82) ∞ |P − B(Z, r)|dr =(cid:80)∞ |P − B(Z, r)|dr, B(Z, r) is union neighborhoods (of radius r) point Z.
 integration is approximated i=0 |P − B(Z, ri+1)|(ri+1 − ri), ri = O((1 + )i).
 critical part is estimate |P − B(Z, ri−1)|.
 paper constructed counting data structure based grids side length O(ri).
 input point is snapped grid point.
 obtain accurate counts |P − B(Z, ri−1)|, data structure needs query many grid points Z.
 Such data structure is implemented using pair-wise independent hash functions, uses memory O(|Z|/O(d)).
 Notice paper gives approximation cost.
 is coreset.
 Therefore, obtain k-median solution, exhaustive search is used.
 Furthermore, technique fails extend k-means, lacks integration formula cost function.
 Does [FS05] require exponential dependence d, (1/)d?
 grid structure paper is inspired [FS05].
 [FS05], ﬁrst build deterministic quadtree-like structure: Consider big cube containing entire data set.
 cube is treated root cell tree.
 Going level, partition parent cell subcells.
 leaf tree contains most single point dataset.
 Notice cell side length decrease level increase.
 mark cell “heavy” cell containing enough points moving points cell center cell incurs much error cost optimal k-means/median solution.
 side length (or diameter) cells decreases level number points required have eﬀect becomes larger.
 Eventually, cells are non-heavy level.
 such have tree heavy cells.
 heavy cell, coreset is constructed assigning point non-heavy children center.
 turns want epsilon-approximated coreset, threshold non-heavy cell is exponential d, i.e., non-heavy cell level i cannot contain more (cid:101)O(O(d) · OPT /2i) points, OPT is optimal cost.
 small threshold gives many heavy cells.
 do previous insertion-only streaming coreset construction algorithms fail dynamic stream model?
 Many previous insertion streaming coreset construction algorithms (e.g., [FS12]) depend merge-reduce technique, i.e. read points stream, construct coreset, read part, construct new coreset, merge coresets.
 repeat procedure stream ends.
 technique works insertion streaming model.
 points get deleted, framework fails immediately.
 [BFL16] shows new framework other merge-reduce, algorithm relies non- deleting structure data stream well.
 Do fail extend k-means?
 k-median coreset construction techniques be extended k-means coreset construction (see e.g. [BFL16, FS12]), construction be implemented insertion streaming model.
 p∈P (dist(ci p∈P (dist(ci p, z), ci z)− dist(cL−2 p, z) − dist(ci−1 [BFL+17] showed k-median coreset construction dynamic streaming model.
 construction cannot be extended k-means coreset construction directly.
 k-median algorithm relies writing cost point telescope sum.
 example, consider cell i-th level containing p.
 total 1-median cost(cid:80) 1-median problem.
 let z be candidate center point p ∈ P be point, dist(p, z) = dist(p, z)− dist(cL−1 z) + dist(cL−1 set P z be split L pieces, i.e.,(cid:80) p is center z) +···− dist(c0 p∈P dist(p, z) point p, z)−dist(ci−1 z)) is(cid:80) estimator to(cid:80) z)) i ∈ [L].
 [BFL+17] estimates cost L pieces sampling points, i.e., let Si be samples i-th level, p, z) − dist(ci−1 p p∈Si(dist(ci p is probability point p is sampled samples Si. A crucial observation is ζ i have | dist(ci, z)−dist(ci−1, z)| ≤ ∆/2i – cell size level i is independent location inequality show high concentration estimator(cid:80) z.
 nice upper bound | dist(ci, z)−dist(ci−1, z)|, apply Bernstein’s (cid:101)O(1/2) samples level.
 framework does work 1-means write telescope sum structure as(cid:80) p estimator as(cid:80) z)), setup p, z) − dist2(ci−1 z)| is upper p∈Si(dist2(ci bounded cell size.
 Instead, magnitude is depending location z.
 example, p, z) − dist2(ci−1 z)| ≥ ∆.
 apply Bernstein’s inequality here, be large | dist2(ci points need (cid:101)Ω(2i) samples, be large ∆d.
 space saving upper bound | dist2(ci z)| is larger sample is possible.
 p, z)− dist2(ci−1 p.
 | dist2(ci p, z) − dist2(ci−1 p, z) − dist2(ci−1 p, z)− dist(ci−1 p(dist2(ci z))/ζ i p∈Si(dist(ci z))/ζ i z))/ζ i D Coreset Construction k-means Based Chen’s Framework introduce coreset construction, explain high level outline.
 impose shifted grid structure universe [∆]d.
 is done dataset.
 grid structure be stored memory negligible amount space.
 prove are good properties randomly shifted grid, ﬁxed point set.
 show extract coreset using help grid hash functions universe.
 prove construction.
 center point Z, total cost be written telescope sum(cid:80) Figure Telescope sum [BFL+17] fails k-means.
 k-median problem, ﬁxed set p, Z)−dist(ci−1 Z)).
 (cid:80) p) is independent piece, | dist(ci choice Z.
 However, k-means problem, telescope sum total cost is p, Z)2−dist(ci−1 Z)2| Z)2).
 piece, upper bound | dist(ci depend location Z, be least ∆ worst case.
 p∈P (dist(ci Z)| is upper bounded dist(ci−1 p, Z)2−dist(ci−1 p∈P (dist(ci p, Z)−dist(ci−1 D.1 Deﬁnitions Properties Without loss generality, assume dataset is [∆]d ∆ = positive integer L.
 enlarge ∆ closest power
 space [∆]d is partitioned grid structure follows.
 ﬁrst level grid contains single cell, is taken entire space.
 higher level, reﬁne partition splitting cell equal square sub-cells.
 ﬁnest level, i.e., L-th level, cell contains single point.
 further shift boundary grids achieve certain properties, show.
 Formally, grid structure is deﬁned follows.
 Deﬁnition D.1 (Grids cells).
 Let g0 = ∆.
 Let v be vector chosen random [0, g0 −
 Partition space {1, ∆}d regular Cartesian grid G0 side-length g0 translated vertex grid falls v.
 cell C ⊂ [∆]d grid be expressed [v1 + v1 + (n1 + × ··· × [vd + vd + (nd + ⊂ [∆]d (n1, n2,··· nd) ∈ Zd. (Note cell is cartesian product batch continuous interval) i ≥ deﬁne regular grid Gi grid side-length gi = g0/2i aligned such cell Gi−1 contains cells Gi. ﬁnest grid is GL L = (cid:100)log2 ∆(cid:101); cells grid have side-length most contain most single input point.
 grid forms partition point-set Q.
 is d-ary tree such vertex depth i corresponds cell Gi vertex has children are cells Gi+1 contains.
 convenience, deﬁne G−1 entire dataset contains single cell.
 Center Cells Next, show shifted grid structure has good properties.
 ﬁx optimal k-set Z∗ = {z∗ k} ⊂ [∆]d optimal k-means solution (cid:88) input dataset Q, i.e.,


 z∗ z∗ dist2(q, Z) = OPT
 cost(Q, Z = min Z⊂[∆]d:|Z|=k q∈Q z∆k-mediantelescopesumd(p,z)=d(p,z)−d(c2p,z)+d(c2p,z)−d(c1p,z)+d(c1p,z)−d(c0p,z)+d(c0p,z)where|d(p,z)−d(c2p,z)|≤d(p,c2p)where|d(c2p,z)−d(c1p,z)|≤d(c2p,c1p)where|d(c1p,z)−d(c0p,z)|≤d(c1p,c0p)k-meanstelescopesumd2(p,z)=d2(p,z)−d2(c2p,z)+d2(c2p,z)−d2(c1p,z)+d2(c1p,z)−d2(c0p,z)+d2(c0p,z)but|d2(p,z)−d2(c2p,z)|≥∆but|d2(c2p,z)−d2(c1p,z)|≥∆but|d2(c1p,z)−d2(c0p,z)|≥∆c0pc1pc2pp call cell C level Gi center cell is close centers Z∗.
 dist(C, Z = min q∈C,z∗∈Z∗ dist2(q, z ≤ gi (1) gi is side length cell d is dimension space.
 show are small number center cells.
 Lemma D.2. Let ξ denote number center cells grids is upper bounded
 event ξ holds probability least − ρ.
 Proof.
 i consider grid Gi. optimal center z∗ j denote indicator random variable event distance boundary dimension α grid Gi is most gi/(2d).
 dimension, center is boundary, contributes factor most total number center cells.
 follows number cells have distance most gi/(2d) z∗ j is most (cid:80)d N = α=1 Xj,α.
 denote be E[N ] = E (cid:35) Yj,α (cid:34) d(cid:89) α=1 d(cid:89) α=1 E[Yj,α].
 Using Pr[Xj,α ≤ (2gi/(2d))/gi = obtain E[Yj,α] ≤ E[1 + Xj,α] = + E[Xj,α] ≤ +
 E[N ] =(cid:81)d α=1 E[Yj,α] ≤ (1 + ≤ e.
 expected number center cells single grid is most (1 + ≤ ek ≤
 linearity expected number center cells grids is most ekL.
 Markov’s inequality, probability have more center cells grids is most ρ.
 Heavy Cells property small number center cells allows bound number cells containing many points.
 introduce notion heavy cells, are large number input points.
 use OPT, optimal cost k-means solution deﬁne heavy cells.
 However, OPT is known us.
 use arbitrary guess o OPT deﬁne heavy cells.
 Deﬁnition D.3 (Heavy Cell).
 Let > be ﬁxed number.
 i ∈ L − cell Gi is called o-heavy contains least Ti(o) = d2 points.
 cell G−1 is heavy, g2 cell GL is called heavy.
 call threshold function level i.
 o is omitted is clear context.
 i · ρo side length gi is decreasing i increases, thresholding function Ti(o) is increasing i increases.
 particular, Ti+1(o)/Ti(o) = o > i ∈ {0, L −
 Therefore, have following lemma.
 Lemma D.4. ≤ i ≤ L, cell C Gi is heavy, parent cell C(cid:48) Gi−1 be heavy.
 cell C(cid:48) Gi−1 is heavy, children cells Gi cannot be heavy.
 Figure Example o-Partition next lemma bounds number heavy cells.
 Lemma D.5. number center cells is most number o-heavy cells is most OPT /(ρo).
 Proof.
 non-center heavy cell C, contribution input points C optimal k-means cost is least g2 Ti(o) ≥ oρ/(12k), is distance least gi/(2d) optimal center Z∗.
 are most OPT /(oρ) many non-center heavy cells grid.
 total are most OPT /(oρ) + many non-center heavy cells, +1 term comes G−1.
 are most many center cells, total number heavy cells is most + (3kL/ρ)(1 + OPT /o) ≤ OPT /(ρo).
 heavy cells allows construct coreset.
 number heavy cells gives bound many samples need obtain original dataset.
 D.2 Recursive Partition Approach Recall coreset construction, ﬁrst partition input datasets L + disjoint subsets.
 sample points partitions.
 prove Theorem D.14, ﬁrst reﬁne partition follows.
 Deﬁnition D.6 (o-Partition).
 Suppose parameter o >
 Let Q ⊂ [∆]d be input set.
 level ≤ i ≤ L deﬁne set sets P i follows.
 Initialize P i empty set.
 heavy cell C Gi−1, group non-heavy children cells C following manner.
 First note Ti(o) = non-heaviness child cell contains most points Q.
 non-heavy children contain less points Q, take points Q containing single set, add P i.
 make groups such group non-heavy children cells contains least Ti(o) points most points Q.
 put points Q groups set add P i.
 full partition Q is deﬁned P = P ∪P ∪


 ∪ P L.
 i ∈ L}, write set Pi following way, Pi = {Pi,1,Pi,2,··· ,Pi,|Pi|}.
 j ∈ [|Pi|], set Pi,j contains least Ti(o) points, say set Pi,j is heavy set.
 use |Pi,j| denote number points Q contained partition Pi,j.
 Remark D.7. remark algorithm coreset construction does need know ﬁnd such partition.
 partition given is analysis purposes.
 note i ∈ L}, sets P i give partition Qi, i.e., set points falling non-heavy children heavy cell.
 Claim D.8 (Upper bound |Pi,j|).
 i ∈ L}, write set Pi following way, Pi = {Pi,1,Pi,2,··· ,Pi,|Pi|}.
 j ∈ have |Pi,j|
 Proof.
 follows Deﬁnition D.6. Fact D.9. partition P points set Q, have L(cid:88) |Pi|(cid:88) L(cid:88) |P| = |Pi|, |Pi,j| = |Q| i=0 i=0 j=1 Deﬁnition D.10.
 say partition P(d, g) is α, β partition |P| ≤ αk |Pi,j|dg2 i−1 ≤ β OPT
 Proof.
 loss generality, assume L = log ∆ is large, i.e., L ≥
 ﬁrst cardinality P.
 partition sets, split groups.
 ﬁrst group are sets containing center cell.
 are most center cells, number such sets is bounded
 second group are non-heavy sets, sets containing less Ti points.
 number sets are bounded heavy cells heavy cell produce most set.
 Lemma D.5 shows number heavy cells is most OPT /(ρo).
 |P| ≤ αk |Pi,j|dg2 i−1 ≤ β OPT Lemma D.11.
 Given parameters o, α, β < o ≤ OPT, α = OPT /(ρo), β = conditioned number center cells is most
 o-partition P satisﬁes L(cid:88) |Pi|(cid:88) i=0 j=1 L(cid:88) |Pi|(cid:88) i=0 j=1 last group are remaining sets.
 sets contain least Ti−1 many points construction.
 point such set, distance center is least gi/(2d).
 contribution OPT is least Tig2 i /(4d2).
 number such sets level is bounded OPT /(ρo).
 total, upper bound |P| following sense, |P| ≤ OPT /(ρo) + OPT /(ρo) ≤ OPT /(ρo).
 (2) Next, show L(cid:88) |Pi|(cid:88) i=0 j=1 |Pi,j|(gi−1√d)2 ≤ L(cid:88) L(cid:88) i=0 |Pi|(cid:88) |Pi|(cid:88) j=1 i=0 j=1 ≤ |P| · ≤ OPT
 ﬁrst step follows |Pi,j| ≤ (Claim D.8), second step follows Ti(o) ≤ last step follows Eq. (2).
 D.3 Bounding Close Parts Given set Z k points, partition P i ≤ i ≤ L r = O(log(∆√d)) parts based distance Z.
 Formally, let i (Z) = {P ∈ P i dist(P, Z) ≤
 P < j ≤ r (3) (4) i (Z) = {P ∈ P i < dist(P, Z) ≤
 P j distance is upper bounded ∆√d, r = (cid:100)log(∆√d)(cid:101) is suﬃcient partition P i.
 is easy see i ∈ [0, L] j have max p∈P,P⊂P j i (Z) dist(p, Z) ≤ (2j+1 +
 Based deﬁnition, ﬁrst error P Lemma D.12 (Cost parts).
 Suppose are given input point set Q, (α, β)-partition P ∪P ∪


 ∪ P L arbitrary ﬁxed set k points Z ⊂ [∆]d.
 ≤ i ≤ L, let P i (Z) be deﬁned (3) partition sets is distance √dgi−1 Z.
 Let P i denote points i sampling probability ζ P weight
 i be independent sample points P i (Z) coreset next lemma.
 i (Z).
 Let S0 (cid:20) ζ ≥ min 2 · β2 kL · Ti(o) · ln (cid:19) (cid:21) , δ ∈ (0, o ∈ (0, OPT], then, probability least − δ, i |, kL · Ti(o)) · dg2 · max(|P i Z) − cost(S0 i Z) i−1.
 (cid:12)(cid:12) cost(P (cid:12)(cid:12) ≤ Proof.
 recall cost(P i Z) = (cid:88) p∈P d(p, Z)2.
 Let Ip be indicator point p ∈ P cost(S0 i Z) = is sampled.
 Ip d(p, Z)2.
 (cid:88) p∈Pj diﬀerence be written as, | cost(P i Z) − cost(S0 i Z)| ≤ (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) (cid:88) p∈P (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
 (1 − Ip )d(p, Z)2 have p ∈ P i (cid:88) p∈P  = (1 − Ip )d(p, Z)2 (cid:20) (1 − Ip )2d(p, Z)4 (cid:21) (1 − ζ)d(p, Z)4 i−1 Note that, |(1− Ip (cid:88) compute σ2 b follows, ζ )d(p, Z)2| ≤ d(p, Z)2/ζ.
 order apply Bernstein’s inequality, Lemma B.8, (cid:20) d(p, Z)2 (cid:21) i−1 (1 − Ip )2d(p, Z)4 ≤ |P i | b = max p∈P i−1 σ2 = p∈P |P i | ≥ kLTi(o), choose t =  i−1 calculate Bernstein’s inequality, have t2 i |dg2 i |2 · ζ · |P = |P (cid:18) (cid:19) d(p, Z)2 Ip − (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) (cid:88) p∈P Pr (cid:19) t2 i | · ζ (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) > t  ≤ exp (cid:18) (cid:18) − min ≤ exp δ.
 + t2 (cid:20) t2 t2 |P i | ≤ kLTi(o), choose t =  kLTi(o) |P i | t2 = Pr conclude proof.
 · kLTi(o)dg2 kLTi(o)2 · ζ (cid:19) (cid:18) (cid:88) Ip − (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) p∈P i−1 calculate t2 · ζ (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) > t  ≤ δ.
 d(p, Z)2 D.4 Bounding Far Parts goal section is prove Lemma D.13.
 show error coreset S P j i i ∈ [0, L], j ∈ [1, r] is bound.
 Lemma D.13 (Cost parts).
 Suppose are given input point set Q, (α, β)-partition P ∪P ∪


 ∪ P L arbitrary ﬁxed set k points Z ⊂ [∆]d.
 ≤ i ≤ L j ∈ [r] r = (cid:100)log(∆√d)(cid:101), let P j i (Z) be deﬁned (4) partition sets is dis- tance (2j−1√dgi−1, Z.
 Let P j i be independent sample points P j i denote points P j i sampling probability ζ weight
 i (Z).
 Let Sj ζ ≥ min (cid:19) (cid:21) 2 · β2 Ti(o) · ln (cid:12)(cid:12) ≤  · cost(P j , δ ∈ (0, o ∈ (0, OPT], then, probability least − β2 cost(Q, Z).
 i Z) − cost(Sj (cid:12)(cid:12) cost(P j i Z) + i Z) Proof.
 proof is similar Lemma D.12 need consider partition sets containing few number points.
 partition P j i,2(Z) denote P j i,1(Z)∪P j i (Z) = P j i,2(Z) such i,1 P j i,2 points P j ∀P ∈ P j i,1 Sj i,1(Z),|P| ≥ i,1(Z) P j Ti(o) ∀P β2 (cid:88) cost(P j i,r, Z) = d(p, Z)2.
 (cid:48) ∈ P j i,2(Z),|P (cid:48) | < Ti(o) β2
 denote Sj i,2 sampled points set.
 r ∈ {0, recall Let Ip be indicator point p ∈ P j cost(Sj i,r, Z) = i,r is sampled.
 Ip d(p, Z)2.
 p∈P j i,b (cid:88) p∈Pi,r diﬀerence be written as, | cost(P j i,r, Z) − cost(Sj i,b, Z)| ≤ have  (cid:88) p∈P j i,r (1 − Ip )d(p, Z)2 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) (cid:88) p∈P j i,r (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
 (1 − Ip )d(p, Z)2  = p ∈ P j i,r, (cid:20) (1 − (cid:21) Ip )2d(p, Z)4 (1 − ζ)d(p, Z)4 i−1 order apply Bernstein’s inequality, Lemma B.8, compute σ2 b follows, Note that, |(1 − Ip ζ )d(p, Z)2| ≤ d(p, Z)2/ζ.
 (cid:20) (cid:88) σ2 = p∈P j i,r (cid:21) (1 − Ip )2d(p, Z)4 ≤ |P j i,r| i−1 b = max p∈P j i,r d(p, Z)2 (2j+1 + i−1 Case
 r =
 r consider cases: Case
 |P j i,1| ≥ |P j Bernstein’s inequality, have (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  (cid:88) p∈P i,1 Pr dist(P, Z) ≥ P ∈ P j i,1, probability least − δ, have i−1 compute, (cid:21)(cid:19) t2 ≤ δ.
 i,1| · ζ (cid:18) t2 i,1|2 · ζ j i,1 |Ti(o), choose t =  i,1|22jdg2 ≥ |P j = |P j t2 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) > t  ≤ exp (cid:20) t2 (cid:18) (cid:19) (cid:12)(cid:12)(cid:12) ≤  cost(P j i,1, Z) − cost(Sj − min d(p, Z)2 i,1, Z) − Ip i,1, Z).
 (cid:12)(cid:12)(cid:12)cost(P j (cid:113) i,1| < |P j ≤ |P j |P j i,1| · |P j · i,1 |Ti(o), choose i,1 | · Ti(o) · · dg2 i−1 ≤ j i,1|22jdg2 i−1.
 Case
 | P j i,1 |Ti(o) β2 t = compute, t2 = |P j Bernstein’s inequality, have i,1 |Ti(o) ·  · ζ (cid:113) |P j i,1||P j (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) > t  t2 (cid:19) (cid:18) i,1 |Ti(o)2 · ζ (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  (cid:88) (cid:18) p∈P j Pr i,1 Ip − (cid:20) t2 d(p, Z)2 (cid:21)(cid:19) t2 − min ≤ exp ≤ δ.
 (cid:12)(cid:12)(cid:12)cost(P j (cid:12)(cid:12)(cid:12) ≤  cost(P j i,1, Z).
 i,1, Z) − cost(Sj i,1, Z) Thus probability least − δ, have Case
 r =
 Lastly, consider b =
 Notice set contains most Ti(o)/β2 points.
 choose t =  j i,2 |Ti(o)22jdg2 = i−1, compute j i,2 |Ti(o) · i−1 · ζ ≥ σ2 implies (cid:1) (cid:1) t2 + t2  + t 2 · ζ · Ti(o) exp(cid:0) ≤ exp(cid:0) ≤ exp(− ≤ exp(− (cid:19) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ second step follows + ≤
 taking union bound, probability − δ, (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) (cid:18) (cid:88) p∈P j i,2 Ip − d(p, Z)2 j i,2 |Ti(o)22jdg2 i−1.
 construction, part P ∈ P j dist(P, Z) ≥ j dist(CP ∩ Q, Z) ≥
 i,2, corresponding unique heavy cell CP Gi−1.
 j i,2 |Ti(o) · i−1 ≤ ∩ Q| · · i−1 ≤ β2 cost(Q, Z).
 (cid:88) P∈P j i,2 concludes proof.
 Figure (a) show example partitioning set points top bottom.
 are levels total.
 top level is ﬁrst level, bottom level is fourth level.
 number blocks is decreasing, levels is increasing.
 blue bock is heavy cell.
 ﬁrst level, is single blue block.
 second level, partition cells.
 are heavy cells.
 third level, partition heavy cells (in second level) cells.
 has heavy (blue) cell, other has (blue) heavy cells.
 (b)We show example sampling points non-heavy (green) cells.
 second level, are green cells, sample (red) points them.
 third level, are ﬁve green cells, sample (red) points them.
 fourth (bottom) level, are green cells, sample (red) points them.
 D.5 Coreset Construction explain coreset construction, ﬁrst deﬁned probability sampling level Gi, i ∈ [0, L], follows, 2 (144d3L)2 Ti(o) · ln πi(o, , δ) := min (cid:18) + (cid:19) (cid:21) (5) , δ ∈ (0, o > are ﬁxed parameters.
 deﬁne L sets weighted samples follows.
 Denote input point set Q ⊂ [∆]d.
 ≤ i ≤ L, let Ci ⊂ Gi be set non-heavy cells, parent cell Gi−1 is o-heavy.
 Let Qi be set points falling cell Ci. Notice Lemma D.4, Q0, Q1,


 are disjoint sets form partition Q.
 Coreset Construction Let Si be multiset, obtained sampling point q Qi indepen- probability πi(o, , δ).
 point Si has weight , δ)).
 weight set is obtained S(o, , δ) = S0 ∪ S1 ∪


 ∪ SL.
 omit (o, , δ) is clear context.
 Notice coreset set construction is determined parameter o,  δ.
 show next theorem construction gives -coreset k-means Q.
 Theorem D.14.
 < o ≤ OPT, given point set Q ∈ [∆]d.
 Let S(o, , δ) be multiset deﬁned (6).
 conditioning event ξ holds, have ekL/ρ center cells, probability least − δ, S(o, , δ) is -coreset k-means Q (cid:19) −2k2d7L4 log(1/δ) · OPT·o (cid:20) Proof.
 Let (cid:48) = /3.
 Take sampling probability β2 Ti(o) · ln + log d)2∆dk E[|S|] = O( −1).
 (cid:21) ζ = π(o, , δ) = min (cid:48)2 · Let S = S(o, , δ) be obtained multiset.
 (4) i ∈ [0, L] j ∈ [r] r (cid:100)log(∆√d)(cid:101).
 Let P j falling part P j ﬁxed set Z ⊂ [∆]d k points, let P j i
 Let Sj i be subsets partitions deﬁned (3) i ⊂ Q be corresponding point sets i be corresponding sampled set P j i
 Notice S = ∪i∈[0,L],j∈[r]Sj i
 Lemma D.12, Lemma D.13 union bound possible i ∈ [0, L] j ∈ [r], probability least − δ ∆dk Conditioning above inequalities, have (cid:48) · |P i | · Ti(o) · dg2 i−1 (cid:88) (cid:48) |cost(S, Z) − cost(Q, Z)| ≤ cost(P j i∈[0,L] i Z) + i∈[0,L],j∈[r] (cid:19) β2 cost(Q, Z) Lemma D.11, (cid:88) i∈[0,L] |P i | OPT (cid:48) · |P i | · Ti(o) · dg2 i−1 ≤ (cid:48) (cid:48) · OPT =  OPT
 β = Hence have (L + log d/2)/β2 ≤ (cid:88) β2 cost(Q, Z) ≤  (cid:48) i∈[0,L],j∈[r] cost(Q, Z).
 i Z) (cid:12)(cid:12)cost(S0 (cid:12)(cid:12) cost(P j i Z) − cost(P (cid:12)(cid:12) ≤ i Z)(cid:12)(cid:12) ≤  i Z) − cost(Sj (cid:88) (cid:48) (cid:48) · |P i | · Ti(o) · dg2 i−1 · cost(P j i Z) + β2 cost(Q, Z).
 have (cid:48) |cost(S, Z) − cost(Q, Z)| ≤ cost(Q, Z).
 Notice = .
 are most ∆dk possible Z, union bound, have Z ⊂ [∆]d |Z| ≤ k, |cost(S, Z) − cost(Q, Z)| ≤  cost(Q, Z).
 bound number points sampled.
 Lemma D.11, are most |P| ≤ αk = OPT parts o-partition.
 parts contains most points.
 Therefore, expectation, (cid:88) i∈[0,L] E(|S|) = |P i | · πi(o, , δ) OPT (cid:18) k2d7L4 × (cid:18) (cid:19) 2 (cid:19) · (3 × · ln OPT = O log 2 (cid:18) + (cid:19) Remark D.15.
 coreset be constructed oblivious point set, i.e., using hash functions ho,i [∆]d → such probability ho,i(p) = is πi(o, , δ).
 Later, pick set points hash value is available.
 construct coreset
 Observing proof, only requirement bound cost partition.
 have following proposition.
 Proposition D.16.
 < o ≤ OPT, given point set Q ∈ [∆]d.
 Let S(o, , δ) be mul- tiset deﬁned (6).
 conditioning event exists o-partition (see Deﬁnition D.6) satisfying |P | ≤ αk |P| · Ti(o) · dg2 i−1 ≤ d3L OPT (cid:88) (cid:88) i∈[0,L] P∈P i probability least − δ, S(o, , δ) is -coreset k-means Q E[|S|] = O( −2k2d7L3α log
 E Coreset Dynamic Data Stream E.1 Streaming Coreset Construction ﬁrst step, modify coreset construction such streaming algorithm maintain coreset construction.
 only diﬃculty is cannot obtain number points cell exactly.
 overcome diﬃculty modifying heavy cell deﬁnitions follows.
 Deﬁnition E.1 ((, o)-Heavy Cell Scheme).
 Fixing number  ∈ [0, set input points Q ⊂ [∆]d grid structure G = {G−1, G0,


 GL} [∆]d deﬁned Deﬁnition D.1. call procedure (o, )-heavy cell scheme satisﬁes,
 ∀C ∈ G−1, C is o-heavy; ∀C ∈ GL, C is heavy;
 i ∈ [0, L], cell C ∈ Gi |C ∩ Q| ≥ Ti(o), C is identiﬁed o-heavy;
 i ∈ [−1, L], cell C |C ∩ Q| < (1 − )Ti(o), C is identiﬁed heavy;
 i ∈ [−1, L − cell C ∈ Gi is identiﬁed heavy, children cells C(cid:48) verify heavy cells Deﬁnition D.1 is given (o, 0)-heavy cell is identiﬁed heavy; ∈ Gi+1 scheme.
 particular, have following theorem.
 Theorem E.2. Suppose < ≤ OPT, ≤  ≤ are given point set Q ∈ [∆]d.
 Let S(o, , δ) be multiset deﬁned (6) using heavy cells deﬁned (o, )-heavy cell scheme.
 conditioning event ξ holds, have ekL/ρ center cells, probability least δ, S(o, , δ) is -coreset k-means Q −1).
 E[|S|] = O( −2k2d7L4 log(L/δ) · OPT·o E.2 Dynamic Point-Cell Storing Data Structure section, introduce data structure maintains coreset dynamic data stream.
 begin, ﬁrst introduce K-Set data structure, use sub-routine algorithm.
 Lemma E.3 (K-Set).
 Given parameter M ≥ N ≥ k ≥ δ ∈ (0,
 is data structure requires O(k(log M +log N log(k/δ)) bits is able process data stream S(which contains insertion deletion).
 data structure processes stream operation (i,±) (i ∈ [N ]) O(log(k/δ)) time.
 time t, let Mt denote sum frequency element.
 Let M = maxt Mt. point time t, supports operation Query, number distinct elements is ≤ k, probability least δ returns set items frequencies items.
 returns ∅ otherwise.
 coreset relies data structure deﬁned Algorithm
 deﬁnes data structure supports inserting deleting cell-point pairs.
 example, input is form (C, p, op), C is cell grid, identiﬁed ID (including information level coordinates), p ∈ C op is + − representing insertion deletion.
 has following guarantees.
 Lemma E.4. is data structure (procedure SampleStream+ Algorithm supports insertion deletions cell-point pairs [2L∆d] × [∆]d query operation.
 time point, query operation returns follow results, (1) C, f S = {SC ∀C ∈ C}, C is cells undeleted, f is function encoding number points cell C, ∀C ∈ C, SC is set points C |C| ≤ β; (2) ∅.
 conditioning following events, (I) are most α non-empty cells; (II) are most θ cells contains more β points; (III) number points other cells containing most β points is most γ, algorithm outputs ∅ probability most δ.
 algorithm uses bits space worst case.
 O[(θβ + γ) log(∆d) · log((θβ + γ)/δ)].
 Proof.
 ﬁrst show conditioning algorithm outputs set points high probability.
 show events do happen, output algorithm matches requirement statement.
 Let C denote set cells, let P denote set points.
 identify cell C ∈ C coordinate.
 deﬁne hash function h C → [a1 · α], g [P ] → [a2 · β] f [a1 · α] × [a2 · β] → [a3(θβ + γ)], constant a1, a2, a3 ≥ be decided later) h, g, f are pair-wise independence function.
 w ∈ [a3(θβ + γ)], j ∈ [2 log(∆d)] z ∈ {0, deﬁne counter counter(w, j, z) ∈ {0,
 initialize entries counter be
 Note number counters is + γ) log(∆d)).
 update (C, p, update update counter following way, counter(f (h(C), g(p)), j, (C, p)j) ← counter(f (h(C), g(p)), j, (C, p)j) ± mod ∈ [2 log(∆d)], (C, p)j is j-th bit pair (C, p), “+” is corresponding insertion, “−” is corre- sponding deletion.
 initialize K-Set structure store most α cells, update cell C it.
 w ∈ [a3(θβ + γ)] deﬁne counter size(w) ∈ {0, ∆d}.
 initialize entries counter be
 Note number counters is a3(θβ + γ).
 update (C, p, update counter following way, size(f (h(c), g(p))) ← size(f (h(c), g(p))) ± “+” is corresponding insertion, “−” is corresponding deletion.
 ﬁxed cell C, cell C(cid:48) (cid:54)= C, have (cid:48) [h(C) = h(C XC(cid:48) = otherwise.
 deﬁne X deﬁne random boolean variable XC(cid:48) such XC(cid:48) = h(C(cid:48)) = h(C) b is non-empty; C(cid:48) /∈C \{C} xC(cid:48), compute E[X] = Θ(1/a1).
 Markov’s Pr h∼H )] = Θ(1/(a1α)) inequality, Pr[X ≥ ≤ E[X]/1 ≤ Θ(1/a1).
 choosing a1 be large constant, obtain probability least is non-empty cell C(cid:48) ∈ C \{C}, satisfying h(C(cid:48)) = h(C).
 ﬁxed x ∈ [a1α], conditioned non-empty cell c is hashed bin c has most β points.
 consider ﬁxed y ∈ [a2β], suppose is point p such g(p) = y.
 other point p(cid:48)(belong same cell C), g is pair-wise independence hash function, have (cid:48) [g(p) = g(p Pr g∼G )] = Θ(1/(a2β)) Y =(cid:80) deﬁne random variable Yp(cid:48) such Yp(cid:48) = g(p) = g(p(cid:48)); Yp(cid:48) = otherwise.
 deﬁne p(cid:48) /∈C∩P\{p}, compute E[Y ] ≤ Θ(1/a2).
 Markov’s inequality, Pr[Y ≥ ≤ E[Y ]/1 ≤ Θ(1/a3).
 choosing a3 be large constant, obtain probability least is p(cid:48) ∈ P ∩ C\{p}, satisfying g(p) = g(p(cid:48)).
 Moreover, are most (θβ + γ) distinct pairs p).
 Repeating above argument, observe give pair (h(C), g(p)), probability least other pairs (h(C(cid:48)), g(p(cid:48))) has same hash value f (h(C), g(p)).
 say entry (C, p) is good, cell C is non-empty contains most β points.
 show good entry (C, recover point p probability least total number good pairs is most γ, repeating above procedure Θ(log(γ/δ)) have good entry (C, recover point p probability least − δ/γ.
 taking union bound most γ pairs, have probability least − recover good entries (C, p).
 taking union event K-Set data-structure is working (with probability least − δ/2), obtain probability − δ.
 remains show events happening algorithm outputs incorrect result.
 number cells is greater α, K-set structure fails, know number cells are greater α, algorithm outputs ∅.
 remaining events does happen are good pairs collide hash algorithm detect counters.
 points recovered are stream.
 proper pruning detects possible error.
 Overall, single repeat, number bits used hash function is O(log(∆d)), number bits used counter counteri() is O(a3(θβ + γ)) log(∆d), number bits used counter sizei() is O(a3(θβ + γ)) log(∆d).
 number bits K-Set is O(α log(∆d) log(α/δ)).
 Putting together, th number bits used repeats is O((θβ + γ) log(∆d) · log((θβ + γ)/δ)).
 E.3 Main Algorithm Theorem E.5 (k-means).
 Fix , δ ∈ (0, k, ∆ ∈ N+, let L = log ∆.
 is data structure supporting insertions deletions point set P ⊂ [∆]d, maintaining weighted set S positive weights point, such probability least δ, S is -coreset k-means size data structure uses −2k2d7L4 log(1/δ)), O( (cid:101)O( −2k2d8L5 · (dkL + log(1/δ)) · log(1/δ)) bits worst case.
 update input, algorithm needs poly(d, L, log k) time process outputs coreset time poly(d, k, L, log k) pass stream.
 Figure Proof.
 Let point set end insertion deletion operations be Q ⊂ [∆]d.
 loss generality, assume optimal cost k-means is most OPT OPT >
 OPT = set Q contains most k points.
 use simple K-set structure maintain k points.
 correctness such algorithm is guaranteed Lemma E.3. Further observe OPT ≥ pair points is distance least
 show data structure, ﬁrst pick constant number ρ > use failure probability.
 show boost small δ >
 prove theorem showing aδ failure probability, is absolute constant.
 boost probability losing constant factor space.
 Data Structure Let ρ > be constant, i.e., take ρ =
 data structure ﬁrst initializes randomized grid structure G−1, G0, G1,


 GL Deﬁnition D.1. guess logarithmic many possible values OPT, i.e., let O = {1,


 poly log(∆d+2d)}.
 guess o, initialize SampleStream+ data structure SSo parameters α, β, γ, θ, δ, α, β, γ, θ be determined.
 o level i ∈ [−1, L], initialize new hash function ho,i [∆]d → such ∀p ∈ [∆]d Pr[ho,i = = πi(o, , δ) , δ ∈ (0, πi(·,·,·) is deﬁned (5).
 representation simplicity, ﬁrst assume ho,is are independent hash functions.
 show de-randomize using limited independence hash functions.
 o ∈ O i ∈ [−1, L], point p operation, let Ci(p) be cell containing p Gi. identify Ci(p) ID.
 i ∈ [−1, L], ﬁrst compute hash value ho,i(p), ho,i(o) update (Ci(p), p) data structure SSo corresponding operations Ch:C→[a1α]h:C→[a1α]Pg:[P]→[a2β]g:[P]→[a2β]f:[a1α]×[a2β]→[a3(θβ+γ)]f:[a1α]×[a2β]→[a3(θβ+γ)]wsize(w)counter(w,∗,0)counter(w,∗,1) Choose hi C → [a1α] be pair-wise independent hash function Choose gi [P ] → [a2β] be pair-wise independent hash function Choose fi [a1α] × [a2β] → [a3(θβ + γ)] be pair-wise independent hash function Initialize counter counteri(w, j, z) ← ∈ [a3(θβ + γ)], j ∈ [2 log(∆d)], z ∈ {0, Initialize counter sizei(w) ← ∈ [a3(θβ + γ)] (cid:46) C, p cell point, op ∈ {−, +} (cid:46) Hash cell bins (cid:46) Hash point bins (cid:46) Update counter (cid:46) Update counter Si,C ← ∅, x ← hi(C) y ∈ [bβ] do w ← fi(x, y) sizei(w) == ∀j ∈ [2 log(∆d)], counter(w, j, + counter(w, j, = end end procedure procedure Update(C, p, op) procedure Init(∆, d, α, β, γ, θ, δ) KS.Update(C, op) i = → Rrepeats do j → [log ∆d] do a1 ← Θ(1), a2 ← Θ(1), a3 ← Θ(1), Rrepeats ← Θ(log((θβ + γ)/δ)) Initialize K-Set data-structure KS.Init(α, ∆d, δ/α) C ← set cells (i.e., L + levels grids).
 i = → Rrepeats do x ← hi(C) y ← gi(p) w ← fi(x, y) z ← j-th bit bit representation (C, p) counteri(w, j, z) ← counteri(w, j, z) op mod sizei(w) ← sizei(w) op Algorithm Better Data Structure Small Number Bits procedure SampleStream+ end procedure Si ← ∅ KS.Check() (cid:54)= Fail KS.size ≤ α end S ← Merge(S1, S2,··· SRepeats) ∃C ∈(cid:101)C,(cid:101)f (C) ≤ β |SC| (cid:54)= (cid:101)f (C) return (cid:101)C,(cid:101)f S return ∅ (C, p) ← BitToNumber(counter(w,∗,∗)), Si,C ← Si,C ∪ {(C, p)} (cid:101)C,(cid:101)f ← KS.Query() c ∈ (cid:101)C do end procedure procedure Query() i = → Rrepeats do end return ∅ end S ← S ∪ Si end end procedure end end end end p.
 completes description data structure.
 complete data structure is presented Algorithm
 Retrieve Coreset Next describe retrieve coreset data maintained above data structure.
 ﬁnd smallest o∗ ∈ O such SSo∗ does return ∅.
 SSo∗, obtain set cells (cid:101)C function (cid:101)f encodes number points cell in(cid:101)C.
 obtain set SC C ∈(cid:101)C ∩ Gi is ∅ SC = C ∩ Qo,i, Qo,i = {q ∈ Q ho,i(q) =
 C estimate number points C, |C ∩ Q|, (cid:101)f (C)(1 + /2)/πi(o, , δ).
 this, identify heavy cells in(cid:101)C following Deﬁnition D.14.
 this, initialize multiset S = ∅.
 non-heavy cell C (cid:101)C, assign weight , δ) point SC.
 this, add points SC S.
 completes description retrieving coreset.
 complete data structure is presented Algorithm
 Correctness Constant Probability ﬁrst show chosen constant ρ, i.e., ρ > event ξ happens, i.e., are most ekL/ρ center cells grid, probability least − data structure maintains -coreset S k-means.
 Notice Pr[ξ] ≥ − ρ.
 show correctness data structure, show exists ≤ o ≤ OPT, output S is -coreset k-means Q.
 Heavy Cell Identiﬁcation First, ≤ o ≤ OPT, show probability least − δ, i ∈ [0,−L] C ∈ Gi, estimator points cell, (cid:98)co,i = |Qo,i ∩ C|/π(o, , δ) · (1 + /2), operations Deﬁnition D.14 give (o, )-heavy cell scheme (Deﬁnition E.1) identifying heavy cells level −1 L removing heavy cells parent cells identiﬁed non-heavy.
 is deﬁned (5).
 is suﬃce show heavy cells C ∈ Gi, |C ∩ Qo,i|/π(o, , δ) is (cid:18) (1 ± /2)-approximation |Qo,i ∩ C|.
 Observing 2 · (d3L)2 · E [|C ∩ Qo,i|] ≥ O (cid:19)(cid:19) dkL + log Chernoﬀ bound union bound heavy cell cells, obtain desired precision estimation.
 Correct Parameters SampleStream section, show exists ≤ o ≤ OPT such probability least − δ, SampleStream instance SSo returns set points non-heavy cells estimates number points heavy cells.
 show this, is suﬃce show OPT /2 ≤ ≤ OPT satisﬁes above statement probability least − δ.
 show probability least − δ/4 are most α (to be determined) non-empty cells grids containing sampled point (i.e., ho,i(p) =
 (cid:18) (cid:19)(cid:19) πi(o, , δ) = O (d3L)2 Ti(o) · dkL + log (cid:18) 2 · (cid:18) Indeed, point non center cell, contributes OPT least g2 i /(4d2) (see Deﬁnition (1)).
 Therefore, are most OPT g2 i /(4d2) g2 OPT = · ek ρ · OPT points non-center cells level grid, Ti(o) = d2 ek is thresholding function.
 Let g2 Q(cid:48) o,i be set points non-center cells level i ho,i value set sampled points non-center cells.
 Therefore, i · ρo E [|Qo,i|] = · ek ρ · OPT · πi(o, , δ) = O dkL + log use fact OPT /o ≤
 Chernoﬀ bound union bound grids, are most (cid:19)(cid:19) (cid:18) k (cid:18) 2 · (d3L)2 · (cid:19)(cid:19) points non-center cells being sampled.
 Hence are most number non-center cells containing sampled point.
 number center cells, are most cells containing sample point.
 pick (cid:18) kL (cid:18) kL (cid:18) 2 · (d3L)2 · dkL + log α = O 2 · (d3L)2 · dkL + log β = Θ 2 · (d3L)2 · dkL + log (cid:19)(cid:19) (cid:19)(cid:19) (cid:18) (cid:18) (cid:18) (cid:18) kL (cid:19) (7) (8) Chernoﬀ bound union bound cells, probability least − δ/4, have non-heavy cell containing most β/2 sample points.
 Next, are most O(kL/ρ) heavy cells, set θ = Θ (9) are most θ cells containing more β sample points.
 Lastly, pick γ = α upper bound number points cells most β sample points.
 such, conditioning above events, Lemma E.4, probability least − δ, data structure SSo outputs desired set points cells counts.
 Theorem E.2, multiset is -coreset k-means Q.
 Boost Probability − ρ − O(δ) − probability boosting procedure is δ times take output repeating above procedure parallel u = O(log follows.
 r ∈ [u], suppose r-th repeat, output guess (if exists) OPT is SampleStream structure is SSr multiset is Sr number heavy cells returned r, i.e, SSr estimates number points non-heavy children heavy cell, divide Ti(o) obtain α(cid:48)(cid:48) r.
 Consider repeats r.
 Based heavy cell scheme, estimate number parts α(cid:48)(cid:48) is α(cid:48) large constant a1.
 Let set repeats satisfying requirement be R.
 (cid:48)(cid:48) r ≤ a1kL Let r∗ = arg minr∈R or.
 ﬁnal output is taken Sr∗.
 show overall failure probability such operation is most O(δ), ﬁrst claim is that, ∃or ≤ OPT α(cid:48)(cid:48) r ≤ a1kL, repeat r, probability least δ, number parts or-partition (see Deﬁnition D.6) is most O(kL).
 follows Chernoﬀ bound, probability least − number points set more points be estimated.
 is straight verify partition satisﬁes requirement Proposition D.16.
 Thus, Proposition D.16, probability least − output multiset Sr is -coreset k-means Q.
 Therefore, exists guess ≤ OPT output sequence, minimum, or∗ ≤ OPT probability least − O(δ) obtain desired coreset.
 show exists r ∈ [u], ≤ OPT.
 repeat is independent, probability least − O(δ), exists r most ekL/ρ centers cells grid r-th repeat.
 Conditioning event, r-th repeat outputs multiset ≤ OPT probability least − O(δ).
 concludes probability boosting.
 Space Bound Random Bits Lastly show space algorithm.
 SampleStream data structure uses α log( · dL = O 2 · (d3L)2 · dL · dkL + log log(kLd/) + log log bits space.
 O(d log ∆) repeats, overall space complexity is (cid:16) (cid:101)O (cid:18) kd2L3 (cid:17) (cid:18) kL (cid:18) (cid:18) (cid:19) (cid:19) = (cid:101)O (cid:18) (cid:19) (cid:18) kd8L5 (cid:18) (cid:19)(cid:19) (cid:19) log (cid:19) · (d3L)2 · 2 dkL + log · log 2 dkL + log independent hash function use, de-randomize method [FS05] using z-wise independent hash functions large z, use pseudo-random generator [Ind00b] apply auxiliary algorithm argument [BFL+17].
 case, show space random bits is dominating.
 completes proof.
 F General Clustering Problem F.1 M-Estimator Clustering framework be extended other clustering problems, e.g., consider following (cid:88) M-Estimator clustering problem, ﬁxing Q ⊂ [∆]d, (10) min Z⊂[∆]d:|Z|≤k p∈Q M (dist(p, Z)) M (·) is non-decreasing function satisﬁes ∀x > M (x) > ∀c > M (cx) ≤ f (c)M (x) f (c) > is bounded function.
 Algorithm Data Structure procedure SampleKMeans procedure Init(k, ∆, d, , δ, δ) O ← {1, poly(d, ∆d)} ρ ← Construct G−1, G0,


 GL o ∈ O do i = → L − do (cid:46) grids deﬁned Section D.1 using parameter ρ Construct hash function ho,i [∆]d → {0, s.t. Pr ho,i∼H α, θ, γ β are determined (7), (8) (9) RecordPointso ← SampleStream.Init(∆, d, α, β, γ, θ, δ) end [ho,i(p) = = πi(o, , δ) (cid:46) ∈ {−, +} (cid:46) So = {So,0, So,1,··· S0,L−1} end end procedure return S end end procedure procedure Update(p, op) o ∈ O do i = → L − do ho,i(p) c ←Cell(p) Gi RecordPointso.Update(c, p, op) end end end procedure procedure Query() o ∈ O do end o∗ (cid:101)Co,(cid:101)f o, So ← RecordPointso.Query() S ← PointsToCoreset((cid:101)Co∗ ← arg (cid:54)= ∅,∀i ∈ {0, L − ,(cid:101)f o∗ So∗ end procedure Theorem F.1 (M-Estimator Clustering).
 Fix , δ ∈ (0, k, ∆ ∈ N+, let L = log ∆.
 is data structure supporting insertions deletions point set P ⊂ [∆]d, maintaining weighted set S positive weights point, such probability least − δ, S is -coreset M-estimator clustering problem deﬁned (10) size −2k2dηL4 log(1/δ)), O( η is absolute constant depending M.
 data structure uses (cid:101)O( −2k2dηL5 · (dkL + log(1/δ)) · log(1/δ)) bits worst case.
 update input, algorithm needs poly(d, L, log k) time process outputs coreset time poly(d, k, L, log k) pass stream.
 Proof.
 repeat proof k-means.
 proof, need modify heavy cell deﬁnition, i.e., thresholding function Ti(o), Ti(o) = M (gi/(2d)) · proof Theorem D.14, need modify β be, L · f (√d) (cid:32) β = Θ (cid:33) ρ · ek f rest proof follows replacing dist2(·) M (dist(·)).
 verify dependence d changes.
 Improvements k-median F.2 section, show developed techniques improve k-median con- struction [BFL+17].
 particular, have following guarantee.
 Theorem F.2 (k-median).
 Fix , δ ∈ (0, k, ∆ ∈ N+, let L = log ∆.
 is data structure supporting insertions deletions point set P ⊂ [∆]d, maintaining weighted set S positive weights point, such probability least − δ, S is -coreset k- median size data structure uses −2k · poly(d, L) · log(1/δ).
 (cid:101)O( −2k · poly(d, L) · log(1/δ)) bits worst case.
 update input, algorithm needs poly(d, L, log k) time process outputs coreset time poly(d, k, L, log k) pass stream.
 Proof.
 Using same construction [BFL+17].
 improvements be stated follows.
 take union level union bounding center cells, bound overall center cells.
 saves L factor space.
 doing be achieved using constructed SampleStream procedure, saves L factor.
 boost failure probability ρ arbitrary δ > using procedure proof Theorem E.5. Remark F.3. Using developed technique, save space complexity O(1/δ) factor, comparing [BFL+17].
 Furthermore, developed stream sampling algorithm saves space poly(dL) factors.
 G Applications section, show dynamic data structure be used maintain solution many problems dynamic streaming setting.
 G.1 A Dynamic Streaming Approximation Max-CUT section, show coreset construction be used obtain 1/2-approximation Max-CUT form M (dist(p, q)) (11) (cid:88) (cid:88) p∈C1 q∈C2 max C1∪C2 M (·) is M-estimator C1 C2 partition streaming point set P ⊂ [∆]d.
 obtain following result.
 Theorem G.1 (Max-CUT).
 Fix , δ ∈ (0, ∆ ∈ N+, let L = log ∆.
 is data struc- O(cid:0) (cid:1) bits worst case.
 update input, algorithm needs ture supporting insertions deletions point set P ⊂ [∆]d, outputs 1/2-solution cost estimation (1 ± ) factor probability least − δ.
 data structure uses poly(d, L) time process outputs coreset time poly(d, L, pass stream.
 Proof.
 solve Max-CUT, use random solution, i.e. use hash function maintain random cuts C1 C2.
 Let OPTCUT be optimal value Max-CUT.
 is standard result that, 2 · poly(d, L) · log (cid:88) (cid:88) p∈C1 q∈C2 (cid:88) (cid:88) p(cid:48)∈S1 (cid:88) (cid:88) q(cid:48)∈S2 p(cid:48)∈S1 q(cid:48)∈S2 (cid:48) M (dist(p (cid:48) q OPTCUT M (dist(p, q))  ≥ (cid:88) (cid:88) = (1 ± /2)2 (cid:88) p(cid:48)∈S1 p∈C1 )) = (1 ± ) )) = (1 ± /2) q∈C2 (cid:88) (cid:88) (cid:88) q∈C2 M (dist(p (cid:48) q)) M (dist(p, q)).
 approximate cost, use coreset obtain (/2)-coresets 1-M-clustering C1 C2.
 Let coresets be S1 S2.
 show M (dist(p (cid:48) (cid:48) q M (dist(p, q)).
 Indeed, q∈C2 desired.
 probability boosting solution is standard.
 p∈C1 G.2 A Dynamic Streaming Approximation Average Distance similar proof Max-CUT, obtain dynamic estimation average distance, (cid:88) |Q| − p,q∈Q M (dist(p, q)) M (·) is M-estimator.
 Theorem G.2 (Average Distance).
 Fix , δ ∈ (0, ∆ ∈ N+, let L = log ∆.
 is O(cid:0) (cid:1) bits worst case.
 update input, algorithm needs data structure supporting insertions deletions point set P ⊂ [∆]d, outputs (1 ± ) approximation average distance P probability least − δ.
 data structure uses poly(d, L) time process outputs coreset time poly(d, L, pass stream.
 2 · poly(d, L) · log Proof.
 proof is similar Theorem G.1 constructing corset 1-M-Clustering Q.
 H (cid:101)O(k) Space Algorithm Based Sensitivity Sampling H.1 Deﬁnitions Preliminaries Deﬁnition H.1 (γ-important partition).
 Given set points Q ⊂ [∆]d, parameter γ ∈ (0,
 Let Pi,j be partition sets deﬁned level i ∈ L].
 Let P denote partition, deﬁne sets R, R ⊂ {0, L} follows i (cid:12)(cid:12)(cid:12)(cid:12) |Pi|(cid:88) j=1 R = |Pi,j| ≤ γ · Ti(o) P = ∪L i=0Pi = ∪L i=0 ∪ |Pi| j=1 {Pi,j}.
  R = i (cid:12)(cid:12)(cid:12)(cid:12) |Pi|(cid:88) j=1 
 |Pi,j| > γ · Ti(o) deﬁne important partition non-important partition follows P = ∪i /∈RPi P N = ∪i∈RPi. Let QI denote set γ-important points Q contained γ-important partition P I.
 Let QN denote set γ-non-important points Q contained γ-non-important partition P N.
 context, value γ is clear, omit γ, call QI important points, call P important partition.
 Theorem H.2 (Oﬀ-line Algorithm, [FL11]).
 Given set points Q ⊂ [∆]d, point p ∈ Q, let s(p) denote sensitivity point p is deﬁned follows s(p) = max Z∈[∆]d,|Z|=k (cid:80) dist2(p, Z) q∈Q dist2(q, Z) sample, chooses point p probability s(cid:48)(p)/t(cid:48) s(cid:48)(p) ∈ [s(p), t(cid:48) =(cid:80) Let A denote sampling procedure deﬁned following way: repeats m independent samples; p∈Q s(cid:48)(p); sample, point p got chosen, associated weight w(p) = t(cid:48)/(ms(cid:48)(p)) point.
 Let (S, w(·)) denote set points weights outputted A.
 m ≥ Ω(t(cid:48)−2(log |Q| log t(cid:48)+ log(1/δ))), probability least − δ, (S, w(·)) is (1 ± )-coreset size m Q.
 H.2 Reducing Original Problem Important Points section, show want output coreset original point set, need ﬁnd coreset important points.
 means drop points are “non-important” level.
 Lemma H.3. Given set points Q ⊂ [∆]d associated partition P deﬁned follows, Let P N ⊂ Q denote set non-important points associated partition P N deﬁned follows, P = P0 ∪ P1 ∪ ··· ∪ PL.
 P N = ∪i∈RPi, (cid:80)|Pi| j=1 |Pi,j| ≤ γTi(o)} γ ≤ /(200Ld3ρ).
 Let QI denote set points R = {i | contained non-important partitions P I.
 Z ⊂ [∆]d |Z|, have (I) cost(Q, Z) ≥ cost(QI Z) (II) cost(Q, Z) ≤ (1 + ) cost(QI Z) Proof.
 Proof (I).
 is true, QN is subset Q.
 Proof (II).
 consider i satisﬁes |Pi|(cid:88) j=1 |Pi,j| ≤ γ · Ti(o), ﬁx j ∈ [|Pi|].
 Let c(Pi,j) denote cell (i − 1)-th level contains points partition Pi,j.
 point p ∈ Pi,j, let Ni,j denote set important points (⊂ QI) contained cell c(Pi,j), exists point q ∈ Ni,j satisﬁes dist2(p, Z) ≤ dist2(p, q) + dist2(q, Z) ≤ ≤ i−1 + dist2(q, Z) i−1 + (cid:88) |Ni,j| q∈Ni,j dist2(q, Z) (12) ﬁrst step follows triangle inequality, second step follows deﬁnition grids, last step follows averaging argument.
 According deﬁnition Ni,j, |Ni,j| ≥ Ti−1(o) − γTi(o) ≥ Ti−1(o).
 lower bound cost(Q, Z) following sense, cost(Q, Z) = cost(QI Z) + cost(QN Z) = cost(QI Z) + dist2(p, z) ≤ cost(QI Z) + |Pi,j| · i−1 + |Ni,j| dist2(q, Z) ≤ cost(QI Z) + |Pi,j| · i−1 + Ti−1(o) q∈Ni,j dist2(q, Z) ≤ cost(QI Z) + |Pi,j| · i−1 + Ti−1(o) dist2(q, Z) ≤ cost(QI Z) + i−1 + Ti−1(o) dist2(q, Z) ≤ cost(QI Z) + · Ti−1(o) q∈QI dist2(q, Z) (cid:19) dg2 = cost(QI Z) + · ≤ cost(QI Z) + + cost(QI Z)) ≤ cost(QI Z) + cost(Q, Z)/(3k) + cost(QI Z)).
 cost(QI Z) Ti−1(o) i−1 + j=1 p∈Pi,j j=1 i∈R i∈R (cid:88) |Pi|(cid:88) (cid:88) (cid:88) |Pi|(cid:88) (cid:88) |Pi|(cid:88) |Pi|(cid:88) (cid:88) (cid:88) i∈R j=1 i∈R j=1 γTi(o) · i∈R dg2 dg2 dg2 dg2 dg2 (cid:18) i−1 + q∈Ni,j (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) q∈QI q∈QI      (cid:80)|Pi| second step follows deﬁnition cost, third step follows Eq. (12), fourth step follows |Ni,j| ≥ Ti−1(o)/2, ﬁfth step follows Ni,j ⊂ QI, sixth step follows i=1 |Pi,j| ≤ γTi(o), seventh step follows |R| ≤ L + ≤ ninth step follows Ti(o) = Ti(o) = d2ρo/(3g2 implies i k), last step follows o ≤ OPT ≤ cost(Q, Z).
 cost(Q, Z) cost(Q\QN Z) ≤ + − ≤ + − /4 ≤ + , second step follows γ ≤ /(200Ld3ρ), last step follows γ <
 complete proof.
 H.3 Sampling Scores Important Points section, focus sensitivity important points.
 ﬁrst give good upper bound sensitivity important points.
 show sum upper bound sensitivities is small.
 Lemma H.4. Given point set Q associated partition P parameter o ∈ (0, OPT].
 p ∈ Q, exist unique partition Pi,j contains p.
 Let QI ⊂ Q denote set γ-important points, γ /(200Ld3ρ).
 have, Z ⊂ [∆]d |Z| = k, p ∈ QI, (cid:80) dist2(p, Z) q∈QI dist2(q, Z) ≤ d3 Ti(o) Proof.
 Fix p ∈ QI.
 Let Pi,j denote partition contains point.
 Let c(Pi,j) denote cell i − 1-th level contains points partition Pi,j.
 Let Ni,j denote set non-important points cell c(Pi,j).
 is easy observe (cid:48) ∃p ∈ Ni,j, dist2(p (cid:48) Z) ≤ |Ni,j| q∈Ni,j dist2(q, Z) (13) (cid:88) (cid:80) + dist2(p, p(cid:48)) q∈QI dist2(q, Z) q∈Ni,j dist2(q, Z) dist2(p(cid:48), Z) q∈QI dist2(q, Z) i−1 dg2 q∈QI dist2(q, Z) q∈QI dist2(q, Z) q∈QI dist2(q, Z) dg2 i−1 q∈QI dist2(q, Z) q∈QI dist2(q, Z) (cid:80) (cid:80) + + have (cid:80) dist2(p, Z) q∈QI dist2(q, Z) ≤ ≤ ≤ = ≤ ≤ ≤ (cid:80) (cid:80) (cid:80) (cid:80) (cid:80) |Ni,j| |Ni,j| |Ni,j| |Ni,j| |Ni,j| |Ni,j| (cid:80) + + + dg2 i−1 q∈QI dist2(q, Z) dg2 i−1 OPT dg2 OPT + d3ρo Ti(o)k OPT + d3ρo Ti(o)k OPT + d3 Ti(o) ≤ ≤ ≤ Ti(o) Ti(o) d3 Ti(o) ﬁfth step follows (cid:80) ﬁrst step follows triangle inequality, second step follows Eq. (13) p(cid:48) ∈ c(Pi,j), q∈QI dist2(q, Z) ≥ (1 − ) OPT ≥ OPT /2 (By Lemma H.3), sixth step follows g2 eighth step follows Ti(o) = ≤ according o-partition γ-important partition, see Deﬁnition D.6 Deﬁnition H.1), ninth step follows ρ ∈ (0, ≤ o ≤ OPT.
 i seventh step follows Ti(o) = d2 g2 i−1 ≤ ρo Thus, complete proof.
 Lemma H.5. Given point set Q associated partition P parameter o ∈ (0, OPT].
 Let QI ⊂ Q denote set γ-important points, γ /(200Ld3ρ).
 important level i ∈ R, let(cid:80)|Pi| p∈QI Proof.
 j=1 |Pi,j| = ai · Ti(o).
 have, Z ⊂ [∆]d |Z| = k, p ∈ QI, (cid:88) i∈R ai
 (cid:88) (cid:80) dist2(p, Z) q∈QI dist2(q, Z) ≤ · (cid:88) (cid:88) (cid:88) (cid:80) (cid:88) dist2(p, Z) q∈QI dist2(q, Z) Pi,j p∈Pi,j∩QI p∈QI Ti(o) d3 d3 |Pi,j| · Ti(o) Pi,j ≤60d3 · (cid:88) i∈R ai
 ﬁrst inequality follows Lemma H.4. H.4 Algorithm goal Section is prove Theorem H.6, Theorem H.6 (k-means, linear k).
 Fix , δ ∈ (0, k, ∆ ∈ N+, let L = log ∆.
 is data structure supporting insertions deletions point set P ⊂ [∆]d, maintaining weighted set S positive weights point, such probability least − δ, S is -coreset k-means size data structure uses −3k poly(d, L, log(1/δ)).
 −3k poly(d, L, log(1/δ)) bits worst case.
 update input, algorithm needs poly(d, L, log k) time process outputs coreset time poly(d, k, L, log k) pass stream.
 Remark H.7. level i, are kinds points, (1) points partition, (2) points heavy cell (these points partition level j ∈ {i + L}), (3) points non- partition non-heavy cell (these points partition level j ∈ {0, i−
 ﬁrst introduce auxiliary concentration results.
 Theorem H.8 ([BR94]).
 Let k be integer, let X be sum n k-wise independent random variables taking values
 Let µ = E[X] >
 have (cid:20) (cid:21) (cid:18) kµ + k2 (cid:19)k/2 a2 Pr |X − µ| > ≤ · Algorithm Sensitivity-Based Sampling procedure Oracle() procedure Init(i, o, Gi) (cid:46) Gi is i-th grid m ← c + d3Lk−2(dL log(dLk) + log(1/δ))) constant c RecordPoints ← SampleStream+.Init(∆, d, c1kL + c3mL, c2m/k, c3mL, c1kL, δ/(d∆d)c4) (cid:46) c1, c2, c3, c4 are constants j do Prhj∼H [hj(p) = = [∆]d → {0, s.t. (cid:46) Ti(o) is given Deﬁnition D.3 (log(1/δ))-wise independent hash hj independent Construct end procedure procedure Update(c, p, op) j do hj(p) c ← Cell(p) Gi RecordPoints.Update(c, (p, j), op) end end end end procedure end procedure procedure Query(j(cid:48)) (cid:101)C,(cid:101)f S1, S2,


 S100m ← RecordPoints.Query() (cid:46) Sj contains samples form (p, j) (cid:98)Sj ← ∅ C ∈ (cid:101)C do Return Fail RecordPoints returns Fail Identify heavy cells using samples S1, S2,


 S100m (cid:46) Partition cell: cell is heavy, parent is criterion is based (cid:101)f (C) (cid:98)Sj ← (cid:98)Sj ∪ Sj,C (cid:46) Sj,C denotes set samples Sj containing C C is partition cell level end end end procedure procedure EstNumPts( Let j denotes j(cid:48)-th non-empty (cid:98)Sj(cid:48).
 such j(cid:48) exists, return Fail return A uniform sample (cid:98)Sj Denote same (cid:98)Sj Query return | ∪j (cid:98)Sj| · Ti(o) · k/(100m) Return step Query returns Fail end procedure Lemma H.9. Let π ∈ [0, be ﬁxed value.
 Suppose have m independent (4 log(1/δ))-wise hash functions h1, h2,


 hm [∆]d → {0, ∀i ∈ [m], p ∈ [∆]d Pr[hi(p) = = π.
 Let set S ⊂ [∆]d denote hi(S) = {s | hi(s) s ∈ S}.
 i ∈ [m], s ∈ S, deﬁne be hi(s).
 Let X =(cid:80)m s∈S Xi,s.
 probability least − δ, (cid:80) i=1 (cid:12)(cid:12)X − mπ|S| (cid:12)(cid:12) ≤ mπ|S| provided m ≥ c log δ(1/δ) π|S|2 large constant c ≥
 O ← {1, poly(d, ∆d)} ρ ← Construct G−1, G0,··· GL Initialize ← oracle.Init(l, o, Gi) procedure Init(k, ∆, d, , δ) Algorithm procedure SampleKMeansLinear() end procedure end procedure procedure Update(p, op) l → L do end oracleo,l.Update(p,op) o ∈ O do end procedure end (cid:46) grids deﬁned Section D.1 using parameter ρ (cid:46) Algorithm Ao,i, ﬂago,i ← Oracleo,i.EstNumPts() So,i ← ∅ end γ ← /(200Ld3ρ) (cid:46) Important levels R ← {i | Ao,i > γ · Ti(o)} m ← c + d3Lk−2(dL log(dLk) + log(1/δ))) constant c j → m do Ao,i Sample level i ∈ R probability q, ﬂag ← Oracleo,i.Query(j) ﬂag (cid:54)= Fail i(cid:48)∈R Ao,i(cid:48)/Ti(cid:48)(o))/(m/Ti(o)) (cid:80) i(cid:48)∈R Ao,i(cid:48) o ∈ O do procedure Query() i = → L do Algorithm procedure SampleKMeansLinear() end procedure So,i ← So,i ∪ (q, wq) So,i ← ∅ break wq ← ((cid:80) end o∗ return So end procedure end else end ← arg mino∈O{So,i (cid:54)= ∅, ﬂago,i (cid:54)= Fail,∀i ∈ {0, L − (cid:46) Algorithm (cid:46) weights i-th level (cid:46) So = So,0 ∪ So,1 ∪ ··· ∪ S0,L−1 Proof.
 is obvious m(cid:88) (cid:88) i=1 s∈S X = Xi,s = m(cid:88) (cid:88) i=1 s∈S hi(s) = m(cid:88) i=1 |hi(S)|.
 have Xi,s is (log(1/δ))-wise independent random variable.
 m ≥ c log(1/δ) π|S|2 have Let k = log(1/δ).
 Lemma H.8, probability least mπ|S| ≥ c log(1/δ) 2mπ|S| ≥ ck.
 have − + k2)/(mπ|S|)2)k/2 ≥ − (c/2)k/2 ≥ − δ, (cid:12)(cid:12)X − mπ|S| (cid:12)(cid:12) ≤ mπ|S| provided c ≥
 Lemma H.10.
 Let k ≥ be ﬁxed value.
 Suppose have independent pairwise hash functions h1, h2,


 h100m [∆]d → {0, ∀i ∈ [100m], p ∈ [∆]d |S|· Pr[hi(p) = =
 i=1
 probability least − δ, Let set S ⊂ [∆]d denote hi(S) = {s hi(s) (cid:54)= s ∈ S}.
 Let X =(cid:80)m X > m/k, provided m/k ≥ c log(1/δ) large constant c.
 Proof.
 Denote, j =


 Yj = i=10(j−1)k+1 |hi(S)|.
 E[Yj] = Var[Yj] ≤
 Chebyshev’s inequality, have Pr[|Yj − ≥ ≤ = Thus E[X] ≥
 Denote Xj = i=10(j−1)k+1
 j=1 |Pi,j| = ai · Ti(o).
 Conditioned StreamSampling+ Xj is independent random variable.
 lemma follows applying Chernoﬀ bound over(cid:80) Xj. Claim H.11.
 i ∈ R, let (cid:80)|Pi| doesn’t fail, probability least − δ, o, i, number non-empty oracleo,i.(cid:98)S1, oracleo,i.(cid:98)S1, ··· oracleo,i.(cid:98)S100m is least mai/k.
 Notice expectation |oracleo,i.(cid:98)Sj| = ai/k.
 mai/k ≥ c(cid:48) log(∆d/δ) suﬃcient least − number non-empty oracleo,i.(cid:98)S1, oracleo,i.(cid:98)S1, ··· oracleo,i.(cid:98)S100m is Claim H.12.
 i ∈ R, let(cid:80)|Pi| o∗, probability least − δ,(cid:80) Proof.
 i ∈ R, γ = /(200Ld3ρ).
 know ai ≥ /(200Ld3ρ) Deﬁnition H.1. large constant c(cid:48), Lemma H.10 taking union bound o, i, have probability j=1 |Pi,j| = ai · Ti(o).
 smallest valid o ∈ (0, OPT], i,e, i(cid:48)∈R ai(cid:48) >
 least mai/k.
 Proof.
 Let ﬁx o ∈ O, i ∈ {0,··· L −
 E[memory size kTi(o) · (#partition points #non-partition non-heavy points) kTi(o) (cid:33) (cid:88) (cid:32) c2m k · #heavy cells level i ai(cid:48)Ti(cid:48)(o) L(cid:88) ai · Ti(o) + i(cid:48)<i c2m k · #heavy cells level i i=0 i=0 ai ai (cid:33) k · (100 + c2) c2m k · #heavy cells level i (cid:32) L(cid:88) o is smallest and(cid:80)L i=0 ai let = o/2.
 probability point is sampled blow expected memory blows twice.
 m is large enough, probability least − actual memory used o(cid:48) is most expectation used memory size.
 means o(cid:48) is smaller valid guess, leads contradiction o is smallest valid guess.
 Claim H.13.
 Given parameters γ ∈ smallest valid guess o ∈ (0, OPT].
 Let j=1 |Pi,j| = ai · Ti(o).
 Let R = {i ∈ {0, L} | m denote number samples sampled levels (Algorithm
 probability least − number samples sampled level i is most mai/(50k).
 Proof.
 Recall m is number samples Algorithm
 compute expectation number samples choosing level i, j=1 |Pi,j| > γ · Ti(o)}.
 i ∈ R, let (cid:80)|Pi| (cid:80)|Pi| E[#samples choosing level i] = m · = m · (cid:20) Pr ∃o, i, #samples choosing level i ≥ m · (cid:80) (cid:80) (cid:80) ai(cid:80) i(cid:48)∈R i(cid:48)∈R ai(cid:48) p∈Pi d3/Ti(o) p∈Pi(cid:48) d3/Ti(cid:48)(o) (cid:21) ai(cid:80) i(cid:48)∈R ai(cid:48) · ≤ δ, m is large, using Chernoﬀ bound taking union bound o, i, have Further using Claim H.12, have ai(cid:80) i(cid:48)∈R ai(cid:48) · m · ≤ mai Thus, probability − δ, o, i, number samples sampled level i is most mai/(50k).
 Lemma H.14.
 end one-pass stream, ﬁxed ≤ OPT, i ∈ L}, know Pi is important not.
 Pi is γ-important, output Ao,i such |Pi|(cid:88) j=1 |Pi|(cid:88) j=1 (1 − ) |Pi,j| ≤ Ao,i ≤ (1 + ) |Pi,j| probability − δ.
 Proof.
 Note algorithm point is copied times.
 ith level, point is sampled probability
 heavy cell has least Ti(o) number points, according Lemma H.9, number points heavy cell are sampled is least (1 − )100m/k.
 Thus, end stream, are able identify heavy cells means given are able determine is partition point, heavy cell point non-partition non-heavy cell point ith level.
 consider level i is important, means number partition points is least γTi(o).
 Recall γ /(200Ld3ρ).
 m is apply Lemma H.9 such number partition points level i are sampled is range (1±)100m/(kTi(o))· j=1 |Pi,j|.
 j=1 |Pi,j| ≤ Ao,i ≤ (1+)(cid:80)|Pi| (cid:80)|Pi| j=1 |Pi,j|.
 Thus, have (1−)(cid:80)|Pi| (cid:80)|Pi| j=1 |Pi,j| > γ · Ti(o)}.
 i ∈ R, let (cid:80)|Pi| probability least − δ,(cid:80) Proof.
 prove contradiction.
 Suppose(cid:80) i∈R ai ≤ O(kL).
 Claim H.15.
 Given parameters γ ∈ valid guess o ∈ (0, OPT].
 Let R = {i ∈ j=1 |Pi,j| = ai · Ti(o).
 {0, L} | i∈R ai ≥ ω(kL).
 According Lemma H.9, number partition points stored data structure be least contradicts total space used data structure.
 following, give whole proof main theorem.
 proof Theorem H.6. is easy see total space used is small: maintained |O| × L number oracles, Lemma E.4, oracle uses space most k−3 poly(L, d, log k, log d, log(1/δ)) bits.
 Now, let look correctness.
 argue algorithm output valid o∗ o∗ < OPT
 Let ∈ [OPT /2, OPT], look level i, total number non-heavy cell i /(2d)2) ﬁrst term is non-heavy cell points points is most /(g2 are center cells, second term is non-heavy cell points are i /(2d)2) have total number non- center cells.
 OPT /(g2 heavy cell points level i is most
 Thus, due Lemma H.9, total number sampled non-heavy cell points is most O(mL).
 Furthermore, due Lemma D.5, total number heavy cells is most O(kL).
 Thus, oracles FAIL.
 algorithm choose o∗ is most OPT
 o∗ is valid guess, according Claim H.15, Lemma H.5 Theorem H.2, m samples is enough get good coreset.
 following proof, goal is prove m samples is good.
 According Lemma H.14, identify important levels.
 according Lemma H.3, need show m samples provide coreset important points.
 According Lemma H.14, estimate number partition points important level approximation factor ± .
 sample important level i probability proportional (cid:80) (cid:80) (cid:80) p∈Pi d3/Ti(o) i(cid:48)∈R p∈Pi(cid:48) d3/Ti(cid:48)(o) factor ± .
 only thing remaining prove is implement uniform sampling partition points important level.
 o∗ is smallest valid guess o, Claim H.13 shows number uniform samples needed level i is upper bounded mai/(50k).
 Due Claim H.11, number uniform samples corresponding oracle provided is least mai/k.
 get enough uniform samples.
 are able sample m i.i.d. samples such point p is chosen probability proportional (cid:80) i(cid:48)∈R (1 ± ) (cid:80) d3/Ti(o) p∈Pi(cid:48) d3/Ti(cid:48)(o) p is partition point level i.
 applying Lemma H.4, Theorem H.2 Lemma H.3, complete proof correctness.

