. The extension of deep learning towards temporal data pro-
cessing is gaining an increasing research interest. In this paper we inves-
tigate the properties of state dynamics developed in successive levels of
deep recurrent neural networks (RNNs) in terms of short-term memory
abilities. Our results reveal interesting insights that shed light on the na-
ture of layering as a factor of RNN design. Noticeably, higher layers in a
hierarchically organized RNN architecture results to be inherently biased
towards longer memory spans even prior to training of the recurrent con-
nections. Moreover, in the context of Reservoir Computing framework,
our analysis also points out the beneﬁt of a layered recurrent organization
as an eﬃcient approach to improve the memory skills of reservoir models.
