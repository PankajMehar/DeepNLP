
This paper deals with the reality gap from a novel
perspective, targeting transferring Deep Reinforce-
ment Learning (DRL) policies learned in simulated
environments to the real-world domain for visual
control tasks. Instead of adopting the common so-
lutions to the problem by increasing the visual ﬁ-
delity of synthetic images output from simulators
during the training phase, this paper seeks to tackle
the problem by translating the real-world image
streams back to the synthetic domain during the de-
ployment phase, to make the robot feel at home. We
propose this as a lightweight, ﬂexible, and efﬁcient
solution for visual control, as 1) no extra trans-
fer steps are required during the expensive training
of DRL agents in simulation; 2) the trained DRL
agents will not be constrained to being deployable
in only one speciﬁc real-world environment; 3) the
policy training and the transfer operations are de-
coupled, and can be conducted in parallel. Besides
this, we propose a conceptually simple yet very ef-
fective shift loss to constrain the consistency be-
tween subsequent frames, eliminating the need for
optical ﬂow. We validate the shift loss for artis-
tic style transfer for videos and domain adaptation,
and validate our visual control approach in real-
world robot experiments. A video of our results
is available at: https://goo.gl/b1xz1s.
