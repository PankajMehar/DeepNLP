
Though deep neural networks (DNNs) achieve
remarkable performances in many artiÔ¨Åcial intel-
ligence tasks, the lack of training instances re-
mains a notorious challenge. As the network goes
deeper, the generalization accuracy decays rapidly
in the situation of lacking massive amounts of
training data. In this paper, we propose novel
deep neural network structures that can be inher-
ited from all existing DNNs with almost the same
level of complexity, and develop simple training
algorithms. We show our paradigm successfully
resolves the lack of data issue. Tests on the CI-
FAR10 and CIFAR100 image recognition datasets
show that the new paradigm leads to 20% to 30%
relative error rate reduction compared to their base
DNNs. The intuition of our algorithms for deep
residual network stems from theories of the par-
tial differential equation (PDE) control problems.
Code will be made available.
