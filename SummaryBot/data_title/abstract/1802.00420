
We identify obfuscated gradients as a phe-
nomenon that leads to a false sense of security
in defenses against adversarial examples. While
defenses that cause obfuscated gradients appear
to defeat optimization-based attacks, we ﬁnd de-
fenses relying on this effect can be circumvented.
For each of the three types of obfuscated gradients
we discover, we describe indicators of defenses ex-
hibiting this effect and develop attack techniques
to overcome it. In a case study, examining all
defenses accepted to ICLR 2018, we ﬁnd obfus-
cated gradients are a common occurrence, with
7 of 8 defenses relying on obfuscated gradients.
Using our new attack techniques, we successfully
circumvent all 7 of them.
