—In the last decade, an active area of research has
been devoted to design novel activation functions that are able
to help deep neural networks to converge, obtaining better per-
formance. The training procedure of these architectures usually
involves optimization of the weights of their layers only, while
non-linearities are generally pre-speciﬁed and their (possible)
parameters are usually considered as hyper-parameters to be
tuned manually. In this paper, we introduce two approaches
to automatically learn different combinations of base activation
functions (such as the identity function, ReLU, and tanh) during
the training phase. We present a thorough comparison of our
novel approaches with well-known architectures (such as LeNet-
5, AlexNet, and ResNet-56) on three standard datasets (Fashion-
MNIST, CIFAR-10, and ILSVRC-2012), showing substantial
improvements in the overall performance, such as an increase
in the top-1 accuracy for AlexNet on ILSVRC-2012 of 3.01
percentage points.
